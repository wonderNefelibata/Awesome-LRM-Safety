import arxiv
import datetime
import json
import re
from tenacity import retry, stop_after_attempt, wait_fixed

# é…ç½®å‚æ•°
QUERY_TERMS = {
    'include': [
        "DeepSeek-R1",
        "DeepSeek R1",
        "o1",
        "o3",
        "o3-mini",
        "safety",
        "jailbreak",
        "Long Chain-of-Thought Reasoning",
        "CoT-enabled models",
    ],
    'exclude': []
}
MAX_NEW_PAPERS = 100
LATEST_PAPERS_COUNT = 30

def extract_arxiv_id(url):
    """ä»arXiv URLä¸­æå–åŸºç¡€IDï¼ˆä¸å«ç‰ˆæœ¬å·ï¼‰"""
    match = re.search(r'/(?:abs|pdf)/(.*?)(?:v\d+)?(?:\.pdf)?$', url)
    return match.group(1) if match else None

@retry(stop=stop_after_attempt(3), wait=wait_fixed(5))
def fetch_papers():
    """ä»arXivè·å–æœ€æ–°è®ºæ–‡"""
    query = " OR ".join([f'all:"{term}"' for term in QUERY_TERMS['include']])
    search = arxiv.Search(
        query=query,
        max_results=MAX_NEW_PAPERS,
        sort_by=arxiv.SortCriterion.SubmittedDate,
        sort_order=arxiv.SortOrder.Descending
    )
    
    client = arxiv.Client(delay_seconds=1)
    return [{
        "title": result.title,
        "url": result.entry_id,
        "arxiv_id": result.get_short_id(),
        "authors": [a.name for a in result.authors],
        "published": result.published.isoformat(),
        "summary": result.summary.replace('\n', ' ')[:150] + '...'
    } for result in client.results(search)]

def update_article_json(new_papers):
    """æ›´æ–°è®ºæ–‡æ•°æ®åº“"""
    try:
        with open('article.json', 'r') as f:
            existing_papers = json.load(f)
    except (FileNotFoundError, json.JSONDecodeError):
        existing_papers = []

    # åˆå¹¶æ–°æ—§è®ºæ–‡å¹¶å»é‡
    papers_dict = {}
    for paper in existing_papers + new_papers:
        # å¤„ç†æ—§æ•°æ®ç¼ºå°‘arxiv_idçš„æƒ…å†µ
        if 'arxiv_id' not in paper:
            paper['arxiv_id'] = extract_arxiv_id(paper['url'])
        
        # ä¿ç•™æœ€æ–°ç‰ˆæœ¬
        pid = paper['arxiv_id']
        existing = papers_dict.get(pid)
        if not existing or datetime.datetime.fromisoformat(paper['published']) > datetime.datetime.fromisoformat(existing['published']):
            papers_dict[pid] = paper

    updated_papers = sorted(papers_dict.values(), 
                          key=lambda x: x['published'], 
                          reverse=True)
    
    with open('article.json', 'w') as f:
        json.dump(updated_papers, f, indent=2)
    print(f"è®ºæ–‡æ•°æ®åº“å·²æ›´æ–°ï¼Œå½“å‰æ€»æ•°ï¼š{len(updated_papers)}ç¯‡")

def generate_markdown_table(papers, title=""):
    """ç”ŸæˆMarkdownè¡¨æ ¼"""
    if not papers:
        return ""
    
    table = ""
    if title:
        table += f"\n\n## {title}\n\n"
    table += "| Date       | Title                                      | Authors           | Abstract Summary          |\n"
    table += "|------------|--------------------------------------------|-------------------|---------------------------|\n"
    
    for p in papers:
        authors = ', '.join(p['authors'][:2]) + (' et al.' if len(p['authors']) > 2 else '')
        table += f"| {p['published'][:10]} | [{p['title']}]({p['url']}) | {authors} | {p['summary']} |\n"
    return table

def update_readme():
    """æ›´æ–°READMEæ–‡ä»¶"""
    # è¯»å–è®ºæ–‡æ•°æ®
    try:
        with open('article.json', 'r') as f:
            all_papers = sorted(json.load(f), 
                              key=lambda x: x['published'], 
                              reverse=True)
    except FileNotFoundError:
        all_papers = []
    
    # åˆ†å‰²æœ€æ–°å’Œå†å²è®ºæ–‡
    latest = all_papers[:LATEST_PAPERS_COUNT]
    historical = all_papers[LATEST_PAPERS_COUNT:]
    
    # ç”Ÿæˆæœ€æ–°è¡¨æ ¼
    latest_table = generate_markdown_table(latest, "Latest arXiv Papers (Auto-Updated)")
    
    # ç”Ÿæˆå†å²è¡¨æ ¼ï¼ˆå¯æŠ˜å ï¼‰
    history_section = ""
    if historical:
        history_table = generate_markdown_table(historical, "Historical Papers")
        history_section = f"""
<details>
<summary>ğŸ“š View Historical Papers ({len(historical)} entries)</summary>

{history_table}
</details>
"""
    # æ›´æ–°READMEå†…å®¹
    with open('README.md', 'r+', encoding='utf-8') as f:
        content = f.read()
        placeholder = '<!-- ARXIV_PAPERS -->'
        start = content.find(placeholder)
        
        if start != -1:
            new_content = content[:start + len(placeholder)] + latest_table + history_section
            f.seek(0)
            f.truncate()
            f.write(new_content)
            print("READMEæ›´æ–°æˆåŠŸï¼")
        else:
            print("âš ï¸ æœªæ‰¾åˆ°å ä½ç¬¦ï¼Œè¯·ç¡®è®¤READMEä¸­åŒ…å«<!-- ARXIV_PAPERS -->")

if __name__ == "__main__":
    new_papers = fetch_papers()
    update_article_json(new_papers)
    update_readme()
