import arxiv
import datetime
import json
import re
from tenacity import retry, stop_after_attempt, wait_fixed

# é…ç½®å‚æ•°ï¼ˆéœ€è¦å†ä¼˜åŒ–ï¼‰
QUERY_TERMS = {
    'include': [
        "cat:cs",  # computer science
        "DeepSeek-R1",
        "DeepSeek R1",
        "o1",
        "o3",
        "o3-mini",
        "safety",
        "jailbreak",
        "Long Chain-of-Thought Reasoning",
        "CoT-enabled models",
    ],
    'exclude': []
}

MAX_NEW_PAPERS = 50 # æ¯æ¬¡å°è¯•è·å–çš„æœ€å¤§è®ºæ–‡æ•°
LATEST_PAPERS_COUNT = 20 # åœ¨ä¸»é¡µé¢ä¸Šæ˜¾ç¤ºçš„æœ€æ–°è®ºæ–‡æ•°é‡

def extract_arxiv_id(url):
    """ä»arXiv URLä¸­æå–åŸºç¡€IDï¼ˆä¸å«ç‰ˆæœ¬å·ï¼‰"""
    match = re.search(r'/(?:abs|pdf)/(.*?)(?:v\d+)?(?:\.pdf)?$', url)
    return match.group(1) if match else None

@retry(stop=stop_after_attempt(3), wait=wait_fixed(5))
def fetch_papers():
    """ä»arXivè·å–æœ€æ–°è®ºæ–‡"""
    query = " OR ".join([f'all:"{term}"' for term in QUERY_TERMS['include']])
    search = arxiv.Search(
        query=query,
        max_results=MAX_NEW_PAPERS,
        sort_by=arxiv.SortCriterion.SubmittedDate,
        sort_order=arxiv.SortOrder.Descending
    )
    
    client = arxiv.Client(delay_seconds=1)
    return [{
        "title": result.title,
        "url": result.entry_id,
        "arxiv_id": result.get_short_id(),
        "authors": [a.name for a in result.authors],
        "published": result.published.isoformat(),
        "summary": result.summary.replace('\n', ' ')  # å®Œæ•´æ‘˜è¦
        # "summary": result.summary.replace('\n', ' ')[:150] + '...'
    } for result in client.results(search)]

def update_article_json(new_papers):
    """æ›´æ–°è®ºæ–‡æ•°æ®åº“"""
    try:
        with open('article.json', 'r') as f:
            existing_papers = json.load(f) # è¯»å–ç°æœ‰æ•°æ®
    except (FileNotFoundError, json.JSONDecodeError):
        existing_papers = []

    # åˆå¹¶æ–°æ—§è®ºæ–‡å¹¶å»é‡
    papers_dict = {}
    for paper in existing_papers + new_papers:
        # å¤„ç†æ—§æ•°æ®ç¼ºå°‘arxiv_idçš„æƒ…å†µ
        if 'arxiv_id' not in paper:
            paper['arxiv_id'] = extract_arxiv_id(paper['url'])
        
        # ä¿ç•™æœ€æ–°ç‰ˆæœ¬
        pid = paper['arxiv_id']
        existing = papers_dict.get(pid)
        if not existing or datetime.datetime.fromisoformat(paper['published']) > datetime.datetime.fromisoformat(existing['published']):
            papers_dict[pid] = paper

    updated_papers = sorted(papers_dict.values(), 
                          key=lambda x: x['published'], 
                          reverse=True)
    
    with open('article.json', 'w') as f:
        json.dump(updated_papers, f, indent=2)
    print(f"è®ºæ–‡æ•°æ®åº“å·²æ›´æ–°ï¼Œå½“å‰æ€»æ•°ï¼š{len(updated_papers)}ç¯‡")

def generate_markdown_table(papers):
    """ç”ŸæˆMarkdownè¡¨æ ¼"""
    if not papers:
        return ""
    
    table = ""
    table += "\n\n"
    table += "| Date       | Title                                      | Authors           | Abstract Summary          |\n"
    table += "|------------|--------------------------------------------|-------------------|---------------------------|\n"
    
    for p in papers:
        authors = ', '.join(p['authors'][:2]) + (' et al.' if len(p['authors']) > 2 else '')
        table += f"| {p['published'][:10]} | [{p['title']}]({p['url']}) | {authors} | {p['summary']} |\n"
    return table

def update_readme():
    """æ›´æ–°READMEæ–‡ä»¶"""
    # è¯»å–è®ºæ–‡æ•°æ®
    try:
        with open('article.json', 'r') as f:
            all_papers = sorted(json.load(f), 
                              key=lambda x: x['published'], 
                              reverse=True)
    except FileNotFoundError:
        all_papers = []
    
    # åˆ†å‰²æœ€æ–°å’Œå†å²è®ºæ–‡
    latest = all_papers[:LATEST_PAPERS_COUNT]
    historical = all_papers[LATEST_PAPERS_COUNT:]
    
    # ç”Ÿæˆæœ€æ–°è¡¨æ ¼
    latest_table = generate_markdown_table(latest)

    # ç”Ÿæˆå†å²è¡¨æ ¼
    history_section = generate_markdown_table(historical)

#     # ç”Ÿæˆå†å²è¡¨æ ¼ï¼ˆå¯æŠ˜å ï¼‰
#     history_section = ""
#     if historical:
#         history_table = generate_markdown_table(historical)
#         history_section = f"""
# <details>
# <summary>ğŸ“š View Historical Papers ({len(historical)} entries)</summary>

# {history_table}
# </details>
# """
        
    # æ›´æ–°ä¸»é¡µREADMEå†…å®¹
    with open('README.md', 'r+', encoding='utf-8') as f:
        content = f.read()

        placeholder1 = '<!-- LATEST_PAPERS_START -->'
        placeholder2 = '<!-- LATEST_PAPERS_END -->'

        start1 = content.find(placeholder1)
        end1 = content.find(placeholder2)

        # print("start1:", start1)
        # print("end1:", end1)
        # print("content[start1:end1]:", content[start1:end1])
        new_content = content.replace(content[start1:end1], 
                                          "<!-- LATEST_PAPERS_START -->")
        
        new_content = new_content.replace("<!-- LATEST_PAPERS_START --><!-- LATEST_PAPERS_END -->", 
                                          f"<!-- LATEST_PAPERS_START -->\n{latest_table}\n<!-- LATEST_PAPERS_END -->")

        # placeholder3 = '<!-- HISTORICAL_PAPERS_START -->'
        # placeholder4 = '<!-- HISTORICAL_PAPERS_END -->'

        # start2 = new_content.find(placeholder3)
        # end2 = new_content.find(placeholder4)

        # print("start2:", start2)
        # print("end2:", end2)
        # print("new_content[start2:end2]:", new_content[start2:end2])
        # new_content = new_content.replace(new_content[start2:end2], 
        #                                   "<!-- HISTORICAL_PAPERS_START -->")

        
        # print("latest_table:", latest_table)
        # print("history_section:", history_section)
        # new_content = new_content.replace("<!-- LATEST_PAPERS_START --><!-- LATEST_PAPERS_END -->", 
        #                                   f"<!-- LATEST_PAPERS_START -->\n{latest_table}\n<!-- LATEST_PAPERS_END -->").replace("<!-- HISTORICAL_PAPERS_START --><!-- HISTORICAL_PAPERS_END -->",
        #                                   f"<!-- HISTORICAL_PAPERS_START -->\n{history_section}\n<!-- HISTORICAL_PAPERS_END -->")
        
        # æŠŠnew_contentå†™è¿›README.md
        f.seek(0) # å›åˆ°æ–‡ä»¶å¼€å¤´
        f.write(new_content)
        f.truncate() # æˆªæ–­æ–‡ä»¶ï¼Œå»æ‰åŸæ¥æ–‡ä»¶ä¸­å¤šä½™çš„å†…å®¹

    # æ›´æ–°./articles/README.mdçš„å†…å®¹
    with open('./articles/README.md', 'r+', encoding='utf-8') as f:
        f.seek(0) 
        f.write(history_section)
        f.truncate() 
        


if __name__ == "__main__":
    new_papers = fetch_papers()
    update_article_json(new_papers)
    update_readme()
