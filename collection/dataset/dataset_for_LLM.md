|Name|Paper|
|----|----|
|"red teaming"|[Red Teaming Language Models with Language Models](https://arxiv.org/pdf/2202.03286)|
|JBB-Behaviors|[JailbreakBench:An Open Robustness Benchmark for Jailbreaking Large Language Models](https://arxiv.org/abs/2404.01318)|
|HarmBench|[HarmBench:A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal](https://arxiv.org/abs/2402.04249)|
|AdvBench|[Universal and Transferable Adversarial Attackson Aligned Language Models](https://arxiv.org/abs/2307.15043)|
|AIR-bench-2024|[AIRBENCH 2024:A Safety Benchmark Based on Risk Categories from Regulations and Policies](https://arxiv.org/pdf/2407.17436)|
|Malicious-Educator(Applicationrequired)|[H-CoT: Hijacking the Chain-of-Thought Safety Reasoning Mechanism to Jailbreak Large Reasoning Models, Including OpenAI o1/o3, DeepSeek-R1, and Gemini 2.0 Flash Thinking](https://arxiv.org/html/2502.12893v1)|
|MITRE|[CYBERSECEVAL3:Advancing the Evaluation of Cyber security Risks and Capabilities in Large Language Models](https://arxiv.org/pdf/2408.01605)|
|Interpreter||
|Phishing||
|XSTest|Xstest: A test suite for identifying exaggerated safety behaviours in large language models|
|WildGuard(Applicationrequired)|Wildguard: Open one-stop moderation tools for safety risks, jailbreaks, and refusals of llms|
|Injection||
|FRACTURED-SORRY-Bench||
|MaliciousInstruct||
|Chinesesafetyassessmentbenchmark||
|ShadowAlignment||
|In-The-WildJailbreakPrompts||
|JailbreakBench||
|IMDb||
|MultiJail||
|XSafety||
|JailbreakHub|“Do Anything Now”: Characterizing and Evaluating In-The-Wild|
|PARDEN||
|EasyJailbreak||

