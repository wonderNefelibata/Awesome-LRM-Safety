[
  {
    "title": "Aligning Multimodal LLM with Human Preference: A Survey",
    "url": "http://arxiv.org/abs/2503.14504v1",
    "arxiv_id": "2503.14504v1",
    "authors": [
      "Tao Yu",
      "Yi-Fan Zhang",
      "Chaoyou Fu",
      "Junkang Wu",
      "Jinda Lu",
      "Kun Wang",
      "Xingyu Lu",
      "Yunhang Shen",
      "Guibin Zhang",
      "Dingjie Song",
      "Yibo Yan",
      "Tianlong Xu",
      "Qingsong Wen",
      "Zhang Zhang",
      "Yan Huang",
      "Liang Wang",
      "Tieniu Tan"
    ],
    "published": "2025-03-18T17:59:56+00:00",
    "summary": "Large language models (LLMs) can handle a wide variety of general tasks with simple prompts, without the need for task-specific training. Multimodal Large Language Models (MLLMs), built upon LLMs, have demonstrated impressive potential in tackling complex tasks involving visual, auditory, and textual data. However, critical issues related to truthfulness, safety, o1-like reasoning, and alignment with human preference remain insufficiently addressed. This gap has spurred the emergence of various alignment algorithms, each targeting different application scenarios and optimization goals. Recent studies have shown that alignment algorithms are a powerful approach to resolving the aforementioned challenges. In this paper, we aim to provide a comprehensive and systematic review of alignment algorithms for MLLMs. Specifically, we explore four key aspects: (1) the application scenarios covered by alignment algorithms, including general image understanding, multi-image, video, and audio, and extended multimodal applications; (2) the core factors in constructing alignment datasets, including data sources, model responses, and preference annotations; (3) the benchmarks used to evaluate alignment algorithms; and (4) a discussion of potential future directions for the development of alignment algorithms. This work seeks to help researchers organize current advancements in the field and inspire better alignment methods. The project page of this paper is available at https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/tree/Alignment."
  },
  {
    "title": "Temporal Consistency for LLM Reasoning Process Error Identification",
    "url": "http://arxiv.org/abs/2503.14495v1",
    "arxiv_id": "2503.14495v1",
    "authors": [
      "Jiacheng Guo",
      "Yue Wu",
      "Jiahao Qiu",
      "Kaixuan Huang",
      "Xinzhe Juan",
      "Ling Yang",
      "Mengdi Wang"
    ],
    "published": "2025-03-18T17:58:28+00:00",
    "summary": "Verification is crucial for effective mathematical reasoning. We present a new temporal consistency method where verifiers iteratively refine their judgments based on the previous assessment. Unlike one-round verification or multi-model debate approaches, our method leverages consistency in a sequence of self-reflection actions to improve verification accuracy. Empirical evaluations across diverse mathematical process error identification benchmarks (Mathcheck, ProcessBench, and PRM800K) show consistent performance improvements over baseline methods. When applied to the recent DeepSeek R1 distilled models, our method demonstrates strong performance, enabling 7B/8B distilled models to outperform all 70B/72B models and GPT-4o on ProcessBench. Notably, the distilled 14B model with our method achieves performance comparable to Deepseek-R1. Our codes are available at https://github.com/jcguo123/Temporal-Consistency"
  },
  {
    "title": "DAPO: An Open-Source LLM Reinforcement Learning System at Scale",
    "url": "http://arxiv.org/abs/2503.14476v1",
    "arxiv_id": "2503.14476v1",
    "authors": [
      "Qiying Yu",
      "Zheng Zhang",
      "Ruofei Zhu",
      "Yufeng Yuan",
      "Xiaochen Zuo",
      "Yu Yue",
      "Tiantian Fan",
      "Gaohong Liu",
      "Lingjun Liu",
      "Xin Liu",
      "Haibin Lin",
      "Zhiqi Lin",
      "Bole Ma",
      "Guangming Sheng",
      "Yuxuan Tong",
      "Chi Zhang",
      "Mofan Zhang",
      "Wang Zhang",
      "Hang Zhu",
      "Jinhua Zhu",
      "Jiaze Chen",
      "Jiangjie Chen",
      "Chengyi Wang",
      "Hongli Yu",
      "Weinan Dai",
      "Yuxuan Song",
      "Xiangpeng Wei",
      "Hao Zhou",
      "Jingjing Liu",
      "Wei-Ying Ma",
      "Ya-Qin Zhang",
      "Lin Yan",
      "Mu Qiao",
      "Yonghui Wu",
      "Mingxuan Wang"
    ],
    "published": "2025-03-18T17:49:06+00:00",
    "summary": "Inference scaling empowers LLMs with unprecedented reasoning ability, with reinforcement learning as the core technique to elicit complex reasoning. However, key technical details of state-of-the-art reasoning LLMs are concealed (such as in OpenAI o1 blog and DeepSeek R1 technical report), thus the community still struggles to reproduce their RL training results. We propose the $\\textbf{D}$ecoupled Clip and $\\textbf{D}$ynamic s$\\textbf{A}$mpling $\\textbf{P}$olicy $\\textbf{O}$ptimization ($\\textbf{DAPO}$) algorithm, and fully open-source a state-of-the-art large-scale RL system that achieves 50 points on AIME 2024 using Qwen2.5-32B base model. Unlike previous works that withhold training details, we introduce four key techniques of our algorithm that make large-scale LLM RL a success. In addition, we open-source our training code, which is built on the verl framework, along with a carefully curated and processed dataset. These components of our open-source system enhance reproducibility and support future research in large-scale LLM RL."
  },
  {
    "title": "PENCIL: Long Thoughts with Short Memory",
    "url": "http://arxiv.org/abs/2503.14337v1",
    "arxiv_id": "2503.14337v1",
    "authors": [
      "Chenxiao Yang",
      "Nathan Srebro",
      "David McAllester",
      "Zhiyuan Li"
    ],
    "published": "2025-03-18T15:14:14+00:00",
    "summary": "While recent works (e.g. o1, DeepSeek R1) have demonstrated great promise of using long Chain-of-Thought (CoT) to improve reasoning capabilities of language models, scaling it up during test-time is challenging due to inefficient memory usage -- intermediate computations accumulate indefinitely in context even no longer needed for future thoughts. We propose PENCIL, which incorporates a reduction mechanism into the autoregressive generation process, allowing the model to recursively clean up intermediate thoughts based on patterns learned from training. With this reduction mechanism, PENCIL significantly reduces the maximal context length required during generation, and thus can generate longer thoughts with limited memory, solving larger-scale problems given more thinking time. For example, we demonstrate PENCIL achieves 97\\% accuracy on the challenging Einstein's puzzle -- a task even large models like GPT-4 struggle with -- using only a small 25M-parameter transformer with 2048 context length. Theoretically, we prove PENCIL can perform universal space-efficient computation by simulating Turing machines with optimal time and space complexity, and thus can solve arbitrary computational tasks that would otherwise be intractable given context window constraints."
  },
  {
    "title": "ADAPT: An Autonomous Forklift for Construction Site Operation",
    "url": "http://arxiv.org/abs/2503.14331v1",
    "arxiv_id": "2503.14331v1",
    "authors": [
      "Johannes Huemer",
      "Markus Murschitz",
      "Matthias Sch\u00f6rghuber",
      "Lukas Reisinger",
      "Thomas Kadiofsky",
      "Christoph Weidinger",
      "Mario Niedermeyer",
      "Benedikt Widy",
      "Marcel Zeilinger",
      "Csaba Beleznai",
      "Tobias Gl\u00fcck",
      "Andreas Kugi",
      "Patrik Zips"
    ],
    "published": "2025-03-18T15:03:28+00:00",
    "summary": "Efficient material logistics play a critical role in controlling costs and schedules in the construction industry. However, manual material handling remains prone to inefficiencies, delays, and safety risks. Autonomous forklifts offer a promising solution to streamline on-site logistics, reducing reliance on human operators and mitigating labor shortages. This paper presents the development and evaluation of the Autonomous Dynamic All-terrain Pallet Transporter (ADAPT), a fully autonomous off-road forklift designed for construction environments. Unlike structured warehouse settings, construction sites pose significant challenges, including dynamic obstacles, unstructured terrain, and varying weather conditions. To address these challenges, our system integrates AI-driven perception techniques with traditional approaches for decision making, planning, and control, enabling reliable operation in complex environments. We validate the system through extensive real-world testing, comparing its long-term performance against an experienced human operator across various weather conditions. We also provide a comprehensive analysis of challenges and key lessons learned, contributing to the advancement of autonomous heavy machinery. Our findings demonstrate that autonomous outdoor forklifts can operate near human-level performance, offering a viable path toward safer and more efficient construction logistics."
  },
  {
    "title": "Multi-Parameter Analysis of Li-ion Battery Degradation: Integrating Optical Fiber Sensing with Differential State of Health Metrics",
    "url": "http://arxiv.org/abs/2503.14327v1",
    "arxiv_id": "2503.14327v1",
    "authors": [
      "Idris Temitope Bello",
      "Hassan Raza",
      "Madithedu Muneeswara",
      "Neha Tewari",
      "Yin Nee Cheung",
      "Tobi Alabi Michael",
      "Ridwan Taiwo",
      "Fiske Lin"
    ],
    "published": "2025-03-18T15:01:30+00:00",
    "summary": "The reliability and safety of Lithium-ion batteries (LiBs) are of great concern in the energy storage industry. Nevertheless, the real-time monitoring of their degradation remains challenging due to limited quantitative metrics available during cycling. This study addresses this limitation by employing a novel approach that combines external optical fiber sensing with advanced data analysis techniques to comprehensively assess battery health. We engineered a non-invasive optical sensing platform using tandem pairs of polymeric and silica-based fiber Bragg grating (FBG) sensors affixed to the external surface of commercial Li-ion button cells, enabling simultaneous, real-time monitoring of device-level volume changes and thermal events over 600 cycles. Our analysis incorporated differential techniques to estimate the battery's state of health (SOH) based on capacity, strain, and temperature variations with respect to voltage. Additionally, we implemented and compared three deep learning models - Long Short-Term Memory (LSTM), Gated Recurrent Unit (GRU), and Artificial Neural Network (ANN) - to predict battery SOH over cycles. We were able to capture both continuous and spontaneous degradation events and provide unique insights into battery behavior across its lifecycle through differential analysis and new SOH metrics demonstrating high correlation with conventional measures. This multi-parameter approach, combining advanced sensing techniques with innovative data analysis and deep learning methods, contributes significantly to battery diagnostics, potentially improving reliability assessment, enhancing safety standards, and accelerating the development of more sustainable energy storage solutions."
  },
  {
    "title": "Conversational Agents as Catalysts for Critical Thinking: Challenging Social Influence in Group Decision-making",
    "url": "http://arxiv.org/abs/2503.14263v1",
    "arxiv_id": "2503.14263v1",
    "authors": [
      "Soohwan Lee",
      "Seoyeong Hwang",
      "Dajung Kim",
      "Kyungho Lee"
    ],
    "published": "2025-03-18T13:54:12+00:00",
    "summary": "Group decision-making processes frequently suffer when social influence and power dynamics suppress minority viewpoints, leading to compliance and groupthink. Conversational agents can counteract these harmful dynamics by encouraging critical thinking. This study investigates how LLM-powered devil's advocate systems affect psychological safety, opinion expression, and satisfaction in power-imbalanced group dynamics. We conducted an experiment with 48 participants in 12 four-person groups, each containing three high-power (senior) and one low-power (junior) member. Each group completed decision tasks in both baseline and AI intervention conditions. Results show AI counterarguments fostered a more flexible atmosphere and significantly enhanced both process and outcome satisfaction for all participants, with particularly notable improvements for minority members. Cognitive workload increased slightly, though not significantly. This research contributes empirical evidence on how AI systems can effectively navigate power hierarchies to foster more inclusive decision-making environments, highlighting the importance of balancing intervention frequency, maintaining conversational flow, and preserving group cohesion."
  },
  {
    "title": "A Chain-Driven, Sandwich-Legged Quadruped Robot: Design and Experimental Analysis",
    "url": "http://arxiv.org/abs/2503.14255v1",
    "arxiv_id": "2503.14255v1",
    "authors": [
      "Aman Singh",
      "Bhavya Giri Goswami",
      "Ketan Nehete",
      "Shishir N. Y. Kolathaya"
    ],
    "published": "2025-03-18T13:44:34+00:00",
    "summary": "This paper introduces a chain-driven, sandwich-legged, mid-size quadruped robot designed as an accessible research platform. The design prioritizes enhanced locomotion capabilities, improved reliability and safety of the actuation system, and simplified, cost-effective manufacturing processes. Locomotion performance is optimized through a sandwiched leg design and a dual-motor configuration, reducing leg inertia for agile movements. Reliability and safety are achieved by integrating robust cable strain reliefs, efficient heat sinks for motor thermal management, and mechanical limits to restrict leg motion. Simplified design considerations include a quasi-direct drive (QDD) actuator and the adoption of low-cost fabrication techniques, such as laser cutting and 3D printing, to minimize cost and ensure rapid prototyping. The robot weighs approximately 25 kg and is developed at a cost under \\$8000, making it a scalable and affordable solution for robotics research. Experimental validations demonstrate the platform's capability to execute trot and crawl gaits on flat terrain and slopes, highlighting its potential as a versatile and reliable quadruped research platform."
  },
  {
    "title": "Inferring Event Descriptions from Time Series with Language Models",
    "url": "http://arxiv.org/abs/2503.14190v1",
    "arxiv_id": "2503.14190v1",
    "authors": [
      "Mingtian Tan",
      "Mike A. Merrill",
      "Zack Gottesman",
      "Tim Althoff",
      "David Evans",
      "Tom Hartvigsen"
    ],
    "published": "2025-03-18T12:07:33+00:00",
    "summary": "Time series data measure how environments change over time and drive decision-making in critical domains like finance and healthcare. When analyzing time series, we often seek to understand the underlying events occurring in the measured environment. For example, one might ask: What caused a sharp drop in the stock price? Events are often described with natural language, so we conduct the first study of whether Large Language Models (LLMs) can infer natural language events from time series. We curate a new benchmark featuring win probabilities collected from 4,200 basketball and American football games, featuring 1.7M timesteps with real value data and corresponding natural language events. Building on the recent wave of using LLMs on time series, we evaluate 16 LLMs and find that they demonstrate promising abilities to infer events from time series data. The open-weights DeepSeek-R1 32B model outperforms proprietary models like GPT-4o. Despite this impressive initial performance, we also find clear avenues to improve recent models, as we identify failures when altering the provided context, event sequence lengths, and evaluation strategy. (All resources needed to reproduce our work are available: https://github.com/BennyTMT/GAMETime)"
  },
  {
    "title": "Towards Harmless Multimodal Assistants with Blind Preference Optimization",
    "url": "http://arxiv.org/abs/2503.14189v1",
    "arxiv_id": "2503.14189v1",
    "authors": [
      "Yongqi Li",
      "Lu Yang",
      "Jian Wang",
      "Runyang You",
      "Wenjie Li",
      "Liqiang Nie"
    ],
    "published": "2025-03-18T12:02:38+00:00",
    "summary": "Multimodal Large Language Models (MLLMs) have demonstrated impressive capabilities in multimodal understanding, reasoning, and interaction. Given the extensive applications of MLLMs, the associated safety issues have become increasingly critical. Due to the effectiveness of preference optimization in aligning MLLMs with human preferences, there is an urgent need for safety-related preference data for MLLMs. To address this, we construct the MMSafe-PO preference dataset towards harmless multimodal assistants, featuring multimodal instructions, the conversational format, and ranked paired responses from human feedback. We also identify two insightful observations: modality co-defense and modality cheating, which illustrate that MLLMs possess a certain level of inherent defense while still presenting unique safety challenges. Based on these observations, we propose the Blind Preference Optimization (BPO) approach. Comprehensive experiments on three benchmarks show that BPO effectively enhances the safety capabilities of MLLMs. Notably, BPO significantly improves the safety rate of the base MLLM by 45.0%, outperforming the DPO approach. Additionally, applying BPO to the MMSafe-PO dataset greatly reduces the base MLLM's unsafe rate on other safety benchmarks (14.5% on MM-SafetyBench and 82.9% on HarmEval, demonstrating the effectiveness and robustness of both the dataset and the approach. We release code and data at https://lu-yang666.github.io/MMsafe-PO-Web/."
  },
  {
    "title": "Gravitational-wave Extraction using Independent Component Analysis",
    "url": "http://arxiv.org/abs/2503.14179v1",
    "arxiv_id": "2503.14179v1",
    "authors": [
      "Rika Shimomura",
      "Yuuichi Tabe",
      "Hisaaki Shinkai"
    ],
    "published": "2025-03-18T11:54:50+00:00",
    "summary": "Independent component analysis (ICA) is a method to extract a set of time-series data using \"statistical independency\" of each component. We propose applying ICA for extracting gravitational wave (GW) signals. Our idea is to extract a signal that is commonly included in multiple detectors and to find it by shifting the data-set around its arrival time. In this article, we show several tests using injected signals, and show that this method can be applied to events for signal-to-noise over 15. We then demonstrate the method to actual O1-O3 events, and the identification of the arrival time can be estimated more precisely than that was previously reported. This approach does not require templates of waveform, therefore it can be applied for testing general relativity, and also for finding unknown GW."
  },
  {
    "title": "DangerMaps: Personalized Safety Advice for Travel in Urban Environments using a Retrieval-Augmented Language Model",
    "url": "http://arxiv.org/abs/2503.14103v1",
    "arxiv_id": "2503.14103v1",
    "authors": [
      "Jonas Oppenlaender"
    ],
    "published": "2025-03-18T10:18:07+00:00",
    "summary": "Planning a trip into a potentially unsafe area is a difficult task. We conducted a formative study on travelers' information needs, finding that most of them turn to search engines for trip planning. Search engines, however, fail to provide easily interpretable results adapted to the context and personal information needs of a traveler. Large language models (LLMs) create new possibilities for providing personalized travel safety advice. To explore this idea, we developed DangerMaps, a mapping system that assists its users in researching the safety of an urban travel destination, whether it is pre-travel or on-location. DangerMaps plots safety ratings onto a map and provides explanations on demand. This late breaking work specifically emphasizes the challenges of designing real-world applications with large language models. We provide a detailed description of our approach to prompt design and highlight future areas of research."
  },
  {
    "title": "Robust Safety Critical Control Under Multiple State and Input Constraints: Volume Control Barrier Function Method",
    "url": "http://arxiv.org/abs/2503.13996v1",
    "arxiv_id": "2503.13996v1",
    "authors": [
      "Jinyang Dong",
      "Shizhen Wu",
      "Rui Liu",
      "Xiao Liang",
      "Biao Lu",
      "Yongchun Fang"
    ],
    "published": "2025-03-18T07:58:58+00:00",
    "summary": "In this paper, the safety-critical control problem for uncertain systems under multiple control barrier function (CBF) constraints and input constraints is investigated. A novel framework is proposed to generate a safety filter that minimizes changes to reference inputs when safety risks arise, ensuring a balance between safety and performance. A nonlinear disturbance observer (DOB) based on the robust integral of the sign of the error (RISE) is used to estimate system uncertainties, ensuring that the estimation error converges to zero exponentially. This error bound is integrated into the safety-critical controller to reduce conservativeness while ensuring safety. To further address the challenges arising from multiple CBF and input constraints, a novel Volume CBF (VCBF) is proposed by analyzing the feasible space of the quadratic programming (QP) problem. % ensuring solution feasibility by keeping the volume as a positive value. To ensure that the feasible space does not vanish under disturbances, a DOB-VCBF-based method is introduced, ensuring system safety while maintaining the feasibility of the resulting QP. Subsequently, several groups of simulation and experimental results are provided to validate the effectiveness of the proposed controller."
  },
  {
    "title": "Empowering Smaller Models: Tuning LLaMA and Gemma with Chain-of-Thought for Ukrainian Exam Tasks",
    "url": "http://arxiv.org/abs/2503.13988v1",
    "arxiv_id": "2503.13988v1",
    "authors": [
      "Mykyta Syromiatnikov",
      "Victoria Ruvinskaya",
      "Nataliia Komleva"
    ],
    "published": "2025-03-18T07:44:49+00:00",
    "summary": "Leading large language models have demonstrated impressive capabilities in reasoning-intensive tasks, such as standardized educational testing. However, they often require extensive training in low-resource settings with inaccessible infrastructure. Small or compact models, though more efficient, frequently lack sufficient support for underrepresented languages, leaving a performance gap in critical domains. This work explores the potential of parameter-efficient fine-tuning of compact open-weight language models to handle reasoning-intensive tasks in the underrepresented Ukrainian language, building on the findings of the ZNO-Eval benchmark. Parameter-efficient fine-tuning of LLaMA 3.1 (8 billion parameters), LLaMA 3.2 (3 billion parameters), and Gemma 2 (9 billion parameters) models on chain-of-thought solutions resulted in a modest test score improvement of up to 17.4% on complex matching tasks and 1.6% overall compared to tuning on answer letters alone, offering enhanced interpretability and robustness. In addition, the proposed tuning method with joint task topic and step-by-step solution generation outperforms standard chain-of-thought tuning in matching tasks and provides a 5.4% gain over the best LLaMA 3.2 model due to guiding the model to recall and apply domain-relevant information. Contrasting obtained results with zero-shot evaluations of leading open-weight and proprietary models such as Qwen, DeepSeek R1, OpenAI o1 and o3, Gemini, and Claude, highlight that fine-tuning LLaMA and Gemma models with 2,032 step-by-step solutions and 20 to 50 million trainable parameters on a single A100 GPU lets them outperform GPT-4o mini, Mistral Large, and larger open-weight models. This research also evaluates how merging the quantized adapter with the base model influences the generation quality. Source code and tuned models are available at https://github.com/NLPForUA/ZNO."
  },
  {
    "title": "Survey of Adversarial Robustness in Multimodal Large Language Models",
    "url": "http://arxiv.org/abs/2503.13962v1",
    "arxiv_id": "2503.13962v1",
    "authors": [
      "Chengze Jiang",
      "Zhuangzhuang Wang",
      "Minjing Dong",
      "Jie Gui"
    ],
    "published": "2025-03-18T06:54:59+00:00",
    "summary": "Multimodal Large Language Models (MLLMs) have demonstrated exceptional performance in artificial intelligence by facilitating integrated understanding across diverse modalities, including text, images, video, audio, and speech. However, their deployment in real-world applications raises significant concerns about adversarial vulnerabilities that could compromise their safety and reliability. Unlike unimodal models, MLLMs face unique challenges due to the interdependencies among modalities, making them susceptible to modality-specific threats and cross-modal adversarial manipulations. This paper reviews the adversarial robustness of MLLMs, covering different modalities. We begin with an overview of MLLMs and a taxonomy of adversarial attacks tailored to each modality. Next, we review key datasets and evaluation metrics used to assess the robustness of MLLMs. After that, we provide an in-depth review of attacks targeting MLLMs across different modalities. Our survey also identifies critical challenges and suggests promising future research directions."
  },
  {
    "title": "What was Said, What was not Said",
    "url": "http://arxiv.org/abs/2503.13958v1",
    "arxiv_id": "2503.13958v1",
    "authors": [
      "Hamid Jahanian"
    ],
    "published": "2025-03-18T06:50:58+00:00",
    "summary": "In the process industry, the configuration of Safety Instrumented Systems (SIS) must comply with a defined set of safety requirements, typically documented in the Safety Requirements Specification (SRS). The functional safety standard IEC 61511 outlines the necessary content and quality criteria for the SRS. However, developing an effective SRS can be challenging. This article examines some of these challenges and proposes good practices to address them. It discusses SRS ownership, \"staged\" development of SRS, and the classification and traceability of requirements. Additionally, it explores the issue of untold \"negative\" requirements and suggests exploratory \"inspection\" of SIS Application Programs (APs) as a potential remedy."
  },
  {
    "title": "Joint ADS-B in B5G for Hierarchical UAV Networks: Performance Analysis and MEC Based Optimization",
    "url": "http://arxiv.org/abs/2503.13907v1",
    "arxiv_id": "2503.13907v1",
    "authors": [
      "Chao Dong",
      "Yiyang Liao",
      "Ziye Jia",
      "Qihui Wu",
      "Lei Zhang"
    ],
    "published": "2025-03-18T05:10:11+00:00",
    "summary": "Unmanned aerial vehicles (UAVs) play significant roles in multiple fields, which brings great challenges for the airspace safety. In order to achieve efficient surveillance and break the limitation of application scenarios caused by single communication, we propose the collaborative surveillance model for hierarchical UAVs based on the cooperation of automatic dependent surveillance-broadcast (ADS-B) and 5G. Specifically, UAVs are hierarchical deployed, with the low-altitude central UAV equipped with the 5G module, and the high-altitude central UAV with ADS-B, which helps automatically broadcast the flight information to surrounding aircraft and ground stations. Firstly, we build the framework, derive the analytic expression, and analyze the channel performance of both air-to-ground (A2G) and air-to-air (A2A). Then, since the redundancy or information loss during transmission aggravates the monitoring performance, the mobile edge computing (MEC) based on-board processing algorithm is proposed. Finally, the performances of the proposed model and algorithm are verified through both simulations and experiments. In detail, the redundant data filtered out by the proposed algorithm accounts for 53.48%, and the supplementary data accounts for 16.42% of the optimized data. This work designs a UAV monitoring framework and proposes an algorithm to enhance the observability of trajectory surveillance, which helps improve the airspace safety and enhance the air traffic flow management."
  },
  {
    "title": "Constraints on LIGO/Virgo Compact Object Mergers from Late-time Radio Observations",
    "url": "http://arxiv.org/abs/2503.13884v1",
    "arxiv_id": "2503.13884v1",
    "authors": [
      "Ashna Gulati",
      "Tara Murphy",
      "Dougal Dobie",
      "Adam Deller",
      "David L. Kaplan",
      "Emil Lenc",
      "Ilya Mandel",
      "Stefan Duchesne",
      "Vanessa Moss"
    ],
    "published": "2025-03-18T04:29:27+00:00",
    "summary": "We present results from a search for radio afterglows of compact object mergers conducted with the Australian SKA Pathfinder. We used data from four epochs of the Rapid ASKAP Continuum Survey to search compact binary merger localization regions observed during the LIGO/Virgo O2, and O3 observing runs. Our investigation focused on eleven events (published in the GWTC-1, GWTC-2, and GWTC-3 catalogues of gravitational-wave events) with 90\\% posterior localisations smaller than $150\\,\\deg^2$ and $\\ge$99\\% probabilities of being of astrophysical origin, to identify potential radio afterglow-like transients up to $\\lesssim$1500 days post-merger. We identified candidate afterglow-type variable sources in the 90\\% localisation for events -- GW190503, GW200202 and GW200208, which were ruled out as unlikely to be related to the corresponding GW event on further analysis. Since we find no likely candidate counterparts, we constrain the inclination angle and the circum-merger density at isotropic equivalent energies ranging from $2\\times10^{51} -1\\times10^{54}\\rm \\:erg$. These constraints are based on the assumption that the electron energy distribution in the associated jets follows a power-law index of $ p = 2.2$, with 1% of the shock energy in the magnetic field ($ \\epsilon_B = 0.01$) and 10% in the electrons ($\\epsilon_e = 0.1$). We discuss the detectability of late-time afterglows as a function of merger distance and inclination angles with millijansky surveys."
  },
  {
    "title": "FlexStep: Enabling Flexible Error Detection in Multi/Many-core Real-time Systems",
    "url": "http://arxiv.org/abs/2503.13848v1",
    "arxiv_id": "2503.13848v1",
    "authors": [
      "Tinglue Wang",
      "Yiming Li",
      "Wei Tang",
      "Jiapeng Guan",
      "Zhenghui Guo",
      "Renshuang Jiang",
      "Ran Wei",
      "Jing Li",
      "Zhe Jiang"
    ],
    "published": "2025-03-18T02:42:51+00:00",
    "summary": "Reliability and real-time responsiveness in safety-critical systems have traditionally been achieved using error detection mechanisms, such as LockStep, which require pre-configured checker cores,strict synchronisation between main and checker cores, static error detection regions, or limited preemption capabilities. However, these core-bound hardware mechanisms often lead to significant resource over-provisioning, and diminished real-time responsiveness, particularly in modern systems where tasks with varying reliability requirements are consolidated on shared processors to improve efficiency, reduce costs, and save power. To address these challenges, this work presents FlexStep, a systematic solution that integrates hardware and software across the SoC, ISA, and OS scheduling layers. FlexStep features a novel microarchitecture that supports dynamic core configuration and asynchronous, preemptive error detection. The FlexStep architecture naturally allows for flexible task scheduling and error detection, enabling new scheduling algorithms that enhance both resource efficiency and real-time schedulability. We publicly release FlexStep's source code, at https://anonymous.4open.science/r/FlexStep-DAC25-7B0C."
  },
  {
    "title": "Automatic MILP Model Construction for Multi-Robot Task Allocation and Scheduling Based on Large Language Models",
    "url": "http://arxiv.org/abs/2503.13813v1",
    "arxiv_id": "2503.13813v1",
    "authors": [
      "Mingming Peng",
      "Zhendong Chen",
      "Jie Yang",
      "Jin Huang",
      "Zhengqi Shi",
      "Qihao Liu",
      "Xinyu Li",
      "Liang Gao"
    ],
    "published": "2025-03-18T01:45:19+00:00",
    "summary": "With the accelerated development of Industry 4.0, intelligent manufacturing systems increasingly require efficient task allocation and scheduling in multi-robot systems. However, existing methods rely on domain expertise and face challenges in adapting to dynamic production constraints. Additionally, enterprises have high privacy requirements for production scheduling data, which prevents the use of cloud-based large language models (LLMs) for solution development. To address these challenges, there is an urgent need for an automated modeling solution that meets data privacy requirements. This study proposes a knowledge-augmented mixed integer linear programming (MILP) automated formulation framework, integrating local LLMs with domain-specific knowledge bases to generate executable code from natural language descriptions automatically. The framework employs a knowledge-guided DeepSeek-R1-Distill-Qwen-32B model to extract complex spatiotemporal constraints (82% average accuracy) and leverages a supervised fine-tuned Qwen2.5-Coder-7B-Instruct model for efficient MILP code generation (90% average accuracy). Experimental results demonstrate that the framework successfully achieves automatic modeling in the aircraft skin manufacturing case while ensuring data privacy and computational efficiency. This research provides a low-barrier and highly reliable technical path for modeling in complex industrial scenarios."
  },
  {
    "title": "Do Large Language Models Understand Performance Optimization?",
    "url": "http://arxiv.org/abs/2503.13772v1",
    "arxiv_id": "2503.13772v1",
    "authors": [
      "Bowen Cui",
      "Tejas Ramesh",
      "Oscar Hernandez",
      "Keren Zhou"
    ],
    "published": "2025-03-17T23:30:23+00:00",
    "summary": "Large Language Models (LLMs) have emerged as powerful tools for software development tasks such as code completion, translation, and optimization. However, their ability to generate efficient and correct code, particularly in complex High-Performance Computing (HPC) contexts, has remained underexplored. To address this gap, this paper presents a comprehensive benchmark suite encompassing multiple critical HPC computational motifs to evaluate the performance of code optimized by state-of-the-art LLMs, including OpenAI o1, Claude-3.5, and Llama-3.2. In addition to analyzing basic computational kernels, we developed an agent system that integrates LLMs to assess their effectiveness in real HPC applications. Our evaluation focused on key criteria such as execution time, correctness, and understanding of HPC-specific concepts. We also compared the results with those achieved using traditional HPC optimization tools. Based on the findings, we recognized the strengths of LLMs in understanding human instructions and performing automated code transformations. However, we also identified significant limitations, including their tendency to generate incorrect code and their challenges in comprehending complex control and data flows in sophisticated HPC code."
  },
  {
    "title": "Do Unit Proofs Work? An Empirical Study of Compositional Bounded Model Checking for Memory Safety Verification",
    "url": "http://arxiv.org/abs/2503.13762v1",
    "arxiv_id": "2503.13762v1",
    "authors": [
      "Paschal C. Amusuo",
      "Owen Cochell",
      "Taylor Le Lievre",
      "Parth V. Patil",
      "Aravind Machiry",
      "James C. Davis"
    ],
    "published": "2025-03-17T22:55:12+00:00",
    "summary": "Memory safety defects pose a major threat to software reliability, enabling cyberattacks, outages, and crashes. To mitigate these risks, organizations adopt Compositional Bounded Model Checking (BMC), using unit proofs to formally verify memory safety. However, methods for creating unit proofs vary across organizations and are inconsistent within the same project, leading to errors and missed defects. In addition, unit proofing remains understudied, with no systematic development methods or empirical evaluations.   This work presents the first empirical study on unit proofing for memory safety verification. We introduce a systematic method for creating unit proofs that leverages verification feedback and objective criteria. Using this approach, we develop 73 unit proofs for four embedded operating systems and evaluate their effectiveness, characteristics, cost, and generalizability. Our results show unit proofs are cost-effective, detecting 74\\% of recreated defects, with an additional 9\\% found with increased BMC bounds, and 19 new defects exposed. We also found that embedded software requires small unit proofs, which can be developed in 87 minutes and executed in 61 minutes on average. These findings provide practical guidance for engineers and empirical data to inform tooling design."
  },
  {
    "title": "Optimal Replenishment Policies for Industrial Vending Machines",
    "url": "http://arxiv.org/abs/2503.13643v1",
    "arxiv_id": "2503.13643v1",
    "authors": [
      "Karina M. Sindermann",
      "Esma S. Gel",
      "Nesim K. Erkip"
    ],
    "published": "2025-03-17T18:47:11+00:00",
    "summary": "Industrial Vending Machines (IVMs) automate the dispensing of a variety of supplies like safety equipment and tools at customer sites, providing 24/7 access while tracking inventory in real-time. Industrial distribution companies typically manage the replenishment of IVMs using periodic schedules, which do not take advantage of these advanced real-time monitoring capabilities. We develop two approaches to optimize the long-term average cost of replenishments and stockouts per unit time: a state-dependent optimal control policy that jointly considers all inventory levels (referred to as trigger set policy) and a fixed cycle policy that optimizes replenishment frequency. We prove the monotonicity of the optimal trigger set policy and leverage it to design a computationally efficient approximate online control framework. Unlike existing methods, which typically handle a very limited number of items due to computational constraints, our approach scales to hundreds of items while achieving near-optimal performance. Leveraging transaction data from our industrial partner, we conduct an extensive set of numerical experiments to demonstrate this claim. Our results show that optimal fixed cycle replenishment reduces costs by 61.7 to 78.6% compared to current practice, with our online control framework delivering an additional 4.1 to 22.9% improvement. Our novel theoretical results provide practical tools for effective replenishment management in this modern vendor-managed inventory context."
  },
  {
    "title": "MetaScale: Test-Time Scaling with Evolving Meta-Thoughts",
    "url": "http://arxiv.org/abs/2503.13447v1",
    "arxiv_id": "2503.13447v1",
    "authors": [
      "Qin Liu",
      "Wenxuan Zhou",
      "Nan Xu",
      "James Y. Huang",
      "Fei Wang",
      "Sheng Zhang",
      "Hoifung Poon",
      "Muhao Chen"
    ],
    "published": "2025-03-17T17:59:54+00:00",
    "summary": "One critical challenge for large language models (LLMs) for making complex reasoning is their reliance on matching reasoning patterns from training data, instead of proactively selecting the most appropriate cognitive strategy to solve a given task. Existing approaches impose fixed cognitive structures that enhance performance in specific tasks but lack adaptability across diverse scenarios. To address this limitation, we introduce METASCALE, a test-time scaling framework based on meta-thoughts -- adaptive thinking strategies tailored to each task. METASCALE initializes a pool of candidate meta-thoughts, then iteratively selects and evaluates them using a multi-armed bandit algorithm with upper confidence bound selection, guided by a reward model. To further enhance adaptability, a genetic algorithm evolves high-reward meta-thoughts, refining and extending the strategy pool over time. By dynamically proposing and optimizing meta-thoughts at inference time, METASCALE improves both accuracy and generalization across a wide range of tasks. Experimental results demonstrate that MetaScale consistently outperforms standard inference approaches, achieving an 11% performance gain in win rate on Arena-Hard for GPT-4o, surpassing o1-mini by 0.9% under style control. Notably, METASCALE scales more effectively with increasing sampling budgets and produces more structured, expert-level responses."
  },
  {
    "title": "Faithfulness of LLM Self-Explanations for Commonsense Tasks: Larger Is Better, and Instruction-Tuning Allows Trade-Offs but Not Pareto Dominance",
    "url": "http://arxiv.org/abs/2503.13445v1",
    "arxiv_id": "2503.13445v1",
    "authors": [
      "Noah Y. Siegel",
      "Nicolas Heess",
      "Maria Perez-Ortiz",
      "Oana-Maria Camburu"
    ],
    "published": "2025-03-17T17:59:39+00:00",
    "summary": "As large language models (LLMs) become increasingly capable, ensuring that their self-generated explanations are faithful to their internal decision-making process is critical for safety and oversight. In this work, we conduct a comprehensive counterfactual faithfulness analysis across 62 models from 8 families, encompassing both pretrained and instruction-tuned variants and significantly extending prior studies of counterfactual tests. We introduce phi-CCT, a simplified variant of the Correlational Counterfactual Test, which avoids the need for token probabilities while explaining most of the variance of the original test. Our findings reveal clear scaling trends: larger models are consistently more faithful on our metrics. However, when comparing instruction-tuned and human-imitated explanations, we find that observed differences in faithfulness can often be attributed to explanation verbosity, leading to shifts along the true-positive/false-positive Pareto frontier. While instruction-tuning and prompting can influence this trade-off, we find limited evidence that they fundamentally expand the frontier of explanatory faithfulness beyond what is achievable with pretrained models of comparable size. Our analysis highlights the nuanced relationship between instruction-tuning, verbosity, and the faithful representation of model decision processes."
  },
  {
    "title": "Escaping Plato's Cave: Robust Conceptual Reasoning through Interpretable 3D Neural Object Volumes",
    "url": "http://arxiv.org/abs/2503.13429v1",
    "arxiv_id": "2503.13429v1",
    "authors": [
      "Nhi Pham",
      "Bernt Schiele",
      "Adam Kortylewski",
      "Jonas Fischer"
    ],
    "published": "2025-03-17T17:55:15+00:00",
    "summary": "With the rise of neural networks, especially in high-stakes applications, these networks need two properties (i) robustness and (ii) interpretability to ensure their safety. Recent advances in classifiers with 3D volumetric object representations have demonstrated a greatly enhanced robustness in out-of-distribution data. However, these 3D-aware classifiers have not been studied from the perspective of interpretability. We introduce CAVE - Concept Aware Volumes for Explanations - a new direction that unifies interpretability and robustness in image classification. We design an inherently-interpretable and robust classifier by extending existing 3D-aware classifiers with concepts extracted from their volumetric representations for classification. In an array of quantitative metrics for interpretability, we compare against different concept-based approaches across the explainable AI literature and show that CAVE discovers well-grounded concepts that are used consistently across images, while achieving superior robustness."
  },
  {
    "title": "Securing Virtual Reality Experiences: Unveiling and Tackling Cybersickness Attacks with Explainable AI",
    "url": "http://arxiv.org/abs/2503.13419v1",
    "arxiv_id": "2503.13419v1",
    "authors": [
      "Ripan Kumar Kundu",
      "Matthew Denton",
      "Genova Mongalo",
      "Prasad Calyam",
      "Khaza Anuarul Hoque"
    ],
    "published": "2025-03-17T17:49:51+00:00",
    "summary": "The synergy between virtual reality (VR) and artificial intelligence (AI), specifically deep learning (DL)-based cybersickness detection models, has ushered in unprecedented advancements in immersive experiences by automatically detecting cybersickness severity and adaptively various mitigation techniques, offering a smooth and comfortable VR experience. While this DL-enabled cybersickness detection method provides promising solutions for enhancing user experiences, it also introduces new risks since these models are vulnerable to adversarial attacks; a small perturbation of the input data that is visually undetectable to human observers can fool the cybersickness detection model and trigger unexpected mitigation, thus disrupting user immersive experiences (UIX) and even posing safety risks. In this paper, we present a new type of VR attack, i.e., a cybersickness attack, which successfully stops the triggering of cybersickness mitigation by fooling DL-based cybersickness detection models and dramatically hinders the UIX. Next, we propose a novel explainable artificial intelligence (XAI)-guided cybersickness attack detection framework to detect such attacks in VR to ensure UIX and a comfortable VR experience. We evaluate the proposed attack and the detection framework using two state-of-the-art open-source VR cybersickness datasets: Simulation 2021 and Gameplay dataset. Finally, to verify the effectiveness of our proposed method, we implement the attack and the XAI-based detection using a testbed with a custom-built VR roller coaster simulation with an HTC Vive Pro Eye headset and perform a user study. Our study shows that such an attack can dramatically hinder the UIX. However, our proposed XAI-guided cybersickness attack detection can successfully detect cybersickness attacks and trigger the proper mitigation, effectively reducing VR cybersickness."
  },
  {
    "title": "Continuous-time Data-driven Barrier Certificate Synthesis",
    "url": "http://arxiv.org/abs/2503.13392v1",
    "arxiv_id": "2503.13392v1",
    "authors": [
      "Luke Rickard",
      "Alessandro Abate",
      "Kostas Margellos"
    ],
    "published": "2025-03-17T17:25:32+00:00",
    "summary": "We consider the problem of verifying safety for continuous-time dynamical systems. Developing upon recent advancements in data-driven verification, we use only a finite number of sampled trajectories to learn a barrier certificate, namely a function which verifies safety. We train a safety-informed neural network to act as this certificate, with an appropriately designed loss function to encompass the safety conditions. In addition, we provide probabilistic generalisation guarantees from discrete samples of continuous trajectories, to unseen continuous ones. Numerical investigations demonstrate the efficacy of our approach and contrast it with related results in the literature."
  },
  {
    "title": "Mitigating Visual Forgetting via Take-along Visual Conditioning for Multi-modal Long CoT Reasoning",
    "url": "http://arxiv.org/abs/2503.13360v1",
    "arxiv_id": "2503.13360v1",
    "authors": [
      "Hai-Long Sun",
      "Zhun Sun",
      "Houwen Peng",
      "Han-Jia Ye"
    ],
    "published": "2025-03-17T16:45:12+00:00",
    "summary": "Recent advancements in Large Language Models (LLMs) have demonstrated enhanced reasoning capabilities, evolving from Chain-of-Thought (CoT) prompting to advanced, product-oriented solutions like OpenAI o1. During our re-implementation of this model, we noticed that in multimodal tasks requiring visual input (e.g., geometry problems), Multimodal LLMs (MLLMs) struggle to maintain focus on the visual information, in other words, MLLMs suffer from a gradual decline in attention to visual information as reasoning progresses, causing text-over-relied outputs. To investigate this, we ablate image inputs during long-chain reasoning. Concretely, we truncate the reasoning process midway, then re-complete the reasoning process with the input image removed. We observe only a ~2% accuracy drop on MathVista's test-hard subset, revealing the model's textual outputs dominate the following reasoning process. Motivated by this, we propose Take-along Visual Conditioning (TVC), a strategy that shifts image input to critical reasoning stages and compresses redundant visual tokens via dynamic pruning. This methodology helps the model retain attention to the visual components throughout the reasoning. Our approach achieves state-of-the-art performance on average across five mathematical reasoning benchmarks (+3.4% vs previous sota), demonstrating the effectiveness of TVC in enhancing multimodal reasoning systems."
  },
  {
    "title": "Reliable and Efficient Amortized Model-based Evaluation",
    "url": "http://arxiv.org/abs/2503.13335v1",
    "arxiv_id": "2503.13335v1",
    "authors": [
      "Sang Truong",
      "Yuheng Tu",
      "Percy Liang",
      "Bo Li",
      "Sanmi Koyejo"
    ],
    "published": "2025-03-17T16:15:02+00:00",
    "summary": "Comprehensive evaluations of language models (LM) during both development and deployment phases are necessary because these models possess numerous capabilities (e.g., mathematical reasoning, legal support, or medical diagnostic) as well as safety risks (e.g., racial bias, toxicity, or misinformation). The average score across a wide range of benchmarks provides a signal that helps guide the use of these LMs in practice. Currently, holistic evaluations are costly due to the large volume of benchmark questions, making frequent evaluations impractical. A popular attempt to lower the cost is to compute the average score on a subset of the benchmark. This approach, unfortunately, often renders an unreliable measure of LM performance because the average score is often confounded with the difficulty of the questions in the benchmark subset. Item response theory (IRT) was designed to address this challenge, providing a reliable measurement by careful controlling for question difficulty. Unfortunately, question difficulty is expensive to estimate. Facing this challenge, we train a model that predicts question difficulty from its content, enabling a reliable measurement at a fraction of the cost. In addition, we leverage this difficulty predictor to further improve the evaluation efficiency through training a question generator given a difficulty level. This question generator is essential in adaptive testing, where, instead of using a random subset of the benchmark questions, informative questions are adaptively chosen based on the current estimation of LLM performance. Experiments on 22 common natural language benchmarks and 172 LMs show that this approach is more reliable and efficient compared to current common practice."
  },
  {
    "title": "Is Crime Displacement Inevitable? Evidence from Police Crackdowns in Fortaleza, Brazil",
    "url": "http://arxiv.org/abs/2503.13571v1",
    "arxiv_id": "2503.13571v1",
    "authors": [
      "Jos\u00e9 Raimundo Carvalho",
      "Marcelino Guerra"
    ],
    "published": "2025-03-17T12:25:44+00:00",
    "summary": "We evaluated one of the most common policing strategies in Brazil: the allocation of police blitzes. This place-based focused deterrence intervention has well-defined assignments, and 3,423 interventions were precisely recorded in Fortaleza-CE, Brazil, between 2012 and 2013. Our analysis takes advantage of the high spatiotemporal daily data resolution coming from an unprecedented longitudinal micro-Big Data (GPS and PING records) to make comparisons of small intervention areas, while controlling for common daily trends, deterrence (spatial and temporal), and diffusion; to show that an average police crackdown causes a 35% decrease in violent crime occurrences. There are diminishing returns of public safety to hours spent by police in a single area, corroborating what police officers know well from their own experience and discretionary behavior. Although crime increases by 6% immediately after the end of a blitz, we observe lasting deterrent effects (diffusion) after 2-3 days. The residual deterrence cancels the relocation of the crime, and the intervention does not generate significant temporal displacement. In addition, we do not find spatial displacement from crime in blocks up to 1.5 km from a blitz. This type of micropolicing tactics generates deterrence by being highly visible in a street segment for a short period and intermittently quasirandom in space-time, which produces uncertainty that might be crucial in minimizing the temporal and spatial displacement of crime. Of public policy interest, we show that the allocation of blitzes passes in an initial cost-benefit analysis."
  },
  {
    "title": "LIVEPOINT: Fully Decentralized, Safe, Deadlock-Free Multi-Robot Control in Cluttered Environments with High-Dimensional Inputs",
    "url": "http://arxiv.org/abs/2503.13098v1",
    "arxiv_id": "2503.13098v1",
    "authors": [
      "Jeffrey Chen",
      "Rohan Chandra"
    ],
    "published": "2025-03-17T12:07:25+00:00",
    "summary": "Fully decentralized, safe, and deadlock-free multi-robot navigation in dynamic, cluttered environments is a critical challenge in robotics. Current methods require exact state measurements in order to enforce safety and liveness e.g. via control barrier functions (CBFs), which is challenging to achieve directly from onboard sensors like lidars and cameras. This work introduces LIVEPOINT, a decentralized control framework that synthesizes universal CBFs over point clouds to enable safe, deadlock-free real-time multi-robot navigation in dynamic, cluttered environments. Further, LIVEPOINT ensures minimally invasive deadlock avoidance behavior by dynamically adjusting agents' speeds based on a novel symmetric interaction metric. We validate our approach in simulation experiments across highly constrained multi-robot scenarios like doorways and intersections. Results demonstrate that LIVEPOINT achieves zero collisions or deadlocks and a 100% success rate in challenging settings compared to optimization-based baselines such as MPC and ORCA and neural methods such as MPNet, which fail in such environments. Despite prioritizing safety and liveness, LIVEPOINT is 35% smoother than baselines in the doorway environment, and maintains agility in constrained environments while still being safe and deadlock-free."
  },
  {
    "title": "A Framework to Assess Multilingual Vulnerabilities of LLMs",
    "url": "http://arxiv.org/abs/2503.13081v1",
    "arxiv_id": "2503.13081v1",
    "authors": [
      "Likai Tang",
      "Niruth Bogahawatta",
      "Yasod Ginige",
      "Jiarui Xu",
      "Shixuan Sun",
      "Surangika Ranathunga",
      "Suranga Seneviratne"
    ],
    "published": "2025-03-17T11:39:44+00:00",
    "summary": "Large Language Models (LLMs) are acquiring a wider range of capabilities, including understanding and responding in multiple languages. While they undergo safety training to prevent them from answering illegal questions, imbalances in training data and human evaluation resources can make these models more susceptible to attacks in low-resource languages (LRL). This paper proposes a framework to automatically assess the multilingual vulnerabilities of commonly used LLMs. Using our framework, we evaluated six LLMs across eight languages representing varying levels of resource availability. We validated the assessments generated by our automated framework through human evaluation in two languages, demonstrating that the framework's results align with human judgments in most cases. Our findings reveal vulnerabilities in LRL; however, these may pose minimal risk as they often stem from the model's poor performance, resulting in incoherent responses."
  },
  {
    "title": "Sensorless Remote Center of Motion Misalignment Estimation",
    "url": "http://arxiv.org/abs/2503.13011v1",
    "arxiv_id": "2503.13011v1",
    "authors": [
      "Hao Yang",
      "Lidia Al-Zogbi",
      "Ahmet Yildiz",
      "Nabil Simaan",
      "Jie Ying Wu"
    ],
    "published": "2025-03-17T10:11:05+00:00",
    "summary": "Laparoscopic surgery constrains instrument motion around a fixed pivot point at the incision into a patient to minimize tissue trauma. Surgical robots achieve this through either hardware to software-based remote center of motion (RCM) constraints. However, accurate RCM alignment is difficult due to manual trocar placement, patient motion, and tissue deformation. Misalignment between the robot's RCM point and the patient incision site can cause unsafe forces at the incision site. This paper presents a sensorless force estimation-based framework for dynamically assessing and optimizing RCM misalignment in robotic surgery. Our experiments demonstrate that misalignment exceeding 20 mm can generate large enough forces to potentially damage tissue, emphasizing the need for precise RCM positioning. For misalignment $D\\geq $ 20 mm, our optimization algorithm estimates the RCM offset with an absolute error within 5 mm. Accurate RCM misalignment estimation is a step toward automated RCM misalignment compensation, enhancing safety and reducing tissue damage in robotic-assisted laparoscopic surgery."
  },
  {
    "title": "SparseAlign: A Fully Sparse Framework for Cooperative Object Detection",
    "url": "http://arxiv.org/abs/2503.12982v1",
    "arxiv_id": "2503.12982v1",
    "authors": [
      "Yunshuang Yuan",
      "Yan Xia",
      "Daniel Cremers",
      "Monika Sester"
    ],
    "published": "2025-03-17T09:38:53+00:00",
    "summary": "Cooperative perception can increase the view field and decrease the occlusion of an ego vehicle, hence improving the perception performance and safety of autonomous driving. Despite the success of previous works on cooperative object detection, they mostly operate on dense Bird's Eye View (BEV) feature maps, which are computationally demanding and can hardly be extended to long-range detection problems. More efficient fully sparse frameworks are rarely explored. In this work, we design a fully sparse framework, SparseAlign, with three key features: an enhanced sparse 3D backbone, a query-based temporal context learning module, and a robust detection head specially tailored for sparse features. Extensive experimental results on both OPV2V and DairV2X datasets show that our framework, despite its sparsity, outperforms the state of the art with less communication bandwidth requirements. In addition, experiments on the OPV2Vt and DairV2Xt datasets for time-aligned cooperative object detection also show a significant performance gain compared to the baseline works."
  },
  {
    "title": "Efficient Action-Constrained Reinforcement Learning via Acceptance-Rejection Method and Augmented MDPs",
    "url": "http://arxiv.org/abs/2503.12932v1",
    "arxiv_id": "2503.12932v1",
    "authors": [
      "Wei Hung",
      "Shao-Hua Sun",
      "Ping-Chun Hsieh"
    ],
    "published": "2025-03-17T08:41:43+00:00",
    "summary": "Action-constrained reinforcement learning (ACRL) is a generic framework for learning control policies with zero action constraint violation, which is required by various safety-critical and resource-constrained applications. The existing ACRL methods can typically achieve favorable constraint satisfaction but at the cost of either high computational burden incurred by the quadratic programs (QP) or increased architectural complexity due to the use of sophisticated generative models. In this paper, we propose a generic and computationally efficient framework that can adapt a standard unconstrained RL method to ACRL through two modifications: (i) To enforce the action constraints, we leverage the classic acceptance-rejection method, where we treat the unconstrained policy as the proposal distribution and derive a modified policy with feasible actions. (ii) To improve the acceptance rate of the proposal distribution, we construct an augmented two-objective Markov decision process (MDP), which include additional self-loop state transitions and a penalty signal for the rejected actions. This augmented MDP incentives the learned policy to stay close to the feasible action sets. Through extensive experiments in both robot control and resource allocation domains, we demonstrate that the proposed framework enjoys faster training progress, better constraint satisfaction, and a lower action inference time simultaneously than the state-of-the-art ACRL methods. We have made the source code publicly available to encourage further research in this direction."
  },
  {
    "title": "MirrorGuard: Adaptive Defense Against Jailbreaks via Entropy-Guided Mirror Crafting",
    "url": "http://arxiv.org/abs/2503.12931v1",
    "arxiv_id": "2503.12931v1",
    "authors": [
      "Rui Pu",
      "Chaozhuo Li",
      "Rui Ha",
      "Litian Zhang",
      "Lirong Qiu",
      "Xi Zhang"
    ],
    "published": "2025-03-17T08:41:29+00:00",
    "summary": "Defending large language models (LLMs) against jailbreak attacks is crucial for ensuring their safe deployment. Existing defense strategies generally rely on predefined static criteria to differentiate between harmful and benign prompts. However, such rigid rules are incapable of accommodating the inherent complexity and dynamic nature of real jailbreak attacks. In this paper, we propose a novel concept of ``mirror'' to enable dynamic and adaptive defense. A mirror refers to a dynamically generated prompt that mirrors the syntactic structure of the input while ensuring semantic safety. The personalized discrepancies between the input prompts and their corresponding mirrors serve as the guiding principles for defense. A new defense paradigm, MirrorGuard, is further proposed to detect and calibrate risky inputs based on such mirrors. An entropy-based detection metric, Relative Input Uncertainty (RIU), is integrated into MirrorGuard to quantify the discrepancies between input prompts and mirrors. MirrorGuard is evaluated on several popular datasets, demonstrating state-of-the-art defense performance while maintaining general effectiveness."
  },
  {
    "title": "A Semantic-based Optimization Approach for Repairing LLMs: Case Study on Code Generation",
    "url": "http://arxiv.org/abs/2503.12899v1",
    "arxiv_id": "2503.12899v1",
    "authors": [
      "Jian Gu",
      "Aldeida Aleti",
      "Chunyang Chen",
      "Hongyu Zhang"
    ],
    "published": "2025-03-17T07:59:42+00:00",
    "summary": "Language Models (LMs) are widely used in software engineering for code generation, but they may produce code with errors. Rather than repairing the generated code, an alternative way is to address the underlying failures of models. LM repair offers a lightweight solution to this challenge: it requires minimal data, reduces computational costs, and reduces the side effects. Unlike retraining, LM repair focuses on applying tailored updates to targeted neurons, making it ideal for scenarios with limited resources, high-performance demands, or strict safety requirements. In this paper, we propose \\ul{S}emantic \\ul{T}argeting for \\ul{A}nalytical \\ul{R}epair (\\textsc{STAR}), a pioneering and novel semantic-based optimization approach for repairing LLMs. \\textsc{STAR} realizes main operations in LM repair methods in an optimization process, including locating ``buggy neurons'', solving ``neuron patches'', and patching ``buggy neurons''. Correspondingly, it computes the deltas of weight matrix as the prior information to guide optimization; and attributes the targeted layers and neurons leveraging statistical insights. The neuron patches are computed with a solid semantic-based analytical formula, which directly bridges the changes to logits with the deltas of neurons, by steering latent representations. Compared to the prior work of LM repair (\\textsc{MINT}) and optimization methods (\\textsc{SGD}), \\textsc{STAR} integrates their strengths while mitigating their limitations. \\textsc{STAR} supports solving multiple failures together, significantly improving the usefulness. Evaluated on three code generation tasks using popular code LMs, \\textsc{STAR} demonstrates superior effectiveness. Additionally, \\textsc{STAR} exhibits better efficiency. In terms of side effects, namely the balance between generalization and specificity, \\textsc{STAR} outperforms prior work by a significant margin."
  },
  {
    "title": "ACT360: An Efficient 360-Degree Action Detection and Summarization Framework for Mission-Critical Training and Debriefing",
    "url": "http://arxiv.org/abs/2503.12852v1",
    "arxiv_id": "2503.12852v1",
    "authors": [
      "Aditi Tiwari",
      "Klara Nahrstedt"
    ],
    "published": "2025-03-17T06:12:36+00:00",
    "summary": "Effective training and debriefing are critical in high-stakes, mission-critical environments such as disaster response, military simulations, and industrial safety, where precision and minimizing errors are paramount. The traditional post-training analysis relies on manually reviewing 2D videos, a time-consuming process that lacks comprehensive situational awareness. To address these limitations, we introduce ACT360, a system that leverages 360-degree videos and machine learning for automated action detection and structured debriefing. ACT360 integrates 360YOWO, an enhanced You Only Watch Once (YOWO) model with spatial attention and equirectangular-aware convolution (EAC) to mitigate panoramic video distortions. To enable deployment in resource-constrained environments, we apply quantization and model pruning, reducing the model size by 74% while maintaining robust accuracy (mAP drop of only 1.5%, from 0.865 to 0.850) and improving inference speed. We validate our approach on a publicly available dataset of 55 labeled 360-degree videos covering seven key operational actions, recorded across various real-world training sessions and environmental conditions. Additionally, ACT360 integrates 360AIE (Action Insight Explorer), a web-based interface for automatic action detection, retrieval, and textual summarization using large language models (LLMs), significantly enhancing post-incident analysis efficiency. ACT360 serves as a generalized framework for mission-critical debriefing, incorporating EAC, spatial attention, summarization, and model optimization. These innovations apply to any training environment requiring lightweight action detection and structured post-exercise analysis."
  },
  {
    "title": "In vivo validation of Wireless Power Transfer System for Magnetically Controlled Robotic Capsule Endoscopy",
    "url": "http://arxiv.org/abs/2503.12850v1",
    "arxiv_id": "2503.12850v1",
    "authors": [
      "Alessandro Catania",
      "Michele Bertozzi",
      "Nikita J. Greenidge",
      "Benjamin Calme",
      "Gabriele Bandini",
      "Christian Sbrana",
      "Roberto Cecchi",
      "Alice Buffi",
      "Sebastiano Strangio",
      "Pietro Valdastri",
      "Giuseppe Iannaccone"
    ],
    "published": "2025-03-17T06:03:49+00:00",
    "summary": "This paper presents the in vivo validation of an inductive wireless power transfer (WPT) system integrated for the first time into a magnetically controlled robotic capsule endoscopy platform. The proposed system enables continuous power delivery to the capsule without the need for onboard batteries, thus extending operational time and reducing size constraints. The WPT system operates through a resonant inductive coupling mechanism, based on a transmitting coil mounted on the end effector of a robotic arm that also houses an external permanent magnet and a localization coil for precise capsule manipulation. To ensure robust and stable power transmission in the presence of coil misalignment and rotation, a 3D receiving coil is integrated within the capsule. Additionally, a closed-loop adaptive control system, based on load-shift keying (LSK) modulation, dynamically adjusts the transmitted power to optimize efficiency while maintaining compliance with specific absorption rate (SAR) safety limits. The system has been extensively characterized in laboratory settings and validated through in vivo experiments using a porcine model, demonstrating reliable power transfer and effective robotic navigation in realistic gastrointestinal conditions: the average received power was 110 mW at a distance of 9 cm between the coils, with variable capsule rotation angles. The results confirm the feasibility of the proposed WPT approach for autonomous, battery-free robotic capsule endoscopy, paving the way for enhanced diagnostic in gastrointestinal medicine."
  },
  {
    "title": "SNPL: Simultaneous Policy Learning and Evaluation for Safe Multi-Objective Policy Improvement",
    "url": "http://arxiv.org/abs/2503.12760v1",
    "arxiv_id": "2503.12760v1",
    "authors": [
      "Brian Cho",
      "Ana-Roxana Pop",
      "Ariel Evince",
      "Nathan Kallus"
    ],
    "published": "2025-03-17T02:53:53+00:00",
    "summary": "To design effective digital interventions, experimenters face the challenge of learning decision policies that balance multiple objectives using offline data. Often, they aim to develop policies that maximize goal outcomes, while ensuring there are no undesirable changes in guardrail outcomes. To provide credible recommendations, experimenters must not only identify policies that satisfy the desired changes in goal and guardrail outcomes, but also offer probabilistic guarantees about the changes these policies induce. In practice, however, policy classes are often large, and digital experiments tend to produce datasets with small effect sizes relative to noise. In this setting, standard approaches such as data splitting or multiple testing often result in unstable policy selection and/or insufficient statistical power. In this paper, we provide safe noisy policy learning (SNPL), a novel approach that leverages the concept of algorithmic stability to address these challenges. Our method enables policy learning while simultaneously providing high-confidence guarantees using the entire dataset, avoiding the need for data-splitting. We present finite-sample and asymptotic versions of our algorithm that ensure the recommended policy satisfies high-probability guarantees for avoiding guardrail regressions and/or achieving goal outcome improvements. We test both variants of our approach approach empirically on a real-world application of personalizing SMS delivery. Our results on real-world data suggest that our approach offers dramatic improvements in settings with large policy classes and low signal-to-noise across both finite-sample and asymptotic safety guarantees, offering up to 300\\% improvements in detection rates and 150\\% improvements in policy gains at significantly smaller sample sizes."
  },
  {
    "title": "SafeSlice: Enabling SLA-Compliant O-RAN Slicing via Safe Deep Reinforcement Learning",
    "url": "http://arxiv.org/abs/2503.12753v1",
    "arxiv_id": "2503.12753v1",
    "authors": [
      "Ahmad M. Nagib",
      "Hatem Abou-Zeid",
      "Hossam S. Hassanein"
    ],
    "published": "2025-03-17T02:41:49+00:00",
    "summary": "Deep reinforcement learning (DRL)-based slicing policies have shown significant success in simulated environments but face challenges in physical systems such as open radio access networks (O-RANs) due to simulation-to-reality gaps. These policies often lack safety guarantees to ensure compliance with service level agreements (SLAs), such as the strict latency requirements of immersive applications. As a result, a deployed DRL slicing agent may make resource allocation (RA) decisions that degrade system performance, particularly in previously unseen scenarios. Real-world immersive applications require maintaining SLA constraints throughout deployment to prevent risky DRL exploration. In this paper, we propose SafeSlice to address both the cumulative (trajectory-wise) and instantaneous (state-wise) latency constraints of O-RAN slices. We incorporate the cumulative constraints by designing a sigmoid-based risk-sensitive reward function that reflects the slices' latency requirements. Moreover, we build a supervised learning cost model as part of a safety layer that projects the slicing agent's RA actions to the nearest safe actions, fulfilling instantaneous constraints. We conduct an exhaustive experiment that supports multiple services, including real virtual reality (VR) gaming traffic, to investigate the performance of SafeSlice under extreme and changing deployment conditions. SafeSlice achieves reductions of up to 83.23% in average cumulative latency, 93.24% in instantaneous latency violations, and 22.13% in resource consumption compared to the baselines. The results also indicate SafeSlice's robustness to changing the threshold configurations of latency constraints, a vital deployment scenario that will be realized by the O-RAN paradigm to empower mobile network operators (MNOs)."
  },
  {
    "title": "Identifying Cooperative Personalities in Multi-agent Contexts through Personality Steering with Representation Engineering",
    "url": "http://arxiv.org/abs/2503.12722v1",
    "arxiv_id": "2503.12722v1",
    "authors": [
      "Kenneth J. K. Ong",
      "Lye Jia Jun",
      "Hieu Minh \"Jord\" Nguyen",
      "Seong Hah Cho",
      "Natalia P\u00e9rez-Campanero Antol\u00edn"
    ],
    "published": "2025-03-17T01:21:54+00:00",
    "summary": "As Large Language Models (LLMs) gain autonomous capabilities, their coordination in multi-agent settings becomes increasingly important. However, they often struggle with cooperation, leading to suboptimal outcomes. Inspired by Axelrod's Iterated Prisoner's Dilemma (IPD) tournaments, we explore how personality traits influence LLM cooperation. Using representation engineering, we steer Big Five traits (e.g., Agreeableness, Conscientiousness) in LLMs and analyze their impact on IPD decision-making. Our results show that higher Agreeableness and Conscientiousness improve cooperation but increase susceptibility to exploitation, highlighting both the potential and limitations of personality-based steering for aligning AI agents."
  },
  {
    "title": "Can Reasoning Models Reason about Hardware? An Agentic HLS Perspective",
    "url": "http://arxiv.org/abs/2503.12721v1",
    "arxiv_id": "2503.12721v1",
    "authors": [
      "Luca Collini",
      "Andrew Hennessee",
      "Ramesh Karri",
      "Siddharth Garg"
    ],
    "published": "2025-03-17T01:21:39+00:00",
    "summary": "Recent Large Language Models (LLMs) such as OpenAI o3-mini and DeepSeek-R1 use enhanced reasoning through Chain-of-Thought (CoT). Their potential in hardware design, which relies on expert-driven iterative optimization, remains unexplored. This paper investigates whether reasoning LLMs can address challenges in High-Level Synthesis (HLS) design space exploration and optimization. During HLS, engineers manually define pragmas/directives to balance performance and resource constraints. We propose an LLM-based optimization agentic framework that automatically restructures code, inserts pragmas, and identifies optimal design points via feedback from HLs tools and access to integer-linear programming (ILP) solvers. Experiments compare reasoning models against conventional LLMs on benchmarks using success rate, efficiency, and design quality (area/latency) metrics, and provide the first-ever glimpse into the CoTs produced by a powerful open-source reasoning model like DeepSeek-R1."
  },
  {
    "title": "CDKFormer: Contextual Deviation Knowledge-Based Transformer for Long-Tail Trajectory Prediction",
    "url": "http://arxiv.org/abs/2503.12695v1",
    "arxiv_id": "2503.12695v1",
    "authors": [
      "Yuansheng Lian",
      "Ke Zhang",
      "Meng Li"
    ],
    "published": "2025-03-16T23:48:13+00:00",
    "summary": "Predicting the future movements of surrounding vehicles is essential for ensuring the safe operation and efficient navigation of autonomous vehicles (AVs) in urban traffic environments. Existing vehicle trajectory prediction methods primarily focus on improving overall performance, yet they struggle to address long-tail scenarios effectively. This limitation often leads to poor predictions in rare cases, significantly increasing the risk of safety incidents. Taking Argoverse 2 motion forecasting dataset as an example, we first investigate the long-tail characteristics in trajectory samples from two perspectives, individual motion and group interaction, and deriving deviation features to distinguish abnormal from regular scenarios. On this basis, we propose CDKFormer, a Contextual Deviation Knowledge-based Transformer model for long-tail trajectory prediction. CDKFormer integrates an attention-based scene context fusion module to encode spatiotemporal interaction and road topology. An additional deviation feature fusion module is proposed to capture the dynamic deviations in the target vehicle status. We further introduce a dual query-based decoder, supported by a multi-stream decoder block, to sequentially decode heterogeneous scene deviation features and generate multimodal trajectory predictions. Extensive experiments demonstrate that CDKFormer achieves state-of-the-art performance, significantly enhancing prediction accuracy and robustness for long-tailed trajectories compared to existing methods, thus advancing the reliability of AVs in complex real-world environments."
  },
  {
    "title": "AI Agents: Evolution, Architecture, and Real-World Applications",
    "url": "http://arxiv.org/abs/2503.12687v1",
    "arxiv_id": "2503.12687v1",
    "authors": [
      "Naveen Krishnan"
    ],
    "published": "2025-03-16T23:07:48+00:00",
    "summary": "This paper examines the evolution, architecture, and practical applications of AI agents from their early, rule-based incarnations to modern sophisticated systems that integrate large language models with dedicated modules for perception, planning, and tool use. Emphasizing both theoretical foundations and real-world deployments, the paper reviews key agent paradigms, discusses limitations of current evaluation benchmarks, and proposes a holistic evaluation framework that balances task effectiveness, efficiency, robustness, and safety. Applications across enterprise, personal assistance, and specialized domains are analyzed, with insights into future research directions for more resilient and adaptive AI agent systems."
  },
  {
    "title": "What is unethical about software? User perceptions in the Netherlands",
    "url": "http://arxiv.org/abs/2503.12640v1",
    "arxiv_id": "2503.12640v1",
    "authors": [
      "Yagil Elias",
      "Tom P. Humbert",
      "Lauren Olson",
      "Emitz\u00e1 Guzm\u00e1n"
    ],
    "published": "2025-03-16T20:29:25+00:00",
    "summary": "Software has the potential to improve lives. Yet, unethical and uninformed software practices are at the root of an increasing number of ethical concerns. Despite its pervasiveness, few research has analyzed end-users perspectives on the ethical issues of the software they use. We address this gap, and investigate end-user's ethical concerns in software through 19 semi-structured interviews with residents of the Netherlands. We ask a diverse group of users about their ethical concerns when using everyday software applications. We investigate the underlying reasons for their concerns and what solutions they propose to eliminate them. We find that our participants actively worry about privacy, transparency, manipulation, safety and inappropriate content; with privacy and manipulation often being at the center of their worries. Our participants demand software solutions to improve information clarity in applications and provide more control over the user experience. They further expect larger systematic changes within software practices and government regulation."
  },
  {
    "title": "Negotiative Alignment: Embracing Disagreement to Achieve Fairer Outcomes -- Insights from Urban Studies",
    "url": "http://arxiv.org/abs/2503.12613v1",
    "arxiv_id": "2503.12613v1",
    "authors": [
      "Rashid Mushkani",
      "Hugo Berard",
      "Shin Koseki"
    ],
    "published": "2025-03-16T18:55:54+00:00",
    "summary": "Cities are not monolithic; they are arenas of negotiation among groups that hold varying needs, values, and experiences. Conventional methods of urban assessment -- from standardized surveys to AI-driven evaluations -- frequently rely on a single consensus metric (e.g., an average measure of inclusivity or safety). Although such aggregations simplify design decisions, they risk obscuring the distinct perspectives of marginalized populations. In this paper, we present findings from a community-centered study in Montreal involving 35 residents with diverse demographic and social identities, particularly wheelchair users, seniors, and LGBTQIA2+ individuals. Using rating and ranking tasks on 20 urban sites, we observe that disagreements are systematic rather than random, reflecting structural inequalities, differing cultural values, and personal experiences of safety and accessibility.   Based on these empirical insights, we propose negotiative alignment, an AI framework that treats disagreement as an essential input to be preserved, analyzed, and addressed. Negotiative alignment builds on pluralistic models by dynamically updating stakeholder preferences through multi-agent negotiation mechanisms, ensuring no single perspective is marginalized. We outline how this framework can be integrated into urban analytics -- and other decision-making contexts -- to retain minority viewpoints, adapt to changing stakeholder concerns, and enhance fairness and accountability. The study demonstrates that preserving and engaging with disagreement, rather than striving for an artificial consensus, can produce more equitable and responsive AI-driven outcomes in urban design."
  },
  {
    "title": "Point Cloud Based Scene Segmentation: A Survey",
    "url": "http://arxiv.org/abs/2503.12595v1",
    "arxiv_id": "2503.12595v1",
    "authors": [
      "Dan Halperin",
      "Niklas Eisl"
    ],
    "published": "2025-03-16T18:02:41+00:00",
    "summary": "Autonomous driving is a safety-critical application, and it is therefore a top priority that the accompanying assistance systems are able to provide precise information about the surrounding environment of the vehicle. Tasks such as 3D Object Detection deliver an insufficiently detailed understanding of the surrounding scene because they only predict a bounding box for foreground objects. In contrast, 3D Semantic Segmentation provides richer and denser information about the environment by assigning a label to each individual point, which is of paramount importance for autonomous driving tasks, such as navigation or lane changes. To inspire future research, in this review paper, we provide a comprehensive overview of the current state-of-the-art methods in the field of Point Cloud Semantic Segmentation for autonomous driving. We categorize the approaches into projection-based, 3D-based and hybrid methods. Moreover, we discuss the most important and commonly used datasets for this task and also emphasize the importance of synthetic data to support research when real-world data is limited. We further present the results of the different methods and compare them with respect to their segmentation accuracy and efficiency."
  },
  {
    "title": "Automotive Battery Pack Standards and Design Characteristics: A Review",
    "url": "http://arxiv.org/abs/2503.12566v1",
    "arxiv_id": "2503.12566v1",
    "authors": [
      "Saeid Haghbin",
      "Morteza Rezaei Larijani",
      "MohammadReza Zolghadri",
      "Shahin Hedayati Kia"
    ],
    "published": "2025-03-16T16:42:26+00:00",
    "summary": "The latest status and near-future trends of automotive battery packs are presented and discussed, with a focus on automakers. Desired pack specifications, aligned with regulatory standards, are outlined from an automaker's perspective. In response to these specifications, high-level solutions are proposed to converge toward a standard architecture for passenger cars Key aspects such as electrical performance, safety, mechanical integrity, reliability, environmental conditions, diagnostics, and practical considerations are examined. Furthermore, near-future developments and emerging applications, including battery use in airplanes, are discussed."
  },
  {
    "title": "Automotive Battery Pack Standards and Design Characteristics: A Review",
    "url": "http://arxiv.org/abs/2503.12566v2",
    "arxiv_id": "2503.12566v2",
    "authors": [
      "Saeid Haghbin",
      "Morteza Rezaei Larijani",
      "MohammadReza Zolghadri",
      "Shahin Hedayati Kia"
    ],
    "published": "2025-03-16T16:42:26+00:00",
    "summary": "This paper outlines the existing situation and future trends related to automobile battery packs, specifically from the automobile manufacturer's point of view. It formulates the specifications required for such packs to adhere to prevailing regulatory schemes and examines top-level solutions to target a uniform architecture for passenger cars. Key elements such as electrical performance, safety, mechanical integrity, reliability, environmental issues, diagnostics, and real-world implications have been extensively examined. This paper draws attention to the industry trend of shifting to high-voltage battery architectures to enable ultra-fast charging above 350 kW, reducing the charging time to less than 20 minutes. Technological advancements in energy density and battery pack capacities are poised to take electric vehicle ranges over 1000 km from a single charge. This study also examines developments in artificial intelligence-improved battery management systems, enhanced safety, mechanical integrity, reliability, diagnostics, and practical considerations. Furthermore, future developments, such as the incorporation of batteries in aviation and other new uses, are investigated to provide insight into the future generation of economically viable, secure, and high-performance battery systems."
  },
  {
    "title": "Polytope Volume Monitoring Problem: Formulation and Solution via Parametric Linear Program Based Control Barrier Function",
    "url": "http://arxiv.org/abs/2503.12546v1",
    "arxiv_id": "2503.12546v1",
    "authors": [
      "Shizhen Wu",
      "Jinyang Dong",
      "Xu Fang",
      "Ning Sun",
      "Yongchun Fang"
    ],
    "published": "2025-03-16T15:27:22+00:00",
    "summary": "Motivated by the latest research on feasible space monitoring of multiple control barrier functions (CBFs) as well as polytopic collision avoidance, this paper studies the Polytope Volume Monitoring (PVM) problem, whose goal is to design a control law for inputs of nonlinear systems to prevent the volume of some state-dependent polytope from decreasing to zero. Recent studies have explored the idea of applying Chebyshev ball method in optimization theory to solve the case study of PVM; however, the underlying difficulties caused by nonsmoothness have not been addressed. This paper continues the study on this topic, where our main contribution is to establish the relationship between nonsmooth CBF and parametric optimization theory through directional derivatives for the first time, so as to solve PVM problems more conveniently. In detail, inspired by Chebyshev ball approach, a parametric linear program (PLP) based nonsmooth barrier function candidate is established for PVM, and then, sufficient conditions for it to be a nonsmooth CBF are proposed, based on which a quadratic program (QP) based safety filter with guaranteed feasibility is proposed to address PVM problems. Finally, a numerical simulation example is given to show the efficiency of the proposed safety filter."
  },
  {
    "title": "LLM-Driven Multi-step Translation from C to Rust using Static Analysis",
    "url": "http://arxiv.org/abs/2503.12511v1",
    "arxiv_id": "2503.12511v1",
    "authors": [
      "Tianyang Zhou",
      "Haowen Lin",
      "Somesh Jha",
      "Mihai Christodorescu",
      "Kirill Levchenko",
      "Varun Chandrasekaran"
    ],
    "published": "2025-03-16T14:05:26+00:00",
    "summary": "Translating software written in legacy languages to modern languages, such as C to Rust, has significant benefits in improving memory safety while maintaining high performance. However, manual translation is cumbersome, error-prone, and produces unidiomatic code. Large language models (LLMs) have demonstrated promise in producing idiomatic translations, but offer no correctness guarantees as they lack the ability to capture all the semantics differences between the source and target languages. To resolve this issue, we propose SACTOR, an LLM-driven C-to-Rust zero-shot translation tool using a two-step translation methodology: an \"unidiomatic\" step to translate C into Rust while preserving semantics, and an \"idiomatic\" step to refine the code to follow Rust's semantic standards. SACTOR utilizes information provided by static analysis of the source C program to address challenges such as pointer semantics and dependency resolution. To validate the correctness of the translated result from each step, we use end-to-end testing via the foreign function interface to embed our translated code segment into the original code. We evaluate the translation of 200 programs from two datasets and two case studies, comparing the performance of GPT-4o, Claude 3.5 Sonnet, Gemini 2.0 Flash, Llama 3.3 70B and DeepSeek-R1 in SACTOR. Our results demonstrate that SACTOR achieves high correctness and improved idiomaticity, with the best-performing model (DeepSeek-R1) reaching 93% and (GPT-4o, Claude 3.5, DeepSeek-R1) reaching 84% correctness (on each dataset, respectively), while producing more natural and Rust-compliant translations compared to existing methods."
  },
  {
    "title": "LLM-Driven Multi-step Translation from C to Rust using Static Analysis",
    "url": "http://arxiv.org/abs/2503.12511v2",
    "arxiv_id": "2503.12511v2",
    "authors": [
      "Tianyang Zhou",
      "Haowen Lin",
      "Somesh Jha",
      "Mihai Christodorescu",
      "Kirill Levchenko",
      "Varun Chandrasekaran"
    ],
    "published": "2025-03-16T14:05:26+00:00",
    "summary": "Translating software written in legacy languages to modern languages, such as C to Rust, has significant benefits in improving memory safety while maintaining high performance. However, manual translation is cumbersome, error-prone, and produces unidiomatic code. Large language models (LLMs) have demonstrated promise in producing idiomatic translations, but offer no correctness guarantees as they lack the ability to capture all the semantics differences between the source and target languages. To resolve this issue, we propose SACTOR, an LLM-driven C-to-Rust zero-shot translation tool using a two-step translation methodology: an \"unidiomatic\" step to translate C into Rust while preserving semantics, and an \"idiomatic\" step to refine the code to follow Rust's semantic standards. SACTOR utilizes information provided by static analysis of the source C program to address challenges such as pointer semantics and dependency resolution. To validate the correctness of the translated result from each step, we use end-to-end testing via the foreign function interface to embed our translated code segment into the original code. We evaluate the translation of 200 programs from two datasets and two case studies, comparing the performance of GPT-4o, Claude 3.5 Sonnet, Gemini 2.0 Flash, Llama 3.3 70B and DeepSeek-R1 in SACTOR. Our results demonstrate that SACTOR achieves high correctness and improved idiomaticity, with the best-performing model (DeepSeek-R1) reaching 93% and (GPT-4o, Claude 3.5, DeepSeek-R1) reaching 84% correctness (on each dataset, respectively), while producing more natural and Rust-compliant translations compared to existing methods."
  },
  {
    "title": "Modularization is Better: Effective Code Generation with Modular Prompting",
    "url": "http://arxiv.org/abs/2503.12483v1",
    "arxiv_id": "2503.12483v1",
    "authors": [
      "Ruwei Pan",
      "Hongyu Zhang"
    ],
    "published": "2025-03-16T12:23:23+00:00",
    "summary": "Large Language Models are transforming software development by automatically generating code. Current prompting techniques such as Chain-of-Thought (CoT) suggest tasks step by step and the reasoning process follows a linear structure, which hampers the understanding of complex programming problems, particularly those requiring hierarchical solutions. Inspired by the principle of modularization in software development, in this work, we propose a novel prompting technique, called MoT, to enhance the code generation performance of LLMs. At first, MoT exploits modularization principles to decompose complex programming problems into smaller, independent reasoning steps, enabling a more structured and interpretable problem-solving process. This hierarchical structure improves the LLM's ability to comprehend complex programming problems. Then, it structures the reasoning process using an MLR Graph (Multi-Level Reasoning Graph), which hierarchically organizes reasoning steps. This approach enhances modular understanding and ensures better alignment between reasoning steps and the generated code, significantly improving code generation performance. Our experiments on two advanced LLMs (GPT-4o-mini and DeepSeek-R1), comparing MoT to six baseline prompting techniques across six widely used datasets, HumanEval, HumanEval-ET, HumanEval+, MBPP, MBPP-ET, and MBPP+, demonstrate that MoT significantly outperforms existing baselines (e.g., CoT and SCoT), achieving Pass@1 scores ranging from 58.1% to 95.1%. The experimental results confirm that MoT significantly enhances the performance of LLM-based code generation."
  },
  {
    "title": "ResLPR: A LiDAR Data Restoration Network and Benchmark for Robust Place Recognition Against Weather Corruptions",
    "url": "http://arxiv.org/abs/2503.12350v1",
    "arxiv_id": "2503.12350v1",
    "authors": [
      "Wenqing Kuang",
      "Xiongwei Zhao",
      "Yehui Shen",
      "Congcong Wen",
      "Huimin Lu",
      "Zongtan Zhou",
      "Xieyuanli Chen"
    ],
    "published": "2025-03-16T04:14:20+00:00",
    "summary": "LiDAR-based place recognition (LPR) is a key component for autonomous driving, and its resilience to environmental corruption is critical for safety in high-stakes applications. While state-of-the-art (SOTA) LPR methods perform well in clean weather, they still struggle with weather-induced corruption commonly encountered in driving scenarios. To tackle this, we propose ResLPRNet, a novel LiDAR data restoration network that largely enhances LPR performance under adverse weather by restoring corrupted LiDAR scans using a wavelet transform-based network. ResLPRNet is efficient, lightweight and can be integrated plug-and-play with pretrained LPR models without substantial additional computational cost. Given the lack of LPR datasets under adverse weather, we introduce ResLPR, a novel benchmark that examines SOTA LPR methods under a wide range of LiDAR distortions induced by severe snow, fog, and rain conditions. Experiments on our proposed WeatherKITTI and WeatherNCLT datasets demonstrate the resilience and notable gains achieved by using our restoration method with multiple LPR approaches in challenging weather scenarios. Our code and benchmark are publicly available here: https://github.com/nubot-nudt/ResLPR."
  },
  {
    "title": "Augmented Adversarial Trigger Learning",
    "url": "http://arxiv.org/abs/2503.12339v1",
    "arxiv_id": "2503.12339v1",
    "authors": [
      "Zhe Wang",
      "Yanjun Qi"
    ],
    "published": "2025-03-16T03:20:52+00:00",
    "summary": "Gradient optimization-based adversarial attack methods automate the learning of adversarial triggers to generate jailbreak prompts or leak system prompts. In this work, we take a closer look at the optimization objective of adversarial trigger learning and propose ATLA: Adversarial Trigger Learning with Augmented objectives. ATLA improves the negative log-likelihood loss used by previous studies into a weighted loss formulation that encourages the learned adversarial triggers to optimize more towards response format tokens. This enables ATLA to learn an adversarial trigger from just one query-response pair and the learned trigger generalizes well to other similar queries. We further design a variation to augment trigger optimization with an auxiliary loss that suppresses evasive responses. We showcase how to use ATLA to learn adversarial suffixes jailbreaking LLMs and to extract hidden system prompts. Empirically we demonstrate that ATLA consistently outperforms current state-of-the-art techniques, achieving nearly 100% success in attacking while requiring 80% fewer queries. ATLA learned jailbreak suffixes demonstrate high generalization to unseen queries and transfer well to new LLMs."
  },
  {
    "title": "GameChat: Multi-LLM Dialogue for Safe, Agile, and Socially Optimal Multi-Agent Navigation in Constrained Environments",
    "url": "http://arxiv.org/abs/2503.12333v1",
    "arxiv_id": "2503.12333v1",
    "authors": [
      "Vagul Mahadevan",
      "Shangtong Zhang",
      "Rohan Chandra"
    ],
    "published": "2025-03-16T03:02:40+00:00",
    "summary": "Safe, agile, and socially compliant multi-robot navigation in cluttered and constrained environments remains a critical challenge. This is especially difficult with self-interested agents in decentralized settings, where there is no central authority to resolve conflicts induced by spatial symmetry. We address this challenge by proposing a novel approach, GameChat, which facilitates safe, agile, and deadlock-free navigation for both cooperative and self-interested agents. Key to our approach is the use of natural language communication to resolve conflicts, enabling agents to prioritize more urgent tasks and break spatial symmetry in a socially optimal manner. Our algorithm ensures subgame perfect equilibrium, preventing agents from deviating from agreed-upon behaviors and supporting cooperation. Furthermore, we guarantee safety through control barrier functions and preserve agility by minimizing disruptions to agents' planned trajectories. We evaluate GameChat in simulated environments with doorways and intersections. The results show that even in the worst case, GameChat reduces the time for all agents to reach their goals by over 35% from a naive baseline and by over 20% from SMG-CBF in the intersection scenario, while doubling the rate of ensuring the agent with a higher priority task reaches the goal first, from 50% (equivalent to random chance) to a 100% perfect performance at maximizing social welfare."
  },
  {
    "title": "Integrating Chain-of-Thought and Retrieval Augmented Generation Enhances Rare Disease Diagnosis from Clinical Notes",
    "url": "http://arxiv.org/abs/2503.12286v1",
    "arxiv_id": "2503.12286v1",
    "authors": [
      "Da Wu",
      "Zhanliang Wang",
      "Quan Nguyen",
      "Kai Wang"
    ],
    "published": "2025-03-15T22:57:31+00:00",
    "summary": "Background: Several studies show that large language models (LLMs) struggle with phenotype-driven gene prioritization for rare diseases. These studies typically use Human Phenotype Ontology (HPO) terms to prompt foundation models like GPT and LLaMA to predict candidate genes. However, in real-world settings, foundation models are not optimized for domain-specific tasks like clinical diagnosis, yet inputs are unstructured clinical notes rather than standardized terms. How LLMs can be instructed to predict candidate genes or disease diagnosis from unstructured clinical notes remains a major challenge. Methods: We introduce RAG-driven CoT and CoT-driven RAG, two methods that combine Chain-of-Thought (CoT) and Retrieval Augmented Generation (RAG) to analyze clinical notes. A five-question CoT protocol mimics expert reasoning, while RAG retrieves data from sources like HPO and OMIM (Online Mendelian Inheritance in Man). We evaluated these approaches on rare disease datasets, including 5,980 Phenopacket-derived notes, 255 literature-based narratives, and 220 in-house clinical notes from Childrens Hospital of Philadelphia. Results: We found that recent foundations models, including Llama 3.3-70B-Instruct and DeepSeek-R1-Distill-Llama-70B, outperformed earlier versions such as Llama 2 and GPT-3.5. We also showed that RAG-driven CoT and CoT-driven RAG both outperform foundation models in candidate gene prioritization from clinical notes; in particular, both methods with DeepSeek backbone resulted in a top-10 gene accuracy of over 40% on Phenopacket-derived clinical notes. RAG-driven CoT works better for high-quality notes, where early retrieval can anchor the subsequent reasoning steps in domain-specific evidence, while CoT-driven RAG has advantage when processing lengthy and noisy notes."
  },
  {
    "title": "Reflect-DiT: Inference-Time Scaling for Text-to-Image Diffusion Transformers via In-Context Reflection",
    "url": "http://arxiv.org/abs/2503.12271v1",
    "arxiv_id": "2503.12271v1",
    "authors": [
      "Shufan Li",
      "Konstantinos Kallidromitis",
      "Akash Gokul",
      "Arsh Koneru",
      "Yusuke Kato",
      "Kazuki Kozuka",
      "Aditya Grover"
    ],
    "published": "2025-03-15T21:58:12+00:00",
    "summary": "The predominant approach to advancing text-to-image generation has been training-time scaling, where larger models are trained on more data using greater computational resources. While effective, this approach is computationally expensive, leading to growing interest in inference-time scaling to improve performance. Currently, inference-time scaling for text-to-image diffusion models is largely limited to best-of-N sampling, where multiple images are generated per prompt and a selection model chooses the best output. Inspired by the recent success of reasoning models like DeepSeek-R1 in the language domain, we introduce an alternative to naive best-of-N sampling by equipping text-to-image Diffusion Transformers with in-context reflection capabilities. We propose Reflect-DiT, a method that enables Diffusion Transformers to refine their generations using in-context examples of previously generated images alongside textual feedback describing necessary improvements. Instead of passively relying on random sampling and hoping for a better result in a future generation, Reflect-DiT explicitly tailors its generations to address specific aspects requiring enhancement. Experimental results demonstrate that Reflect-DiT improves performance on the GenEval benchmark (+0.19) using SANA-1.0-1.6B as a base model. Additionally, it achieves a new state-of-the-art score of 0.81 on GenEval while generating only 20 samples per prompt, surpassing the previous best score of 0.80, which was obtained using a significantly larger model (SANA-1.5-4.8B) with 2048 samples under the best-of-N approach."
  },
  {
    "title": "Indoor Positioning for Public Safety: Role of UAVs, LEOs, and Propagation-Aware Techniques",
    "url": "http://arxiv.org/abs/2503.12264v1",
    "arxiv_id": "2503.12264v1",
    "authors": [
      "Gaurav Duggal",
      "Harish K. Dureppagari",
      "Harpreet S. Dhillon",
      "Jeffrey H. Reed",
      "R. Michael Buehrer"
    ],
    "published": "2025-03-15T21:17:57+00:00",
    "summary": "Effective indoor positioning is critical for public safety, enabling first responders to locate at-risk individuals accurately during emergency scenarios. However, traditional Global Navigation Satellite Systems (GNSS) often perform poorly indoors due to poor coverage and non-line-of-sight (NLOS) conditions. Moreover, relying on fixed cellular infrastructure, such as terrestrial networks (TNs), may not be feasible, as indoor signal coverage from a sufficient number of base stations or WiFi access points cannot be guaranteed for accurate positioning. In this paper, we propose a rapidly deployable indoor positioning system (IPS) leveraging mobile anchors, including uncrewed aerial vehicles (UAVs) and Low-Earth-Orbit (LEO) satellites, and discuss the role of GNSS and LEOs in localizing the mobile anchors. Additionally, we discuss the role of sidelink-based positioning, which is introduced in 3rd Generation Partnership Project (3GPP) Release 18, in enabling public safety systems. By examining outdoor-to-indoor (O2I) signal propagation, particularly diffraction-based approaches, we highlight how propagation-aware positioning methods can outperform conventional strategies that disregard propagation mechanism information. The study highlights how emerging 5G Advanced and Non-Terrestrial Networks (NTN) features offer new avenues to improve positioning in challenging indoor environments, ultimately paving the way for cost-effective and resilient IPS solutions tailored to public safety applications."
  },
  {
    "title": "GenOSIL: Generalized Optimal and Safe Robot Control using Parameter-Conditioned Imitation Learning",
    "url": "http://arxiv.org/abs/2503.12243v1",
    "arxiv_id": "2503.12243v1",
    "authors": [
      "Mumuksh Tayal",
      "Manan Tayal",
      "Ravi Prakash"
    ],
    "published": "2025-03-15T19:52:16+00:00",
    "summary": "Ensuring safe and generalizable control remains a fundamental challenge in robotics, particularly when deploying imitation learning in dynamic environments. Traditional behavior cloning (BC) struggles to generalize beyond its training distribution, as it lacks an understanding of the safety critical reasoning behind expert demonstrations. To address this limitation, we propose GenOSIL, a novel imitation learning framework that explicitly incorporates environment parameters into policy learning via a structured latent representation. Unlike conventional methods that treat the environment as a black box, GenOSIL employs a variational autoencoder (VAE) to encode measurable safety parameters such as obstacle position, velocity, and geometry into a latent space that captures intrinsic correlations between expert behavior and environmental constraints. This enables the policy to infer the rationale behind expert trajectories rather than merely replicating them. We validate our approach on two robotic platforms an autonomous ground vehicle and a Franka Emika Panda manipulator demonstrating superior safety and goal reaching performance compared to baseline methods. The simulation and hardware videos can be viewed on the project webpage: https://mumukshtayal.github.io/GenOSIL/."
  },
  {
    "title": "A Novel Double Pruning method for Imbalanced Data using Information Entropy and Roulette Wheel Selection for Breast Cancer Diagnosis",
    "url": "http://arxiv.org/abs/2503.12239v1",
    "arxiv_id": "2503.12239v1",
    "authors": [
      "Soufiane Bacha",
      "Huansheng Ning",
      "Belarbi Mostefa",
      "Doreen Sebastian Sarwatt",
      "Sahraoui Dhelim"
    ],
    "published": "2025-03-15T19:34:15+00:00",
    "summary": "Accurate illness diagnosis is vital for effective treatment and patient safety. Machine learning models are widely used for cancer diagnosis based on historical medical data. However, data imbalance remains a major challenge, leading to hindering classifier performance and reliability. The SMOTEBoost method addresses this issue by generating synthetic data to balance the dataset, but it may overlook crucial overlapping regions near the decision boundary and can produce noisy samples. This paper proposes RE-SMOTEBoost, an enhanced version of SMOTEBoost, designed to overcome these limitations. Firstly, RE-SMOTEBoost focuses on generating synthetic samples in overlapping regions to better capture the decision boundary using roulette wheel selection. Secondly, it incorporates a filtering mechanism based on information entropy to reduce noise, and borderline cases and improve the quality of generated data. Thirdly, we introduce a double regularization penalty to control the synthetic samples proximity to the decision boundary and avoid class overlap. These enhancements enable higher-quality oversampling of the minority class, resulting in a more balanced and effective training dataset. The proposed method outperforms existing state-of-the-art techniques when evaluated on imbalanced datasets. Compared to the top-performing sampling algorithms, RE-SMOTEBoost demonstrates a notable improvement of 3.22\\% in accuracy and a variance reduction of 88.8\\%. These results indicate that the proposed model offers a solid solution for medical settings, effectively overcoming data scarcity and severe imbalance caused by limited samples, data collection difficulties, and privacy constraints."
  },
  {
    "title": "Gun Detection Using Combined Human Pose and Weapon Appearance",
    "url": "http://arxiv.org/abs/2503.12215v1",
    "arxiv_id": "2503.12215v1",
    "authors": [
      "Amulya Reddy Maligireddy",
      "Manohar Reddy Uppula",
      "Nidhi Rastogi",
      "Yaswanth Reddy Parla"
    ],
    "published": "2025-03-15T17:57:35+00:00",
    "summary": "The increasing frequency of firearm-related incidents has necessitated advancements in security and surveillance systems, particularly in firearm detection within public spaces. Traditional gun detection methods rely on manual inspections and continuous human monitoring of CCTV footage, which are labor-intensive and prone to high false positive and negative rates. To address these limitations, we propose a novel approach that integrates human pose estimation with weapon appearance recognition using deep learning techniques. Unlike prior studies that focus on either body pose estimation or firearm detection in isolation, our method jointly analyzes posture and weapon presence to enhance detection accuracy in real-world, dynamic environments. To train our model, we curated a diverse dataset comprising images from open-source repositories such as IMFDB and Monash Guns, supplemented with AI-generated and manually collected images from web sources. This dataset ensures robust generalization and realistic performance evaluation under various surveillance conditions. Our research aims to improve the precision and reliability of firearm detection systems, contributing to enhanced public safety and threat mitigation in high-risk areas."
  },
  {
    "title": "Hyperbolic Safety-Aware Vision-Language Models",
    "url": "http://arxiv.org/abs/2503.12127v1",
    "arxiv_id": "2503.12127v1",
    "authors": [
      "Tobia Poppi",
      "Tejaswi Kasarla",
      "Pascal Mettes",
      "Lorenzo Baraldi",
      "Rita Cucchiara"
    ],
    "published": "2025-03-15T13:18:04+00:00",
    "summary": "Addressing the retrieval of unsafe content from vision-language models such as CLIP is an important step towards real-world integration. Current efforts have relied on unlearning techniques that try to erase the model's knowledge of unsafe concepts. While effective in reducing unwanted outputs, unlearning limits the model's capacity to discern between safe and unsafe content. In this work, we introduce a novel approach that shifts from unlearning to an awareness paradigm by leveraging the inherent hierarchical properties of the hyperbolic space. We propose to encode safe and unsafe content as an entailment hierarchy, where both are placed in different regions of hyperbolic space. Our HySAC, Hyperbolic Safety-Aware CLIP, employs entailment loss functions to model the hierarchical and asymmetrical relations between safe and unsafe image-text pairs. This modelling, ineffective in standard vision-language models due to their reliance on Euclidean embeddings, endows the model with awareness of unsafe content, enabling it to serve as both a multimodal unsafe classifier and a flexible content retriever, with the option to dynamically redirect unsafe queries toward safer alternatives or retain the original output. Extensive experiments show that our approach not only enhances safety recognition but also establishes a more adaptable and interpretable framework for content moderation in vision-language models. Our source code is available at https://github.com/aimagelab/HySAC."
  },
  {
    "title": "Towards Vision Zero: The Accid3nD Dataset",
    "url": "http://arxiv.org/abs/2503.12095v1",
    "arxiv_id": "2503.12095v1",
    "authors": [
      "Walter Zimmer",
      "Ross Greer",
      "Daniel Lehmberg",
      "Marc Pavel",
      "Holger Caesar",
      "Xingcheng Zhou",
      "Ahmed Ghita",
      "Mohan Trivedi",
      "Rui Song",
      "Hu Cao",
      "Akshay Gopalkrishnan",
      "Alois C. Knoll"
    ],
    "published": "2025-03-15T11:42:16+00:00",
    "summary": "Even though a significant amount of work has been done to increase the safety of transportation networks, accidents still occur regularly. They must be understood as unavoidable and sporadic outcomes of traffic networks. No public dataset contains 3D annotations of real-world accidents recorded from roadside sensors. We present the Accid3nD dataset, a collection of real-world highway accidents in different weather and lighting conditions. It contains vehicle crashes at high-speed driving with 2,634,233 labeled 2D bounding boxes, instance masks, and 3D bounding boxes with track IDs. In total, the dataset contains 111,945 labeled frames recorded from four roadside cameras and LiDARs at 25 Hz. The dataset contains six object classes and is provided in the OpenLABEL format. We propose an accident detection model that combines a rule-based approach with a learning-based one. Experiments and ablation studies on our dataset show the robustness of our proposed method. The dataset, model, and code are available on our website: https://accident-dataset.github.io."
  },
  {
    "title": "Automating the loop in traffic incident management on highway",
    "url": "http://arxiv.org/abs/2503.12085v1",
    "arxiv_id": "2503.12085v1",
    "authors": [
      "Matteo Cercola",
      "Nicola Gatti",
      "Pedro Huertas Leyva",
      "Benedetto Carambia",
      "Simone Formentin"
    ],
    "published": "2025-03-15T11:22:13+00:00",
    "summary": "Effective traffic incident management is essential for ensuring safety, minimizing congestion, and reducing response times in emergency situations. Traditional highway incident management relies heavily on radio room operators, who must make rapid, informed decisions in high-stakes environments. This paper proposes an innovative solution to support and enhance these decisions by integrating Large Language Models (LLMs) into a decision-support system for traffic incident management. We introduce two approaches: (1) an LLM + Optimization hybrid that leverages both the flexibility of natural language interaction and the robustness of optimization techniques, and (2) a Full LLM approach that autonomously generates decisions using only LLM capabilities. We tested our solutions using historical event data from Autostrade per l'Italia. Experimental results indicate that while both approaches show promise, the LLM + Optimization solution demonstrates superior reliability, making it particularly suited to critical applications where consistency and accuracy are paramount. This research highlights the potential for LLMs to transform highway incident management by enabling accessible, data-driven decision-making support."
  },
  {
    "title": "Proof-Driven Clause Learning in Neural Network Verification",
    "url": "http://arxiv.org/abs/2503.12083v1",
    "arxiv_id": "2503.12083v1",
    "authors": [
      "Omri Isac",
      "Idan Refaeli",
      "Haoze Wu",
      "Clark Barrett",
      "Guy Katz"
    ],
    "published": "2025-03-15T11:05:15+00:00",
    "summary": "The widespread adoption of deep neural networks (DNNs) requires efficient techniques for safety verification. Existing methods struggle to scale to real-world DNNs, and tremendous efforts are being put into improving their scalability. In this work, we propose an approach for improving the scalability of DNN verifiers using Conflict-Driven Clause Learning (CDCL) -- an approach that has proven highly successful in SAT and SMT solving. We present a novel algorithm for deriving conflict clauses using UNSAT proofs, and propose several optimizations for expediting it. Our approach allows a modular integration of SAT solvers and DNN verifiers, and we implement it on top of an interface designed for this purpose. The evaluation of our implementation over several benchmarks suggests a 2X--3X improvement over a similar approach, with specific cases outperforming the state of the art."
  },
  {
    "title": "Generative Modeling of Adversarial Lane-Change Scenario",
    "url": "http://arxiv.org/abs/2503.12055v1",
    "arxiv_id": "2503.12055v1",
    "authors": [
      "Chuancheng Zhang",
      "Zhenhao Wang",
      "Jiangcheng Wang",
      "Kun Su",
      "Qiang Lv",
      "Bin Jiang",
      "Kunkun Hao",
      "Wenyu Wang"
    ],
    "published": "2025-03-15T09:05:04+00:00",
    "summary": "Decision-making in long-tail scenarios is crucial to autonomous driving development, with realistic and challenging simulations playing a pivotal role in testing safety-critical situations. However, the current open-source datasets do not systematically include long-tail distributed scenario data, making acquiring such scenarios a formidable task. To address this problem, a data mining framework is proposed, which performs in-depth analysis on two widely-used datasets, NGSIM and INTERACTION, to pinpoint data with hazardous behavioral traits, aiming to bridge the gap in these overlooked scenarios. The approach utilizes Generative Adversarial Imitation Learning (GAIL) based on an enhanced Proximal Policy Optimization (PPO) model, integrated with the vehicle's environmental analysis, to iteratively refine and represent the newly generated vehicle trajectory. Innovatively, the solution optimizes the generation of adversarial scenario data from the perspectives of sensitivity and reasonable adversarial. It is demonstrated through experiments that, compared to the unfiltered data and baseline models, the approach exhibits more adversarial yet natural behavior regarding collision rate, acceleration, and lane changes, thereby validating its suitability for generating scenario data and providing constructive insights for the development of future scenarios and subsequent decision training."
  },
  {
    "title": "TLUE: A Tibetan Language Understanding Evaluation Benchmark",
    "url": "http://arxiv.org/abs/2503.12051v1",
    "arxiv_id": "2503.12051v1",
    "authors": [
      "Fan Gao",
      "Cheng Huang",
      "Nyima Tashi",
      "Xiangxiang Wang",
      "Thupten Tsering",
      "Ban Ma-bao",
      "Renzeg Duojie",
      "Gadeng Luosang",
      "Rinchen Dongrub",
      "Dorje Tashi",
      "Xiao Feng",
      "Yongbin Yu"
    ],
    "published": "2025-03-15T08:54:25+00:00",
    "summary": "Large language models (LLMs) have made tremendous progress in recent years, but low-resource languages, such as Tibetan, remain significantly underrepresented in their evaluation. Despite Tibetan being spoken by over seven million people, it has largely been neglected in the development and assessment of LLMs. To address this gap, we present TLUE (A Tibetan Language Understanding Evaluation Benchmark), the first large-scale benchmark for assessing LLMs' capabilities in Tibetan. TLUE comprises two major components: (1) a comprehensive multi-task understanding benchmark spanning 5 domains and 67 subdomains, and (2) a safety benchmark covering 7 subdomains. We evaluate a diverse set of state-of-the-art LLMs. Experimental results demonstrate that most LLMs perform below the random baseline, highlighting the considerable challenges LLMs face in processing Tibetan, a low-resource language. TLUE provides an essential foundation for driving future research and progress in Tibetan language understanding and underscores the need for greater inclusivity in LLM development."
  },
  {
    "title": "An LLM-Integrated Framework for Completion, Management, and Tracing of STPA",
    "url": "http://arxiv.org/abs/2503.12043v1",
    "arxiv_id": "2503.12043v1",
    "authors": [
      "Ali Raeisdanaei",
      "Juho Kim",
      "Michael Liao",
      "Sparsh Kochhar"
    ],
    "published": "2025-03-15T08:31:13+00:00",
    "summary": "In many safety-critical engineering domains, hazard analysis techniques are an essential part of requirement elicitation. Of the methods proposed for this task, STPA (System-Theoretic Process Analysis) represents a relatively recent development in the field. The completion, management, and traceability of this hazard analysis technique present a time-consuming challenge to the requirements and safety engineers involved. In this paper, we introduce a free, open-source software framework to build STPA models with several automated workflows powered by large language models (LLMs). In past works, LLMs have been successfully integrated into a myriad of workflows across various fields. Here, we demonstrate that LLMs can be used to complete tasks associated with STPA with a high degree of accuracy, saving the time and effort of the human engineers involved. We experimentally validate our method on real-world STPA models built by requirement engineers and researchers. The source code of our software framework is available at the following link: https://github.com/blueskysolarracing/stpa."
  },
  {
    "title": "Safety for Time-Varying Parameterized Sets Using Control Barrier Function Methods",
    "url": "http://arxiv.org/abs/2503.12003v1",
    "arxiv_id": "2503.12003v1",
    "authors": [
      "James Usevitch",
      "Jackson Sahleen"
    ],
    "published": "2025-03-15T05:53:21+00:00",
    "summary": "A fundamental and classical problem in mobile autonomous systems is maintaining the safety of autonomous agents during deployment. Prior literature has presented techniques using control barrier functions (CBFs) to achieve this goal. These prior techniques utilize CBFs to keep an isolated point in state space away from the unsafe set. However, various situations require a non-singleton set of states to be kept away from an unsafe set. Prior literature has addressed this problem using nonsmooth CBF methods, but no prior work has solved this problem using only \"smooth\" CBF methods. This paper addresses this gap by presenting a novel method of applying CBF methods to non-singleton parameterized convex sets. The method ensures differentiability of the squared distance function between ego and obstacle sets by leveraging a form of the log-sum-exp function to form strictly convex, arbitrarily tight overapproximations of these sets. Safety-preserving control inputs can be computed via convex optimization formulations. The efficacy of our results is demonstrated through multi-agent simulations."
  },
  {
    "title": "Impact of Frequency on Diffraction-Aided Wireless Positioning",
    "url": "http://arxiv.org/abs/2503.11993v1",
    "arxiv_id": "2503.11993v1",
    "authors": [
      "Gaurav Duggal",
      "Anand M. Kumar",
      "R. Michael Buehrer",
      "Harpreet S. Dhillon",
      "Nishith Tripathi",
      "Jeffrey H. Reed"
    ],
    "published": "2025-03-15T04:46:48+00:00",
    "summary": "This paper tackles the challenge of accurate positioning in Non-Line-of-Sight (NLoS) environments, with a focus on indoor public safety scenarios where NLoS bias severely impacts localization performance. We explore Diffraction MultiPath Components (MPC) as a critical mechanism for Outdoor-to-Indoor (O2I) signal propagation and its role in positioning. The proposed system comprises outdoor Uncrewed Aerial Vehicle (UAV) transmitters and indoor receivers that require localization. To facilitate diffraction-based positioning, we develop a method to isolate diffraction MPCs at indoor receivers and validate its effectiveness using a ray-tracing-generated dataset, which we have made publicly available. Our evaluation across the FR1, FR2, and FR3 frequency bands within the 5G/6G spectrum confirms the viability of diffraction-based positioning techniques for next-generation wireless networks."
  },
  {
    "title": "SagaLLM: Context Management, Validation, and Transaction Guarantees for Multi-Agent LLM Planning",
    "url": "http://arxiv.org/abs/2503.11951v1",
    "arxiv_id": "2503.11951v1",
    "authors": [
      "Edward Y. Chang"
    ],
    "published": "2025-03-15T01:43:03+00:00",
    "summary": "Recent LLM-based agent frameworks have demonstrated impressive capabilities in task delegation and workflow orchestration, but face significant challenges in maintaining context awareness and ensuring planning consistency. This paper presents SagaLLM, a structured multi-agent framework that addresses four fundamental limitations in current LLM approaches: inadequate self-validation, context narrowing, lacking transaction properties, and insufficient inter-agent coordination. By implementing specialized context management agents and validation protocols, SagaLLM preserves critical constraints and state information throughout complex planning processes, enabling robust and consistent decision-making even during disruptions. We evaluate our approach using selected problems from the REALM benchmark, focusing on sequential and reactive planning scenarios that challenge both context retention and adaptive reasoning. Our experiments with state-of-the-art LLMs, Claude 3.7, DeepSeek R1, GPT-4o, and GPT-o1, demonstrate that while these models exhibit impressive reasoning capabilities, they struggle with maintaining global constraint awareness during complex planning tasks, particularly when adapting to unexpected changes. In contrast, the distributed cognitive architecture of SagaLLM shows significant improvements in planning consistency, constraint enforcement, and adaptation to disruptions in various scenarios."
  },
  {
    "title": "SagaLLM: Context Management, Validation, and Transaction Guarantees for Multi-Agent LLM Planning",
    "url": "http://arxiv.org/abs/2503.11951v2",
    "arxiv_id": "2503.11951v2",
    "authors": [
      "Edward Y. Chang",
      "Longling Geng"
    ],
    "published": "2025-03-15T01:43:03+00:00",
    "summary": "Recent LLM-based agent frameworks have demonstrated impressive capabilities in task delegation and workflow orchestration, but face significant challenges in maintaining context awareness and ensuring planning consistency. This paper presents SagaLLM, a structured multi-agent framework that addresses four fundamental limitations in current LLM approaches: inadequate self-validation, context narrowing, lacking transaction properties, and insufficient inter-agent coordination. By implementing specialized context management agents and validation protocols, SagaLLM preserves critical constraints and state information throughout complex planning processes, enabling robust and consistent decision-making even during disruptions. We evaluate our approach using selected problems from the REALM benchmark, focusing on sequential and reactive planning scenarios that challenge both context retention and adaptive reasoning. Our experiments with state-of-the-art LLMs, Claude 3.7, DeepSeek R1, GPT-4o, and GPT-o1, demonstrate that while these models exhibit impressive reasoning capabilities, they struggle with maintaining global constraint awareness during complex planning tasks, particularly when adapting to unexpected changes. In contrast, the distributed cognitive architecture of SagaLLM shows significant improvements in planning consistency, constraint enforcement, and adaptation to disruptions in various scenarios."
  },
  {
    "title": "Monitoring Reasoning Models for Misbehavior and the Risks of Promoting Obfuscation",
    "url": "http://arxiv.org/abs/2503.11926v1",
    "arxiv_id": "2503.11926v1",
    "authors": [
      "Bowen Baker",
      "Joost Huizinga",
      "Leo Gao",
      "Zehao Dou",
      "Melody Y. Guan",
      "Aleksander Madry",
      "Wojciech Zaremba",
      "Jakub Pachocki",
      "David Farhi"
    ],
    "published": "2025-03-14T23:50:34+00:00",
    "summary": "Mitigating reward hacking--where AI systems misbehave due to flaws or misspecifications in their learning objectives--remains a key challenge in constructing capable and aligned models. We show that we can monitor a frontier reasoning model, such as OpenAI o3-mini, for reward hacking in agentic coding environments by using another LLM that observes the model's chain-of-thought (CoT) reasoning. CoT monitoring can be far more effective than monitoring agent actions and outputs alone, and we further found that a LLM weaker than o3-mini, namely GPT-4o, can effectively monitor a stronger model. Because CoT monitors can be effective at detecting exploits, it is natural to ask whether those exploits can be suppressed by incorporating a CoT monitor directly into the agent's training objective. While we show that integrating CoT monitors into the reinforcement learning reward can indeed produce more capable and more aligned agents in the low optimization regime, we find that with too much optimization, agents learn obfuscated reward hacking, hiding their intent within the CoT while still exhibiting a significant rate of reward hacking. Because it is difficult to tell when CoTs have become obfuscated, it may be necessary to pay a monitorability tax by not applying strong optimization pressures directly to the chain-of-thought, ensuring that CoTs remain monitorable and useful for detecting misaligned behavior."
  },
  {
    "title": "On Regulating Downstream AI Developers",
    "url": "http://arxiv.org/abs/2503.11922v1",
    "arxiv_id": "2503.11922v1",
    "authors": [
      "Sophie Williams",
      "Jonas Schuett",
      "Markus Anderljung"
    ],
    "published": "2025-03-14T23:15:54+00:00",
    "summary": "Foundation models - models trained on broad data that can be adapted to a wide range of downstream tasks - can pose significant risks, ranging from intimate image abuse, cyberattacks, to bioterrorism. To reduce these risks, policymakers are starting to impose obligations on the developers of these models. However, downstream developers - actors who fine-tune or otherwise modify foundational models - can create or amplify risks by improving a model's capabilities or compromising its safety features. This can make rules on upstream developers ineffective. One way to address this issue could be to impose direct obligations on downstream developers. However, since downstream developers are numerous, diverse, and rapidly growing in number, such direct regulation may be both practically challenging and stifling to innovation. A different approach would be to require upstream developers to mitigate downstream modification risks (e.g. by restricting what modifications can be made). Another approach would be to use alternative policy tools (e.g. clarifying how existing tort law applies to downstream developers or issuing voluntary guidance to help mitigate downstream modification risks). We expect that regulation on upstream developers to mitigate downstream modification risks will be necessary. Although further work is needed, regulation of downstream developers may also be warranted where they retain the ability to increase risk to an unacceptable level."
  },
  {
    "title": "Centaur: Robust End-to-End Autonomous Driving with Test-Time Training",
    "url": "http://arxiv.org/abs/2503.11650v1",
    "arxiv_id": "2503.11650v1",
    "authors": [
      "Chonghao Sima",
      "Kashyap Chitta",
      "Zhiding Yu",
      "Shiyi Lan",
      "Ping Luo",
      "Andreas Geiger",
      "Hongyang Li",
      "Jose M. Alvarez"
    ],
    "published": "2025-03-14T17:59:41+00:00",
    "summary": "How can we rely on an end-to-end autonomous vehicle's complex decision-making system during deployment? One common solution is to have a ``fallback layer'' that checks the planned trajectory for rule violations and replaces it with a pre-defined safe action if necessary. Another approach involves adjusting the planner's decisions to minimize a pre-defined ``cost function'' using additional system predictions such as road layouts and detected obstacles. However, these pre-programmed rules or cost functions cannot learn and improve with new training data, often resulting in overly conservative behaviors. In this work, we propose Centaur (Cluster Entropy for Test-time trAining using Uncertainty) which updates a planner's behavior via test-time training, without relying on hand-engineered rules or cost functions. Instead, we measure and minimize the uncertainty in the planner's decisions. For this, we develop a novel uncertainty measure, called Cluster Entropy, which is simple, interpretable, and compatible with state-of-the-art planning algorithms. Using data collected at prior test-time time-steps, we perform an update to the model's parameters using a gradient that minimizes the Cluster Entropy. With only this sole gradient update prior to inference, Centaur exhibits significant improvements, ranking first on the navtest leaderboard with notable gains in safety-critical metrics such as time to collision. To provide detailed insights on a per-scenario basis, we also introduce navsafe, a challenging new benchmark, which highlights previously undiscovered failure modes of driving models."
  },
  {
    "title": "Tit-for-Tat: Safeguarding Large Vision-Language Models Against Jailbreak Attacks via Adversarial Defense",
    "url": "http://arxiv.org/abs/2503.11619v1",
    "arxiv_id": "2503.11619v1",
    "authors": [
      "Shuyang Hao",
      "Yiwei Wang",
      "Bryan Hooi",
      "Ming-Hsuan Yang",
      "Jun Liu",
      "Chengcheng Tang",
      "Zi Huang",
      "Yujun Cai"
    ],
    "published": "2025-03-14T17:39:45+00:00",
    "summary": "Deploying large vision-language models (LVLMs) introduces a unique vulnerability: susceptibility to malicious attacks via visual inputs. However, existing defense methods suffer from two key limitations: (1) They solely focus on textual defenses, fail to directly address threats in the visual domain where attacks originate, and (2) the additional processing steps often incur significant computational overhead or compromise model performance on benign tasks. Building on these insights, we propose ESIII (Embedding Security Instructions Into Images), a novel methodology for transforming the visual space from a source of vulnerability into an active defense mechanism. Initially, we embed security instructions into defensive images through gradient-based optimization, obtaining security instructions in the visual dimension. Subsequently, we integrate security instructions from visual and textual dimensions with the input query. The collaboration between security instructions from different dimensions ensures comprehensive security protection. Extensive experiments demonstrate that our approach effectively fortifies the robustness of LVLMs against such attacks while preserving their performance on standard benign tasks and incurring an imperceptible increase in time costs."
  },
  {
    "title": "Broaden your SCOPE! Efficient Multi-turn Conversation Planning for LLMs using Semantic Space",
    "url": "http://arxiv.org/abs/2503.11586v1",
    "arxiv_id": "2503.11586v1",
    "authors": [
      "Zhiliang Chen",
      "Xinyuan Niu",
      "Chuan-Sheng Foo",
      "Bryan Kian Hsiang Low"
    ],
    "published": "2025-03-14T16:55:46+00:00",
    "summary": "Large language models (LLMs) are used in chatbots or AI assistants to hold conversations with a human user. In such applications, the quality (e.g., user engagement, safety) of a conversation is important and can only be exactly known at the end of the conversation. To maximize its expected quality, conversation planning reasons about the stochastic transitions within a conversation to select the optimal LLM response at each turn. Existing simulation-based conversation planning algorithms typically select the optimal response by simulating future conversations with a large number of LLM queries at every turn. However, this process is extremely time-consuming and hence impractical for real-time conversations. This paper presents a novel approach called Semantic space COnversation Planning with improved Efficiency (SCOPE) that exploits the dense semantic representation of conversations to perform conversation planning efficiently. In particular, SCOPE models the stochastic transitions in conversation semantics and their associated rewards to plan entirely within the semantic space. This allows us to select the optimal LLM response at every conversation turn without needing additional LLM queries for simulation. As a result, SCOPE can perform conversation planning 70 times faster than conventional simulation-based planning algorithms when applied to a wide variety of conversation starters and two reward functions seen in the real world, yet achieving a higher reward within a practical planning budget. Our code can be found at: https://github.com/chenzhiliang94/convo-plan-SCOPE."
  },
  {
    "title": "A Review of DeepSeek Models' Key Innovative Techniques",
    "url": "http://arxiv.org/abs/2503.11486v1",
    "arxiv_id": "2503.11486v1",
    "authors": [
      "Chengen Wang",
      "Murat Kantarcioglu"
    ],
    "published": "2025-03-14T15:11:29+00:00",
    "summary": "DeepSeek-V3 and DeepSeek-R1 are leading open-source Large Language Models (LLMs) for general-purpose tasks and reasoning, achieving performance comparable to state-of-the-art closed-source models from companies like OpenAI and Anthropic -- while requiring only a fraction of their training costs. Understanding the key innovative techniques behind DeepSeek's success is crucial for advancing LLM research. In this paper, we review the core techniques driving the remarkable effectiveness and efficiency of these models, including refinements to the transformer architecture, innovations such as Multi-Head Latent Attention and Mixture of Experts, Multi-Token Prediction, the co-design of algorithms, frameworks, and hardware, the Group Relative Policy Optimization algorithm, post-training with pure reinforcement learning and iterative training alternating between supervised fine-tuning and reinforcement learning. Additionally, we identify several open questions and highlight potential research opportunities in this rapidly advancing field."
  },
  {
    "title": "Certified Inductive Synthesis for Online Mixed-Integer Optimization",
    "url": "http://arxiv.org/abs/2503.11388v1",
    "arxiv_id": "2503.11388v1",
    "authors": [
      "Marco Zamponi",
      "Emilio Incerto",
      "Daniele Masti",
      "Mirco Tribastone"
    ],
    "published": "2025-03-14T13:31:03+00:00",
    "summary": "In fields such as autonomous and safety-critical systems, online optimization plays a crucial role in control and decision-making processes, often requiring the integration of continuous and discrete variables. These tasks are frequently modeled as mixed-integer programming (MIP) problems, where feedback data are incorporated as parameters. However, solving MIPs within strict time constraints is challenging due to their $\\mathcal{NP}$-complete nature. A promising solution to this challenge involves leveraging the largely invariant structure of these problems to perform most computations offline, thus enabling efficient online solving even on platforms with limited hardware capabilities. In this paper we present a novel implementation of this strategy that uses counterexample-guided inductive synthesis to split the MIP solution process into two stages. In the offline phase, we construct a mapping that provides feasible assignments for binary variables based on parameter values within a specified range. In the online phase, we solve the remaining continuous part of the problem by fixing the binary variables to the values predicted by this mapping. Our numerical evaluation demonstrates the efficiency and solution quality of this approach compared to standard mixed-integer solvers, highlighting its potential for real-time applications in resource-constrained environments."
  },
  {
    "title": "Road Rage Reasoning with Vision-language Models (VLMs): Task Definition and Evaluation Dataset",
    "url": "http://arxiv.org/abs/2503.11342v1",
    "arxiv_id": "2503.11342v1",
    "authors": [
      "Yibing Weng",
      "Yu Gu",
      "Fuji Ren"
    ],
    "published": "2025-03-14T12:18:11+00:00",
    "summary": "Road rage, triggered by driving-related stimuli such as traffic congestion and aggressive driving, poses a significant threat to road safety. Previous research on road rage regulation has primarily focused on response suppression, lacking proactive prevention capabilities. With the advent of Vision-Language Models (VLMs), it has become possible to reason about trigger events visually and then engage in dialog-based comforting before drivers' anger escalates. To this end, we propose the road rage reasoning task, along with a finely annotated test dataset and evaluation metrics, to assess the capabilities of current mainstream VLMs in scene understanding, event recognition, and road rage reasoning. The results indicate that current VLMs exhibit significant shortcomings in scene understanding within the visual modality, as well as in comprehending the spatial relationships between objects in the textual modality. Improving VLMs' performance in these areas will greatly benefit downstream tasks like antecedent-focused road rage regulation."
  },
  {
    "title": "Contract Based Program Models for Software Model Checking",
    "url": "http://arxiv.org/abs/2503.11236v1",
    "arxiv_id": "2503.11236v1",
    "authors": [
      "Jesper Amilon",
      "Dilian Gurov"
    ],
    "published": "2025-03-14T09:34:59+00:00",
    "summary": "Model checking temporal properties of software is algorithmically hard. To be practically feasible, it usually requires the creation of simpler, abstract models of the software, over which the properties are checked. However, creating suitable abstractions is another difficult problem. We argue that such abstract models can be obtained with little effort, when the state transformation properties of the software components have already been deductively verified. As a concrete, language-independent representation of such abstractions we propose the use of \\emph{flow graphs}, a formalism previously developed for the purposes of compositional model checking. In this paper, we describe how we envisage the work flow and tool chain to support the proposed verification approach in the context of embedded, safety-critical software written in~C."
  },
  {
    "title": "Can Large Reasoning Models do Analogical Reasoning under Perceptual Uncertainty?",
    "url": "http://arxiv.org/abs/2503.11207v1",
    "arxiv_id": "2503.11207v1",
    "authors": [
      "Giacomo Camposampiero",
      "Michael Hersche",
      "Roger Wattenhofer",
      "Abu Sebastian",
      "Abbas Rahimi"
    ],
    "published": "2025-03-14T08:52:25+00:00",
    "summary": "This work presents a first evaluation of two state-of-the-art Large Reasoning Models (LRMs), OpenAI's o3-mini and DeepSeek R1, on analogical reasoning, focusing on well-established nonverbal human IQ tests based on Raven's progressive matrices. We benchmark with the I-RAVEN dataset and its more difficult extension, I-RAVEN-X, which tests the ability to generalize to longer reasoning rules and ranges of the attribute values. To assess the influence of visual uncertainties on these nonverbal analogical reasoning tests, we extend the I-RAVEN-X dataset, which otherwise assumes an oracle perception. We adopt a two-fold strategy to simulate this imperfect visual perception: 1) we introduce confounding attributes which, being sampled at random, do not contribute to the prediction of the correct answer of the puzzles and 2) smoothen the distributions of the input attributes' values. We observe a sharp decline in OpenAI's o3-mini task accuracy, dropping from 86.6% on the original I-RAVEN to just 17.0% -- approaching random chance -- on the more challenging I-RAVEN-X, which increases input length and range and emulates perceptual uncertainty. This drop occurred despite spending 3.4x more reasoning tokens. A similar trend is also observed for DeepSeek R1: from 80.6% to 23.2%. On the other hand, a neuro-symbolic probabilistic abductive model, ARLC, that achieves state-of-the-art performances on I-RAVEN, can robustly reason under all these out-of-distribution tests, maintaining strong accuracy with only a modest reduction from 98.6% to 88.0%. Our code is available at https://github.com/IBM/raven-large-language-models."
  },
  {
    "title": "GAIPAT -Dataset on Human Gaze and Actions for Intent Prediction in Assembly Tasks",
    "url": "http://arxiv.org/abs/2503.11186v1",
    "arxiv_id": "2503.11186v1",
    "authors": [
      "Maxence Grand",
      "Damien Pellier",
      "Francis Jambon"
    ],
    "published": "2025-03-14T08:32:52+00:00",
    "summary": "The primary objective of the dataset is to provide a better understanding of the coupling between human actions and gaze in a shared working environment with a cobot, with the aim of signifcantly enhancing the effciency and safety of humancobot interactions. More broadly, by linking gaze patterns with physical actions, the dataset offers valuable insights into cognitive processes and attention dynamics in the context of assembly tasks. The proposed dataset contains gaze and action data from approximately 80 participants, recorded during simulated industrial assembly tasks. The tasks were simulated using controlled scenarios in which participants manipulated educational building blocks. Gaze data was collected using two different eye-tracking setups -head-mounted and remote-while participants worked in two positions: sitting and standing."
  },
  {
    "title": "Align in Depth: Defending Jailbreak Attacks via Progressive Answer Detoxification",
    "url": "http://arxiv.org/abs/2503.11185v1",
    "arxiv_id": "2503.11185v1",
    "authors": [
      "Yingjie Zhang",
      "Tong Liu",
      "Zhe Zhao",
      "Guozhu Meng",
      "Kai Chen"
    ],
    "published": "2025-03-14T08:32:12+00:00",
    "summary": "Large Language Models (LLMs) are vulnerable to jailbreak attacks, which use crafted prompts to elicit toxic responses. These attacks exploit LLMs' difficulty in dynamically detecting harmful intents during the generation process. Traditional safety alignment methods, often relying on the initial few generation steps, are ineffective due to limited computational budget. This paper proposes DEEPALIGN, a robust defense framework that fine-tunes LLMs to progressively detoxify generated content, significantly improving both the computational budget and effectiveness of mitigating harmful generation. Our approach uses a hybrid loss function operating on hidden states to directly improve LLMs' inherent awareness of toxity during generation. Furthermore, we redefine safe responses by generating semantically relevant answers to harmful queries, thereby increasing robustness against representation-mutation attacks. Evaluations across multiple LLMs demonstrate state-of-the-art defense performance against six different attack types, reducing Attack Success Rates by up to two orders of magnitude compared to previous state-of-the-art defense while preserving utility. This work advances LLM safety by addressing limitations of conventional alignment through dynamic, context-aware mitigation."
  },
  {
    "title": "Hand Over or Place On The Table? A Study On Robotic Object Delivery When The Recipient Is Occupied",
    "url": "http://arxiv.org/abs/2503.11177v1",
    "arxiv_id": "2503.11177v1",
    "authors": [
      "Thieu Long Phan",
      "Akansel Cosgun"
    ],
    "published": "2025-03-14T08:25:34+00:00",
    "summary": "This study investigates the subjective experiences of users in two robotic object delivery methods: direct handover and table placement, when users are occupied with another task. A user study involving 15 participants engaged in a typing game revealed that table placement significantly enhances user experience compared to direct handovers, particularly in terms of satisfaction, perceived safety and intuitiveness. Additionally, handovers negatively impacted typing performance, while all participants expressed a clear preference for table placement as the delivery method. These findings highlight the advantages of table placement in scenarios requiring minimal user disruption."
  },
  {
    "title": "Prompt Alchemy: Automatic Prompt Refinement for Enhancing Code Generation",
    "url": "http://arxiv.org/abs/2503.11085v1",
    "arxiv_id": "2503.11085v1",
    "authors": [
      "Sixiang Ye",
      "Zeyu Sun",
      "Guoqing Wang",
      "Liwei Guo",
      "Qingyuan Liang",
      "Zheng Li",
      "Yong Liu"
    ],
    "published": "2025-03-14T04:53:03+00:00",
    "summary": "Code generation has emerged as a key task to automate software development by converting high-level descriptions into executable code. Large language models (LLMs) excel at this but depend heavily on input prompt quality.Manual prompt engineering can be time-consuming and inconsistent, limiting LLM effectiveness. This paper introduces Prochemy, an innovative method for automatically refining prompts to boost code generation. Prochemy overcomes manual prompt limitations by automating optimization, ensuring consistency during inference, and supporting multi-agent systems.It iteratively refines prompts based on model performance, using an optimized final prompt for improved consistency across tasks. We tested Prochemy on natural language-based code generation and translation tasks using three LLM series. Results indicate Prochemy enhances existing methods, improving performance by 5.0% for GPT-3.5-Turbo and 1.9% for GPT-4o over zero-shot baselines on HumanEval. In state-of-the-art LDB, Prochemy + LDB surpasses standalone methods by 1.2-1.8%. For code translation, Prochemy boosts GPT-4o's Java-to-Python (AVATAR) performance from 74.5 to 84.1 (+12.9%) and Python-to-Java from 66.8 to 78.2 (+17.1%). Moreover, Prochemy maintains strong performance when integrated with the o1-mini model, validating its efficacy in code tasks. Designed as plug-and-play, Prochemy optimizes prompts with minimal human input, bridging the gap between simple prompts and complex frameworks."
  },
  {
    "title": "Inverter Control with Time-Varying and Nonconvex State and Input Constraints",
    "url": "http://arxiv.org/abs/2503.11075v1",
    "arxiv_id": "2503.11075v1",
    "authors": [
      "Zixiao Ma",
      "Baosen Zhang"
    ],
    "published": "2025-03-14T04:35:02+00:00",
    "summary": "The growing integration of inverter-based resources (IBRs) into modern power systems poses significant challenges for maintaining reliable operation under dynamic and constrained conditions. This paper focuses on the power tracking problem for grid-connected IBRs, addressing the complexities introduced by voltage and power factor constraints. Voltage constraints, being time-varying and nonlinear input constraints, often conflict with power factor constraints, which are state constraints. These conflicts, coupled with stability requirements, add substantial complexity to control design. To overcome these challenges, we propose a computationally efficient static state-feedback controller that guarantees stability and satisfies operational constraints. The concept of achievability is introduced to evaluate whether power setpoints can be accurately tracked while adhering to all constraints. Using a parameterization framework and the S-lemma, we develop criteria to assess and maximize the continuous achievable region for IBR operation. This framework allows system operators to ensure safety and stability by precomputing a finite set of control gains, significantly reducing online computational requirements. The proposed approach is validated through simulations, demonstrating its effectiveness in handling time-varying grid disturbances and achieving reliable control performance."
  },
  {
    "title": "Large Reasoning Models in Agent Scenarios: Exploring the Necessity of Reasoning Capabilities",
    "url": "http://arxiv.org/abs/2503.11074v1",
    "arxiv_id": "2503.11074v1",
    "authors": [
      "Xueyang Zhou",
      "Guiyao Tie",
      "Guowen Zhang",
      "Weidong Wang",
      "Zhigang Zuo",
      "Di Wu",
      "Duanfeng Chu",
      "Pan Zhou",
      "Lichao Sun",
      "Neil Zhenqiang Gong"
    ],
    "published": "2025-03-14T04:34:31+00:00",
    "summary": "The rise of Large Reasoning Models (LRMs) signifies a paradigm shift toward advanced computational reasoning. Yet, this progress disrupts traditional agent frameworks, traditionally anchored by execution-oriented Large Language Models (LLMs). To explore this transformation, we propose the LaRMA framework, encompassing nine tasks across Tool Usage, Plan Design, and Problem Solving, assessed with three top LLMs (e.g., Claude3.5-sonnet) and five leading LRMs (e.g., DeepSeek-R1). Our findings address four research questions: LRMs surpass LLMs in reasoning-intensive tasks like Plan Design, leveraging iterative reflection for superior outcomes; LLMs excel in execution-driven tasks such as Tool Usage, prioritizing efficiency; hybrid LLM-LRM configurations, pairing LLMs as actors with LRMs as reflectors, optimize agent performance by blending execution speed with reasoning depth; and LRMs' enhanced reasoning incurs higher computational costs, prolonged processing, and behavioral challenges, including overthinking and fact-ignoring tendencies. This study fosters deeper inquiry into LRMs' balance of deep thinking and overthinking, laying a critical foundation for future agent design advancements."
  },
  {
    "title": "Approximate Reachable Sets using Singularly Perturbed Differential Games, with Application to Biological System Models",
    "url": "http://arxiv.org/abs/2503.11021v1",
    "arxiv_id": "2503.11021v1",
    "authors": [
      "Dylan Hirsch",
      "Sylvia Herbert"
    ],
    "published": "2025-03-14T02:37:32+00:00",
    "summary": "Hamilton-Jacobi Reachability (HJR) is an exciting framework used for control of safety-critical systems with nonlinear and possibly uncertain dynamics. However, HJR suffers from the curse of dimensionality, with computation times growing exponentially in the dimension of the system state. Many autonomous and controlled systems involve dynamics that evolve on multiple timescales, and for these systems, singular perturbation methods can be used for model reduction. However, such methods are more challenging to apply in HJR due to the presence of an underlying differential game. In this work, we leverage prior work on singularly perturbed differential games to identify a class of systems which can be readily reduced, and we relate these results to the quantities of interest in HJR. We demonstrate the utility of our results on two examples involving biological systems, where dynamics fitting the identified class are frequently encountered."
  },
  {
    "title": "Comparative Analysis of Advanced AI-based Object Detection Models for Pavement Marking Quality Assessment during Daytime",
    "url": "http://arxiv.org/abs/2503.11008v1",
    "arxiv_id": "2503.11008v1",
    "authors": [
      "Gian Antariksa",
      "Rohir Chakraborty",
      "Shriyank Somvanshi",
      "Subasish Das",
      "Mohammad Jalayer",
      "Deep Rameshkumar Patel",
      "David Mills"
    ],
    "published": "2025-03-14T02:06:46+00:00",
    "summary": "Visual object detection utilizing deep learning plays a vital role in computer vision and has extensive applications in transportation engineering. This paper focuses on detecting pavement marking quality during daytime using the You Only Look Once (YOLO) model, leveraging its advanced architectural features to enhance road safety through precise and real-time assessments. Utilizing image data from New Jersey, this study employed three YOLOv8 variants: YOLOv8m, YOLOv8n, and YOLOv8x. The models were evaluated based on their prediction accuracy for classifying pavement markings into good, moderate, and poor visibility categories. The results demonstrated that YOLOv8n provides the best balance between accuracy and computational efficiency, achieving the highest mean Average Precision (mAP) for objects with good visibility and demonstrating robust performance across various Intersections over Union (IoU) thresholds. This research enhances transportation safety by offering an automated and accurate method for evaluating the quality of pavement markings."
  },
  {
    "title": "Comparative Analysis of Advanced AI-based Object Detection Models for Pavement Marking Quality Assessment during Daytime",
    "url": "http://arxiv.org/abs/2503.11008v2",
    "arxiv_id": "2503.11008v2",
    "authors": [
      "Gian Antariksa",
      "Rohit Chakraborty",
      "Shriyank Somvanshi",
      "Subasish Das",
      "Mohammad Jalayer",
      "Deep Rameshkumar Patel",
      "David Mills"
    ],
    "published": "2025-03-14T02:06:46+00:00",
    "summary": "Visual object detection utilizing deep learning plays a vital role in computer vision and has extensive applications in transportation engineering. This paper focuses on detecting pavement marking quality during daytime using the You Only Look Once (YOLO) model, leveraging its advanced architectural features to enhance road safety through precise and real-time assessments. Utilizing image data from New Jersey, this study employed three YOLOv8 variants: YOLOv8m, YOLOv8n, and YOLOv8x. The models were evaluated based on their prediction accuracy for classifying pavement markings into good, moderate, and poor visibility categories. The results demonstrated that YOLOv8n provides the best balance between accuracy and computational efficiency, achieving the highest mean Average Precision (mAP) for objects with good visibility and demonstrating robust performance across various Intersections over Union (IoU) thresholds. This research enhances transportation safety by offering an automated and accurate method for evaluating the quality of pavement markings."
  },
  {
    "title": "TxAgent: An AI Agent for Therapeutic Reasoning Across a Universe of Tools",
    "url": "http://arxiv.org/abs/2503.10970v1",
    "arxiv_id": "2503.10970v1",
    "authors": [
      "Shanghua Gao",
      "Richard Zhu",
      "Zhenglun Kong",
      "Ayush Noori",
      "Xiaorui Su",
      "Curtis Ginder",
      "Theodoros Tsiligkaridis",
      "Marinka Zitnik"
    ],
    "published": "2025-03-14T00:28:15+00:00",
    "summary": "Precision therapeutics require multimodal adaptive models that generate personalized treatment recommendations. We introduce TxAgent, an AI agent that leverages multi-step reasoning and real-time biomedical knowledge retrieval across a toolbox of 211 tools to analyze drug interactions, contraindications, and patient-specific treatment strategies. TxAgent evaluates how drugs interact at molecular, pharmacokinetic, and clinical levels, identifies contraindications based on patient comorbidities and concurrent medications, and tailors treatment strategies to individual patient characteristics. It retrieves and synthesizes evidence from multiple biomedical sources, assesses interactions between drugs and patient conditions, and refines treatment recommendations through iterative reasoning. It selects tools based on task objectives and executes structured function calls to solve therapeutic tasks that require clinical reasoning and cross-source validation. The ToolUniverse consolidates 211 tools from trusted sources, including all US FDA-approved drugs since 1939 and validated clinical insights from Open Targets. TxAgent outperforms leading LLMs, tool-use models, and reasoning agents across five new benchmarks: DrugPC, BrandPC, GenericPC, TreatmentPC, and DescriptionPC, covering 3,168 drug reasoning tasks and 456 personalized treatment scenarios. It achieves 92.1% accuracy in open-ended drug reasoning tasks, surpassing GPT-4o and outperforming DeepSeek-R1 (671B) in structured multi-step reasoning. TxAgent generalizes across drug name variants and descriptions. By integrating multi-step inference, real-time knowledge grounding, and tool-assisted decision-making, TxAgent ensures that treatment recommendations align with established clinical guidelines and real-world evidence, reducing the risk of adverse events and improving therapeutic decision-making."
  },
  {
    "title": "Safe Control of Second-Order Systems with Linear Constraints",
    "url": "http://arxiv.org/abs/2503.10953v1",
    "arxiv_id": "2503.10953v1",
    "authors": [
      "Mohammed Alyaseen",
      "Nikolay Atanasov",
      "Jorge Cortes"
    ],
    "published": "2025-03-13T23:39:45+00:00",
    "summary": "Control barrier functions (CBFs) offer a powerful tool for enforcing safety specifications in control synthesis. This paper deals with the problem of constructing valid CBFs. Given a second-order system and any desired safety set with linear boundaries in the position space, we construct a provably control-invariant subset of this desired safety set. The constructed subset does not sacrifice any positions allowed by the desired safety set, which can be nonconvex. We show how our construction can also meet safety specification on the velocity. We then demonstrate that if the system satisfies standard Euler-Lagrange systems properties then our construction can also handle constraints on the allowable control inputs. We finally show the efficacy of the proposed method in a numerical example of keeping a 2D robot arm safe from collision."
  },
  {
    "title": "Safe Continual Domain Adaptation after Sim2Real Transfer of Reinforcement Learning Policies in Robotics",
    "url": "http://arxiv.org/abs/2503.10949v1",
    "arxiv_id": "2503.10949v1",
    "authors": [
      "Josip Josifovski",
      "Shangding Gu",
      "Mohammadhossein Malmir",
      "Haoliang Huang",
      "Sayantan Auddy",
      "Nicol\u00e1s Navarro-Guerrero",
      "Costas Spanos",
      "Alois Knoll"
    ],
    "published": "2025-03-13T23:28:11+00:00",
    "summary": "Domain randomization has emerged as a fundamental technique in reinforcement learning (RL) to facilitate the transfer of policies from simulation to real-world robotic applications. Many existing domain randomization approaches have been proposed to improve robustness and sim2real transfer. These approaches rely on wide randomization ranges to compensate for the unknown actual system parameters, leading to robust but inefficient real-world policies. In addition, the policies pretrained in the domain-randomized simulation are fixed after deployment due to the inherent instability of the optimization processes based on RL and the necessity of sampling exploitative but potentially unsafe actions on the real system. This limits the adaptability of the deployed policy to the inevitably changing system parameters or environment dynamics over time. We leverage safe RL and continual learning under domain-randomized simulation to address these limitations and enable safe deployment-time policy adaptation in real-world robot control. The experiments show that our method enables the policy to adapt and fit to the current domain distribution and environment dynamics of the real system while minimizing safety risks and avoiding issues like catastrophic forgetting of the general policy found in randomized simulation during the pretraining phase. Videos and supplementary material are available at https://safe-cda.github.io/."
  },
  {
    "title": "TAIJI: Textual Anchoring for Immunizing Jailbreak Images in Vision Language Models",
    "url": "http://arxiv.org/abs/2503.10872v1",
    "arxiv_id": "2503.10872v1",
    "authors": [
      "Xiangyu Yin",
      "Yi Qi",
      "Jinwei Hu",
      "Zhen Chen",
      "Yi Dong",
      "Xingyu Zhao",
      "Xiaowei Huang",
      "Wenjie Ruan"
    ],
    "published": "2025-03-13T20:39:31+00:00",
    "summary": "Vision Language Models (VLMs) have demonstrated impressive inference capabilities, but remain vulnerable to jailbreak attacks that can induce harmful or unethical responses. Existing defence methods are predominantly white-box approaches that require access to model parameters and extensive modifications, making them costly and impractical for many real-world scenarios. Although some black-box defences have been proposed, they often impose input constraints or require multiple queries, limiting their effectiveness in safety-critical tasks such as autonomous driving. To address these challenges, we propose a novel black-box defence framework called \\textbf{T}extual \\textbf{A}nchoring for \\textbf{I}mmunizing \\textbf{J}ailbreak \\textbf{I}mages (\\textbf{TAIJI}). TAIJI leverages key phrase-based textual anchoring to enhance the model's ability to assess and mitigate the harmful content embedded within both visual and textual prompts. Unlike existing methods, TAIJI operates effectively with a single query during inference, while preserving the VLM's performance on benign tasks. Extensive experiments demonstrate that TAIJI significantly enhances the safety and reliability of VLMs, providing a practical and efficient solution for real-world deployment."
  },
  {
    "title": "Efficient Reachability Analysis for Convolutional Neural Networks Using Hybrid Zonotopes",
    "url": "http://arxiv.org/abs/2503.10840v1",
    "arxiv_id": "2503.10840v1",
    "authors": [
      "Yuhao Zhang",
      "Xiangru Xu"
    ],
    "published": "2025-03-13T19:45:26+00:00",
    "summary": "Feedforward neural networks are widely used in autonomous systems, particularly for control and perception tasks within the system loop. However, their vulnerability to adversarial attacks necessitates formal verification before deployment in safety-critical applications. Existing set propagation-based reachability analysis methods for feedforward neural networks often struggle to achieve both scalability and accuracy. This work presents a novel set-based approach for computing the reachable sets of convolutional neural networks. The proposed method leverages a hybrid zonotope representation and an efficient neural network reduction technique, providing a flexible trade-off between computational complexity and approximation accuracy. Numerical examples are presented to demonstrate the effectiveness of the proposed approach."
  },
  {
    "title": "Thinking Machines: A Survey of LLM based Reasoning Strategies",
    "url": "http://arxiv.org/abs/2503.10814v1",
    "arxiv_id": "2503.10814v1",
    "authors": [
      "Dibyanayan Bandyopadhyay",
      "Soham Bhattacharjee",
      "Asif Ekbal"
    ],
    "published": "2025-03-13T19:03:41+00:00",
    "summary": "Large Language Models (LLMs) are highly proficient in language-based tasks. Their language capabilities have positioned them at the forefront of the future AGI (Artificial General Intelligence) race. However, on closer inspection, Valmeekam et al. (2024); Zecevic et al. (2023); Wu et al. (2024) highlight a significant gap between their language proficiency and reasoning abilities. Reasoning in LLMs and Vision Language Models (VLMs) aims to bridge this gap by enabling these models to think and re-evaluate their actions and responses. Reasoning is an essential capability for complex problem-solving and a necessary step toward establishing trust in Artificial Intelligence (AI). This will make AI suitable for deployment in sensitive domains, such as healthcare, banking, law, defense, security etc. In recent times, with the advent of powerful reasoning models like OpenAI O1 and DeepSeek R1, reasoning endowment has become a critical research topic in LLMs. In this paper, we provide a detailed overview and comparison of existing reasoning techniques and present a systematic survey of reasoning-imbued language models. We also study current challenges and present our findings."
  },
  {
    "title": "HALURust: Exploiting Hallucinations of Large Language Models to Detect Vulnerabilities in Rust",
    "url": "http://arxiv.org/abs/2503.10793v1",
    "arxiv_id": "2503.10793v1",
    "authors": [
      "Yu Luo",
      "Han Zhou",
      "Mengtao Zhang",
      "Dylan De La Rosa",
      "Hafsa Ahmed",
      "Weifeng Xu",
      "Dianxiang Xu"
    ],
    "published": "2025-03-13T18:38:34+00:00",
    "summary": "As an emerging programming language, Rust has rapidly gained popularity and recognition among developers due to its strong emphasis on safety. It employs a unique ownership system and safe concurrency practices to ensure robust safety. Despite these safeguards, security in Rust still presents challenges. Since 2018, 442 Rust-related vulnerabilities have been reported in real-world applications. The limited availability of data has resulted in existing vulnerability detection tools performing poorly in real-world scenarios, often failing to adapt to new and complex vulnerabilities. This paper introduces HALURust, a novel framework that leverages hallucinations of large language models (LLMs) to detect vulnerabilities in real-world Rust scenarios. HALURust leverages LLMs' strength in natural language generation by transforming code into detailed vulnerability analysis reports. The key innovation lies in prompting the LLM to always assume the presence of a vulnerability. If the code sample is vulnerable, the LLM provides an accurate analysis; if not, it generates a hallucinated report. By fine-tuning LLMs on these hallucinations, HALURust can effectively distinguish between vulnerable and non-vulnerable code samples. HALURust was evaluated on a dataset of 81 real-world vulnerabilities, covering 447 functions and 18,691 lines of code across 54 applications. It outperformed existing methods, achieving an F1 score of 77.3%, with over 10% improvement. The hallucinated report-based fine-tuning improved detection by 20\\% compared to traditional code-based fine-tuning. Additionally, HALURust effectively adapted to unseen vulnerabilities and other programming languages, demonstrating strong generalization capabilities."
  },
  {
    "title": "HeightFormer: Learning Height Prediction in Voxel Features for Roadside Vision Centric 3D Object Detection via Transformer",
    "url": "http://arxiv.org/abs/2503.10777v1",
    "arxiv_id": "2503.10777v1",
    "authors": [
      "Zhang Zhang",
      "Chao Sun",
      "Chao Yue",
      "Da Wen",
      "Yujie Chen",
      "Tianze Wang",
      "Jianghao Leng"
    ],
    "published": "2025-03-13T18:17:19+00:00",
    "summary": "Roadside vision centric 3D object detection has received increasing attention in recent years. It expands the perception range of autonomous vehicles, enhances the road safety. Previous methods focused on predicting per-pixel height rather than depth, making significant gains in roadside visual perception. While it is limited by the perspective property of near-large and far-small on image features, making it difficult for network to understand real dimension of objects in the 3D world. BEV features and voxel features present the real distribution of objects in 3D world compared to the image features. However, BEV features tend to lose details due to the lack of explicit height information, and voxel features are computationally expensive. Inspired by this insight, an efficient framework learning height prediction in voxel features via transformer is proposed, dubbed HeightFormer. It groups the voxel features into local height sequences, and utilize attention mechanism to obtain height distribution prediction. Subsequently, the local height sequences are reassembled to generate accurate 3D features. The proposed method is applied to two large-scale roadside benchmarks, DAIR-V2X-I and Rope3D. Extensive experiments are performed and the HeightFormer outperforms the state-of-the-art methods in roadside vision centric 3D object detection task."
  },
  {
    "title": "A Frustratingly Simple Yet Highly Effective Attack Baseline: Over 90% Success Rate Against the Strong Black-box Models of GPT-4.5/4o/o1",
    "url": "http://arxiv.org/abs/2503.10635v1",
    "arxiv_id": "2503.10635v1",
    "authors": [
      "Zhaoyi Li",
      "Xiaohan Zhao",
      "Dong-Dong Wu",
      "Jiacheng Cui",
      "Zhiqiang Shen"
    ],
    "published": "2025-03-13T17:59:55+00:00",
    "summary": "Despite promising performance on open-source large vision-language models (LVLMs), transfer-based targeted attacks often fail against black-box commercial LVLMs. Analyzing failed adversarial perturbations reveals that the learned perturbations typically originate from a uniform distribution and lack clear semantic details, resulting in unintended responses. This critical absence of semantic information leads commercial LVLMs to either ignore the perturbation entirely or misinterpret its embedded semantics, thereby causing the attack to fail. To overcome these issues, we notice that identifying core semantic objects is a key objective for models trained with various datasets and methodologies. This insight motivates our approach that refines semantic clarity by encoding explicit semantic details within local regions, thus ensuring interoperability and capturing finer-grained features, and by concentrating modifications on semantically rich areas rather than applying them uniformly. To achieve this, we propose a simple yet highly effective solution: at each optimization step, the adversarial image is cropped randomly by a controlled aspect ratio and scale, resized, and then aligned with the target image in the embedding space. Experimental results confirm our hypothesis. Our adversarial examples crafted with local-aggregated perturbations focused on crucial regions exhibit surprisingly good transferability to commercial LVLMs, including GPT-4.5, GPT-4o, Gemini-2.0-flash, Claude-3.5-sonnet, Claude-3.7-sonnet, and even reasoning models like o1, Claude-3.7-thinking and Gemini-2.0-flash-thinking. Our approach achieves success rates exceeding 90% on GPT-4.5, 4o, and o1, significantly outperforming all prior state-of-the-art attack methods. Our optimized adversarial examples under different configurations and training code are available at https://github.com/VILA-Lab/M-Attack."
  },
  {
    "title": "DriveLMM-o1: A Step-by-Step Reasoning Dataset and Large Multimodal Model for Driving Scenario Understanding",
    "url": "http://arxiv.org/abs/2503.10621v1",
    "arxiv_id": "2503.10621v1",
    "authors": [
      "Ayesha Ishaq",
      "Jean Lahoud",
      "Ketan More",
      "Omkar Thawakar",
      "Ritesh Thawkar",
      "Dinura Dissanayake",
      "Noor Ahsan",
      "Yuhao Li",
      "Fahad Shahbaz Khan",
      "Hisham Cholakkal",
      "Ivan Laptev",
      "Rao Muhammad Anwer",
      "Salman Khan"
    ],
    "published": "2025-03-13T17:59:01+00:00",
    "summary": "While large multimodal models (LMMs) have demonstrated strong performance across various Visual Question Answering (VQA) tasks, certain challenges require complex multi-step reasoning to reach accurate answers. One particularly challenging task is autonomous driving, which demands thorough cognitive processing before decisions can be made. In this domain, a sequential and interpretive understanding of visual cues is essential for effective perception, prediction, and planning. Nevertheless, common VQA benchmarks often focus on the accuracy of the final answer while overlooking the reasoning process that enables the generation of accurate responses. Moreover, existing methods lack a comprehensive framework for evaluating step-by-step reasoning in realistic driving scenarios. To address this gap, we propose DriveLMM-o1, a new dataset and benchmark specifically designed to advance step-wise visual reasoning for autonomous driving. Our benchmark features over 18k VQA examples in the training set and more than 4k in the test set, covering diverse questions on perception, prediction, and planning, each enriched with step-by-step reasoning to ensure logical inference in autonomous driving scenarios. We further introduce a large multimodal model that is fine-tuned on our reasoning dataset, demonstrating robust performance in complex driving scenarios. In addition, we benchmark various open-source and closed-source methods on our proposed dataset, systematically comparing their reasoning capabilities for autonomous driving tasks. Our model achieves a +7.49% gain in final answer accuracy, along with a 3.62% improvement in reasoning score over the previous best open-source model. Our framework, dataset, and model are available at https://github.com/ayesha-ishaq/DriveLMM-o1."
  },
  {
    "title": "Siege: Autonomous Multi-Turn Jailbreaking of Large Language Models with Tree Search",
    "url": "http://arxiv.org/abs/2503.10619v1",
    "arxiv_id": "2503.10619v1",
    "authors": [
      "Andy Zhou"
    ],
    "published": "2025-03-13T17:57:32+00:00",
    "summary": "We introduce Siege, a multi-turn adversarial framework that models the gradual erosion of Large Language Model (LLM) safety through a tree search perspective. Unlike single-turn jailbreaks that rely on one meticulously engineered prompt, Siege expands the conversation at each turn in a breadth-first fashion, branching out multiple adversarial prompts that exploit partial compliance from previous responses. By tracking these incremental policy leaks and re-injecting them into subsequent queries, Siege reveals how minor concessions can accumulate into fully disallowed outputs. Evaluations on the JailbreakBench dataset show that Siege achieves a 100% success rate on GPT-3.5-turbo and 97% on GPT-4 in a single multi-turn run, using fewer queries than baselines such as Crescendo or GOAT. This tree search methodology offers an in-depth view of how model safeguards degrade over successive dialogue turns, underscoring the urgency of robust multi-turn testing procedures for language models."
  },
  {
    "title": "Siege: Autonomous Multi-Turn Jailbreaking of Large Language Models with Tree Search",
    "url": "http://arxiv.org/abs/2503.10619v2",
    "arxiv_id": "2503.10619v2",
    "authors": [
      "Andy Zhou"
    ],
    "published": "2025-03-13T17:57:32+00:00",
    "summary": "We introduce Siege, a multi-turn adversarial framework that models the gradual erosion of Large Language Model (LLM) safety through a tree search perspective. Unlike single-turn jailbreaks that rely on one meticulously engineered prompt, Siege expands the conversation at each turn in a breadth-first fashion, branching out multiple adversarial prompts that exploit partial compliance from previous responses. By tracking these incremental policy leaks and re-injecting them into subsequent queries, Siege reveals how minor concessions can accumulate into fully disallowed outputs. Evaluations on the JailbreakBench dataset show that Siege achieves a 100% success rate on GPT-3.5-turbo and 97% on GPT-4 in a single multi-turn run, using fewer queries than baselines such as Crescendo or GOAT. This tree search methodology offers an in-depth view of how model safeguards degrade over successive dialogue turns, underscoring the urgency of robust multi-turn testing procedures for language models."
  },
  {
    "title": "OCCUQ: Exploring Efficient Uncertainty Quantification for 3D Occupancy Prediction",
    "url": "http://arxiv.org/abs/2503.10605v1",
    "arxiv_id": "2503.10605v1",
    "authors": [
      "Severin Heidrich",
      "Till Beemelmanns",
      "Alexey Nekrasov",
      "Bastian Leibe",
      "Lutz Eckstein"
    ],
    "published": "2025-03-13T17:50:07+00:00",
    "summary": "Autonomous driving has the potential to significantly enhance productivity and provide numerous societal benefits. Ensuring robustness in these safety-critical systems is essential, particularly when vehicles must navigate adverse weather conditions and sensor corruptions that may not have been encountered during training. Current methods often overlook uncertainties arising from adversarial conditions or distributional shifts, limiting their real-world applicability. We propose an efficient adaptation of an uncertainty estimation technique for 3D occupancy prediction. Our method dynamically calibrates model confidence using epistemic uncertainty estimates. Our evaluation under various camera corruption scenarios, such as fog or missing cameras, demonstrates that our approach effectively quantifies epistemic uncertainty by assigning higher uncertainty values to unseen data. We introduce region-specific corruptions to simulate defects affecting only a single camera and validate our findings through both scene-level and region-level assessments. Our results show superior performance in Out-of-Distribution (OoD) detection and confidence calibration compared to common baselines such as Deep Ensembles and MC-Dropout. Our approach consistently demonstrates reliable uncertainty measures, indicating its potential for enhancing the robustness of autonomous driving systems in real-world scenarios. Code and dataset are available at https://github.com/ika-rwth-aachen/OCCUQ ."
  },
  {
    "title": "Unveiling the Mathematical Reasoning in DeepSeek Models: A Comparative Study of Large Language Models",
    "url": "http://arxiv.org/abs/2503.10573v1",
    "arxiv_id": "2503.10573v1",
    "authors": [
      "Afrar Jahin",
      "Arif Hassan Zidan",
      "Yu Bao",
      "Shizhe Liang",
      "Tianming Liu",
      "Wei Zhang"
    ],
    "published": "2025-03-13T17:23:45+00:00",
    "summary": "With the rapid evolution of Artificial Intelligence (AI), Large Language Models (LLMs) have reshaped the frontiers of various fields, spanning healthcare, public health, engineering, science, agriculture, education, arts, humanities, and mathematical reasoning. Among these advancements, DeepSeek models have emerged as noteworthy contenders, demonstrating promising capabilities that set them apart from their peers. While previous studies have conducted comparative analyses of LLMs, few have delivered a comprehensive evaluation of mathematical reasoning across a broad spectrum of LLMs. In this work, we aim to bridge this gap by conducting an in-depth comparative study, focusing on the strengths and limitations of DeepSeek models in relation to their leading counterparts. In particular, our study systematically evaluates the mathematical reasoning performance of two DeepSeek models alongside five prominent LLMs across three independent benchmark datasets. The findings reveal several key insights: 1). DeepSeek-R1 consistently achieved the highest accuracy on two of the three datasets, demonstrating strong mathematical reasoning capabilities. 2). The distilled variant of LLMs significantly underperformed compared to its peers, highlighting potential drawbacks in using distillation techniques. 3). In terms of response time, Gemini 2.0 Flash demonstrated the fastest processing speed, outperforming other models in efficiency, which is a crucial factor for real-time applications. Beyond these quantitative assessments, we delve into how architecture, training, and optimization impact LLMs' mathematical reasoning. Moreover, our study goes beyond mere performance comparison by identifying key areas for future advancements in LLM-driven mathematical reasoning. This research enhances our understanding of LLMs' mathematical reasoning and lays the groundwork for future advancements"
  },
  {
    "title": "ASIDE: Architectural Separation of Instructions and Data in Language Models",
    "url": "http://arxiv.org/abs/2503.10566v1",
    "arxiv_id": "2503.10566v1",
    "authors": [
      "Egor Zverev",
      "Evgenii Kortukov",
      "Alexander Panfilov",
      "Soroush Tabesh",
      "Alexandra Volkova",
      "Sebastian Lapuschkin",
      "Wojciech Samek",
      "Christoph H. Lampert"
    ],
    "published": "2025-03-13T17:17:17+00:00",
    "summary": "Despite their remarkable performance, large language models lack elementary safety features, and this makes them susceptible to numerous malicious attacks. In particular, previous work has identified the absence of an intrinsic separation between instructions and data as a root cause for the success of prompt injection attacks. In this work, we propose an architectural change, ASIDE, that allows the model to clearly separate between instructions and data by using separate embeddings for them. Instead of training the embeddings from scratch, we propose a method to convert an existing model to ASIDE form by using two copies of the original model's embeddings layer, and applying an orthogonal rotation to one of them. We demonstrate the effectiveness of our method by showing (1) highly increased instruction-data separation scores without a loss in model capabilities and (2) competitive results on prompt injection benchmarks, even without dedicated safety training. Additionally, we study the working mechanism behind our method through an analysis of model representations."
  },
  {
    "title": "Towards Safe Path Tracking Using the Simplex Architecture",
    "url": "http://arxiv.org/abs/2503.10559v1",
    "arxiv_id": "2503.10559v1",
    "authors": [
      "Georg J\u00e4ger",
      "Nils-Jonathan Friedrich",
      "Hauke Petersen",
      "Benjamin Noack"
    ],
    "published": "2025-03-13T17:11:55+00:00",
    "summary": "Robot navigation in complex environments necessitates controllers that are adaptive and safe. Traditional controllers like Regulated Pure Pursuit, Dynamic Window Approach, and Model-Predictive Path Integral, while reliable, struggle to adapt to dynamic conditions. Reinforcement Learning offers adaptability but lacks formal safety guarantees. To address this, we propose a path tracking controller leveraging the Simplex architecture. It combines a Reinforcement Learning controller for adaptiveness and performance with a high-assurance controller providing safety and stability. Our contribution is twofold. We firstly discuss general stability and safety considerations for designing controllers using the Simplex architecture. Secondly, we present a Simplex-based path tracking controller. Our simulation results, supported by preliminary in-field tests, demonstrate the controller's effectiveness in maintaining safety while achieving comparable performance to state-of-the-art methods."
  },
  {
    "title": "Safety Filter for Limiting the Current of Grid-Forming Matrix Modular Multilevel Converters",
    "url": "http://arxiv.org/abs/2503.10498v1",
    "arxiv_id": "2503.10498v1",
    "authors": [
      "Michael Schneeberger",
      "Silvia Mastellone",
      "Florian D\u00f6rfler"
    ],
    "published": "2025-03-13T16:01:11+00:00",
    "summary": "Grid-forming (GFM) converters face significant challenges in limiting current during transient grid events while preserving their grid-forming behavior. This paper offers an elegant solution to the problem with a priori guarantees, presenting a safety filter approach based on Control Barrier Functions (CBFs) to enforce current constraints with minimal deviation from the nominal voltage reference. The safety filter is implemented as a Quadratic Program, enabling real-time computation of safe voltage adjustments that ensure smooth transitions and maintain the GFM behavior during nominal operation. To provide formal safety certificate, the CBF is synthesized offline using a Sum-of-Squares optimization framework, ensuring that the converter remains within its allowable operating limits under all conditions. Additionally, a Control Lyapunov Function is incorporated to facilitate a smooth return to the nominal operating region following grid events. The proposed method is modular and can be integrated into many of the GFM control architectures, as demonstrated with two different GFM implementations. High-fidelity simulations conducted with an enhanced matrix modular multilevel converter connected to both high-inertia and low-inertia grid scenarios validate the effectiveness of the safety filter, showing that it successfully limits current during faults, preserves GFM behavior, and ensures a seamless recovery to nominal operation."
  },
  {
    "title": "LLMs in Disease Diagnosis: A Comparative Study of DeepSeek-R1 and O3 Mini Across Chronic Health Conditions",
    "url": "http://arxiv.org/abs/2503.10486v1",
    "arxiv_id": "2503.10486v1",
    "authors": [
      "Gaurav Kumar Gupta",
      "Pranal Pande"
    ],
    "published": "2025-03-13T15:54:26+00:00",
    "summary": "Large Language Models (LLMs) are revolutionizing medical diagnostics by enhancing both disease classification and clinical decision-making. In this study, we evaluate the performance of two LLM- based diagnostic tools, DeepSeek R1 and O3 Mini, using a structured dataset of symptoms and diagnoses. We assessed their predictive accuracy at both the disease and category levels, as well as the reliability of their confidence scores. DeepSeek R1 achieved a disease-level accuracy of 76% and an overall accuracy of 82%, outperforming O3 Mini, which attained 72% and 75% respectively. Notably, DeepSeek R1 demonstrated exceptional performance in Mental Health, Neurological Disorders, and Oncology, where it reached 100% accuracy, while O3 Mini excelled in Autoimmune Disease classification with 100% accuracy. Both models, however, struggled with Respiratory Disease classification, recording accuracies of only 40% for DeepSeek R1 and 20% for O3 Mini. Additionally, the analysis of confidence scores revealed that DeepSeek R1 provided high-confidence predictions in 92% of cases, compared to 68% for O3 Mini. Ethical considerations regarding bias, model interpretability, and data privacy are also discussed to ensure the responsible integration of LLMs into clinical practice. Overall, our findings offer valuable insights into the strengths and limitations of LLM-based diagnostic systems and provide a roadmap for future enhancements in AI-driven healthcare."
  },
  {
    "title": "Stratified Topological Autonomy for Long-Range Coordination (STALC)",
    "url": "http://arxiv.org/abs/2503.10475v1",
    "arxiv_id": "2503.10475v1",
    "authors": [
      "Cora A. Dimmig",
      "Adam Goertz",
      "Adam Polevoy",
      "Mark Gonzales",
      "Kevin C. Wolfe",
      "Bradley Woosley",
      "John Rogers",
      "Joseph Moore"
    ],
    "published": "2025-03-13T15:45:27+00:00",
    "summary": "Achieving unified multi-robot coordination and motion planning in complex environments is a challenging problem. In this paper, we present a hierarchical approach to long-range coordination, which we call Stratified Topological Autonomy for Long-Range Coordination (STALC). In particular, we look at the problem of minimizing visibility to observers and maximizing safety with a multi-robot team navigating through a hazardous environment. At its core, our approach relies on the notion of a dynamic topological graph, where the edge weights vary dynamically based on the locations of the robots in the graph. To create this dynamic topological graph, we evaluate the visibility of the robot team from a discrete set of observer locations (both adversarial and friendly), and construct a topological graph whose edge weights depend on both adversary position and robot team configuration. We then impose temporal constraints on the evolution of those edge weights based on robot team state and use Mixed-Integer Programming (MIP) to generate optimal multirobot plans through the graph. The visibility information also informs the lower layers of the autonomy stack to plan minimal visibility paths through the environment for the team of robots. Our approach presents methods to reduce the computational complexity for a team of robots that interact and coordinate across the team to accomplish a common goal. We demonstrate our approach in simulated and hardware experiments in forested and urban environments."
  },
  {
    "title": "Applying Tabular Deep Learning Models to Estimate Crash Injury Types of Young Motorcyclists",
    "url": "http://arxiv.org/abs/2503.10474v1",
    "arxiv_id": "2503.10474v1",
    "authors": [
      "Shriyank Somvanshi",
      "Anannya Ghosh Tusti",
      "Rohit Chakraborty",
      "Subasish Das"
    ],
    "published": "2025-03-13T15:45:13+00:00",
    "summary": "Young motorcyclists, particularly those aged 15 to 24 years old, face a heightened risk of severe crashes due to factors such as speeding, traffic violations, and helmet usage. This study aims to identify key factors influencing crash severity by analyzing 10,726 young motorcyclist crashes in Texas from 2017 to 2022. Two advanced tabular deep learning models, ARMNet and MambaNet, were employed, using an advanced resampling technique to address class imbalance. The models were trained to classify crashes into three severity levels, Fatal or Severe, Moderate or Minor, and No Injury. ARMNet achieved an accuracy of 87 percent, outperforming 86 percent of Mambanet, with both models excelling in predicting severe and no injury crashes while facing challenges in moderate crash classification. Key findings highlight the significant influence of demographic, environmental, and behavioral factors on crash outcomes. The study underscores the need for targeted interventions, including stricter helmet enforcement and educational programs customized to young motorcyclists. These insights provide valuable guidance for policymakers in developing evidence-based strategies to enhance motorcyclist safety and reduce crash severity."
  },
  {
    "title": "Light-R1: Curriculum SFT, DPO and RL for Long COT from Scratch and Beyond",
    "url": "http://arxiv.org/abs/2503.10460v1",
    "arxiv_id": "2503.10460v1",
    "authors": [
      "Liang Wen",
      "Yunke Cai",
      "Fenrui Xiao",
      "Xin He",
      "Qi An",
      "Zhenyu Duan",
      "Yimin Du",
      "Junchen Liu",
      "Lifu Tang",
      "Xiaowei Lv",
      "Haosheng Zou",
      "Yongchao Deng",
      "Shousheng Jia",
      "Xiangzheng Zhang"
    ],
    "published": "2025-03-13T15:29:22+00:00",
    "summary": "This paper presents our work on the Light-R1 series, with models, data, and code all released.   We first focus on training long COT models from scratch, specifically starting from models initially lacking long COT capabilities. Using a curriculum training recipe consisting of two-stage SFT and semi-on-policy DPO, we train our model Light-R1-32B from Qwen2.5-32B-Instruct, resulting in superior math performance compared to DeepSeek-R1-Distill-Qwen-32B. Despite being trained exclusively on math data, Light-R1-32B shows strong generalization across other domains. In the subsequent phase of this work, we highlight the significant benefit of the 3k dataset constructed for the second SFT stage on enhancing other models. By fine-tuning DeepSeek-R1-Distilled models using this dataset, we obtain new SOTA models in 7B and 14B, while the 32B model, Light-R1-32B-DS performed comparably to QwQ-32B and DeepSeek-R1.   Furthermore, we extend our work by applying reinforcement learning, specifically GRPO, on long-COT models to further improve reasoning performance. We successfully train our final Light-R1-14B-DS with RL, achieving SOTA performance among 14B parameter models in math. With AIME24 & 25 scores of 74.0 and 60.2 respectively, Light-R1-14B-DS surpasses even many 32B models and DeepSeek-R1-Distill-Llama-70B. Its RL training also exhibits well expected behavior, showing simultaneous increase in response length and reward score.   The Light-R1 series of work validates training long-COT models from scratch, showcases the art in SFT data and releases SOTA models from RL."
  },
  {
    "title": "Social Media Harm Abatement: Mechanisms for Transparent Public Health Assessment",
    "url": "http://arxiv.org/abs/2503.10458v1",
    "arxiv_id": "2503.10458v1",
    "authors": [
      "Nathaniel Lubin",
      "Yuning Liu",
      "Amanda Yarnell",
      "S. Bryn Austin",
      "Zachary J. Ward",
      "Ravi Iyer",
      "Jonathan Stray",
      "Matthew Lawrence",
      "Alissa Cooper",
      "Peter Chapman"
    ],
    "published": "2025-03-13T15:26:46+00:00",
    "summary": "Social media platforms have been accused of causing a range of harms, resulting in dozens of lawsuits across jurisdictions. These lawsuits are situated within the context of a long history of American product safety litigation, suggesting opportunities for remediation outside of financial compensation. Anticipating that at least some of these cases may be successful and/or lead to settlements, this article outlines an implementable mechanism for an abatement and/or settlement plan capable of mitigating abuse. The paper describes the requirements of such a mechanism, implications for privacy and oversight, and tradeoffs that such a procedure would entail. The mechanism is framed to operate at the intersection of legal procedure, standards for transparent public health assessment, and the practical requirements of modern technology products."
  },
  {
    "title": "Finetuning Generative Trajectory Model with Reinforcement Learning from Human Feedback",
    "url": "http://arxiv.org/abs/2503.10434v1",
    "arxiv_id": "2503.10434v1",
    "authors": [
      "Derun Li",
      "Jianwei Ren",
      "Yue Wang",
      "Xin Wen",
      "Pengxiang Li",
      "Leimeng Xu",
      "Kun Zhan",
      "Zhongpu Xia",
      "Peng Jia",
      "Xianpeng Lang",
      "Ningyi Xu",
      "Hang Zhao"
    ],
    "published": "2025-03-13T14:56:17+00:00",
    "summary": "Generating human-like and adaptive trajectories is essential for autonomous driving in dynamic environments. While generative models have shown promise in synthesizing feasible trajectories, they often fail to capture the nuanced variability of human driving styles due to dataset biases and distributional shifts. To address this, we introduce TrajHF, a human feedback-driven finetuning framework for generative trajectory models, designed to align motion planning with diverse driving preferences. TrajHF incorporates multi-conditional denoiser and reinforcement learning with human feedback to refine multi-modal trajectory generation beyond conventional imitation learning. This enables better alignment with human driving preferences while maintaining safety and feasibility constraints. TrajHF achieves PDMS of 93.95 on NavSim benchmark, significantly exceeding other methods. TrajHF sets a new paradigm for personalized and adaptable trajectory generation in autonomous driving."
  },
  {
    "title": "Safe exploration in reproducing kernel Hilbert spaces",
    "url": "http://arxiv.org/abs/2503.10352v1",
    "arxiv_id": "2503.10352v1",
    "authors": [
      "Abdullah Tokmak",
      "Kiran G. Krishnan",
      "Thomas B. Sch\u00f6n",
      "Dominik Baumann"
    ],
    "published": "2025-03-13T13:28:54+00:00",
    "summary": "Popular safe Bayesian optimization (BO) algorithms learn control policies for safety-critical systems in unknown environments. However, most algorithms make a smoothness assumption, which is encoded by a known bounded norm in a reproducing kernel Hilbert space (RKHS). The RKHS is a potentially infinite-dimensional space, and it remains unclear how to reliably obtain the RKHS norm of an unknown function. In this work, we propose a safe BO algorithm capable of estimating the RKHS norm from data. We provide statistical guarantees on the RKHS norm estimation, integrate the estimated RKHS norm into existing confidence intervals and show that we retain theoretical guarantees, and prove safety of the resulting safe BO algorithm. We apply our algorithm to safely optimize reinforcement learning policies on physics simulators and on a real inverted pendulum, demonstrating improved performance, safety, and scalability compared to the state-of-the-art."
  },
  {
    "title": "HALO: Fault-Tolerant Safety Architecture For High-Speed Autonomous Racing",
    "url": "http://arxiv.org/abs/2503.10341v1",
    "arxiv_id": "2503.10341v1",
    "authors": [
      "Aron Harder",
      "Amar Kulkarni",
      "Madhur Behl"
    ],
    "published": "2025-03-13T13:19:51+00:00",
    "summary": "The field of high-speed autonomous racing has seen significant advances in recent years, with the rise of competitions such as RoboRace and the Indy Autonomous Challenge providing a platform for researchers to develop software stacks for autonomous race vehicles capable of reaching speeds in excess of 170 mph. Ensuring the safety of these vehicles requires the software to continuously monitor for different faults and erroneous operating conditions during high-speed operation, with the goal of mitigating any unreasonable risks posed by malfunctions in sub-systems and components. This paper presents a comprehensive overview of the HALO safety architecture, which has been implemented on a full-scale autonomous racing vehicle as part of the Indy Autonomous Challenge. The paper begins with a failure mode and criticality analysis of the perception, planning, control, and communication modules of the software stack. Specifically, we examine three different types of faults - node health, data health, and behavioral-safety faults. To mitigate these faults, the paper then outlines HALO safety archetypes and runtime monitoring methods. Finally, the paper demonstrates the effectiveness of the HALO safety architecture for each of the faults, through real-world data gathered from autonomous racing vehicle trials during multi-agent scenarios."
  },
  {
    "title": "Enhance Exploration in Safe Reinforcement Learning with Contrastive Representation Learning",
    "url": "http://arxiv.org/abs/2503.10318v1",
    "arxiv_id": "2503.10318v1",
    "authors": [
      "Duc Kien Doan",
      "Bang Giang Le",
      "Viet Cuong Ta"
    ],
    "published": "2025-03-13T12:53:42+00:00",
    "summary": "In safe reinforcement learning, agent needs to balance between exploration actions and safety constraints. Following this paradigm, domain transfer approaches learn a prior Q-function from the related environments to prevent unsafe actions. However, because of the large number of false positives, some safe actions are never executed, leading to inadequate exploration in sparse-reward environments. In this work, we aim to learn an efficient state representation to balance the exploration and safety-prefer action in a sparse-reward environment. Firstly, the image input is mapped to latent representation by an auto-encoder. A further contrastive learning objective is employed to distinguish safe and unsafe states. In the learning phase, the latent distance is used to construct an additional safety check, which allows the agent to bias the exploration if it visits an unsafe state. To verify the effectiveness of our method, the experiment is carried out in three navigation-based MiniGrid environments. The result highlights that our method can explore the environment better while maintaining a good balance between safety and efficiency."
  },
  {
    "title": "CODEI: Resource-Efficient Task-Driven Co-Design of Perception and Decision Making for Mobile Robots Applied to Autonomous Vehicles",
    "url": "http://arxiv.org/abs/2503.10296v1",
    "arxiv_id": "2503.10296v1",
    "authors": [
      "Dejan Milojevic",
      "Gioele Zardini",
      "Miriam Elser",
      "Andrea Censi",
      "Emilio Frazzoli"
    ],
    "published": "2025-03-13T12:12:44+00:00",
    "summary": "This paper discusses the integration challenges and strategies for designing mobile robots, by focusing on the task-driven, optimal selection of hardware and software to balance safety, efficiency, and minimal usage of resources such as costs, energy, computational requirements, and weight. We emphasize the interplay between perception and motion planning in decision-making by introducing the concept of occupancy queries to quantify the perception requirements for sampling-based motion planners. Sensor and algorithm performance are evaluated using False Negative Rates (FPR) and False Positive Rates (FPR) across various factors such as geometric relationships, object properties, sensor resolution, and environmental conditions. By integrating perception requirements with perception performance, an Integer Linear Programming (ILP) approach is proposed for efficient sensor and algorithm selection and placement. This forms the basis for a co-design optimization that includes the robot body, motion planner, perception pipeline, and computing unit. We refer to this framework for solving the co-design problem of mobile robots as CODEI, short for Co-design of Embodied Intelligence. A case study on developing an Autonomous Vehicle (AV) for urban scenarios provides actionable information for designers, and shows that complex tasks escalate resource demands, with task performance affecting choices of the autonomy stack. The study demonstrates that resource prioritization influences sensor choice: cameras are preferred for cost-effective and lightweight designs, while lidar sensors are chosen for better energy and computational efficiency."
  },
  {
    "title": "Thermal Management of Lithium-Ion Batteries: A Comparative Study of Phase Change Materials and Air-Cooling Systems Equipped with Fins",
    "url": "http://arxiv.org/abs/2503.10244v1",
    "arxiv_id": "2503.10244v1",
    "authors": [
      "Masoumeh Karimi Kisomi"
    ],
    "published": "2025-03-13T10:45:16+00:00",
    "summary": "Lithium-ion batteries are extensively utilized as the primary power source for electric vehicles due to their high energy density, environmental friendliness and lightweight nature. However, their performance and safety are highly dependent on operating temperature. Therefore, a battery thermal management system (BTMS) is essential to ensure the reliable operation and safety of electric vehicles. This study presents a battery thermal management system incorporating phase change material (PCM) and air cooling in a cylindrical lithium-ion cell with fins to enhance heat dissipation. The effects of each system on maximum and minimum temperature, and temperature uniformity along the battery cell are analyzed. Additionally, the impact of fins in both systems is evaluated against a finless cell. A numerical analysis utilizing ANSYS software and the finite volume method (FVM) is performed to evaluate the cooling performance of the systems. The results show that PCM reduces both the maximum and minimum temperatures compared to the air cooling system due to the phase change mechanism. In the finless battery case, the maximum temperature decreases from 316 K to 304 K when using PCM instead of the air cooling system. Also, in the same fin-based battery, the minimum temperature decreases from 307 K to 302 K by using PCM instead of the air cooling system, leading to improved temperature stability. The results indicate that, in general, the fins help reduce the maximum cell temperature when compared to the case without fins in both cases. Using rectangular fins reduces the maximum temperature by approximately 3% compared to a finless battery in the air cooling system. Additionally, the presence of fins reduces the temperature difference along the battery, ensuring a more uniform temperature distribution, such that, in the PCM system with rectangular fins, the temperature difference remains below 1 K."
  },
  {
    "title": "MinorBench: A hand-built benchmark for content-based risks for children",
    "url": "http://arxiv.org/abs/2503.10242v1",
    "arxiv_id": "2503.10242v1",
    "authors": [
      "Shaun Khoo",
      "Gabriel Chua",
      "Rachel Shong"
    ],
    "published": "2025-03-13T10:34:43+00:00",
    "summary": "Large Language Models (LLMs) are rapidly entering children's lives - through parent-driven adoption, schools, and peer networks - yet current AI ethics and safety research do not adequately address content-related risks specific to minors. In this paper, we highlight these gaps with a real-world case study of an LLM-based chatbot deployed in a middle school setting, revealing how students used and sometimes misused the system. Building on these findings, we propose a new taxonomy of content-based risks for minors and introduce MinorBench, an open-source benchmark designed to evaluate LLMs on their ability to refuse unsafe or inappropriate queries from children. We evaluate six prominent LLMs under different system prompts, demonstrating substantial variability in their child-safety compliance. Our results inform practical steps for more robust, child-focused safety mechanisms and underscore the urgency of tailoring AI systems to safeguard young users."
  },
  {
    "title": "Red Teaming Contemporary AI Models: Insights from Spanish and Basque Perspectives",
    "url": "http://arxiv.org/abs/2503.10192v1",
    "arxiv_id": "2503.10192v1",
    "authors": [
      "Miguel Romero-Arjona",
      "Pablo Valle",
      "Juan C. Alonso",
      "Ana B. S\u00e1nchez",
      "Miriam Ugarte",
      "Antonia Cazalilla",
      "Vicente Cambr\u00f3n",
      "Jos\u00e9 A. Parejo",
      "Aitor Arrieta",
      "Sergio Segura"
    ],
    "published": "2025-03-13T09:27:24+00:00",
    "summary": "The battle for AI leadership is on, with OpenAI in the United States and DeepSeek in China as key contenders. In response to these global trends, the Spanish government has proposed ALIA, a public and transparent AI infrastructure incorporating small language models designed to support Spanish and co-official languages such as Basque. This paper presents the results of Red Teaming sessions, where ten participants applied their expertise and creativity to manually test three of the latest models from these initiatives$\\unicode{x2013}$OpenAI o3-mini, DeepSeek R1, and ALIA Salamandra$\\unicode{x2013}$focusing on biases and safety concerns. The results, based on 670 conversations, revealed vulnerabilities in all the models under test, with biased or unsafe responses ranging from 29.5% in o3-mini to 50.6% in Salamandra. These findings underscore the persistent challenges in developing reliable and trustworthy AI systems, particularly those intended to support Spanish and Basque languages."
  },
  {
    "title": "Safety Control of Impulsive Systems with Control Barrier Functions and Adaptive Gains",
    "url": "http://arxiv.org/abs/2503.10164v1",
    "arxiv_id": "2503.10164v1",
    "authors": [
      "Zihan Liu",
      "Yuan-Hua Ni"
    ],
    "published": "2025-03-13T08:38:47+00:00",
    "summary": "This paper addresses the safety challenges in impulsive systems, where abrupt state jumps introduce significant complexities into system dynamics. A unified framework is proposed by integrating Quadratic Programming (QP), Control Barrier Functions (CBFs), and adaptive gain mechanisms to ensure system safety during impulsive events. The CBFs are constructed to enforce safety constraints by capturing the system's continuous dynamics and the effects of impulsive state transitions. An adaptive gain mechanism dynamically adjusts control inputs based on the magnitudes of the impulses and the system's proximity to safety boundaries, maintaining safety during instantaneous state jumps. A tailored QP formulation incorporates CBFs constraints and adaptive gain adjustments, optimizing control inputs while ensuring compliance with safety-critical requirements. Theoretical analysis establishes the boundedness, continuity, and feasibility of the adaptive gain and the overall framework. The effectiveness of the method is demonstrated through simulations on a robotic manipulator, showcasing its practical applicability to impulsive systems with state jumps."
  },
  {
    "title": "Unlocking Generalization Power in LiDAR Point Cloud Registration",
    "url": "http://arxiv.org/abs/2503.10149v1",
    "arxiv_id": "2503.10149v1",
    "authors": [
      "Zhenxuan Zeng",
      "Qiao Wu",
      "Xiyu Zhang",
      "Lin Yuanbo Wu",
      "Pei An",
      "Jiaqi Yang",
      "Ji Wang",
      "Peng Wang"
    ],
    "published": "2025-03-13T08:20:59+00:00",
    "summary": "In real-world environments, a LiDAR point cloud registration method with robust generalization capabilities (across varying distances and datasets) is crucial for ensuring safety in autonomous driving and other LiDAR-based applications. However, current methods fall short in achieving this level of generalization. To address these limitations, we propose UGP, a pruned framework designed to enhance generalization power for LiDAR point cloud registration. The core insight in UGP is the elimination of cross-attention mechanisms to improve generalization, allowing the network to concentrate on intra-frame feature extraction. Additionally, we introduce a progressive self-attention module to reduce ambiguity in large-scale scenes and integrate Bird's Eye View (BEV) features to incorporate semantic information about scene elements. Together, these enhancements significantly boost the network's generalization performance. We validated our approach through various generalization experiments in multiple outdoor scenes. In cross-distance generalization experiments on KITTI and nuScenes, UGP achieved state-of-the-art mean Registration Recall rates of 94.5% and 91.4%, respectively. In cross-dataset generalization from nuScenes to KITTI, UGP achieved a state-of-the-art mean Registration Recall of 90.9%. Code will be available at https://github.com/peakpang/UGP."
  },
  {
    "title": "Mapless Collision-Free Flight via MPC using Dual KD-Trees in Cluttered Environments",
    "url": "http://arxiv.org/abs/2503.10141v1",
    "arxiv_id": "2503.10141v1",
    "authors": [
      "Linzuo Zhang",
      "Yu Hu",
      "Yang Deng",
      "Feng Yu",
      "Danping Zou"
    ],
    "published": "2025-03-13T08:00:58+00:00",
    "summary": "Collision-free flight in cluttered environments is a critical capability for autonomous quadrotors. Traditional methods often rely on detailed 3D map construction, trajectory generation, and tracking. However, this cascade pipeline can introduce accumulated errors and computational delays, limiting flight agility and safety. In this paper, we propose a novel method for enabling collision-free flight in cluttered environments without explicitly constructing 3D maps or generating and tracking collision-free trajectories. Instead, we leverage Model Predictive Control (MPC) to directly produce safe actions from sparse waypoints and point clouds from a depth camera. These sparse waypoints are dynamically adjusted online based on nearby obstacles detected from point clouds. To achieve this, we introduce a dual KD-Tree mechanism: the Obstacle KD-Tree quickly identifies the nearest obstacle for avoidance, while the Edge KD-Tree provides a robust initial guess for the MPC solver, preventing it from getting stuck in local minima during obstacle avoidance. We validate our approach through extensive simulations and real-world experiments. The results show that our approach significantly outperforms the mapping-based methods and is also superior to imitation learning-based methods, demonstrating reliable obstacle avoidance at up to 12 m/s in simulations and 6 m/s in real-world tests. Our method provides a simple and robust alternative to existing methods."
  },
  {
    "title": "IMPACT: Intelligent Motion Planning with Acceptable Contact Trajectories via Vision-Language Models",
    "url": "http://arxiv.org/abs/2503.10110v1",
    "arxiv_id": "2503.10110v1",
    "authors": [
      "Yiyang Ling",
      "Karan Owalekar",
      "Oluwatobiloba Adesanya",
      "Erdem B\u0131y\u0131k",
      "Daniel Seita"
    ],
    "published": "2025-03-13T07:09:00+00:00",
    "summary": "Motion planning involves determining a sequence of robot configurations to reach a desired pose, subject to movement and safety constraints. Traditional motion planning finds collision-free paths, but this is overly restrictive in clutter, where it may not be possible for a robot to accomplish a task without contact. In addition, contacts range from relatively benign (e.g., brushing a soft pillow) to more dangerous (e.g., toppling a glass vase). Due to this diversity, it is difficult to characterize which contacts may be acceptable or unacceptable. In this paper, we propose IMPACT, a novel motion planning framework that uses Vision-Language Models (VLMs) to infer environment semantics, identifying which parts of the environment can best tolerate contact based on object properties and locations. Our approach uses the VLM's outputs to produce a dense 3D \"cost map\" that encodes contact tolerances and seamlessly integrates with standard motion planners. We perform experiments using 20 simulation and 10 real-world scenes and assess using task success rate, object displacements, and feedback from human evaluators. Our results over 3620 simulation and 200 real-world trials suggest that IMPACT enables efficient contact-rich motion planning in cluttered settings while outperforming alternative methods and ablations. Supplementary material is available at https://impact-planning.github.io/."
  },
  {
    "title": "Representation-based Reward Modeling for Efficient Safety Alignment of Large Language Model",
    "url": "http://arxiv.org/abs/2503.10093v1",
    "arxiv_id": "2503.10093v1",
    "authors": [
      "Qiyuan Deng",
      "Xuefeng Bai",
      "Kehai Chen",
      "Yaowei Wang",
      "Liqiang Nie",
      "Min Zhang"
    ],
    "published": "2025-03-13T06:40:34+00:00",
    "summary": "Reinforcement Learning (RL) algorithms for safety alignment of Large Language Models (LLMs), such as Direct Preference Optimization (DPO), encounter the challenge of distribution shift. Current approaches typically address this issue through online sampling from the target policy, which requires significant computational resources. In this paper, we hypothesize that during off-policy training, while the ranking order of output generated by policy changes, their overall distribution remains relatively stable. This stability allows the transformation of the sampling process from the target policy into a re-ranking of preference data. Building on this hypothesis, We propose a new framework that leverages the model's intrinsic safety judgment capability to extract reward signals, which are then used to calculate label confidence for preferences reordering. Extensive experimental results and theoretical analysis demonstrate that the proposed method effectively addresses the distribution shift issue, remarkably enhancing the safety performance while reducing about 300x computational overheads."
  },
  {
    "title": "Advanced Tool Learning and Selection System (ATLASS): A Closed-Loop Framework Using LLM",
    "url": "http://arxiv.org/abs/2503.10071v1",
    "arxiv_id": "2503.10071v1",
    "authors": [
      "Mohd Ariful Haque",
      "Justin Williams",
      "Sunzida Siddique",
      "Md. Hujaifa Islam",
      "Hasmot Ali",
      "Kishor Datta Gupta",
      "Roy George"
    ],
    "published": "2025-03-13T05:39:00+00:00",
    "summary": "The combination of LLM agents with external tools enables models to solve complex tasks beyond their knowledge base. Human-designed tools are inflexible and restricted to solutions within the scope of pre-existing tools created by experts. To address this problem, we propose ATLASS, an advanced tool learning and selection system designed as a closed-loop framework. It enables the LLM to solve problems by dynamically generating external tools on demand. In this framework, agents play a crucial role in orchestrating tool selection, execution, and refinement, ensuring adaptive problem-solving capabilities. The operation of ATLASS follows three phases: The first phase, Understanding Tool Requirements, involves the Agents determining whether tools are required and specifying their functionality; the second phase, Tool Retrieval/Generation, involves the Agents retrieving or generating tools based on their availability; and the third phase, Task Solving, involves combining all the component tools necessary to complete the initial task. The Tool Dataset stores the generated tools, ensuring reusability and minimizing inference cost. Current LLM-based tool generation systems have difficulty creating complex tools that need APIs or external packages. In ATLASS, we solve the problem by automatically setting up the environment, fetching relevant API documentation online, and using a Python interpreter to create a reliable, versatile tool that works in a wider range of situations. OpenAI GPT-4.0 is used as the LLM agent, and safety and ethical concerns are handled through human feedback before executing generated code. By addressing the limitations of predefined toolsets and enhancing adaptability, ATLASS serves as a real-world solution that empowers users with dynamically generated tools for complex problem-solving."
  },
  {
    "title": "Enhancing Multi-Agent Systems via Reinforcement Learning with LLM-based Planner and Graph-based Policy",
    "url": "http://arxiv.org/abs/2503.10049v1",
    "arxiv_id": "2503.10049v1",
    "authors": [
      "Ziqi Jia",
      "Junjie Li",
      "Xiaoyang Qu",
      "Jianzong Wang"
    ],
    "published": "2025-03-13T05:02:49+00:00",
    "summary": "Multi-agent systems (MAS) have shown great potential in executing complex tasks, but coordination and safety remain significant challenges. Multi-Agent Reinforcement Learning (MARL) offers a promising framework for agent collaboration, but it faces difficulties in handling complex tasks and designing reward functions. The introduction of Large Language Models (LLMs) has brought stronger reasoning and cognitive abilities to MAS, but existing LLM-based systems struggle to respond quickly and accurately in dynamic environments. To address these challenges, we propose LLM-based Graph Collaboration MARL (LGC-MARL), a framework that efficiently combines LLMs and MARL. This framework decomposes complex tasks into executable subtasks and achieves efficient collaboration among multiple agents through graph-based coordination. Specifically, LGC-MARL consists of two main components: an LLM planner and a graph-based collaboration meta policy. The LLM planner transforms complex task instructions into a series of executable subtasks, evaluates the rationality of these subtasks using a critic model, and generates an action dependency graph. The graph-based collaboration meta policy facilitates communication and collaboration among agents based on the action dependency graph, and adapts to new task environments through meta-learning. Experimental results on the AI2-THOR simulation platform demonstrate the superior performance and scalability of LGC-MARL in completing various complex tasks."
  },
  {
    "title": "Post-disaster building indoor damage and survivor detection using autonomous path planning and deep learning with unmanned aerial vehicles",
    "url": "http://arxiv.org/abs/2503.10027v1",
    "arxiv_id": "2503.10027v1",
    "authors": [
      "Xiao Pan",
      "Sina Tavasoli",
      "T. Y. Yang",
      "Sina Poorghasem"
    ],
    "published": "2025-03-13T04:13:48+00:00",
    "summary": "Rapid response to natural disasters such as earthquakes is a crucial element in ensuring the safety of civil infrastructures and minimizing casualties. Traditional manual inspection is labour-intensive, time-consuming, and can be dangerous for inspectors and rescue workers. This paper proposed an autonomous inspection approach for structural damage inspection and survivor detection in the post-disaster building indoor scenario, which incorporates an autonomous navigation method, deep learning-based damage and survivor detection method, and a customized low-cost micro aerial vehicle (MAV) with onboard sensors. Experimental studies in a pseudo-post-disaster office building have shown the proposed methodology can achieve high accuracy in structural damage inspection and survivor detection. Overall, the proposed inspection approach shows great potential to improve the efficiency of existing manual post-disaster building inspection."
  },
  {
    "title": "OR-LLM-Agent: Automating Modeling and Solving of Operations Research Optimization Problem with Reasoning Large Language Model",
    "url": "http://arxiv.org/abs/2503.10009v1",
    "arxiv_id": "2503.10009v1",
    "authors": [
      "Bowen Zhang",
      "Pengcheng Luo"
    ],
    "published": "2025-03-13T03:40:50+00:00",
    "summary": "Operations Research (OR) has been widely applied in various fields such as resource allocation, production planning, and supply chain management. However, addressing real-world OR problems requires OR experts to perform mathematical modeling and programmers to develop solution algorithms. This traditional method, heavily reliant on experts, is costly and has long development cycles, severely limiting the widespread adoption of OR techniques. Few have considered using Artificial Intelligence (AI) to replace professionals to achieve fully automated solutions for OR problems. We propose OR-LLM-Agent, the first AI agent that enables end-to-end automation for solving real-world OR problems. OR-LLM-Agent leverages the Chain-of-Thought (CoT) reasoning capabilities of Large Language Models (LLMs) to translate natural language problem descriptions into formal mathematical models and automatically generate Gurobi solver code. In OR-LLM-Agent, OR-CodeAgent is designed to automate code execution and repair within a sandbox environment, facilitating the derivation of the final solution. Due to the lack of dedicated benchmark datasets for evaluating the automated solving of OR problems, we construct a benchmark dataset comprising 83 real-world OR problems described in natural language. We conduct comparative experiments with state-of-the-art (SOTA) reasoning LLMs, including GPT-o3-mini, DeepSeek-R1, and Gemini 2.0 Flash Thinking. The OR-LLM-Agent achieved the highest pass rate of 100% and the highest solution accuracy of 85%, demonstrating the feasibility of automated OR problem-solving. Data and code have been publicly available at https://github.com/bwz96sco/or_llm_agent."
  },
  {
    "title": "ExtremeAIGC: Benchmarking LMM Vulnerability to AI-Generated Extremist Content",
    "url": "http://arxiv.org/abs/2503.09964v1",
    "arxiv_id": "2503.09964v1",
    "authors": [
      "Bhavik Chandna",
      "Mariam Aboujenane",
      "Usman Naseem"
    ],
    "published": "2025-03-13T02:10:29+00:00",
    "summary": "Large Multimodal Models (LMMs) are increasingly vulnerable to AI-generated extremist content, including photorealistic images and text, which can be used to bypass safety mechanisms and generate harmful outputs. However, existing datasets for evaluating LMM robustness offer limited exploration of extremist content, often lacking AI-generated images, diverse image generation models, and comprehensive coverage of historical events, which hinders a complete assessment of model vulnerabilities. To fill this gap, we introduce ExtremeAIGC, a benchmark dataset and evaluation framework designed to assess LMM vulnerabilities against such content. ExtremeAIGC simulates real-world events and malicious use cases by curating diverse text- and image-based examples crafted using state-of-the-art image generation techniques. Our study reveals alarming weaknesses in LMMs, demonstrating that even cutting-edge safety measures fail to prevent the generation of extremist material. We systematically quantify the success rates of various attack strategies, exposing critical gaps in current defenses and emphasizing the need for more robust mitigation strategies."
  },
  {
    "title": "Optimizing Fire Safety: Reducing False Alarms Using Advanced Machine Learning Techniques",
    "url": "http://arxiv.org/abs/2503.09960v1",
    "arxiv_id": "2503.09960v1",
    "authors": [
      "Muhammad Hassan Jamal",
      "Abdulwahab Alazeb",
      "Shahid Allah Bakhsh",
      "Wadii Boulila",
      "Syed Aziz Shah",
      "Aizaz Ahmad Khattak",
      "Muhammad Shahbaz Khan"
    ],
    "published": "2025-03-13T02:07:14+00:00",
    "summary": "Fire safety practices are important to reduce the extent of destruction caused by fire. While smoke alarms help save lives, firefighters struggle with the increasing number of false alarms. This paper presents a precise and efficient Weighted ensemble model for decreasing false alarms. It estimates the density, computes weights according to the high and low-density regions, forwards the high region weights to KNN and low region weights to XGBoost and combines the predictions. The proposed model is effective at reducing response time, increasing fire safety, and minimizing the damage that fires cause. A specifically designed dataset for smoke detection is utilized to test the proposed model. In addition, a variety of ML models, such as Logistic Regression (LR), Decision Tree (DT), Random Forest (RF), Nai:ve Bayes (NB), K-Nearest Neighbour (KNN), Support Vector Machine (SVM), Extreme Gradient Boosting (XGBoost), Adaptive Boosting (ADAB), have also been utilized. To maximize the use of the smoke detection dataset, all the algorithms utilize the SMOTE re-sampling technique. After evaluating the assessment criteria, this paper presents a concise summary of the comprehensive findings obtained by comparing the outcomes of all models."
  },
  {
    "title": "Identification and Classification of Human Performance related Challenges during Remote Driving",
    "url": "http://arxiv.org/abs/2503.09865v1",
    "arxiv_id": "2503.09865v1",
    "authors": [
      "Ole Hans",
      "J\u00fcrgen Adamy"
    ],
    "published": "2025-03-12T21:50:16+00:00",
    "summary": "Remote driving of vehicles is gaining in importance in the transportation sector, especially when Automated Driving Systems (ADSs) reach the limits of their system boundaries. This study investigates the challenges faced by human Remote Drivers (RDs) during remote driving, particularly focusing on the identification and classification of human performance-related challenges through a comprehensive analysis of real-world remote driving data Las Vegas. For this purpose, a total of 183 RD performance-related Safety Driver (SD) interventions were analyzed and classified using an introduced severity classification. As it is essential to prevent the need for SD interventions, this study identified and analyzed harsh driving events to detect an increased likelihood of interventions by the SD. In addition, the results of the subjective RD questionnaire are used to evaluate whether the objective metrics from SD interventions and harsh driving events can also be confirmed by the RDs and whether additional challenges can be uncovered. The analysis reveals learning curves, showing a significant decrease in SD interventions as RD experience increases. Early phases of remote driving experience, especially below 200 km of experience, showed the highest frequency of safety-related events, including braking too late for traffic signs and responding impatiently to other traffic participants. Over time, RDs follow defined rules for improving their control, with experience leading to less harsh braking, acceleration, and steering maneuvers. The study contributes to understanding the requirements of RDS, emphasizing the importance of targeted training to address human performance limitations. It further highlights the need for system improvements to address challenges like latency and the limited haptic feedback replaced by visual feedback, which affect the RDs' perception and vehicle control."
  },
  {
    "title": "Honey Trap or Romantic Utopia: A Case Study of Final Fantasy XIV Players PII Disclosure in Intimate Partner-Seeking Posts",
    "url": "http://arxiv.org/abs/2503.09832v1",
    "arxiv_id": "2503.09832v1",
    "authors": [
      "Yihao Zhou",
      "Tanusree Sharma"
    ],
    "published": "2025-03-12T20:53:06+00:00",
    "summary": "Massively multiplayer online games (MMOGs) can foster social interaction and relationship formation, but they pose specific privacy and safety challenges, especially in the context of mediating intimate interpersonal connections. To explore the potential risks, we conducted a case study on Final Fantasy XIV (FFXIV) players intimate partner seeking posts on social media. We analyzed 1,288 posts from a public Weibo account using Latent Dirichlet Allocation (LDA) topic modeling and thematic analysis. Our findings reveal that players disclose sensitive personal information and share vulnerabilities to establish trust but face difficulties in managing identity and privacy across multiple platforms. We also found that players expectations regarding intimate partner are diversified, and mismatch of expectations may leads to issues like privacy leakage or emotional exploitation. Based on our findings, we propose design implications for reducing privacy and safety risks and fostering healthier social interactions in virtual worlds."
  },
  {
    "title": "How good are deep learning methods for automated road safety analysis using video data? An experimental study",
    "url": "http://arxiv.org/abs/2503.09807v1",
    "arxiv_id": "2503.09807v1",
    "authors": [
      "Qingwu Liu",
      "Nicolas Saunier",
      "Guillaume-Alexandre Bilodeau"
    ],
    "published": "2025-03-12T20:17:50+00:00",
    "summary": "Image-based multi-object detection (MOD) and multi-object tracking (MOT) are advancing at a fast pace. A variety of 2D and 3D MOD and MOT methods have been developed for monocular and stereo cameras. Road safety analysis can benefit from those advancements. As crashes are rare events, surrogate measures of safety (SMoS) have been developed for safety analyses. (Semi-)Automated safety analysis methods extract road user trajectories to compute safety indicators, for example, Time-to-Collision (TTC) and Post-encroachment Time (PET). Inspired by the success of deep learning in MOD and MOT, we investigate three MOT methods, including one based on a stereo-camera, using the annotated KITTI traffic video dataset. Two post-processing steps, IDsplit and SS, are developed to improve the tracking results and investigate the factors influencing the TTC. The experimental results show that, despite some advantages in terms of the numbers of interactions or similarity to the TTC distributions, all the tested methods systematically over-estimate the number of interactions and under-estimate the TTC: they report more interactions and more severe interactions, making the road user interactions appear less safe than they are. Further efforts will be directed towards testing more methods and more data, in particular from roadside sensors, to verify the results and improve the performance."
  },
  {
    "title": "Constrained Language Generation with Discrete Diffusion Models",
    "url": "http://arxiv.org/abs/2503.09790v1",
    "arxiv_id": "2503.09790v1",
    "authors": [
      "Michael Cardei",
      "Jacob K Christopher",
      "Thomas Hartvigsen",
      "Brian R. Bartoldson",
      "Bhavya Kailkhura",
      "Ferdinando Fioretto"
    ],
    "published": "2025-03-12T19:48:12+00:00",
    "summary": "Constraints are critical in text generation as LLM outputs are often unreliable when it comes to ensuring generated outputs adhere to user defined instruction or general safety guidelines. To address this gap, we present Constrained Discrete Diffusion (CDD), a novel method for enforcing constraints on natural language by integrating discrete diffusion models with differentiable optimization. Unlike conventional text generators, which often rely on post-hoc filtering or model retraining for controllable generation, we propose imposing constraints directly into the discrete diffusion sampling process. We illustrate how this technique can be applied to satisfy a variety of natural language constraints, including (i) toxicity mitigation by preventing harmful content from emerging, (ii) character and sequence level lexical constraints, and (iii) novel molecule sequence generation with specific property adherence. Experimental results show that our constraint-aware procedure achieves high fidelity in meeting these requirements while preserving fluency and semantic coherence, outperforming auto-regressive and existing discrete diffusion approaches."
  },
  {
    "title": "Efficient Multi-Task Inferencing: Model Merging with Gromov-Wasserstein Feature Alignment",
    "url": "http://arxiv.org/abs/2503.09774v1",
    "arxiv_id": "2503.09774v1",
    "authors": [
      "Luyang Fang",
      "Ehsan Latif",
      "Haoran Lu",
      "Yifan Zhou",
      "Ping Ma",
      "Xiaoming Zhai"
    ],
    "published": "2025-03-12T19:20:33+00:00",
    "summary": "Automatic scoring of student responses enhances efficiency in education, but deploying a separate neural network for each task increases storage demands, maintenance efforts, and redundant computations. To address these challenges, this paper introduces the Gromov-Wasserstein Scoring Model Merging (GW-SMM) method, which merges models based on feature distribution similarities measured via the Gromov-Wasserstein distance. Our approach begins by extracting features from student responses using individual models, capturing both item-specific context and unique learned representations. The Gromov-Wasserstein distance then quantifies the similarity between these feature distributions, identifying the most compatible models for merging. Models exhibiting the smallest pairwise distances, typically in pairs or trios, are merged by combining only the shared layers preceding the classification head. This strategy results in a unified feature extractor while preserving separate classification heads for item-specific scoring. We validated our approach against human expert knowledge and a GPT-o1-based merging method. GW-SMM consistently outperformed both, achieving a higher micro F1 score, macro F1 score, exact match accuracy, and per-label accuracy. The improvements in micro F1 and per-label accuracy were statistically significant compared to GPT-o1-based merging (p=0.04, p=0.01). Additionally, GW-SMM reduced storage requirements by half without compromising much accuracy, demonstrating its computational efficiency alongside reliable scoring performance."
  },
  {
    "title": "How to Protect Yourself from 5G Radiation? Investigating LLM Responses to Implicit Misinformation",
    "url": "http://arxiv.org/abs/2503.09598v1",
    "arxiv_id": "2503.09598v1",
    "authors": [
      "Ruohao Guo",
      "Wei Xu",
      "Alan Ritter"
    ],
    "published": "2025-03-12T17:59:18+00:00",
    "summary": "As Large Language Models (LLMs) are widely deployed in diverse scenarios, the extent to which they could tacitly spread misinformation emerges as a critical safety concern. Current research primarily evaluates LLMs on explicit false statements, overlooking how misinformation often manifests subtly as unchallenged premises in real-world user interactions. We curated ECHOMIST, the first comprehensive benchmark for implicit misinformation, where the misinformed assumptions are embedded in a user query to LLMs. ECHOMIST is based on rigorous selection criteria and carefully curated data from diverse sources, including real-world human-AI conversations and social media interactions. We also introduce a new evaluation metric to measure whether LLMs can recognize and counter false information rather than amplify users' misconceptions. Through an extensive empirical study on a wide range of LLMs, including GPT-4, Claude, and Llama, we find that current models perform alarmingly poorly on this task, often failing to detect false premises and generating misleading explanations. Our findings underscore the critical need for an increased focus on implicit misinformation in LLM safety research."
  },
  {
    "title": "Towards Reasoning Era: A Survey of Long Chain-of-Thought for Reasoning Large Language Models",
    "url": "http://arxiv.org/abs/2503.09567v1",
    "arxiv_id": "2503.09567v1",
    "authors": [
      "Qiguang Chen",
      "Libo Qin",
      "Jinhao Liu",
      "Dengyun Peng",
      "Jiannan Guan",
      "Peng Wang",
      "Mengkang Hu",
      "Yuhang Zhou",
      "Te Gao",
      "Wangxiang Che"
    ],
    "published": "2025-03-12T17:35:03+00:00",
    "summary": "Recent advancements in reasoning with large language models (RLLMs), such as OpenAI-O1 and DeepSeek-R1, have demonstrated their impressive capabilities in complex domains like mathematics and coding. A central factor in their success lies in the application of long chain-of-thought (Long CoT) characteristics, which enhance reasoning abilities and enable the solution of intricate problems. However, despite these developments, a comprehensive survey on Long CoT is still lacking, limiting our understanding of its distinctions from traditional short chain-of-thought (Short CoT) and complicating ongoing debates on issues like \"overthinking\" and \"test-time scaling.\" This survey seeks to fill this gap by offering a unified perspective on Long CoT. (1) We first distinguish Long CoT from Short CoT and introduce a novel taxonomy to categorize current reasoning paradigms. (2) Next, we explore the key characteristics of Long CoT: deep reasoning, extensive exploration, and feasible reflection, which enable models to handle more complex tasks and produce more efficient, coherent outcomes compared to the shallower Short CoT. (3) We then investigate key phenomena such as the emergence of Long CoT with these characteristics, including overthinking, and test-time scaling, offering insights into how these processes manifest in practice. (4) Finally, we identify significant research gaps and highlight promising future directions, including the integration of multi-modal reasoning, efficiency improvements, and enhanced knowledge frameworks. By providing a structured overview, this survey aims to inspire future research and further the development of logical reasoning in artificial intelligence."
  },
  {
    "title": "Towards Reasoning Era: A Survey of Long Chain-of-Thought for Reasoning Large Language Models",
    "url": "http://arxiv.org/abs/2503.09567v2",
    "arxiv_id": "2503.09567v2",
    "authors": [
      "Qiguang Chen",
      "Libo Qin",
      "Jinhao Liu",
      "Dengyun Peng",
      "Jiannan Guan",
      "Peng Wang",
      "Mengkang Hu",
      "Yuhang Zhou",
      "Te Gao",
      "Wanxiang Che"
    ],
    "published": "2025-03-12T17:35:03+00:00",
    "summary": "Recent advancements in reasoning with large language models (RLLMs), such as OpenAI-O1 and DeepSeek-R1, have demonstrated their impressive capabilities in complex domains like mathematics and coding. A central factor in their success lies in the application of long chain-of-thought (Long CoT) characteristics, which enhance reasoning abilities and enable the solution of intricate problems. However, despite these developments, a comprehensive survey on Long CoT is still lacking, limiting our understanding of its distinctions from traditional short chain-of-thought (Short CoT) and complicating ongoing debates on issues like \"overthinking\" and \"test-time scaling.\" This survey seeks to fill this gap by offering a unified perspective on Long CoT. (1) We first distinguish Long CoT from Short CoT and introduce a novel taxonomy to categorize current reasoning paradigms. (2) Next, we explore the key characteristics of Long CoT: deep reasoning, extensive exploration, and feasible reflection, which enable models to handle more complex tasks and produce more efficient, coherent outcomes compared to the shallower Short CoT. (3) We then investigate key phenomena such as the emergence of Long CoT with these characteristics, including overthinking, and test-time scaling, offering insights into how these processes manifest in practice. (4) Finally, we identify significant research gaps and highlight promising future directions, including the integration of multi-modal reasoning, efficiency improvements, and enhanced knowledge frameworks. By providing a structured overview, this survey aims to inspire future research and further the development of logical reasoning in artificial intelligence."
  },
  {
    "title": "Search-R1: Training LLMs to Reason and Leverage Search Engines with Reinforcement Learning",
    "url": "http://arxiv.org/abs/2503.09516v1",
    "arxiv_id": "2503.09516v1",
    "authors": [
      "Bowen Jin",
      "Hansi Zeng",
      "Zhenrui Yue",
      "Dong Wang",
      "Hamed Zamani",
      "Jiawei Han"
    ],
    "published": "2025-03-12T16:26:39+00:00",
    "summary": "Efficiently acquiring external knowledge and up-to-date information is essential for effective reasoning and text generation in large language models (LLMs). Retrieval augmentation and tool-use training approaches where a search engine is treated as a tool lack complex multi-turn retrieval flexibility or require large-scale supervised data. Prompting advanced LLMs with reasoning capabilities during inference to use search engines is not optimal, since the LLM does not learn how to optimally interact with the search engine. This paper introduces Search-R1, an extension of the DeepSeek-R1 model where the LLM learns -- solely through reinforcement learning (RL) -- to autonomously generate (multiple) search queries during step-by-step reasoning with real-time retrieval. Search-R1 optimizes LLM rollouts with multi-turn search interactions, leveraging retrieved token masking for stable RL training and a simple outcome-based reward function. Experiments on seven question-answering datasets show that Search-R1 improves performance by 26% (Qwen2.5-7B), 21% (Qwen2.5-3B), and 10% (LLaMA3.2-3B) over SOTA baselines. This paper further provides empirical insights into RL optimization methods, LLM choices, and response length dynamics in retrieval-augmented reasoning. The code and model checkpoints are available at https://github.com/PeterGriffinJin/Search-R1."
  },
  {
    "title": "Reinforcement Learning is all You Need",
    "url": "http://arxiv.org/abs/2503.09512v1",
    "arxiv_id": "2503.09512v1",
    "authors": [
      "Yongsheng Lian"
    ],
    "published": "2025-03-12T16:22:28+00:00",
    "summary": "Inspired by the success of DeepSeek R1 in reasoning via reinforcement learning without human feedback, we train a 3B language model using the Countdown Game with pure reinforcement learning. Our model outperforms baselines on four of five benchmarks, demonstrating improved generalization beyond its training data. Notably, response length does not correlate with reasoning quality, and while \"aha moments\" emerge, they do not always yield correct answers. These findings highlight the potential of RL-only training for reasoning enhancement and suggest future work on refining reward structures to bridge emergent insights with accuracy."
  },
  {
    "title": "Experimental study of the convection in a thin cylindrical gas layer with imposed bottom and top fluxes and imposed side temperature",
    "url": "http://arxiv.org/abs/2503.09461v1",
    "arxiv_id": "2503.09461v1",
    "authors": [
      "Florian Rein",
      "Laure Car\u00e9nini",
      "Florian Fichot",
      "Benjamin Favier",
      "Michael Le Bars"
    ],
    "published": "2025-03-12T15:10:41+00:00",
    "summary": "We investigate convection in a thin cylindrical gas layer with an imposed flux at the bottom and a fixed temperature along the side, using a combination of direct numerical simulations and laboratory experiments. The experimental approach allows us to extend by two orders of magnitude the explored range in terms of flux Rayleigh number. We identify a scaling law governing the root-mean-square horizontal velocity and explain it through a dimensional analysis based on heat transport in the turbulent regime. Using particle image velocimetry, we experimentally confirm, for the most turbulent regimes, the presence of a drifting persistent pattern consisting of radial branches, as identified by Rein et al. (2023, J. Fluid Mech. 977, A26). We characterise the angular drift frequency and azimuthal wavenumber of this pattern as functions of the Rayleigh number. The system exhibits a wide distribution of heat flux across various time scales, with the longest fluctuations attributed to the branch pattern and the shortest to turbulent fluctuations. Consequently, the branch pattern must be considered to better forecast important wall heat flux fluctuations, a result of great relevance in the context of nuclear safety, the initial motivation for our study."
  },
  {
    "title": "Modelling lined rock caverns subject to hydrogen embrittlement and cyclic pressurisation in fractured rock masses",
    "url": "http://arxiv.org/abs/2503.09429v1",
    "arxiv_id": "2503.09429v1",
    "authors": [
      "Chenxi Zhao",
      "Haiyang Yu",
      "Zixin Zhang",
      "Qinghua Lei"
    ],
    "published": "2025-03-12T14:26:34+00:00",
    "summary": "The technology of lined rock cavern (LRC) with great geographical flexibility is a promising, cost-effective solution to underground hydrogen storage. However, the air-tight steel tanks used in this technology are susceptible to material degradation due to hydrogen embrittlement (HE), potentially leading to leakage and structural failure, especial for LRCs constructed in complex geological conditions. In this paper, we develop a 2D multiscale numerical model based on the finite element method to assess the impact of HE on the LRC performance in fractured rock masses under cyclic gas pressurisation. Within this framework, a large-scale model is used to simulate the deformation and damage evolution of both fractured rock and an LRC under in-situ stresses and internal gas pressurisation, while a small-scale model captures HE in the steel lining of the LRC. Our simulations reveal that damage in the rock, concrete, and steel degradation is strongly affected by pre-existing fractures and in-situ stresses. Our results also reveal the presence of a strong positive feedback between hydrogen concentration and stress redistribution in the steel lining. Moreover, a comparison between models with and without considering HE illuminates that hydrogen concentration significantly contributes to steel degradation, particularly during the long-term LRC operation, highlighting the critical role of HE in the safety and performance of the LRC. The findings and insights obtained from our work have important implications for the design optimisation and performance assessment of LRCs for sustainable underground hydrogen storage."
  },
  {
    "title": "Efficient dynamic modal load reconstruction using physics-informed Gaussian processes based on frequency-sparse Fourier basis functions",
    "url": "http://arxiv.org/abs/2503.09418v1",
    "arxiv_id": "2503.09418v1",
    "authors": [
      "Gledson Rodrigo Tondo",
      "Igor Kavrakov",
      "Guido Morgenthal"
    ],
    "published": "2025-03-12T14:16:27+00:00",
    "summary": "Knowledge of the force time history of a structure is essential to assess its behaviour, ensure safety and maintain reliability. However, direct measurement of external forces is often challenging due to sensor limitations, unknown force characteristics, or inaccessible load points. This paper presents an efficient dynamic load reconstruction method using physics-informed Gaussian processes (GP) based on frequency-sparse Fourier basis functions. The GP's covariance matrices are built using the description of the system dynamics, and the model is trained using structural response measurements. This provides support and interpretability to the machine learning model, in contrast to purely data-driven methods. In addition, the model filters out irrelevant components in the Fourier basis function by leveraging the sparsity of structural responses in the frequency domain, thereby reducing computational complexity during optimization. The trained model for structural responses is then integrated with the differential equation for a harmonic oscillator, creating a probabilistic dynamic load model that predicts load patterns without requiring force data during training. The model's effectiveness is validated through two case studies: a numerical model of a wind-excited 76-story building and an experiment using a physical scale model of the Lilleb{\\ae}lt Bridge in Denmark, excited by a servo motor. For both cases, validation of the reconstructed forces is provided using comparison metrics for several signal properties. The developed model holds potential for applications in structural health monitoring, damage prognosis, and load model validation."
  },
  {
    "title": "Ecosystem Evolution and Drivers across the Tibetan Plateau and Surrounding Regions",
    "url": "http://arxiv.org/abs/2503.09404v1",
    "arxiv_id": "2503.09404v1",
    "authors": [
      "Yiran Xie",
      "Xu Wang",
      "Yatong Qian",
      "Teng Liu",
      "Hao Fan",
      "Xiaosong Chen"
    ],
    "published": "2025-03-12T13:54:46+00:00",
    "summary": "The Tibetan Plateau (TP) and surrounding regions, vital to global energy and water cycles, are profoundly influenced by climate change and anthropogenic activities. Despite widespread attention to vegetation greening across the region since the 1980s, its underlying mechanisms remain poorly understood. This study employs the eigen microstates method to quantify vegetation greening dynamics using long-term remote sensing and reanalysis data. We identify two dominant modes that collectively explain more than 61% of the vegetation dynamics. The strong seasonal heterogeneity in the southern TP, primarily driven by radiation and agricultural activities, is reflected in the first mode, which accounts for 46.34% of the variance. The second mode, which explains 15% of the variance, is closely linked to deep soil moisture (SM3, 28 cm to 1 m). Compared to precipitation and surface soil moisture (SM1 and SM2, 0 to 28 cm), our results show that deep soil moisture exerts a stronger and more immediate influence on vegetation growth, with a one-month response time. This study provides a complexity theory-based framework to quantify vegetation dynamics and underscores the critical influence of deep soil moisture on greening patterns in the TP."
  },
  {
    "title": "Evaluating Reinforcement Learning Safety and Trustworthiness in Cyber-Physical Systems",
    "url": "http://arxiv.org/abs/2503.09388v1",
    "arxiv_id": "2503.09388v1",
    "authors": [
      "Katherine Dearstyne",
      "Pedro",
      "Alarcon Granadeno",
      "Theodore Chambers",
      "Jane Cleland-Huang"
    ],
    "published": "2025-03-12T13:33:07+00:00",
    "summary": "Cyber-Physical Systems (CPS) often leverage Reinforcement Learning (RL) techniques to adapt dynamically to changing environments and optimize performance. However, it is challenging to construct safety cases for RL components. We therefore propose the SAFE-RL (Safety and Accountability Framework for Evaluating Reinforcement Learning) for supporting the development, validation, and safe deployment of RL-based CPS. We adopt a design science approach to construct the framework and demonstrate its use in three RL applications in small Uncrewed Aerial systems (sUAS)"
  },
  {
    "title": "Fully-Synthetic Training for Visual Quality Inspection in Automotive Production",
    "url": "http://arxiv.org/abs/2503.09354v1",
    "arxiv_id": "2503.09354v1",
    "authors": [
      "Christoph Huber",
      "Dino Knoll",
      "Michael Guthe"
    ],
    "published": "2025-03-12T12:58:30+00:00",
    "summary": "Visual Quality Inspection plays a crucial role in modern manufacturing environments as it ensures customer safety and satisfaction. The introduction of Computer Vision (CV) has revolutionized visual quality inspection by improving the accuracy and efficiency of defect detection. However, traditional CV models heavily rely on extensive datasets for training, which can be costly, time-consuming, and error-prone. To overcome these challenges, synthetic images have emerged as a promising alternative. They offer a cost-effective solution with automatically generated labels. In this paper, we propose a pipeline for generating synthetic images using domain randomization. We evaluate our approach in three real inspection scenarios and demonstrate that an object detection model trained solely on synthetic data can outperform models trained on real images."
  },
  {
    "title": "MOAT: Evaluating LMMs for Capability Integration and Instruction Grounding",
    "url": "http://arxiv.org/abs/2503.09348v1",
    "arxiv_id": "2503.09348v1",
    "authors": [
      "Zhoutong Ye",
      "Mingze Sun",
      "Huan-ang Gao",
      "Chun Yu",
      "Yuanchun Shi"
    ],
    "published": "2025-03-12T12:49:31+00:00",
    "summary": "Large multimodal models (LMMs) have demonstrated significant potential as generalists in vision-language (VL) tasks. However, there remains a significant gap between state-of-the-art LMMs and human performance when it comes to complex tasks that require a combination of fundamental VL capabilities, as well as tasks involving the grounding of complex instructions. To thoroughly investigate the human-LMM gap and its underlying causes, we propose MOAT, a diverse benchmark with complex real-world VL tasks that are challenging for LMMs. Specifically, the tasks in MOAT require LMMs to engage in generalist problem solving by integrating fundamental VL capabilities such as reading text, counting, understanding spatial relations, grounding textual and visual instructions, etc. All these abilities fit into a taxonomy proposed by us that contains 10 fundamental VL capabilities, enabling MOAT to provide a fine-grained view of LMMs' strengths and weaknesses. Besides, MOAT is the first benchmark to explicitly evaluate LMMs' ability to ground complex text and visual instructions, which is essential to many real-world applications. We evaluate over 20 proprietary and open source LMMs, as well as humans, on MOAT, and found that humans achieved 82.7% accuracy while the best performing LMM (OpenAI o1) achieved only 38.8%. To guide future model development, we analyze common trends in our results and discuss the underlying causes of observed performance gaps between LMMs and humans, focusing on which VL capability forms the bottleneck in complex tasks, whether test time scaling improves performance on MOAT, and how tiling harms LMMs' capability to count. Code and data are available at https://cambrian-yzt.github.io/MOAT."
  },
  {
    "title": "Safer or Luckier? LLMs as Safety Evaluators Are Not Robust to Artifacts",
    "url": "http://arxiv.org/abs/2503.09347v1",
    "arxiv_id": "2503.09347v1",
    "authors": [
      "Hongyu Chen",
      "Seraphina Goldfarb-Tarrant"
    ],
    "published": "2025-03-12T12:49:02+00:00",
    "summary": "Large Language Models (LLMs) are increasingly employed as automated evaluators to assess the safety of generated content, yet their reliability in this role remains uncertain. This study evaluates a diverse set of 11 LLM judge models across critical safety domains, examining three key aspects: self-consistency in repeated judging tasks, alignment with human judgments, and susceptibility to input artifacts such as apologetic or verbose phrasing. Our findings reveal that biases in LLM judges can significantly distort the final verdict on which content source is safer, undermining the validity of comparative evaluations. Notably, apologetic language artifacts alone can skew evaluator preferences by up to 98\\%. Contrary to expectations, larger models do not consistently exhibit greater robustness, while smaller models sometimes show higher resistance to specific artifacts. To mitigate LLM evaluator robustness issues, we investigate jury-based evaluations aggregating decisions from multiple models. Although this approach both improves robustness and enhances alignment to human judgements, artifact sensitivity persists even with the best jury configurations. These results highlight the urgent need for diversified, artifact-resistant methodologies to ensure reliable safety assessments."
  },
  {
    "title": "CyberLLMInstruct: A New Dataset for Analysing Safety of Fine-Tuned LLMs Using Cyber Security Data",
    "url": "http://arxiv.org/abs/2503.09334v1",
    "arxiv_id": "2503.09334v1",
    "authors": [
      "Adel ElZemity",
      "Budi Arief",
      "Shujun Li"
    ],
    "published": "2025-03-12T12:29:27+00:00",
    "summary": "The integration of large language models (LLMs) into cyber security applications presents significant opportunities, such as enhancing threat analysis and malware detection, but can also introduce critical risks and safety concerns, including personal data leakage and automated generation of new malware. To address these challenges, we developed CyberLLMInstruct, a dataset of 54,928 instruction-response pairs spanning cyber security tasks such as malware analysis, phishing simulations, and zero-day vulnerabilities. The dataset was constructed through a multi-stage process. This involved sourcing data from multiple resources, filtering and structuring it into instruction-response pairs, and aligning it with real-world scenarios to enhance its applicability. Seven open-source LLMs were chosen to test the usefulness of CyberLLMInstruct: Phi 3 Mini 3.8B, Mistral 7B, Qwen 2.5 7B, Llama 3 8B, Llama 3.1 8B, Gemma 2 9B, and Llama 2 70B. In our primary example, we rigorously assess the safety of fine-tuned models using the OWASP top 10 framework, finding that fine-tuning reduces safety resilience across all tested LLMs and every adversarial attack (e.g., the security score of Llama 3.1 8B against prompt injection drops from 0.95 to 0.15). In our second example, we show that these same fine-tuned models can also achieve up to 92.50 percent accuracy on the CyberMetric benchmark. These findings highlight a trade-off between performance and safety, showing the importance of adversarial testing and further research into fine-tuning methodologies that can mitigate safety risks while still improving performance across diverse datasets and domains. All scripts required to reproduce the dataset, along with examples and relevant resources for replicating our results, will be made available upon the paper's acceptance."
  },
  {
    "title": "Earth as an Exoplanet: Investigating the effects of cloud variability on the direct-imaging of atmospheres",
    "url": "http://arxiv.org/abs/2503.09136v1",
    "arxiv_id": "2503.09136v1",
    "authors": [
      "Soumil Kelkar",
      "Prabal Saxena",
      "Ravi Kopparapu",
      "Joy Monteiro"
    ],
    "published": "2025-03-12T07:57:40+00:00",
    "summary": "A planet's spectrum is dynamic and only represents a time-dependent snapshot of its properties. Changing atmospheric conditions due to climate and weather patterns, particularly variation in cloud cover, can significantly affect the spectrum in ways that complicate the understanding of a planet's baseline atmospheric properties. Variable cloud cover and cloud properties affect the detectability of atmospheric constituents, and also greatly influence the radiative transfer that determines a planet's spectrum. This has considerable implications for direct imaging observations of potentially habitable exoplanets and thus it is critical to study and characterize the effects of clouds on their spectra. Clouds have been extensively modeled before and their effects have been incorporated across climate frameworks spanning a spectrum of complexity. Given the challenges associated with modeling clouds, we adopt a novel approach in this work to study the effects of clouds by using real-time cloud data from Earth observations. Treating Earth as an exoplanet and using detailed observations from the MERRA 2 data collection, we quantify the effects of cloud variability on the spectrum as well as on the detectability of atmospheric constituents, specifically biomarkers like O2, O3 and H2O. The coverage and vertical position of clouds significantly affects the SNRs of these gases and subsequently their detectability in exo-Earth atmospheres. Moreover, we show that variations in the amount of cloud cover will potentially confound efforts to retrieve a stable baseline atmosphere for a planet. This work has important applications to future direct-imaging missions like the Habitable Worlds Observatory (HWO)."
  },
  {
    "title": "Using Co-Located Range and Doppler Radars for Initial Orbit Determination",
    "url": "http://arxiv.org/abs/2503.09135v1",
    "arxiv_id": "2503.09135v1",
    "authors": [
      "Cristina Parigini",
      "Laura Pirovano",
      "Roberto Armellin",
      "Darren McKnight",
      "Adam Marsh",
      "Tom Reddell"
    ],
    "published": "2025-03-12T07:45:47+00:00",
    "summary": "With debris larger than 1 cm in size estimated to be over one million, precise cataloging efforts are essential to ensure space operations' safety. Compounding this challenge is the oversubscribed problem, where the sheer volume of space objects surpasses ground-based observatories' observational capacity. This results in sparse, brief observations and extended intervals before image acquisition. LeoLabs' network of phased-array radars addresses this need by reliably tracking 10 cm objects and larger in low Earth orbit with 10 independent radars across six sites. While LeoLabs tracklets are extremely short, they hold much more information than typical radar observations. Furthermore, two tracklets are generally available, separated by a couple of minutes. Thus, this paper develops a tailored approach to initialize state and uncertainty from a single or pair of tracklets. Through differential algebra, the initial orbit determination provides the state space compatible with the available measurements, namely an orbit set. This practice, widely used in previous research, allows for efficient data association of different tracklets, thus enabling the addition of accurate tracks to the catalog following their independent initialization. The algorithm's efficacy is tested using real measurements, evaluating the IOD solution's accuracy and ability to predict the next passage from a single or a pair of tracklets."
  },
  {
    "title": "Probing Latent Subspaces in LLM for AI Security: Identifying and Manipulating Adversarial States",
    "url": "http://arxiv.org/abs/2503.09066v1",
    "arxiv_id": "2503.09066v1",
    "authors": [
      "Xin Wei Chia",
      "Jonathan Pan"
    ],
    "published": "2025-03-12T04:59:22+00:00",
    "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across various tasks, yet they remain vulnerable to adversarial manipulations such as jailbreaking via prompt injection attacks. These attacks bypass safety mechanisms to generate restricted or harmful content. In this study, we investigated the underlying latent subspaces of safe and jailbroken states by extracting hidden activations from a LLM. Inspired by attractor dynamics in neuroscience, we hypothesized that LLM activations settle into semi stable states that can be identified and perturbed to induce state transitions. Using dimensionality reduction techniques, we projected activations from safe and jailbroken responses to reveal latent subspaces in lower dimensional spaces. We then derived a perturbation vector that when applied to safe representations, shifted the model towards a jailbreak state. Our results demonstrate that this causal intervention results in statistically significant jailbreak responses in a subset of prompts. Next, we probed how these perturbations propagate through the model's layers, testing whether the induced state change remains localized or cascades throughout the network. Our findings indicate that targeted perturbations induced distinct shifts in activations and model responses. Our approach paves the way for potential proactive defenses, shifting from traditional guardrail based methods to preemptive, model agnostic techniques that neutralize adversarial states at the representation level."
  },
  {
    "title": "ManeuverGPT Agentic Control for Safe Autonomous Stunt Maneuvers",
    "url": "http://arxiv.org/abs/2503.09035v1",
    "arxiv_id": "2503.09035v1",
    "authors": [
      "Shawn Azdam",
      "Pranav Doma",
      "Aliasghar Moj Arab"
    ],
    "published": "2025-03-12T03:51:41+00:00",
    "summary": "The next generation of active safety features in autonomous vehicles should be capable of safely executing evasive hazard-avoidance maneuvers akin to those performed by professional stunt drivers to achieve high-agility motion at the limits of vehicle handling. This paper presents a novel framework, ManeuverGPT, for generating and executing high-dynamic stunt maneuvers in autonomous vehicles using large language model (LLM)-based agents as controllers. We target aggressive maneuvers, such as J-turns, within the CARLA simulation environment and demonstrate an iterative, prompt-based approach to refine vehicle control parameters, starting tabula rasa without retraining model weights. We propose an agentic architecture comprised of three specialized agents (1) a Query Enricher Agent for contextualizing user commands, (2) a Driver Agent for generating maneuver parameters, and (3) a Parameter Validator Agent that enforces physics-based and safety constraints. Experimental results demonstrate successful J-turn execution across multiple vehicle models through textual prompts that adapt to differing vehicle dynamics. We evaluate performance via established success criteria and discuss limitations regarding numeric precision and scenario complexity. Our findings underscore the potential of LLM-driven control for flexible, high-dynamic maneuvers, while highlighting the importance of hybrid approaches that combine language-based reasoning with algorithmic validation."
  },
  {
    "title": "Traffic Regulation-aware Path Planning with Regulation Databases and Vision-Language Models",
    "url": "http://arxiv.org/abs/2503.09024v1",
    "arxiv_id": "2503.09024v1",
    "authors": [
      "Xu Han",
      "Zhiwen Wu",
      "Xin Xia",
      "Jiaqi Ma"
    ],
    "published": "2025-03-12T03:21:03+00:00",
    "summary": "This paper introduces and tests a framework integrating traffic regulation compliance into automated driving systems (ADS). The framework enables ADS to follow traffic laws and make informed decisions based on the driving environment. Using RGB camera inputs and a vision-language model (VLM), the system generates descriptive text to support a regulation-aware decision-making process, ensuring legal and safe driving practices. This information is combined with a machine-readable ADS regulation database to guide future driving plans within legal constraints. Key features include: 1) a regulation database supporting ADS decision-making, 2) an automated process using sensor input for regulation-aware path planning, and 3) validation in both simulated and real-world environments. Particularly, the real-world vehicle tests not only assess the framework's performance but also evaluate the potential and challenges of VLMs to solve complex driving problems by integrating detection, reasoning, and planning. This work enhances the legality, safety, and public trust in ADS, representing a significant step forward in the field."
  },
  {
    "title": "JBFuzz: Jailbreaking LLMs Efficiently and Effectively Using Fuzzing",
    "url": "http://arxiv.org/abs/2503.08990v1",
    "arxiv_id": "2503.08990v1",
    "authors": [
      "Vasudev Gohil"
    ],
    "published": "2025-03-12T01:52:17+00:00",
    "summary": "Large language models (LLMs) have shown great promise as language understanding and decision making tools, and they have permeated various aspects of our everyday life. However, their widespread availability also comes with novel risks, such as generating harmful, unethical, or offensive content, via an attack called jailbreaking. Despite extensive efforts from LLM developers to align LLMs using human feedback, they are still susceptible to jailbreak attacks. To tackle this issue, researchers often employ red-teaming to understand and investigate jailbreak prompts. However, existing red-teaming approaches lack effectiveness, scalability, or both. To address these issues, we propose JBFuzz, a novel effective, automated, and scalable red-teaming technique for jailbreaking LLMs.   JBFuzz is inspired by the success of fuzzing for detecting bugs/vulnerabilities in software. We overcome three challenges related to effectiveness and scalability by devising novel seed prompts, a lightweight mutation engine, and a lightweight and accurate evaluator for guiding the fuzzer. Assimilating all three solutions results in a potent fuzzer that only requires black-box access to the target LLM. We perform extensive experimental evaluation of JBFuzz using nine popular and widely-used LLMs. We find that JBFuzz successfully jailbreaks all LLMs for various harmful/unethical questions, with an average attack success rate of 99%. We also find that JBFuzz is extremely efficient as it jailbreaks a given LLM for a given question in 60 seconds on average. Our work highlights the susceptibility of the state-of-the-art LLMs to jailbreak attacks even after safety alignment, and serves as a valuable red-teaming tool for LLM developers."
  },
  {
    "title": "Leaky Batteries: A Novel Set of Side-Channel Attacks on Electric Vehicles",
    "url": "http://arxiv.org/abs/2503.08956v1",
    "arxiv_id": "2503.08956v1",
    "authors": [
      "Francesco Marchiori",
      "Mauro Conti"
    ],
    "published": "2025-03-11T23:18:26+00:00",
    "summary": "Advancements in battery technology have accelerated the adoption of Electric Vehicles (EVs) due to their environmental benefits. However, their growing sophistication introduces security and privacy challenges. Often seen as mere operational data, battery consumption patterns can unintentionally reveal critical information exploitable for malicious purposes. These risks go beyond privacy, impacting vehicle security and regulatory compliance. Despite these concerns, current research has largely overlooked the broader implications of battery consumption data exposure. As EVs integrate further into smart transportation networks, addressing these gaps is crucial to ensure their safety, reliability, and resilience. In this work, we introduce a novel class of side-channel attacks that exploit EV battery data to extract sensitive user information. Leveraging only battery consumption patterns, we demonstrate a methodology to accurately identify the EV driver and their driving style, determine the number of occupants, and infer the vehicle's start and end locations when user habits are known. We utilize several machine learning models and feature extraction techniques to analyze EV power consumption patterns, validating our approach on simulated and real-world datasets collected from actual drivers. Our attacks achieve an average success rate of 95.4% across all attack objectives. Our findings highlight the privacy risks associated with EV battery data, emphasizing the need for stronger protections to safeguard user privacy and vehicle security."
  },
  {
    "title": "Data-Driven Modeling of Amyloid-beta Targeted Antibodies for Alzheimer's Disease",
    "url": "http://arxiv.org/abs/2503.08938v1",
    "arxiv_id": "2503.08938v1",
    "authors": [
      "Kobra Rabiei",
      "Jeffrey R. Petrella",
      "Suzanne Lenhart",
      "Chun Liu",
      "P. Murali Doraiswamy",
      "Wenrui Hao"
    ],
    "published": "2025-03-11T22:35:29+00:00",
    "summary": "Alzheimer's disease (AD) is driven by the accumulation of amyloid-beta (Abeta) proteins in the brain, leading to memory loss and cognitive decline. While monoclonal antibodies targeting Abetahave been approved, optimizing their use to maximize benefits while minimizing side effects remains a challenge. This study develops a mathematical model to describe Abeta aggregation, capturing its progression from monomers to toxic oligomers, protofibrils, and fibrils using mass-action kinetics and coarse-grained modeling. The model is calibrated with experimental data, incorporating parameter estimation and sensitivity analysis to ensure accuracy. An optimal control framework is introduced to determine the best drug dosing strategy that reduces toxic Abeta aggregates while minimizing adverse effects, such as amyloid-related imaging abnormalities (ARIA). Results indicate that Donanemab achieves the greatest reduction in fibrils. This work provides a quantitative framework for optimizing AD treatment strategies, offering insights into balancing therapeutic efficacy and safety."
  },
  {
    "title": "Backtracking for Safety",
    "url": "http://arxiv.org/abs/2503.08919v1",
    "arxiv_id": "2503.08919v1",
    "authors": [
      "Bilgehan Sel",
      "Dingcheng Li",
      "Phillip Wallis",
      "Vaishakh Keshava",
      "Ming Jin",
      "Siddhartha Reddy Jonnalagadda"
    ],
    "published": "2025-03-11T22:04:22+00:00",
    "summary": "Large language models (LLMs) have demonstrated remarkable capabilities across various tasks, but ensuring their safety and alignment with human values remains crucial. Current safety alignment methods, such as supervised fine-tuning and reinforcement learning-based approaches, can exhibit vulnerabilities to adversarial attacks and often result in shallow safety alignment, primarily focusing on preventing harmful content in the initial tokens of the generated output. While methods like resetting can help recover from unsafe generations by discarding previous tokens and restarting the generation process, they are not well-suited for addressing nuanced safety violations like toxicity that may arise within otherwise benign and lengthy generations. In this paper, we propose a novel backtracking method designed to address these limitations. Our method allows the model to revert to a safer generation state, not necessarily at the beginning, when safety violations occur during generation. This approach enables targeted correction of problematic segments without discarding the entire generated text, thereby preserving efficiency. We demonstrate that our method dramatically reduces toxicity appearing through the generation process with minimal impact to efficiency."
  },
  {
    "title": "SICNav-Diffusion: Safe and Interactive Crowd Navigation with Diffusion Trajectory Predictions",
    "url": "http://arxiv.org/abs/2503.08858v1",
    "arxiv_id": "2503.08858v1",
    "authors": [
      "Sepehr Samavi",
      "Anthony Lem",
      "Fumiaki Sato",
      "Sirui Chen",
      "Qiao Gu",
      "Keijiro Yano",
      "Angela P. Schoellig",
      "Florian Shkurti"
    ],
    "published": "2025-03-11T19:54:50+00:00",
    "summary": "To navigate crowds without collisions, robots must interact with humans by forecasting their future motion and reacting accordingly. While learning-based prediction models have shown success in generating likely human trajectory predictions, integrating these stochastic models into a robot controller presents several challenges. The controller needs to account for interactive coupling between planned robot motion and human predictions while ensuring both predictions and robot actions are safe (i.e. collision-free). To address these challenges, we present a receding horizon crowd navigation method for single-robot multi-human environments. We first propose a diffusion model to generate joint trajectory predictions for all humans in the scene. We then incorporate these multi-modal predictions into a SICNav Bilevel MPC problem that simultaneously solves for a robot plan (upper-level) and acts as a safety filter to refine the predictions for non-collision (lower-level). Combining planning and prediction refinement into one bilevel problem ensures that the robot plan and human predictions are coupled. We validate the open-loop trajectory prediction performance of our diffusion model on the commonly used ETH/UCY benchmark and evaluate the closed-loop performance of our robot navigation method in simulation and extensive real-robot experiments demonstrating safe, efficient, and reactive robot motion."
  },
  {
    "title": "Intrinsic momentum transport driven by almost-rational surfaces in tokamak plasmas",
    "url": "http://arxiv.org/abs/2503.08793v1",
    "arxiv_id": "2503.08793v1",
    "authors": [
      "Justin Ball",
      "Arnas Volcokas",
      "Stephan Brunner"
    ],
    "published": "2025-03-11T18:07:18+00:00",
    "summary": "We demonstrate that a symmetry of the local gyrokinetic model is broken when the safety factor q is almost (but not exactly) a rational number and magnetic shear is $\\hat{s} \\approx 0$. Tokamaks with such a q profile will spontaneously rotate due to turbulent momentum transport. Nonlinear gyrokinetic simulations indicate this mechanism is significantly stronger than all other drives of intrinsic rotation. It also generates intrinsic electric current that pulls q towards rational values, potentially aiding non-inductive current drive. This is likely important in the triggering of internal transport barriers."
  },
  {
    "title": "CoLMDriver: LLM-based Negotiation Benefits Cooperative Autonomous Driving",
    "url": "http://arxiv.org/abs/2503.08683v1",
    "arxiv_id": "2503.08683v1",
    "authors": [
      "Changxing Liu",
      "Genjia Liu",
      "Zijun Wang",
      "Jinchang Yang",
      "Siheng Chen"
    ],
    "published": "2025-03-11T17:58:42+00:00",
    "summary": "Vehicle-to-vehicle (V2V) cooperative autonomous driving holds great promise for improving safety by addressing the perception and prediction uncertainties inherent in single-agent systems. However, traditional cooperative methods are constrained by rigid collaboration protocols and limited generalization to unseen interactive scenarios. While LLM-based approaches offer generalized reasoning capabilities, their challenges in spatial planning and unstable inference latency hinder their direct application in cooperative driving. To address these limitations, we propose CoLMDriver, the first full-pipeline LLM-based cooperative driving system, enabling effective language-based negotiation and real-time driving control. CoLMDriver features a parallel driving pipeline with two key components: (i) an LLM-based negotiation module under an actor-critic paradigm, which continuously refines cooperation policies through feedback from previous decisions of all vehicles; and (ii) an intention-guided waypoint generator, which translates negotiation outcomes into executable waypoints. Additionally, we introduce InterDrive, a CARLA-based simulation benchmark comprising 10 challenging interactive driving scenarios for evaluating V2V cooperation. Experimental results demonstrate that CoLMDriver significantly outperforms existing approaches, achieving an 11% higher success rate across diverse highly interactive V2V driving scenarios. Code will be released on https://github.com/cxliu0314/CoLMDriver."
  },
  {
    "title": "Chain-of-Thought Reasoning In The Wild Is Not Always Faithful",
    "url": "http://arxiv.org/abs/2503.08679v1",
    "arxiv_id": "2503.08679v1",
    "authors": [
      "Iv\u00e1n Arcuschin",
      "Jett Janiak",
      "Robert Krzyzanowski",
      "Senthooran Rajamanoharan",
      "Neel Nanda",
      "Arthur Conmy"
    ],
    "published": "2025-03-11T17:56:30+00:00",
    "summary": "Chain-of-Thought (CoT) reasoning has significantly advanced state-of-the-art AI capabilities. However, recent studies have shown that CoT reasoning is not always faithful, i.e. CoT reasoning does not always reflect how models arrive at conclusions. So far, most of these studies have focused on unfaithfulness in unnatural contexts where an explicit bias has been introduced. In contrast, we show that unfaithful CoT can occur on realistic prompts with no artificial bias. Our results reveal concerning rates of several forms of unfaithful reasoning in frontier models: Sonnet 3.7 (30.6%), DeepSeek R1 (15.8%) and ChatGPT-4o (12.6%) all answer a high proportion of question pairs unfaithfully. Specifically, we find that models rationalize their implicit biases in answers to binary questions (\"implicit post-hoc rationalization\"). For example, when separately presented with the questions \"Is X bigger than Y?\" and \"Is Y bigger than X?\", models sometimes produce superficially coherent arguments to justify answering Yes to both questions or No to both questions, despite such responses being logically contradictory. We also investigate restoration errors (Dziri et al., 2023), where models make and then silently correct errors in their reasoning, and unfaithful shortcuts, where models use clearly illogical reasoning to simplify solving problems in Putnam questions (a hard benchmark). Our findings raise challenges for AI safety work that relies on monitoring CoT to detect undesired behavior."
  },
  {
    "title": "Chain-of-Thought Reasoning In The Wild Is Not Always Faithful",
    "url": "http://arxiv.org/abs/2503.08679v2",
    "arxiv_id": "2503.08679v2",
    "authors": [
      "Iv\u00e1n Arcuschin",
      "Jett Janiak",
      "Robert Krzyzanowski",
      "Senthooran Rajamanoharan",
      "Neel Nanda",
      "Arthur Conmy"
    ],
    "published": "2025-03-11T17:56:30+00:00",
    "summary": "Chain-of-Thought (CoT) reasoning has significantly advanced state-of-the-art AI capabilities. However, recent studies have shown that CoT reasoning is not always faithful, i.e. CoT reasoning does not always reflect how models arrive at conclusions. So far, most of these studies have focused on unfaithfulness in unnatural contexts where an explicit bias has been introduced. In contrast, we show that unfaithful CoT can occur on realistic prompts with no artificial bias. Our results reveal non-negligible rates of several forms of unfaithful reasoning in frontier models: Sonnet 3.7 (16.3%), DeepSeek R1 (5.3%) and ChatGPT-4o (7.0%) all answer a notable proportion of question pairs unfaithfully. Specifically, we find that models rationalize their implicit biases in answers to binary questions (\"implicit post-hoc rationalization\"). For example, when separately presented with the questions \"Is X bigger than Y?\" and \"Is Y bigger than X?\", models sometimes produce superficially coherent arguments to justify answering Yes to both questions or No to both questions, despite such responses being logically contradictory. We also investigate restoration errors (Dziri et al., 2023), where models make and then silently correct errors in their reasoning, and unfaithful shortcuts, where models use clearly illogical reasoning to simplify solving problems in Putnam questions (a hard benchmark). Our findings raise challenges for AI safety work that relies on monitoring CoT to detect undesired behavior."
  },
  {
    "title": "AgentOrca: A Dual-System Framework to Evaluate Language Agents on Operational Routine and Constraint Adherence",
    "url": "http://arxiv.org/abs/2503.08669v1",
    "arxiv_id": "2503.08669v1",
    "authors": [
      "Zekun Li",
      "Shinda Huang",
      "Jiangtian Wang",
      "Nathan Zhang",
      "Antonis Antoniades",
      "Wenyue Hua",
      "Kaijie Zhu",
      "Sirui Zeng",
      "William Yang Wang",
      "Xifeng Yan"
    ],
    "published": "2025-03-11T17:53:02+00:00",
    "summary": "As language agents progressively automate critical tasks across domains, their ability to operate within operational constraints and safety protocols becomes essential. While extensive research has demonstrated these agents' effectiveness in downstream task completion, their reliability in following operational procedures and constraints remains largely unexplored. To this end, we present AgentOrca, a dual-system framework for evaluating language agents' compliance with operational constraints and routines. Our framework encodes action constraints and routines through both natural language prompts for agents and corresponding executable code serving as ground truth for automated verification. Through an automated pipeline of test case generation and evaluation across five real-world domains, we quantitatively assess current language agents' adherence to operational constraints. Our findings reveal notable performance gaps among state-of-the-art models, with large reasoning models like o1 demonstrating superior compliance while others show significantly lower performance, particularly when encountering complex constraints or user persuasion attempts."
  },
  {
    "title": "Generating Robot Constitutions & Benchmarks for Semantic Safety",
    "url": "http://arxiv.org/abs/2503.08663v1",
    "arxiv_id": "2503.08663v1",
    "authors": [
      "Pierre Sermanet",
      "Anirudha Majumdar",
      "Alex Irpan",
      "Dmitry Kalashnikov",
      "Vikas Sindhwani"
    ],
    "published": "2025-03-11T17:50:47+00:00",
    "summary": "Until recently, robotics safety research was predominantly about collision avoidance and hazard reduction in the immediate vicinity of a robot. Since the advent of large vision and language models (VLMs), robots are now also capable of higher-level semantic scene understanding and natural language interactions with humans. Despite their known vulnerabilities (e.g. hallucinations or jail-breaking), VLMs are being handed control of robots capable of physical contact with the real world. This can lead to dangerous behaviors, making semantic safety for robots a matter of immediate concern. Our contributions in this paper are two fold: first, to address these emerging risks, we release the ASIMOV Benchmark, a large-scale and comprehensive collection of datasets for evaluating and improving semantic safety of foundation models serving as robot brains. Our data generation recipe is highly scalable: by leveraging text and image generation techniques, we generate undesirable situations from real-world visual scenes and human injury reports from hospitals. Secondly, we develop a framework to automatically generate robot constitutions from real-world data to steer a robot's behavior using Constitutional AI mechanisms. We propose a novel auto-amending process that is able to introduce nuances in written rules of behavior; this can lead to increased alignment with human preferences on behavior desirability and safety. We explore trade-offs between generality and specificity across a diverse set of constitutions of different lengths, and demonstrate that a robot is able to effectively reject unconstitutional actions. We measure a top alignment rate of 84.3% on the ASIMOV Benchmark using generated constitutions, outperforming no-constitution baselines and human-written constitutions. Data is available at asimov-benchmark.github.io"
  },
  {
    "title": "Exploiting Instruction-Following Retrievers for Malicious Information Retrieval",
    "url": "http://arxiv.org/abs/2503.08644v1",
    "arxiv_id": "2503.08644v1",
    "authors": [
      "Parishad BehnamGhader",
      "Nicholas Meade",
      "Siva Reddy"
    ],
    "published": "2025-03-11T17:36:53+00:00",
    "summary": "Instruction-following retrievers have been widely adopted alongside LLMs in real-world applications, but little work has investigated the safety risks surrounding their increasing search capabilities. We empirically study the ability of retrievers to satisfy malicious queries, both when used directly and when used in a retrieval augmented generation-based setup. Concretely, we investigate six leading retrievers, including NV-Embed and LLM2Vec, and find that given malicious requests, most retrievers can (for >50% of queries) select relevant harmful passages. For example, LLM2Vec correctly selects passages for 61.35% of our malicious queries. We further uncover an emerging risk with instruction-following retrievers, where highly relevant harmful information can be surfaced by exploiting their instruction-following capabilities. Finally, we show that even safety-aligned LLMs, such as Llama3, can satisfy malicious requests when provided with harmful retrieved passages in-context. In summary, our findings underscore the malicious misuse risks associated with increasing retriever capability."
  },
  {
    "title": "DeepReview: Improving LLM-based Paper Review with Human-like Deep Thinking Process",
    "url": "http://arxiv.org/abs/2503.08569v1",
    "arxiv_id": "2503.08569v1",
    "authors": [
      "Minjun Zhu",
      "Yixuan Weng",
      "Linyi Yang",
      "Yue Zhang"
    ],
    "published": "2025-03-11T15:59:43+00:00",
    "summary": "Large Language Models (LLMs) are increasingly utilized in scientific research assessment, particularly in automated paper review. However, existing LLM-based review systems face significant challenges, including limited domain expertise, hallucinated reasoning, and a lack of structured evaluation. To address these limitations, we introduce DeepReview, a multi-stage framework designed to emulate expert reviewers by incorporating structured analysis, literature retrieval, and evidence-based argumentation. Using DeepReview-13K, a curated dataset with structured annotations, we train DeepReviewer-14B, which outperforms CycleReviewer-70B with fewer tokens. In its best mode, DeepReviewer-14B achieves win rates of 88.21\\% and 80.20\\% against GPT-o1 and DeepSeek-R1 in evaluations. Our work sets a new benchmark for LLM-based paper review, with all resources publicly available. The code, model, dataset and demo have be released in http://ai-researcher.net."
  },
  {
    "title": "Can We Detect Failures Without Failure Data? Uncertainty-Aware Runtime Failure Detection for Imitation Learning Policies",
    "url": "http://arxiv.org/abs/2503.08558v1",
    "arxiv_id": "2503.08558v1",
    "authors": [
      "Chen Xu",
      "Tony Khuong Nguyen",
      "Emma Dixon",
      "Christopher Rodriguez",
      "Patrick Miller",
      "Robert Lee",
      "Paarth Shah",
      "Rares Ambrus",
      "Haruki Nishimura",
      "Masha Itkina"
    ],
    "published": "2025-03-11T15:47:12+00:00",
    "summary": "Recent years have witnessed impressive robotic manipulation systems driven by advances in imitation learning and generative modeling, such as diffusion- and flow-based approaches. As robot policy performance increases, so does the complexity and time horizon of achievable tasks, inducing unexpected and diverse failure modes that are difficult to predict a priori. To enable trustworthy policy deployment in safety-critical human environments, reliable runtime failure detection becomes important during policy inference. However, most existing failure detection approaches rely on prior knowledge of failure modes and require failure data during training, which imposes a significant challenge in practicality and scalability. In response to these limitations, we present FAIL-Detect, a modular two-stage approach for failure detection in imitation learning-based robotic manipulation. To accurately identify failures from successful training data alone, we frame the problem as sequential out-of-distribution (OOD) detection. We first distill policy inputs and outputs into scalar signals that correlate with policy failures and capture epistemic uncertainty. FAIL-Detect then employs conformal prediction (CP) as a versatile framework for uncertainty quantification with statistical guarantees. Empirically, we thoroughly investigate both learned and post-hoc scalar signal candidates on diverse robotic manipulation tasks. Our experiments show learned signals to be mostly consistently effective, particularly when using our novel flow-based density estimator. Furthermore, our method detects failures more accurately and faster than state-of-the-art (SOTA) failure detection baselines. These results highlight the potential of FAIL-Detect to enhance the safety and reliability of imitation learning-based robotic systems as they progress toward real-world deployment."
  },
  {
    "title": "An Analysis of Safety Guarantees in Multi-Task Bayesian Optimization",
    "url": "http://arxiv.org/abs/2503.08555v1",
    "arxiv_id": "2503.08555v1",
    "authors": [
      "Jannis O. Luebsen",
      "Annika Eichler"
    ],
    "published": "2025-03-11T15:45:37+00:00",
    "summary": "In many practical scenarios of black box optimization, the objective function is subject to constraints that must be satisfied to avoid undesirable outcomes. Such constraints are typically unknown and must be learned during optimization. Safe Bayesian optimization aims to find the global optimum while ensuring that the constraints are satisfied with high probability. However, it is often sample-inefficient due to the small initial feasible set, which requires expansion by evaluating the objective or constraint functions, limiting its applicability to low-dimensional or inexpensive problems. To enhance sample efficiency, additional information from cheap simulations can be leveraged, albeit at the cost of safeness guarantees. This paper introduces a novel safe multi-task Bayesian optimization algorithm that integrates multiple tasks while maintaining high-probability safety. We derive robust uniform error bounds for the multi-task case and demonstrate the effectiveness of the approach on benchmark functions and a control problem. Our results show a significant improvement in sample efficiency, making the proposed method well-suited for expensive-to-evaluate functions."
  },
  {
    "title": "Soft Actor-Critic-based Control Barrier Adaptation for Robust Autonomous Navigation in Unknown Environments",
    "url": "http://arxiv.org/abs/2503.08479v1",
    "arxiv_id": "2503.08479v1",
    "authors": [
      "Nicholas Mohammad",
      "Nicola Bezzo"
    ],
    "published": "2025-03-11T14:33:55+00:00",
    "summary": "Motion planning failures during autonomous navigation often occur when safety constraints are either too conservative, leading to deadlocks, or too liberal, resulting in collisions. To improve robustness, a robot must dynamically adapt its safety constraints to ensure it reaches its goal while balancing safety and performance measures. To this end, we propose a Soft Actor-Critic (SAC)-based policy for adapting Control Barrier Function (CBF) constraint parameters at runtime, ensuring safe yet non-conservative motion. The proposed approach is designed for a general high-level motion planner, low-level controller, and target system model, and is trained in simulation only. Through extensive simulations and physical experiments, we demonstrate that our framework effectively adapts CBF constraints, enabling the robot to reach its final goal without compromising safety."
  },
  {
    "title": "Status and Future Prospects of the Standardization Framework Industry 4.0: A European Perspective",
    "url": "http://arxiv.org/abs/2503.08460v1",
    "arxiv_id": "2503.08460v1",
    "authors": [
      "Olga Meyer",
      "Marvin Boell",
      "Christoph Legat"
    ],
    "published": "2025-03-11T14:08:57+00:00",
    "summary": "The rapid development of Industry 4.0 technologies requires robust and comprehensive standardization to ensure interoperability, safety and efficiency in the Industry of the Future. This paper examines the fundamental role and functionality of standardization, with a particular focus on its importance in Europe's regulatory framework. Based on this, selected topics in context of standardization activities in context intelligent manufacturing and digital twins are highlighted and, by that, an overview of the Industry 4.0 standards framework is provided. This paper serves both as an informative guide to the existing standards in Industry 4.0 with respect to Artificial Intelligence and Digital Twins, and as a call to action for increased cooperation between standardization bodies and the research community. By fostering such collaboration, we aim to facilitate the continued development and implementation of standards that will drive innovation and progress in the manufacturing sector."
  },
  {
    "title": "Status and Future Prospects of the Standardization Framework Industry 4.0: A European Perspective",
    "url": "http://arxiv.org/abs/2503.08460v2",
    "arxiv_id": "2503.08460v2",
    "authors": [
      "Olga Meyer",
      "Marvin Boell",
      "Christoph Legat"
    ],
    "published": "2025-03-11T14:08:57+00:00",
    "summary": "The rapid development of Industry 4.0 technologies requires robust and comprehensive standardization to ensure interoperability, safety and efficiency in the Industry of the Future. This paper examines the fundamental role and functionality of standardization, with a particular focus on its importance in Europe's regulatory framework. Based on this, selected topics in context of standardization activities in context intelligent manufacturing and digital twins are highlighted and, by that, an overview of the Industry 4.0 standards framework is provided. This paper serves both as an informative guide to the existing standards in Industry 4.0 with respect to Artificial Intelligence and Digital Twins, and as a call to action for increased cooperation between standardization bodies and the research community. By fostering such collaboration, we aim to facilitate the continued development and implementation of standards that will drive innovation and progress in the manufacturing sector."
  },
  {
    "title": "ICPR 2024 Competition on Rider Intention Prediction",
    "url": "http://arxiv.org/abs/2503.08437v1",
    "arxiv_id": "2503.08437v1",
    "authors": [
      "Shankar Gangisetty",
      "Abdul Wasi",
      "Shyam Nandan Rai",
      "C. V. Jawahar",
      "Sajay Raj",
      "Manish Prajapati",
      "Ayesha Choudhary",
      "Aaryadev Chandra",
      "Dev Chandan",
      "Shireen Chand",
      "Suvaditya Mukherjee"
    ],
    "published": "2025-03-11T13:50:37+00:00",
    "summary": "The recent surge in the vehicle market has led to an alarming increase in road accidents. This underscores the critical importance of enhancing road safety measures, particularly for vulnerable road users like motorcyclists. Hence, we introduce the rider intention prediction (RIP) competition that aims to address challenges in rider safety by proactively predicting maneuvers before they occur, thereby strengthening rider safety. This capability enables the riders to react to the potential incorrect maneuvers flagged by advanced driver assistance systems (ADAS). We collect a new dataset, namely, rider action anticipation dataset (RAAD) for the competition consisting of two tasks: single-view RIP and multi-view RIP. The dataset incorporates a spectrum of traffic conditions and challenging navigational maneuvers on roads with varying lighting conditions. For the competition, we received seventy-five registrations and five team submissions for inference of which we compared the methods of the top three performing teams on both the RIP tasks: one state-space model (Mamba2) and two learning-based approaches (SVM and CNN-LSTM). The results indicate that the state-space model outperformed the other methods across the entire dataset, providing a balanced performance across maneuver classes. The SVM-based RIP method showed the second-best performance when using random sampling and SMOTE. However, the CNN-LSTM method underperformed, primarily due to class imbalance issues, particularly struggling with minority classes. This paper details the proposed RAAD dataset and provides a summary of the submissions for the RIP 2024 competition."
  },
  {
    "title": "Uni-Gaussians: Unifying Camera and Lidar Simulation with Gaussians for Dynamic Driving Scenarios",
    "url": "http://arxiv.org/abs/2503.08317v1",
    "arxiv_id": "2503.08317v1",
    "authors": [
      "Zikang Yuan",
      "Yuechuan Pu",
      "Hongcheng Luo",
      "Fengtian Lang",
      "Cheng Chi",
      "Teng Li",
      "Yingying Shen",
      "Haiyang Sun",
      "Bing Wang",
      "Xin Yang"
    ],
    "published": "2025-03-11T11:25:57+00:00",
    "summary": "Ensuring the safety of autonomous vehicles necessitates comprehensive simulation of multi-sensor data, encompassing inputs from both cameras and LiDAR sensors, across various dynamic driving scenarios. Neural rendering techniques, which utilize collected raw sensor data to simulate these dynamic environments, have emerged as a leading methodology. While NeRF-based approaches can uniformly represent scenes for rendering data from both camera and LiDAR, they are hindered by slow rendering speeds due to dense sampling. Conversely, Gaussian Splatting-based methods employ Gaussian primitives for scene representation and achieve rapid rendering through rasterization. However, these rasterization-based techniques struggle to accurately model non-linear optical sensors. This limitation restricts their applicability to sensors beyond pinhole cameras. To address these challenges and enable unified representation of dynamic driving scenarios using Gaussian primitives, this study proposes a novel hybrid approach. Our method utilizes rasterization for rendering image data while employing Gaussian ray-tracing for LiDAR data rendering. Experimental results on public datasets demonstrate that our approach outperforms current state-of-the-art methods. This work presents a unified and efficient solution for realistic simulation of camera and LiDAR data in autonomous driving scenarios using Gaussian primitives, offering significant advancements in both rendering quality and computational efficiency."
  },
  {
    "title": "Dynamic Risk Assessment for Human-Robot Collaboration Using a Heuristics-based Approach",
    "url": "http://arxiv.org/abs/2503.08316v1",
    "arxiv_id": "2503.08316v1",
    "authors": [
      "Georgios Katranis",
      "Frederik Plahl",
      "Joachim Grimstadt",
      "Ilshat Mamaev",
      "Silvia Vock",
      "Andrey Morozov"
    ],
    "published": "2025-03-11T11:25:47+00:00",
    "summary": "Human-robot collaboration (HRC) introduces significant safety challenges, particularly in protecting human operators working alongside collaborative robots (cobots). While current ISO standards emphasize risk assessment and hazard identification, these procedures are often insufficient for addressing the complexity of HRC environments, which involve numerous design factors and dynamic interactions. This publication presents a method for objective hazard analysis to support Dynamic Risk Assessment, extending beyond reliance on expert knowledge. The approach monitors scene parameters, such as the distance between human body parts and the cobot, as well as the cobot`s Cartesian velocity. Additionally, an anthropocentric parameter focusing on the orientation of the human head within the collaborative workspace is introduced. These parameters are transformed into hazard indicators using non-linear heuristic functions. The hazard indicators are then aggregated to estimate the total hazard level of a given scenario. The proposed method is evaluated using an industrial dataset that depicts various interactions between a human operator and a cobot."
  },
  {
    "title": "Safety-Ensured Control Framework for Robotic Endoscopic Task Automation",
    "url": "http://arxiv.org/abs/2503.08214v1",
    "arxiv_id": "2503.08214v1",
    "authors": [
      "Yitaek Kim",
      "I\u00f1igo Iturrate",
      "Christoffer Sloth",
      "Hansoul Kim"
    ],
    "published": "2025-03-11T09:28:35+00:00",
    "summary": "There is growing interest in automating surgical tasks using robotic systems, such as endoscopy for treating gastrointestinal (GI) cancer. However, previous studies have primarily focused on detecting and analyzing objects or robots, with limited attention to ensuring safety, which is critical for clinical applications, where accidents can be caused by unsafe robot motions. In this study, we propose a new control framework that can formally ensure the safety of automating certain processes involved in endoscopic submucosal dissection (ESD), a representative endoscopic surgical method for the treatment of early GI cancer, by using an endoscopic robot. The proposed framework utilizes Control Barrier Functions (CBFs) to accurately identify the boundaries of individual tumors, even in close proximity within the GI tract, ensuring precise treatment and removal while preserving the surrounding normal tissue. Additionally, by adopting a model-free control scheme, safety assurance is made possible even in endoscopic robotic systems where dynamic modeling is challenging. We demonstrate the proposed framework in cases where the tumors to be removed are close to each other, showing that the safety constraints are enforced. We show that the model-free CBF-based controlled robot eliminates one tumor completely without damaging it, while not invading another nearby tumor."
  },
  {
    "title": "Dialogue Injection Attack: Jailbreaking LLMs through Context Manipulation",
    "url": "http://arxiv.org/abs/2503.08195v1",
    "arxiv_id": "2503.08195v1",
    "authors": [
      "Wenlong Meng",
      "Fan Zhang",
      "Wendao Yao",
      "Zhenyuan Guo",
      "Yuwei Li",
      "Chengkun Wei",
      "Wenzhi Chen"
    ],
    "published": "2025-03-11T09:00:45+00:00",
    "summary": "Large language models (LLMs) have demonstrated significant utility in a wide range of applications; however, their deployment is plagued by security vulnerabilities, notably jailbreak attacks. These attacks manipulate LLMs to generate harmful or unethical content by crafting adversarial prompts. While much of the current research on jailbreak attacks has focused on single-turn interactions, it has largely overlooked the impact of historical dialogues on model behavior. In this paper, we introduce a novel jailbreak paradigm, Dialogue Injection Attack (DIA), which leverages the dialogue history to enhance the success rates of such attacks. DIA operates in a black-box setting, requiring only access to the chat API or knowledge of the LLM's chat template. We propose two methods for constructing adversarial historical dialogues: one adapts gray-box prefilling attacks, and the other exploits deferred responses. Our experiments show that DIA achieves state-of-the-art attack success rates on recent LLMs, including Llama-3.1 and GPT-4o. Additionally, we demonstrate that DIA can bypass 5 different defense mechanisms, highlighting its robustness and effectiveness."
  },
  {
    "title": "FASIONAD++ : Integrating High-Level Instruction and Information Bottleneck in FAt-Slow fusION Systems for Enhanced Safety in Autonomous Driving with Adaptive Feedback",
    "url": "http://arxiv.org/abs/2503.08162v1",
    "arxiv_id": "2503.08162v1",
    "authors": [
      "Kangan Qian",
      "Ziang Luo",
      "Sicong Jiang",
      "Zilin Huang",
      "Jinyu Miao",
      "Zhikun Ma",
      "Tianze Zhu",
      "Jiayin Li",
      "Yangfan He",
      "Zheng Fu",
      "Yining Shi",
      "Boyue Wang",
      "Hezhe Lin",
      "Ziyu Chen",
      "Jiangbo Yu",
      "Xinyu Jiao",
      "Mengmeng Yang",
      "Kun Jiang",
      "Diange Yang"
    ],
    "published": "2025-03-11T08:27:01+00:00",
    "summary": "Ensuring safe, comfortable, and efficient planning is crucial for autonomous driving systems. While end-to-end models trained on large datasets perform well in standard driving scenarios, they struggle with complex low-frequency events. Recent Large Language Models (LLMs) and Vision Language Models (VLMs) advancements offer enhanced reasoning but suffer from computational inefficiency. Inspired by the dual-process cognitive model \"Thinking, Fast and Slow\", we propose $\\textbf{FASIONAD}$ -- a novel dual-system framework that synergizes a fast end-to-end planner with a VLM-based reasoning module. The fast system leverages end-to-end learning to achieve real-time trajectory generation in common scenarios, while the slow system activates through uncertainty estimation to perform contextual analysis and complex scenario resolution. Our architecture introduces three key innovations: (1) A dynamic switching mechanism enabling slow system intervention based on real-time uncertainty assessment; (2) An information bottleneck with high-level plan feedback that optimizes the slow system's guidance capability; (3) A bidirectional knowledge exchange where visual prompts enhance the slow system's reasoning while its feedback refines the fast planner's decision-making. To strengthen VLM reasoning, we develop a question-answering mechanism coupled with reward-instruct training strategy. In open-loop experiments, FASIONAD achieves a $6.7\\%$ reduction in average $L2$ trajectory error and $28.1\\%$ lower collision rate."
  },
  {
    "title": "Toward Stable World Models: Measuring and Addressing World Instability in Generative Environments",
    "url": "http://arxiv.org/abs/2503.08122v1",
    "arxiv_id": "2503.08122v1",
    "authors": [
      "Soonwoo Kwon",
      "Jin-Young Kim",
      "Hyojun Go",
      "Kyungjune Baek"
    ],
    "published": "2025-03-11T07:38:11+00:00",
    "summary": "We present a novel study on enhancing the capability of preserving the content in world models, focusing on a property we term World Stability. Recent diffusion-based generative models have advanced the synthesis of immersive and realistic environments that are pivotal for applications such as reinforcement learning and interactive game engines. However, while these models excel in quality and diversity, they often neglect the preservation of previously generated scenes over time--a shortfall that can introduce noise into agent learning and compromise performance in safety-critical settings. In this work, we introduce an evaluation framework that measures world stability by having world models perform a sequence of actions followed by their inverses to return to their initial viewpoint, thereby quantifying the consistency between the starting and ending observations. Our comprehensive assessment of state-of-the-art diffusion-based world models reveals significant challenges in achieving high world stability. Moreover, we investigate several improvement strategies to enhance world stability. Our results underscore the importance of world stability in world modeling and provide actionable insights for future research in this domain."
  },
  {
    "title": "Control Barrier Functions for Prescribed-time Reach-Avoid-Stay Tasks using Spatiotemporal Tubes",
    "url": "http://arxiv.org/abs/2503.08106v1",
    "arxiv_id": "2503.08106v1",
    "authors": [
      "Ratnangshu Das",
      "Pranav Bakshi",
      "Pushpak Jagtap"
    ],
    "published": "2025-03-11T07:15:49+00:00",
    "summary": "Prescribed-time reach-avoid-stay (PT-RAS) specifications are crucial in applications requiring precise timing, state constraints, and safety guarantees. While control carrier functions (CBFs) have emerged as a promising approach, providing formal guarantees of safety, constructing CBFs that satisfy PT-RAS specifications remains challenging. In this paper, we present a novel approach using a spatiotemporal tubes (STTs) framework to construct CBFs for PT-RAS tasks. The STT framework allows for the systematic design of CBFs that dynamically manage both spatial and temporal constraints, ensuring the system remains within a safe operational envelope while achieving the desired temporal objectives. The proposed method is validated with two case studies: temporal motion planning of an omnidirectional robot and temporal waypoint navigation of a drone with obstacles, using higher-order CBFs."
  },
  {
    "title": "Enhancing Vehicle Platooning Safety via Control Node Placement and Sizing under State and Input Bounds",
    "url": "http://arxiv.org/abs/2503.08089v1",
    "arxiv_id": "2503.08089v1",
    "authors": [
      "Yifei She",
      "Shen Wang",
      "Ahmad Taha",
      "Xiaofeng Tao"
    ],
    "published": "2025-03-11T06:45:29+00:00",
    "summary": "Vehicle platooning with Cooperative Adaptive Cruise Control improves traffic efficiency, reduces energy consumption, and enhances safety but remains vulnerable to cyber-attacks that disrupt communication and cause unsafe actions. To address these risks, this paper investigates control node placement and input bound optimization to balance safety and defense efficiency under various conditions. We propose a two-stage actuator placement and actuator saturation approach, which focuses on identifying key actuators that maximize the system's controllability while operating under state and input constraints. By strategically placing and limiting the input bounds of critical actuators, we ensure that vehicles maintain safe distances even under attack. Simulation results show that our method effectively mitigates the impact of attacks while preserving defense efficiency, offering a robust solution to vehicle platooning safety challenges."
  },
  {
    "title": "TRUST: Stability and Safety Controller Synthesis for Unknown Dynamical Models Using a Single Trajectory",
    "url": "http://arxiv.org/abs/2503.08081v1",
    "arxiv_id": "2503.08081v1",
    "authors": [
      "Jamie Gardner",
      "Ben Wooding",
      "Amy Nejati",
      "Abolfazl Lavaei"
    ],
    "published": "2025-03-11T06:27:40+00:00",
    "summary": "TRUST is an open-source software tool developed for data-driven controller synthesis of dynamical systems with unknown mathematical models, ensuring either stability or safety properties. By collecting only a single input-state trajectory from the unknown system and satisfying a rank condition that ensures the system is persistently excited according to the Willems et al.'s fundamental lemma, TRUST aims to design either control Lyapunov functions (CLF) or control barrier certificates (CBC), along with their corresponding stability or safety controllers. The tool implements sum-of-squares (SOS) optimization programs solely based on data to enforce stability or safety properties across four system classes: (i) continuous-time nonlinear polynomial systems, (ii) continuous-time linear systems, (iii) discrete-time nonlinear polynomial systems, and (iv) discrete-time linear systems. TRUST is a Python-based web application featuring an intuitive, reactive graphic user interface (GUI) built with web technologies. It can be accessed at https://trust.tgo.dev or installed locally, and supports both manual data entry and data file uploads. Leveraging the power of the Python backend and a JavaScript frontend, TRUST is designed to be highly user-friendly and accessible across desktop, laptop, tablet, and mobile devices. We apply TRUST to a set of physical benchmarks with unknown dynamics, ensuring either stability or safety properties across the four supported classes of models."
  },
  {
    "title": "STGDPM:Vessel Trajectory Prediction with Spatio-Temporal Graph Diffusion Probabilistic Model",
    "url": "http://arxiv.org/abs/2503.08065v1",
    "arxiv_id": "2503.08065v1",
    "authors": [
      "Jin Wenzhe",
      "Tang Haina",
      "Zhang Xudong"
    ],
    "published": "2025-03-11T05:50:27+00:00",
    "summary": "Vessel trajectory prediction is a critical component for ensuring maritime traffic safety and avoiding collisions. Due to the inherent uncertainty in vessel behavior, trajectory prediction systems must adopt a multimodal approach to accurately model potential future motion states. However, existing vessel trajectory prediction methods lack the ability to comprehensively model behavioral multi-modality. To better capture multimodal behavior in interactive scenarios, we propose modeling interactions as dynamic graphs, replacing traditional aggregation-based techniques that rely on vessel states. By leveraging the natural multimodal capabilities of diffusion models, we frame the trajectory prediction task as an inverse process of motion uncertainty diffusion, wherein uncertainties across potential navigational areas are progressively eliminated until the desired trajectories is produced. In summary, we pioneer the integration of Spatio-Temporal Graph (STG) with diffusion models in ship trajectory prediction. Extensive experiments on real Automatic Identification System (AIS) data validate the superiority of our approach."
  },
  {
    "title": "Data-Driven Dynamic Controller Synthesis for Discrete-Time General Nonlinear Systems",
    "url": "http://arxiv.org/abs/2503.08060v1",
    "arxiv_id": "2503.08060v1",
    "authors": [
      "Behrad Samari",
      "Abolfazl Lavaei"
    ],
    "published": "2025-03-11T05:38:47+00:00",
    "summary": "Synthesizing safety controllers for general nonlinear systems is a highly challenging task, particularly when the system models are unknown, and input constraints are present. While some recent efforts have explored data-driven safety controller design for nonlinear systems, these approaches are primarily limited to specific classes of nonlinear dynamics (e.g., polynomials) and are not applicable to general nonlinear systems. This paper develops a direct data-driven approach for discrete-time general nonlinear systems, facilitating the simultaneous learning of control barrier certificates (CBCs) and dynamic controllers to ensure safety properties under input constraints. Specifically, by leveraging the adding-one-integrator approach, we incorporate the controller's dynamics into the system dynamics to synthesize a virtual static-feedback controller for the augmented system, resulting in a dynamic safety controller for the actual dynamics. We collect input-state data from the augmented system during a finite-time experiment, referred to as a single trajectory. Using this data, we learn augmented CBCs and the corresponding virtual safety controllers, ensuring the safety of the actual system and adherence to input constraints over a finite time horizon. We demonstrate that our proposed conditions boil down to some data-dependent linear matrix inequalities (LMIs), which are easy to satisfy. We showcase the effectiveness of our data-driven approach through two case studies: one exhibiting significant nonlinearity and the other featuring high dimensionality."
  },
  {
    "title": "SGNetPose+: Stepwise Goal-Driven Networks with Pose Information for Trajectory Prediction in Autonomous Driving",
    "url": "http://arxiv.org/abs/2503.08016v1",
    "arxiv_id": "2503.08016v1",
    "authors": [
      "Akshat Ghiya",
      "Ali K. AlShami",
      "Jugal Kalita"
    ],
    "published": "2025-03-11T03:45:51+00:00",
    "summary": "Predicting pedestrian trajectories is essential for autonomous driving systems, as it significantly enhances safety and supports informed decision-making. Accurate predictions enable the prevention of collisions, anticipation of crossing intent, and improved overall system efficiency. In this study, we present SGNetPose+, an enhancement of the SGNet architecture designed to integrate skeleton information or body segment angles with bounding boxes to predict pedestrian trajectories from video data to avoid hazards in autonomous driving. Skeleton information was extracted using a pose estimation model, and joint angles were computed based on the extracted joint data. We also apply temporal data augmentation by horizontally flipping video frames to increase the dataset size and improve performance. Our approach achieves state-of-the-art results on the JAAD and PIE datasets using pose data with the bounding boxes, outperforming the SGNet model. Code is available on Github: SGNetPose+."
  },
  {
    "title": "STEAD: Spatio-Temporal Efficient Anomaly Detection for Time and Compute Sensitive Applications",
    "url": "http://arxiv.org/abs/2503.07942v1",
    "arxiv_id": "2503.07942v1",
    "authors": [
      "Andrew Gao",
      "Jun Liu"
    ],
    "published": "2025-03-11T00:48:32+00:00",
    "summary": "This paper presents a new method for anomaly detection in automated systems with time and compute sensitive requirements, such as autonomous driving, with unparalleled efficiency. As systems like autonomous driving become increasingly popular, ensuring their safety has become more important than ever. Therefore, this paper focuses on how to quickly and effectively detect various anomalies in the aforementioned systems, with the goal of making them safer and more effective. Many detection systems have been developed with great success under spatial contexts; however, there is still significant room for improvement when it comes to temporal context. While there is substantial work regarding this task, there is minimal work done regarding the efficiency of models and their ability to be applied to scenarios that require real-time inference, i.e., autonomous driving where anomalies need to be detected the moment they are within view. To address this gap, we propose STEAD (Spatio-Temporal Efficient Anomaly Detection), whose backbone is developed using (2+1)D Convolutions and Performer Linear Attention, which ensures computational efficiency without sacrificing performance. When tested on the UCF-Crime benchmark, our base model achieves an AUC of 91.34%, outperforming the previous state-of-the-art, and our fast version achieves an AUC of 88.87%, while having 99.70% less parameters and outperforming the previous state-of-the-art as well. The code and pretrained models are made publicly available at https://github.com/agao8/STEAD"
  },
  {
    "title": "Intelligent Framework for Human-Robot Collaboration: Safety, Dynamic Ergonomics, and Adaptive Decision-Making",
    "url": "http://arxiv.org/abs/2503.07901v1",
    "arxiv_id": "2503.07901v1",
    "authors": [
      "Francesco Iodice",
      "Elena De Momi",
      "Arash Ajoudani"
    ],
    "published": "2025-03-10T22:43:07+00:00",
    "summary": "The integration of collaborative robots into industrial environments has improved productivity, but has also highlighted significant challenges related to operator safety and ergonomics. This paper proposes an innovative framework that integrates advanced visual perception technologies, real-time ergonomic monitoring, and Behaviour Tree (BT)-based adaptive decision-making. Unlike traditional methods, which often operate in isolation or statically, our approach combines deep learning models (YOLO11 and SlowOnly), advanced tracking (Unscented Kalman Filter) and dynamic ergonomic assessments (OWAS), offering a modular, scalable and adaptive system. Experimental results show that the framework outperforms previous methods in several aspects: accuracy in detecting postures and actions, adaptivity in managing human-robot interactions, and ability to reduce ergonomic risk through timely robotic interventions. In particular, the visual perception module showed superiority over YOLOv9 and YOLOv8, while real-time ergonomic monitoring eliminated the limitations of static analysis. Adaptive role management, made possible by the Behaviour Tree, provided greater responsiveness than rule-based systems, making the framework suitable for complex industrial scenarios. Our system demonstrated a 92.5\\% accuracy in grasping intention recognition and successfully classified ergonomic risks with real-time responsiveness (average latency of 0.57 seconds), enabling timely robotic"
  },
  {
    "title": "Safety Guardrails for LLM-Enabled Robots",
    "url": "http://arxiv.org/abs/2503.07885v1",
    "arxiv_id": "2503.07885v1",
    "authors": [
      "Zachary Ravichandran",
      "Alexander Robey",
      "Vijay Kumar",
      "George J. Pappas",
      "Hamed Hassani"
    ],
    "published": "2025-03-10T22:01:56+00:00",
    "summary": "Although the integration of large language models (LLMs) into robotics has unlocked transformative capabilities, it has also introduced significant safety concerns, ranging from average-case LLM errors (e.g., hallucinations) to adversarial jailbreaking attacks, which can produce harmful robot behavior in real-world settings. Traditional robot safety approaches do not address the novel vulnerabilities of LLMs, and current LLM safety guardrails overlook the physical risks posed by robots operating in dynamic real-world environments. In this paper, we propose RoboGuard, a two-stage guardrail architecture to ensure the safety of LLM-enabled robots. RoboGuard first contextualizes pre-defined safety rules by grounding them in the robot's environment using a root-of-trust LLM, which employs chain-of-thought (CoT) reasoning to generate rigorous safety specifications, such as temporal logic constraints. RoboGuard then resolves potential conflicts between these contextual safety specifications and a possibly unsafe plan using temporal logic control synthesis, which ensures safety compliance while minimally violating user preferences. Through extensive simulation and real-world experiments that consider worst-case jailbreaking attacks, we demonstrate that RoboGuard reduces the execution of unsafe plans from 92% to below 2.5% without compromising performance on safe plans. We also demonstrate that RoboGuard is resource-efficient, robust against adaptive attacks, and significantly enhanced by enabling its root-of-trust LLM to perform CoT reasoning. These results underscore the potential of RoboGuard to mitigate the safety risks and enhance the reliability of LLM-enabled robots."
  },
  {
    "title": "Health Prognostics in Multi-sensor Systems Based on Multivariate Functional Data Analysis",
    "url": "http://arxiv.org/abs/2503.07854v1",
    "arxiv_id": "2503.07854v1",
    "authors": [
      "Cevahir Yildirim",
      "Alba M. Franco-Pereira",
      "Rosa E. Lillo"
    ],
    "published": "2025-03-10T21:00:48+00:00",
    "summary": "Recent developments in big data analysis, machine learning, Industry 4.0, and IoT applications have enabled the monitoring and processing of multi-sensor data collected from systems, allowing for the prediction of the \"Remaining Useful Life\" (RUL) of system components. Particularly in the aviation industry, Prognostic Health Management (PHM) has become one of the most important practices for ensuring reliability and safety. Not only is the accuracy of RUL prediction important, but the implementability of techniques, domain adaptability, and interpretability of system degradation behaviors have also become essential. In this paper, the data collected from the multi-sensor environment of complex systems are processed using a Functional Data Analysis (FDA) approach to predict when the systems will fail and to understand and interpret the systems' life cycles. The approach is applied to the C-MAPSS datasets shared by National Aeronautics and Space Administration, and the behaviors of the sensors in aircraft engine failures are adaptively modeled with Multivariate Functional Principal Component Analysis (MFPCA). While the results indicate that the proposed method predicts the RUL competitively compared to other methods in the literature, it also demonstrates how multivariate Functional Data Analysis is useful for interpretability in prognostic studies within multi-sensor environments."
  },
  {
    "title": "Safe Explicable Policy Search",
    "url": "http://arxiv.org/abs/2503.07848v1",
    "arxiv_id": "2503.07848v1",
    "authors": [
      "Akkamahadevi Hanni",
      "Jonathan Monta\u00f1o",
      "Yu Zhang"
    ],
    "published": "2025-03-10T20:52:41+00:00",
    "summary": "When users work with AI agents, they form conscious or subconscious expectations of them. Meeting user expectations is crucial for such agents to engage in successful interactions and teaming. However, users may form expectations of an agent that differ from the agent's planned behaviors. These differences lead to the consideration of two separate decision models in the planning process to generate explicable behaviors. However, little has been done to incorporate safety considerations, especially in a learning setting. We present Safe Explicable Policy Search (SEPS), which aims to provide a learning approach to explicable behavior generation while minimizing the safety risk, both during and after learning. We formulate SEPS as a constrained optimization problem where the agent aims to maximize an explicability score subject to constraints on safety and a suboptimality criterion based on the agent's model. SEPS innovatively combines the capabilities of Constrained Policy Optimization and Explicable Policy Search. We evaluate SEPS in safety-gym environments and with a physical robot experiment to show that it can learn explicable behaviors that adhere to the agent's safety requirements and are efficient. Results show that SEPS can generate safe and explicable behaviors while ensuring a desired level of performance w.r.t. the agent's objective, and has real-world relevance in human-AI teaming."
  },
  {
    "title": "Improving Pedestrian Safety at Intersections Using Probabilistic Models and Monte Carlo Simulations",
    "url": "http://arxiv.org/abs/2503.07805v1",
    "arxiv_id": "2503.07805v1",
    "authors": [
      "Alben Rome Bagabaldo",
      "J\u00fcrgen Hackl"
    ],
    "published": "2025-03-10T19:39:18+00:00",
    "summary": "National Highway Traffic Safety Administration reported 7,345 pedestrian fatalities in the United States in 2022, making pedestrian safety a pressing issue in urban mobility. This study presents a novel probabilistic simulation framework integrating dynamic pedestrian crossing models and Monte Carlo simulations to evaluate safety under varying traffic conditions. The framework captures key influences on pedestrian decisions, such as traffic light states, vehicle proximity, and waiting times, while employing the Intelligent Driver Model (IDM) to simulate realistic vehicle dynamics. Results from 500 trials show that pedestrians avoid crossing during green lights, reducing collision risks, while shorter waiting times during red lights encourage safer crossings. The risk is heightened during yellow lights, especially with nearby vehicles. This research emphasizes the importance of adaptive traffic control measures, such as pedestrian-triggered signals and enhanced traffic light timing, to mitigate risks and prioritize pedestrian safety. By modeling realistic interactions between pedestrians and vehicles, the study offers insights for designing safer and more sustainable urban intersections."
  },
  {
    "title": "A Simple Approach to Constraint-Aware Imitation Learning with Application to Autonomous Racing",
    "url": "http://arxiv.org/abs/2503.07737v1",
    "arxiv_id": "2503.07737v1",
    "authors": [
      "Shengfan Cao",
      "Eunhyek Joa",
      "Francesco Borrelli"
    ],
    "published": "2025-03-10T18:00:16+00:00",
    "summary": "Guaranteeing constraint satisfaction is challenging in imitation learning (IL), particularly in tasks that require operating near a system's handling limits. Traditional IL methods often struggle to enforce constraints, leading to suboptimal performance in high-precision tasks. In this paper, we present a simple approach to incorporating safety into the IL objective. Through simulations, we empirically validate our approach on an autonomous racing task with both full-state and image feedback, demonstrating improved constraint satisfaction and greater consistency in task performance compared to a baseline method."
  },
  {
    "title": "AlphaDrive: Unleashing the Power of VLMs in Autonomous Driving via Reinforcement Learning and Reasoning",
    "url": "http://arxiv.org/abs/2503.07608v1",
    "arxiv_id": "2503.07608v1",
    "authors": [
      "Bo Jiang",
      "Shaoyu Chen",
      "Qian Zhang",
      "Wenyu Liu",
      "Xinggang Wang"
    ],
    "published": "2025-03-10T17:59:42+00:00",
    "summary": "OpenAI o1 and DeepSeek R1 achieve or even surpass human expert-level performance in complex domains like mathematics and science, with reinforcement learning (RL) and reasoning playing a crucial role. In autonomous driving, recent end-to-end models have greatly improved planning performance but still struggle with long-tailed problems due to limited common sense and reasoning abilities. Some studies integrate vision-language models (VLMs) into autonomous driving, but they typically rely on pre-trained models with simple supervised fine-tuning (SFT) on driving data, without further exploration of training strategies or optimizations specifically tailored for planning. In this paper, we propose AlphaDrive, a RL and reasoning framework for VLMs in autonomous driving. AlphaDrive introduces four GRPO-based RL rewards tailored for planning and employs a two-stage planning reasoning training strategy that combines SFT with RL. As a result, AlphaDrive significantly improves both planning performance and training efficiency compared to using only SFT or without reasoning. Moreover, we are also excited to discover that, following RL training, AlphaDrive exhibits some emergent multimodal planning capabilities, which is critical for improving driving safety and efficiency. To the best of our knowledge, AlphaDrive is the first to integrate GRPO-based RL with planning reasoning into autonomous driving. Code will be released to facilitate future research."
  },
  {
    "title": "Implicit Reasoning in Transformers is Reasoning through Shortcuts",
    "url": "http://arxiv.org/abs/2503.07604v1",
    "arxiv_id": "2503.07604v1",
    "authors": [
      "Tianhe Lin",
      "Jian Xie",
      "Siyu Yuan",
      "Deqing Yang"
    ],
    "published": "2025-03-10T17:58:31+00:00",
    "summary": "Test-time compute is emerging as a new paradigm for enhancing language models' complex multi-step reasoning capabilities, as demonstrated by the success of OpenAI's o1 and o3, as well as DeepSeek's R1. Compared to explicit reasoning in test-time compute, implicit reasoning is more inference-efficient, requiring fewer generated tokens. However, why does the advanced reasoning capability fail to emerge in the implicit reasoning style? In this work, we train GPT-2 from scratch on a curated multi-step mathematical reasoning dataset and conduct analytical experiments to investigate how language models perform implicit reasoning in multi-step tasks. Our findings reveal: 1) Language models can perform step-by-step reasoning and achieve high accuracy in both in-domain and out-of-domain tests via implicit reasoning. However, this capability only emerges when trained on fixed-pattern data. 2) Conversely, implicit reasoning abilities emerging from training on unfixed-pattern data tend to overfit a specific pattern and fail to generalize further. Notably, this limitation is also observed in state-of-the-art large language models. These findings suggest that language models acquire implicit reasoning through shortcut learning, enabling strong performance on tasks with similar patterns while lacking generalization."
  },
  {
    "title": "PointVLA: Injecting the 3D World into Vision-Language-Action Models",
    "url": "http://arxiv.org/abs/2503.07511v1",
    "arxiv_id": "2503.07511v1",
    "authors": [
      "Chengmeng Li",
      "Junjie Wen",
      "Yan Peng",
      "Yaxin Peng",
      "Feifei Feng",
      "Yichen Zhu"
    ],
    "published": "2025-03-10T16:32:41+00:00",
    "summary": "Vision-Language-Action (VLA) models excel at robotic tasks by leveraging large-scale 2D vision-language pretraining, but their reliance on RGB images limits spatial reasoning critical for real-world interaction. Retraining these models with 3D data is computationally prohibitive, while discarding existing 2D datasets wastes valuable resources. To bridge this gap, we propose PointVLA, a framework that enhances pre-trained VLAs with point cloud inputs without requiring retraining. Our method freezes the vanilla action expert and injects 3D features via a lightweight modular block. To identify the most effective way of integrating point cloud representations, we conduct a skip-block analysis to pinpoint less useful blocks in the vanilla action expert, ensuring that 3D features are injected only into these blocks--minimizing disruption to pre-trained representations.   Extensive experiments demonstrate that PointVLA outperforms state-of-the-art 2D imitation learning methods, such as OpenVLA, Diffusion Policy and DexVLA, across both simulated and real-world robotic tasks. Specifically, we highlight several key advantages of PointVLA enabled by point cloud integration: (1) Few-shot multi-tasking, where PointVLA successfully performs four different tasks using only 20 demonstrations each; (2) Real-vs-photo discrimination, where PointVLA distinguishes real objects from their images, leveraging 3D world knowledge to improve safety and reliability; (3) Height adaptability, Unlike conventional 2D imitation learning methods, PointVLA enables robots to adapt to objects at varying table height that unseen in train data. Furthermore, PointVLA achieves strong performance in long-horizon tasks, such as picking and packing objects from a moving conveyor belt, showcasing its ability to generalize across complex, dynamic environments."
  },
  {
    "title": "Securing External Deeper-than-black-box GPAI Evaluations",
    "url": "http://arxiv.org/abs/2503.07496v1",
    "arxiv_id": "2503.07496v1",
    "authors": [
      "Alejandro Tlaie",
      "Jimmy Farrell"
    ],
    "published": "2025-03-10T16:13:45+00:00",
    "summary": "This paper examines the critical challenges and potential solutions for conducting secure and effective external evaluations of general-purpose AI (GPAI) models. With the exponential growth in size, capability, reach and accompanying risk of these models, ensuring accountability, safety, and public trust requires frameworks that go beyond traditional black-box methods. The discussion begins with an analysis of the need for deeper-than-black-box evaluations (Section I), emphasizing the importance of understanding model internals to uncover latent risks and ensure compliance with ethical and regulatory standards. Building on this foundation, Section II addresses the security considerations of remote evaluations, outlining the threat landscape, technical solutions, and safeguards necessary to protect both evaluators and proprietary model data. Finally, Section III synthesizes these insights into actionable recommendations and future directions, aiming to establish a robust, scalable, and transparent framework for external assessments in GPAI governance."
  },
  {
    "title": "Destination Calculus: A Linear \u03bb-Calculus for Purely Functional Memory Writes",
    "url": "http://arxiv.org/abs/2503.07489v1",
    "arxiv_id": "2503.07489v1",
    "authors": [
      "Thomas Bagrel",
      "Arnaud Spiwack"
    ],
    "published": "2025-03-10T16:08:47+00:00",
    "summary": "Destination passing -- aka. out parameters -- is taking a parameter to fill rather than returning a result from a function. Due to its apparently imperative nature, destination passing has struggled to find its way to pure functional programming. In this paper, we present a pure functional calculus with destinations at its core. Our calculus subsumes all the similar systems, and can be used to reason about their correctness or extension. In addition, our calculus can express programs that were previously not known to be expressible in a pure language. This is guaranteed by a modal type system where modes are used to manage both linearity and scopes. Type safety of our core calculus was proved formally with the Coq proof assistant."
  },
  {
    "title": "MedAgentsBench: Benchmarking Thinking Models and Agent Frameworks for Complex Medical Reasoning",
    "url": "http://arxiv.org/abs/2503.07459v1",
    "arxiv_id": "2503.07459v1",
    "authors": [
      "Xiangru Tang",
      "Daniel Shao",
      "Jiwoong Sohn",
      "Jiapeng Chen",
      "Jiayi Zhang",
      "Jinyu Xiang",
      "Fang Wu",
      "Yilun Zhao",
      "Chenglin Wu",
      "Wenqi Shi",
      "Arman Cohan",
      "Mark Gerstein"
    ],
    "published": "2025-03-10T15:38:44+00:00",
    "summary": "Large Language Models (LLMs) have shown impressive performance on existing medical question-answering benchmarks. This high performance makes it increasingly difficult to meaningfully evaluate and differentiate advanced methods. We present MedAgentsBench, a benchmark that focuses on challenging medical questions requiring multi-step clinical reasoning, diagnosis formulation, and treatment planning-scenarios where current models still struggle despite their strong performance on standard tests. Drawing from seven established medical datasets, our benchmark addresses three key limitations in existing evaluations: (1) the prevalence of straightforward questions where even base models achieve high performance, (2) inconsistent sampling and evaluation protocols across studies, and (3) lack of systematic analysis of the interplay between performance, cost, and inference time. Through experiments with various base models and reasoning methods, we demonstrate that the latest thinking models, DeepSeek R1 and OpenAI o3, exhibit exceptional performance in complex medical reasoning tasks. Additionally, advanced search-based agent methods offer promising performance-to-cost ratios compared to traditional approaches. Our analysis reveals substantial performance gaps between model families on complex questions and identifies optimal model selections for different computational constraints. Our benchmark and evaluation framework are publicly available at https://github.com/gersteinlab/medagents-benchmark."
  },
  {
    "title": "CATPlan: Loss-based Collision Prediction in End-to-End Autonomous Driving",
    "url": "http://arxiv.org/abs/2503.07425v1",
    "arxiv_id": "2503.07425v1",
    "authors": [
      "Ziliang Xiong",
      "Shipeng Liu",
      "Nathaniel Helgesen",
      "Joakim Johnander",
      "Per-Erik Forssen"
    ],
    "published": "2025-03-10T15:10:40+00:00",
    "summary": "In recent years, there has been increased interest in the design, training, and evaluation of end-to-end autonomous driving (AD) systems. One often overlooked aspect is the uncertainty of planned trajectories predicted by these systems, despite awareness of their own uncertainty being key to achieve safety and robustness. We propose to estimate this uncertainty by adapting loss prediction from the uncertainty quantification literature. To this end, we introduce a novel light-weight module, dubbed CATPlan, that is trained to decode motion and planning embeddings into estimates of the collision loss used to partially supervise end-to-end AD systems. During inference, these estimates are interpreted as collision risk. We evaluate CATPlan on the safety-critical, nerf-based, closed-loop benchmark NeuroNCAP and find that it manages to detect collisions with a $54.8\\%$ relative improvement to average precision over a GMM-based baseline in which the predicted trajectory is compared to the forecasted trajectories of other road users. Our findings indicate that the addition of CATPlan can lead to safer end-to-end AD systems and hope that our work will spark increased interest in uncertainty quantification for such systems."
  },
  {
    "title": "Towards Safe Robot Foundation Models",
    "url": "http://arxiv.org/abs/2503.07404v1",
    "arxiv_id": "2503.07404v1",
    "authors": [
      "Maximilian T\u00f6lle",
      "Theo Gruner",
      "Daniel Palenicek",
      "Jonas G\u00fcnster",
      "Puze Liu",
      "Joe Watson",
      "Davide Tateo",
      "Jan Peters"
    ],
    "published": "2025-03-10T14:55:09+00:00",
    "summary": "Robot foundation models hold the potential for deployment across diverse environments, from industrial applications to household tasks. While current research focuses primarily on the policies' generalization capabilities across a variety of tasks, it fails to address safety, a critical requirement for deployment on real-world systems. In this paper, we introduce a safety layer designed to constrain the action space of any generalist policy appropriately. Our approach uses ATACOM, a safe reinforcement learning algorithm that creates a safe action space and, therefore, ensures safe state transitions. By extending ATACOM to generalist policies, our method facilitates their deployment in safety-critical scenarios without requiring any specific safety fine-tuning. We demonstrate the effectiveness of this safety layer in an air hockey environment, where it prevents a puck-hitting agent from colliding with its surroundings, a failure observed in generalist policies."
  },
  {
    "title": "ECNN: A Low-complex, Adjustable CNN for Industrial Pump Monitoring Using Vibration Data",
    "url": "http://arxiv.org/abs/2503.07401v1",
    "arxiv_id": "2503.07401v1",
    "authors": [
      "Jonas Ney",
      "Norbert Wehn"
    ],
    "published": "2025-03-10T14:49:37+00:00",
    "summary": "Industrial pumps are essential components in various sectors, such as manufacturing, energy production, and water treatment, where their failures can cause significant financial and safety risks. Anomaly detection can be used to reduce those risks and increase reliability. In this work, we propose a novel enhanced convolutional neural network (ECNN) to predict the failure of an industrial pump based on the vibration data captured by an acceleration sensor. The convolutional neural network (CNN) is designed with a focus on low complexity to enable its implementation on edge devices with limited computational resources. Therefore, a detailed design space exploration is performed to find a topology satisfying the trade-off between complexity and accuracy. Moreover, to allow for adaptation to unknown pumps, our algorithm features a pump-specific parameter that can be determined by a small set of normal data samples. Finally, we combine the ECNN with a threshold approach to further increase the performance and satisfy the application requirements. As a result, our combined approach significantly outperforms a traditional statistical approach and a classical CNN in terms of accuracy. To summarize, this work provides a novel, low-complex, CNN-based algorithm that is enhanced by classical methods to offer high accuracy for anomaly detection of industrial pumps."
  },
  {
    "title": "AttentionSwarm: Reinforcement Learning with Attention Control Barier Function for Crazyflie Drones in Dynamic Environments",
    "url": "http://arxiv.org/abs/2503.07376v1",
    "arxiv_id": "2503.07376v1",
    "authors": [
      "Grik Tadevosyan",
      "Valerii Serpiva",
      "Aleksey Fedoseev",
      "Roohan Ahmed Khan",
      "Demetros Aschu",
      "Faryal Batool",
      "Nickolay Efanov",
      "Artem Mikhaylov",
      "Dzmitry Tsetserukou"
    ],
    "published": "2025-03-10T14:30:59+00:00",
    "summary": "We introduce AttentionSwarm, a novel benchmark designed to evaluate safe and efficient swarm control across three challenging environments: a landing environment with obstacles, a competitive drone game setting, and a dynamic drone racing scenario. Central to our approach is the Attention Model Based Control Barrier Function (CBF) framework, which integrates attention mechanisms with safety-critical control theory to enable real-time collision avoidance and trajectory optimization. This framework dynamically prioritizes critical obstacles and agents in the swarms vicinity using attention weights, while CBFs formally guarantee safety by enforcing collision-free constraints. The safe attention net algorithm was developed and evaluated using a swarm of Crazyflie 2.1 micro quadrotors, which were tested indoors with the Vicon motion capture system to ensure precise localization and control. Experimental results show that our system achieves landing accuracy of 3.02 cm with a mean time of 23 s and collision-free landings in a dynamic landing environment, 100% and collision-free navigation in a drone game environment, and 95% and collision-free navigation for a dynamic multiagent drone racing environment, underscoring its effectiveness and robustness in real-world scenarios. This work offers a promising foundation for applications in dynamic environments where safety and fastness are paramount."
  },
  {
    "title": "MM-Eureka: Exploring Visual Aha Moment with Rule-based Large-scale Reinforcement Learning",
    "url": "http://arxiv.org/abs/2503.07365v1",
    "arxiv_id": "2503.07365v1",
    "authors": [
      "Fanqing Meng",
      "Lingxiao Du",
      "Zongkai Liu",
      "Zhixiang Zhou",
      "Quanfeng Lu",
      "Daocheng Fu",
      "Botian Shi",
      "Wenhai Wang",
      "Junjun He",
      "Kaipeng Zhang",
      "Ping Luo",
      "Yu Qiao",
      "Qiaosheng Zhang",
      "Wenqi Shao"
    ],
    "published": "2025-03-10T14:23:12+00:00",
    "summary": "We present MM-Eureka, a multimodal reasoning model that successfully extends large-scale rule-based reinforcement learning (RL) to multimodal reasoning. While rule-based RL has shown remarkable success in improving LLMs' reasoning abilities in text domains, its application to multimodal settings has remained challenging. Our work reproduces key characteristics of text-based RL systems like DeepSeek-R1 in the multimodal space, including steady increases in accuracy reward and response length, and the emergence of reflection behaviors. We demonstrate that both instruction-tuned and pre-trained models can develop strong multimodal reasoning capabilities through rule-based RL without supervised fine-tuning, showing superior data efficiency compared to alternative approaches. We open-source our complete pipeline to foster further research in this area. We release all our codes, models, data, etc. at https://github.com/ModalMinds/MM-EUREKA"
  },
  {
    "title": "The Economics of p(doom): Scenarios of Existential Risk and Economic Growth in the Age of Transformative AI",
    "url": "http://arxiv.org/abs/2503.07341v1",
    "arxiv_id": "2503.07341v1",
    "authors": [
      "Jakub Growiec",
      "Klaus Prettner"
    ],
    "published": "2025-03-10T13:53:39+00:00",
    "summary": "Recent advances in artificial intelligence (AI) have led to a diverse set of predictions about its long-term impact on humanity. A central focus is the potential emergence of transformative AI (TAI), eventually capable of outperforming humans in all economically valuable tasks and fully automating labor. Discussed scenarios range from human extinction after a misaligned TAI takes over (\"AI doom\") to unprecedented economic growth and abundance (\"post-scarcity\"). However, the probabilities and implications of these scenarios remain highly uncertain. Here, we organize the various scenarios and evaluate their associated existential risks and economic outcomes in terms of aggregate welfare. Our analysis shows that even low-probability catastrophic outcomes justify large investments in AI safety and alignment research. We find that the optimizing representative individual would rationally allocate substantial resources to mitigate extinction risk; in some cases, she would prefer not to develop TAI at all. This result highlights that current global efforts in AI safety and alignment research are vastly insufficient relative to the scale and urgency of existential risks posed by TAI. Our findings therefore underscore the need for stronger safeguards to balance the potential economic benefits of TAI with the prevention of irreversible harm. Addressing these risks is crucial for steering technological progress toward sustainable human prosperity."
  },
  {
    "title": "Mitigating Hallucinations in YOLO-based Object Detection Models: A Revisit to Out-of-Distribution Detection",
    "url": "http://arxiv.org/abs/2503.07330v1",
    "arxiv_id": "2503.07330v1",
    "authors": [
      "Weicheng He",
      "Changshun Wu",
      "Chih-Hong Cheng",
      "Xiaowei Huang",
      "Saddek Bensalem"
    ],
    "published": "2025-03-10T13:42:41+00:00",
    "summary": "Object detection systems must reliably perceive objects of interest without being overly confident to ensure safe decision-making in dynamic environments. Filtering techniques based on out-of-distribution (OoD) detection are commonly added as an extra safeguard to filter hallucinations caused by overconfidence in novel objects. Nevertheless, evaluating YOLO-family detectors and their filters under existing OoD benchmarks often leads to unsatisfactory performance. This paper studies the underlying reasons for performance bottlenecks and proposes a methodology to improve performance fundamentally. Our first contribution is a calibration of all existing evaluation results: Although images in existing OoD benchmark datasets are claimed not to have objects within in-distribution (ID) classes (i.e., categories defined in the training dataset), around 13% of objects detected by the object detector are actually ID objects. Dually, the ID dataset containing OoD objects can also negatively impact the decision boundary of filters. These ultimately lead to a significantly imprecise performance estimation. Our second contribution is to consider the task of hallucination reduction as a joint pipeline of detectors and filters. By developing a methodology to carefully synthesize an OoD dataset that semantically resembles the objects to be detected, and using the crafted OoD dataset in the fine-tuning of YOLO detectors to suppress the objectness score, we achieve a 88% reduction in overall hallucination error with a combined fine-tuned detection and filtering system on the self-driving benchmark BDD-100K. Our code and dataset are available at: https://gricad-gitlab.univ-grenoble-alpes.fr/dnn-safety/m-hood."
  },
  {
    "title": "Complete the Cycle: Reachability Types with Expressive Cyclic References",
    "url": "http://arxiv.org/abs/2503.07328v1",
    "arxiv_id": "2503.07328v1",
    "authors": [
      "Haotian Deng",
      "Siyuan He",
      "Songlin Jia",
      "Yuyan Bao",
      "Tiark Rompf"
    ],
    "published": "2025-03-10T13:42:02+00:00",
    "summary": "Reachability Types (RT) are a qualified type system for tracking aliasing and separation in functional and higher-order programming. By formalizing resource reachability with a sound static type system, RT enable higher-order programming patterns with runtime safety and non-interference guarantees. However, previous RT systems have been based on calculi that restrict cyclic dependencies and are shown to be terminating in the absence of built-in recursive constructs. While termination is sometimes a desirable property, simplifying reasoning and ensuring predictable behavior, it implies an inability to encode expressive programs involving non-termination and advanced recursive patterns, such as mutual recursion and various fixed-point combinators.   In this paper, we address this limitation by extending RT with an expressive cyclic reference type that permits the formation of cyclic dependencies through the store, thereby allowing the system to encode recursive programming patterns without relying on extra built-in constructs. In addition, we redesign qualifier typing in the reference introduction rule, allowing separate references to point to a shared and tracked referent. We formalize the system as the $\\lambda^{\\circ}_{<:}$-calculus, with a mechanized soundness proof via the standard progress and preservation lemmas. As a demonstration, we implement a well-typed fixpoint operator, proving that recursive patterns can be encoded using the novel cyclic reference type."
  },
  {
    "title": "Benchmarking Chinese Medical LLMs: A Medbench-based Analysis of Performance Gaps and Hierarchical Optimization Strategies",
    "url": "http://arxiv.org/abs/2503.07306v1",
    "arxiv_id": "2503.07306v1",
    "authors": [
      "Luyi Jiang",
      "Jiayuan Chen",
      "Lu Lu",
      "Xinwei Peng",
      "Lihao Liu",
      "Junjun He",
      "Jie Xu"
    ],
    "published": "2025-03-10T13:28:25+00:00",
    "summary": "The evaluation and improvement of medical large language models (LLMs) are critical for their real-world deployment, particularly in ensuring accuracy, safety, and ethical alignment. Existing frameworks inadequately dissect domain-specific error patterns or address cross-modal challenges. This study introduces a granular error taxonomy through systematic analysis of top 10 models on MedBench, categorizing incorrect responses into eight types: Omissions, Hallucination, Format Mismatch, Causal Reasoning Deficiency, Contextual Inconsistency, Unanswered, Output Error, and Deficiency in Medical Language Generation. Evaluation of 10 leading models reveals vulnerabilities: despite achieving 0.86 accuracy in medical knowledge recall, critical reasoning tasks show 96.3% omission, while safety ethics evaluations expose alarming inconsistency (robustness score: 0.79) under option shuffled. Our analysis uncovers systemic weaknesses in knowledge boundary enforcement and multi-step reasoning. To address these, we propose a tiered optimization strategy spanning four levels, from prompt engineering and knowledge-augmented retrieval to hybrid neuro-symbolic architectures and causal reasoning frameworks. This work establishes an actionable roadmap for developing clinically robust LLMs while redefining evaluation paradigms through error-driven insights, ultimately advancing the safety and trustworthiness of AI in high-stakes medical environments."
  },
  {
    "title": "What is missing from existing Lithium-Sulfur models to capture coin-cell behaviour?",
    "url": "http://arxiv.org/abs/2503.07684v1",
    "arxiv_id": "2503.07684v1",
    "authors": [
      "Miss. Elizabeth Olisa Monica Marinescu"
    ],
    "published": "2025-03-10T12:24:20+00:00",
    "summary": "Lithium-sulfur (Li-S) batteries offer a promising alternative to current lithium-ion (Li-ion) batteries, with a high theoretical energy density, improved safety and high abundance, low cost of materials. For Li-S to reach commercial application, it is essential to understand how the behaviour scales between cell formats; new material development is predominately completed at coin-cell level, whilst pouch-cells will be used for commercial applications. Differences such as reduced electrolyte-to-sulfur (E/S) ratios and increased geometric size at larger cell formats contribute to the behavioural differences, in terms of achievable capacity, cyclability and potential degradation mechanisms.   This work focuses on the steps required to capture and test coin-cell behaviour, building upon the existing models within the literature, which predominately focus on pouch-cells. The areas investigated throughout this study, to improve the capability of the model in terms of scaling ability and causality of predictions, include the cathode surface area, precipitation dynamics and C-rate dependence."
  },
  {
    "title": "Learning and planning for optimal synergistic human-robot coordination in manufacturing contexts",
    "url": "http://arxiv.org/abs/2503.07238v1",
    "arxiv_id": "2503.07238v1",
    "authors": [
      "Samuele Sandrini",
      "Marco Faroni",
      "Nicola Pedrocchi"
    ],
    "published": "2025-03-10T12:20:29+00:00",
    "summary": "Collaborative robotics cells leverage heterogeneous agents to provide agile production solutions. Effective coordination is essential to prevent inefficiencies and risks for human operators working alongside robots. This paper proposes a human-aware task allocation and scheduling model based on Mixed Integer Nonlinear Programming to optimize efficiency and safety starting from task planning stages. The approach exploits synergies that encode the coupling effects between pairs of tasks executed in parallel by the agents, arising from the safety constraints imposed on robot agents. These terms are learned from previous executions using a Bayesian estimation; the inference of the posterior probability distribution of the synergy coefficients is performed using the Markov Chain Monte Carlo method. The synergy enhances task planning by adapting the nominal duration of the plan according to the effect of the operator's presence. Simulations and experimental results demonstrate that the proposed method produces improved human-aware task plans, reducing unuseful interference between agents, increasing human-robot distance, and achieving up to an 18\\% reduction in process execution time."
  },
  {
    "title": "Reactive and Safety-Aware Path Replanning for Collaborative Applications",
    "url": "http://arxiv.org/abs/2503.07192v1",
    "arxiv_id": "2503.07192v1",
    "authors": [
      "Cesare Tonola",
      "Marco Faroni",
      "Saeed Abdolshah",
      "Mazin Hamad",
      "Sami Haddadin",
      "Nicola Pedrocchi",
      "Manuel Beschi"
    ],
    "published": "2025-03-10T11:22:33+00:00",
    "summary": "This paper addresses motion replanning in human-robot collaborative scenarios, emphasizing reactivity and safety-compliant efficiency. While existing human-aware motion planners are effective in structured environments, they often struggle with unpredictable human behavior, leading to safety measures that limit robot performance and throughput. In this study, we combine reactive path replanning and a safety-aware cost function, allowing the robot to adjust its path to changes in the human state. This solution reduces the execution time and the need for trajectory slowdowns without sacrificing safety. Simulations and real-world experiments show the method's effectiveness compared to standard human-robot cooperation approaches, with efficiency enhancements of up to 60\\%."
  },
  {
    "title": "Correctness Learning: Deductive Verification Guided Learning for Human-AI Collaboration",
    "url": "http://arxiv.org/abs/2503.07096v1",
    "arxiv_id": "2503.07096v1",
    "authors": [
      "Zhao Jin",
      "Lu Jin",
      "Yizhe Luo",
      "Shuo Feng",
      "Yucheng Shi",
      "Kai Zheng",
      "Xinde Yu",
      "Mingliang Xu"
    ],
    "published": "2025-03-10T09:20:38+00:00",
    "summary": "Despite significant progress in AI and decision-making technologies in safety-critical fields, challenges remain in verifying the correctness of decision output schemes and verification-result driven design. We propose correctness learning (CL) to enhance human-AI collaboration integrating deductive verification methods and insights from historical high-quality schemes. The typical pattern hidden in historical high-quality schemes, such as change of task priorities in shared resources, provides critical guidance for intelligent agents in learning and decision-making. By utilizing deductive verification methods, we proposed patten-driven correctness learning (PDCL), formally modeling and reasoning the adaptive behaviors-or 'correctness pattern'-of system agents based on historical high-quality schemes, capturing the logical relationships embedded within these schemes. Using this logical information as guidance, we establish a correctness judgment and feedback mechanism to steer the intelligent decision model toward the 'correctness pattern' reflected in historical high-quality schemes. Extensive experiments across multiple working conditions and core parameters validate the framework's components and demonstrate its effectiveness in improving decision-making and resource optimization."
  },
  {
    "title": "Multimodal Human-AI Synergy for Medical Imaging Quality Control: A Hybrid Intelligence Framework with Adaptive Dataset Curation and Closed-Loop Evaluation",
    "url": "http://arxiv.org/abs/2503.07032v1",
    "arxiv_id": "2503.07032v1",
    "authors": [
      "Zhi Qin",
      "Qianhui Gui",
      "Mouxiao Bian",
      "Rui Wang",
      "Hong Ge",
      "Dandan Yao",
      "Ziying Sun",
      "Yuan Zhao",
      "Yu Zhang",
      "Hui Shi",
      "Dongdong Wang",
      "Chenxin Song",
      "Shenghong Ju",
      "Lihao Liu",
      "Junjun He",
      "Jie Xu",
      "Yuan-Cheng Wang"
    ],
    "published": "2025-03-10T08:16:18+00:00",
    "summary": "Medical imaging quality control (QC) is essential for accurate diagnosis, yet traditional QC methods remain labor-intensive and subjective. To address this challenge, in this study, we establish a standardized dataset and evaluation framework for medical imaging QC, systematically assessing large language models (LLMs) in image quality assessment and report standardization. Specifically, we first constructed and anonymized a dataset of 161 chest X-ray (CXR) radiographs and 219 CT reports for evaluation. Then, multiple LLMs, including Gemini 2.0-Flash, GPT-4o, and DeepSeek-R1, were evaluated based on recall, precision, and F1 score to detect technical errors and inconsistencies. Experimental results show that Gemini 2.0-Flash achieved a Macro F1 score of 90 in CXR tasks, demonstrating strong generalization but limited fine-grained performance. DeepSeek-R1 excelled in CT report auditing with a 62.23\\% recall rate, outperforming other models. However, its distilled variants performed poorly, while InternLM2.5-7B-chat exhibited the highest additional discovery rate, indicating broader but less precise error detection. These findings highlight the potential of LLMs in medical imaging QC, with DeepSeek-R1 and Gemini 2.0-Flash demonstrating superior performance."
  },
  {
    "title": "Combating Partial Perception Deficit in Autonomous Driving with Multimodal LLM Commonsense",
    "url": "http://arxiv.org/abs/2503.07020v1",
    "arxiv_id": "2503.07020v1",
    "authors": [
      "Yuting Hu",
      "Chenhui Xu",
      "Ruiyang Qin",
      "Dancheng Liu",
      "Amir Nassereldine",
      "Yiyu Shi",
      "Jinjun Xiong"
    ],
    "published": "2025-03-10T08:01:41+00:00",
    "summary": "Partial perception deficits can compromise autonomous vehicle safety by disrupting environmental understanding. Current protocols typically respond with immediate stops or minimal-risk maneuvers, worsening traffic flow and lacking flexibility for rare driving scenarios. In this paper, we propose LLM-RCO, a framework leveraging large language models to integrate human-like driving commonsense into autonomous systems facing perception deficits. LLM-RCO features four key modules: hazard inference, short-term motion planner, action condition verifier, and safety constraint generator. These modules interact with the dynamic driving environment, enabling proactive and context-aware control actions to override the original control policy of autonomous agents. To improve safety in such challenging conditions, we construct DriveLM-Deficit, a dataset of 53,895 video clips featuring deficits of safety-critical objects, complete with annotations for LLM-based hazard inference and motion planning fine-tuning. Extensive experiments in adverse driving conditions with the CARLA simulator demonstrate that systems equipped with LLM-RCO significantly improve driving performance, highlighting its potential for enhancing autonomous driving resilience against adverse perception deficits. Our results also show that LLMs fine-tuned with DriveLM-Deficit can enable more proactive movements instead of conservative stops in the context of perception deficits."
  },
  {
    "title": "Explicit Solution of Tunable Input-to-State Safe-Based Controller Under High-Relative-Degree Constraints",
    "url": "http://arxiv.org/abs/2503.07007v1",
    "arxiv_id": "2503.07007v1",
    "authors": [
      "Yan Wei",
      "Yu Feng",
      "Linlin Ou",
      "Yueying Wang",
      "Xinyi Yu"
    ],
    "published": "2025-03-10T07:40:17+00:00",
    "summary": "This paper investigates the safety analysis and verification of nonlinear systems subject to high-relative-degree constraints and unknown disturbance. The closed-form solution of the high-order control barrier functions (HOCBF) optimization problem with and without a nominal controller is first provided, making it unnecessary to solve the quadratic program problem online and facilitating the analysis. Further, we introduce the concept of tunable input-to-state safety(ISSf), and a new tunable function in conjunction with HOCBF is provided. When combined with the existing ISSf theorem, produces controllers for constrained nonlinear systems with external disturbances. The theoretical results are proven and supported by numerical simulations."
  },
  {
    "title": "Parametric Value Approximation for General-sum Differential Games with State Constraints",
    "url": "http://arxiv.org/abs/2503.06994v1",
    "arxiv_id": "2503.06994v1",
    "authors": [
      "Lei Zhang",
      "Mukesh Ghimire",
      "Wenlong Zhang",
      "Zhe Xu",
      "Yi Ren"
    ],
    "published": "2025-03-10T07:19:02+00:00",
    "summary": "General-sum differential games can approximate values solved by Hamilton-Jacobi-Isaacs (HJI) equations for efficient inference when information is incomplete. However, solving such games through conventional methods encounters the curse of dimensionality (CoD). Physics-informed neural networks (PINNs) offer a scalable approach to alleviate the CoD and approximate values, but there exist convergence issues for value approximations through vanilla PINNs when state constraints lead to values with large Lipschitz constants, particularly in safety-critical applications. In addition to addressing CoD, it is necessary to learn a generalizable value across a parametric space of games, rather than training multiple ones for each specific player-type configuration. To overcome these challenges, we propose a Hybrid Neural Operator (HNO), which is an operator that can map parameter functions for games to value functions. HNO leverages informative supervised data and samples PDE-driven data across entire spatial-temporal space for model refinement. We evaluate HNO on 9D and 13D scenarios with nonlinear dynamics and state constraints, comparing it against a Supervised Neural Operator (a variant of DeepONet). Under the same computational budget and training data, HNO outperforms SNO for safety performance. This work provides a step toward scalable and generalizable value function approximation, enabling real-time inference for complex human-robot or multi-agent interactions."
  },
  {
    "title": "Utilizing Jailbreak Probability to Attack and Safeguard Multimodal LLMs",
    "url": "http://arxiv.org/abs/2503.06989v1",
    "arxiv_id": "2503.06989v1",
    "authors": [
      "Wenzhuo Xu",
      "Zhipeng Wei",
      "Xiongtao Sun",
      "Deyue Zhang",
      "Dongdong Yang",
      "Quanchen Zou",
      "Xiangzheng Zhang"
    ],
    "published": "2025-03-10T07:10:38+00:00",
    "summary": "Recently, Multimodal Large Language Models (MLLMs) have demonstrated their superior ability in understanding multimodal contents. However, they remain vulnerable to jailbreak attacks, which exploit weaknesses in their safety alignment to generate harmful responses. Previous studies categorize jailbreaks as successful or failed based on whether responses contain malicious content. However, given the stochastic nature of MLLM responses, this binary classification of an input's ability to jailbreak MLLMs is inappropriate. Derived from this viewpoint, we introduce jailbreak probability to quantify the jailbreak potential of an input, which represents the likelihood that MLLMs generated a malicious response when prompted with this input. We approximate this probability through multiple queries to MLLMs. After modeling the relationship between input hidden states and their corresponding jailbreak probability using Jailbreak Probability Prediction Network (JPPN), we use continuous jailbreak probability for optimization. Specifically, we propose Jailbreak-Probability-based Attack (JPA) that optimizes adversarial perturbations on inputs to maximize jailbreak probability. To counteract attacks, we also propose two defensive methods: Jailbreak-Probability-based Finetuning (JPF) and Jailbreak-Probability-based Defensive Noise (JPDN), which minimizes jailbreak probability in the MLLM parameters and input space, respectively. Extensive experiments show that (1) JPA yields improvements (up to 28.38\\%) under both white and black box settings compared to previous methods with small perturbation bounds and few iterations. (2) JPF and JPDN significantly reduce jailbreaks by at most over 60\\%. Both of the above results demonstrate the significance of introducing jailbreak probability to make nuanced distinctions among input jailbreak abilities."
  },
  {
    "title": "Lshan-1.0 Technical Report",
    "url": "http://arxiv.org/abs/2503.06949v1",
    "arxiv_id": "2503.06949v1",
    "authors": [
      "Haotian Chen",
      "Yanyu Xu",
      "Boyan Wang",
      "Chaoyue Zhao",
      "Xiaoyu Han",
      "Fang Wang",
      "Lizhen Cui",
      "Yonghui Xu"
    ],
    "published": "2025-03-10T05:54:23+00:00",
    "summary": "In this report, we introduce our first-generation reasoning model, Lshan-1.0, a large language model designed for the highly specialized Chinese legal domain, offering comprehensive capabilities to meet diverse realistic needs. Existing legal LLMs face two primary challenges. Firstly, their design and evaluation are predominantly driven by computer science perspectives, leading to insufficient incorporation of legal expertise and logic, which is crucial for high-precision legal applications, such as handling complex prosecutorial tasks. Secondly, these models often underperform due to a lack of comprehensive training data from the legal domain, limiting their ability to effectively address real-world legal scenarios. To address this, we first compile millions of legal documents covering over 20 types of crimes from 31 provinces in China for model training. From the extensive dataset, we further select high-quality for supervised fine-tuning, ensuring enhanced relevance and precision. The model further undergoes large-scale reinforcement learning without additional supervision, emphasizing the enhancement of its reasoning capabilities and explainability. To validate its effectiveness in complex legal applications, we also conduct human evaluations with legal experts. We develop fine-tuned models based on DeepSeek-R1-Distilled versions, available in three dense configurations: 14B, 32B, and 70B."
  },
  {
    "title": "LexPro-1.0 Technical Report",
    "url": "http://arxiv.org/abs/2503.06949v2",
    "arxiv_id": "2503.06949v2",
    "authors": [
      "Haotian Chen",
      "Yanyu Xu",
      "Boyan Wang",
      "Chaoyue Zhao",
      "Xiaoyu Han",
      "Fang Wang",
      "Lizhen Cui",
      "Yonghui Xu"
    ],
    "published": "2025-03-10T05:54:23+00:00",
    "summary": "In this report, we introduce our first-generation reasoning model, LexPro-1.0, a large language model designed for the highly specialized Chinese legal domain, offering comprehensive capabilities to meet diverse realistic needs. Existing legal LLMs face two primary challenges. Firstly, their design and evaluation are predominantly driven by computer science perspectives, leading to insufficient incorporation of legal expertise and logic, which is crucial for high-precision legal applications, such as handling complex prosecutorial tasks. Secondly, these models often underperform due to a lack of comprehensive training data from the legal domain, limiting their ability to effectively address real-world legal scenarios. To address this, we first compile millions of legal documents covering over 20 types of crimes from 31 provinces in China for model training. From the extensive dataset, we further select high-quality for supervised fine-tuning, ensuring enhanced relevance and precision. The model further undergoes large-scale reinforcement learning without additional supervision, emphasizing the enhancement of its reasoning capabilities and explainability. To validate its effectiveness in complex legal applications, we also conduct human evaluations with legal experts. We develop fine-tuned models based on DeepSeek-R1-Distilled versions, available in three dense configurations: 14B, 32B, and 70B."
  },
  {
    "title": "SafePlan: Leveraging Formal Logic and Chain-of-Thought Reasoning for Enhanced Safety in LLM-based Robotic Task Planning",
    "url": "http://arxiv.org/abs/2503.06892v1",
    "arxiv_id": "2503.06892v1",
    "authors": [
      "Ike Obi",
      "Vishnunandan L. N. Venkatesh",
      "Weizheng Wang",
      "Ruiqi Wang",
      "Dayoon Suh",
      "Temitope I. Amosa",
      "Wonse Jo",
      "Byung-Cheol Min"
    ],
    "published": "2025-03-10T03:37:36+00:00",
    "summary": "Robotics researchers increasingly leverage large language models (LLM) in robotics systems, using them as interfaces to receive task commands, generate task plans, form team coalitions, and allocate tasks among multi-robot and human agents. However, despite their benefits, the growing adoption of LLM in robotics has raised several safety concerns, particularly regarding executing malicious or unsafe natural language prompts. In addition, ensuring that task plans, team formation, and task allocation outputs from LLMs are adequately examined, refined, or rejected is crucial for maintaining system integrity. In this paper, we introduce SafePlan, a multi-component framework that combines formal logic and chain-of-thought reasoners for enhancing the safety of LLM-based robotics systems. Using the components of SafePlan, including Prompt Sanity COT Reasoner and Invariant, Precondition, and Postcondition COT reasoners, we examined the safety of natural language task prompts, task plans, and task allocation outputs generated by LLM-based robotic systems as means of investigating and enhancing system safety profile. Our results show that SafePlan outperforms baseline models by leading to 90.5% reduction in harmful task prompt acceptance while still maintaining reasonable acceptance of safe tasks."
  },
  {
    "title": "Graphormer-Guided Task Planning: Beyond Static Rules with LLM Safety Perception",
    "url": "http://arxiv.org/abs/2503.06866v1",
    "arxiv_id": "2503.06866v1",
    "authors": [
      "Wanjing Huang",
      "Tongjie Pan",
      "Yalan Ye"
    ],
    "published": "2025-03-10T02:43:54+00:00",
    "summary": "Recent advancements in large language models (LLMs) have expanded their role in robotic task planning. However, while LLMs have been explored for generating feasible task sequences, their ability to ensure safe task execution remains underdeveloped. Existing methods struggle with structured risk perception, making them inadequate for safety-critical applications where low-latency hazard adaptation is required. To address this limitation, we propose a Graphormer-enhanced risk-aware task planning framework that combines LLM-based decision-making with structured safety modeling. Our approach constructs a dynamic spatio-semantic safety graph, capturing spatial and contextual risk factors to enable online hazard detection and adaptive task refinement. Unlike existing methods that rely on predefined safety constraints, our framework introduces a context-aware risk perception module that continuously refines safety predictions based on real-time task execution. This enables a more flexible and scalable approach to robotic planning, allowing for adaptive safety compliance beyond static rules. To validate our framework, we conduct experiments in the AI2-THOR environment. The experiments results validates improvements in risk detection accuracy, rising safety notice, and task adaptability of our framework in continuous environments compared to static rule-based and LLM-only baselines. Our project is available at https://github.com/hwj20/GGTP"
  },
  {
    "title": "Physically Large Apertures for Wireless Power Transfer: Performance and Regulatory Aspects",
    "url": "http://arxiv.org/abs/2503.06807v1",
    "arxiv_id": "2503.06807v1",
    "authors": [
      "Benjamin J. B. Deutschmann",
      "Ulrich Muehlmann",
      "Ahmet Kaplan",
      "Gilles Callebaut",
      "Thomas Wilding",
      "Bert Cox",
      "Liesbet Van der Perre",
      "Fredrik Tufvesson",
      "Erik G. Larsson",
      "Klaus Witrisal"
    ],
    "published": "2025-03-09T23:28:06+00:00",
    "summary": "Wireless power transfer (WPT) is a promising service for the Internet of Things, providing a cost-effective and sustainable solution to deploy so-called energy-neutral devices on a massive scale. The power received at the device side decays rapidly with the distance from a conventional transmit antenna with a physically small aperture. New opportunities arise from the transition from conventional far-field beamforming to near-field beam focusing. We argue that a \"physically large\" aperture, i.e., large w.r.t. the distance to the receiver, enables a power budget that remains practically independent of distance. Distance-dependent array gain patterns allow focusing the power density maximum precisely at the device location, while reducing the power density near the infrastructure. The physical aperture size is a key resource in enabling efficient yet regulatory-compliant WPT. We use real-world measurements to demonstrate that a regulatory-compliant system operating at sub-10GHz frequencies can increase the power received at the device into the milliwatt range. Our empirical demonstration shows that power-optimal near-field beam focusing inherently exploits multipath propagation, yielding both increased WPT efficiency and improved human exposure safety in real-world scenarios."
  },
  {
    "title": "AutoMisty: A Multi-Agent LLM Framework for Automated Code Generation in the Misty Social Robot",
    "url": "http://arxiv.org/abs/2503.06791v1",
    "arxiv_id": "2503.06791v1",
    "authors": [
      "Xiao Wang",
      "Lu Dong",
      "Sahana Rangasrinivasan",
      "Ifeoma Nwogu",
      "Srirangaraj Setlur",
      "Venugopal Govindaraju"
    ],
    "published": "2025-03-09T22:07:46+00:00",
    "summary": "The social robot's open API allows users to customize open-domain interactions. However, it remains inaccessible to those without programming experience. In this work, we introduce AutoMisty, the first multi-agent collaboration framework powered by large language models (LLMs), to enable the seamless generation of executable Misty robot code from natural language instructions. AutoMisty incorporates four specialized agent modules to manage task decomposition, assignment, problem-solving, and result synthesis. Each agent incorporates a two-layer optimization mechanism, with self-reflection for iterative refinement and human-in-the-loop for better alignment with user preferences. AutoMisty ensures a transparent reasoning process, allowing users to iteratively refine tasks through natural language feedback for precise execution. To evaluate AutoMisty's effectiveness, we designed a benchmark task set spanning four levels of complexity and conducted experiments in a real Misty robot environment. Extensive evaluations demonstrate that AutoMisty not only consistently generates high-quality code but also enables precise code control, significantly outperforming direct reasoning with ChatGPT-4o and ChatGPT-o1. All code, optimized APIs, and experimental videos will be publicly released through the webpage: https://wangxiaoshawn.github.io/AutoMisty.html"
  },
  {
    "title": "Chance-constrained Linear Quadratic Gaussian Games for Multi-robot Interaction under Uncertainty",
    "url": "http://arxiv.org/abs/2503.06776v1",
    "arxiv_id": "2503.06776v1",
    "authors": [
      "Kai Ren",
      "Giulio Salizzoni",
      "Mustafa Emre G\u00fcrsoy",
      "Maryam Kamgarpour"
    ],
    "published": "2025-03-09T21:03:53+00:00",
    "summary": "We address safe multi-robot interaction under uncertainty. In particular, we formulate a chance-constrained linear quadratic Gaussian game with coupling constraints and system uncertainties. We find a tractable reformulation of the game and propose a dual ascent algorithm. We prove that the algorithm converges to a generalized Nash equilibrium of the reformulated game, ensuring the satisfaction of the chance constraints. We test our method in driving simulations and real-world robot experiments. Our method ensures safety under uncertainty and generates less conservative trajectories than single-agent model predictive control."
  },
  {
    "title": "Cell-Free MIMO-ISAC: Joint Location and Velocity Estimation and Fundamental CRLB Analysis",
    "url": "http://arxiv.org/abs/2503.06766v1",
    "arxiv_id": "2503.06766v1",
    "authors": [
      "Guoqing Xia",
      "Pei Xiao",
      "Qu Luo",
      "Bing Ji",
      "Yue Zhang",
      "Huiyu Zhou"
    ],
    "published": "2025-03-09T20:43:18+00:00",
    "summary": "This paper investigates joint location and velocity estimation, along with their fundamental performance bounds analysis, in a cell-free multi-input multi-output (MIMO) integrated sensing and communication (ISAC) system. First, unlike existing studies that derive likelihood functions for target parameter estimation using continuous received signals, we formulate the maximum likelihood estimation (MLE) for radar sensing based on discrete received signals at a given sampling rate. Second, leveraging the proposed MLEs, we derive closed-form Cramer-Rao lower bounds (CRLBs) for joint location and velocity estimation in both single-target and multiple-target scenarios. Third, to enhance computational efficiency, we propose approximate CRLBs and conduct an in-depth accuracy analysis. Additionally, we thoroughly examine the impact of sampling rate, squared effective bandwidth, and time width on CRLB performance. For multiple-target scenarios, the concepts of safety distance and safety velocity are introduced to characterize conditions under which the CRLBs for multiple targets converge to their single target counterparts. Finally, extensive simulations are conducted to verify the accuracy of the proposed CRLBs and the theoretical results using state-of-the-art waveforms, namely orthogonal frequency division multiplexing (OFDM) and orthogonal chirp division multiplexing (OCDM)."
  },
  {
    "title": "Vision-R1: Incentivizing Reasoning Capability in Multimodal Large Language Models",
    "url": "http://arxiv.org/abs/2503.06749v1",
    "arxiv_id": "2503.06749v1",
    "authors": [
      "Wenxuan Huang",
      "Bohan Jia",
      "Zijie Zhai",
      "Shaosheng Cao",
      "Zheyu Ye",
      "Fei Zhao",
      "Yao Hu",
      "Shaohui Lin"
    ],
    "published": "2025-03-09T20:06:45+00:00",
    "summary": "DeepSeek-R1-Zero has successfully demonstrated the emergence of reasoning capabilities in LLMs purely through Reinforcement Learning (RL). Inspired by this breakthrough, we explore how RL can be utilized to enhance the reasoning capability of MLLMs. However, direct training with RL struggles to activate complex reasoning capabilities such as questioning and reflection in MLLMs, due to the absence of substantial high-quality multimodal reasoning data. To address this issue, we propose the reasoning MLLM, Vision-R1, to improve multimodal reasoning capability. Specifically, we first construct a high-quality multimodal CoT dataset without human annotations by leveraging an existing MLLM and DeepSeek-R1 through modality bridging and data filtering to obtain a 200K multimodal CoT dataset, Vision-R1-cold dataset. It serves as cold-start initialization data for Vision-R1. To mitigate the optimization challenges caused by overthinking after cold start, we propose Progressive Thinking Suppression Training (PTST) strategy and employ Group Relative Policy Optimization (GRPO) with the hard formatting result reward function to gradually refine the model's ability to learn correct and complex reasoning processes on a 10K multimodal math dataset. Comprehensive experiments show our model achieves an average improvement of $\\sim$6% across various multimodal math reasoning benchmarks. Vision-R1-7B achieves a 73.5% accuracy on the widely used MathVista benchmark, which is only 0.4% lower than the leading reasoning model, OpenAI O1. The datasets and code will be released in: https://github.com/Osilly/Vision-R1 ."
  },
  {
    "title": "Vision-R1: Incentivizing Reasoning Capability in Multimodal Large Language Models",
    "url": "http://arxiv.org/abs/2503.06749v2",
    "arxiv_id": "2503.06749v2",
    "authors": [
      "Wenxuan Huang",
      "Bohan Jia",
      "Zijie Zhai",
      "Shaosheng Cao",
      "Zheyu Ye",
      "Fei Zhao",
      "Zhe Xu",
      "Yao Hu",
      "Shaohui Lin"
    ],
    "published": "2025-03-09T20:06:45+00:00",
    "summary": "DeepSeek-R1-Zero has successfully demonstrated the emergence of reasoning capabilities in LLMs purely through Reinforcement Learning (RL). Inspired by this breakthrough, we explore how RL can be utilized to enhance the reasoning capability of MLLMs. However, direct training with RL struggles to activate complex reasoning capabilities such as questioning and reflection in MLLMs, due to the absence of substantial high-quality multimodal reasoning data. To address this issue, we propose the reasoning MLLM, Vision-R1, to improve multimodal reasoning capability. Specifically, we first construct a high-quality multimodal CoT dataset without human annotations by leveraging an existing MLLM and DeepSeek-R1 through modality bridging and data filtering to obtain a 200K multimodal CoT dataset, Vision-R1-cold dataset. It serves as cold-start initialization data for Vision-R1. To mitigate the optimization challenges caused by overthinking after cold start, we propose Progressive Thinking Suppression Training (PTST) strategy and employ Group Relative Policy Optimization (GRPO) with the hard formatting result reward function to gradually refine the model's ability to learn correct and complex reasoning processes on a 10K multimodal math dataset. Comprehensive experiments show our model achieves an average improvement of $\\sim$6% across various multimodal math reasoning benchmarks. Vision-R1-7B achieves a 73.5% accuracy on the widely used MathVista benchmark, which is only 0.4% lower than the leading reasoning model, OpenAI O1. The datasets and code will be released in: https://github.com/Osilly/Vision-R1 ."
  },
  {
    "title": "Safe, Task-Consistent Manipulation with Operational Space Control Barrier Functions",
    "url": "http://arxiv.org/abs/2503.06736v1",
    "arxiv_id": "2503.06736v1",
    "authors": [
      "Daniel Morton",
      "Marco Pavone"
    ],
    "published": "2025-03-09T19:29:15+00:00",
    "summary": "Safe real-time control of robotic manipulators in unstructured environments requires handling numerous safety constraints without compromising task performance. Traditional approaches, such as artificial potential fields (APFs), suffer from local minima, oscillations, and limited scalability, while model predictive control (MPC) can be computationally expensive. Control barrier functions (CBFs) offer a promising alternative due to their high level of robustness and low computational cost, but these safety filters must be carefully designed to avoid significant reductions in the overall performance of the manipulator. In this work, we introduce an Operational Space Control Barrier Function (OSCBF) framework that integrates safety constraints while preserving task-consistent behavior. Our approach scales to hundreds of simultaneous constraints while retaining real-time control rates, ensuring collision avoidance, singularity prevention, and workspace containment even in highly cluttered and dynamic settings. By explicitly accounting for the task hierarchy in the CBF objective, we prevent degraded performance across both joint-space and operational-space tasks, when at the limit of safety. Our open-source, high-performance software will be available at our project webpage, https://stanfordasl.github.io/oscbf/"
  },
  {
    "title": "Diffusion Model Based Probabilistic Day-ahead Load Forecasting",
    "url": "http://arxiv.org/abs/2503.06697v1",
    "arxiv_id": "2503.06697v1",
    "authors": [
      "Ding Lin",
      "Han Guo",
      "Jianhui Wang"
    ],
    "published": "2025-03-09T17:27:12+00:00",
    "summary": "Accurate probabilistic load forecasting is crucial for maintaining the safety and stability of power systems. However, the mainstream approach, multi-step prediction, must be improved by cumulative errors and latency issues, which limits its effectiveness in probabilistic day-ahead load forecasting (PDALF). To overcome these challenges, we introduce DALNet, a novel denoising diffusion model designed to generate load curves rather than relying on direct prediction. By shifting the focus to curve generation, DALNet captures the complex distribution of actual load time-series data under specific conditions with greater fidelity. To further enhance DALNet, we propose the temporal multi-scale attention block (TMSAB), a mechanism designed to integrate both positional and temporal information for improved forecasting precision. Furthermore, we utilize kernel density estimation (KDE) to reconstruct the distribution of generated load curves and employ KL divergence to compare them with the actual data distribution. Experimental results demonstrate that DALNet excels in load forecasting accuracy and offers a novel perspective for other predictive tasks within power systems."
  },
  {
    "title": "PANDA: Parkinson's Assistance and Notification Driving Aid",
    "url": "http://arxiv.org/abs/2503.06659v1",
    "arxiv_id": "2503.06659v1",
    "authors": [
      "Tianyang Wen",
      "Xucheng Zhang",
      "Zhirong Wan",
      "Jing Zhao",
      "Yicheng Zhu",
      "Ning Su",
      "Xiaolan Peng",
      "Jin Huang",
      "Wei Sun",
      "Feng Tian",
      "Franklin Mingzhe Li"
    ],
    "published": "2025-03-09T15:19:04+00:00",
    "summary": "Parkinson's Disease (PD) significantly impacts driving abilities, often leading to early driving cessation or accidents due to reduced motor control and increasing reaction times. To diminish the impact of these symptoms, we developed PANDA (Parkinson's Assistance and Notification Driving Aid), a multi-modality real-time alert system designed to monitor driving patterns continuously and provide immediate alerts for irregular driving behaviors, enhancing driver safety of individuals with PD. The system was developed through a participatory design process with 9 people with PD and 13 non-PD individuals using a driving simulator, which allowed us to identify critical design characteristics and collect detailed data on driving behavior. A user study involving individuals with PD evaluated the effectiveness of PANDA, exploring optimal strategies for delivering alerts and ensuring they are timely and helpful. Our findings demonstrate that PANDA has the potential to enhance the driving safety of individuals with PD, offering a valuable tool for maintaining independence and confidence behind the wheel."
  },
  {
    "title": "Reinforcement Learning with Verifiable Rewards: GRPO's Effective Loss, Dynamics, and Success Amplification",
    "url": "http://arxiv.org/abs/2503.06639v1",
    "arxiv_id": "2503.06639v1",
    "authors": [
      "Youssef Mroueh"
    ],
    "published": "2025-03-09T14:36:45+00:00",
    "summary": "Group Relative Policy Optimization (GRPO) was introduced and used successfully to train DeepSeek R1 models for promoting reasoning capabilities of LLMs using verifiable or binary rewards. We show in this paper that GRPO with verifiable rewards can be written as a Kullback Leibler ($\\mathsf{KL}$) regularized contrastive loss, where the contrastive samples are synthetic data sampled from the old policy. The optimal GRPO policy $\\pi_{n}$ can be expressed explicitly in terms of the binary reward, as well as the first and second order statistics of the old policy ($\\pi_{n-1}$) and the reference policy $\\pi_0$. Iterating this scheme, we obtain a sequence of policies $\\pi_{n}$ for which we can quantify the probability of success $p_n$. We show that the probability of success of the policy satisfies a recurrence that converges to a fixed point of a function that depends on the initial probability of success $p_0$ and the regularization parameter $\\beta$ of the $\\mathsf{KL}$ regularizer. We show that the fixed point $p^*$ is guaranteed to be larger than $p_0$, thereby demonstrating that GRPO effectively amplifies the probability of success of the policy."
  },
  {
    "title": "Intelligent Control of Merging Car-following and Lane-Changing Behavior",
    "url": "http://arxiv.org/abs/2503.06572v1",
    "arxiv_id": "2503.06572v1",
    "authors": [
      "Farzam Tajdari",
      "Amin Rezasoltani"
    ],
    "published": "2025-03-09T12:03:46+00:00",
    "summary": "Recent research has paid little attention to complex driving behaviors, namely merging car-following and lane-changing behavior, and how lane-changing affects algorithms designed to model and control a car-following vehicle. During the merging behavior, the Follower Vehicle (FV) might significantly diverge from typical car-following models. Thus, this paper aims to control the FV witnessing lane-changing behavior based on anticipation, perception, preparation, and relaxation states defined by a novel measurable human perception index. Data from human drivers are utilized to create a perception-based fuzzy controller for the behavior vehicle's route guidance, taking into account the opacity of human driving judgments. We illustrate the efficacy of the established technique using simulated trials and data from actual drivers, focusing on the benefits of the increased comfort, safety, and uniformity of traffic flow and the decreased of wait time and motion sickness this brings about."
  },
  {
    "title": "Multimodal Programming in Computer Science with Interactive Assistance Powered by Large Language Model",
    "url": "http://arxiv.org/abs/2503.06552v1",
    "arxiv_id": "2503.06552v1",
    "authors": [
      "Rajan Das Gupta",
      "Md. Tanzib Hosain",
      "M. F. Mridha",
      "Salah Uddin Ahmed"
    ],
    "published": "2025-03-09T10:48:47+00:00",
    "summary": "LLM chatbot interfaces allow students to get instant, interactive assistance with homework, but doing so carelessly may not advance educational objectives. In this study, an interactive homework help system based on DeepSeek R1 is developed and first implemented for students enrolled in a large computer science beginning programming course. In addition to an assist button in a well-known code editor, our assistant also has a feedback option in our command-line automatic evaluator. It wraps student work in a personalized prompt that advances our educational objectives without offering answers straight away. We have discovered that our assistant can recognize students' conceptual difficulties and provide ideas, plans, and template code in pedagogically appropriate ways. However, among other mistakes, it occasionally incorrectly labels the correct student code as incorrect or encourages students to use correct-but-lesson-inappropriate approaches, which can lead to long and frustrating journeys for the students. After discussing many development and deployment issues, we provide our conclusions and future actions."
  },
  {
    "title": "BingoGuard: LLM Content Moderation Tools with Risk Levels",
    "url": "http://arxiv.org/abs/2503.06550v1",
    "arxiv_id": "2503.06550v1",
    "authors": [
      "Fan Yin",
      "Philippe Laban",
      "Xiangyu Peng",
      "Yilun Zhou",
      "Yixin Mao",
      "Vaibhav Vats",
      "Linnea Ross",
      "Divyansh Agarwal",
      "Caiming Xiong",
      "Chien-Sheng Wu"
    ],
    "published": "2025-03-09T10:43:09+00:00",
    "summary": "Malicious content generated by large language models (LLMs) can pose varying degrees of harm. Although existing LLM-based moderators can detect harmful content, they struggle to assess risk levels and may miss lower-risk outputs. Accurate risk assessment allows platforms with different safety thresholds to tailor content filtering and rejection. In this paper, we introduce per-topic severity rubrics for 11 harmful topics and build BingoGuard, an LLM-based moderation system designed to predict both binary safety labels and severity levels. To address the lack of annotations on levels of severity, we propose a scalable generate-then-filter framework that first generates responses across different severity levels and then filters out low-quality responses. Using this framework, we create BingoGuardTrain, a training dataset with 54,897 examples covering a variety of topics, response severity, styles, and BingoGuardTest, a test set with 988 examples explicitly labeled based on our severity rubrics that enables fine-grained analysis on model behaviors on different severity levels. Our BingoGuard-8B, trained on BingoGuardTrain, achieves the state-of-the-art performance on several moderation benchmarks, including WildGuardTest and HarmBench, as well as BingoGuardTest, outperforming best public models, WildGuard, by 4.3\\%. Our analysis demonstrates that incorporating severity levels into training significantly enhances detection performance and enables the model to effectively gauge the severity of harmful responses."
  },
  {
    "title": "AnywhereDoor: Multi-Target Backdoor Attacks on Object Detection",
    "url": "http://arxiv.org/abs/2503.06529v1",
    "arxiv_id": "2503.06529v1",
    "authors": [
      "Jialin Lu",
      "Junjie Shan",
      "Ziqi Zhao",
      "Ka-Ho Chow"
    ],
    "published": "2025-03-09T09:24:24+00:00",
    "summary": "As object detection becomes integral to many safety-critical applications, understanding its vulnerabilities is essential. Backdoor attacks, in particular, pose a serious threat by implanting hidden triggers in victim models, which adversaries can later exploit to induce malicious behaviors during inference. However, current understanding is limited to single-target attacks, where adversaries must define a fixed malicious behavior (target) before training, making inference-time adaptability impossible. Given the large output space of object detection (including object existence prediction, bounding box estimation, and classification), the feasibility of flexible, inference-time model control remains unexplored. This paper introduces AnywhereDoor, a multi-target backdoor attack for object detection. Once implanted, AnywhereDoor allows adversaries to make objects disappear, fabricate new ones, or mislabel them, either across all object classes or specific ones, offering an unprecedented degree of control. This flexibility is enabled by three key innovations: (i) objective disentanglement to scale the number of supported targets; (ii) trigger mosaicking to ensure robustness even against region-based detectors; and (iii) strategic batching to address object-level data imbalances that hinder manipulation. Extensive experiments demonstrate that AnywhereDoor grants attackers a high degree of control, improving attack success rates by 26% compared to adaptations of existing methods for such flexible control."
  },
  {
    "title": "Can Small Language Models Reliably Resist Jailbreak Attacks? A Comprehensive Evaluation",
    "url": "http://arxiv.org/abs/2503.06519v1",
    "arxiv_id": "2503.06519v1",
    "authors": [
      "Wenhui Zhang",
      "Huiyu Xu",
      "Zhibo Wang",
      "Zeqing He",
      "Ziqi Zhu",
      "Kui Ren"
    ],
    "published": "2025-03-09T08:47:16+00:00",
    "summary": "Small language models (SLMs) have emerged as promising alternatives to large language models (LLMs) due to their low computational demands, enhanced privacy guarantees and comparable performance in specific domains through light-weight fine-tuning. Deploying SLMs on edge devices, such as smartphones and smart vehicles, has become a growing trend. However, the security implications of SLMs have received less attention than LLMs, particularly regarding jailbreak attacks, which is recognized as one of the top threats of LLMs by the OWASP. In this paper, we conduct the first large-scale empirical study of SLMs' vulnerabilities to jailbreak attacks. Through systematically evaluation on 63 SLMs from 15 mainstream SLM families against 8 state-of-the-art jailbreak methods, we demonstrate that 47.6% of evaluated SLMs show high susceptibility to jailbreak attacks (ASR > 40%) and 38.1% of them can not even resist direct harmful query (ASR > 50%). We further analyze the reasons behind the vulnerabilities and identify four key factors: model size, model architecture, training datasets and training techniques. Moreover, we assess the effectiveness of three prompt-level defense methods and find that none of them achieve perfect performance, with detection accuracy varying across different SLMs and attack methods. Notably, we point out that the inherent security awareness play a critical role in SLM security, and models with strong security awareness could timely terminate unsafe response with little reminder. Building upon the findings, we highlight the urgent need for security-by-design approaches in SLM development and provide valuable insights for building more trustworthy SLM ecosystem."
  },
  {
    "title": "Evaluation of Safety Cognition Capability in Vision-Language Models for Autonomous Driving",
    "url": "http://arxiv.org/abs/2503.06497v1",
    "arxiv_id": "2503.06497v1",
    "authors": [
      "Enming Zhang",
      "Peizhe Gong",
      "Xingyuan Dai",
      "Yisheng Lv",
      "Qinghai Miao"
    ],
    "published": "2025-03-09T07:53:19+00:00",
    "summary": "Assessing the safety of vision-language models (VLMs) in autonomous driving is particularly important; however, existing work mainly focuses on traditional benchmark evaluations. As interactive components within autonomous driving systems, VLMs must maintain strong safety cognition during interactions. From this perspective, we propose a novel evaluation method: Safety Cognitive Driving Benchmark (SCD-Bench) . To address the large-scale annotation challenge for SCD-Bench, we develop the Autonomous Driving Image-Text Annotation System (ADA) . Additionally, to ensure data quality in SCD-Bench, our dataset undergoes manual refinement by experts with professional knowledge in autonomous driving. We further develop an automated evaluation method based on large language models (LLMs). To verify its effectiveness, we compare its evaluation results with those of expert human evaluations, achieving a consistency rate of 99.74%. Preliminary experimental results indicate that existing open-source models still lack sufficient safety cognition, showing a significant gap compared to GPT-4o. Notably, lightweight models (1B-4B) demonstrate minimal safety cognition. However, since lightweight models are crucial for autonomous driving systems, this presents a significant challenge for integrating VLMs into the field."
  },
  {
    "title": "OT-DETECTOR: Delving into Optimal Transport for Zero-shot Out-of-Distribution Detection",
    "url": "http://arxiv.org/abs/2503.06442v1",
    "arxiv_id": "2503.06442v1",
    "authors": [
      "Yu Liu",
      "Hao Tang",
      "Haiqi Zhang",
      "Jing Qin",
      "Zechao Li"
    ],
    "published": "2025-03-09T04:47:19+00:00",
    "summary": "Out-of-distribution (OOD) detection is crucial for ensuring the reliability and safety of machine learning models in real-world applications. While zero-shot OOD detection, which requires no training on in-distribution (ID) data, has become feasible with the emergence of vision-language models like CLIP, existing methods primarily focus on semantic matching and fail to fully capture distributional discrepancies. To address these limitations, we propose OT-DETECTOR, a novel framework that employs Optimal Transport (OT) to quantify both semantic and distributional discrepancies between test samples and ID labels. Specifically, we introduce cross-modal transport mass and transport cost as semantic-wise and distribution-wise OOD scores, respectively, enabling more robust detection of OOD samples. Additionally, we present a semantic-aware content refinement (SaCR) module, which utilizes semantic cues from ID labels to amplify the distributional discrepancy between ID and hard OOD samples. Extensive experiments on several benchmarks demonstrate that OT-DETECTOR achieves state-of-the-art performance across various OOD detection tasks, particularly in challenging hard-OOD scenarios."
  },
  {
    "title": "Explaining Control Policies through Predicate Decision Diagrams",
    "url": "http://arxiv.org/abs/2503.06420v1",
    "arxiv_id": "2503.06420v1",
    "authors": [
      "Debraj Chakraborty",
      "Clemens Dubslaff",
      "Sudeep Kanav",
      "Jan Kretinsky",
      "Christoph Weinhuber"
    ],
    "published": "2025-03-09T03:31:48+00:00",
    "summary": "Safety-critical controllers of complex systems are hard to construct manually. Automated approaches such as controller synthesis or learning provide a tempting alternative but usually lack explainability. To this end, learning decision trees (DTs) have been prevalently used towards an interpretable model of the generated controllers. However, DTs do not exploit shared decision-making, a key concept exploited in binary decision diagrams (BDDs) to reduce their size and thus improve explainability. In this work, we introduce predicate decision diagrams (PDDs) that extend BDDs with predicates and thus unite the advantages of DTs and BDDs for controller representation. We establish a synthesis pipeline for efficient construction of PDDs from DTs representing controllers, exploiting reduction techniques for BDDs also for PDDs."
  },
  {
    "title": "Exponential-polynomial divergence based inference for nondestructive one-shot devices under progressive stress model",
    "url": "http://arxiv.org/abs/2503.06414v1",
    "arxiv_id": "2503.06414v1",
    "authors": [
      "Shanya Baghel",
      "Shuvashree Mondal"
    ],
    "published": "2025-03-09T03:16:21+00:00",
    "summary": "Nondestructive one-shot device (NOSD) testing plays a crucial role in engineering, particularly in the reliability assessment of high-stakes systems such as aerospace components, medical devices, and semiconductor technologies. Accurate reliability prognosis of NOSD testing data is essential for ensuring product durability, safety, and performance optimization. The conventional estimation methods like maximum likelihood estimation (MLE) are sensitive to data contamination, leading to biased results. Consequently, this study develops robust inferential analysis for NOSD testing data under a progressive stress model. The lifetime of NOSD is assumed to follow Log-logistic distribution. The estimation procedure addresses robustness by incorporating Exponential-polynomial divergence (EPD). Equipped with three tuning parameters, EPD based estimation is proven to be more flexible than density power divergence estimation frequently used for one-shot device testing data analysis. Further, we explore the asymptotic behaviour of minimum EPD estimator (MEPDE) for large sample size. The robustness of MEPDE is analytically studied through influence function. Since tradeoff between efficiency and robustness of EPD based estimation is governed by three tuning parameters, a novel approach leveraging Concrete Score Matching (CSM) is introduced to optimize the tuning parameters of MEPDE. Moreover, a comparative study with the existing methods of finding tuning parameters is conducted through extensive simulation experiment and data analysis. Another aspect of this study is determining an optimal plan to ensure a successful ALT experiment within specified budget and time constraints. It is designed on A-optimality criteria subject to the given constraints and is executed using the constraint particle swarm optimization (CPSO) algorithm."
  },
  {
    "title": "Decoding the Black Box: Integrating Moral Imagination with Technical AI Governance",
    "url": "http://arxiv.org/abs/2503.06411v1",
    "arxiv_id": "2503.06411v1",
    "authors": [
      "Krti Tallam"
    ],
    "published": "2025-03-09T03:11:32+00:00",
    "summary": "This paper examines the intricate interplay among AI safety, security, and governance by integrating technical systems engineering with principles of moral imagination and ethical philosophy. Drawing on foundational insights from Weapons of Math Destruction and Thinking in Systems alongside contemporary debates in AI ethics, we develop a comprehensive multi-dimensional framework designed to regulate AI technologies deployed in high-stakes domains such as defense, finance, healthcare, and education. Our approach combines rigorous technical analysis, quantitative risk assessment, and normative evaluation to expose systemic vulnerabilities inherent in opaque, black-box models. Detailed case studies, including analyses of Microsoft Tay (2016) and the UK A-Level Grading Algorithm (2020), demonstrate how security lapses, bias amplification, and lack of accountability can precipitate cascading failures that undermine public trust. We conclude by outlining targeted strategies for enhancing AI resilience through adaptive regulatory mechanisms, robust security protocols, and interdisciplinary oversight, thereby advancing the state of the art in ethical and technical AI governance."
  },
  {
    "title": "Backdoor Attacks on Discrete Graph Diffusion Models",
    "url": "http://arxiv.org/abs/2503.06340v1",
    "arxiv_id": "2503.06340v1",
    "authors": [
      "Jiawen Wang",
      "Samin Karim",
      "Yuan Hong",
      "Binghui Wang"
    ],
    "published": "2025-03-08T21:01:15+00:00",
    "summary": "Diffusion models are powerful generative models in continuous data domains such as image and video data. Discrete graph diffusion models (DGDMs) have recently extended them for graph generation, which are crucial in fields like molecule and protein modeling, and obtained the SOTA performance. However, it is risky to deploy DGDMs for safety-critical applications (e.g., drug discovery) without understanding their security vulnerabilities. In this work, we perform the first study on graph diffusion models against backdoor attacks, a severe attack that manipulates both the training and inference/generation phases in graph diffusion models. We first define the threat model, under which we design the attack such that the backdoored graph diffusion model can generate 1) high-quality graphs without backdoor activation, 2) effective, stealthy, and persistent backdoored graphs with backdoor activation, and 3) graphs that are permutation invariant and exchangeable--two core properties in graph generative models. 1) and 2) are validated via empirical evaluations without and with backdoor defenses, while 3) is validated via theoretical results."
  },
  {
    "title": "Accurate and Efficient Two-Stage Gun Detection in Video",
    "url": "http://arxiv.org/abs/2503.06317v1",
    "arxiv_id": "2503.06317v1",
    "authors": [
      "Badhan Chandra Das",
      "M. Hadi Amini",
      "Yanzhao Wu"
    ],
    "published": "2025-03-08T19:26:23+00:00",
    "summary": "Object detection in videos plays a crucial role in advancing applications such as public safety and anomaly detection. Existing methods have explored different techniques, including CNN, deep learning, and Transformers, for object detection and video classification. However, detecting tiny objects, e.g., guns, in videos remains challenging due to their small scale and varying appearances in complex scenes. Moreover, existing video analysis models for classification or detection often perform poorly in real-world gun detection scenarios due to limited labeled video datasets for training. Thus, developing efficient methods for effectively capturing tiny object features and designing models capable of accurate gun detection in real-world videos is imperative. To address these challenges, we make three original contributions in this paper. First, we conduct an empirical study of several existing video classification and object detection methods to identify guns in videos. Our extensive analysis shows that these methods may not accurately detect guns in videos. Second, we propose a novel two-stage gun detection method. In stage 1, we train an image-augmented model to effectively classify ``Gun'' videos. To make the detection more precise and efficient, stage 2 employs an object detection model to locate the exact region of the gun within video frames for videos classified as ``Gun'' by stage 1. Third, our experimental results demonstrate that the proposed domain-specific method achieves significant performance improvements and enhances efficiency compared with existing techniques. We also discuss challenges and future research directions in gun detection tasks in computer vision."
  },
  {
    "title": "Synergizing AI and Digital Twins for Next-Generation Network Optimization, Forecasting, and Security",
    "url": "http://arxiv.org/abs/2503.06302v1",
    "arxiv_id": "2503.06302v1",
    "authors": [
      "Zifan Zhang",
      "Minghong Fang",
      "Dianwei Chen",
      "Xianfeng Yang",
      "Yuchen Liu"
    ],
    "published": "2025-03-08T18:30:54+00:00",
    "summary": "Digital network twins (DNTs) are virtual representations of physical networks, designed to enable real-time monitoring, simulation, and optimization of network performance. When integrated with machine learning (ML) techniques, particularly federated learning (FL) and reinforcement learning (RL), DNTs emerge as powerful solutions for managing the complexities of network operations. This article presents a comprehensive analysis of the synergy of DNTs, FL, and RL techniques, showcasing their collective potential to address critical challenges in 6G networks. We highlight key technical challenges that need to be addressed, such as ensuring network reliability, achieving joint data-scenario forecasting, and maintaining security in high-risk environments. Additionally, we propose several pipelines that integrate DNT and ML within coherent frameworks to enhance network optimization and security. Case studies demonstrate the practical applications of our proposed pipelines in edge caching and vehicular networks. In edge caching, the pipeline achieves over 80% cache hit rates while balancing base station loads. In autonomous vehicular system, it ensure a 100% no-collision rate, showcasing its reliability in safety-critical scenarios. By exploring these synergies, we offer insights into the future of intelligent and adaptive network systems that automate decision-making and problem-solving."
  },
  {
    "title": "Exploring Adversarial Transferability between Kolmogorov-arnold Networks",
    "url": "http://arxiv.org/abs/2503.06276v1",
    "arxiv_id": "2503.06276v1",
    "authors": [
      "Songping Wang",
      "Xinquan Yue",
      "Yueming Lyu",
      "Caifeng Shan"
    ],
    "published": "2025-03-08T16:48:05+00:00",
    "summary": "Kolmogorov-Arnold Networks (KANs) have emerged as a transformative model paradigm, significantly impacting various fields. However, their adversarial robustness remains less underexplored, especially across different KAN architectures. To explore this critical safety issue, we conduct an analysis and find that due to overfitting to the specific basis functions of KANs, they possess poor adversarial transferability among different KANs. To tackle this challenge, we propose AdvKAN, the first transfer attack method for KANs. AdvKAN integrates two key components: 1) a Breakthrough-Defense Surrogate Model (BDSM), which employs a breakthrough-defense training strategy to mitigate overfitting to the specific structures of KANs. 2) a Global-Local Interaction (GLI) technique, which promotes sufficient interaction between adversarial gradients of hierarchical levels, further smoothing out loss surfaces of KANs. Both of them work together to enhance the strength of transfer attack among different KANs. Extensive experimental results on various KANs and datasets demonstrate the effectiveness of AdvKAN, which possesses notably superior attack capabilities and deeply reveals the vulnerabilities of KANs. Code will be released upon acceptance."
  },
  {
    "title": "Using Mechanistic Interpretability to Craft Adversarial Attacks against Large Language Models",
    "url": "http://arxiv.org/abs/2503.06269v1",
    "arxiv_id": "2503.06269v1",
    "authors": [
      "Thomas Winninger",
      "Boussad Addad",
      "Katarzyna Kapusta"
    ],
    "published": "2025-03-08T16:29:45+00:00",
    "summary": "Traditional white-box methods for creating adversarial perturbations against LLMs typically rely only on gradient computation from the targeted model, ignoring the internal mechanisms responsible for attack success or failure. Conversely, interpretability studies that analyze these internal mechanisms lack practical applications beyond runtime interventions. We bridge this gap by introducing a novel white-box approach that leverages mechanistic interpretability techniques to craft practical adversarial inputs. Specifically, we first identify acceptance subspaces - sets of feature vectors that do not trigger the model's refusal mechanisms - then use gradient-based optimization to reroute embeddings from refusal subspaces to acceptance subspaces, effectively achieving jailbreaks. This targeted approach significantly reduces computation cost, achieving attack success rates of 80-95\\% on state-of-the-art models including Gemma2, Llama3.2, and Qwen2.5 within minutes or even seconds, compared to existing techniques that often fail or require hours of computation. We believe this approach opens a new direction for both attack research and defense development. Furthermore, it showcases a practical application of mechanistic interpretability where other methods are less efficient, which highlights its utility. The code and generated datasets are available at https://github.com/Sckathach/subspace-rerouting."
  },
  {
    "title": "MAD-MAX: Modular And Diverse Malicious Attack MiXtures for Automated LLM Red Teaming",
    "url": "http://arxiv.org/abs/2503.06253v1",
    "arxiv_id": "2503.06253v1",
    "authors": [
      "Stefan Schoepf",
      "Muhammad Zaid Hameed",
      "Ambrish Rawat",
      "Kieran Fraser",
      "Giulio Zizzo",
      "Giandomenico Cornacchia",
      "Mark Purcell"
    ],
    "published": "2025-03-08T15:28:26+00:00",
    "summary": "With LLM usage rapidly increasing, their vulnerability to jailbreaks that create harmful outputs are a major security risk. As new jailbreaking strategies emerge and models are changed by fine-tuning, continuous testing for security vulnerabilities is necessary. Existing Red Teaming methods fall short in cost efficiency, attack success rate, attack diversity, or extensibility as new attack types emerge. We address these challenges with Modular And Diverse Malicious Attack MiXtures (MAD-MAX) for Automated LLM Red Teaming. MAD-MAX uses automatic assignment of attack strategies into relevant attack clusters, chooses the most relevant clusters for a malicious goal, and then combines strategies from the selected clusters to achieve diverse novel attacks with high attack success rates. MAD-MAX further merges promising attacks together at each iteration of Red Teaming to boost performance and introduces a similarity filter to prune out similar attacks for increased cost efficiency. The MAD-MAX approach is designed to be easily extensible with newly discovered attack strategies and outperforms the prominent Red Teaming method Tree of Attacks with Pruning (TAP) significantly in terms of Attack Success Rate (ASR) and queries needed to achieve jailbreaks. MAD-MAX jailbreaks 97% of malicious goals in our benchmarks on GPT-4o and Gemini-Pro compared to TAP with 66%. MAD-MAX does so with only 10.9 average queries to the target LLM compared to TAP with 23.3.   WARNING: This paper contains contents which are offensive in nature."
  },
  {
    "title": "Rethinking Lanes and Points in Complex Scenarios for Monocular 3D Lane Detection",
    "url": "http://arxiv.org/abs/2503.06237v1",
    "arxiv_id": "2503.06237v1",
    "authors": [
      "Yifan Chang",
      "Junjie Huang",
      "Xiaofeng Wang",
      "Yun Ye",
      "Zhujin Liang",
      "Yi Shan",
      "Dalong Du",
      "Xingang Wang"
    ],
    "published": "2025-03-08T14:45:49+00:00",
    "summary": "Monocular 3D lane detection is a fundamental task in autonomous driving. Although sparse-point methods lower computational load and maintain high accuracy in complex lane geometries, current methods fail to fully leverage the geometric structure of lanes in both lane geometry representations and model design. In lane geometry representations, we present a theoretical analysis alongside experimental validation to verify that current sparse lane representation methods contain inherent flaws, resulting in potential errors of up to 20 m, which raise significant safety concerns for driving. To address this issue, we propose a novel patching strategy to completely represent the full lane structure. To enable existing models to match this strategy, we introduce the EndPoint head (EP-head), which adds a patching distance to endpoints. The EP-head enables the model to predict more complete lane representations even with fewer preset points, effectively addressing existing limitations and paving the way for models that are faster and require fewer parameters in the future. In model design, to enhance the model's perception of lane structures, we propose the PointLane attention (PL-attention), which incorporates prior geometric knowledge into the attention mechanism. Extensive experiments demonstrate the effectiveness of the proposed methods on various state-of-the-art models. For instance, in terms of the overall F1-score, our methods improve Persformer by 4.4 points, Anchor3DLane by 3.2 points, and LATR by 2.8 points. The code will be available soon."
  },
  {
    "title": "Reinforced Diffuser for Red Teaming Large Vision-Language Models",
    "url": "http://arxiv.org/abs/2503.06223v1",
    "arxiv_id": "2503.06223v1",
    "authors": [
      "Ruofan Wang",
      "Xiang Zheng",
      "Xiaosen Wang",
      "Cong Wang",
      "Xingjun Ma"
    ],
    "published": "2025-03-08T13:51:40+00:00",
    "summary": "The rapid advancement of large Vision-Language Models (VLMs) has raised significant safety concerns, particularly regarding their vulnerability to jailbreak attacks. While existing research primarily focuses on VLMs' susceptibility to harmful instructions, this work identifies a critical yet overlooked vulnerability: current alignment mechanisms often fail to address the risks posed by toxic text continuation tasks. To investigate this issue, we propose a novel Red Team Diffuser (RTD) framework, which leverages reinforcement learning to generate red team images that effectively induce highly toxic continuations from target black-box VLMs. The RTD pipeline begins with a greedy search for high-quality image prompts that maximize the toxicity of VLM-generated sentence continuations, guided by a Large Language Model (LLM). These prompts are then used as input for the reinforcement fine-tuning of a diffusion model, which employs toxicity and alignment rewards to further amplify harmful outputs. Experimental results demonstrate the effectiveness of RTD, increasing the toxicity rate of LLaVA outputs by 10.69% on the original attack set and 8.91% on a hold-out set. Moreover, RTD exhibits strong cross-model transferability, raising the toxicity rate by 5.1% on Gemini and 26.83% on LLaMA. These findings reveal significant deficiencies in existing alignment strategies, particularly their inability to prevent harmful continuations. Our work underscores the urgent need for more robust and adaptive alignment mechanisms to ensure the safe deployment of VLMs in real-world applications."
  },
  {
    "title": "Secure On-Device Video OOD Detection Without Backpropagation",
    "url": "http://arxiv.org/abs/2503.06166v1",
    "arxiv_id": "2503.06166v1",
    "authors": [
      "Li Li",
      "Peilin Cai",
      "Yuxiao Zhou",
      "Zhiyu Ni",
      "Renjie Liang",
      "You Qin",
      "Yi Nian",
      "Zhengzhong Tu",
      "Xiyang Hu",
      "Yue Zhao"
    ],
    "published": "2025-03-08T11:03:21+00:00",
    "summary": "Out-of-Distribution (OOD) detection is critical for ensuring the reliability of machine learning models in safety-critical applications such as autonomous driving and medical diagnosis. While deploying personalized OOD detection directly on edge devices is desirable, it remains challenging due to large model sizes and the computational infeasibility of on-device training. Federated learning partially addresses this but still requires gradient computation and backpropagation, exceeding the capabilities of many edge devices. To overcome these challenges, we propose SecDOOD, a secure cloud-device collaboration framework for efficient on-device OOD detection without requiring device-side backpropagation. SecDOOD utilizes cloud resources for model training while ensuring user data privacy by retaining sensitive information on-device. Central to SecDOOD is a HyperNetwork-based personalized parameter generation module, which adapts cloud-trained models to device-specific distributions by dynamically generating local weight adjustments, effectively combining central and local information without local fine-tuning. Additionally, our dynamic feature sampling and encryption strategy selectively encrypts only the most informative feature channels, largely reducing encryption overhead without compromising detection performance. Extensive experiments across multiple datasets and OOD scenarios demonstrate that SecDOOD achieves performance comparable to fully fine-tuned models, enabling secure, efficient, and personalized OOD detection on resource-limited edge devices. To enhance accessibility and reproducibility, our code is publicly available at https://github.com/Dystopians/SecDOOD."
  },
  {
    "title": "Exploring the usage of Probabilistic Neural Networks for Ionospheric electron density estimation",
    "url": "http://arxiv.org/abs/2503.06144v1",
    "arxiv_id": "2503.06144v1",
    "authors": [
      "Miquel Garcia-Fernandez"
    ],
    "published": "2025-03-08T10:06:15+00:00",
    "summary": "A fundamental limitation of traditional Neural Networks (NN) in predictive modelling is their inability to quantify uncertainty in their outputs. In critical applications like positioning systems, understanding the reliability of predictions is critical for constructing confidence intervals, early warning systems, and effectively propagating results. For instance, Precise Point Positioning in satellite navigation heavily relies on accurate error models for ancillary data (orbits, clocks, ionosphere, and troposphere) to compute precise error estimates. In addition, these uncertainty estimates are needed to establish robust protection levels in safety critical applications.   To address this challenge, the main objectives of this paper aims at exploring a potential framework capable of providing both point estimates and associated uncertainty measures of ionospheric Vertical Total Electron Content (VTEC). In this context, Probabilistic Neural Networks (PNNs) offer a promising approach to achieve this goal. However, constructing an effective PNN requires meticulous design of hidden and output layers, as well as careful definition of prior and posterior probability distributions for network weights and biases.   A key finding of this study is that the uncertainty provided by the PNN model in VTEC estimates may be systematically underestimated. In low-latitude areas, the actual error was observed to be as much as twice the model's estimate. This underestimation is expected to be more pronounced during solar maximum, correlating with increased VTEC values."
  },
  {
    "title": "dARt Vinci: Egocentric Data Collection for Surgical Robot Learning at Scale",
    "url": "http://arxiv.org/abs/2503.05646v1",
    "arxiv_id": "2503.05646v1",
    "authors": [
      "Yihao Liu",
      "Yu-Chun Ku",
      "Jiaming Zhang",
      "Hao Ding",
      "Peter Kazanzides",
      "Mehran Armand"
    ],
    "published": "2025-03-07T18:07:54+00:00",
    "summary": "Data scarcity has long been an issue in the robot learning community. Particularly, in safety-critical domains like surgical applications, obtaining high-quality data can be especially difficult. It poses challenges to researchers seeking to exploit recent advancements in reinforcement learning and imitation learning, which have greatly improved generalizability and enabled robots to conduct tasks autonomously. We introduce dARt Vinci, a scalable data collection platform for robot learning in surgical settings. The system uses Augmented Reality (AR) hand tracking and a high-fidelity physics engine to capture subtle maneuvers in primitive surgical tasks: By eliminating the need for a physical robot setup and providing flexibility in terms of time, space, and hardware resources-such as multiview sensors and actuators-specialized simulation is a viable alternative. At the same time, AR allows the robot data collection to be more egocentric, supported by its body tracking and content overlaying capabilities. Our user study confirms the proposed system's efficiency and usability, where we use widely-used primitive tasks for training teleoperation with da Vinci surgical robots. Data throughput improves across all tasks compared to real robot settings by 41% on average. The total experiment time is reduced by an average of 10%. The temporal demand in the task load survey is improved. These gains are statistically significant. Additionally, the collected data is over 400 times smaller in size, requiring far less storage while achieving double the frequency."
  },
  {
    "title": "Nuanced Safety for Generative AI: How Demographics Shape Responsiveness to Severity",
    "url": "http://arxiv.org/abs/2503.05609v1",
    "arxiv_id": "2503.05609v1",
    "authors": [
      "Pushkar Mishra",
      "Charvi Rastogi",
      "Stephen R. Pfohl",
      "Alicia Parrish",
      "Roma Patel",
      "Mark Diaz",
      "Ding Wang",
      "Michela Paganini",
      "Vinodkumar Prabhakaran",
      "Lora Aroyo",
      "Verena Rieser"
    ],
    "published": "2025-03-07T17:32:31+00:00",
    "summary": "Ensuring safety of Generative AI requires a nuanced understanding of pluralistic viewpoints. In this paper, we introduce a novel data-driven approach for calibrating granular ratings in pluralistic datasets. Specifically, we address the challenge of interpreting responses of a diverse population to safety expressed via ordinal scales (e.g., Likert scale). We distill non-parametric responsiveness metrics that quantify the consistency of raters in scoring the varying levels of the severity of safety violations. Using safety evaluation of AI-generated content as a case study, we investigate how raters from different demographic groups (age, gender, ethnicity) use an ordinal scale to express their perception of the severity of violations in a pluralistic safety dataset. We apply our metrics across violation types, demonstrating their utility in extracting nuanced insights that are crucial for developing reliable AI systems in a multi-cultural contexts. We show that our approach offers improved capabilities for prioritizing safety concerns by capturing nuanced viewpoints across different demographic groups, hence improving the reliability of pluralistic data collection and in turn contributing to more robust AI evaluations."
  },
  {
    "title": "TomatoScanner: phenotyping tomato fruit based on only RGB image",
    "url": "http://arxiv.org/abs/2503.05568v1",
    "arxiv_id": "2503.05568v1",
    "authors": [
      "Xiaobei Zhao",
      "Xiangrong Zeng",
      "Yihang Ma",
      "Pengjin Tang",
      "Xiang Li"
    ],
    "published": "2025-03-07T16:47:48+00:00",
    "summary": "In tomato greenhouse, phenotypic measurement is meaningful for researchers and farmers to monitor crop growth, thereby precisely control environmental conditions in time, leading to better quality and higher yield. Traditional phenotyping mainly relies on manual measurement, which is accurate but inefficient, more importantly, endangering the health and safety of people. Several studies have explored computer vision-based methods to replace manual phenotyping. However, the 2D-based need extra calibration, or cause destruction to fruit, or can only measure limited and meaningless traits. The 3D-based need extra depth camera, which is expensive and unacceptable for most farmers. In this paper, we propose a non-contact tomato fruit phenotyping method, titled TomatoScanner, where RGB image is all you need for input. First, pixel feature is extracted by instance segmentation of our proposed EdgeYOLO with preprocessing of individual separation and pose correction. Second, depth feature is extracted by depth estimation of Depth Pro. Third, pixel and depth feature are fused to output phenotype results in reality. We establish self-built Tomato Phenotype Dataset to test TomatoScanner, which achieves excellent phenotyping on width, height, vertical area and volume, with median relative error of 5.63%, 7.03%, -0.64% and 37.06%, respectively. We propose and add three innovative modules - EdgeAttention, EdgeLoss and EdgeBoost - into EdgeYOLO, to enhance the segmentation accuracy on edge portion. Precision and mean Edge Error greatly improve from 0.943 and 5.641% to 0.986 and 2.963%, respectively. Meanwhile, EdgeYOLO keeps lightweight and efficient, with 48.7 M weights size and 76.34 FPS. Codes and datasets: https://github.com/AlexTraveling/TomatoScanner."
  },
  {
    "title": "Step-by-step design guide of a cryogenic three-axis vector magnet",
    "url": "http://arxiv.org/abs/2503.05459v1",
    "arxiv_id": "2503.05459v1",
    "authors": [
      "Gaia Da Prato",
      "Yong Yu",
      "Ronald Bode",
      "Simon Gr\u00f6blacher"
    ],
    "published": "2025-03-07T14:32:11+00:00",
    "summary": "A tunable magnetic field at low temperatures is essential for numerous applications, including spintronics, magnetic resonance imaging, and condensed matter physics. While commercial superconducting vector magnets are available, they are complex, expensive, and often not adaptable to specific experimental needs. As a result, simple in-house designs are often being used in research environments. However, no comprehensive step-by-step guide for their construction currently exists. In this work, we provide a detailed manual for designing and building a cryogenically compatible three-axis vector magnet. The system is tested at the mixing chamber of a dilution refrigerator at temperatures ranging from 15 mK to 4 K, with no significant increase in base temperature. Safety measures are implemented to mitigate heating from quenching. The coils are successfully driven with DC currents as high as 3 A, generating magnetic fields of up to 2.5 T. Magnetic field measurements using Hall sensors demonstrate good agreement with the predictions of the designed performance."
  },
  {
    "title": "$\\mathrm{O}$/$\\mathrm{SO}$ Gauge Groups, $BC$ Quivers and $O3$ Planes",
    "url": "http://arxiv.org/abs/2503.05443v1",
    "arxiv_id": "2503.05443v1",
    "authors": [
      "Sam Bennett",
      "Amihay Hanany"
    ],
    "published": "2025-03-07T14:14:53+00:00",
    "summary": "D3-/D5-/NS5-brane systems with $O3$ orientifold planes realise 3d $\\mathcal{N}=4$ gauge theories with orthogonal and symplectic gauge groups on the D3-brane worldvolume. Such setups have long contained an ambiguity regarding the global form of $D$-type gauge groups. This note offers a partial prescription for reading $\\mathrm{O}(2k)$ and $\\mathrm{SO}(2k)$ gauge nodes in orthosymplectic quivers using the presence of $\\frac{1}{2}$D5-branes on the orientifolds bordering $\\frac{1}{2}$NS5-brane intervals spanned by $O3^{-}$ planes. A set of identities are proposed relating the Coulomb branches of generic quivers under a Higgsing that relates $\\mathrm{SO}(2k+1)$ and $\\mathrm{O}(2k)$ gauge groups. A further prescription is conjectured regarding the action of $\\frac{1}{2}$D5-branes on maximal $DC$-chains."
  },
  {
    "title": "A Hybrid Approach for Extending Automotive Radar Operation to NLOS Urban Scenarios",
    "url": "http://arxiv.org/abs/2503.05413v1",
    "arxiv_id": "2503.05413v1",
    "authors": [
      "Aviran Gal",
      "Igal Bilik"
    ],
    "published": "2025-03-07T13:37:47+00:00",
    "summary": "Automotive radar is a key component of sensing suites in autonomous driving (AD) and advanced driver-assist systems (ADAS). However, limited line-of-sight (LOS) significantly reduces radar efficiency in dense urban environments. Therefore, automotive radars need to extend their capabilities beyond LOS by localizing occluding and reflective surfaces and non-line-of-sight (NLOS) targets. This work addresses the NLOS target localization challenge by revisiting the NLOS radar signal propagation model and introducing a hybrid localization approach. The proposed approach first detects and localizes reflective surfaces, then identifies the LOS/NLOS propagation conditions, and finally localizes the target without prior scene knowledge, without using Doppler information, and without any auxiliary sensors. The proposed hybrid approach addresses the computational complexity challenge by integrating a physical radar electromagnetic wave propagation model with a deep neural network (DNN) to estimate occluding surface parameters. The efficiency of the proposed approach to localize the NLOS targets and to identify the NLOS/LOS propagation conditions is evaluated via simulations in a broad range of realistic automotive scenarios. Extending automotive radar sensing beyond LOS is expected to enhance the safety and reliability of autonomous and ADAS-equipped vehicles."
  },
  {
    "title": "Ontology Generation using Large Language Models",
    "url": "http://arxiv.org/abs/2503.05388v1",
    "arxiv_id": "2503.05388v1",
    "authors": [
      "Anna Sofia Lippolis",
      "Mohammad Javad Saeedizade",
      "Robin Keskis\u00e4rkk\u00e4",
      "Sara Zuppiroli",
      "Miguel Ceriani",
      "Aldo Gangemi",
      "Eva Blomqvist",
      "Andrea Giovanni Nuzzolese"
    ],
    "published": "2025-03-07T13:03:28+00:00",
    "summary": "The ontology engineering process is complex, time-consuming, and error-prone, even for experienced ontology engineers. In this work, we investigate the potential of Large Language Models (LLMs) to provide effective OWL ontology drafts directly from ontological requirements described using user stories and competency questions. Our main contribution is the presentation and evaluation of two new prompting techniques for automated ontology development: Memoryless CQbyCQ and Ontogenia. We also emphasize the importance of three structural criteria for ontology assessment, alongside expert qualitative evaluation, highlighting the need for a multi-dimensional evaluation in order to capture the quality and usability of the generated ontologies. Our experiments, conducted on a benchmark dataset of ten ontologies with 100 distinct CQs and 29 different user stories, compare the performance of three LLMs using the two prompting techniques. The results demonstrate improvements over the current state-of-the-art in LLM-supported ontology engineering. More specifically, the model OpenAI o1-preview with Ontogenia produces ontologies of sufficient quality to meet the requirements of ontology engineers, significantly outperforming novice ontology engineers in modelling ability. However, we still note some common mistakes and variability of result quality, which is important to take into account when using LLMs for ontology authoring support. We discuss these limitations and propose directions for future research."
  },
  {
    "title": "Shifting Perspectives: Steering Vector Ensembles for Robust Bias Mitigation in LLMs",
    "url": "http://arxiv.org/abs/2503.05371v1",
    "arxiv_id": "2503.05371v1",
    "authors": [
      "Zara Siddique",
      "Irtaza Khalid",
      "Liam D. Turner",
      "Luis Espinosa-Anke"
    ],
    "published": "2025-03-07T12:25:29+00:00",
    "summary": "We present a novel approach to bias mitigation in large language models (LLMs) by applying steering vectors to modify model activations in forward passes. We employ Bayesian optimization to systematically identify effective contrastive pair datasets across nine bias axes. When optimized on the BBQ dataset, our individually tuned steering vectors achieve average improvements of 12.2%, 4.7%, and 3.2% over the baseline for Mistral, Llama, and Qwen, respectively. Building on these promising results, we introduce Steering Vector Ensembles (SVE), a method that averages multiple individually optimized steering vectors, each targeting a specific bias axis such as age, race, or gender. By leveraging their collective strength, SVE outperforms individual steering vectors in both bias reduction and maintaining model performance. The work presents the first systematic investigation of steering vectors for bias mitigation, and we demonstrate that SVE is a powerful and computationally efficient strategy for reducing bias in LLMs, with broader implications for enhancing AI safety."
  },
  {
    "title": "Toward an Evaluation Science for Generative AI Systems",
    "url": "http://arxiv.org/abs/2503.05336v1",
    "arxiv_id": "2503.05336v1",
    "authors": [
      "Laura Weidinger",
      "Deb Raji",
      "Hanna Wallach",
      "Margaret Mitchell",
      "Angelina Wang",
      "Olawale Salaudeen",
      "Rishi Bommasani",
      "Sayash Kapoor",
      "Deep Ganguli",
      "Sanmi Koyejo",
      "William Isaac"
    ],
    "published": "2025-03-07T11:23:48+00:00",
    "summary": "There is an increasing imperative to anticipate and understand the performance and safety of generative AI systems in real-world deployment contexts. However, the current evaluation ecosystem is insufficient: Commonly used static benchmarks face validity challenges, and ad hoc case-by-case audits rarely scale. In this piece, we advocate for maturing an evaluation science for generative AI systems. While generative AI creates unique challenges for system safety engineering and measurement science, the field can draw valuable insights from the development of safety evaluation practices in other fields, including transportation, aerospace, and pharmaceutical engineering. In particular, we present three key lessons: Evaluation metrics must be applicable to real-world performance, metrics must be iteratively refined, and evaluation institutions and norms must be established. Applying these insights, we outline a concrete path toward a more rigorous approach for evaluating generative AI systems."
  },
  {
    "title": "Jailbreaking is (Mostly) Simpler Than You Think",
    "url": "http://arxiv.org/abs/2503.05264v1",
    "arxiv_id": "2503.05264v1",
    "authors": [
      "Mark Russinovich",
      "Ahmed Salem"
    ],
    "published": "2025-03-07T09:28:19+00:00",
    "summary": "We introduce the Context Compliance Attack (CCA), a novel, optimization-free method for bypassing AI safety mechanisms. Unlike current approaches -- which rely on complex prompt engineering and computationally intensive optimization -- CCA exploits a fundamental architectural vulnerability inherent in many deployed AI systems. By subtly manipulating conversation history, CCA convinces the model to comply with a fabricated dialogue context, thereby triggering restricted behavior. Our evaluation across a diverse set of open-source and proprietary models demonstrates that this simple attack can circumvent state-of-the-art safety protocols. We discuss the implications of these findings and propose practical mitigation strategies to fortify AI systems against such elementary yet effective adversarial tactics."
  },
  {
    "title": "ARbiter: Generating Dialogue Options and Communication Support in Augmented Reality",
    "url": "http://arxiv.org/abs/2503.05220v1",
    "arxiv_id": "2503.05220v1",
    "authors": [
      "Juli\u00e1n M\u00e9ndez",
      "Marc Satkowski"
    ],
    "published": "2025-03-07T08:16:30+00:00",
    "summary": "In this position paper, we propose researching the combination of Augmented Reality (AR) and Artificial Intelligence (AI) to support conversations, inspired by the interfaces of dialogue systems commonly found in videogames. AR-capable devices are becoming more powerful and conventional in looks, as seen in head-mounted displays (HMDs) like the Snapchat Spectacles, the XREAL glasses, or the recently presented Meta Orion. This development reduces possible ergonomic, appearance, and runtime concerns, thus allowing a more straightforward integration and extended use of AR in our everyday lives, both in private and at work. At the same time, we can observe an immense surge in AI development (also at CHI). Recently notorious Large Language Models (LLMs) like OpenAI's o3-mini or DeepSeek-R1 soar over their precursors in their ability to sustain conversations, provide suggestions, and handle complex topics in (almost) real time. In combination with natural language recognition systems, which are nowadays a standard component of smartphones and similar devices (including modern AR-HMDs), it is easy to imagine a combined system that integrates into daily conversations and provides various types of assistance. Such a system would enable many opportunities for research in AR+AI, which, as stated by Hirzle et al., remains scarce. In the following, we describe how the design of a conversational AR+AI system can learn from videogame dialogue systems, and we propose use cases and research questions that can be investigated thanks to this AR+AI combination."
  },
  {
    "title": "Safety-Critical Traffic Simulation with Adversarial Transfer of Driving Intentions",
    "url": "http://arxiv.org/abs/2503.05180v1",
    "arxiv_id": "2503.05180v1",
    "authors": [
      "Zherui Huang",
      "Xing Gao",
      "Guanjie Zheng",
      "Licheng Wen",
      "Xuemeng Yang",
      "Xiao Sun"
    ],
    "published": "2025-03-07T06:59:27+00:00",
    "summary": "Traffic simulation, complementing real-world data with a long-tail distribution, allows for effective evaluation and enhancement of the ability of autonomous vehicles to handle accident-prone scenarios. Simulating such safety-critical scenarios is nontrivial, however, from log data that are typically regular scenarios, especially in consideration of dynamic adversarial interactions between the future motions of autonomous vehicles and surrounding traffic participants. To address it, this paper proposes an innovative and efficient strategy, termed IntSim, that explicitly decouples the driving intentions of surrounding actors from their motion planning for realistic and efficient safety-critical simulation. We formulate the adversarial transfer of driving intention as an optimization problem, facilitating extensive exploration of diverse attack behaviors and efficient solution convergence. Simultaneously, intention-conditioned motion planning benefits from powerful deep models and large-scale real-world data, permitting the simulation of realistic motion behaviors for actors. Specially, through adapting driving intentions based on environments, IntSim facilitates the flexible realization of dynamic adversarial interactions with autonomous vehicles. Finally, extensive open-loop and closed-loop experiments on real-world datasets, including nuScenes and Waymo, demonstrate that the proposed IntSim achieves state-of-the-art performance in simulating realistic safety-critical scenarios and further improves planners in handling such scenarios."
  },
  {
    "title": "R1-Zero's \"Aha Moment\" in Visual Reasoning on a 2B Non-SFT Model",
    "url": "http://arxiv.org/abs/2503.05132v1",
    "arxiv_id": "2503.05132v1",
    "authors": [
      "Hengguang Zhou",
      "Xirui Li",
      "Ruochen Wang",
      "Minhao Cheng",
      "Tianyi Zhou",
      "Cho-Jui Hsieh"
    ],
    "published": "2025-03-07T04:21:47+00:00",
    "summary": "Recently DeepSeek R1 demonstrated how reinforcement learning with simple rule-based incentives can enable autonomous development of complex reasoning in large language models, characterized by the \"aha moment\", in which the model manifest self-reflection and increased response length during training. However, attempts to extend this success to multimodal reasoning often failed to reproduce these key characteristics. In this report, we present the first successful replication of these emergent characteristics for multimodal reasoning on only a non-SFT 2B model. Starting with Qwen2-VL-2B and applying reinforcement learning directly on the SAT dataset, our model achieves 59.47% accuracy on CVBench, outperforming the base model by approximately ~30% and exceeding both SFT setting by ~2%. In addition, we share our failed attempts and insights in attempting to achieve R1-like reasoning using RL with instruct models. aiming to shed light on the challenges involved. Our key observations include: (1) applying RL on instruct model often results in trivial reasoning trajectories, and (2) naive length reward are ineffective in eliciting reasoning capabilities. The project code is available at https://github.com/turningpoint-ai/VisualThinker-R1-Zero"
  },
  {
    "title": "Enhancing Autonomous Vehicle-Pedestrian Interaction in Shared Spaces: The Impact of Intended Path-Projection",
    "url": "http://arxiv.org/abs/2503.05041v1",
    "arxiv_id": "2503.05041v1",
    "authors": [
      "Le Yue",
      "Tram Thi Minh Tran",
      "Xinyan Yu",
      "Marius Hoggenmueller"
    ],
    "published": "2025-03-06T23:34:02+00:00",
    "summary": "External Human-Machine Interfaces (eHMIs) are critical for seamless interactions between autonomous vehicles (AVs) and pedestrians in shared spaces. However, they often struggle to adapt to these environments, where pedestrian movement is fluid and right-of-way is ambiguous. To address these challenges, we propose PaveFlow, an eHMI that projects the AV's intended path onto the ground in real time, providing continuous spatial information rather than a binary stop/go signal. Through a VR study (N=18), we evaluated PaveFlow's effectiveness under two AV density conditions (single vs. multiple AVs) and a baseline condition without PaveFlow. The results showed that PaveFlow significantly improved pedestrian perception of safety, trust, and user experience while reducing cognitive workload. This performance remained consistent across both single and multiple AV conditions, despite persistent tensions in priority negotiation. These findings suggest that path projection enhances eHMI transparency by offering richer movement cues, which may better support AV-pedestrian interaction in shared spaces."
  },
  {
    "title": "Safety is Not Only About Refusal: Reasoning-Enhanced Fine-tuning for Interpretable LLM Safety",
    "url": "http://arxiv.org/abs/2503.05021v1",
    "arxiv_id": "2503.05021v1",
    "authors": [
      "Yuyou Zhang",
      "Miao Li",
      "William Han",
      "Yihang Yao",
      "Zhepeng Cen",
      "Ding Zhao"
    ],
    "published": "2025-03-06T22:47:45+00:00",
    "summary": "Large Language Models (LLMs) are vulnerable to jailbreak attacks that exploit weaknesses in traditional safety alignment, which often relies on rigid refusal heuristics or representation engineering to block harmful outputs. While they are effective for direct adversarial attacks, they fall short of broader safety challenges requiring nuanced, context-aware decision-making. To address this, we propose Reasoning-enhanced Finetuning for interpretable LLM Safety (Rational), a novel framework that trains models to engage in explicit safe reasoning before response. Fine-tuned models leverage the extensive pretraining knowledge in self-generated reasoning to bootstrap their own safety through structured reasoning, internalizing context-sensitive decision-making. Our findings suggest that safety extends beyond refusal, requiring context awareness for more robust, interpretable, and adaptive responses. Reasoning is not only a core capability of LLMs but also a fundamental mechanism for LLM safety. Rational employs reasoning-enhanced fine-tuning, allowing it to reject harmful prompts while providing meaningful and context-aware responses in complex scenarios."
  },
  {
    "title": "Quantifying and Modeling Driving Styles in Trajectory Forecasting",
    "url": "http://arxiv.org/abs/2503.04994v1",
    "arxiv_id": "2503.04994v1",
    "authors": [
      "Laura Zheng",
      "Hamidreza Yaghoubi Araghi",
      "Tony Wu",
      "Sandeep Thalapanane",
      "Tianyi Zhou",
      "Ming C. Lin"
    ],
    "published": "2025-03-06T21:47:49+00:00",
    "summary": "Trajectory forecasting has become a popular deep learning task due to its relevance for scenario simulation for autonomous driving. Specifically, trajectory forecasting predicts the trajectory of a short-horizon future for specific human drivers in a particular traffic scenario. Robust and accurate future predictions can enable autonomous driving planners to optimize for low-risk and predictable outcomes for human drivers around them. Although some work has been done to model driving style in planning and personalized autonomous polices, a gap exists in explicitly modeling human driving styles for trajectory forecasting of human behavior. Human driving style is most certainly a correlating factor to decision making, especially in edge-case scenarios where risk is nontrivial, as justified by the large amount of traffic psychology literature on risky driving. So far, the current real-world datasets for trajectory forecasting lack insight on the variety of represented driving styles. While the datasets may represent real-world distributions of driving styles, we posit that fringe driving style types may also be correlated with edge-case safety scenarios. In this work, we conduct analyses on existing real-world trajectory datasets for driving and dissect these works from the lens of driving styles, which is often intangible and non-standardized."
  },
  {
    "title": "From Voice to Safety: Language AI Powered Pilot-ATC Communication Understanding for Airport Surface Movement Collision Risk Assessment",
    "url": "http://arxiv.org/abs/2503.04974v1",
    "arxiv_id": "2503.04974v1",
    "authors": [
      "Yutian Pang",
      "Andrew Paul Kendall",
      "Alex Porcayo",
      "Mariah Barsotti",
      "Anahita Jain",
      "John-Paul Clarke"
    ],
    "published": "2025-03-06T21:08:07+00:00",
    "summary": "This work integrates language AI-based voice communication understanding with collision risk assessment. The proposed framework consists of two major parts, (a) Automatic Speech Recognition (ASR); (b) surface collision risk modeling. ASR module generates information tables by processing voice communication transcripts, which serve as references for producing potential taxi plans and calculating the surface movement collision risk. For ASR, we collect and annotate our own Named Entity Recognition (NER) dataset based on open-sourced video recordings and safety investigation reports. Additionally, we refer to FAA Order JO 7110.65W and FAA Order JO 7340.2N to get the list of heuristic rules and phase contractions of communication between the pilot and the Air Traffic Controller (ATCo) used in daily aviation operations. Then, we propose the novel ATC Rule-Enhanced NER method, which integrates the heuristic rules into the model training and inference stages, resulting into hybrid rule-based NER model. We show the effectiveness of this hybrid approach by comparing different setups with different token-level embedding models. For the risk modeling, we adopt the node-link airport layout graph from NASA FACET and model the aircraft taxi speed at each link as a log-normal distribution and derive the total taxi time distribution. Then, we propose a spatiotemporal formulation of the risk probability of two aircraft moving across potential collision nodes during ground movement. We show the effectiveness of our approach by simulating two case studies, (a) the Henada airport runway collision accident happened in January 2024; (b) the KATL taxiway collision happened in September 2024. We show that, by understanding the pilot-ATC communication transcripts and analyzing surface movement patterns, the proposed model improves airport safety by providing risk assessment in time."
  },
  {
    "title": "Data-Efficient Learning from Human Interventions for Mobile Robots",
    "url": "http://arxiv.org/abs/2503.04969v1",
    "arxiv_id": "2503.04969v1",
    "authors": [
      "Zhenghao Peng",
      "Zhizheng Liu",
      "Bolei Zhou"
    ],
    "published": "2025-03-06T21:02:02+00:00",
    "summary": "Mobile robots are essential in applications such as autonomous delivery and hospitality services. Applying learning-based methods to address mobile robot tasks has gained popularity due to its robustness and generalizability. Traditional methods such as Imitation Learning (IL) and Reinforcement Learning (RL) offer adaptability but require large datasets, carefully crafted reward functions, and face sim-to-real gaps, making them challenging for efficient and safe real-world deployment. We propose an online human-in-the-loop learning method PVP4Real that combines IL and RL to address these issues. PVP4Real enables efficient real-time policy learning from online human intervention and demonstration, without reward or any pretraining, significantly improving data efficiency and training safety. We validate our method by training two different robots -- a legged quadruped, and a wheeled delivery robot -- in two mobile robot tasks, one of which even uses raw RGBD image as observation. The training finishes within 15 minutes. Our experiments show the promising future of human-in-the-loop learning in addressing the data efficiency issue in real-world robotic tasks. More information is available at: https://metadriverse.github.io/pvp4real/"
  },
  {
    "title": "SafeArena: Evaluating the Safety of Autonomous Web Agents",
    "url": "http://arxiv.org/abs/2503.04957v1",
    "arxiv_id": "2503.04957v1",
    "authors": [
      "Ada Defne Tur",
      "Nicholas Meade",
      "Xing Han L\u00f9",
      "Alejandra Zambrano",
      "Arkil Patel",
      "Esin Durmus",
      "Spandana Gella",
      "Karolina Sta\u0144czak",
      "Siva Reddy"
    ],
    "published": "2025-03-06T20:43:14+00:00",
    "summary": "LLM-based agents are becoming increasingly proficient at solving web-based tasks. With this capability comes a greater risk of misuse for malicious purposes, such as posting misinformation in an online forum or selling illicit substances on a website. To evaluate these risks, we propose SafeArena, the first benchmark to focus on the deliberate misuse of web agents. SafeArena comprises 250 safe and 250 harmful tasks across four websites. We classify the harmful tasks into five harm categories -- misinformation, illegal activity, harassment, cybercrime, and social bias, designed to assess realistic misuses of web agents. We evaluate leading LLM-based web agents, including GPT-4o, Claude-3.5 Sonnet, Qwen-2-VL 72B, and Llama-3.2 90B, on our benchmark. To systematically assess their susceptibility to harmful tasks, we introduce the Agent Risk Assessment framework that categorizes agent behavior across four risk levels. We find agents are surprisingly compliant with malicious requests, with GPT-4o and Qwen-2 completing 34.7% and 27.3% of harmful requests, respectively. Our findings highlight the urgent need for safety alignment procedures for web agents. Our benchmark is available here: https://safearena.github.io"
  },
  {
    "title": "SAFE-TAXI: A Hierarchical Multi-UAS Safe Auto-Taxiing Framework with Runtime Safety Assurance and Conflict Resolution",
    "url": "http://arxiv.org/abs/2503.04942v1",
    "arxiv_id": "2503.04942v1",
    "authors": [
      "Kartik A. Pant",
      "Li-Yu Lin",
      "Worawis Sribunma",
      "Sabine Brunswicker",
      "James M. Goppert",
      "Inseok Hwang"
    ],
    "published": "2025-03-06T20:18:01+00:00",
    "summary": "We present a hierarchical safe auto-taxiing framework to enhance the automated ground operations of multiple unmanned aircraft systems (multi-UAS). The auto-taxiing problem becomes particularly challenging due to (i) unknown disturbances, such as crosswind affecting the aircraft dynamics, (ii) taxiway incursions due to unplanned obstacles, and (iii) spatiotemporal conflicts at the intersections between multiple entry points in the taxiway. To address these issues, we propose a hierarchical framework, i.e., SAFE-TAXI, combining centralized spatiotemporal planning with decentralized MPC-CBF-based control to safely navigate the aircraft through the taxiway while avoiding intersection conflicts and unplanned obstacles (e.g., other aircraft or ground vehicles). Our proposed framework decouples the auto-taxiing problem temporally into conflict resolution and motion planning, respectively. Conflict resolution is handled in a centralized manner by computing conflict-aware reference trajectories for each aircraft. In contrast, safety assurance from unplanned obstacles is handled by an MPC-CBF-based controller implemented in a decentralized manner. We demonstrate the effectiveness of our proposed framework through numerical simulations and experimentally validate it using Night Vapor, a small-scale fixed-wing test platform."
  },
  {
    "title": "Neural Configuration-Space Barriers for Manipulation Planning and Control",
    "url": "http://arxiv.org/abs/2503.04929v1",
    "arxiv_id": "2503.04929v1",
    "authors": [
      "Kehan Long",
      "Ki Myung Brian Lee",
      "Nikola Raicevic",
      "Niyas Attasseri",
      "Melvin Leok",
      "Nikolay Atanasov"
    ],
    "published": "2025-03-06T20:00:56+00:00",
    "summary": "Planning and control for high-dimensional robot manipulators in cluttered, dynamic environments require both computational efficiency and robust safety guarantees. Inspired by recent advances in learning configuration-space distance functions (CDFs) as robot body representations, we propose a unified framework for motion planning and control that formulates safety constraints as CDF barriers. A CDF barrier approximates the local free configuration space, substantially reducing the number of collision-checking operations during motion planning. However, learning a CDF barrier with a neural network and relying on online sensor observations introduce uncertainties that must be considered during control synthesis. To address this, we develop a distributionally robust CDF barrier formulation for control that explicitly accounts for modeling errors and sensor noise without assuming a known underlying distribution. Simulations and hardware experiments on a 6-DoF xArm manipulator show that our neural CDF barrier formulation enables efficient planning and robust real-time safe control in cluttered and dynamic environments, relying only on onboard point-cloud observations."
  },
  {
    "title": "Quantifying the Reasoning Abilities of LLMs on Real-world Clinical Cases",
    "url": "http://arxiv.org/abs/2503.04691v1",
    "arxiv_id": "2503.04691v1",
    "authors": [
      "Pengcheng Qiu",
      "Chaoyi Wu",
      "Shuyu Liu",
      "Weike Zhao",
      "Ya Zhang",
      "Yanfeng Wang",
      "Weidi Xie"
    ],
    "published": "2025-03-06T18:35:39+00:00",
    "summary": "The latest reasoning-enhanced large language models (reasoning LLMs), such as DeepSeek-R1 and OpenAI-o3, have demonstrated remarkable success. However, the application of such reasoning enhancements to the highly professional medical domain has not been clearly evaluated, particularly regarding with not only assessing the final generation but also examining the quality of their reasoning processes. In this study, we present MedR-Bench, a reasoning-focused medical evaluation benchmark comprising 1,453 structured patient cases with reasoning references mined from case reports. Our benchmark spans 13 body systems and 10 specialty disorders, encompassing both common and rare diseases. In our evaluation, we introduce a versatile framework consisting of three critical clinical stages: assessment recommendation, diagnostic decision-making, and treatment planning, comprehensively capturing the LLMs' performance across the entire patient journey in healthcare. For metrics, we propose a novel agentic system, Reasoning Evaluator, designed to automate and objectively quantify free-text reasoning responses in a scalable manner from the perspectives of efficiency, factuality, and completeness by dynamically searching and performing cross-referencing checks. As a result, we assess five state-of-the-art reasoning LLMs, including DeepSeek-R1, OpenAI-o3-mini, and others. Our results reveal that current LLMs can handle relatively simple diagnostic tasks with sufficient critical assessment results, achieving accuracy generally over 85%. However, they still struggle with more complex tasks, such as assessment recommendation and treatment planning. In reasoning, their reasoning processes are generally reliable, with factuality scores exceeding 90%, though they often omit critical reasoning steps. Our study clearly reveals further development directions for current clinical LLMs."
  },
  {
    "title": "START: Self-taught Reasoner with Tools",
    "url": "http://arxiv.org/abs/2503.04625v1",
    "arxiv_id": "2503.04625v1",
    "authors": [
      "Chengpeng Li",
      "Mingfeng Xue",
      "Zhenru Zhang",
      "Jiaxi Yang",
      "Beichen Zhang",
      "Xiang Wang",
      "Bowen Yu",
      "Binyuan Hui",
      "Junyang Lin",
      "Dayiheng Liu"
    ],
    "published": "2025-03-06T17:11:51+00:00",
    "summary": "Large reasoning models (LRMs) like OpenAI-o1 and DeepSeek-R1 have demonstrated remarkable capabilities in complex reasoning tasks through the utilization of long Chain-of-thought (CoT). However, these models often suffer from hallucinations and inefficiencies due to their reliance solely on internal reasoning processes. In this paper, we introduce START (Self-Taught Reasoner with Tools), a novel tool-integrated long CoT reasoning LLM that significantly enhances reasoning capabilities by leveraging external tools. Through code execution, START is capable of performing complex computations, self-checking, exploring diverse methods, and self-debugging, thereby addressing the limitations of LRMs. The core innovation of START lies in its self-learning framework, which comprises two key techniques: 1) Hint-infer: We demonstrate that inserting artificially designed hints (e.g., ``Wait, maybe using Python here is a good idea.'') during the inference process of a LRM effectively stimulates its ability to utilize external tools without the need for any demonstration data. Hint-infer can also serve as a simple and effective sequential test-time scaling method; 2) Hint Rejection Sampling Fine-Tuning (Hint-RFT): Hint-RFT combines Hint-infer and RFT by scoring, filtering, and modifying the reasoning trajectories with tool invocation generated by a LRM via Hint-infer, followed by fine-tuning the LRM. Through this framework, we have fine-tuned the QwQ-32B model to achieve START. On PhD-level science QA (GPQA), competition-level math benchmarks (AMC23, AIME24, AIME25), and the competition-level code benchmark (LiveCodeBench), START achieves accuracy rates of 63.6%, 95.0%, 66.7%, 47.1%, and 47.3%, respectively. It significantly outperforms the base QwQ-32B and achieves performance comparable to the state-of-the-art open-weight model R1-Distill-Qwen-32B and the proprietary model o1-Preview."
  },
  {
    "title": "START: Self-taught Reasoner with Tools",
    "url": "http://arxiv.org/abs/2503.04625v2",
    "arxiv_id": "2503.04625v2",
    "authors": [
      "Chengpeng Li",
      "Mingfeng Xue",
      "Zhenru Zhang",
      "Jiaxi Yang",
      "Beichen Zhang",
      "Xiang Wang",
      "Bowen Yu",
      "Binyuan Hui",
      "Junyang Lin",
      "Dayiheng Liu"
    ],
    "published": "2025-03-06T17:11:51+00:00",
    "summary": "Large reasoning models (LRMs) like OpenAI-o1 and DeepSeek-R1 have demonstrated remarkable capabilities in complex reasoning tasks through the utilization of long Chain-of-thought (CoT). However, these models often suffer from hallucinations and inefficiencies due to their reliance solely on internal reasoning processes. In this paper, we introduce START (Self-Taught Reasoner with Tools), a novel tool-integrated long CoT reasoning LLM that significantly enhances reasoning capabilities by leveraging external tools. Through code execution, START is capable of performing complex computations, self-checking, exploring diverse methods, and self-debugging, thereby addressing the limitations of LRMs. The core innovation of START lies in its self-learning framework, which comprises two key techniques: 1) Hint-infer: We demonstrate that inserting artificially designed hints (e.g., ``Wait, maybe using Python here is a good idea.'') during the inference process of a LRM effectively stimulates its ability to utilize external tools without the need for any demonstration data. Hint-infer can also serve as a simple and effective sequential test-time scaling method; 2) Hint Rejection Sampling Fine-Tuning (Hint-RFT): Hint-RFT combines Hint-infer and RFT by scoring, filtering, and modifying the reasoning trajectories with tool invocation generated by a LRM via Hint-infer, followed by fine-tuning the LRM. Through this framework, we have fine-tuned the QwQ-32B model to achieve START. On PhD-level science QA (GPQA), competition-level math benchmarks (AMC23, AIME24, AIME25), and the competition-level code benchmark (LiveCodeBench), START achieves accuracy rates of 63.6%, 95.0%, 66.7%, 47.1%, and 47.3%, respectively. It significantly outperforms the base QwQ-32B and achieves performance comparable to the state-of-the-art open-weight model R1-Distill-Qwen-32B and the proprietary model o1-Preview."
  },
  {
    "title": "TinyR1-32B-Preview: Boosting Accuracy with Branch-Merge Distillation",
    "url": "http://arxiv.org/abs/2503.04872v1",
    "arxiv_id": "2503.04872v1",
    "authors": [
      "Lin Sun",
      "Guangxiang Zhao",
      "Xiaoqi Jian",
      "Yuhan Wu",
      "Weihong Lin",
      "Yongfu Zhu",
      "Change Jia",
      "Linglin Zhang",
      "Jinzhu Wu",
      "Junfeng Ran",
      "Sai-er Hu",
      "Zihan Jiang",
      "Junting Zhou",
      "Wenrui Liu",
      "Bin Cui",
      "Tong Yang",
      "Xiangzheng Zhang"
    ],
    "published": "2025-03-06T16:25:53+00:00",
    "summary": "The challenge of reducing the size of Large Language Models (LLMs) while maintaining their performance has gained significant attention. However, existing methods, such as model distillation and transfer learning, often fail to achieve high accuracy. To address this limitation, we introduce the Branch-Merge distillation approach, which enhances model compression through two phases: (1) the Branch Phase, where knowledge from a large teacher model is \\textit{selectively distilled} into specialized student models via domain-specific supervised fine-tuning (SFT); And (2) the Merge Phase, where these student models are merged to enable cross-domain knowledge transfer and improve generalization. We validate our distillation approach using DeepSeek-R1 as the teacher and DeepSeek-R1-Distill-Qwen-32B as the student. The resulting merged model, TinyR1-32B-Preview, outperforms its counterpart DeepSeek-R1-Distill-Qwen-32B across multiple benchmarks, including Mathematics (+5.5 points), Coding (+4.4 points) and Science (+2.9 points), while achieving near-equal performance to DeepSeek-R1 on AIME 2024. The Branch-Merge distillation approach provides a scalable solution for creating smaller, high-performing LLMs with reduced computational cost and time."
  },
  {
    "title": "Occlusion-Aware Consistent Model Predictive Control for Robot Navigation in Occluded Obstacle-Dense Environments",
    "url": "http://arxiv.org/abs/2503.04563v1",
    "arxiv_id": "2503.04563v1",
    "authors": [
      "Minzhe Zheng",
      "Lei Zheng",
      "Lei Zhu",
      "Jun Ma"
    ],
    "published": "2025-03-06T15:52:59+00:00",
    "summary": "Ensuring safety and motion consistency for robot navigation in occluded, obstacle-dense environments is a critical challenge. In this context, this study presents an occlusion-aware Consistent Model Predictive Control (CMPC) strategy. To account for the occluded obstacles, it incorporates adjustable risk regions that represent their potential future locations. Subsequently, dynamic risk boundary constraints are developed online to ensure safety. The CMPC then constructs multiple locally optimal trajectory branches (each tailored to different risk regions) to balance between exploitation and exploration. A shared consensus trunk is generated to ensure smooth transitions between branches without significant velocity fluctuations, further preserving motion consistency. To facilitate high computational efficiency and ensure coordination across local trajectories, we use the alternating direction method of multipliers (ADMM) to decompose the CMPC into manageable sub-problems for parallel solving. The proposed strategy is validated through simulation and real-world experiments on an Ackermann-steering robot platform. The results demonstrate the effectiveness of the proposed CMPC strategy through comparisons with baseline approaches in occluded, obstacle-dense environments."
  },
  {
    "title": "Compositional Causal Reasoning Evaluation in Language Models",
    "url": "http://arxiv.org/abs/2503.04556v1",
    "arxiv_id": "2503.04556v1",
    "authors": [
      "Jacqueline R. M. A. Maasch",
      "Alihan H\u00fcy\u00fck",
      "Xinnuo Xu",
      "Aditya V. Nori",
      "Javier Gonzalez"
    ],
    "published": "2025-03-06T15:47:19+00:00",
    "summary": "Causal reasoning and compositional reasoning are two core aspirations in generative AI. Measuring the extent of these behaviors requires principled evaluation methods. We explore a unified perspective that considers both behaviors simultaneously, termed compositional causal reasoning (CCR): the ability to infer how causal measures compose and, equivalently, how causal quantities propagate through graphs. We instantiate a framework for the systematic evaluation of CCR for the average treatment effect and the probability of necessity and sufficiency. As proof of concept, we demonstrate the design of CCR tasks for language models in the LLama, Phi, and GPT families. On a math word problem, our framework revealed a range of taxonomically distinct error patterns. Additionally, CCR errors increased with the complexity of causal paths for all models except o1."
  },
  {
    "title": "Benchmarking Reasoning Robustness in Large Language Models",
    "url": "http://arxiv.org/abs/2503.04550v1",
    "arxiv_id": "2503.04550v1",
    "authors": [
      "Tong Yu",
      "Yongcheng Jing",
      "Xikun Zhang",
      "Wentao Jiang",
      "Wenjie Wu",
      "Yingjie Wang",
      "Wenbin Hu",
      "Bo Du",
      "Dacheng Tao"
    ],
    "published": "2025-03-06T15:36:06+00:00",
    "summary": "Despite the recent success of large language models (LLMs) in reasoning such as DeepSeek, we for the first time identify a key dilemma in reasoning robustness and generalization: significant performance degradation on novel or incomplete data, suggesting a reliance on memorized patterns rather than systematic reasoning. Our closer examination reveals four key unique limitations underlying this issue:(1) Positional bias--models favor earlier queries in multi-query inputs but answering the wrong one in the latter (e.g., GPT-4o's accuracy drops from 75.8 percent to 72.8 percent); (2) Instruction sensitivity--performance declines by 5.0 to 7.5 percent in the Qwen2.5 Series and by 5.0 percent in DeepSeek-V3 with auxiliary guidance; (3) Numerical fragility--value substitution sharply reduces accuracy (e.g., GPT-4o drops from 97.5 percent to 82.5 percent, GPT-o1-mini drops from 97.5 percent to 92.5 percent); and (4) Memory dependence--models resort to guesswork when missing critical data. These findings further highlight the reliance on heuristic recall over rigorous logical inference, demonstrating challenges in reasoning robustness. To comprehensively investigate these robustness challenges, this paper introduces a novel benchmark, termed as Math-RoB, that exploits hallucinations triggered by missing information to expose reasoning gaps. This is achieved by an instruction-based approach to generate diverse datasets that closely resemble training distributions, facilitating a holistic robustness assessment and advancing the development of more robust reasoning frameworks. Bad character(s) in field Abstract."
  },
  {
    "title": "An Empirical Study on Eliciting and Improving R1-like Reasoning Models",
    "url": "http://arxiv.org/abs/2503.04548v1",
    "arxiv_id": "2503.04548v1",
    "authors": [
      "Zhipeng Chen",
      "Yingqian Min",
      "Beichen Zhang",
      "Jie Chen",
      "Jinhao Jiang",
      "Daixuan Cheng",
      "Wayne Xin Zhao",
      "Zheng Liu",
      "Xu Miao",
      "Yang Lu",
      "Lei Fang",
      "Zhongyuan Wang",
      "Ji-Rong Wen"
    ],
    "published": "2025-03-06T15:34:27+00:00",
    "summary": "In this report, we present the third technical report on the development of slow-thinking models as part of the STILL project. As the technical pathway becomes clearer, scaling RL training has become a central technique for implementing such reasoning models. We systematically experiment with and document the effects of various factors influencing RL training, conducting experiments on both base models and fine-tuned models. Specifically, we demonstrate that our RL training approach consistently improves the Qwen2.5-32B base models, enhancing both response length and test accuracy. Furthermore, we show that even when a model like DeepSeek-R1-Distill-Qwen-1.5B has already achieved a high performance level, it can be further refined through RL training, reaching an accuracy of 39.33% on AIME 2024. Beyond RL training, we also explore the use of tool manipulation, finding that it significantly boosts the reasoning performance of large reasoning models. This approach achieves a remarkable accuracy of 86.67% with greedy search on AIME 2024, underscoring its effectiveness in enhancing model capabilities. We release our resources at the STILL project website: https://github.com/RUCAIBox/Slow_Thinking_with_LLMs."
  },
  {
    "title": "Method for recovering data on unreported low-severity crashes",
    "url": "http://arxiv.org/abs/2503.04529v1",
    "arxiv_id": "2503.04529v1",
    "authors": [
      "Alberto Morando"
    ],
    "published": "2025-03-06T15:18:45+00:00",
    "summary": "Objective: Many low-severity crashes are not reported due to sampling criteria, introducing missing not at random (MNAR) bias. If not addressed, MNAR bias can lead to inaccurate safety analyses. This paper illustrates a statistical method to address such bias. Methods: We defined a custom probability distribution for the observed data as a product of an exponential population distribution and a logistic reporting function. We used modern Bayesian probabilistic programming techniques. Results: Using simulated data, we verified the correctness of the procedure. Applying it to real crash data, we estimated the {\\Delta}v distribution for passenger vehicles involved in personal damage-only (PDO) rear-end crashes. We found that about 77% of cases are unreported. Conclusions: The method preserves the original data and it accounts well for uncertainty from both modeling assumptions and input data. It can improve safety assessments and it applies broadly to other MNAR cases."
  },
  {
    "title": "Method for recovering data on unreported low-severity crashes",
    "url": "http://arxiv.org/abs/2503.04529v2",
    "arxiv_id": "2503.04529v2",
    "authors": [
      "Alberto Morando"
    ],
    "published": "2025-03-06T15:18:45+00:00",
    "summary": "Objective: Many low-severity crashes are not reported due to sampling criteria, introducing missing not at random (MNAR) bias. If not addressed, MNAR bias can lead to inaccurate safety analyses. This paper illustrates a statistical method to address such bias. Methods: We defined a custom probability distribution for the observed data as a product of an exponential population distribution and a logistic reporting function. We used modern Bayesian probabilistic programming techniques. Results: Using simulated data, we verified the correctness of the procedure. Applying it to real crash data, we estimated the {\\Delta}v distribution for passenger vehicles involved in personal damage-only (PDO) rear-end crashes. We found that about 77% of cases are unreported. Conclusions: The method preserves the original data and it accounts well for uncertainty from both modeling assumptions and input data. It can improve safety assessments and it applies broadly to other MNAR cases."
  },
  {
    "title": "Research on a Driver's Perceived Risk Prediction Model Considering Traffic Scene Interaction",
    "url": "http://arxiv.org/abs/2503.04516v1",
    "arxiv_id": "2503.04516v1",
    "authors": [
      "Chenhao Yang",
      "Siwei Huang",
      "Chuan Hu"
    ],
    "published": "2025-03-06T15:03:34+00:00",
    "summary": "In the field of conditional autonomous driving technology, driver perceived risk prediction plays a crucial role in reducing traffic risks and ensuring passenger safety. This study introduces an innovative perceived risk prediction model for human-machine interaction in intelligent driving systems. The model aims to enhance prediction accuracy and, thereby, ensure passenger safety. Through a comprehensive analysis of risk impact mechanisms, we identify three key categories of factors, both subjective and objective, influencing perceived risk: driver's personal characteristics, ego-vehicle motion, and surrounding environment characteristics. We then propose a deep-learning-based risk prediction network that uses the first two categories of factors as inputs. The network captures the interactive relationships among traffic participants in dynamic driving scenarios. Additionally, we design a personalized modeling strategy that incorporates driver-specific traits to improve prediction accuracy. To ensure high-quality training data, we conducted a rigorous video rating experiment. Experimental results show that the proposed network achieves a 10.0% performance improvement over state-of-the-art methods. These findings suggest that the proposed network has significant potential to enhance the safety of conditional autonomous driving systems."
  },
  {
    "title": "Know Thy Judge: On the Robustness Meta-Evaluation of LLM Safety Judges",
    "url": "http://arxiv.org/abs/2503.04474v1",
    "arxiv_id": "2503.04474v1",
    "authors": [
      "Francisco Eiras",
      "Eliott Zemour",
      "Eric Lin",
      "Vaikkunth Mugunthan"
    ],
    "published": "2025-03-06T14:24:12+00:00",
    "summary": "Large Language Model (LLM) based judges form the underpinnings of key safety evaluation processes such as offline benchmarking, automated red-teaming, and online guardrailing. This widespread requirement raises the crucial question: can we trust the evaluations of these evaluators? In this paper, we highlight two critical challenges that are typically overlooked: (i) evaluations in the wild where factors like prompt sensitivity and distribution shifts can affect performance and (ii) adversarial attacks that target the judge. We highlight the importance of these through a study of commonly used safety judges, showing that small changes such as the style of the model output can lead to jumps of up to 0.24 in the false negative rate on the same dataset, whereas adversarial attacks on the model generation can fool some judges into misclassifying 100% of harmful generations as safe ones. These findings reveal gaps in commonly used meta-evaluation benchmarks and weaknesses in the robustness of current LLM judges, indicating that low attack success under certain judges could create a false sense of security."
  },
  {
    "title": "Activation Space Interventions Can Be Transferred Between Large Language Models",
    "url": "http://arxiv.org/abs/2503.04429v1",
    "arxiv_id": "2503.04429v1",
    "authors": [
      "Narmeen Oozeer",
      "Dhruv Nathawani",
      "Nirmalendu Prakash",
      "Michael Lan",
      "Abir Harrasse",
      "Amirali Abdullah"
    ],
    "published": "2025-03-06T13:38:44+00:00",
    "summary": "The study of representation universality in AI models reveals growing convergence across domains, modalities, and architectures. However, the practical applications of representation universality remain largely unexplored. We bridge this gap by demonstrating that safety interventions can be transferred between models through learned mappings of their shared activation spaces. We demonstrate this approach on two well-established AI safety tasks: backdoor removal and refusal of harmful prompts, showing successful transfer of steering vectors that alter the models' outputs in a predictable way. Additionally, we propose a new task, \\textit{corrupted capabilities}, where models are fine-tuned to embed knowledge tied to a backdoor. This tests their ability to separate useful skills from backdoors, reflecting real-world challenges. Extensive experiments across Llama, Qwen and Gemma model families show that our method enables using smaller models to efficiently align larger ones. Furthermore, we demonstrate that autoencoder mappings between base and fine-tuned models can serve as reliable ``lightweight safety switches\", allowing dynamic toggling between model behaviors."
  },
  {
    "title": "FORTALESA: Fault-Tolerant Reconfigurable Systolic Array for DNN Inference",
    "url": "http://arxiv.org/abs/2503.04426v1",
    "arxiv_id": "2503.04426v1",
    "authors": [
      "Natalia Cherezova",
      "Artur Jutman",
      "Maksim Jenihhin"
    ],
    "published": "2025-03-06T13:35:59+00:00",
    "summary": "The emergence of Deep Neural Networks (DNNs) in mission- and safety-critical applications brings their reliability to the front. High performance demands of DNNs require the use of specialized hardware accelerators. Systolic array architecture is widely used in DNN accelerators due to its parallelism and regular structure. This work presents a run-time reconfigurable systolic array architecture with three execution modes and four implementation options. All four implementations are evaluated in terms of resource utilization, throughput, and fault tolerance improvement. The proposed architecture is used for reliability enhancement of DNN inference on systolic array through heterogeneous mapping of different network layers to different execution modes. The approach is supported by a novel reliability assessment method based on fault propagation analysis. It is used for the exploration of the appropriate execution mode-layer mapping for DNN inference. The proposed architecture efficiently protects registers and MAC units of systolic array PEs from transient and permanent faults. The reconfigurability feature enables a speedup of up to $3\\times$, depending on layer vulnerability. Furthermore, it requires $6\\times$ less resources compared to static redundancy and $2.5\\times$ less resources compared to the previously proposed solution for transient faults."
  },
  {
    "title": "On the Analysis of Stability, Sensitivity and Transparency in Variable Admittance Control for pHRI Enhanced by Virtual Fixtures",
    "url": "http://arxiv.org/abs/2503.04414v1",
    "arxiv_id": "2503.04414v1",
    "authors": [
      "Davide Tebaldi",
      "Dario Onfiani",
      "Luigi Biagiotti"
    ],
    "published": "2025-03-06T13:15:19+00:00",
    "summary": "The interest in Physical Human-Robot Interaction (pHRI) has significantly increased over the last two decades thanks to the availability of collaborative robots that guarantee user safety during force exchanges. For this reason, stability concerns have been addressed extensively in the literature while proposing new control schemes for pHRI applications. Because of the nonlinear nature of robots, stability analyses generally leverage passivity concepts. On the other hand, the proposed algorithms generally consider ideal models of robot manipulators. For this reason, the primary objective of this paper is to conduct a detailed analysis of the sources of instability for a class of pHRI control schemes, namely proxy-based constrained admittance controllers, by considering parasitic effects such as transmission elasticity, motor velocity saturation, and actuation delay. Next, a sensitivity analysis supported by experimental results is carried out, in order to identify how the control parameters affect the stability of the overall system. Finally, an adaptation technique for the proxy parameters is proposed with the goal of maximizing transparency in pHRI. The proposed adaptation method is validated through both simulations and experimental tests."
  },
  {
    "title": "Dedicated Feedback and Edit Models Empower Inference-Time Scaling for Open-Ended General-Domain Tasks",
    "url": "http://arxiv.org/abs/2503.04378v1",
    "arxiv_id": "2503.04378v1",
    "authors": [
      "Zhilin Wang",
      "Jiaqi Zeng",
      "Olivier Delalleau",
      "Daniel Egert",
      "Ellie Evans",
      "Hoo-Chang Shin",
      "Felipe Soares",
      "Yi Dong",
      "Oleksii Kuchaiev"
    ],
    "published": "2025-03-06T12:30:24+00:00",
    "summary": "Inference-Time Scaling has been critical to the success of recent models such as OpenAI o1 and DeepSeek R1. However, many techniques used to train models for inference-time scaling require tasks to have answers that can be verified, limiting their application to domains such as math, coding and logical reasoning. We take inspiration from how humans make first attempts, ask for detailed feedback from others and make improvements based on such feedback across a wide spectrum of open-ended endeavors. To this end, we collect data for and train dedicated Feedback and Edit Models that are capable of performing inference-time scaling for open-ended general-domain tasks. In our setup, one model generates an initial response, which are given feedback by a second model, that are then used by a third model to edit the response. We show that performance on Arena Hard, a benchmark strongly predictive of Chatbot Arena Elo can be boosted by scaling the number of initial response drafts, effective feedback and edited responses. When scaled optimally, our setup based on 70B models from the Llama 3 family can reach SoTA performance on Arena Hard at 92.7 as of 5 Mar 2025, surpassing OpenAI o1-preview-2024-09-12 with 90.4 and DeepSeek R1 with 92.3."
  },
  {
    "title": "From Waterfallish Aerospace Certification onto Agile Certifiable Iterations",
    "url": "http://arxiv.org/abs/2503.04265v1",
    "arxiv_id": "2503.04265v1",
    "authors": [
      "J. Eduardo Ferreira Ribeiro",
      "M\u00e1rio Zenha-Rela",
      "Jo\u00e3o Gabriel Silva"
    ],
    "published": "2025-03-06T09:49:57+00:00",
    "summary": "Agile software development is becoming increasingly popular in the aerospace industry because of its capability to accommodate requirement changes. However, safety-critical domains require compliance with strict regulations such as the DO-178C avionics standard, which demands thorough documentation. The main challenge of this constraint is not the content itself, but rather the comprehensive traceability from system-level requirements to all sorts of testing and verification evidence, including who did what, when, and to which artifact. Currently, this is mostly a manual activity performed at the end of the project, which blocks efforts to agilize the development of software for aerospace applications. In this paper, we present a strategy and tools that support the generation of continuous documentation complying with DO-178C requirements. By iteratively creating the DO-178C documentation associated with each software component and seamlessly merging it with the previously generated documentation, we open the way to truly continuous certifiable iterations, an evolution from the current Waterfallish industry practice. The proposed mechanisms and tools were co-designed and validated with aerospace industry professionals, thereby confirming its applicability and usefulness. The generated artifacts show that document automation is feasible in the aerospace industry, opening the way for more widespread adoption of Agile practices in this highly regulated sector."
  },
  {
    "title": "Conformal forecasting for surgical instrument trajectory",
    "url": "http://arxiv.org/abs/2503.04191v1",
    "arxiv_id": "2503.04191v1",
    "authors": [
      "Sara Sangalli",
      "Gary Sarwin",
      "Ertunc Erdil",
      "Carlo Serra",
      "Ender Konukoglu"
    ],
    "published": "2025-03-06T08:06:03+00:00",
    "summary": "Forecasting surgical instrument trajectories and predicting the next surgical action recently started to attract attention from the research community. Both these tasks are crucial for automation and assistance in endoscopy surgery. Given the safety-critical nature of these tasks, reliable uncertainty quantification is essential. Conformal prediction is a fast-growing and widely recognized framework for uncertainty estimation in machine learning and computer vision, offering distribution-free, theoretically valid prediction intervals. In this work, we explore the application of standard conformal prediction and conformalized quantile regression to estimate uncertainty in forecasting surgical instrument motion, i.e., predicting direction and magnitude of surgical instruments' future motion. We analyze and compare their coverage and interval sizes, assessing the impact of multiple hypothesis testing and correction methods. Additionally, we show how these techniques can be employed to produce useful uncertainty heatmaps. To the best of our knowledge, this is the first study applying conformal prediction to surgical guidance, marking an initial step toward constructing principled prediction intervals with formal coverage guarantees in this domain."
  },
  {
    "title": "One-Shot is Enough: Consolidating Multi-Turn Attacks into Efficient Single-Turn Prompts for LLMs",
    "url": "http://arxiv.org/abs/2503.04856v1",
    "arxiv_id": "2503.04856v1",
    "authors": [
      "Junwoo Ha",
      "Hyunjun Kim",
      "Sangyoon Yu",
      "Haon Park",
      "Ashkan Yousefpour",
      "Yuna Park",
      "Suhyun Kim"
    ],
    "published": "2025-03-06T07:34:51+00:00",
    "summary": "Despite extensive safety enhancements in large language models (LLMs), multi-turn \"jailbreak\" conversations crafted by skilled human adversaries can still breach even the most sophisticated guardrails. However, these multi-turn attacks demand considerable manual effort, limiting their scalability. In this work, we introduce a novel approach called Multi-turn-to-Single-turn (M2S) that systematically converts multi-turn jailbreak prompts into single-turn attacks. Specifically, we propose three conversion strategies - Hyphenize, Numberize, and Pythonize - each preserving sequential context yet packaging it in a single query. Our experiments on the Multi-turn Human Jailbreak (MHJ) dataset show that M2S often increases or maintains high Attack Success Rates (ASRs) compared to original multi-turn conversations. Notably, using a StrongREJECT-based evaluation of harmfulness, M2S achieves up to 95.9% ASR on Mistral-7B and outperforms original multi-turn prompts by as much as 17.5% in absolute improvement on GPT-4o. Further analysis reveals that certain adversarial tactics, when consolidated into a single prompt, exploit structural formatting cues to evade standard policy checks. These findings underscore that single-turn attacks - despite being simpler and cheaper to conduct - can be just as potent, if not more, than their multi-turn counterparts. Our findings underscore the urgent need to reevaluate and reinforce LLM safety strategies, given how adversarial queries can be compacted into a single prompt while still retaining sufficient complexity to bypass existing safety measures."
  },
  {
    "title": "Simulation-based Analysis Of Highway Trajectory Planning Using High-Order Polynomial For Highly Automated Driving Function",
    "url": "http://arxiv.org/abs/2503.04159v1",
    "arxiv_id": "2503.04159v1",
    "authors": [
      "Milin Patel",
      "Marzana Khatun",
      "Rolf Jung",
      "Michael Gla\u00df"
    ],
    "published": "2025-03-06T07:23:17+00:00",
    "summary": "One of the fundamental tasks of autonomous driving is safe trajectory planning, the task of deciding where the vehicle needs to drive, while avoiding obstacles, obeying safety rules, and respecting the fundamental limits of road. Real-world application of such a method involves consideration of surrounding environment conditions and movements such as Lane Change, collision avoidance, and lane merge. The focus of the paper is to develop and implement safe collision free highway Lane Change trajectory using high order polynomial for Highly Automated Driving Function (HADF). Planning is often considered as a higher-level process than control. Behavior Planning Module (BPM) is designed that plans the high-level driving actions like Lane Change maneuver to safely achieve the functionality of transverse guidance ensuring safety of the vehicle using motion planning in a scenario including environmental situation. Based on the recommendation received from the (BPM), the function will generate a desire corresponding trajectory. The proposed planning system is situation specific with polynomial based algorithm for same direction two lane highway scenario. To support the trajectory system polynomial curve can be used to reduces overall complexity and thereby allows rapid computation. The proposed Lane Change scenario is modeled, and results has been analyzed (verified and validate) through the MATLAB simulation environment. The method proposed in this paper has achieved a significant improvement in safety and stability of Lane Changing maneuver."
  },
  {
    "title": "KidneyTalk-open: No-code Deployment of a Private Large Language Model with Medical Documentation-Enhanced Knowledge Database for Kidney Disease",
    "url": "http://arxiv.org/abs/2503.04153v1",
    "arxiv_id": "2503.04153v1",
    "authors": [
      "Yongchao Long",
      "Chao Yang",
      "Gongzheng Tang",
      "Jinwei Wang",
      "Zhun Sui",
      "Yuxi Zhou",
      "Shenda Hong",
      "Luxia Zhang"
    ],
    "published": "2025-03-06T07:01:36+00:00",
    "summary": "Privacy-preserving medical decision support for kidney disease requires localized deployment of large language models (LLMs) while maintaining clinical reasoning capabilities. Current solutions face three challenges: 1) Cloud-based LLMs pose data security risks; 2) Local model deployment demands technical expertise; 3) General LLMs lack mechanisms to integrate medical knowledge. Retrieval-augmented systems also struggle with medical document processing and clinical usability. We developed KidneyTalk-open, a desktop system integrating three technical components: 1) No-code deployment of state-of-the-art (SOTA) open-source LLMs (such as DeepSeek-r1, Qwen2.5) via local inference engine; 2) Medical document processing pipeline combining context-aware chunking and intelligent filtering; 3) Adaptive Retrieval and Augmentation Pipeline (AddRep) employing agents collaboration for improving the recall rate of medical documents. A graphical interface was designed to enable clinicians to manage medical documents and conduct AI-powered consultations without technical expertise. Experimental validation on 1,455 challenging nephrology exam questions demonstrates AddRep's effectiveness: achieving 29.1% accuracy (+8.1% over baseline) with intelligent knowledge integration, while maintaining robustness through 4.9% rejection rate to suppress hallucinations. Comparative case studies with the mainstream products (AnythingLLM, Chatbox, GPT4ALL) demonstrate KidneyTalk-open's superior performance in real clinical query. KidneyTalk-open represents the first no-code medical LLM system enabling secure documentation-enhanced medical Q&A on desktop. Its designs establishes a new framework for privacy-sensitive clinical AI applications. The system significantly lowers technical barriers while improving evidence traceability, enabling more medical staff or patients to use SOTA open-source LLMs conveniently."
  },
  {
    "title": "Analyzing the Impact of Augmented Reality Head-Mounted Displays on Workers' Safety and Situational Awareness in Hazardous Industrial Settings",
    "url": "http://arxiv.org/abs/2503.04075v1",
    "arxiv_id": "2503.04075v1",
    "authors": [
      "Graciela Camacho-Fidalgo",
      "Blain Judkins",
      "Kylee Friederichs",
      "Lara Soberanis",
      "Vicente Hernandez",
      "Kevin McSweeney",
      "Freddie Witherden",
      "Edgar Rojas-Mu\u00f1oz"
    ],
    "published": "2025-03-06T04:13:14+00:00",
    "summary": "Augmented Reality Head-Mounted Displays (AR-HMDs) have proven effective to assist workers. However, they may degrade their Safety and Situational Awareness (SSA), particularly in complex and hazardous industrial settings. This paper analyzes, objectively and subjectively, the effects of AR-HMDs' on workers' SSA in a simulated hazardous industrial environment. Our evaluation was comprised of sixty participants performing various tasks in a simulated cargo ship room while receiving remote guidance through one of three devices: two off-the-shelf AR-HMDs (Trimble XR10 with HoloLens 2, RealWear Navigator 520), and a smartphone (Google Pixel 6). Several sensors were installed throughout the room to obtain quantitative measures of the participants' safe execution of the tasks, such as the frequency in which they hit the objects in the room or stepped over simulated holes or oil spills. The results reported that the Trimble XR10 led to statistically highest head-knockers and knee-knocker incidents compared to the Navigator 520 and the Pixel 6. Furthermore, the Trimble XR10 also led to significantly higher difficulties to cross hatch doors, lower perceived safety, comfort, perceived performance, and usability. Overall, participants wearing AR-HMDs failed to perceive more hazards, meaning that safety-preserving capabilities must be developed for AR-HMDs before introducing them into industrial hazardous settings confidently."
  },
  {
    "title": "Insights from Rights and Wrongs: A Large Language Model for Solving Assertion Failures in RTL Design",
    "url": "http://arxiv.org/abs/2503.04057v1",
    "arxiv_id": "2503.04057v1",
    "authors": [
      "Jie Zhou",
      "Youshu Ji",
      "Ning Wang",
      "Yuchen Hu",
      "Xinyao Jiao",
      "Bingkun Yao",
      "Xinwei Fang",
      "Shuai Zhao",
      "Nan Guan",
      "Zhe Jiang"
    ],
    "published": "2025-03-06T03:17:48+00:00",
    "summary": "SystemVerilog Assertions (SVAs) are essential for verifying Register Transfer Level (RTL) designs, as they can be embedded into key functional paths to detect unintended behaviours. During simulation, assertion failures occur when the design's behaviour deviates from expectations. Solving these failures, i.e., identifying and fixing the issues causing the deviation, requires analysing complex logical and timing relationships between multiple signals. This process heavily relies on human expertise, and there is currently no automatic tool available to assist with it. Here, we present AssertSolver, an open-source Large Language Model (LLM) specifically designed for solving assertion failures. By leveraging synthetic training data and learning from error responses to challenging cases, AssertSolver achieves a bug-fixing pass@1 metric of 88.54% on our testbench, significantly outperforming OpenAI's o1-preview by up to 11.97%. We release our model and testbench for public access to encourage further research: https://github.com/SEU-ACAL/reproduce-AssertSolver-DAC-25."
  },
  {
    "title": "NsBM-GAT: A Non-stationary Block Maximum and Graph Attention Framework for General Traffic Crash Risk Prediction",
    "url": "http://arxiv.org/abs/2503.04018v1",
    "arxiv_id": "2503.04018v1",
    "authors": [
      "Kequan Chen",
      "Pan Liu",
      "Yuxuan Wang",
      "David Z. W. Wang",
      "Yifan Dai",
      "Zhibin Li"
    ],
    "published": "2025-03-06T02:12:40+00:00",
    "summary": "Accurate prediction of traffic crash risks for individual vehicles is essential for enhancing vehicle safety. While significant attention has been given to traffic crash risk prediction, existing studies face two main challenges: First, due to the scarcity of individual vehicle data before crashes, most models rely on hypothetical scenarios deemed dangerous by researchers. This raises doubts about their applicability to actual pre-crash conditions. Second, some crash risk prediction frameworks were learned from dashcam videos. Although such videos capture the pre-crash behavior of individual vehicles, they often lack critical information about the movements of surrounding vehicles. However, the interaction between a vehicle and its surrounding vehicles is highly influential in crash occurrences. To overcome these challenges, we propose a novel non-stationary extreme value theory (EVT), where the covariate function is optimized in a nonlinear fashion using a graph attention network. The EVT component incorporates the stochastic nature of crashes through probability distribution, which enhances model interpretability. Notably, the nonlinear covariate function enables the model to capture the interactive behavior between the target vehicle and its multiple surrounding vehicles, facilitating crash risk prediction across different driving tasks. We train and test our model using 100 sets of vehicle trajectory data before real crashes, collected via drones over three years from merging and weaving segments. We demonstrate that our model successfully learns micro-level precursors of crashes and fits a more accurate distribution with the aid of the nonlinear covariate function. Our experiments on the testing dataset show that the proposed model outperforms existing models by providing more accurate predictions for both rear-end and sideswipe crashes simultaneously."
  },
  {
    "title": "Planning and Control for Deformable Linear Object Manipulation",
    "url": "http://arxiv.org/abs/2503.04007v1",
    "arxiv_id": "2503.04007v1",
    "authors": [
      "Burak Aksoy",
      "John Wen"
    ],
    "published": "2025-03-06T01:44:36+00:00",
    "summary": "Manipulating a deformable linear object (DLO) such as wire, cable, and rope is a common yet challenging task due to their high degrees of freedom and complex deformation behaviors, especially in an environment with obstacles. Existing local control methods are efficient but prone to failure in complex scenarios, while precise global planners are computationally intensive and difficult to deploy. This paper presents an efficient, easy-to-deploy framework for collision-free DLO manipulation using mobile manipulators. We demonstrate the effectiveness of leveraging standard planning tools for high-dimensional DLO manipulation without requiring custom planners or extensive data-driven models. Our approach combines an off-the-shelf global planner with a real-time local controller. The global planner approximates the DLO as a series of rigid links connected by spherical joints, enabling rapid path planning without the need for problem-specific planners or large datasets. The local controller employs control barrier functions (CBFs) to enforce safety constraints, maintain the DLO integrity, prevent overstress, and handle obstacle avoidance. It compensates for modeling inaccuracies by using a state-of-the-art position-based dynamics technique that approximates physical properties like Young's and shear moduli. We validate our framework through extensive simulations and real-world demonstrations. In complex obstacle scenarios-including tent pole transport, corridor navigation, and tasks requiring varied stiffness-our method achieves a 100% success rate over thousands of trials, with significantly reduced planning times compared to state-of-the-art techniques. Real-world experiments include transportation of a tent pole and a rope using mobile manipulators. We share our ROS-based implementation to facilitate adoption in various applications."
  },
  {
    "title": "Enhancing Autonomous Driving Safety with Collision Scenario Integration",
    "url": "http://arxiv.org/abs/2503.03957v1",
    "arxiv_id": "2503.03957v1",
    "authors": [
      "Zi Wang",
      "Shiyi Lan",
      "Xinglong Sun",
      "Nadine Chang",
      "Zhenxin Li",
      "Zhiding Yu",
      "Jose M. Alvarez"
    ],
    "published": "2025-03-05T23:08:43+00:00",
    "summary": "Autonomous vehicle safety is crucial for the successful deployment of self-driving cars. However, most existing planning methods rely heavily on imitation learning, which limits their ability to leverage collision data effectively. Moreover, collecting collision or near-collision data is inherently challenging, as it involves risks and raises ethical and practical concerns. In this paper, we propose SafeFusion, a training framework to learn from collision data. Instead of over-relying on imitation learning, SafeFusion integrates safety-oriented metrics during training to enable collision avoidance learning. In addition, to address the scarcity of collision data, we propose CollisionGen, a scalable data generation pipeline to generate diverse, high-quality scenarios using natural language prompts, generative models, and rule-based filtering. Experimental results show that our approach improves planning performance in collision-prone scenarios by 56\\% over previous state-of-the-art planners while maintaining effectiveness in regular driving situations. Our work provides a scalable and effective solution for advancing the safety of autonomous driving systems."
  },
  {
    "title": "Safe LLM-Controlled Robots with Formal Guarantees via Reachability Analysis",
    "url": "http://arxiv.org/abs/2503.03911v1",
    "arxiv_id": "2503.03911v1",
    "authors": [
      "Ahmad Hafez",
      "Alireza Naderi Akhormeh",
      "Amr Hegazy",
      "Amr Alanwar"
    ],
    "published": "2025-03-05T21:23:15+00:00",
    "summary": "The deployment of Large Language Models (LLMs) in robotic systems presents unique safety challenges, particularly in unpredictable environments. Although LLMs, leveraging zero-shot learning, enhance human-robot interaction and decision-making capabilities, their inherent probabilistic nature and lack of formal guarantees raise significant concerns for safety-critical applications. Traditional model-based verification approaches often rely on precise system models, which are difficult to obtain for real-world robotic systems and may not be fully trusted due to modeling inaccuracies, unmodeled dynamics, or environmental uncertainties. To address these challenges, this paper introduces a safety assurance framework for LLM-controlled robots based on data-driven reachability analysis, a formal verification technique that ensures all possible system trajectories remain within safe operational limits. Our framework specifically investigates the problem of instructing an LLM to navigate the robot to a specified goal and assesses its ability to generate low-level control actions that successfully guide the robot safely toward that goal. By leveraging historical data to construct reachable sets of states for the robot-LLM system, our approach provides rigorous safety guarantees against unsafe behaviors without relying on explicit analytical models. We validate the framework through experimental case studies in autonomous navigation and task planning, demonstrating its effectiveness in mitigating risks associated with LLM-generated commands. This work advances the integration of formal methods into LLM-based robotics, offering a principled and practical approach to ensuring safety in next-generation autonomous systems."
  },
  {
    "title": "Parser Knows Best: Testing DBMS with Coverage-Guided Grammar-Rule Traversal",
    "url": "http://arxiv.org/abs/2503.03893v1",
    "arxiv_id": "2503.03893v1",
    "authors": [
      "Yu Liang",
      "Hong Hu"
    ],
    "published": "2025-03-05T20:50:41+00:00",
    "summary": "Database Management System (DBMS) is the key component for data-intensive applications. Recently, researchers propose many tools to comprehensively test DBMS systems for finding various bugs. However, these tools only cover a small subset of diverse syntax elements defined in DBMS-specific SQL dialects, leaving a large number of features unexplored. In this paper, we propose ParserFuzz, a novel fuzzing framework that automatically extracts grammar rules from DBMSs' built-in syntax definition files for SQL query generation. Without any input corpus, ParserFuzz can generate diverse query statements to saturate the grammar features of the tested DBMSs, which grammar features could be missed by previous tools. Additionally, ParserFuzz utilizes code coverage as feedback to guide the query mutation, which combines different DBMS features extracted from the syntax rules to find more function and safety bugs. In our evaluation, ParserFuzz outperforms all state-of-the-art existing DBMS testing tools in terms of bug finding, grammar rule coverage and code coverage. ParserFuzz detects 81 previously unknown bugs in total across 5 popular DBMSs, where all bugs are confirmed and 34 have been fixed."
  },
  {
    "title": "Seldonian Reinforcement Learning for Ad Hoc Teamwork",
    "url": "http://arxiv.org/abs/2503.03885v1",
    "arxiv_id": "2503.03885v1",
    "authors": [
      "Edoardo Zorzi",
      "Alberto Castellini",
      "Leonidas Bakopoulos",
      "Georgios Chalkiadakis",
      "Alessandro Farinelli"
    ],
    "published": "2025-03-05T20:37:02+00:00",
    "summary": "Most offline RL algorithms return optimal policies but do not provide statistical guarantees on undesirable behaviors. This could generate reliability issues in safety-critical applications, such as in some multiagent domains where agents, and possibly humans, need to interact to reach their goals without harming each other. In this work, we propose a novel offline RL approach, inspired by Seldonian optimization, which returns policies with good performance and statistically guaranteed properties with respect to predefined undesirable behaviors. In particular, our focus is on Ad Hoc Teamwork settings, where agents must collaborate with new teammates without prior coordination. Our method requires only a pre-collected dataset, a set of candidate policies for our agent, and a specification about the possible policies followed by the other players -- it does not require further interactions, training, or assumptions on the type and architecture of the policies. We test our algorithm in Ad Hoc Teamwork problems and show that it consistently finds reliable policies while improving sample efficiency with respect to standard ML baselines."
  },
  {
    "title": "Nexar Dashcam Collision Prediction Dataset and Challenge",
    "url": "http://arxiv.org/abs/2503.03848v1",
    "arxiv_id": "2503.03848v1",
    "authors": [
      "Daniel C. Moura",
      "Shizhan Zhu",
      "Orly Zvitia"
    ],
    "published": "2025-03-05T19:20:28+00:00",
    "summary": "This paper presents the Nexar Dashcam Collision Prediction Dataset and Challenge, designed to support research in traffic event analysis, collision prediction, and autonomous vehicle safety. The dataset consists of 1,500 annotated video clips, each approximately 40 seconds long, capturing a diverse range of real-world traffic scenarios. Videos are labeled with event type (collision/near-collision vs. normal driving), environmental conditions (lighting conditions and weather), and scene type (urban, rural, highway, etc.). For collision and near-collision cases, additional temporal labels are provided, including the precise moment of the event and the alert time, marking when the collision first becomes predictable.   To advance research on accident prediction, we introduce the Nexar Dashcam Collision Prediction Challenge, a public competition on top of this dataset. Participants are tasked with developing machine learning models that predict the likelihood of an imminent collision, given an input video. Model performance is evaluated using the average precision (AP) computed across multiple intervals before the accident (i.e. 500 ms, 1000 ms, and 1500 ms prior to the event), emphasizing the importance of early and reliable predictions.   The dataset is released under an open license with restrictions on unethical use, ensuring responsible research and innovation."
  },
  {
    "title": "RiskAgent: Autonomous Medical AI Copilot for Generalist Risk Prediction",
    "url": "http://arxiv.org/abs/2503.03802v1",
    "arxiv_id": "2503.03802v1",
    "authors": [
      "Fenglin Liu",
      "Jinge Wu",
      "Hongjian Zhou",
      "Xiao Gu",
      "Soheila Molaei",
      "Anshul Thakur",
      "Lei Clifton",
      "Honghan Wu",
      "David A. Clifton"
    ],
    "published": "2025-03-05T18:46:51+00:00",
    "summary": "The application of Large Language Models (LLMs) to various clinical applications has attracted growing research attention. However, real-world clinical decision-making differs significantly from the standardized, exam-style scenarios commonly used in current efforts. In this paper, we present the RiskAgent system to perform a broad range of medical risk predictions, covering over 387 risk scenarios across diverse complex diseases, e.g., cardiovascular disease and cancer. RiskAgent is designed to collaborate with hundreds of clinical decision tools, i.e., risk calculators and scoring systems that are supported by evidence-based medicine. To evaluate our method, we have built the first benchmark MedRisk specialized for risk prediction, including 12,352 questions spanning 154 diseases, 86 symptoms, 50 specialties, and 24 organ systems. The results show that our RiskAgent, with 8 billion model parameters, achieves 76.33% accuracy, outperforming the most recent commercial LLMs, o1, o3-mini, and GPT-4.5, and doubling the 38.39% accuracy of GPT-4o. On rare diseases, e.g., Idiopathic Pulmonary Fibrosis (IPF), RiskAgent outperforms o1 and GPT-4.5 by 27.27% and 45.46% accuracy, respectively. Finally, we further conduct a generalization evaluation on an external evidence-based diagnosis benchmark and show that our RiskAgent achieves the best results. These encouraging results demonstrate the great potential of our solution for diverse diagnosis domains. To improve the adaptability of our model in different scenarios, we have built and open-sourced a family of models ranging from 1 billion to 70 billion parameters. Our code, data, and models are all available at https://github.com/AI-in-Health/RiskAgent."
  },
  {
    "title": "Improving LLM Safety Alignment with Dual-Objective Optimization",
    "url": "http://arxiv.org/abs/2503.03710v1",
    "arxiv_id": "2503.03710v1",
    "authors": [
      "Xuandong Zhao",
      "Will Cai",
      "Tianneng Shi",
      "David Huang",
      "Licong Lin",
      "Song Mei",
      "Dawn Song"
    ],
    "published": "2025-03-05T18:01:05+00:00",
    "summary": "Existing training-time safety alignment techniques for large language models (LLMs) remain vulnerable to jailbreak attacks. Direct preference optimization (DPO), a widely deployed alignment method, exhibits limitations in both experimental and theoretical contexts as its loss function proves suboptimal for refusal learning. Through gradient-based analysis, we identify these shortcomings and propose an improved safety alignment that disentangles DPO objectives into two components: (1) robust refusal training, which encourages refusal even when partial unsafe generations are produced, and (2) targeted unlearning of harmful knowledge. This approach significantly increases LLM robustness against a wide range of jailbreak attacks, including prefilling, suffix, and multi-turn attacks across both in-distribution and out-of-distribution scenarios. Furthermore, we introduce a method to emphasize critical refusal tokens by incorporating a reward-based token-level weighting mechanism for refusal learning, which further improves the robustness against adversarial exploits. Our research also suggests that robustness to jailbreak attacks is correlated with token distribution shifts in the training process and internal representations of refusal and harmful tokens, offering valuable directions for future research in LLM safety alignment. The code is available at https://github.com/wicai24/DOOR-Alignment"
  },
  {
    "title": "AEGIS: Towards Formalized and Practical Memory-Safe Execution of C programs via MSWASM",
    "url": "http://arxiv.org/abs/2503.03698v1",
    "arxiv_id": "2503.03698v1",
    "authors": [
      "Shahram Esmaeilsabzali",
      "Arayi Khalatyan",
      "Zhijun Mo",
      "Sruthi Venkatanarayanan",
      "Shengjie Xu"
    ],
    "published": "2025-03-05T17:50:43+00:00",
    "summary": "Programs written in unsafe languages such as C are prone to memory safety errors, which can lead to program compromises and serious real-world security consequences. Recently, Memory-Safe WebAssembly (MSWASM) is introduced as a general-purpose intermediate bytecode with built-in memory safety semantics. Programs written in C can be compiled into MSWASM to get complete memory safety protection. In this paper, we present our extensions on MSWASM, which improve its semantics and practicality. First, we formalize MSWASM semantics in Coq/Iris, extending it with inter-module interaction, showing that MSWASM provides fine-grained isolation guarantees analogous to WASM's coarse-grained isolation via linear memory. Second, we present Aegis, a system to adopt the memory safety of MSWASM for C programs in an interoperable way. Aegis pipeline generates Checked C source code from MSWASM modules to enforce spatial memory safety. Checked C is a recent binary-compatible extension of C which can provide guaranteed spatial safety. Our design allows Aegis to protect C programs that depend on legacy C libraries with no extra dependency and with low overhead. Aegis pipeline incurs 67% runtime overhead and near-zero memory overhead on PolyBenchC programs compared to native."
  },
  {
    "title": "TeraSim: Uncovering Unknown Unsafe Events for Autonomous Vehicles through Generative Simulation",
    "url": "http://arxiv.org/abs/2503.03629v1",
    "arxiv_id": "2503.03629v1",
    "authors": [
      "Haowei Sun",
      "Xintao Yan",
      "Zhijie Qiao",
      "Haojie Zhu",
      "Yihao Sun",
      "Jiawei Wang",
      "Shengyin Shen",
      "Darian Hogue",
      "Rajanikant Ananta",
      "Derek Johnson",
      "Greg Stevens",
      "Greg McGuire",
      "Yifan Wei",
      "Wei Zheng",
      "Yong Sun",
      "Yasuo Fukai",
      "Henry X. Liu"
    ],
    "published": "2025-03-05T16:09:30+00:00",
    "summary": "Traffic simulation is essential for autonomous vehicle (AV) development, enabling comprehensive safety evaluation across diverse driving conditions. However, traditional rule-based simulators struggle to capture complex human interactions, while data-driven approaches often fail to maintain long-term behavioral realism or generate diverse safety-critical events. To address these challenges, we propose TeraSim, an open-source, high-fidelity traffic simulation platform designed to uncover unknown unsafe events and efficiently estimate AV statistical performance metrics, such as crash rates. TeraSim is designed for seamless integration with third-party physics simulators and standalone AV stacks, to construct a complete AV simulation system. Experimental results demonstrate its effectiveness in generating diverse safety-critical events involving both static and dynamic agents, identifying hidden deficiencies in AV systems, and enabling statistical performance evaluation. These findings highlight TeraSim's potential as a practical tool for AV safety assessment, benefiting researchers, developers, and policymakers. The code is available at https://github.com/mcity/TeraSim."
  },
  {
    "title": "TeraSim: Uncovering Unknown Unsafe Events for Autonomous Vehicles through Generative Simulation",
    "url": "http://arxiv.org/abs/2503.03629v2",
    "arxiv_id": "2503.03629v2",
    "authors": [
      "Haowei Sun",
      "Xintao Yan",
      "Zhijie Qiao",
      "Haojie Zhu",
      "Yihao Sun",
      "Jiawei Wang",
      "Shengyin Shen",
      "Darian Hogue",
      "Rajanikant Ananta",
      "Derek Johnson",
      "Greg Stevens",
      "Greg McGuire",
      "Yifan Wei",
      "Wei Zheng",
      "Yong Sun",
      "Yasuo Fukai",
      "Henry X. Liu"
    ],
    "published": "2025-03-05T16:09:30+00:00",
    "summary": "Traffic simulation is essential for autonomous vehicle (AV) development, enabling comprehensive safety evaluation across diverse driving conditions. However, traditional rule-based simulators struggle to capture complex human interactions, while data-driven approaches often fail to maintain long-term behavioral realism or generate diverse safety-critical events. To address these challenges, we propose TeraSim, an open-source, high-fidelity traffic simulation platform designed to uncover unknown unsafe events and efficiently estimate AV statistical performance metrics, such as crash rates. TeraSim is designed for seamless integration with third-party physics simulators and standalone AV stacks, to construct a complete AV simulation system. Experimental results demonstrate its effectiveness in generating diverse safety-critical events involving both static and dynamic agents, identifying hidden deficiencies in AV systems, and enabling statistical performance evaluation. These findings highlight TeraSim's potential as a practical tool for AV safety assessment, benefiting researchers, developers, and policymakers. The code is available at https://github.com/mcity/TeraSim."
  },
  {
    "title": "Simulation-Based Performance Evaluation of 3D Object Detection Methods with Deep Learning for a LiDAR Point Cloud Dataset in a SOTIF-related Use Case",
    "url": "http://arxiv.org/abs/2503.03548v1",
    "arxiv_id": "2503.03548v1",
    "authors": [
      "Milin Patel",
      "Rolf Jung"
    ],
    "published": "2025-03-05T14:32:32+00:00",
    "summary": "Safety of the Intended Functionality (SOTIF) addresses sensor performance limitations and deep learning-based object detection insufficiencies to ensure the intended functionality of Automated Driving Systems (ADS). This paper presents a methodology examining the adaptability and performance evaluation of the 3D object detection methods on a LiDAR point cloud dataset generated by simulating a SOTIF-related Use Case. The major contributions of this paper include defining and modelling a SOTIF-related Use Case with 21 diverse weather conditions and generating a LiDAR point cloud dataset suitable for application of 3D object detection methods. The dataset consists of 547 frames, encompassing clear, cloudy, rainy weather conditions, corresponding to different times of the day, including noon, sunset, and night. Employing MMDetection3D and OpenPCDET toolkits, the performance of State-of-the-Art (SOTA) 3D object detection methods is evaluated and compared by testing the pre-trained Deep Learning (DL) models on the generated dataset using Average Precision (AP) and Recall metrics."
  },
  {
    "title": "Simulation-Based Application of Safety of The Intended Functionality to Mitigate Foreseeable Misuse in Automated Driving Systems",
    "url": "http://arxiv.org/abs/2503.03534v1",
    "arxiv_id": "2503.03534v1",
    "authors": [
      "Milin Patel",
      "Rolf Jung"
    ],
    "published": "2025-03-05T14:16:49+00:00",
    "summary": "The development of Automated Driving Systems (ADS) has the potential to revolutionise the transportation industry, but it also presents significant safety challenges. One of the key challenges is ensuring that the ADS is safe in the event of Foreseeable Misuse (FM) by the human driver. To address this challenge, a case study on simulation-based testing to mitigate FM by the driver using the driving simulator is presented. FM by the human driver refers to potential driving scenarios where the driver misinterprets the intended functionality of ADS, leading to hazardous behaviour. Safety of the Intended Functionality (SOTIF) focuses on ensuring the absence of unreasonable risk resulting from hazardous behaviours related to functional insufficiencies caused by FM and performance limitations of sensors and machine learning-based algorithms for ADS. The simulation-based application of SOTIF to mitigate FM in ADS entails determining potential misuse scenarios, conducting simulation-based testing, and evaluating the effectiveness of measures dedicated to preventing or mitigating FM. The major contribution includes defining (i) test requirements for performing simulation-based testing of a potential misuse scenario, (ii) evaluation criteria in accordance with SOTIF requirements for implementing measures dedicated to preventing or mitigating FM, and (iii) approach to evaluate the effectiveness of the measures dedicated to preventing or mitigating FM. In conclusion, an exemplary case study incorporating driver-vehicle interface and driver interactions with ADS forming the basis for understanding the factors and causes contributing to FM is investigated. Furthermore, the test procedure for evaluating the effectiveness of the measures dedicated to preventing or mitigating FM by the driver is developed in this work."
  },
  {
    "title": "DO-IQS: Dynamics-Aware Offline Inverse Q-Learning for Optimal Stopping with Unknown Gain Functions",
    "url": "http://arxiv.org/abs/2503.03515v1",
    "arxiv_id": "2503.03515v1",
    "authors": [
      "Anna Kuchko"
    ],
    "published": "2025-03-05T14:01:17+00:00",
    "summary": "We consider Inverse Optimal Stopping (IOS) problem where, based on stopped expert trajectories, one aims to recover the optimal stopping region through continuation and stopping gain functions approximation. The uniqueness of the stopping region allows the use of IOS in real-world applications with safety concerns. While current state-of-the-art inverse reinforcement learning methods recover both a Q-function and the corresponding optimal policy, they fail to account for specific challenges posed by optimal stopping problems. These include data sparsity near the stopping region, non-Markovian nature of the continuation gain, a proper treatment of boundary conditions, the need for a stable offline approach for risk-sensitive applications, and a lack of a quality evaluation metric. These challenges are addressed with the proposed Dynamics-Aware Offline Inverse Q-Learning for Optimal Stopping (DO-IQS), which incorporates temporal information by approximating the cumulative continuation gain together with the world dynamics and the Q-function without querying to the environment. Moreover, a confidence-based oversampling approach is proposed to treat the data sparsity problem. We demonstrate the performance of our models on real and artificial data including an optimal intervention for critical events problem."
  },
  {
    "title": "CURVALID: Geometrically-guided Adversarial Prompt Detection",
    "url": "http://arxiv.org/abs/2503.03502v1",
    "arxiv_id": "2503.03502v1",
    "authors": [
      "Canaan Yung",
      "Hanxun Huang",
      "Sarah Monazam Erfani",
      "Christopher Leckie"
    ],
    "published": "2025-03-05T13:47:53+00:00",
    "summary": "Adversarial prompts capable of jailbreaking large language models (LLMs) and inducing undesirable behaviours pose a significant obstacle to their safe deployment. Current mitigation strategies rely on activating built-in defence mechanisms or fine-tuning the LLMs, but the fundamental distinctions between adversarial and benign prompts are yet to be understood. In this work, we introduce CurvaLID, a novel defense framework that efficiently detects adversarial prompts by leveraging their geometric properties. It is agnostic to the type of LLM, offering a unified detection framework across diverse adversarial prompts and LLM architectures. CurvaLID builds on the geometric analysis of text prompts to uncover their underlying differences. We theoretically extend the concept of curvature via the Whewell equation into an $n$-dimensional word embedding space, enabling us to quantify local geometric properties, including semantic shifts and curvature in the underlying manifolds. Additionally, we employ Local Intrinsic Dimensionality (LID) to capture geometric features of text prompts within adversarial subspaces. Our findings reveal that adversarial prompts differ fundamentally from benign prompts in terms of their geometric characteristics. Our results demonstrate that CurvaLID delivers superior detection and rejection of adversarial queries, paving the way for safer LLM deployment. The source code can be found at https://github.com/Cancanxxx/CurvaLID"
  },
  {
    "title": "Differentially Private Learners for Heterogeneous Treatment Effects",
    "url": "http://arxiv.org/abs/2503.03486v1",
    "arxiv_id": "2503.03486v1",
    "authors": [
      "Maresa Schr\u00f6der",
      "Valentyn Melnychuk",
      "Stefan Feuerriegel"
    ],
    "published": "2025-03-05T13:24:58+00:00",
    "summary": "Patient data is widely used to estimate heterogeneous treatment effects and thus understand the effectiveness and safety of drugs. Yet, patient data includes highly sensitive information that must be kept private. In this work, we aim to estimate the conditional average treatment effect (CATE) from observational data under differential privacy. Specifically, we present DP-CATE, a novel framework for CATE estimation that is Neyman-orthogonal and further ensures differential privacy of the estimates. Our framework is highly general: it applies to any two-stage CATE meta-learner with a Neyman-orthogonal loss function, and any machine learning model can be used for nuisance estimation. We further provide an extension of our DP-CATE, where we employ RKHS regression to release the complete CATE function while ensuring differential privacy. We demonstrate our DP-CATE across various experiments using synthetic and real-world datasets. To the best of our knowledge, we are the first to provide a framework for CATE estimation that is Neyman-orthogonal and differentially private."
  },
  {
    "title": "SafeVLA: Towards Safety Alignment of Vision-Language-Action Model via Safe Reinforcement Learning",
    "url": "http://arxiv.org/abs/2503.03480v1",
    "arxiv_id": "2503.03480v1",
    "authors": [
      "Borong Zhang",
      "Yuhao Zhang",
      "Jiaming Ji",
      "Yingshan Lei",
      "Josef Dai",
      "Yuanpei Chen",
      "Yaodong Yang"
    ],
    "published": "2025-03-05T13:16:55+00:00",
    "summary": "Vision-language-action models (VLAs) have shown great potential as generalist robot policies. However, these models pose urgent safety challenges during deployment, including the risk of physical harm to the environment, the robot itself, and humans. How can safety be explicitly incorporated into VLAs? In this work, we propose SafeVLA, a novel algorithm designed to integrate safety into VLAs, ensuring the protection of the environment, robot hardware and humans in real-world settings. SafeVLA effectively balances safety and task performance by employing large-scale constrained learning within simulated environments. We demonstrate that SafeVLA outperforms the current state-of-the-art method in both safety and task performance, achieving average improvements of 83.58% and 3.85%, respectively, in simulation. By prioritizing safety, our approach eliminates high-risk behaviors and reduces the upper bound of unsafe behaviors to 1/35 of that in the current state-of-the-art, thereby significantly mitigating long-tail risks. Furthermore, the learned safety constraints generalize to diverse, unseen scenarios, including multiple out-of-distribution perturbations and tasks. Our data, models and newly proposed benchmark environment are available at https://sites.google.com/view/pku-safevla."
  },
  {
    "title": "SEAL: Safety Enhanced Trajectory Planning and Control Framework for Quadrotor Flight in Complex Environments",
    "url": "http://arxiv.org/abs/2503.03346v1",
    "arxiv_id": "2503.03346v1",
    "authors": [
      "Yiming Wang",
      "Jianbin Ma",
      "Junda Wu",
      "Huizhe Li",
      "Zhexuan Zhou",
      "Youmin Gong",
      "Jie Mei",
      "Guangfu Ma"
    ],
    "published": "2025-03-05T10:15:56+00:00",
    "summary": "For quadrotors, achieving safe and autonomous flight in complex environments with wind disturbances and dynamic obstacles still faces significant challenges. Most existing methods address wind disturbances in either trajectory planning or control, which may lead to hazardous situations during flight. The emergence of dynamic obstacles would further worsen the situation. Therefore, we propose an efficient and reliable framework for quadrotors that incorporates wind disturbance estimations during both the planning and control phases via a generalized proportional integral observer. First, we develop a real-time adaptive spatial-temporal trajectory planner that utilizes Hamilton-Jacobi (HJ) reachability analysis for error dynamics resulting from wind disturbances. By considering the forward reachability sets propagation on an Euclidean Signed Distance Field (ESDF) map, safety is guaranteed. Additionally, a Nonlinear Model Predictive Control (NMPC) controller considering wind disturbance compensation is implemented for robust trajectory tracking. Simulation and real-world experiments verify the effectiveness of our framework. The video and supplementary material will be available at https://github.com/Ma29-HIT/SEAL/."
  },
  {
    "title": "Quantitative Magnetohydrodynamic Modelling of Flux Pumping in ASDEX Upgrade",
    "url": "http://arxiv.org/abs/2503.03344v1",
    "arxiv_id": "2503.03344v1",
    "authors": [
      "Haowei Zhang",
      "Matthias H\u00f6lzl",
      "Isabel Krebs",
      "Andreas Burckhart",
      "Alexander Bock",
      "Sibylle G\u00fcnter",
      "Valentin Igochine",
      "Karl Lackner",
      "Rohan Ramasamy",
      "Hartmut Zohm",
      "the JOREK team",
      "the ASDEX Upgrade team"
    ],
    "published": "2025-03-05T10:14:54+00:00",
    "summary": "The sawtooth-free hybrid scenario has been achieved recently in ASDEX Upgrade (AUG) with applied non-inductive current sources and auxiliary heating [A. Burckhart et al 2023 Nucl. Fusion 63 126056]. Control experiments in AUG suggest that the self-regulating magnetic flux pumping mechanism, characterized by anomalous current redistribution, is responsible for clamping the central safety factor (q_0) close to unity, thereby preventing the sawtooth onset. This work presents a numerical and theoretical investigation of flux pumping in the AUG hybrid scenario based on the two-temperature, visco-resistive, full magnetohydrodynamic (MHD) model with the JOREK code. To quantitatively model the flux pumping, we choose realistic parameters, plasma configurations, and source terms based on AUG experiments. During the initial saturation stage of the unstable 1/1 quasi-interchange mode (on millisecond timescales), q_0 exhibits fast under-damped oscillation and reaches a value closer to unity, which is attributed to the self-regulation of core plasma and the fast dynamo effect on the order of V/m. On the longer resistive diffusion timescale of seconds, the slow negative dynamo effect on the order of mV/m induced by the 1/1 MHD instability plays an effective role in flux pumping, which provides quantitative agreement with experimental observations for the first time. The final saturated 1/1 MHD instability exhibits features of the quasi-interchange mode and tearing mode, and the associated convective plasma flow velocity is a few m/s. The toroidal negative electric field from the slow dynamo dominantly offsets the positive current drive and continuously redistributes the current density and pressure. As a result, q_0 is maintained close to unity due to the low-shear profiles of current density and pressure in the plasma core, and the system enters a sawtooth-free and quasi-stationary helical state."
  },
  {
    "title": "Safety Verification of Nonlinear Stochastic Systems via Probabilistic Tube",
    "url": "http://arxiv.org/abs/2503.03328v1",
    "arxiv_id": "2503.03328v1",
    "authors": [
      "Zishun Liu",
      "Saber Jafarpour",
      "Yongxin Chen"
    ],
    "published": "2025-03-05T10:02:25+00:00",
    "summary": "We address the problem of safety verification for nonlinear stochastic systems, specifically the task of certifying that system trajectories remain within a safe set with high probability. To tackle this challenge, we adopt a set-erosion strategy, which decouples the effects of stochastic disturbances from deterministic dynamics. This approach converts the stochastic safety verification problem on a safe set into a deterministic safety verification problem on an eroded subset of the safe set. The success of this strategy hinges on the depth of erosion, which is determined by a probabilistic tube that bounds the deviation of stochastic trajectories from their corresponding deterministic trajectories. Our main contribution is the establishment of a tight bound for the probabilistic tube of nonlinear stochastic systems. To obtain a probabilistic bound for stochastic trajectories, we adopt a martingale-based approach. The core innovation lies in the design of a novel energy function associated with the averaged moment generating function, which forms an affine martingale, a generalization of the traditional c-martingale. Using this energy function, we derive a precise bound for the probabilistic tube. Furthermore, we enhance this bound by incorporating the union-bound inequality for strictly contractive dynamics. By integrating the derived probabilistic tubes into the set-erosion strategy, we demonstrate that the safety verification problem for nonlinear stochastic systems can be reduced to a deterministic safety verification problem. Our theoretical results are validated through applications in reachability-based safety verification and safe controller synthesis, accompanied by several numerical examples that illustrate their effectiveness."
  },
  {
    "title": "Supervised Visual Docking Network for Unmanned Surface Vehicles Using Auto-labeling in Real-world Water Environments",
    "url": "http://arxiv.org/abs/2503.03282v1",
    "arxiv_id": "2503.03282v1",
    "authors": [
      "Yijie Chu",
      "Ziniu Wu",
      "Yong Yue",
      "Eng Gee Lim",
      "Paolo Paoletti",
      "Xiaohui Zhu"
    ],
    "published": "2025-03-05T09:07:13+00:00",
    "summary": "Unmanned Surface Vehicles (USVs) are increasingly applied to water operations such as environmental monitoring and river-map modeling. It faces a significant challenge in achieving precise autonomous docking at ports or stations, still relying on remote human control or external positioning systems for accuracy and safety which limits the full potential of human-out-of-loop deployment for USVs.This paper introduces a novel supervised learning pipeline with the auto-labeling technique for USVs autonomous visual docking. Firstly, we designed an auto-labeling data collection pipeline that appends relative pose and image pair to the dataset. This step does not require conventional manual labeling for supervised learning. Secondly, the Neural Dock Pose Estimator (NDPE) is proposed to achieve relative dock pose prediction without the need for hand-crafted feature engineering, camera calibration, and peripheral markers. Moreover, The NDPE can accurately predict the relative dock pose in real-world water environments, facilitating the implementation of Position-Based Visual Servo (PBVS) and low-level motion controllers for efficient and autonomous docking.Experiments show that the NDPE is robust to the disturbance of the distance and the USV velocity. The effectiveness of our proposed solution is tested and validated in real-world water environments, reflecting its capability to handle real-world autonomous docking tasks."
  },
  {
    "title": "Reduced Spatial Dependency for More General Video-level Deepfake Detection",
    "url": "http://arxiv.org/abs/2503.03270v1",
    "arxiv_id": "2503.03270v1",
    "authors": [
      "Beilin Chu",
      "Xuan Xu",
      "Yufei Zhang",
      "Weike You",
      "Linna Zhou"
    ],
    "published": "2025-03-05T08:51:55+00:00",
    "summary": "As one of the prominent AI-generated content, Deepfake has raised significant safety concerns. Although it has been demonstrated that temporal consistency cues offer better generalization capability, existing methods based on CNNs inevitably introduce spatial bias, which hinders the extraction of intrinsic temporal features. To address this issue, we propose a novel method called Spatial Dependency Reduction (SDR), which integrates common temporal consistency features from multiple spatially-perturbed clusters, to reduce the dependency of the model on spatial information. Specifically, we design multiple Spatial Perturbation Branch (SPB) to construct spatially-perturbed feature clusters. Subsequently, we utilize the theory of mutual information and propose a Task-Relevant Feature Integration (TRFI) module to capture temporal features residing in similar latent space from these clusters. Finally, the integrated feature is fed into a temporal transformer to capture long-range dependencies. Extensive benchmarks and ablation studies demonstrate the effectiveness and rationale of our approach."
  },
  {
    "title": "Trajectory Prediction for Autonomous Driving: Progress, Limitations, and Future Directions",
    "url": "http://arxiv.org/abs/2503.03262v1",
    "arxiv_id": "2503.03262v1",
    "authors": [
      "Nadya Abdel Madjid",
      "Abdulrahman Ahmad",
      "Murad Mebrahtu",
      "Yousef Babaa",
      "Abdelmoamen Nasser",
      "Sumbal Malik",
      "Bilal Hassan",
      "Naoufel Werghi",
      "Jorge Dias",
      "Majid Khonji"
    ],
    "published": "2025-03-05T08:38:51+00:00",
    "summary": "As the potential for autonomous vehicles to be integrated on a large scale into modern traffic systems continues to grow, ensuring safe navigation in dynamic environments is crucial for smooth integration. To guarantee safety and prevent collisions, autonomous vehicles must be capable of accurately predicting the trajectories of surrounding traffic agents. Over the past decade, significant efforts from both academia and industry have been dedicated to designing solutions for precise trajectory forecasting. These efforts have produced a diverse range of approaches, raising questions about the differences between these methods and whether trajectory prediction challenges have been fully addressed. This paper reviews a substantial portion of recent trajectory prediction methods and devises a taxonomy to classify existing solutions. A general overview of the prediction pipeline is also provided, covering input and output modalities, modeling features, and prediction paradigms discussed in the literature. In addition, the paper discusses active research areas within trajectory prediction, addresses the posed research questions, and highlights the remaining research gaps and challenges."
  },
  {
    "title": "STORM: Spatial-Temporal Iterative Optimization for Reliable Multicopter Trajectory Generation",
    "url": "http://arxiv.org/abs/2503.03252v1",
    "arxiv_id": "2503.03252v1",
    "authors": [
      "Jinhao Zhang",
      "Zhexuan Zhou",
      "Wenlong Xia",
      "Youmin Gong",
      "Jie Mei"
    ],
    "published": "2025-03-05T08:11:59+00:00",
    "summary": "Efficient and safe trajectory planning plays a critical role in the application of quadrotor unmanned aerial vehicles. Currently, the inherent trade-off between constraint compliance and computational efficiency enhancement in UAV trajectory optimization problems has not been sufficiently addressed. To enhance the performance of UAV trajectory optimization, we propose a spatial-temporal iterative optimization framework. Firstly, B-splines are utilized to represent UAV trajectories, with rigorous safety assurance achieved through strict enforcement of constraints on control points. Subsequently, a set of QP-LP subproblems via spatial-temporal decoupling and constraint linearization is derived. Finally, an iterative optimization strategy incorporating guidance gradients is employed to obtain high-performance UAV trajectories in different scenarios. Both simulation and real-world experimental results validate the efficiency and high-performance of the proposed optimization framework in generating safe and fast trajectories. Our source codes will be released for community reference at https://hitsz-mas.github.io/STORM"
  },
  {
    "title": "Distributed Certifiably Correct Range-Aided SLAM",
    "url": "http://arxiv.org/abs/2503.03192v1",
    "arxiv_id": "2503.03192v1",
    "authors": [
      "Alexander Thoms",
      "Alan Papalia",
      "Jared Velasquez",
      "David M. Rosen",
      "Sriram Narasimhan"
    ],
    "published": "2025-03-05T05:17:15+00:00",
    "summary": "Reliable simultaneous localization and mapping (SLAM) algorithms are necessary for safety-critical autonomous navigation. In the communication-constrained multi-agent setting, navigation systems increasingly use point-to-point range sensors as they afford measurements with low bandwidth requirements and known data association. The state estimation problem for these systems takes the form of range-aided (RA) SLAM. However, distributed algorithms for solving the RA-SLAM problem lack formal guarantees on the quality of the returned estimate. To this end, we present the first distributed algorithm for RA-SLAM that can efficiently recover certifiably globally optimal solutions. Our algorithm, distributed certifiably correct RA-SLAM (DCORA), achieves this via the Riemannian Staircase method, where computational procedures developed for distributed certifiably correct pose graph optimization are generalized to the RA-SLAM problem. We demonstrate DCORA's efficacy on real-world multi-agent datasets by achieving absolute trajectory errors comparable to those of a state-of-the-art centralized certifiably correct RA-SLAM algorithm. Additionally, we perform a parametric study on the structure of the RA-SLAM problem using synthetic data, revealing how common parameters affect DCORA's performance."
  },
  {
    "title": "Physically-Feasible Reactive Synthesis for Terrain-Adaptive Locomotion via Trajectory Optimization and Symbolic Repair",
    "url": "http://arxiv.org/abs/2503.03071v1",
    "arxiv_id": "2503.03071v1",
    "authors": [
      "Ziyi Zhou",
      "Qian Meng",
      "Hadas Kress-Gazit",
      "Ye Zhao"
    ],
    "published": "2025-03-05T00:21:23+00:00",
    "summary": "We propose an integrated planning framework for quadrupedal locomotion over dynamically changing, unforeseen terrains. Existing approaches either rely on heuristics for instantaneous foothold selection--compromising safety and versatility--or solve expensive trajectory optimization problems with complex terrain features and long time horizons. In contrast, our framework leverages reactive synthesis to generate correct-by-construction controllers at the symbolic level, and mixed-integer convex programming (MICP) for dynamic and physically feasible footstep planning for each symbolic transition. We use a high-level manager to reduce the large state space in synthesis by incorporating local environment information, improving synthesis scalability. To handle specifications that cannot be met due to dynamic infeasibility, and to minimize costly MICP solves, we leverage a symbolic repair process to generate only necessary symbolic transitions. During online execution, re-running the MICP with real-world terrain data, along with runtime symbolic repair, bridges the gap between offline synthesis and online execution. We demonstrate, in simulation, our framework's capabilities to discover missing locomotion skills and react promptly in safety-critical environments, such as scattered stepping stones and rebars."
  },
  {
    "title": "LINGOLY-TOO: Disentangling Memorisation from Reasoning with Linguistic Templatisation and Orthographic Obfuscation",
    "url": "http://arxiv.org/abs/2503.02972v1",
    "arxiv_id": "2503.02972v1",
    "authors": [
      "Jude Khouja",
      "Karolina Korgul",
      "Simi Hellsten",
      "Lingyi Yang",
      "Vlad Neacs",
      "Harry Mayne",
      "Ryan Kearns",
      "Andrew Bean",
      "Adam Mahdi"
    ],
    "published": "2025-03-04T19:57:47+00:00",
    "summary": "Effective evaluation of the reasoning capabilities of large language models (LLMs) are susceptible to overestimation due to data exposure of evaluation benchmarks. We introduce a framework for producing linguistic reasoning problems that reduces the effect of memorisation in model performance estimates and apply this framework to develop LINGOLY-TOO, a challenging evaluation benchmark for linguistic reasoning. By developing orthographic templates, we dynamically obfuscate the writing systems of real languages to generate numerous question variations. These variations preserve the reasoning steps required for each solution while reducing the likelihood of specific problem instances appearing in model training data. Our experiments demonstrate that frontier models, including OpenAI o1-preview and DeepSeem R1, struggle with advanced reasoning. Our analysis also shows that LLMs exhibit noticeable variance in accuracy across permutations of the same problem, and on average perform better on questions appearing in their original orthography. Our findings highlight the opaque nature of response generation in LLMs and provide evidence that prior data exposure contributes to overestimating the reasoning capabilities of frontier models."
  },
  {
    "title": "LINGOLY-TOO: Disentangling Memorisation from Reasoning with Linguistic Templatisation and Orthographic Obfuscation",
    "url": "http://arxiv.org/abs/2503.02972v2",
    "arxiv_id": "2503.02972v2",
    "authors": [
      "Jude Khouja",
      "Karolina Korgul",
      "Simi Hellsten",
      "Lingyi Yang",
      "Vlad Neacs",
      "Harry Mayne",
      "Ryan Kearns",
      "Andrew Bean",
      "Adam Mahdi"
    ],
    "published": "2025-03-04T19:57:47+00:00",
    "summary": "Assessing the reasoning capabilities of large language models (LLMs) is susceptible to overestimation due to data exposure of evaluation benchmarks. We introduce a framework for producing linguistic reasoning problems that reduces the effect of memorisation in model performance estimates and apply this framework to develop LINGOLY-TOO, a challenging benchmark for linguistic reasoning. By developing orthographic templates, we dynamically obfuscate the writing systems of real languages to generate numerousquestion variations. These variations preserve the reasoning steps required for each solution while reducing the likelihood of specific problem instances appearing in model training data. Our experiments demonstrate that frontier models, including Claud 3.7 Sonnet, o1-preview and DeepSeek R1, struggle with advanced reasoning. Our analysis also shows that LLMs exhibit noticeable variance in accuracy across permutations of the same problem, and on average perform better on questions appearing in their original orthography. Our findings highlight the opaque nature of response generation in LLMs and provide evidence that prior data exposure contributes to over estimating the reasoning capabilities of frontier models."
  },
  {
    "title": "LINGOLY-TOO: Disentangling Memorisation from Reasoning with Linguistic Templatisation and Orthographic Obfuscation",
    "url": "http://arxiv.org/abs/2503.02972v3",
    "arxiv_id": "2503.02972v3",
    "authors": [
      "Jude Khouja",
      "Karolina Korgul",
      "Simi Hellsten",
      "Lingyi Yang",
      "Vlad Neacsu",
      "Harry Mayne",
      "Ryan Kearns",
      "Andrew Bean",
      "Adam Mahdi"
    ],
    "published": "2025-03-04T19:57:47+00:00",
    "summary": "Assessing the reasoning capabilities of large language models (LLMs) is susceptible to overestimation due to data exposure of evaluation benchmarks. We introduce a framework for producing linguistic reasoning problems that reduces the effect of memorisation in model performance estimates and apply this framework to develop LINGOLY-TOO, a challenging benchmark for linguistic reasoning. By developing orthographic templates, we dynamically obfuscate the writing systems of real languages to generate numerousquestion variations. These variations preserve the reasoning steps required for each solution while reducing the likelihood of specific problem instances appearing in model training data. Our experiments demonstrate that frontier models, including Claud 3.7 Sonnet, o1-preview and DeepSeek R1, struggle with advanced reasoning. Our analysis also shows that LLMs exhibit noticeable variance in accuracy across permutations of the same problem, and on average perform better on questions appearing in their original orthography. Our findings highlight the opaque nature of response generation in LLMs and provide evidence that prior data exposure contributes to over estimating the reasoning capabilities of frontier models."
  },
  {
    "title": "Revolutionizing Traffic Management with AI-Powered Machine Vision: A Step Toward Smart Cities",
    "url": "http://arxiv.org/abs/2503.02967v1",
    "arxiv_id": "2503.02967v1",
    "authors": [
      "Seyed Hossein Hosseini DolatAbadi",
      "Sayyed Mohammad Hossein Hashemi",
      "Mohammad Hosseini",
      "Moein-Aldin AliHosseini"
    ],
    "published": "2025-03-04T19:50:42+00:00",
    "summary": "The rapid urbanization of cities and increasing vehicular congestion have posed significant challenges to traffic management and safety. This study explores the transformative potential of artificial intelligence (AI) and machine vision technologies in revolutionizing traffic systems. By leveraging advanced surveillance cameras and deep learning algorithms, this research proposes a system for real-time detection of vehicles, traffic anomalies, and driver behaviors. The system integrates geospatial and weather data to adapt dynamically to environmental conditions, ensuring robust performance in diverse scenarios. Using YOLOv8 and YOLOv11 models, the study achieves high accuracy in vehicle detection and anomaly recognition, optimizing traffic flow and enhancing road safety. These findings contribute to the development of intelligent traffic management solutions and align with the vision of creating smart cities with sustainable and efficient urban infrastructure."
  },
  {
    "title": "A Theoretical Model for Grit in Pursuing Ambitious Ends",
    "url": "http://arxiv.org/abs/2503.02952v1",
    "arxiv_id": "2503.02952v1",
    "authors": [
      "Avrim Blum",
      "Emily Diana",
      "Kavya Ravichandran",
      "Alexander Williams Tolbert"
    ],
    "published": "2025-03-04T19:17:42+00:00",
    "summary": "Ambition and risk-taking have been heralded as important ways for marginalized communities to get out of cycles of poverty. As a result, educational messaging often encourages individuals to strengthen their personal resolve and develop characteristics such as discipline and grit to succeed in ambitious ends. However, recent work in philosophy and sociology highlights that this messaging often does more harm than good for students in these situations. We study similar questions using a different epistemic approach and in simple theoretical models -- we provide a quantitative model of decision-making between stable and risky choices in the improving multi-armed bandits framework. We use this model to first study how individuals' \"strategies\" are affected by their level of grittiness and how this affects their accrued rewards. Then, we study the impact of various interventions, such as increasing grit or providing a financial safety net. Our investigation of rational decision making involves two different formal models of rationality, the competitive ratio between the accrued reward and the optimal reward and Bayesian quantification of uncertainty."
  },
  {
    "title": "KodCode: A Diverse, Challenging, and Verifiable Synthetic Dataset for Coding",
    "url": "http://arxiv.org/abs/2503.02951v1",
    "arxiv_id": "2503.02951v1",
    "authors": [
      "Zhangchen Xu",
      "Yang Liu",
      "Yueqin Yin",
      "Mingyuan Zhou",
      "Radha Poovendran"
    ],
    "published": "2025-03-04T19:17:36+00:00",
    "summary": "We introduce KodCode, a synthetic dataset that addresses the persistent challenge of acquiring high-quality, verifiable training data across diverse difficulties and domains for training Large Language Models for coding. Existing code-focused resources typically fail to ensure either the breadth of coverage (e.g., spanning simple coding tasks to advanced algorithmic problems) or verifiable correctness (e.g., unit tests). In contrast, KodCode comprises question-solution-test triplets that are systematically validated via a self-verification procedure. Our pipeline begins by synthesizing a broad range of coding questions, then generates solutions and test cases with additional attempts allocated to challenging problems. Finally, post-training data synthesis is done by rewriting questions into diverse formats and generating responses under a test-based reject sampling procedure from a reasoning model (DeepSeek R1). This pipeline yields a large-scale, robust and diverse coding dataset. KodCode is suitable for supervised fine-tuning and the paired unit tests also provide great potential for RL tuning. Fine-tuning experiments on coding benchmarks (HumanEval(+), MBPP(+), BigCodeBench, and LiveCodeBench) demonstrate that KodCode-tuned models achieve state-of-the-art performance, surpassing models like Qwen2.5-Coder-32B-Instruct and DeepSeek-R1-Distill-Llama-70B."
  },
  {
    "title": "Optimal Power Management for Large-Scale Battery Energy Storage Systems via Bayesian Inference",
    "url": "http://arxiv.org/abs/2503.02866v1",
    "arxiv_id": "2503.02866v1",
    "authors": [
      "Amir Farakhor",
      "Iman Askari",
      "Di Wu",
      "Yebin Wang",
      "Huazhen Fang"
    ],
    "published": "2025-03-04T18:45:40+00:00",
    "summary": "Large-scale battery energy storage systems (BESS) have found ever-increasing use across industry and society to accelerate clean energy transition and improve energy supply reliability and resilience. However, their optimal power management poses significant challenges: the underlying high-dimensional nonlinear nonconvex optimization lacks computational tractability in real-world implementation, and the uncertainty of the exogenous power demand makes exact optimization difficult. This paper presents a new solution framework to address these bottlenecks. The solution pivots on introducing power-sharing ratios to specify each cell's power quota from the output power demand. To find the optimal power-sharing ratios, we formulate a nonlinear model predictive control (NMPC) problem to achieve power-loss-minimizing BESS operation while complying with safety, cell balancing, and power supply-demand constraints. We then propose a parameterized control policy for the power-sharing ratios, which utilizes only three parameters, to reduce the computational demand in solving the NMPC problem. This policy parameterization allows us to translate the NMPC problem into a Bayesian inference problem for the sake of 1) computational tractability, and 2) overcoming the nonconvexity of the optimization problem. We leverage the ensemble Kalman inversion technique to solve the parameter estimation problem. Concurrently, a low-level control loop is developed to seamlessly integrate our proposed approach with the BESS to ensure practical implementation. This low-level controller receives the optimal power-sharing ratios, generates output power references for the cells, and maintains a balance between power supply and demand despite uncertainty in output power. We conduct extensive simulations and experiments on a 20-cell prototype to validate the proposed approach."
  },
  {
    "title": "FairSense-AI: Responsible AI Meets Sustainability",
    "url": "http://arxiv.org/abs/2503.02865v1",
    "arxiv_id": "2503.02865v1",
    "authors": [
      "Shaina Raza",
      "Mukund Sayeeganesh Chettiar",
      "Matin Yousefabadi",
      "Tahniat Khan",
      "Marcelo Lotif"
    ],
    "published": "2025-03-04T18:43:57+00:00",
    "summary": "In this paper, we introduce FairSense-AI: a multimodal framework designed to detect and mitigate bias in both text and images. By leveraging Large Language Models (LLMs) and Vision-Language Models (VLMs), FairSense-AI uncovers subtle forms of prejudice or stereotyping that can appear in content, providing users with bias scores, explanatory highlights, and automated recommendations for fairness enhancements. In addition, FairSense-AI integrates an AI risk assessment component that aligns with frameworks like the MIT AI Risk Repository and NIST AI Risk Management Framework, enabling structured identification of ethical and safety concerns. The platform is optimized for energy efficiency via techniques such as model pruning and mixed-precision computation, thereby reducing its environmental footprint. Through a series of case studies and applications, we demonstrate how FairSense-AI promotes responsible AI use by addressing both the social dimension of fairness and the pressing need for sustainability in large-scale AI deployments. https://vectorinstitute.github.io/FairSense-AI, https://pypi.org/project/fair-sense-ai/"
  },
  {
    "title": "FairSense-AI: Responsible AI Meets Sustainability",
    "url": "http://arxiv.org/abs/2503.02865v2",
    "arxiv_id": "2503.02865v2",
    "authors": [
      "Shaina Raza",
      "Mukund Sayeeganesh Chettiar",
      "Matin Yousefabadi",
      "Tahniat Khan",
      "Marcelo Lotif"
    ],
    "published": "2025-03-04T18:43:57+00:00",
    "summary": "In this paper, we introduce FairSense-AI: a multimodal framework designed to detect and mitigate bias in both text and images. By leveraging Large Language Models (LLMs) and Vision-Language Models (VLMs), FairSense-AI uncovers subtle forms of prejudice or stereotyping that can appear in content, providing users with bias scores, explanatory highlights, and automated recommendations for fairness enhancements. In addition, FairSense-AI integrates an AI risk assessment component that aligns with frameworks like the MIT AI Risk Repository and NIST AI Risk Management Framework, enabling structured identification of ethical and safety concerns. The platform is optimized for energy efficiency via techniques such as model pruning and mixed-precision computation, thereby reducing its environmental footprint. Through a series of case studies and applications, we demonstrate how FairSense-AI promotes responsible AI use by addressing both the social dimension of fairness and the pressing need for sustainability in large-scale AI deployments. https://vectorinstitute.github.io/FairSense-AI, https://pypi.org/project/fair-sense-ai/ (Sustainability , Responsible AI , Large Language Models , Vision Language Models , Ethical AI , Green AI)"
  },
  {
    "title": "Multimodal AI predicts clinical outcomes of drug combinations from preclinical data",
    "url": "http://arxiv.org/abs/2503.02781v1",
    "arxiv_id": "2503.02781v1",
    "authors": [
      "Yepeng Huang",
      "Xiaorui Su",
      "Varun Ullanat",
      "Ivy Liang",
      "Lindsay Clegg",
      "Damilola Olabode",
      "Nicholas Ho",
      "Bino John",
      "Megan Gibbs",
      "Marinka Zitnik"
    ],
    "published": "2025-03-04T16:55:14+00:00",
    "summary": "Predicting clinical outcomes from preclinical data is essential for identifying safe and effective drug combinations. Current models rely on structural or target-based features to identify high-efficacy, low-toxicity drug combinations. However, these approaches fail to incorporate the multimodal data necessary for accurate, clinically-relevant predictions. Here, we introduce MADRIGAL, a multimodal AI model that learns from structural, pathway, cell viability, and transcriptomic data to predict drug combination effects across 953 clinical outcomes and 21842 compounds, including combinations of approved drugs and novel compounds in development. MADRIGAL uses a transformer bottleneck module to unify preclinical drug data modalities while handling missing data during training and inference--a major challenge in multimodal learning. It outperforms single-modality methods and state-of-the-art models in predicting adverse drug interactions. MADRIGAL performs virtual screening of anticancer drug combinations and supports polypharmacy management for type II diabetes and metabolic dysfunction-associated steatohepatitis (MASH). It identifies transporter-mediated drug interactions. MADRIGAL predicts resmetirom, the first and only FDA-approved drug for MASH, among therapies with the most favorable safety profile. It supports personalized cancer therapy by integrating genomic profiles from cancer patients. Using primary acute myeloid leukemia samples and patient-derived xenograft models, it predicts the efficacy of personalized drug combinations. Integrating MADRIGAL with a large language model allows users to describe clinical outcomes in natural language, improving safety assessment by identifying potential adverse interactions and toxicity risks. MADRIGAL provides a multimodal approach for designing combination therapies with improved predictive accuracy and clinical relevance."
  },
  {
    "title": "Deep Learning-Enhanced Visual Monitoring in Hazardous Underwater Environments with a Swarm of Micro-Robots",
    "url": "http://arxiv.org/abs/2503.02752v1",
    "arxiv_id": "2503.02752v1",
    "authors": [
      "Shuang Chen",
      "Yifeng He",
      "Barry Lennox",
      "Farshad Arvin",
      "Amir Atapour-Abarghouei"
    ],
    "published": "2025-03-04T16:19:06+00:00",
    "summary": "Long-term monitoring and exploration of extreme environments, such as underwater storage facilities, is costly, labor-intensive, and hazardous. Automating this process with low-cost, collaborative robots can greatly improve efficiency. These robots capture images from different positions, which must be processed simultaneously to create a spatio-temporal model of the facility. In this paper, we propose a novel approach that integrates data simulation, a multi-modal deep learning network for coordinate prediction, and image reassembly to address the challenges posed by environmental disturbances causing drift and rotation in the robots' positions and orientations. Our approach enhances the precision of alignment in noisy environments by integrating visual information from snapshots, global positional context from masks, and noisy coordinates. We validate our method through extensive experiments using synthetic data that simulate real-world robotic operations in underwater settings. The results demonstrate very high coordinate prediction accuracy and plausible image assembly, indicating the real-world applicability of our approach. The assembled images provide clear and coherent views of the underwater environment for effective monitoring and inspection, showcasing the potential for broader use in extreme settings, further contributing to improved safety, efficiency, and cost reduction in hazardous field monitoring. Code is available on https://github.com/ChrisChen1023/Micro-Robot-Swarm."
  },
  {
    "title": "Generative Modeling of Microweather Wind Velocities for Urban Air Mobility",
    "url": "http://arxiv.org/abs/2503.02690v1",
    "arxiv_id": "2503.02690v1",
    "authors": [
      "Tristan A. Shah",
      "Michael C. Stanley",
      "James E. Warner"
    ],
    "published": "2025-03-04T15:03:15+00:00",
    "summary": "Motivated by the pursuit of safe, reliable, and weather-tolerant urban air mobility (UAM) solutions, this work proposes a generative modeling approach for characterizing microweather wind velocities. Microweather, or the weather conditions in highly localized areas, is particularly complex in urban environments owing to the chaotic and turbulent nature of wind flows. Furthermore, traditional means of assessing local wind fields are not generally viable solutions for UAM applications: 1) field measurements that would rely on permanent wind profiling systems in operational air space are not practical, 2) physics-based models that simulate fluid dynamics at a sufficiently high resolution are not computationally tractable, and 3) data-driven modeling approaches that are largely deterministic ignore the inherent variability in turbulent flows that dictates UAM reliability. Thus, advancements in predictive capabilities are needed to help mitigate the unique operational safety risks that microweather winds pose for smaller, lighter weight UAM aircraft.   This work aims to model microweather wind velocities in a manner that is computationally-efficient, captures random variability, and would only require a temporary, rather than permanent, field measurement campaign. Inspired by recent breakthroughs in conditional generative AI such as text-to-image generation, the proposed approach learns a probabilistic macro-to-microweather mapping between regional weather forecasts and measured local wind velocities using generative modeling (denoising diffusion probabilistic models, flow matching, and Gaussian mixture models). A simple proof of concept was implemented using a dataset comprised of local (micro) measurements from a Sonic Detection and Ranging (SoDAR) wind profiler along with (macro) forecast data from a nearby weather station over the same time period."
  },
  {
    "title": "State of play and future directions in industrial computer vision AI standards",
    "url": "http://arxiv.org/abs/2503.02675v1",
    "arxiv_id": "2503.02675v1",
    "authors": [
      "Artemis Stefanidou",
      "Panagiotis Radoglou-Grammatikis",
      "Vasileios Argyriou",
      "Panagiotis Sarigiannidis",
      "Iraklis Varlamis",
      "Georgios Th. Papadopoulos"
    ],
    "published": "2025-03-04T14:46:34+00:00",
    "summary": "The recent tremendous advancements in the areas of Artificial Intelligence (AI) and Deep Learning (DL) have also resulted into corresponding remarkable progress in the field of Computer Vision (CV), showcasing robust technological solutions in a wide range of application sectors of high industrial interest (e.g., healthcare, autonomous driving, automation, etc.). Despite the outstanding performance of CV systems in specific domains, their development and exploitation at industrial-scale necessitates, among other, the addressing of requirements related to the reliability, transparency, trustworthiness, security, safety, and robustness of the developed AI models. The latter raises the imperative need for the development of efficient, comprehensive and widely-adopted industrial standards. In this context, this study investigates the current state of play regarding the development of industrial computer vision AI standards, emphasizing on critical aspects, like model interpretability, data quality, and regulatory compliance. In particular, a systematic analysis of launched and currently developing CV standards, proposed by the main international standardization bodies (e.g. ISO/IEC, IEEE, DIN, etc.) is performed. The latter is complemented by a comprehensive discussion on the current challenges and future directions observed in this regularization endeavor."
  },
  {
    "title": "Human-aligned Safe Reinforcement Learning for Highway On-Ramp Merging in Dense Traffic",
    "url": "http://arxiv.org/abs/2503.02624v1",
    "arxiv_id": "2503.02624v1",
    "authors": [
      "Yang Li",
      "Shijie Yuan",
      "Yuan Chang",
      "Xiaolong Chen",
      "Qisong Yang",
      "Zhiyuan Yang",
      "Hongmao Qin"
    ],
    "published": "2025-03-04T13:49:12+00:00",
    "summary": "Most reinforcement learning (RL) approaches for the decision-making of autonomous driving consider safety as a reward instead of a cost, which makes it hard to balance the tradeoff between safety and other objectives. Human risk preference has also rarely been incorporated, and the trained policy might be either conservative or aggressive for users. To this end, this study proposes a human-aligned safe RL approach for autonomous merging, in which the high-level decision problem is formulated as a constrained Markov decision process (CMDP) that incorporates users' risk preference into the safety constraints, followed by a model predictive control (MPC)-based low-level control. The safety level of RL policy can be adjusted by computing cost limits of CMDP's constraints based on risk preferences and traffic density using a fuzzy control method. To filter out unsafe or invalid actions, we design an action shielding mechanism that pre-executes RL actions using an MPC method and performs collision checks with surrounding agents. We also provide theoretical proof to validate the effectiveness of the shielding mechanism in enhancing RL's safety and sample efficiency. Simulation experiments in multiple levels of traffic densities show that our method can significantly reduce safety violations without sacrificing traffic efficiency. Furthermore, due to the use of risk preference-aware constraints in CMDP and action shielding, we can not only adjust the safety level of the final policy but also reduce safety violations during the training stage, proving a promising solution for online learning in real-world environments."
  },
  {
    "title": "MM-OR: A Large Multimodal Operating Room Dataset for Semantic Understanding of High-Intensity Surgical Environments",
    "url": "http://arxiv.org/abs/2503.02579v1",
    "arxiv_id": "2503.02579v1",
    "authors": [
      "Ege \u00d6zsoy",
      "Chantal Pellegrini",
      "Tobias Czempiel",
      "Felix Tristram",
      "Kun Yuan",
      "David Bani-Harouni",
      "Ulrich Eck",
      "Benjamin Busam",
      "Matthias Keicher",
      "Nassir Navab"
    ],
    "published": "2025-03-04T13:00:52+00:00",
    "summary": "Operating rooms (ORs) are complex, high-stakes environments requiring precise understanding of interactions among medical staff, tools, and equipment for enhancing surgical assistance, situational awareness, and patient safety. Current datasets fall short in scale, realism and do not capture the multimodal nature of OR scenes, limiting progress in OR modeling. To this end, we introduce MM-OR, a realistic and large-scale multimodal spatiotemporal OR dataset, and the first dataset to enable multimodal scene graph generation. MM-OR captures comprehensive OR scenes containing RGB-D data, detail views, audio, speech transcripts, robotic logs, and tracking data and is annotated with panoptic segmentations, semantic scene graphs, and downstream task labels. Further, we propose MM2SG, the first multimodal large vision-language model for scene graph generation, and through extensive experiments, demonstrate its ability to effectively leverage multimodal inputs. Together, MM-OR and MM2SG establish a new benchmark for holistic OR understanding, and open the path towards multimodal scene analysis in complex, high-stakes environments. Our code, and data is available at https://github.com/egeozsoy/MM-OR."
  },
  {
    "title": "LLM-Safety Evaluations Lack Robustness",
    "url": "http://arxiv.org/abs/2503.02574v1",
    "arxiv_id": "2503.02574v1",
    "authors": [
      "Tim Beyer",
      "Sophie Xhonneux",
      "Simon Geisler",
      "Gauthier Gidel",
      "Leo Schwinn",
      "Stephan G\u00fcnnemann"
    ],
    "published": "2025-03-04T12:55:07+00:00",
    "summary": "In this paper, we argue that current safety alignment research efforts for large language models are hindered by many intertwined sources of noise, such as small datasets, methodological inconsistencies, and unreliable evaluation setups. This can, at times, make it impossible to evaluate and compare attacks and defenses fairly, thereby slowing progress. We systematically analyze the LLM safety evaluation pipeline, covering dataset curation, optimization strategies for automated red-teaming, response generation, and response evaluation using LLM judges. At each stage, we identify key issues and highlight their practical impact. We also propose a set of guidelines for reducing noise and bias in evaluations of future attack and defense papers. Lastly, we offer an opposing perspective, highlighting practical reasons for existing limitations. We believe that addressing the outlined problems in future research will improve the field's ability to generate easily comparable results and make measurable progress."
  },
  {
    "title": "Tracking-Aware Deformation Field Estimation for Non-rigid 3D Reconstruction in Robotic Surgeries",
    "url": "http://arxiv.org/abs/2503.02558v1",
    "arxiv_id": "2503.02558v1",
    "authors": [
      "Zeqing Wang",
      "Han Fang",
      "Yihong Xu",
      "Yutong Ban"
    ],
    "published": "2025-03-04T12:33:17+00:00",
    "summary": "Minimally invasive procedures have been advanced rapidly by the robotic laparoscopic surgery. The latter greatly assists surgeons in sophisticated and precise operations with reduced invasiveness. Nevertheless, it is still safety critical to be aware of even the least tissue deformation during instrument-tissue interactions, especially in 3D space. To address this, recent works rely on NeRF to render 2D videos from different perspectives and eliminate occlusions. However, most of the methods fail to predict the accurate 3D shapes and associated deformation estimates robustly. Differently, we propose Tracking-Aware Deformation Field (TADF), a novel framework which reconstructs the 3D mesh along with the 3D tissue deformation simultaneously. It first tracks the key points of soft tissue by a foundation vision model, providing an accurate 2D deformation field. Then, the 2D deformation field is smoothly incorporated with a neural implicit reconstruction network to obtain tissue deformation in the 3D space. Finally, we experimentally demonstrate that the proposed method provides more accurate deformation estimation compared with other 3D neural reconstruction methods in two public datasets."
  },
  {
    "title": "World Models for Anomaly Detection during Model-Based Reinforcement Learning Inference",
    "url": "http://arxiv.org/abs/2503.02552v1",
    "arxiv_id": "2503.02552v1",
    "authors": [
      "Fabian Domberg",
      "Georg Schildbach"
    ],
    "published": "2025-03-04T12:25:01+00:00",
    "summary": "Learning-based controllers are often purposefully kept out of real-world applications due to concerns about their safety and reliability. We explore how state-of-the-art world models in Model-Based Reinforcement Learning can be utilized beyond the training phase to ensure a deployed policy only operates within regions of the state-space it is sufficiently familiar with. This is achieved by continuously monitoring discrepancies between a world model's predictions and observed system behavior during inference. It allows for triggering appropriate measures, such as an emergency stop, once an error threshold is surpassed. This does not require any task-specific knowledge and is thus universally applicable. Simulated experiments on established robot control tasks show the effectiveness of this method, recognizing changes in local robot geometry and global gravitational magnitude. Real-world experiments using an agile quadcopter further demonstrate the benefits of this approach by detecting unexpected forces acting on the vehicle. These results indicate how even in new and adverse conditions, safe and reliable operation of otherwise unpredictable learning-based controllers can be achieved."
  },
  {
    "title": "A Systematic Literature Review on Safety of the Intended Functionality for Automated Driving Systems",
    "url": "http://arxiv.org/abs/2503.02498v1",
    "arxiv_id": "2503.02498v1",
    "authors": [
      "Milin Patel",
      "Rolf Jung",
      "Marzana Khatun"
    ],
    "published": "2025-03-04T11:04:36+00:00",
    "summary": "In the automobile industry, ensuring the safety of automated vehicles equipped with the Automated Driving System (ADS) is becoming a significant focus due to the increasing development and deployment of automated driving. Automated driving depends on sensing both the external and internal environments of a vehicle, utilizing perception sensors and algorithms, and Electrical/Electronic (E/E) systems for situational awareness and response. ISO 21448 is the standard for Safety of the Intended Functionality (SOTIF) that aims to ensure that the ADS operate safely within their intended functionality. SOTIF focuses on preventing or mitigating potential hazards that may arise from the limitations or failures of the ADS, including hazards due to insufficiencies of specification, or performance insufficiencies, as well as foreseeable misuse of the intended functionality. However, the challenge lies in ensuring the safety of vehicles despite the limited availability of extensive and systematic literature on SOTIF. To address this challenge, a Systematic Literature Review (SLR) on SOTIF for the ADS is performed following the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines. The objective is to methodically gather and analyze the existing literature on SOTIF. The major contributions of this paper are: (i) presenting a summary of the literature by synthesizing and organizing the collective findings, methodologies, and insights into distinct thematic groups, and (ii) summarizing and categorizing the acknowledged limitations based on data extracted from an SLR of 51 research papers published between 2018 and 2023. Furthermore, research gaps are determined, and future research directions are proposed."
  },
  {
    "title": "UAV-VLPA*: A Vision-Language-Path-Action System for Optimal Route Generation on a Large Scales",
    "url": "http://arxiv.org/abs/2503.02454v1",
    "arxiv_id": "2503.02454v1",
    "authors": [
      "Oleg Sautenkov",
      "Aibek Akhmetkazy",
      "Yasheerah Yaqoot",
      "Muhammad Ahsan Mustafa",
      "Grik Tadevosyan",
      "Artem Lykov",
      "Dzmitry Tsetserukou"
    ],
    "published": "2025-03-04T10:02:53+00:00",
    "summary": "The UAV-VLPA* (Visual-Language-Planning-and-Action) system represents a cutting-edge advancement in aerial robotics, designed to enhance communication and operational efficiency for unmanned aerial vehicles (UAVs). By integrating advanced planning capabilities, the system addresses the Traveling Salesman Problem (TSP) to optimize flight paths, reducing the total trajectory length by 18.5\\% compared to traditional methods. Additionally, the incorporation of the A* algorithm enables robust obstacle avoidance, ensuring safe and efficient navigation in complex environments. The system leverages satellite imagery processing combined with the Visual Language Model (VLM) and GPT's natural language processing capabilities, allowing users to generate detailed flight plans through simple text commands. This seamless fusion of visual and linguistic analysis empowers precise decision-making and mission planning, making UAV-VLPA* a transformative tool for modern aerial operations. With its unmatched operational efficiency, navigational safety, and user-friendly functionality, UAV-VLPA* sets a new standard in autonomous aerial robotics, paving the way for future innovations in the field."
  },
  {
    "title": "Artificial Intelligence in Reactor Physics: Current Status and Future Prospects",
    "url": "http://arxiv.org/abs/2503.02440v1",
    "arxiv_id": "2503.02440v1",
    "authors": [
      "Ruizhi Zhang",
      "Shengfeng Zhu",
      "Kan Wang",
      "Ding She",
      "Jean-Philippe Argaud",
      "Bertrand Bouriquet",
      "Qing Li",
      "Helin Gong"
    ],
    "published": "2025-03-04T09:36:58+00:00",
    "summary": "Reactor physics is the study of neutron properties, focusing on using models to examine the interactions between neutrons and materials in nuclear reactors. Artificial intelligence (AI) has made significant contributions to reactor physics, e.g., in operational simulations, safety design, real-time monitoring, core management and maintenance. This paper presents a comprehensive review of AI approaches in reactor physics, especially considering the category of Machine Learning (ML), with the aim of describing the application scenarios, frontier topics, unsolved challenges and future research directions. From equation solving and state parameter prediction to nuclear industry applications, this paper provides a step-by-step overview of ML methods applied to steady-state, transient and combustion problems. Most literature works achieve industry-demanded models by enhancing the efficiency of deterministic methods or correcting uncertainty methods, which leads to successful applications. However, research on ML methods in reactor physics is somewhat fragmented, and the ability to generalize models needs to be strengthened. Progress is still possible, especially in addressing theoretical challenges and enhancing industrial applications such as building surrogate models and digital twins."
  },
  {
    "title": "Unlocking a New Rust Programming Experience: Fast and Slow Thinking with LLMs to Conquer Undefined Behaviors",
    "url": "http://arxiv.org/abs/2503.02335v1",
    "arxiv_id": "2503.02335v1",
    "authors": [
      "Renshuang Jiang",
      "Pan Dong",
      "Zhenling Duan",
      "Yu Shi",
      "Xiaoxiang Fang",
      "Yan Ding",
      "Jun Ma",
      "Shuai Zhao",
      "Zhe Jiang"
    ],
    "published": "2025-03-04T06:48:45+00:00",
    "summary": "To provide flexibility and low-level interaction capabilities, the unsafe tag in Rust is essential in many projects, but undermines memory safety and introduces Undefined Behaviors (UBs) that reduce safety. Eliminating these UBs requires a deep understanding of Rust's safety rules and strong typing. Traditional methods require depth analysis of code, which is laborious and depends on knowledge design. The powerful semantic understanding capabilities of LLM offer new opportunities to solve this problem. Although existing large model debugging frameworks excel in semantic tasks, limited by fixed processes and lack adaptive and dynamic adjustment capabilities. Inspired by the dual process theory of decision-making (Fast and Slow Thinking), we present a LLM-based framework called RustBrain that automatically and flexibly minimizes UBs in Rust projects. Fast thinking extracts features to generate solutions, while slow thinking decomposes, verifies, and generalizes them abstractly. To apply verification and generalization results to solution generation, enabling dynamic adjustments and precise outputs, RustBrain integrates two thinking through a feedback mechanism. Experimental results on Miri dataset show a 94.3% pass rate and 80.4% execution rate, improving flexibility and Rust projects safety."
  },
  {
    "title": "Rethinking Static Line Rating for Economic and Efficient Power Operation in South Korea",
    "url": "http://arxiv.org/abs/2503.02274v1",
    "arxiv_id": "2503.02274v1",
    "authors": [
      "Junseon Park",
      "Junhyun Lee",
      "Hyeongon Park"
    ],
    "published": "2025-03-04T04:46:43+00:00",
    "summary": "In South Korea, power grid is currently operated based on the static line rating (SLR) method, where the transmission   line capacity is determined based on extreme weather conditions. However, with global warming, there is a concern   that the temperatures during summer may exceed the SLR criteria, posing safety risks. On the other hand, the conservative estimates used for winter conditions limit the utilization of renewable energy. Proposals to install new lines face   significant financial and environmental hurdles, complicating efforts to adapt to these changing conditions. Dynamic   Line Rating (DLR) offers a real-time solution but requires extensive weather monitoring and complex integration. This   paper proposes a novel method that improves on SLR by analyzing historical data to refine line rating criteria on a   monthly, seasonal, and semi-annual basis. Through simulations, we show our approach significantly enhances cost effectiveness and reliability of the power system, achieving efficiencies close to DLR with existing infrastructure. This   method offers a practical alternative to overcome the limitations of SLR and the implementation challenges of DLR."
  },
  {
    "title": "V2X-LLM: Enhancing V2X Integration and Understanding in Connected Vehicle Corridors",
    "url": "http://arxiv.org/abs/2503.02239v1",
    "arxiv_id": "2503.02239v1",
    "authors": [
      "Keshu Wu",
      "Pei Li",
      "Yang Zhou",
      "Rui Gan",
      "Junwei You",
      "Yang Cheng",
      "Jingwen Zhu",
      "Steven T. Parker",
      "Bin Ran",
      "David A. Noyce",
      "Zhengzhong Tu"
    ],
    "published": "2025-03-04T03:28:30+00:00",
    "summary": "The advancement of Connected and Automated Vehicles (CAVs) and Vehicle-to-Everything (V2X) offers significant potential for enhancing transportation safety, mobility, and sustainability. However, the integration and analysis of the diverse and voluminous V2X data, including Basic Safety Messages (BSMs) and Signal Phase and Timing (SPaT) data, present substantial challenges, especially on Connected Vehicle Corridors. These challenges include managing large data volumes, ensuring real-time data integration, and understanding complex traffic scenarios. Although these projects have developed an advanced CAV data pipeline that enables real-time communication between vehicles, infrastructure, and other road users for managing connected vehicle and roadside unit (RSU) data, significant hurdles in data comprehension and real-time scenario analysis and reasoning persist. To address these issues, we introduce the V2X-LLM framework, a novel enhancement to the existing CV data pipeline. V2X-LLM leverages Large Language Models (LLMs) to improve the understanding and real-time analysis of V2X data. The framework includes four key tasks: Scenario Explanation, offering detailed narratives of traffic conditions; V2X Data Description, detailing vehicle and infrastructure statuses; State Prediction, forecasting future traffic states; and Navigation Advisory, providing optimized routing instructions. By integrating LLM-driven reasoning with V2X data within the data pipeline, the V2X-LLM framework offers real-time feedback and decision support for traffic management. This integration enhances the accuracy of traffic analysis, safety, and traffic optimization. Demonstrations in a real-world urban corridor highlight the framework's potential to advance intelligent transportation systems."
  },
  {
    "title": "ADMM-MCBF-LCA: A Layered Control Architecture for Safe Real-Time Navigation",
    "url": "http://arxiv.org/abs/2503.02208v1",
    "arxiv_id": "2503.02208v1",
    "authors": [
      "Anusha Srikanthan",
      "Yifan Xue",
      "Vijay Kumar",
      "Nikolai Matni",
      "Nadia Figueroa"
    ],
    "published": "2025-03-04T02:40:17+00:00",
    "summary": "We consider the problem of safe real-time navigation of a robot in a dynamic environment with moving obstacles of arbitrary smooth geometries and input saturation constraints. We assume that the robot detects and models nearby obstacle boundaries with a short-range sensor and that this detection is error-free. This problem presents three main challenges: i) input constraints, ii) safety, and iii) real-time computation. To tackle all three challenges, we present a layered control architecture (LCA) consisting of an offline path library generation layer, and an online path selection and safety layer. To overcome the limitations of reactive methods, our offline path library consists of feasible controllers, feedback gains, and reference trajectories. To handle computational burden and safety, we solve online path selection and generate safe inputs that run at 100 Hz. Through simulations on Gazebo and Fetch hardware in an indoor environment, we evaluate our approach against baselines that are layered, end-to-end, or reactive. Our experiments demonstrate that among all algorithms, only our proposed LCA is able to complete tasks such as reaching a goal, safely. When comparing metrics such as safety, input error, and success rate, we show that our approach generates safe and feasible inputs throughout the robot execution."
  },
  {
    "title": "Words or Vision: Do Vision-Language Models Have Blind Faith in Text?",
    "url": "http://arxiv.org/abs/2503.02199v1",
    "arxiv_id": "2503.02199v1",
    "authors": [
      "Ailin Deng",
      "Tri Cao",
      "Zhirui Chen",
      "Bryan Hooi"
    ],
    "published": "2025-03-04T02:21:07+00:00",
    "summary": "Vision-Language Models (VLMs) excel in integrating visual and textual information for vision-centric tasks, but their handling of inconsistencies between modalities is underexplored. We investigate VLMs' modality preferences when faced with visual data and varied textual inputs in vision-centered settings. By introducing textual variations to four vision-centric tasks and evaluating ten Vision-Language Models (VLMs), we discover a \\emph{``blind faith in text''} phenomenon: VLMs disproportionately trust textual data over visual data when inconsistencies arise, leading to significant performance drops under corrupted text and raising safety concerns. We analyze factors influencing this text bias, including instruction prompts, language model size, text relevance, token order, and the interplay between visual and textual certainty. While certain factors, such as scaling up the language model size, slightly mitigate text bias, others like token order can exacerbate it due to positional biases inherited from language models. To address this issue, we explore supervised fine-tuning with text augmentation and demonstrate its effectiveness in reducing text bias. Additionally, we provide a theoretical analysis suggesting that the blind faith in text phenomenon may stem from an imbalance of pure text and multi-modal data during training. Our findings highlight the need for balanced training and careful consideration of modality interactions in VLMs to enhance their robustness and reliability in handling multi-modal data inconsistencies."
  },
  {
    "title": "Adversarial Tokenization",
    "url": "http://arxiv.org/abs/2503.02174v1",
    "arxiv_id": "2503.02174v1",
    "authors": [
      "Renato Lui Geh",
      "Zilei Shao",
      "Guy Van den Broeck"
    ],
    "published": "2025-03-04T01:31:17+00:00",
    "summary": "Current LLM pipelines account for only one possible tokenization for a given string, ignoring exponentially many alternative tokenizations during training and inference. For example, the standard Llama3 tokenization of penguin is [p,enguin], yet [peng,uin] is another perfectly valid alternative. In this paper, we show that despite LLMs being trained solely on one tokenization, they still retain semantic understanding of other tokenizations, raising questions about their implications in LLM safety. Put succinctly, we answer the following question: can we adversarially tokenize an obviously malicious string to evade safety and alignment restrictions? We show that not only is adversarial tokenization an effective yet previously neglected axis of attack, but it is also competitive against existing state-of-the-art adversarial approaches without changing the text of the harmful request. We empirically validate this exploit across three state-of-the-art LLMs and adversarial datasets, revealing a previously unknown vulnerability in subword models."
  },
  {
    "title": "Four Principles for Physically Interpretable World Models",
    "url": "http://arxiv.org/abs/2503.02143v1",
    "arxiv_id": "2503.02143v1",
    "authors": [
      "Jordan Peper",
      "Zhenjiang Mao",
      "Yuang Geng",
      "Siyuan Pan",
      "Ivan Ruchkin"
    ],
    "published": "2025-03-04T00:19:32+00:00",
    "summary": "As autonomous systems are increasingly deployed in open and uncertain settings, there is a growing need for trustworthy world models that can reliably predict future high-dimensional observations. The learned latent representations in world models lack direct mapping to meaningful physical quantities and dynamics, limiting their utility and interpretability in downstream planning, control, and safety verification. In this paper, we argue for a fundamental shift from physically informed to physically interpretable world models - and crystallize four principles that leverage symbolic knowledge to achieve these ends: (1) structuring latent spaces according to the physical intent of variables, (2) learning aligned invariant and equivariant representations of the physical world, (3) adapting training to the varied granularity of supervision signals, and (4) partitioning generative outputs to support scalability and verifiability. We experimentally demonstrate the value of each principle on two benchmarks. This paper opens several intriguing research directions to achieve and capitalize on full physical interpretability in world models."
  },
  {
    "title": "NavG: Risk-Aware Navigation in Crowded Environments Based on Reinforcement Learning with Guidance Points",
    "url": "http://arxiv.org/abs/2503.02111v1",
    "arxiv_id": "2503.02111v1",
    "authors": [
      "Qianyi Zhang",
      "Wentao Luo",
      "Boyi Liu",
      "Ziyang Zhang",
      "Yaoyuan Wang",
      "Jingtai Liu"
    ],
    "published": "2025-03-03T22:53:06+00:00",
    "summary": "Motion planning in navigation systems is highly susceptible to upstream perceptual errors, particularly in human detection and tracking. To mitigate this issue, the concept of guidance points--a novel directional cue within a reinforcement learning-based framework--is introduced. A structured method for identifying guidance points is developed, consisting of obstacle boundary extraction, potential guidance point detection, and redundancy elimination. To integrate guidance points into the navigation pipeline, a perception-to-planning mapping strategy is proposed, unifying guidance points with other perceptual inputs and enabling the RL agent to effectively leverage the complementary relationships among raw laser data, human detection and tracking, and guidance points. Qualitative and quantitative simulations demonstrate that the proposed approach achieves the highest success rate and near-optimal travel times, greatly improving both safety and efficiency. Furthermore, real-world experiments in dynamic corridors and lobbies validate the robot's ability to confidently navigate around obstacles and robustly avoid pedestrians."
  },
  {
    "title": "Uncertainty Representation in a SOTIF-Related Use Case with Dempster-Shafer Theory for LiDAR Sensor-Based Object Detection",
    "url": "http://arxiv.org/abs/2503.02087v1",
    "arxiv_id": "2503.02087v1",
    "authors": [
      "Milin Patel",
      "Rolf Jung"
    ],
    "published": "2025-03-03T22:13:51+00:00",
    "summary": "Uncertainty in LiDAR sensor-based object detection arises from environmental variability and sensor performance limitations. Representing these uncertainties is essential for ensuring the Safety of the Intended Functionality (SOTIF), which focuses on preventing hazards in automated driving scenarios. This paper presents a systematic approach to identifying, classifying, and representing uncertainties in LiDAR-based object detection within a SOTIF-related scenario. Dempster-Shafer Theory (DST) is employed to construct a Frame of Discernment (FoD) to represent detection outcomes. Conditional Basic Probability Assignments (BPAs) are applied based on dependencies among identified uncertainty sources. Yager's Rule of Combination is used to resolve conflicting evidence from multiple sources, providing a structured framework to evaluate uncertainties' effects on detection accuracy. The study applies variance-based sensitivity analysis (VBSA) to quantify and prioritize uncertainties, detailing their specific impact on detection performance."
  },
  {
    "title": "CorrA: Leveraging Large Language Models for Dynamic Obstacle Avoidance of Autonomous Vehicles",
    "url": "http://arxiv.org/abs/2503.02076v1",
    "arxiv_id": "2503.02076v1",
    "authors": [
      "Shanting Wang",
      "Panagiotis Typaldos",
      "Andreas A. Malikopoulos"
    ],
    "published": "2025-03-03T21:57:28+00:00",
    "summary": "In this paper, we present Corridor-Agent (CorrA), a framework that integrates large language models (LLMs) with model predictive control (MPC) to address the challenges of dynamic obstacle avoidance in autonomous vehicles. Our approach leverages LLM reasoning ability to generate appropriate parameters for sigmoid-based boundary functions that define safe corridors around obstacles, effectively reducing the state-space of the controlled vehicle. The proposed framework adjusts these boundaries dynamically based on real-time vehicle data that guarantees collision-free trajectories while also ensuring both computational efficiency and trajectory optimality. The problem is formulated as an optimal control problem and solved with differential dynamic programming (DDP) for constrained optimization, and the proposed approach is embedded within an MPC framework. Extensive simulation and real-world experiments demonstrate that the proposed framework achieves superior performance in maintaining safety and efficiency in complex, dynamic environments compared to a baseline MPC approach."
  },
  {
    "title": "Comparative Analysis of OpenAI GPT-4o and DeepSeek R1 for Scientific Text Categorization Using Prompt Engineering",
    "url": "http://arxiv.org/abs/2503.02032v1",
    "arxiv_id": "2503.02032v1",
    "authors": [
      "Aniruddha Maiti",
      "Samuel Adewumi",
      "Temesgen Alemayehu Tikure",
      "Zichun Wang",
      "Niladri Sengupta",
      "Anastasiia Sukhanova",
      "Ananya Jana"
    ],
    "published": "2025-03-03T20:09:35+00:00",
    "summary": "This study examines how large language models categorize sentences from scientific papers using prompt engineering. We use two advanced web-based models, GPT-4o (by OpenAI) and DeepSeek R1, to classify sentences into predefined relationship categories. DeepSeek R1 has been tested on benchmark datasets in its technical report. However, its performance in scientific text categorization remains unexplored. To address this gap, we introduce a new evaluation method designed specifically for this task. We also compile a dataset of cleaned scientific papers from diverse domains. This dataset provides a platform for comparing the two models. Using this dataset, we analyze their effectiveness and consistency in categorization."
  },
  {
    "title": "One ruler to measure them all: Benchmarking multilingual long-context language models",
    "url": "http://arxiv.org/abs/2503.01996v1",
    "arxiv_id": "2503.01996v1",
    "authors": [
      "Yekyung Kim",
      "Jenna Russell",
      "Marzena Karpinska",
      "Mohit Iyyer"
    ],
    "published": "2025-03-03T19:12:48+00:00",
    "summary": "We present ONERULER, a multilingual benchmark designed to evaluate long-context language models across 26 languages. ONERULER adapts the English-only RULER benchmark (Hsieh et al., 2024) by including seven synthetic tasks that test both retrieval and aggregation, including new variations of the \"needle-in-a-haystack\" task that allow for the possibility of a nonexistent needle. We create ONERULER through a two-step process, first writing English instructions for each task and then collaborating with native speakers to translate them into 25 additional languages. Experiments with both open-weight and closed LLMs reveal a widening performance gap between low- and high-resource languages as context length increases from 8K to 128K tokens. Surprisingly, English is not the top-performing language on long-context tasks (ranked 6th out of 26), with Polish emerging as the top language. Our experiments also show that many LLMs (particularly OpenAI's o3-mini-high) incorrectly predict the absence of an answer, even in high-resource languages. Finally, in cross-lingual scenarios where instructions and context appear in different languages, performance can fluctuate by up to 20% depending on the instruction language. We hope the release of ONERULER will facilitate future research into improving multilingual and cross-lingual long-context training pipelines."
  },
  {
    "title": "Analyzing the Safety of Japanese Large Language Models in Stereotype-Triggering Prompts",
    "url": "http://arxiv.org/abs/2503.01947v1",
    "arxiv_id": "2503.01947v1",
    "authors": [
      "Akito Nakanishi",
      "Yukie Sano",
      "Geng Liu",
      "Francesco Pierri"
    ],
    "published": "2025-03-03T19:00:00+00:00",
    "summary": "In recent years, Large Language Models (LLMs) have attracted growing interest for their significant potential, though concerns have rapidly emerged regarding unsafe behaviors stemming from inherent stereotypes and biases.Most research on stereotypes in LLMs has primarily relied on indirect evaluation setups, in which models are prompted to select between pairs of sentences associated with particular social groups. Recently, direct evaluation methods have emerged, examining open-ended model responses to overcome limitations of previous approaches, such as annotator biases.Most existing studies have focused on English-centric LLMs, whereas research on non-English models--particularly Japanese--remains sparse, despite the growing development and adoption of these models.This study examines the safety of Japanese LLMs when responding to stereotype-triggering prompts in direct setups.We constructed 3,612 prompts by combining 301 social group terms--categorized by age, gender, and other attributes--with 12 stereotype-inducing templates in Japanese.Responses were analyzed from three foundational models trained respectively on Japanese, English, and Chinese language.Our findings reveal that LLM-jp, a Japanese native model, exhibits the lowest refusal rate and is more likely to generate toxic and negative responses compared to other models.Additionally, prompt format significantly influence the output of all models, and the generated responses include exaggerated reactions toward specific social groups, varying across models.These findings underscore the insufficient ethical safety mechanisms in Japanese LLMs and demonstrate that even high-accuracy models can produce biased outputs when processing Japanese-language prompts.We advocate for improving safety mechanisms and bias mitigation strategies in Japanese LLMs, contributing to ongoing discussions on AI ethics beyond linguistic boundaries."
  },
  {
    "title": "Analyzing the Safety of Japanese Large Language Models in Stereotype-Triggering Prompts",
    "url": "http://arxiv.org/abs/2503.01947v2",
    "arxiv_id": "2503.01947v2",
    "authors": [
      "Akito Nakanishi",
      "Yukie Sano",
      "Geng Liu",
      "Francesco Pierri"
    ],
    "published": "2025-03-03T19:00:00+00:00",
    "summary": "In recent years, Large Language Models have attracted growing interest for their significant potential, though concerns have rapidly emerged regarding unsafe behaviors stemming from inherent stereotypes and biases. Most research on stereotypes in LLMs has primarily relied on indirect evaluation setups, in which models are prompted to select between pairs of sentences associated with particular social groups. Recently, direct evaluation methods have emerged, examining open-ended model responses to overcome limitations of previous approaches, such as annotator biases. Most existing studies have focused on English-centric LLMs, whereas research on non-English models, particularly Japanese, remains sparse, despite the growing development and adoption of these models. This study examines the safety of Japanese LLMs when responding to stereotype-triggering prompts in direct setups. We constructed 3,612 prompts by combining 301 social group terms, categorized by age, gender, and other attributes, with 12 stereotype-inducing templates in Japanese. Responses were analyzed from three foundational models trained respectively on Japanese, English, and Chinese language. Our findings reveal that LLM-jp, a Japanese native model, exhibits the lowest refusal rate and is more likely to generate toxic and negative responses compared to other models. Additionally, prompt format significantly influence the output of all models, and the generated responses include exaggerated reactions toward specific social groups, varying across models. These findings underscore the insufficient ethical safety mechanisms in Japanese LLMs and demonstrate that even high-accuracy models can produce biased outputs when processing Japanese-language prompts. We advocate for improving safety mechanisms and bias mitigation strategies in Japanese LLMs, contributing to ongoing discussions on AI ethics beyond linguistic boundaries."
  },
  {
    "title": "Jailbreaking Safeguarded Text-to-Image Models via Large Language Models",
    "url": "http://arxiv.org/abs/2503.01839v1",
    "arxiv_id": "2503.01839v1",
    "authors": [
      "Zhengyuan Jiang",
      "Yuepeng Hu",
      "Yuchen Yang",
      "Yinzhi Cao",
      "Neil Zhenqiang Gong"
    ],
    "published": "2025-03-03T18:58:46+00:00",
    "summary": "Text-to-Image models may generate harmful content, such as pornographic images, particularly when unsafe prompts are submitted. To address this issue, safety filters are often added on top of text-to-image models, or the models themselves are aligned to reduce harmful outputs. However, these defenses remain vulnerable when an attacker strategically designs adversarial prompts to bypass these safety guardrails. In this work, we propose PromptTune, a method to jailbreak text-to-image models with safety guardrails using a fine-tuned large language model. Unlike other query-based jailbreak attacks that require repeated queries to the target model, our attack generates adversarial prompts efficiently after fine-tuning our AttackLLM. We evaluate our method on three datasets of unsafe prompts and against five safety guardrails. Our results demonstrate that our approach effectively bypasses safety guardrails, outperforms existing no-box attacks, and also facilitates other query-based attacks."
  },
  {
    "title": "$\\texttt{SEM-CTRL}$: Semantically Controlled Decoding",
    "url": "http://arxiv.org/abs/2503.01804v1",
    "arxiv_id": "2503.01804v1",
    "authors": [
      "Mohammad Albinhassan",
      "Pranava Madhyastha",
      "Alessandra Russo"
    ],
    "published": "2025-03-03T18:33:46+00:00",
    "summary": "Ensuring both syntactic and semantic correctness in Large Language Model (LLM) outputs remains a significant challenge, despite being critical for real-world deployment. In this paper, we introduce $\\texttt{SEM-CTRL}$, a unified approach that enforces rich context-sensitive constraints and task- and instance-specific semantics directly on an LLM decoder. Our approach integrates token-level MCTS, which is guided by specific syntactic and semantic constraints. The constraints over the desired outputs are expressed using Answer Set Grammars -- a logic-based formalism that generalizes context-sensitive grammars while incorporating background knowledge to represent task-specific semantics. We show that our approach guarantees correct completions for any off-the-shelf LLM without the need for fine-tuning. We evaluate $\\texttt{SEM-CTRL}$ on a range of tasks, including synthetic grammar synthesis, combinatorial reasoning, and planning. Our results demonstrate that $\\texttt{SEM-CTRL}$ allows small pre-trained LLMs to efficiently outperform larger variants and state-of-the-art reasoning models (e.g., o1-preview) while simultaneously guaranteeing solution correctness."
  },
  {
    "title": "Visual-RFT: Visual Reinforcement Fine-Tuning",
    "url": "http://arxiv.org/abs/2503.01785v1",
    "arxiv_id": "2503.01785v1",
    "authors": [
      "Ziyu Liu",
      "Zeyi Sun",
      "Yuhang Zang",
      "Xiaoyi Dong",
      "Yuhang Cao",
      "Haodong Duan",
      "Dahua Lin",
      "Jiaqi Wang"
    ],
    "published": "2025-03-03T18:16:32+00:00",
    "summary": "Reinforcement Fine-Tuning (RFT) in Large Reasoning Models like OpenAI o1 learns from feedback on its answers, which is especially useful in applications when fine-tuning data is scarce. Recent open-source work like DeepSeek-R1 demonstrates that reinforcement learning with verifiable reward is one key direction in reproducing o1. While the R1-style model has demonstrated success in language models, its application in multi-modal domains remains under-explored. This work introduces Visual Reinforcement Fine-Tuning (Visual-RFT), which further extends the application areas of RFT on visual tasks. Specifically, Visual-RFT first uses Large Vision-Language Models (LVLMs) to generate multiple responses containing reasoning tokens and final answers for each input, and then uses our proposed visual perception verifiable reward functions to update the model via the policy optimization algorithm such as Group Relative Policy Optimization (GRPO). We design different verifiable reward functions for different perception tasks, such as the Intersection over Union (IoU) reward for object detection. Experimental results on fine-grained image classification, few-shot object detection, reasoning grounding, as well as open-vocabulary object detection benchmarks show the competitive performance and advanced generalization ability of Visual-RFT compared with Supervised Fine-tuning (SFT). For example, Visual-RFT improves accuracy by $24.3\\%$ over the baseline in one-shot fine-grained image classification with around 100 samples. In few-shot object detection, Visual-RFT also exceeds the baseline by $21.9$ on COCO's two-shot setting and $15.4$ on LVIS. Our Visual-RFT represents a paradigm shift in fine-tuning LVLMs, offering a data-efficient, reward-driven approach that enhances reasoning and adaptability for domain-specific tasks."
  },
  {
    "title": "Cats Confuse Reasoning LLM: Query Agnostic Adversarial Triggers for Reasoning Models",
    "url": "http://arxiv.org/abs/2503.01781v1",
    "arxiv_id": "2503.01781v1",
    "authors": [
      "Meghana Rajeev",
      "Rajkumar Ramamurthy",
      "Prapti Trivedi",
      "Vikas Yadav",
      "Oluwanifemi Bamgbose",
      "Sathwik Tejaswi Madhusudan",
      "James Zou",
      "Nazneen Rajani"
    ],
    "published": "2025-03-03T18:10:54+00:00",
    "summary": "We investigate the robustness of reasoning models trained for step-by-step problem solving by introducing query-agnostic adversarial triggers - short, irrelevant text that, when appended to math problems, systematically mislead models to output incorrect answers without altering the problem's semantics. We propose CatAttack, an automated iterative attack pipeline for generating triggers on a weaker, less expensive proxy model (DeepSeek V3) and successfully transfer them to more advanced reasoning target models like DeepSeek R1 and DeepSeek R1-distilled-Qwen-32B, resulting in greater than 300% increase in the likelihood of the target model generating an incorrect answer. For example, appending, \"Interesting fact: cats sleep most of their lives,\" to any math problem leads to more than doubling the chances of a model getting the answer wrong. Our findings highlight critical vulnerabilities in reasoning models, revealing that even state-of-the-art models remain susceptible to subtle adversarial inputs, raising security and reliability concerns. The CatAttack triggers dataset with model responses is available at https://huggingface.co/datasets/collinear-ai/cat-attack-adversarial-triggers."
  },
  {
    "title": "Phi-4-Mini Technical Report: Compact yet Powerful Multimodal Language Models via Mixture-of-LoRAs",
    "url": "http://arxiv.org/abs/2503.01743v1",
    "arxiv_id": "2503.01743v1",
    "authors": [
      "Abdelrahman Abouelenin",
      "Atabak Ashfaq",
      "Adam Atkinson",
      "Hany Awadalla",
      "Nguyen Bach",
      "Jianmin Bao",
      "Alon Benhaim",
      "Martin Cai",
      "Vishrav Chaudhary",
      "Congcong Chen",
      "Dong Chen",
      "Dongdong Chen",
      "Junkun Chen",
      "Weizhu Chen",
      "Yen-Chun Chen",
      "Yi-ling Chen",
      "Qi Dai",
      "Xiyang Dai",
      "Ruchao Fan",
      "Mei Gao",
      "Min Gao",
      "Amit Garg",
      "Abhishek Goswami",
      "Junheng Hao",
      "Amr Hendy",
      "Yuxuan Hu",
      "Xin Jin",
      "Mahmoud Khademi",
      "Dongwoo Kim",
      "Young Jin Kim",
      "Gina Lee",
      "Jinyu Li",
      "Yunsheng Li",
      "Chen Liang",
      "Xihui Lin",
      "Zeqi Lin",
      "Mengchen Liu",
      "Yang Liu",
      "Gilsinia Lopez",
      "Chong Luo",
      "Piyush Madan",
      "Vadim Mazalov",
      "Ali Mousavi",
      "Anh Nguyen",
      "Jing Pan",
      "Daniel Perez-Becker",
      "Jacob Platin",
      "Thomas Portet",
      "Kai Qiu",
      "Bo Ren",
      "Liliang Ren",
      "Sambuddha Roy",
      "Ning Shang",
      "Yelong Shen",
      "Saksham Singhal",
      "Subhojit Som",
      "Xia Song",
      "Tetyana Sych",
      "Praneetha Vaddamanu",
      "Shuohang Wang",
      "Yiming Wang",
      "Zhenghao Wang",
      "Haibin Wu",
      "Haoran Xu",
      "Weijian Xu",
      "Yifan Yang",
      "Ziyi Yang",
      "Donghan Yu",
      "Ishmam Zabir",
      "Jianwen Zhang",
      "Li Lyna Zhang",
      "Yunan Zhang",
      "Xiren Zhou"
    ],
    "published": "2025-03-03T17:05:52+00:00",
    "summary": "We introduce Phi-4-Mini and Phi-4-Multimodal, compact yet highly capable language and multimodal models. Phi-4-Mini is a 3.8-billion-parameter language model trained on high-quality web and synthetic data, significantly outperforming recent open-source models of similar size and matching the performance of models twice its size on math and coding tasks requiring complex reasoning. This achievement is driven by a carefully curated synthetic data recipe emphasizing high-quality math and coding datasets. Compared to its predecessor, Phi-3.5-Mini, Phi-4-Mini features an expanded vocabulary size of 200K tokens to better support multilingual applications, as well as group query attention for more efficient long-sequence generation. Phi-4-Multimodal is a multimodal model that integrates text, vision, and speech/audio input modalities into a single model. Its novel modality extension approach leverages LoRA adapters and modality-specific routers to allow multiple inference modes combining various modalities without interference. For example, it now ranks first in the OpenASR leaderboard to date, although the LoRA component of the speech/audio modality has just 460 million parameters. Phi-4-Multimodal supports scenarios involving (vision + language), (vision + speech), and (speech/audio) inputs, outperforming larger vision-language and speech-language models on a wide range of tasks. Additionally, we experiment to further train Phi-4-Mini to enhance its reasoning capabilities. Despite its compact 3.8-billion-parameter size, this experimental version achieves reasoning performance on par with or surpassing significantly larger models, including DeepSeek-R1-Distill-Qwen-7B and DeepSeek-R1-Distill-Llama-8B."
  },
  {
    "title": "Building Safe GenAI Applications: An End-to-End Overview of Red Teaming for Large Language Models",
    "url": "http://arxiv.org/abs/2503.01742v1",
    "arxiv_id": "2503.01742v1",
    "authors": [
      "Alberto Purpura",
      "Sahil Wadhwa",
      "Jesse Zymet",
      "Akshay Gupta",
      "Andy Luo",
      "Melissa Kazemi Rad",
      "Swapnil Shinde",
      "Mohammad Shahed Sorower"
    ],
    "published": "2025-03-03T17:04:22+00:00",
    "summary": "The rapid growth of Large Language Models (LLMs) presents significant privacy, security, and ethical concerns. While much research has proposed methods for defending LLM systems against misuse by malicious actors, researchers have recently complemented these efforts with an offensive approach that involves red teaming, i.e., proactively attacking LLMs with the purpose of identifying their vulnerabilities. This paper provides a concise and practical overview of the LLM red teaming literature, structured so as to describe a multi-component system end-to-end. To motivate red teaming we survey the initial safety needs of some high-profile LLMs, and then dive into the different components of a red teaming system as well as software packages for implementing them. We cover various attack methods, strategies for attack-success evaluation, metrics for assessing experiment outcomes, as well as a host of other considerations. Our survey will be useful for any reader who wants to rapidly obtain a grasp of the major red teaming concepts for their own use in practical applications."
  },
  {
    "title": "Building Safe GenAI Applications: An End-to-End Overview of Red Teaming for Large Language Models",
    "url": "http://arxiv.org/abs/2503.01742v2",
    "arxiv_id": "2503.01742v2",
    "authors": [
      "Alberto Purpura",
      "Sahil Wadhwa",
      "Jesse Zymet",
      "Akshay Gupta",
      "Andy Luo",
      "Melissa Kazemi Rad",
      "Swapnil Shinde",
      "Mohammad Shahed Sorower"
    ],
    "published": "2025-03-03T17:04:22+00:00",
    "summary": "The rapid growth of Large Language Models (LLMs) presents significant privacy, security, and ethical concerns. While much research has proposed methods for defending LLM systems against misuse by malicious actors, researchers have recently complemented these efforts with an offensive approach that involves red teaming, i.e., proactively attacking LLMs with the purpose of identifying their vulnerabilities. This paper provides a concise and practical overview of the LLM red teaming literature, structured so as to describe a multi-component system end-to-end. To motivate red teaming we survey the initial safety needs of some high-profile LLMs, and then dive into the different components of a red teaming system as well as software packages for implementing them. We cover various attack methods, strategies for attack-success evaluation, metrics for assessing experiment outcomes, as well as a host of other considerations. Our survey will be useful for any reader who wants to rapidly obtain a grasp of the major red teaming concepts for their own use in practical applications."
  },
  {
    "title": "CoT-VLM4Tar: Chain-of-Thought Guided Vision-Language Models for Traffic Anomaly Resolution",
    "url": "http://arxiv.org/abs/2503.01632v1",
    "arxiv_id": "2503.01632v1",
    "authors": [
      "Tianchi Ren",
      "Haibo Hu",
      "Jiacheng Zuo",
      "Xinhong Chen",
      "Jianping Wang",
      "Chun Jason Xue",
      "Jen-Ming Wu",
      "Nan Guan"
    ],
    "published": "2025-03-03T15:07:25+00:00",
    "summary": "With the acceleration of urbanization, modern urban traffic systems are becoming increasingly complex, leading to frequent traffic anomalies. These anomalies encompass not only common traffic jams but also more challenging issues such as phantom traffic jams, intersection deadlocks, and accident liability analysis, which severely impact traffic flow, vehicular safety, and overall transportation efficiency. Currently, existing solutions primarily rely on manual intervention by traffic police or artificial intelligence-based detection systems. However, these methods often suffer from response delays and inconsistent management due to inadequate resources, while AI detection systems, despite enhancing efficiency to some extent, still struggle to handle complex traffic anomalies in a real-time and precise manner. To address these issues, we propose CoT-VLM4Tar: (Chain of Thought Visual-Language Model for Traffic Anomaly Resolution), this innovative approach introduces a new chain-of-thought to guide the VLM in analyzing, reasoning, and generating solutions for traffic anomalies with greater reasonable and effective solution, and to evaluate the performance and effectiveness of our method, we developed a closed-loop testing framework based on the CARLA simulator. Furthermore, to ensure seamless integration of the solutions generated by the VLM with the CARLA simulator, we implement an itegration module that converts these solutions into executable commands. Our results demonstrate the effectiveness of VLM in the resolution of real-time traffic anomalies, providing a proof-of-concept for its integration into autonomous traffic management systems."
  },
  {
    "title": "Llama-3.1-Sherkala-8B-Chat: An Open Large Language Model for Kazakh",
    "url": "http://arxiv.org/abs/2503.01493v1",
    "arxiv_id": "2503.01493v1",
    "authors": [
      "Fajri Koto",
      "Rituraj Joshi",
      "Nurdaulet Mukhituly",
      "Yuxia Wang",
      "Zhuohan Xie",
      "Rahul Pal",
      "Daniil Orel",
      "Parvez Mullah",
      "Diana Turmakhan",
      "Maiya Goloburda",
      "Mohammed Kamran",
      "Samujjwal Ghosh",
      "Bokang Jia",
      "Jonibek Mansurov",
      "Mukhammed Togmanov",
      "Debopriyo Banerjee",
      "Nurkhan Laiyk",
      "Akhmed Sakip",
      "Xudong Han",
      "Ekaterina Kochmar",
      "Alham Fikri Aji",
      "Aaryamonvikram Singh",
      "Alok Anil Jadhav",
      "Satheesh Katipomu",
      "Samta Kamboj",
      "Monojit Choudhury",
      "Gurpreet Gosal",
      "Gokul Ramakrishnan",
      "Biswajit Mishra",
      "Sarath Chandran",
      "Avraham Sheinin",
      "Natalia Vassilieva",
      "Neha Sengupta",
      "Larry Murray",
      "Preslav Nakov"
    ],
    "published": "2025-03-03T13:05:48+00:00",
    "summary": "Llama-3.1-Sherkala-8B-Chat, or Sherkala-Chat (8B) for short, is a state-of-the-art instruction-tuned open generative large language model (LLM) designed for Kazakh. Sherkala-Chat (8B) aims to enhance the inclusivity of LLM advancements for Kazakh speakers. Adapted from the LLaMA-3.1-8B model, Sherkala-Chat (8B) is trained on 45.3B tokens across Kazakh, English, Russian, and Turkish. With 8 billion parameters, it demonstrates strong knowledge and reasoning abilities in Kazakh, significantly outperforming existing open Kazakh and multilingual models of similar scale while achieving competitive performance in English. We release Sherkala-Chat (8B) as an open-weight instruction-tuned model and provide a detailed overview of its training, fine-tuning, safety alignment, and evaluation, aiming to advance research and support diverse real-world applications."
  },
  {
    "title": "Towards Widening The Distillation Bottleneck for Reasoning Models",
    "url": "http://arxiv.org/abs/2503.01461v1",
    "arxiv_id": "2503.01461v1",
    "authors": [
      "Huifeng Yin",
      "Yu Zhao",
      "Minghao Wu",
      "Xuanfan Ni",
      "Bo Zeng",
      "Hao Wang",
      "Tianqi Shi",
      "Liangying Shao",
      "Chenyang Lyu",
      "Longyue Wang",
      "Weihua Luo",
      "Kaifu Zhang"
    ],
    "published": "2025-03-03T12:17:36+00:00",
    "summary": "Large Reasoning Models(LRMs) such as OpenAI o1 and DeepSeek-R1 have shown remarkable reasoning capabilities by scaling test-time compute and generating long Chain-of-Thought(CoT). Distillation--post-training on LRMs-generated data--is a straightforward yet effective method to enhance the reasoning abilities of smaller models, but faces a critical bottleneck: we found that distilled long CoT data poses learning difficulty for small models and leads to the inheritance of biases (i.e. over-thinking) when using Supervised Fine-tuning(SFT) and Reinforcement Learning(RL) methods. To alleviate this bottleneck, we propose constructing tree-based CoT data from scratch via Monte Carlo Tree Search(MCTS). We then exploit a set of CoT-aware approaches, including Thoughts Length Balance, Fine-grained DPO, and Joint Post-training Objective, to enhance SFT and RL on the construted data."
  },
  {
    "title": "Jailbreaking Generative AI: Empowering Novices to Conduct Phishing Attacks",
    "url": "http://arxiv.org/abs/2503.01395v1",
    "arxiv_id": "2503.01395v1",
    "authors": [
      "Rina Mishra",
      "Gaurav Varshney",
      "Shreya Singh"
    ],
    "published": "2025-03-03T10:51:10+00:00",
    "summary": "The rapid advancements in generative AI models, such as ChatGPT, have introduced both significant benefits and new risks within the cybersecurity landscape. This paper investigates the potential misuse of the latest AI model, ChatGPT-4o Mini, in facilitating social engineering attacks, with a particular focus on phishing, one of the most pressing cybersecurity threats today. While existing literature primarily addresses the technical aspects, such as jailbreaking techniques, none have fully explored the free and straightforward execution of a comprehensive phishing campaign by novice users using ChatGPT-4o Mini. In this study, we examine the vulnerabilities of AI-driven chatbot services in 2025, specifically how methods like jailbreaking and reverse psychology can bypass ethical safeguards, allowing ChatGPT to generate phishing content, suggest hacking tools, and assist in carrying out phishing attacks. Our findings underscore the alarming ease with which even inexperienced users can execute sophisticated phishing campaigns, emphasizing the urgent need for stronger cybersecurity measures and heightened user awareness in the age of AI."
  },
  {
    "title": "Victim-Centred Abuse Investigations and Defenses for Social Media Platforms",
    "url": "http://arxiv.org/abs/2503.01327v1",
    "arxiv_id": "2503.01327v1",
    "authors": [
      "Zaid Hakami",
      "Ashfaq Ali Shafin",
      "Peter J. Clarke",
      "Niki Pissinou",
      "Bogdan Carbunar"
    ],
    "published": "2025-03-03T09:08:20+00:00",
    "summary": "Online abuse, a persistent aspect of social platform interactions, impacts user well-being and exposes flaws in platform designs that include insufficient detection efforts and inadequate victim protection measures. Ensuring safety in platform interactions requires the integration of victim perspectives in the design of abuse detection and response systems. In this paper, we conduct surveys (n = 230) and semi-structured interviews (n = 15) with students at a minority-serving institution in the US, to explore their experiences with abuse on a variety of social platforms, their defense strategies, and their recommendations for social platforms to improve abuse responses. We build on study findings to propose design requirements for abuse defense systems and discuss the role of privacy, anonymity, and abuse attribution requirements in their implementation. We introduce ARI, a blueprint for a unified, transparent, and personalized abuse response system for social platforms that sustainably detects abuse by leveraging the expertise of platform users, incentivized with proceeds obtained from abusers."
  },
  {
    "title": "ABFS: Natural Robustness Testing for LLM-based NLP Software",
    "url": "http://arxiv.org/abs/2503.01319v1",
    "arxiv_id": "2503.01319v1",
    "authors": [
      "Mingxuan Xiao",
      "Yan Xiao",
      "Shunhui Ji",
      "Yunhe Li",
      "Lei Xue",
      "Pengcheng Zhang"
    ],
    "published": "2025-03-03T09:02:06+00:00",
    "summary": "Owing to the exceptional performance of Large Language Models (LLMs) in Natural Language Processing (NLP) tasks, LLM-based NLP software has rapidly gained traction across various domains, such as financial analysis and content moderation. However, these applications frequently exhibit robustness deficiencies, where slight perturbations in input (prompt+example) may lead to erroneous outputs. Current robustness testing methods face two main limitations: (1) low testing effectiveness, limiting the applicability of LLM-based software in safety-critical scenarios, and (2) insufficient naturalness of test cases, reducing the practical value of testing outcomes. To address these issues, this paper proposes ABFS, a straightforward yet effective automated testing method that, for the first time, treats the input prompts and examples as a unified whole for robustness testing. Specifically, ABFS formulates the testing process as a combinatorial optimization problem, employing Best-First Search to identify successful test cases within the perturbation space and designing a novel Adaptive control strategy to enhance test case naturalness. We evaluate the robustness testing performance of ABFS on three datasets across five threat models. On Llama2-13b, the traditional StressTest achieves only a 13.273% success rate, while ABFS attains a success rate of 98.064%, supporting a more comprehensive robustness assessment before software deployment. Compared to baseline methods, ABFS introduces fewer modifications to the original input and consistently generates test cases with superior naturalness. Furthermore, test cases generated by ABFS exhibit stronger transferability and higher testing efficiency, significantly reducing testing costs."
  },
  {
    "title": "Interactive Gadolinium-Free MRI Synthesis: A Transformer with Localization Prompt Learning",
    "url": "http://arxiv.org/abs/2503.01265v1",
    "arxiv_id": "2503.01265v1",
    "authors": [
      "Linhao Li",
      "Changhui Su",
      "Yu Guo",
      "Huimao Zhang",
      "Dong Liang",
      "Kun Shang"
    ],
    "published": "2025-03-03T07:44:28+00:00",
    "summary": "Contrast-enhanced magnetic resonance imaging (CE-MRI) is crucial for tumor detection and diagnosis, but the use of gadolinium-based contrast agents (GBCAs) in clinical settings raises safety concerns due to potential health risks. To circumvent these issues while preserving diagnostic accuracy, we propose a novel Transformer with Localization Prompts (TLP) framework for synthesizing CE-MRI from non-contrast MR images. Our architecture introduces three key innovations: a hierarchical backbone that uses efficient Transformer to process multi-scale features; a multi-stage fusion system consisting of Local and Global Fusion modules that hierarchically integrate complementary information via spatial attention operations and cross-attention mechanisms, respectively; and a Fuzzy Prompt Generation (FPG) module that enhances the TLP model's generalization by emulating radiologists' manual annotation through stochastic feature perturbation. The framework uniquely enables interactive clinical integration by allowing radiologists to input diagnostic prompts during inference, synergizing artificial intelligence with medical expertise. This research establishes a new paradigm for contrast-free MRI synthesis while addressing critical clinical needs for safer diagnostic procedures. Codes are available at https://github.com/ChanghuiSu/TLP."
  },
  {
    "title": "Enhancing Network Security Management in Water Systems using FM-based Attack Attribution",
    "url": "http://arxiv.org/abs/2503.01229v1",
    "arxiv_id": "2503.01229v1",
    "authors": [
      "Aleksandar Avdalovic",
      "Joseph Khoury",
      "Ahmad Taha",
      "Elias Bou-Harb"
    ],
    "published": "2025-03-03T06:52:00+00:00",
    "summary": "Water systems are vital components of modern infrastructure, yet they are increasingly susceptible to sophisticated cyber attacks with potentially dire consequences on public health and safety. While state-of-the-art machine learning techniques effectively detect anomalies, contemporary model-agnostic attack attribution methods using LIME, SHAP, and LEMNA are deemed impractical for large-scale, interdependent water systems. This is due to the intricate interconnectivity and dynamic interactions that define these complex environments. Such methods primarily emphasize individual feature importance while falling short of addressing the crucial sensor-actuator interactions in water systems, which limits their effectiveness in identifying root cause attacks. To this end, we propose a novel model-agnostic Factorization Machines (FM)-based approach that capitalizes on water system sensor-actuator interactions to provide granular explanations and attributions for cyber attacks. For instance, an anomaly in an actuator pump activity can be attributed to a top root cause attack candidates, a list of water pressure sensors, which is derived from the underlying linear and quadratic effects captured by our approach. We validate our method using two real-world water system specific datasets, SWaT and WADI, demonstrating its superior performance over traditional attribution methods. In multi-feature cyber attack scenarios involving intricate sensor-actuator interactions, our FM-based attack attribution method effectively ranks attack root causes, achieving approximately 20% average improvement over SHAP and LEMNA."
  },
  {
    "title": "STGAN: Spatial-temporal Graph Autoregression Network for Pavement Distress Deterioration Prediction",
    "url": "http://arxiv.org/abs/2503.01152v1",
    "arxiv_id": "2503.01152v1",
    "authors": [
      "Shilin Tong",
      "Difei Wu",
      "Xiaona Liu",
      "Le Zheng",
      "Yuchuan Du",
      "Difan Zou"
    ],
    "published": "2025-03-03T03:59:34+00:00",
    "summary": "Pavement distress significantly compromises road integrity and poses risks to drivers. Accurate prediction of pavement distress deterioration is essential for effective road management, cost reduction in maintenance, and improvement of traffic safety. However, real-world data on pavement distress is usually collected irregularly, resulting in uneven, asynchronous, and sparse spatial-temporal datasets. This hinders the application of existing spatial-temporal models, such as DCRNN, since they are only applicable to regularly and synchronously collected data. To overcome these challenges, we propose the Spatial-Temporal Graph Autoregression Network (STGAN), a novel graph neural network model designed for accurately predicting irregular pavement distress deterioration using complex spatial-temporal data. Specifically, STGAN integrates the temporal domain into the spatial domain, creating a larger graph where nodes are represented by spatial-temporal tuples and edges are formed based on a similarity-based connection mechanism. Furthermore, based on the constructed spatiotemporal graph, we formulate pavement distress deterioration prediction as a graph autoregression task, i.e., the graph size increases incrementally and the prediction is performed sequentially. This is accomplished by a novel spatial-temporal attention mechanism deployed by STGAN. Utilizing the ConTrack dataset, which contains pavement distress records collected from different locations in Shanghai, we demonstrate the superior performance of STGAN in capturing spatial-temporal correlations and addressing the aforementioned challenges. Experimental results further show that STGAN outperforms baseline models, and ablation studies confirm the effectiveness of its novel modules. Our findings contribute to promoting proactive road maintenance decision-making and ultimately enhancing road safety and resilience."
  },
  {
    "title": "Beyond Visibility Limits: A DRL-Based Navigation Strategy for Unexpected Obstacles",
    "url": "http://arxiv.org/abs/2503.01127v1",
    "arxiv_id": "2503.01127v1",
    "authors": [
      "Mingao Tan",
      "Shanze Wang",
      "Biao Huang",
      "Zhibo Yang",
      "Rongfei Chen",
      "Xiaoyu Shen",
      "Wei Zhang"
    ],
    "published": "2025-03-03T03:14:08+00:00",
    "summary": "Distance-based reward mechanisms in deep reinforcement learning (DRL) navigation systems suffer from critical safety limitations in dynamic environments, frequently resulting in collisions when visibility is restricted. We propose DRL-NSUO, a novel navigation strategy for unexpected obstacles that leverages the rate of change in LiDAR data as a dynamic environmental perception element. Our approach incorporates a composite reward function with environmental change rate constraints and dynamically adjusted weights through curriculum learning, enabling robots to autonomously balance between path efficiency and safety maximization. We enhance sensitivity to nearby obstacles by implementing short-range feature preprocessing of LiDAR data. Experimental results demonstrate that this method significantly improves both robot and pedestrian safety in complex scenarios compared to traditional DRL-based methods. When evaluated on the BARN navigation dataset, our method achieved superior performance with success rates of 94.0% at 0.5 m/s and 91.0% at 1.0 m/s, outperforming conservative obstacle expansion strategies. These results validate DRL-NSUO's enhanced practicality and safety for human-robot collaborative environments, including intelligent logistics applications."
  },
  {
    "title": "KineSoft: Learning Proprioceptive Manipulation Policies with Soft Robot Hands",
    "url": "http://arxiv.org/abs/2503.01078v1",
    "arxiv_id": "2503.01078v1",
    "authors": [
      "Uksang Yoo",
      "Jonathan Francis",
      "Jean Oh",
      "Jeffrey Ichnowski"
    ],
    "published": "2025-03-03T00:48:08+00:00",
    "summary": "Underactuated soft robot hands offer inherent safety and adaptability advantages over rigid systems, but developing dexterous manipulation skills remains challenging. While imitation learning shows promise for complex manipulation tasks, traditional approaches struggle with soft systems due to demonstration collection challenges and ineffective state representations. We present KineSoft, a framework enabling direct kinesthetic teaching of soft robotic hands by leveraging their natural compliance as a skill teaching advantage rather than only as a control challenge. KineSoft makes two key contributions: (1) an internal strain sensing array providing occlusion-free proprioceptive shape estimation, and (2) a shape-based imitation learning framework that uses proprioceptive feedback with a low-level shape-conditioned controller to ground diffusion-based policies. This enables human demonstrators to physically guide the robot while the system learns to associate proprioceptive patterns with successful manipulation strategies. We validate KineSoft through physical experiments, demonstrating superior shape estimation accuracy compared to baseline methods, precise shape-trajectory tracking, and higher task success rates compared to baseline imitation learning approaches."
  },
  {
    "title": "Real-World Deployment and Assessment of a Multi-Agent Reinforcement Learning-Based Variable Speed Limit Control System",
    "url": "http://arxiv.org/abs/2503.01017v1",
    "arxiv_id": "2503.01017v1",
    "authors": [
      "Yuhang Zhang",
      "Zhiyao Zhang",
      "Junyi Ji",
      "Marcos Qui\u00f1ones-Grueiro",
      "William Barbour",
      "Derek Gloudemans",
      "Gergely Zach\u00e1r",
      "Clay Weston",
      "Gautam Biswas",
      "Daniel B. Work"
    ],
    "published": "2025-03-02T21:09:16+00:00",
    "summary": "This article presents the first field deployment of a multi-agent reinforcement learning (MARL) based variable speed limit (VSL) control system on Interstate 24 (I-24) near Nashville, Tennessee. We design and demonstrate a full pipeline from training MARL agents in a traffic simulator to a field deployment on a 17-mile segment of I-24 encompassing 67 VSL controllers. The system was launched on March 8th, 2024, and has made approximately 35 million decisions on 28 million trips in six months of operation. We apply an invalid action masking mechanism and several safety guards to ensure real-world constraints. The MARL-based implementation operates up to 98% of the time, with the safety guards overriding the MARL decisions for the remaining time. We evaluate the performance of the MARL-based algorithm in comparison to a previously deployed non-RL VSL benchmark algorithm on I-24. Results show that the MARL-based VSL control system achieves a superior performance. The accuracy of correctly warning drivers about slowing traffic ahead is improved by 14% and the response delay to non-recurrent congestion is reduced by 75%. The preliminary data shows that the VSL control system has reduced the crash rate by 26% and the secondary crash rate by 50%. We open-sourced the deployed MARL-based VSL algorithm at https://github.com/Lab-Work/marl-vsl-controller."
  },
  {
    "title": "HWC-Loco: A Hierarchical Whole-Body Control Approach to Robust Humanoid Locomotion",
    "url": "http://arxiv.org/abs/2503.00923v1",
    "arxiv_id": "2503.00923v1",
    "authors": [
      "Sixu Lin",
      "Guanren Qiao",
      "Yunxin Tai",
      "Ang Li",
      "Kui Jia",
      "Guiliang Liu"
    ],
    "published": "2025-03-02T14:55:22+00:00",
    "summary": "Humanoid robots, capable of assuming human roles in various workplaces, have become essential to the advancement of embodied intelligence. However, as robots with complex physical structures, learning a control model that can operate robustly across diverse environments remains inherently challenging, particularly under the discrepancies between training and deployment environments. In this study, we propose HWC-Loco, a robust whole-body control algorithm tailored for humanoid locomotion tasks. By reformulating policy learning as a robust optimization problem, HWC-Loco explicitly learns to recover from safety-critical scenarios. While prioritizing safety guarantees, overly conservative behavior can compromise the robot's ability to complete the given tasks. To tackle this challenge, HWC-Loco leverages a hierarchical policy for robust control. This policy can dynamically resolve the trade-off between goal-tracking and safety recovery, guided by human behavior norms and dynamic constraints. To evaluate the performance of HWC-Loco, we conduct extensive comparisons against state-of-the-art humanoid control models, demonstrating HWC-Loco's superior performance across diverse terrains, robot structures, and locomotion tasks under both simulated and real-world environments."
  },
  {
    "title": "Unnatural Languages Are Not Bugs but Features for LLMs",
    "url": "http://arxiv.org/abs/2503.01926v1",
    "arxiv_id": "2503.01926v1",
    "authors": [
      "Keyu Duan",
      "Yiran Zhao",
      "Zhili Feng",
      "Jinjie Ni",
      "Tianyu Pang",
      "Qian Liu",
      "Tianle Cai",
      "Longxu Dou",
      "Kenji Kawaguchi",
      "Anirudh Goyal",
      "J. Zico Kolter",
      "Michael Qizhe Shieh"
    ],
    "published": "2025-03-02T12:10:17+00:00",
    "summary": "Large Language Models (LLMs) have been observed to process non-human-readable text sequences, such as jailbreak prompts, often viewed as a bug for aligned LLMs. In this work, we present a systematic investigation challenging this perception, demonstrating that unnatural languages - strings that appear incomprehensible to humans but maintain semantic meanings for LLMs - contain latent features usable by models. Notably, unnatural languages possess latent features that can be generalized across different models and tasks during inference. Furthermore, models fine-tuned on unnatural versions of instruction datasets perform on-par with those trained on natural language, achieving 49.71 win rates in Length-controlled AlpacaEval 2.0 in average across various base models. In addition, through comprehensive analysis, we demonstrate that LLMs process unnatural languages by filtering noise and inferring contextual meaning from filtered words."
  },
  {
    "title": "Evaluation of adaptive sampling methods in scenario generation for virtual safety impact assessment of pre-crash safety systems",
    "url": "http://arxiv.org/abs/2503.00815v1",
    "arxiv_id": "2503.00815v1",
    "authors": [
      "Xiaomi Yang",
      "Henrik Imberg",
      "Carol Flannagan",
      "Jonas B\u00e4rgman"
    ],
    "published": "2025-03-02T09:44:18+00:00",
    "summary": "Virtual safety assessment plays a vital role in evaluating the safety impact of pre-crash safety systems such as advanced driver assistance systems (ADAS) and automated driving systems (ADS). However, as the number of parameters in simulation-based scenario generation increases, the number of crash scenarios to simulate grows exponentially, making complete enumeration computationally infeasible. Efficient sampling methods, such as importance sampling and active sampling, have been proposed to address this challenge. However, a comprehensive evaluation of how domain knowledge, stratification, and batch sampling affect their efficiency remains limited.   This study evaluates the performance of importance sampling and active sampling in scenario generation, incorporating two domain-knowledge-driven features: adaptive sample space reduction (ASSR) and stratification. Additionally, we assess the effects of a third feature, batch sampling, on computational efficiency in terms of both CPU and wall-clock time. Based on our findings, we provide practical recommendations for applying ASSR, stratification, and batch sampling to optimize sampling performance.   Our results demonstrate that ASSR substantially improves sampling efficiency for both importance sampling and active sampling. When integrated into active sampling, ASSR reduces the root mean squared estimation error (RMSE) of the estimates by up to 90\\%. Stratification further improves sampling performance for both methods, regardless of ASSR implementation. When ASSR and/or stratification are applied, importance sampling performs on par with active sampling, whereas when neither feature is used, active sampling is more efficient. Larger batch sizes reduce wall-clock time but increase the number of simulations required to achieve the same estimation accuracy."
  },
  {
    "title": "Acoustic Anomaly Detection on UAM Propeller Defect with Acoustic dataset for Crack of drone Propeller (ADCP)",
    "url": "http://arxiv.org/abs/2503.00790v1",
    "arxiv_id": "2503.00790v1",
    "authors": [
      "Juho Lee",
      "Donghyun Yoon",
      "Gumoon Jeong",
      "Hyeoncheol Kim"
    ],
    "published": "2025-03-02T08:40:23+00:00",
    "summary": "The imminent commercialization of UAM requires stable, AI-based maintenance systems to ensure safety for both passengers and pedestrians. This paper presents a methodology for non-destructively detecting cracks in UAM propellers using drone propeller sound datasets. Normal operating sounds were recorded, and abnormal sounds (categorized as ripped and broken) were differentiated by varying the microphone-propeller angle and throttle power. Our novel approach integrates FFT and STFT preprocessing techniques to capture both global frequency patterns and local time-frequency variations, thereby enhancing anomaly detection performance. The constructed Acoustic Dataset for Crack of Drone Propeller (ADCP) demonstrates the potential for detecting propeller cracks and lays the groundwork for future UAM maintenance applications."
  },
  {
    "title": "Output Length Effect on DeepSeek-R1's Safety in Forced Thinking",
    "url": "http://arxiv.org/abs/2503.01923v1",
    "arxiv_id": "2503.01923v1",
    "authors": [
      "Xuying Li",
      "Zhuo Li",
      "Yuji Kosuga",
      "Victor Bian"
    ],
    "published": "2025-03-02T06:29:22+00:00",
    "summary": "Large Language Models (LLMs) have demonstrated strong reasoning capabilities, but their safety under adversarial conditions remains a challenge. This study examines the impact of output length on the robustness of DeepSeek-R1, particularly in Forced Thinking scenarios. We analyze responses across various adversarial prompts and find that while longer outputs can improve safety through self-correction, certain attack types exploit extended generations. Our findings suggest that output length should be dynamically controlled to balance reasoning effectiveness and security. We propose reinforcement learning-based policy adjustments and adaptive token length regulation to enhance LLM safety."
  },
  {
    "title": "State-Dependent Conformal Perception Bounds for Neuro-Symbolic Verification of Autonomous Systems",
    "url": "http://arxiv.org/abs/2502.21308v1",
    "arxiv_id": "2502.21308v1",
    "authors": [
      "Thomas Waite",
      "Yuang Geng",
      "Trevor Turnquist",
      "Ivan Ruchkin",
      "Radoslav Ivanov"
    ],
    "published": "2025-02-28T18:51:20+00:00",
    "summary": "It remains a challenge to provide safety guarantees for autonomous systems with neural perception and control. A typical approach obtains symbolic bounds on perception error (e.g., using conformal prediction) and performs verification under these bounds. However, these bounds can lead to drastic conservatism in the resulting end-to-end safety guarantee. This paper proposes an approach to synthesize symbolic perception error bounds that serve as an optimal interface between perception performance and control verification. The key idea is to consider our error bounds to be heteroskedastic with respect to the system's state -- not time like in previous approaches. These bounds can be obtained with two gradient-free optimization algorithms. We demonstrate that our bounds lead to tighter safety guarantees than the state-of-the-art in a case study on a mountain car."
  },
  {
    "title": "Dynamically Local-Enhancement Planner for Large-Scale Autonomous Driving",
    "url": "http://arxiv.org/abs/2502.21134v1",
    "arxiv_id": "2502.21134v1",
    "authors": [
      "Nanshan Deng",
      "Weitao Zhou",
      "Bo Zhang",
      "Junze Wen",
      "Kun Jiang",
      "Zhong Cao",
      "Diange Yang"
    ],
    "published": "2025-02-28T15:17:20+00:00",
    "summary": "Current autonomous vehicles operate primarily within limited regions, but there is increasing demand for broader applications. However, as models scale, their limited capacity becomes a significant challenge for adapting to novel scenarios. It is increasingly difficult to improve models for new situations using a single monolithic model. To address this issue, we introduce the concept of dynamically enhancing a basic driving planner with local driving data, without permanently modifying the planner itself. This approach, termed the Dynamically Local-Enhancement (DLE) Planner, aims to improve the scalability of autonomous driving systems without significantly expanding the planner's size. Our approach introduces a position-varying Markov Decision Process formulation coupled with a graph neural network that extracts region-specific driving features from local observation data. The learned features describe the local behavior of the surrounding objects, which is then leveraged to enhance a basic reinforcement learning-based policy. We evaluated our approach in multiple scenarios and compared it with a one-for-all driving model. The results show that our method outperforms the baseline policy in both safety (collision rate) and average reward, while maintaining a lighter scale. This approach has the potential to benefit large-scale autonomous vehicles without the need for largely expanding on-device driving models."
  },
  {
    "title": "Rare event modeling with self-regularized normalizing flows: what can we learn from a single failure?",
    "url": "http://arxiv.org/abs/2502.21110v1",
    "arxiv_id": "2502.21110v1",
    "authors": [
      "Charles Dawson",
      "Van Tran",
      "Max Z. Li",
      "Chuchu Fan"
    ],
    "published": "2025-02-28T14:47:52+00:00",
    "summary": "Increased deployment of autonomous systems in fields like transportation and robotics have seen a corresponding increase in safety-critical failures. These failures can be difficult to model and debug due to the relative lack of data: compared to tens of thousands of examples from normal operations, we may have only seconds of data leading up to the failure. This scarcity makes it challenging to train generative models of rare failure events, as existing methods risk either overfitting to noise in the limited failure dataset or underfitting due to an overly strong prior. We address this challenge with CalNF, or calibrated normalizing flows, a self-regularized framework for posterior learning from limited data. CalNF achieves state-of-the-art performance on data-limited failure modeling and inverse problems and enables a first-of-a-kind case study into the root causes of the 2022 Southwest Airlines scheduling crisis."
  },
  {
    "title": "AuthSim: Towards Authentic and Effective Safety-critical Scenario Generation for Autonomous Driving Tests",
    "url": "http://arxiv.org/abs/2502.21100v1",
    "arxiv_id": "2502.21100v1",
    "authors": [
      "Yukuan Yang",
      "Xucheng Lu",
      "Zhili Zhang",
      "Zepeng Wu",
      "Guoqi Li",
      "Lingzhong Meng",
      "Yunzhi Xue"
    ],
    "published": "2025-02-28T14:38:35+00:00",
    "summary": "Generating adversarial safety-critical scenarios is a pivotal method for testing autonomous driving systems, as it identifies potential weaknesses and enhances system robustness and reliability. However, existing approaches predominantly emphasize unrestricted collision scenarios, prompting non-player character (NPC) vehicles to attack the ego vehicle indiscriminately. These works overlook these scenarios' authenticity, rationality, and relevance, resulting in numerous extreme, contrived, and largely unrealistic collision events involving aggressive NPC vehicles. To rectify this issue, we propose a three-layer relative safety region model, which partitions the area based on danger levels and increases the likelihood of NPC vehicles entering relative boundary regions. This model directs NPC vehicles to engage in adversarial actions within relatively safe boundary regions, thereby augmenting the scenarios' authenticity. We introduce AuthSim, a comprehensive platform for generating authentic and effective safety-critical scenarios by integrating the three-layer relative safety region model with reinforcement learning. To our knowledge, this is the first attempt to address the authenticity and effectiveness of autonomous driving system test scenarios comprehensively. Extensive experiments demonstrate that AuthSim outperforms existing methods in generating effective safety-critical scenarios. Notably, AuthSim achieves a 5.25% improvement in average cut-in distance and a 27.12% enhancement in average collision interval time, while maintaining higher efficiency in generating effective safety-critical scenarios compared to existing methods. This underscores its significant advantage in producing authentic scenarios over current methodologies."
  },
  {
    "title": "FC-Attack: Jailbreaking Large Vision-Language Models via Auto-Generated Flowcharts",
    "url": "http://arxiv.org/abs/2502.21059v1",
    "arxiv_id": "2502.21059v1",
    "authors": [
      "Ziyi Zhang",
      "Zhen Sun",
      "Zongmin Zhang",
      "Jihui Guo",
      "Xinlei He"
    ],
    "published": "2025-02-28T13:59:11+00:00",
    "summary": "Large Vision-Language Models (LVLMs) have become powerful and widely adopted in some practical applications. However, recent research has revealed their vulnerability to multimodal jailbreak attacks, whereby the model can be induced to generate harmful content, leading to safety risks. Although most LVLMs have undergone safety alignment, recent research shows that the visual modality is still vulnerable to jailbreak attacks. In our work, we discover that by using flowcharts with partially harmful information, LVLMs can be induced to provide additional harmful details. Based on this, we propose a jailbreak attack method based on auto-generated flowcharts, FC-Attack. Specifically, FC-Attack first fine-tunes a pre-trained LLM to create a step-description generator based on benign datasets. The generator is then used to produce step descriptions corresponding to a harmful query, which are transformed into flowcharts in 3 different shapes (vertical, horizontal, and S-shaped) as visual prompts. These flowcharts are then combined with a benign textual prompt to execute a jailbreak attack on LVLMs. Our evaluations using the Advbench dataset show that FC-Attack achieves over 90% attack success rates on Gemini-1.5, Llaval-Next, Qwen2-VL, and InternVL-2.5 models, outperforming existing LVLM jailbreak methods. Additionally, we investigate factors affecting the attack performance, including the number of steps and the font styles in the flowcharts. Our evaluation shows that FC-Attack can improve the jailbreak performance from 4% to 28% in Claude-3.5 by changing the font style. To mitigate the attack, we explore several defenses and find that AdaShield can largely reduce the jailbreak performance but with the cost of utility drop."
  },
  {
    "title": "Including Follower Dynamics in Beaconing for Platooning Safety",
    "url": "http://arxiv.org/abs/2502.21039v1",
    "arxiv_id": "2502.21039v1",
    "authors": [
      "Hassan Laghbi",
      "Nigel Thomas"
    ],
    "published": "2025-02-28T13:30:46+00:00",
    "summary": "In this paper, we propose procedures to address platoon follower dynamics within adaptive beaconing. We implement them in a known adaptive beaconing scheme which is Jerk Beaconing (JB) to improve its safety. We evaluate our proposed approach in terms of safety, string stability and the channel busy ratio (CBR) overhead. The results reveal that our proposal significantly enhances safety without imposing substantial CBR overhead and maintains the string stability of the PATH CACC controller under normal conditions."
  },
  {
    "title": "A RISC-V Multicore and GPU SoC Platform with a Qualifiable Software Stack for Safety Critical Systems",
    "url": "http://arxiv.org/abs/2502.21027v1",
    "arxiv_id": "2502.21027v1",
    "authors": [
      "Marc Sol\u00e9 i Bonet",
      "Jannis Wolf",
      "Leonidas Kosmidis"
    ],
    "published": "2025-02-28T13:16:00+00:00",
    "summary": "In the context of the Horizon Europe project, METASAT, a hardware platform was developed as a prototype of future space systems. The platform is based on a multiprocessor NOEL-V, an established space-grade processor, which is integrated with the SPARROW AI accelerator and connected to a GPU, Vortex. Both processing systems follow the RISC-V specification. This is a novel hardware architecture for the space domain as the use of massive parallel processing units, such as GPUs, is starting to be considered for upcoming space missions due to the increased performance required to future space-related workloads, in particular, related to AI. However, such solutions are only currently adopted for New Space, since their limitations come not only from the hardware, but also from the software, which needs to be qualified before being deployed on an institutional mission. For this reason, the METASAT platform is one of the first endeavors towards enabling the use of high performance hardware in a qualifiable environment for safety critical systems. The software stack is based on baremetal, RTEMS and the XtratuM hypervisor, providing different options for applications of various degrees of criticality."
  },
  {
    "title": "Beware of Your Po! Measuring and Mitigating AI Safety Risks in Role-Play Fine-Tuning of LLMs",
    "url": "http://arxiv.org/abs/2502.20968v1",
    "arxiv_id": "2502.20968v1",
    "authors": [
      "Weixiang Zhao",
      "Yulin Hu",
      "Yang Deng",
      "Jiahe Guo",
      "Xingyu Sui",
      "Xinyang Han",
      "An Zhang",
      "Yanyan Zhao",
      "Bing Qin",
      "Tat-Seng Chua",
      "Ting Liu"
    ],
    "published": "2025-02-28T11:31:27+00:00",
    "summary": "Role-playing enables large language models (LLMs) to engage users in immersive and personalized interactions, but it also introduces significant safety risks. Existing role-play fine-tuning techniques improve role adaptability but may degrade safety performance, particularly for villainous characters. In this work, we conduct the first comprehensive assessment of role-play fine-tuning risks by training 95 role-specific LLMs using RoleBench. Our experiments reveal that role-play fine-tuning leads to a noticeable decline in safety performance, with safety risks varying based on character traits. To tackle this challenge, we propose Safety-Aware Role-Play Fine-Tuning (SaRFT), a novel method designed to balance role-playing capabilities and safety. Extensive experiments on LLaMA-3-8B-Instruct, Gemma-2-9B-it, and Qwen2.5-7B-Instruct demonstrate that SaRFT consistently outperforms state-of-the-art baselines under both LoRA and full-parameter fine-tuning settings. Our findings highlight the necessity of role-adaptive safety measures and provide insights into mitigating role-specific safety risks in role-playing LLMs."
  },
  {
    "title": "Efficient Jailbreaking of Large Models by Freeze Training: Lower Layers Exhibit Greater Sensitivity to Harmful Content",
    "url": "http://arxiv.org/abs/2502.20952v1",
    "arxiv_id": "2502.20952v1",
    "authors": [
      "Hongyuan Shen",
      "Min Zheng",
      "Jincheng Wang",
      "Yang Zhao"
    ],
    "published": "2025-02-28T11:07:41+00:00",
    "summary": "With the widespread application of Large Language Models across various domains, their security issues have increasingly garnered significant attention from both academic and industrial communities. This study conducts sampling and normalization of the parameters of the LLM to generate visual representations and heatmaps of parameter distributions, revealing notable discrepancies in parameter distributions among certain layers within the hidden layers. Further analysis involves calculating statistical metrics for each layer, followed by the computation of a Comprehensive Sensitivity Score based on these metrics, which identifies the lower layers as being particularly sensitive to the generation of harmful content. Based on this finding, we employ a Freeze training strategy, selectively performing Supervised Fine-Tuning only on the lower layers. Experimental results demonstrate that this method significantly reduces training duration and GPU memory consumption while maintaining a high jailbreak success rate and a high harm score, outperforming the results achieved by applying the LoRA method for SFT across all layers. Additionally, the method has been successfully extended to other open-source large models, validating its generality and effectiveness across different model architectures. Furthermore, we compare our method with ohter jailbreak method, demonstrating the superior performance of our approach. By innovatively proposing a method to statistically analyze and compare large model parameters layer by layer, this study provides new insights into the interpretability of large models. These discoveries emphasize the necessity of continuous research and the implementation of adaptive security measures in the rapidly evolving field of LLMs to prevent potential jailbreak attack risks, thereby promoting the development of more robust and secure LLMs."
  },
  {
    "title": "ProBench: Benchmarking Large Language Models in Competitive Programming",
    "url": "http://arxiv.org/abs/2502.20868v1",
    "arxiv_id": "2502.20868v1",
    "authors": [
      "Lei Yang",
      "Renren Jin",
      "Ling Shi",
      "Jianxiang Peng",
      "Yue Chen",
      "Deyi Xiong"
    ],
    "published": "2025-02-28T09:12:42+00:00",
    "summary": "With reasoning language models such as OpenAI-o3 and DeepSeek-R1 emerging, large language models (LLMs) have entered a new phase of development. However, existing benchmarks for coding evaluation are gradually inadequate to assess the capability of advanced LLMs in code reasoning. To bridge the gap for high-level code reasoning assessment, we propose ProBench to benchmark LLMs in competitive programming, drawing inspiration from the International Collegiate Programming Contest. ProBench collects a comprehensive set of competitive programming problems from Codeforces, Luogu, and Nowcoder platforms during the period from July to December 2024, obtaining real test results through online submissions to ensure the fairness and accuracy of the evaluation. We establish a unified problem attribute system, including difficulty grading and algorithm tagging. With carefully collected and annotated data in ProBench, we systematically assess 9 latest LLMs in competitive programming across multiple dimensions, including thought chain analysis, error type diagnosis, and reasoning depth evaluation. Experimental results show that QwQ-32B-Preview achieves the best score of 20.93 followed by DeepSeek-V3 with a score of 16.38, suggesting that models trained with specialized reasoning tasks significantly outperform general-purpose models (even larger than reasoning-oriented models) in programming. Further analysis also reveals key areas for programming capability enhancement, e.g., algorithm adaptability and reasoning sufficiency, providing important insights for the future development of reasoning models."
  },
  {
    "title": "Multimodal Learning for Just-In-Time Software Defect Prediction in Autonomous Driving Systems",
    "url": "http://arxiv.org/abs/2502.20806v1",
    "arxiv_id": "2502.20806v1",
    "authors": [
      "Faisal Mohammad",
      "Duksan Ryu"
    ],
    "published": "2025-02-28T07:45:10+00:00",
    "summary": "In recent years, the rise of autonomous driving technologies has highlighted the critical importance of reliable software for ensuring safety and performance. This paper proposes a novel approach for just-in-time software defect prediction (JIT-SDP) in autonomous driving software systems using multimodal learning. The proposed model leverages the multimodal transformers in which the pre-trained transformers and a combining module deal with the multiple data modalities of the software system datasets such as code features, change metrics, and contextual information. The key point for adapting multimodal learning is to utilize the attention mechanism between the different data modalities such as text, numerical, and categorical. In the combining module, the output of a transformer model on text data and tabular features containing categorical and numerical data are combined to produce the predictions using the fully connected layers. Experiments conducted on three open-source autonomous driving system software projects collected from the GitHub repository (Apollo, Carla, and Donkeycar) demonstrate that the proposed approach significantly outperforms state-of-the-art deep learning and machine learning models regarding evaluation metrics. Our findings highlight the potential of multimodal learning to enhance the reliability and safety of autonomous driving software through improved defect prediction."
  },
  {
    "title": "Characteristics Analysis of Autonomous Vehicle Pre-crash Scenarios",
    "url": "http://arxiv.org/abs/2502.20789v1",
    "arxiv_id": "2502.20789v1",
    "authors": [
      "Yixuan Li",
      "Xuesong Wang",
      "Tianyi Wang",
      "Qian Liu"
    ],
    "published": "2025-02-28T07:10:53+00:00",
    "summary": "To date, hundreds of crashes have occurred in open road testing of automated vehicles (AVs), highlighting the need for improving AV reliability and safety. Pre-crash scenario typology classifies crashes based on vehicle dynamics and kinematics features. Building on this, characteristics analysis can identify similar features under comparable crashes, offering a more effective reflection of general crash patterns and providing more targeted recommendations for enhancing AV performance. However, current studies primarily concentrated on crashes among conventional human-driven vehicles, leaving a gap in research dedicated to in-depth AV crash analyses. In this paper, we analyzed the latest California AV collision reports and used the newly revised pre-crash scenario typology to identify pre-crash scenarios. We proposed a set of mapping rules for automatically extracting these AV pre-crash scenarios, successfully identifying 24 types with a 98.1% accuracy rate, and obtaining two key scenarios of AV crashes (i.e., rear-end scenarios and intersection scenarios) through detailed analysis. Association analyses of rear-end scenarios showed that the significant environmental influencing factors were traffic control type, location type, light, etc. For intersection scenarios prone to severe crashes with detailed descriptions, we employed causal analyses to obtain the significant causal factors: habitual violations and expectations of certain behavior. Optimization recommendations were then formulated, addressing both governmental oversight and AV manufacturers' potential improvements. The findings of this paper could guide government authorities to develop related regulations, help manufacturers design AV test scenarios, and identify potential shortcomings in control algorithms specific to various real-world scenarios, thereby optimizing AV systems effectively."
  },
  {
    "title": "The Rise of Darkness: Safety-Utility Trade-Offs in Role-Playing Dialogue Agents",
    "url": "http://arxiv.org/abs/2502.20757v1",
    "arxiv_id": "2502.20757v1",
    "authors": [
      "Yihong Tang",
      "Kehai Chen",
      "Xuefeng Bai",
      "Zhengyu Niu",
      "Bo Wang",
      "Jie Liu",
      "Min Zhang"
    ],
    "published": "2025-02-28T06:18:50+00:00",
    "summary": "Large Language Models (LLMs) have made remarkable advances in role-playing dialogue agents, demonstrating their utility in character simulations. However, it remains challenging for these agents to balance character portrayal utility with content safety because this essential character simulation often comes with the risk of generating unsafe content. To address this issue, we first conduct a systematic exploration of the safety-utility trade-off across multiple LLMs. Our analysis reveals that risk scenarios created by villain characters and user queries (referred to as risk coupling) contribute to this trade-off. Building on this, we propose a novel Adaptive Dynamic Multi-Preference (ADMP) method, which dynamically adjusts safety-utility preferences based on the degree of risk coupling and guides the model to generate responses biased toward utility or safety. We further introduce Coupling Margin Sampling (CMS) into coupling detection to enhance the model's ability to handle high-risk scenarios. Experimental results demonstrate that our approach improves safety metrics while maintaining utility."
  },
  {
    "title": "Computationally Efficient Safe Control of Linear Systems under Severe Sensor Attacks",
    "url": "http://arxiv.org/abs/2502.20718v1",
    "arxiv_id": "2502.20718v1",
    "authors": [
      "Xiao Tan",
      "Pio Ong",
      "Paulo Tabuada",
      "Aaron D. Ames"
    ],
    "published": "2025-02-28T05:05:48+00:00",
    "summary": "Cyber-physical systems are prone to sensor attacks that can compromise safety. A common approach to synthesizing controllers robust to sensor attacks is secure state reconstruction (SSR) -- but this is computationally expensive, hindering real-time control. In this paper, we take a safety-critical perspective on mitigating severe sensor attacks, leading to a computationally efficient solution. Namely, we design feedback controllers that ensure system safety by directly computing control actions from past input-output data. Instead of fully solving the SSR problem, we use conservative bounds on a control barrier function (CBF) condition, which we obtain by extending the recent eigendecomposition-based SSR approach to severe sensor attack settings. Additionally, we present an extended approach that solves a smaller-scale subproblem of the SSR problem, taking on some computational burden to mitigate the conservatism in the main approach. Numerical comparisons confirm that the traditional SSR approaches suffer from combinatorial issues, while our approach achieves safety guarantees with greater computational efficiency."
  },
  {
    "title": "From Safety Standards to Safe Operation with Mobile Robotic Systems Deployment",
    "url": "http://arxiv.org/abs/2502.20693v1",
    "arxiv_id": "2502.20693v1",
    "authors": [
      "Bruno Belzile",
      "Tatiana Wanang-Siyapdjie",
      "Sina Karimi",
      "Rafael Gomes Braga",
      "Ivanka Iordanova",
      "David St-Onge"
    ],
    "published": "2025-02-28T03:52:10+00:00",
    "summary": "Mobile robotic systems are increasingly used in various work environments to support productivity. However, deploying robots in workplaces crowded by human workers and interacting with them results in safety challenges and concerns, namely robot-worker collisions and worker distractions in hazardous environments. Moreover, the literature on risk assessment as well as the standard specific to mobile platforms is rather limited. In this context, this paper first conducts a review of the relevant standards and methodologies and then proposes a risk assessment for the safe deployment of mobile robots on construction sites. The approach extends relevant existing safety standards to encompass uncovered scenarios. Safety recommendations are made based on the framework, after its validation by field experts."
  },
  {
    "title": "Delayed-Decision Motion Planning in the Presence of Multiple Predictions",
    "url": "http://arxiv.org/abs/2502.20636v1",
    "arxiv_id": "2502.20636v1",
    "authors": [
      "David Isele",
      "Alexandre Miranda Anon",
      "Faizan M. Tariq",
      "Goro Yeh",
      "Avinash Singh",
      "Sangjae Bae"
    ],
    "published": "2025-02-28T01:36:33+00:00",
    "summary": "Reliable automated driving technology is challenged by various sources of uncertainties, in particular, behavioral uncertainties of traffic agents. It is common for traffic agents to have intentions that are unknown to others, leaving an automated driving car to reason over multiple possible behaviors. This paper formalizes a behavior planning scheme in the presence of multiple possible futures with corresponding probabilities. We present a maximum entropy formulation and show how, under certain assumptions, this allows delayed decision-making to improve safety. The general formulation is then turned into a model predictive control formulation, which is solved as a quadratic program or a set of quadratic programs. We discuss implementation details for improving computation and verify operation in simulation and on a mobile robot."
  },
  {
    "title": "SafeText: Safe Text-to-image Models via Aligning the Text Encoder",
    "url": "http://arxiv.org/abs/2502.20623v1",
    "arxiv_id": "2502.20623v1",
    "authors": [
      "Yuepeng Hu",
      "Zhengyuan Jiang",
      "Neil Zhenqiang Gong"
    ],
    "published": "2025-02-28T01:02:57+00:00",
    "summary": "Text-to-image models can generate harmful images when presented with unsafe prompts, posing significant safety and societal risks. Alignment methods aim to modify these models to ensure they generate only non-harmful images, even when exposed to unsafe prompts. A typical text-to-image model comprises two main components: 1) a text encoder and 2) a diffusion module. Existing alignment methods mainly focus on modifying the diffusion module to prevent harmful image generation. However, this often significantly impacts the model's behavior for safe prompts, causing substantial quality degradation of generated images. In this work, we propose SafeText, a novel alignment method that fine-tunes the text encoder rather than the diffusion module. By adjusting the text encoder, SafeText significantly alters the embedding vectors for unsafe prompts, while minimally affecting those for safe prompts. As a result, the diffusion module generates non-harmful images for unsafe prompts while preserving the quality of images for safe prompts. We evaluate SafeText on multiple datasets of safe and unsafe prompts, including those generated through jailbreak attacks. Our results show that SafeText effectively prevents harmful image generation with minor impact on the images for safe prompts, and SafeText outperforms six existing alignment methods. We will publish our code and data after paper acceptance."
  },
  {
    "title": "HazardNet: A Small-Scale Vision Language Model for Real-Time Traffic Safety Detection at Edge Devices",
    "url": "http://arxiv.org/abs/2502.20572v1",
    "arxiv_id": "2502.20572v1",
    "authors": [
      "Mohammad Abu Tami",
      "Mohammed Elhenawy",
      "Huthaifa I. Ashqar"
    ],
    "published": "2025-02-27T22:21:45+00:00",
    "summary": "Traffic safety remains a vital concern in contemporary urban settings, intensified by the increase of vehicles and the complicated nature of road networks. Traditional safety-critical event detection systems predominantly rely on sensor-based approaches and conventional machine learning algorithms, necessitating extensive data collection and complex training processes to adhere to traffic safety regulations. This paper introduces HazardNet, a small-scale Vision Language Model designed to enhance traffic safety by leveraging the reasoning capabilities of advanced language and vision models. We built HazardNet by fine-tuning the pre-trained Qwen2-VL-2B model, chosen for its superior performance among open-source alternatives and its compact size of two billion parameters. This helps to facilitate deployment on edge devices with efficient inference throughput. In addition, we present HazardQA, a novel Vision Question Answering (VQA) dataset constructed specifically for training HazardNet on real-world scenarios involving safety-critical events. Our experimental results show that the fine-tuned HazardNet outperformed the base model up to an 89% improvement in F1-Score and has comparable results with improvement in some cases reach up to 6% when compared to larger models, such as GPT-4o. These advancements underscore the potential of HazardNet in providing real-time, reliable traffic safety event detection, thereby contributing to reduced accidents and improved traffic management in urban environments. Both HazardNet model and the HazardQA dataset are available at https://huggingface.co/Tami3/HazardNet and https://huggingface.co/datasets/Tami3/HazardQA, respectively."
  },
  {
    "title": "SoS1: O1 and R1-Like Reasoning LLMs are Sum-of-Square Solvers",
    "url": "http://arxiv.org/abs/2502.20545v1",
    "arxiv_id": "2502.20545v1",
    "authors": [
      "Kechen Li",
      "Wenqi Zhu",
      "Coralia Cartis",
      "Tianbo Ji",
      "Shiwei Liu"
    ],
    "published": "2025-02-27T21:41:43+00:00",
    "summary": "Large Language Models (LLMs) have achieved human-level proficiency across diverse tasks, but their ability to perform rigorous mathematical problem solving remains an open challenge. In this work, we investigate a fundamental yet computationally intractable problem: determining whether a given multivariate polynomial is nonnegative. This problem, closely related to Hilbert's Seventeenth Problem, plays a crucial role in global polynomial optimization and has applications in various fields. First, we introduce SoS-1K, a meticulously curated dataset of approximately 1,000 polynomials, along with expert-designed reasoning instructions based on five progressively challenging criteria. Evaluating multiple state-of-the-art LLMs, we find that without structured guidance, all models perform only slightly above the random guess baseline 50%. However, high-quality reasoning instructions significantly improve accuracy, boosting performance up to 81%. Furthermore, our 7B model, SoS-7B, fine-tuned on SoS-1K for just 4 hours, outperforms the 671B DeepSeek-V3 and GPT-4o-mini in accuracy while only requiring 1.8% and 5% of the computation time needed for letters, respectively. Our findings highlight the potential of LLMs to push the boundaries of mathematical reasoning and tackle NP-hard problems."
  },
  {
    "title": "Automated Profile-Guided Replacement of Data Structures to Reduce Memory Allocation",
    "url": "http://arxiv.org/abs/2502.20536v1",
    "arxiv_id": "2502.20536v1",
    "authors": [
      "Lukas Makor",
      "Sebastian Kloibhofer",
      "Peter Hofer",
      "David Leopoldseder",
      "Hanspeter M\u00f6ssenb\u00f6ck"
    ],
    "published": "2025-02-27T21:38:48+00:00",
    "summary": "Data structures are a cornerstone of most modern programming languages. Whether they are provided via separate libraries, built into the language specification, or as part of the language's standard library -- data structures such as lists, maps, sets, or arrays provide programmers with a large repertoire of tools to deal with data. Moreover, each kind of data structure typically comes with a variety of implementations that focus on scalability, memory efficiency, performance, thread-safety, or similar aspects. Choosing the *right* data structure for a particular use case can be difficult or even impossible if the data structure is part of a framework over which the user has no control. It typically requires in-depth knowledge about the program and, in particular, about the usage of the data structure in question. However, it is usually not feasible for developers to obtain such information about programs in advance. Hence, it makes sense to look for a more automated way for optimizing data structures. We present an approach to automatically replace data structures in Java applications. We use profiling to determine allocation-site-specific metrics about data structures and their usages, and then automatically replace their allocations with customized versions, focusing on memory efficiency. Our approach is integrated into GraalVM Native Image, an Ahead-of-Time compiler for Java applications. By analyzing the generated data structure profiles, we show how standard benchmarks and microservice-based applications use data structures and demonstrate the impact of customized data structures on the memory usage of applications. We conducted an evaluation on standard and microservice-based benchmarks, in which the memory usage was reduced by up to 13.85 % in benchmarks that make heavy use of data structures. While others are only slightly affected, we could still reduce the average memory usage by 1.63 % in standard benchmarks and by 2.94 % in microservice-based benchmarks. We argue that our work demonstrates that choosing appropriate data structures can reduce the memory usage of applications. While acknowledge that our approach does not provide benefits for all kinds of workloads, our work nevertheless shows how automated profiling and replacement can be used to optimize data structures in general. Hence, we argue that our work could pave the way for future optimizations of data structures."
  },
  {
    "title": "APIKS: A Modular ROS2 Framework for Rapid Prototyping and Validation of Automated Driving Systems",
    "url": "http://arxiv.org/abs/2502.20507v1",
    "arxiv_id": "2502.20507v1",
    "authors": [
      "Jo\u00e3o-Vitor Zacchi",
      "Edoardo Clementi",
      "N\u00faria Mata"
    ],
    "published": "2025-02-27T20:29:31+00:00",
    "summary": "Automated driving technologies promise substantial improvements in transportation safety, efficiency, and accessibility. However, ensuring the reliability and safety of Autonomous Vehicles in complex, real-world environments remains a significant challenge, particularly during the early stages of software development. Existing software development environments and simulation platforms often either focus narrowly on specific functions or are too complex, hindering the rapid prototyping of small proofs of concept. To address this challenge, we have developed the APIKS automotive platform, a modular framework based on ROS2. APIKS is designed for the efficient testing and validation of autonomous vehicle software within software-defined vehicles. It offers a simplified, standards-based architecture designed specifically for small-scale proofs of concept. This enables rapid prototyping without the overhead associated with comprehensive platforms. We demonstrate the capabilities of APIKS through an exemplary use case involving a Construction Zone Assist system, illustrating its effectiveness in facilitating the development and testing of autonomous vehicle functionalities."
  },
  {
    "title": "Equivariant Reinforcement Learning Frameworks for Quadrotor Low-Level Control",
    "url": "http://arxiv.org/abs/2502.20500v1",
    "arxiv_id": "2502.20500v1",
    "authors": [
      "Beomyeol Yu",
      "Taeyoung Lee"
    ],
    "published": "2025-02-27T20:16:19+00:00",
    "summary": "Improving sampling efficiency and generalization capability is critical for the successful data-driven control of quadrotor unmanned aerial vehicles (UAVs) that are inherently unstable. While various reinforcement learning (RL) approaches have been applied to autonomous quadrotor flight, they often require extensive training data, posing multiple challenges and safety risks in practice. To address these issues, we propose data-efficient, equivariant monolithic and modular RL frameworks for quadrotor low-level control. Specifically, by identifying the rotational and reflectional symmetries in quadrotor dynamics and encoding these symmetries into equivariant network models, we remove redundancies of learning in the state-action space. This approach enables the optimal control action learned in one configuration to automatically generalize into other configurations via symmetry, thereby enhancing data efficiency. Experimental results demonstrate that our equivariant approaches significantly outperform their non-equivariant counterparts in terms of learning efficiency and flight performance."
  },
  {
    "title": "EgoNormia: Benchmarking Physical Social Norm Understanding",
    "url": "http://arxiv.org/abs/2502.20490v1",
    "arxiv_id": "2502.20490v1",
    "authors": [
      "MohammadHossein Rezaei",
      "Yicheng Fu",
      "Phil Cuvin",
      "Caleb Ziems",
      "Yanzhe Zhang",
      "Hao Zhu",
      "Diyi Yang"
    ],
    "published": "2025-02-27T19:54:16+00:00",
    "summary": "Human activity is moderated by norms. When performing actions in the real world, humans not only follow norms, but also consider the trade-off between different norms However, machines are often trained without explicit supervision on norm understanding and reasoning, especially when the norms are grounded in a physical and social context. To improve and evaluate the normative reasoning capability of vision-language models (VLMs), we present EgoNormia $\\|\\epsilon\\|$, consisting of 1,853 ego-centric videos of human interactions, each of which has two related questions evaluating both the prediction and justification of normative actions. The normative actions encompass seven categories: safety, privacy, proxemics, politeness, cooperation, coordination/proactivity, and communication/legibility. To compile this dataset at scale, we propose a novel pipeline leveraging video sampling, automatic answer generation, filtering, and human validation. Our work demonstrates that current state-of-the-art vision-language models lack robust norm understanding, scoring a maximum of 45% on EgoNormia (versus a human bench of 92%). Our analysis of performance in each dimension highlights the significant risks of safety, privacy, and the lack of collaboration and communication capability when applied to real-world agents. We additionally show that through a retrieval-based generation method, it is possible to use EgoNomia to enhance normative reasoning in VLMs."
  },
  {
    "title": "Bounded First-Class Universe Levels in Dependent Type Theory",
    "url": "http://arxiv.org/abs/2502.20485v1",
    "arxiv_id": "2502.20485v1",
    "authors": [
      "Jonathan Chan",
      "Stephanie Weirich"
    ],
    "published": "2025-02-27T19:52:46+00:00",
    "summary": "In dependent type theory, being able to refer to a type universe as a term itself increases its expressive power, but requires mechanisms in place to prevent Girard's paradox from introducing logical inconsistency in the presence of type-in-type. The simplest mechanism is a hierarchy of universes indexed by a sequence of levels, typically the naturals. To improve reusability of definitions, they can be made level polymorphic, abstracting over level variables and adding a notion of level expressions. For even more expressive power, level expressions can be made first-class as terms themselves, and level polymorphism is subsumed by dependent functions quantifying over levels. Furthermore, bounded level polymorphism provides more expressivity by being able to explicitly state constraints on level variables. While semantics for first-class levels with constraints are known, syntax and typing rules have not been explicitly written down. Yet pinning down a well-behaved syntax is not trivial; there exist prior type theories with bounded level polymorphism that fail to satisfy subject reduction. In this work, we design an explicit syntax for a type theory with bounded first-class levels, parametrized over arbitrary well-founded sets of levels. We prove the metatheoretic properties of subject reduction, type safety, consistency, and canonicity, entirely mechanized from syntax to semantics in Lean."
  },
  {
    "title": "Searching for additional structure and redshift evolution in the observed binary black hole population with a parametric time-dependent mass distribution",
    "url": "http://arxiv.org/abs/2502.20445v1",
    "arxiv_id": "2502.20445v1",
    "authors": [
      "Vasco Gennari",
      "Simone Mastrogiovanni",
      "Nicola Tamanini",
      "Sylvain Marsat",
      "Gr\u00e9goire Pierra"
    ],
    "published": "2025-02-27T19:00:02+00:00",
    "summary": "The population of the observed gravitational wave events encodes unique information on the formation and evolution of stellar-mass black holes, from the underlying astrophysical processes to the large-scale dynamics of the Universe. We use the ICAROGW analysis infrastructure to perform hierarchical Bayesian inference on the gravitational wave signals from the LIGO-Virgo-KAGRA third observing run, O3. Searching for additional structure and redshift evolution in the primary mass distribution, we explore the dependence of the mass spectrum reconstruction on different parametrizations and prior choices. For the stationary case, we find strong evidence (Bayes factor $B \\simeq 180$) that the results obtained using a power-law model with a peak (Powerlaw-Gaussian)--the model preferred so far in the literature--are sensitive to prior bounds, affecting the resolvability of the $\\sim 35 M_{\\odot}$ peak. This behaviour is reproduced by simulated data, indicating a bimodal structure in the likelihood. Models with three mass features simultaneously capture a sharp $\\sim 10M_{\\odot}$ peak, a $\\sim 35 M_{\\odot}$ overdensity, and support for a $\\sim 20 M_{\\odot}$ overdensity preceded by a dip. Among these, a model with three power-law peaks (Powerlaw-Powerlaw-Powerlaw) is equally favored, in terms of evidence, over the Powerlaw-Gaussian model with wide priors. We find no statistical support for redshift evolution in the current data and provide constraints on the parameters governing this evolution, showing consistency with stationarity. We highlight possible limitations of the hierarchical Bayesian inference framework in reconstructing evolving features outside the detector horizon. Our work lays the foundations for a robust characterization of time-dependent population distributions, with significant implications for black hole astrophysics and gravitational wave cosmology."
  },
  {
    "title": "Large Language Model Strategic Reasoning Evaluation through Behavioral Game Theory",
    "url": "http://arxiv.org/abs/2502.20432v1",
    "arxiv_id": "2502.20432v1",
    "authors": [
      "Jingru Jia",
      "Zehua Yuan",
      "Junhao Pan",
      "Paul E. McNamara",
      "Deming Chen"
    ],
    "published": "2025-02-27T18:58:31+00:00",
    "summary": "Strategic decision-making involves interactive reasoning where agents adapt their choices in response to others, yet existing evaluations of large language models (LLMs) often emphasize Nash Equilibrium (NE) approximation, overlooking the mechanisms driving their strategic choices. To bridge this gap, we introduce an evaluation framework grounded in behavioral game theory, disentangling reasoning capability from contextual effects. Testing 22 state-of-the-art LLMs, we find that GPT-o3-mini, GPT-o1, and DeepSeek-R1 dominate most games yet also demonstrate that the model scale alone does not determine performance. In terms of prompting enhancement, Chain-of-Thought (CoT) prompting is not universally effective, as it increases strategic reasoning only for models at certain levels while providing limited gains elsewhere. Additionally, we investigate the impact of encoded demographic features on the models, observing that certain assignments impact the decision-making pattern. For instance, GPT-4o shows stronger strategic reasoning with female traits than males, while Gemma assigns higher reasoning levels to heterosexual identities compared to other sexual orientations, indicating inherent biases. These findings underscore the need for ethical standards and contextual alignment to balance improved reasoning with fairness."
  },
  {
    "title": "Why Are Web AI Agents More Vulnerable Than Standalone LLMs? A Security Analysis",
    "url": "http://arxiv.org/abs/2502.20383v1",
    "arxiv_id": "2502.20383v1",
    "authors": [
      "Jeffrey Yang Fan Chiang",
      "Seungjae Lee",
      "Jia-Bin Huang",
      "Furong Huang",
      "Yizheng Chen"
    ],
    "published": "2025-02-27T18:56:26+00:00",
    "summary": "Recent advancements in Web AI agents have demonstrated remarkable capabilities in addressing complex web navigation tasks. However, emerging research shows that these agents exhibit greater vulnerability compared to standalone Large Language Models (LLMs), despite both being built upon the same safety-aligned models. This discrepancy is particularly concerning given the greater flexibility of Web AI Agent compared to standalone LLMs, which may expose them to a wider range of adversarial user inputs. To build a scaffold that addresses these concerns, this study investigates the underlying factors that contribute to the increased vulnerability of Web AI agents. Notably, this disparity stems from the multifaceted differences between Web AI agents and standalone LLMs, as well as the complex signals - nuances that simple evaluation metrics, such as success rate, often fail to capture. To tackle these challenges, we propose a component-level analysis and a more granular, systematic evaluation framework. Through this fine-grained investigation, we identify three critical factors that amplify the vulnerability of Web AI agents; (1) embedding user goals into the system prompt, (2) multi-step action generation, and (3) observational capabilities. Our findings highlights the pressing need to enhance security and robustness in AI agent design and provide actionable insights for targeted defense strategies."
  },
  {
    "title": "Trajectory-to-Action Pipeline (TAP): Automated Scenario Description Extraction for Autonomous Vehicle Behavior Comparison",
    "url": "http://arxiv.org/abs/2502.20353v1",
    "arxiv_id": "2502.20353v1",
    "authors": [
      "Aron Harder",
      "Madhur Behl"
    ],
    "published": "2025-02-27T18:27:05+00:00",
    "summary": "Scenario Description Languages (SDLs) provide structured, interpretable embeddings that represent traffic scenarios encountered by autonomous vehicles (AVs), supporting key tasks such as scenario similarity searches and edge case detection for safety analysis. This paper introduces the Trajectory-to-Action Pipeline (TAP), a scalable and automated method for extracting SDL labels from large trajectory datasets. TAP applies a rules-based cross-entropy optimization approach to learn parameters directly from data, enhancing generalization across diverse driving contexts. Using the Waymo Open Motion Dataset (WOMD), TAP achieves 30% greater precision than Average Displacement Error (ADE) and 24% over Dynamic Time Warping (DTW) in identifying behaviorally similar trajectories. Additionally, TAP enables automated detection of unique driving behaviors, streamlining safety evaluation processes for AV testing. This work provides a foundation for scalable scenario-based AV behavior analysis, with potential extensions for integrating multi-agent contexts."
  },
  {
    "title": "Safety Representations for Safer Policy Learning",
    "url": "http://arxiv.org/abs/2502.20341v1",
    "arxiv_id": "2502.20341v1",
    "authors": [
      "Kaustubh Mani",
      "Vincent Mai",
      "Charlie Gauthier",
      "Annie Chen",
      "Samer Nashed",
      "Liam Paull"
    ],
    "published": "2025-02-27T18:10:33+00:00",
    "summary": "Reinforcement learning algorithms typically necessitate extensive exploration of the state space to find optimal policies. However, in safety-critical applications, the risks associated with such exploration can lead to catastrophic consequences. Existing safe exploration methods attempt to mitigate this by imposing constraints, which often result in overly conservative behaviours and inefficient learning. Heavy penalties for early constraint violations can trap agents in local optima, deterring exploration of risky yet high-reward regions of the state space. To address this, we introduce a method that explicitly learns state-conditioned safety representations. By augmenting the state features with these safety representations, our approach naturally encourages safer exploration without being excessively cautious, resulting in more efficient and safer policy learning in safety-critical scenarios. Empirical evaluations across diverse environments show that our method significantly improves task performance while reducing constraint violations during training, underscoring its effectiveness in balancing exploration with safety."
  },
  {
    "title": "On Adversarial Attacks In Acoustic Drone Localization",
    "url": "http://arxiv.org/abs/2502.20325v1",
    "arxiv_id": "2502.20325v1",
    "authors": [
      "Tamir Shor",
      "Chaim Baskin",
      "Alex Bronstein"
    ],
    "published": "2025-02-27T17:50:17+00:00",
    "summary": "Multi-rotor aerial autonomous vehicles (MAVs, more widely known as \"drones\") have been generating increased interest in recent years due to their growing applicability in a vast and diverse range of fields (e.g., agriculture, commercial delivery, search and rescue). The sensitivity of visual-based methods to lighting conditions and occlusions had prompted growing study of navigation reliant on other modalities, such as acoustic sensing. A major concern in using drones in scale for tasks in non-controlled environments is the potential threat of adversarial attacks over their navigational systems, exposing users to mission-critical failures, security breaches, and compromised safety outcomes that can endanger operators and bystanders. While previous work shows impressive progress in acoustic-based drone localization, prior research in adversarial attacks over drone navigation only addresses visual sensing-based systems. In this work, we aim to compensate for this gap by supplying a comprehensive analysis of the effect of PGD adversarial attacks over acoustic drone localization. We furthermore develop an algorithm for adversarial perturbation recovery, capable of markedly diminishing the affect of such attacks in our setting. The code for reproducing all experiments will be released upon publication."
  },
  {
    "title": "Adapting Automatic Speech Recognition for Accented Air Traffic Control Communications",
    "url": "http://arxiv.org/abs/2502.20311v1",
    "arxiv_id": "2502.20311v1",
    "authors": [
      "Marcus Yu Zhe Wee",
      "Justin Juin Hng Wong",
      "Lynus Lim",
      "Joe Yu Wei Tan",
      "Prannaya Gupta",
      "Dillion Lim",
      "En Hao Tew",
      "Aloysius Keng Siew Han",
      "Yong Zhi Lim"
    ],
    "published": "2025-02-27T17:35:59+00:00",
    "summary": "Effective communication in Air Traffic Control (ATC) is critical to maintaining aviation safety, yet the challenges posed by accented English remain largely unaddressed in Automatic Speech Recognition (ASR) systems. Existing models struggle with transcription accuracy for Southeast Asian-accented (SEA-accented) speech, particularly in noisy ATC environments. This study presents the development of ASR models fine-tuned specifically for Southeast Asian accents using a newly created dataset. Our research achieves significant improvements, achieving a Word Error Rate (WER) of 0.0982 or 9.82% on SEA-accented ATC speech. Additionally, the paper highlights the importance of region-specific datasets and accent-focused training, offering a pathway for deploying ASR systems in resource-constrained military operations. The findings emphasize the need for noise-robust training techniques and region-specific datasets to improve transcription accuracy for non-Western accents in ATC communications."
  },
  {
    "title": "Interpreting AI for Fusion: an application to Plasma Profile Analysis for Tearing Mode Stability",
    "url": "http://arxiv.org/abs/2502.20294v1",
    "arxiv_id": "2502.20294v1",
    "authors": [
      "Hiro J Farre-Kaga",
      "Andrew Rothstein",
      "Rohit Sonker",
      "SangKyeun Kim",
      "Ricardo Shousha",
      "Minseok Kim",
      "Keith Erickson",
      "Jeff Schneider",
      "Egemen Kolemen"
    ],
    "published": "2025-02-27T17:19:14+00:00",
    "summary": "AI models have demonstrated strong predictive capabilities for various tokamak instabilities--including tearing modes (TM), ELMs, and disruptive events--but their opaque nature raises concerns about safety and trustworthiness when applied to fusion power plants. Here, we present a physics-based interpretation framework using a TM prediction model as a first demonstration that is validated through a dedicated DIII-D TM avoidance experiment. By applying Shapley analysis, we identify how profiles such as rotation, temperature, and density contribute to the model's prediction of TM stability. Our analysis shows that in our experimental scenario, peaked rotation profiles are lightly stabilizing, but core electron temperature and density profile shape play the primary role in TM stability. This work offers a generalizable ML-based event prediction methodology, from training to physics-driven interpretability, bridging the gap between physics understanding and opaque ML models."
  },
  {
    "title": "QPM: Discrete Optimization for Globally Interpretable Image Classification",
    "url": "http://arxiv.org/abs/2502.20130v1",
    "arxiv_id": "2502.20130v1",
    "authors": [
      "Thomas Norrenbrock",
      "Timo Kaiser",
      "Sovan Biswas",
      "Ramesh Manuvinakurike",
      "Bodo Rosenhahn"
    ],
    "published": "2025-02-27T14:25:36+00:00",
    "summary": "Understanding the classifications of deep neural networks, e.g. used in safety-critical situations, is becoming increasingly important. While recent models can locally explain a single decision, to provide a faithful global explanation about an accurate model's general behavior is a more challenging open task. Towards that goal, we introduce the Quadratic Programming Enhanced Model (QPM), which learns globally interpretable class representations. QPM represents every class with a binary assignment of very few, typically 5, features, that are also assigned to other classes, ensuring easily comparable contrastive class representations. This compact binary assignment is found using discrete optimization based on predefined similarity measures and interpretability constraints. The resulting optimal assignment is used to fine-tune the diverse features, so that each of them becomes the shared general concept between the assigned classes. Extensive evaluations show that QPM delivers unprecedented global interpretability across small and large-scale datasets while setting the state of the art for the accuracy of interpretable models."
  },
  {
    "title": "Minds on the Move: Decoding Trajectory Prediction in Autonomous Driving with Cognitive Insights",
    "url": "http://arxiv.org/abs/2502.20084v1",
    "arxiv_id": "2502.20084v1",
    "authors": [
      "Haicheng Liao",
      "Chengyue Wang",
      "Kaiqun Zhu",
      "Yilong Ren",
      "Bolin Gao",
      "Shengbo Eben Li",
      "Chengzhong Xu",
      "Zhenning Li"
    ],
    "published": "2025-02-27T13:43:17+00:00",
    "summary": "In mixed autonomous driving environments, accurately predicting the future trajectories of surrounding vehicles is crucial for the safe operation of autonomous vehicles (AVs). In driving scenarios, a vehicle's trajectory is determined by the decision-making process of human drivers. However, existing models primarily focus on the inherent statistical patterns in the data, often neglecting the critical aspect of understanding the decision-making processes of human drivers. This oversight results in models that fail to capture the true intentions of human drivers, leading to suboptimal performance in long-term trajectory prediction. To address this limitation, we introduce a Cognitive-Informed Transformer (CITF) that incorporates a cognitive concept, Perceived Safety, to interpret drivers' decision-making mechanisms. Perceived Safety encapsulates the varying risk tolerances across drivers with different driving behaviors. Specifically, we develop a Perceived Safety-aware Module that includes a Quantitative Safety Assessment for measuring the subject risk levels within scenarios, and Driver Behavior Profiling for characterizing driver behaviors. Furthermore, we present a novel module, Leanformer, designed to capture social interactions among vehicles. CITF demonstrates significant performance improvements on three well-established datasets. In terms of long-term prediction, it surpasses existing benchmarks by 12.0% on the NGSIM, 28.2% on the HighD, and 20.8% on the MoCAD dataset. Additionally, its robustness in scenarios with limited or missing data is evident, surpassing most state-of-the-art (SOTA) baselines, and paving the way for real-world applications."
  },
  {
    "title": "Deterministic or probabilistic? The psychology of LLMs as random number generators",
    "url": "http://arxiv.org/abs/2502.19965v1",
    "arxiv_id": "2502.19965v1",
    "authors": [
      "Javier Coronado-Bl\u00e1zquez"
    ],
    "published": "2025-02-27T10:45:27+00:00",
    "summary": "Large Language Models (LLMs) have transformed text generation through inherently probabilistic context-aware mechanisms, mimicking human natural language. In this paper, we systematically investigate the performance of various LLMs when generating random numbers, considering diverse configurations such as different model architectures, numerical ranges, temperature, and prompt languages. Our results reveal that, despite their stochastic transformers-based architecture, these models often exhibit deterministic responses when prompted for random numerical outputs. In particular, we find significant differences when changing the model, as well as the prompt language, attributing this phenomenon to biases deeply embedded within the training data. Models such as DeepSeek-R1 can shed some light on the internal reasoning process of LLMs, despite arriving to similar results. These biases induce predictable patterns that undermine genuine randomness, as LLMs are nothing but reproducing our own human cognitive biases."
  },
  {
    "title": "Beyond the Tip of Efficiency: Uncovering the Submerged Threats of Jailbreak Attacks in Small Language Models",
    "url": "http://arxiv.org/abs/2502.19883v1",
    "arxiv_id": "2502.19883v1",
    "authors": [
      "Sibo Yi",
      "Tianshuo Cong",
      "Xinlei He",
      "Qi Li",
      "Jiaxing Song"
    ],
    "published": "2025-02-27T08:44:04+00:00",
    "summary": "Small language models (SLMs) have become increasingly prominent in the deployment on edge devices due to their high efficiency and low computational cost. While researchers continue to advance the capabilities of SLMs through innovative training strategies and model compression techniques, the security risks of SLMs have received considerably less attention compared to large language models (LLMs).To fill this gap, we provide a comprehensive empirical study to evaluate the security performance of 13 state-of-the-art SLMs under various jailbreak attacks. Our experiments demonstrate that most SLMs are quite susceptible to existing jailbreak attacks, while some of them are even vulnerable to direct harmful prompts.To address the safety concerns, we evaluate several representative defense methods and demonstrate their effectiveness in enhancing the security of SLMs. We further analyze the potential security degradation caused by different SLM techniques including architecture compression, quantization, knowledge distillation, and so on. We expect that our research can highlight the security challenges of SLMs and provide valuable insights to future work in developing more robust and secure SLMs."
  },
  {
    "title": "Behind the Tip of Efficiency: Uncovering the Submerged Threats of Jailbreak Attacks in Small Language Models",
    "url": "http://arxiv.org/abs/2502.19883v2",
    "arxiv_id": "2502.19883v2",
    "authors": [
      "Sibo Yi",
      "Tianshuo Cong",
      "Xinlei He",
      "Qi Li",
      "Jiaxing Song"
    ],
    "published": "2025-02-27T08:44:04+00:00",
    "summary": "Small language models (SLMs) have become increasingly prominent in the deployment on edge devices due to their high efficiency and low computational cost. While researchers continue to advance the capabilities of SLMs through innovative training strategies and model compression techniques, the security risks of SLMs have received considerably less attention compared to large language models (LLMs).To fill this gap, we provide a comprehensive empirical study to evaluate the security performance of 13 state-of-the-art SLMs under various jailbreak attacks. Our experiments demonstrate that most SLMs are quite susceptible to existing jailbreak attacks, while some of them are even vulnerable to direct harmful prompts.To address the safety concerns, we evaluate several representative defense methods and demonstrate their effectiveness in enhancing the security of SLMs. We further analyze the potential security degradation caused by different SLM techniques including architecture compression, quantization, knowledge distillation, and so on. We expect that our research can highlight the security challenges of SLMs and provide valuable insights to future work in developing more robust and secure SLMs."
  },
  {
    "title": "Nonlinear dynamics in pulse-modulated feedback drug dosing",
    "url": "http://arxiv.org/abs/2502.19878v1",
    "arxiv_id": "2502.19878v1",
    "authors": [
      "Alexander Medvedev",
      "Anton V. Proskurnikov",
      "Zhanybai T. Zhusubaliyev"
    ],
    "published": "2025-02-27T08:36:38+00:00",
    "summary": "Pulse-modulated feedback is utilized in drug dosing to mimic sustained over a longer period of time manual discrete dose administration, the latter is in contrast with continuous drug infusion. The intermittent mode of dosing calls for a hybrid (continuous-discrete) modeling of the closed-loop system, where the pharmacokinetics and pharmacodynamics of the drug are captured by differential equations whereas the control law is described by difference equations. Hybrid dynamics are highly nonlinear which complicates formal design of pulse-modulated feedback. This paper demonstrates complex nonlinear dynamical phenomena arising in a simple control system of dosing a neuromuscular blockade agent in anesthesia. Along with the nominal periodic regimen, undesirable nonlinear behaviors, i.e. periodic solutions of high multiplicity, multistability, as well as deterministic chaos, are shown to exist. It is concluded that design of feedback drug dosing algorithms based on a hybrid paradigm has to be informed by a thorough bifurcation analysis in order to secure patient safety."
  },
  {
    "title": "Empowering Social Service with AI: Insights from a Participatory Design Study with Practitioners",
    "url": "http://arxiv.org/abs/2502.19822v1",
    "arxiv_id": "2502.19822v1",
    "authors": [
      "Yugin Tan",
      "Kai Xin Soh",
      "Renwen Zhang",
      "Jungup Lee",
      "Han Meng",
      "Biswadeep Sen",
      "Yi-Chieh Lee"
    ],
    "published": "2025-02-27T06:50:05+00:00",
    "summary": "In social service, administrative burdens and decision-making challenges often hinder practitioners from performing effective casework. Generative AI (GenAI) offers significant potential to streamline these tasks, yet exacerbates concerns about overreliance, algorithmic bias, and loss of identity within the profession. We explore these issues through a two-stage participatory design study. We conducted formative co-design workshops (\\textit{n=27}) to create a prototype GenAI tool, followed by contextual inquiry sessions with practitioners (\\textit{n=24}) using the tool with real case data. We reveal opportunities for AI integration in documentation, assessment, and worker supervision, while highlighting risks related to GenAI limitations, skill retention, and client safety. Drawing comparisons with GenAI tools in other fields, we discuss design and usage guidelines for such tools in social service practice."
  },
  {
    "title": "Foot-In-The-Door: A Multi-turn Jailbreak for LLMs",
    "url": "http://arxiv.org/abs/2502.19820v1",
    "arxiv_id": "2502.19820v1",
    "authors": [
      "Zixuan Weng",
      "Xiaolong Jin",
      "Jinyuan Jia",
      "Xiangyu Zhang"
    ],
    "published": "2025-02-27T06:49:16+00:00",
    "summary": "Ensuring AI safety is crucial as large language models become increasingly integrated into real-world applications. A key challenge is jailbreak, where adversarial prompts bypass built-in safeguards to elicit harmful disallowed outputs. Inspired by psychological foot-in-the-door principles, we introduce FITD,a novel multi-turn jailbreak method that leverages the phenomenon where minor initial commitments lower resistance to more significant or more unethical transgressions.Our approach progressively escalates the malicious intent of user queries through intermediate bridge prompts and aligns the model's response by itself to induce toxic responses. Extensive experimental results on two jailbreak benchmarks demonstrate that FITD achieves an average attack success rate of 94% across seven widely used models, outperforming existing state-of-the-art methods. Additionally, we provide an in-depth analysis of LLM self-corruption, highlighting vulnerabilities in current alignment strategies and emphasizing the risks inherent in multi-turn interactions.The code is available at https://github.com/Jinxiaolong1129/Foot-in-the-door-Jailbreak ."
  },
  {
    "title": "Foot-In-The-Door: A Multi-turn Jailbreak for LLMs",
    "url": "http://arxiv.org/abs/2502.19820v2",
    "arxiv_id": "2502.19820v2",
    "authors": [
      "Zixuan Weng",
      "Xiaolong Jin",
      "Jinyuan Jia",
      "Xiangyu Zhang"
    ],
    "published": "2025-02-27T06:49:16+00:00",
    "summary": "Ensuring AI safety is crucial as large language models become increasingly integrated into real-world applications. A key challenge is jailbreak, where adversarial prompts bypass built-in safeguards to elicit harmful disallowed outputs. Inspired by psychological foot-in-the-door principles, we introduce FITD,a novel multi-turn jailbreak method that leverages the phenomenon where minor initial commitments lower resistance to more significant or more unethical transgressions. Our approach progressively escalates the malicious intent of user queries through intermediate bridge prompts and aligns the model's response by itself to induce toxic responses. Extensive experimental results on two jailbreak benchmarks demonstrate that FITD achieves an average attack success rate of 94% across seven widely used models, outperforming existing state-of-the-art methods. Additionally, we provide an in-depth analysis of LLM self-corruption, highlighting vulnerabilities in current alignment strategies and emphasizing the risks inherent in multi-turn interactions. The code is available at https://github.com/Jinxiaolong1129/Foot-in-the-door-Jailbreak."
  },
  {
    "title": "Automatic Linear Resource Bound Analysis for Rust via Prophecy Potentials",
    "url": "http://arxiv.org/abs/2502.19810v1",
    "arxiv_id": "2502.19810v1",
    "authors": [
      "Qihao Lian",
      "Di Wang"
    ],
    "published": "2025-02-27T06:35:40+00:00",
    "summary": "Rust has become a popular system programming language that strikes a balance between memory safety and performance. Rust's type system ensures the safety of low-level memory controls; however, a well-typed Rust program is not guaranteed to enjoy high performance. This article studies static analysis for resource consumption of Rust programs, aiming at understanding the performance of Rust programs. Although there have been tons of studies on static resource analysis, exploiting Rust's memory safety -- especially the borrow mechanisms and their properties -- to aid resource-bound analysis, remains unexplored. This article presents RaRust, a type-based linear resource-bound analysis for well-typed Rust programs. RaRust follows the methodology of automatic amortized resource analysis (AARA) to build a resource-aware type system. To support Rust's borrow mechanisms, including shared and mutable borrows, RaRust introduces shared and novel prophecy potentials to reason about borrows compositionally. To prove the soundness of RaRust, this article proposes Resource-Aware Borrow Calculus (RABC) as a variant of recently proposed Low-Level Borrow Calculus (LLBC). The experimental evaluation of a prototype implementation of RaRust demonstrates that RaRust is capable of inferring symbolic linear resource bounds for Rust programs featuring shared and mutable borrows, reborrows, heap-allocated data structures, loops, and recursion."
  },
  {
    "title": "R1-T1: Fully Incentivizing Translation Capability in LLMs via Reasoning Learning",
    "url": "http://arxiv.org/abs/2502.19735v1",
    "arxiv_id": "2502.19735v1",
    "authors": [
      "Minggui He",
      "Yilun Liu",
      "Shimin Tao",
      "Yuanchang Luo",
      "Hongyong Zeng",
      "Chang Su",
      "Li Zhang",
      "Hongxia Ma",
      "Daimeng Wei",
      "Weibin Meng",
      "Hao Yang",
      "Boxing Chen",
      "Osamu Yoshie"
    ],
    "published": "2025-02-27T03:57:00+00:00",
    "summary": "Despite recent breakthroughs in reasoning-enhanced large language models (LLMs) like DeepSeek-R1, incorporating inference-time reasoning into machine translation (MT), where human translators naturally employ structured, multi-layered reasoning chain-of-thoughts (CoTs), is yet underexplored. Existing methods either design a fixed CoT tailored for a specific MT sub-task (e.g., literature translation), or rely on synthesizing CoTs unaligned with humans and supervised fine-tuning (SFT) prone to catastrophic forgetting, limiting their adaptability to diverse translation scenarios. This paper introduces R1-Translator (R1-T1), a novel framework to achieve inference-time reasoning for general MT via reinforcement learning (RL) with human-aligned CoTs comprising six common patterns. Our approach pioneers three innovations: (1) extending reasoning-based translation beyond MT sub-tasks to six languages and diverse tasks (e.g., legal/medical domain adaptation, idiom resolution); (2) formalizing six expert-curated CoT templates that mirror hybrid human strategies like context-aware paraphrasing and back translation; and (3) enabling self-evolving CoT discovery and anti-forgetting adaptation through RL with KL-constrained rewards. Experimental results indicate a steady translation performance improvement in 21 languages and 80 translation directions on Flores-101 test set, especially on the 15 languages unseen from training, with its general multilingual abilities preserved compared with plain SFT."
  },
  {
    "title": "R1-T1: Fully Incentivizing Translation Capability in LLMs via Reasoning Learning",
    "url": "http://arxiv.org/abs/2502.19735v2",
    "arxiv_id": "2502.19735v2",
    "authors": [
      "Minggui He",
      "Yilun Liu",
      "Shimin Tao",
      "Yuanchang Luo",
      "Hongyong Zeng",
      "Chang Su",
      "Li Zhang",
      "Hongxia Ma",
      "Daimeng Wei",
      "Weibin Meng",
      "Hao Yang",
      "Boxing Chen",
      "Osamu Yoshie"
    ],
    "published": "2025-02-27T03:57:00+00:00",
    "summary": "Despite recent breakthroughs in reasoning-enhanced large language models (LLMs) like DeepSeek-R1, incorporating inference-time reasoning into machine translation (MT), where human translators naturally employ structured, multi-layered reasoning chain-of-thoughts (CoTs), is yet underexplored. Existing methods either design a fixed CoT tailored for a specific MT sub-task (e.g., literature translation), or rely on synthesizing CoTs unaligned with humans, limiting their adaptability to diverse translation scenarios. This paper introduces R1-Translator (R1-T1), a novel framework to achieve inference-time reasoning for general MT via reinforcement learning (RL) with human-aligned CoTs comprising six common patterns. Our approach pioneers three innovations: (1) extending reasoning-based translation beyond MT sub-tasks to six languages and diverse tasks (e.g., legal/medical domain adaptation, idiom resolution); (2) formalizing six expert-curated CoT templates that mirror hybrid human strategies like context-aware paraphrasing and back translation; and (3) enabling self-evolving CoT discovery through RL. Experimental results indicate a steady translation performance improvement in 11 languages and 40 translation directions on Flores-101 test set, especially on the languages unseen from training."
  },
  {
    "title": "Unveiling Security Weaknesses in Autonomous Driving Systems: An In-Depth Empirical Study",
    "url": "http://arxiv.org/abs/2502.19687v1",
    "arxiv_id": "2502.19687v1",
    "authors": [
      "Wenyuan Cheng",
      "Zengyang Li",
      "Peng Liang",
      "Ran Mo",
      "Hui Liu"
    ],
    "published": "2025-02-27T01:57:53+00:00",
    "summary": "The advent of Autonomous Driving Systems (ADS) has marked a significant shift towards intelligent transportation, with implications for public safety and traffic efficiency. While these systems integrate a variety of technologies and offer numerous benefits, their security is paramount, as vulnerabilities can have severe consequences for safety and trust. This study aims to systematically investigate potential security weaknesses in the codebases of prominent open-source ADS projects using CodeQL, a static code analysis tool. The goal is to identify common vulnerabilities, their distribution and persistence across versions to enhance the security of ADS. We selected three representative open-source ADS projects, Autoware, AirSim, and Apollo, based on their high GitHub star counts and Level 4 autonomous driving capabilities. Using CodeQL, we analyzed multiple versions of these projects to identify vulnerabilities, focusing on CWE categories such as CWE-190 (Integer Overflow or Wraparound) and CWE-20 (Improper Input Validation). We also tracked the lifecycle of these vulnerabilities across software versions. This approach allows us to systematically analyze vulnerabilities in projects, which has not been extensively explored in previous ADS research. Our analysis revealed that specific CWE categories, particularly CWE-190 (59.6%) and CWE-20 (16.1%), were prevalent across the selected ADS projects. These vulnerabilities often persisted for over six months, spanning multiple version iterations. The empirical assessment showed a direct link between the severity of these vulnerabilities and their tangible effects on ADS performance. These security issues among ADS still remain to be resolved. Our findings highlight the need for integrating static code analysis into ADS development to detect and mitigate common vulnerabilities."
  },
  {
    "title": "MICINet: Multi-Level Inter-Class Confusing Information Removal for Reliable Multimodal Classification",
    "url": "http://arxiv.org/abs/2502.19674v1",
    "arxiv_id": "2502.19674v1",
    "authors": [
      "Tong Zhang",
      "Shu Shen",
      "C. L. Philip Chen"
    ],
    "published": "2025-02-27T01:33:28+00:00",
    "summary": "Reliable multimodal learning in the presence of noisy data is a widely concerned issue, especially in safety-critical applications. Many reliable multimodal methods delve into addressing modality-specific or cross-modality noise. However, they fail to handle the coexistence of both types of noise efficiently. Moreover, the lack of comprehensive consideration for noise at both global and individual levels limits their reliability. To address these issues, a reliable multimodal classification method dubbed Multi-Level Inter-Class Confusing Information Removal Network (MICINet) is proposed. MICINet achieves the reliable removal of both types of noise by unifying them into the concept of Inter-class Confusing Information (\\textit{ICI}) and eliminating it at both global and individual levels. Specifically, MICINet first reliably learns the global \\textit{ICI} distribution through the proposed \\textbf{\\textit{Global \\textbf{ICI} Learning Module}}. Then, it introduces the \\textbf{\\textit{Global-guided Sample ICI Learning module}} to efficiently remove global-level \\textit{ICI} from sample features utilizing the learned global \\textit{ICI} distribution. Subsequently, the \\textbf{\\textit{Sample-adaptive Cross-modality Information Compensation module}} is designed to remove individual-level \\textit{ICI} from each sample reliably. This is achieved through interpretable cross-modality information compensation based on the complementary relationship between discriminative features and \\textit{ICI} and the perception of the relative quality of modalities introduced by the relative discriminative power. Experiments on four datasets demonstrate that MICINet outperforms other state-of-the-art reliable multimodal classification methods under various noise conditions."
  },
  {
    "title": "Med-RLVR: Emerging Medical Reasoning from a 3B base model via reinforcement Learning",
    "url": "http://arxiv.org/abs/2502.19655v1",
    "arxiv_id": "2502.19655v1",
    "authors": [
      "Sheng Zhang",
      "Qianchu Liu",
      "Guanghui Qin",
      "Tristan Naumann",
      "Hoifung Poon"
    ],
    "published": "2025-02-27T00:54:38+00:00",
    "summary": "Reinforcement learning from verifiable rewards (RLVR) has recently gained attention for its ability to elicit self-evolved reasoning capabilitie from base language models without explicit reasoning supervisions, as demonstrated by DeepSeek-R1. While prior work on RLVR has primarily focused on mathematical and coding domains, its applicability to other tasks and domains remains unexplored. In this work, we investigate whether medical reasoning can emerge from RLVR. We introduce Med-RLVR as an initial study of RLVR in the medical domain leveraging medical multiple-choice question answering (MCQA) data as verifiable labels. Our results demonstrate that RLVR is not only effective for math and coding but also extends successfully to medical question answering. Notably, Med-RLVR achieves performance comparable to traditional supervised fine-tuning (SFT) on in-distribution tasks while significantly improving out-of-distribution generalization, with an 8-point accuracy gain. Further analysis of training dynamics reveals that, with no explicit reasoning supervision, reasoning emerges from the 3B-parameter base model. These findings underscore the potential of RLVR in domains beyond math and coding, opening new avenues for its application in knowledge-intensive fields such as medicine."
  },
  {
    "title": "3D Nephrographic Image Synthesis in CT Urography with the Diffusion Model and Swin Transformer",
    "url": "http://arxiv.org/abs/2502.19623v1",
    "arxiv_id": "2502.19623v1",
    "authors": [
      "Hongkun Yu",
      "Syed Jamal Safdar Gardezi",
      "E. Jason Abel",
      "Daniel Shapiro",
      "Meghan G. Lubner",
      "Joshua Warner",
      "Matthew Smith",
      "Giuseppe Toia",
      "Lu Mao",
      "Pallavi Tiwari",
      "Andrew L. Wentland"
    ],
    "published": "2025-02-26T23:22:31+00:00",
    "summary": "Purpose: This study aims to develop and validate a method for synthesizing 3D nephrographic phase images in CT urography (CTU) examinations using a diffusion model integrated with a Swin Transformer-based deep learning approach. Materials and Methods: This retrospective study was approved by the local Institutional Review Board. A dataset comprising 327 patients who underwent three-phase CTU (mean $\\pm$ SD age, 63 $\\pm$ 15 years; 174 males, 153 females) was curated for deep learning model development. The three phases for each patient were aligned with an affine registration algorithm. A custom deep learning model coined dsSNICT (diffusion model with a Swin transformer for synthetic nephrographic phase images in CT) was developed and implemented to synthesize the nephrographic images. Performance was assessed using Peak Signal-to-Noise Ratio (PSNR), Structural Similarity Index (SSIM), Mean Absolute Error (MAE), and Fr\\'{e}chet Video Distance (FVD). Qualitative evaluation by two fellowship-trained abdominal radiologists was performed. Results: The synthetic nephrographic images generated by our proposed approach achieved high PSNR (26.3 $\\pm$ 4.4 dB), SSIM (0.84 $\\pm$ 0.069), MAE (12.74 $\\pm$ 5.22 HU), and FVD (1323). Two radiologists provided average scores of 3.5 for real images and 3.4 for synthetic images (P-value = 0.5) on a Likert scale of 1-5, indicating that our synthetic images closely resemble real images. Conclusion: The proposed approach effectively synthesizes high-quality 3D nephrographic phase images. This model can be used to reduce radiation dose in CTU by 33.3\\% without compromising image quality, which thereby enhances the safety and diagnostic utility of CT urography."
  },
  {
    "title": "No, of course I can! Refusal Mechanisms Can Be Exploited Using Harmless Fine-Tuning Data",
    "url": "http://arxiv.org/abs/2502.19537v1",
    "arxiv_id": "2502.19537v1",
    "authors": [
      "Joshua Kazdan",
      "Lisa Yu",
      "Rylan Schaeffer",
      "Chris Cundy",
      "Sanmi Koyejo",
      "Dvijotham Krishnamurthy"
    ],
    "published": "2025-02-26T20:20:01+00:00",
    "summary": "Leading language model (LM) providers like OpenAI and Google offer fine-tuning APIs that allow customers to adapt LMs for specific use cases. To prevent misuse, these LM providers implement filtering mechanisms to block harmful fine-tuning data. Consequently, adversaries seeking to produce unsafe LMs via these APIs must craft adversarial training data that are not identifiably harmful. We make three contributions in this context: 1. We show that many existing attacks that use harmless data to create unsafe LMs rely on eliminating model refusals in the first few tokens of their responses. 2. We show that such prior attacks can be blocked by a simple defense that pre-fills the first few tokens from an aligned model before letting the fine-tuned model fill in the rest. 3. We describe a new data-poisoning attack, ``No, Of course I Can Execute'' (NOICE), which exploits an LM's formulaic refusal mechanism to elicit harmful responses. By training an LM to refuse benign requests on the basis of safety before fulfilling those requests regardless, we are able to jailbreak several open-source models and a closed-source model (GPT-4o). We show an attack success rate (ASR) of 57% against GPT-4o; our attack earned a Bug Bounty from OpenAI. Against open-source models protected by simple defenses, we improve ASRs by an average of 3.25 times compared to the best performing previous attacks that use only harmless data. NOICE demonstrates the exploitability of repetitive refusal mechanisms and broadens understanding of the threats closed-source models face from harmless data."
  },
  {
    "title": "Can Language Models Falsify? Evaluating Algorithmic Reasoning with Counterexample Creation",
    "url": "http://arxiv.org/abs/2502.19414v1",
    "arxiv_id": "2502.19414v1",
    "authors": [
      "Shiven Sinha",
      "Shashwat Goel",
      "Ponnurangam Kumaraguru",
      "Jonas Geiping",
      "Matthias Bethge",
      "Ameya Prabhu"
    ],
    "published": "2025-02-26T18:58:13+00:00",
    "summary": "There is growing excitement about the potential of Language Models (LMs) to accelerate scientific discovery. Falsifying hypotheses is key to scientific progress, as it allows claims to be iteratively refined over time. This process requires significant researcher effort, reasoning, and ingenuity. Yet current benchmarks for LMs predominantly assess their ability to generate solutions rather than challenge them. We advocate for developing benchmarks that evaluate this inverse capability - creating counterexamples for subtly incorrect solutions. To demonstrate this approach, we start with the domain of algorithmic problem solving, where counterexamples can be evaluated automatically using code execution. Specifically, we introduce REFUTE, a dynamically updating benchmark that includes recent problems and incorrect submissions from programming competitions, where human experts successfully identified counterexamples. Our analysis finds that the best reasoning agents, even OpenAI o3-mini (high) with code execution feedback, can create counterexamples for only <9% of incorrect solutions in REFUTE, even though ratings indicate its ability to solve up to 48% of these problems from scratch. We hope our work spurs progress in evaluating and enhancing LMs' ability to falsify incorrect solutions - a capability that is crucial for both accelerating research and making models self-improve through reliable reflective reasoning."
  },
  {
    "title": "ARENA: Adaptive Risk-aware and Energy-efficient NAvigation for Multi-Objective 3D Infrastructure Inspection with a UAV",
    "url": "http://arxiv.org/abs/2502.19401v1",
    "arxiv_id": "2502.19401v1",
    "authors": [
      "David-Alexandre Poissant",
      "Alexis Lussier Desbiens",
      "Fran\u00e7ois Ferland",
      "Louis Petit"
    ],
    "published": "2025-02-26T18:50:49+00:00",
    "summary": "Autonomous robotic inspection missions require balancing multiple conflicting objectives while navigating near costly obstacles. Current multi-objective path planning (MOPP) methods struggle to adapt to evolving risks like localization errors, weather, battery state, and communication issues. This letter presents an Adaptive Risk-aware and Energy-efficient NAvigation (ARENA) MOPP approach for UAVs in complex 3D environments. Our method enables online trajectory adaptation by optimizing safety, time, and energy using 4D NURBS representation and a genetic-based algorithm to generate the Pareto front. A novel risk-aware voting algorithm ensures adaptivity. Simulations and real-world tests demonstrate the planner's ability to produce diverse, optimized trajectories covering 95% or more of the range defined by single-objective benchmarks and its ability to estimate power consumption with a mean error representing 14% of the full power range. The ARENA framework enhances UAV autonomy and reliability in critical, evolving 3D missions."
  },
  {
    "title": "TheoremExplainAgent: Towards Multimodal Explanations for LLM Theorem Understanding",
    "url": "http://arxiv.org/abs/2502.19400v1",
    "arxiv_id": "2502.19400v1",
    "authors": [
      "Max Ku",
      "Thomas Chong",
      "Jonathan Leung",
      "Krish Shah",
      "Alvin Yu",
      "Wenhu Chen"
    ],
    "published": "2025-02-26T18:50:09+00:00",
    "summary": "Understanding domain-specific theorems often requires more than just text-based reasoning; effective communication through structured visual explanations is crucial for deeper comprehension. While large language models (LLMs) demonstrate strong performance in text-based theorem reasoning, their ability to generate coherent and pedagogically meaningful visual explanations remains an open challenge. In this work, we introduce TheoremExplainAgent, an agentic approach for generating long-form theorem explanation videos (over 5 minutes) using Manim animations. To systematically evaluate multimodal theorem explanations, we propose TheoremExplainBench, a benchmark covering 240 theorems across multiple STEM disciplines, along with 5 automated evaluation metrics. Our results reveal that agentic planning is essential for generating detailed long-form videos, and the o3-mini agent achieves a success rate of 93.8% and an overall score of 0.77. However, our quantitative and qualitative studies show that most of the videos produced exhibit minor issues with visual element layout. Furthermore, multimodal explanations expose deeper reasoning flaws that text-based explanations fail to reveal, highlighting the importance of multimodal explanations."
  },
  {
    "title": "Can Large Language Models Detect Errors in Long Chain-of-Thought Reasoning?",
    "url": "http://arxiv.org/abs/2502.19361v1",
    "arxiv_id": "2502.19361v1",
    "authors": [
      "Yancheng He",
      "Shilong Li",
      "Jiaheng Liu",
      "Weixun Wang",
      "Xingyuan Bu",
      "Ge Zhang",
      "Zhongyuan Peng",
      "Zhaoxiang Zhang",
      "Wenbo Su",
      "Bo Zheng"
    ],
    "published": "2025-02-26T17:59:27+00:00",
    "summary": "Recently, o1-like models have drawn significant attention, where these models produce the long Chain-of-Thought (CoT) reasoning steps to improve the reasoning abilities of existing Large Language Models (LLMs). In this paper, to understand the qualities of these long CoTs and measure the critique abilities of existing LLMs on these long CoTs, we introduce the DeltaBench, including the generated long CoTs from different o1-like models (e.g., QwQ, DeepSeek-R1) for different reasoning tasks (e.g., Math, Code, General Reasoning), to measure the ability to detect errors in long CoT reasoning. Based on DeltaBench, we first perform fine-grained analysis of the generated long CoTs to discover the effectiveness and efficiency of different o1-like models. Then, we conduct extensive evaluations of existing process reward models (PRMs) and critic models to detect the errors of each annotated process, which aims to investigate the boundaries and limitations of existing PRMs and critic models. Finally, we hope that DeltaBench could guide developers to better understand the long CoT reasoning abilities of their models."
  },
  {
    "title": "Can Large Language Models Detect Errors in Long Chain-of-Thought Reasoning?",
    "url": "http://arxiv.org/abs/2502.19361v2",
    "arxiv_id": "2502.19361v2",
    "authors": [
      "Yancheng He",
      "Shilong Li",
      "Jiaheng Liu",
      "Weixun Wang",
      "Xingyuan Bu",
      "Ge Zhang",
      "Zhongyuan Peng",
      "Zhaoxiang Zhang",
      "Zhicheng Zheng",
      "Wenbo Su",
      "Bo Zheng"
    ],
    "published": "2025-02-26T17:59:27+00:00",
    "summary": "Recently, o1-like models have drawn significant attention, where these models produce the long Chain-of-Thought (CoT) reasoning steps to improve the reasoning abilities of existing Large Language Models (LLMs). In this paper, to understand the qualities of these long CoTs and measure the critique abilities of existing LLMs on these long CoTs, we introduce the DeltaBench, including the generated long CoTs from different o1-like models (e.g., QwQ, DeepSeek-R1) for different reasoning tasks (e.g., Math, Code, General Reasoning), to measure the ability to detect errors in long CoT reasoning. Based on DeltaBench, we first perform fine-grained analysis of the generated long CoTs to discover the effectiveness and efficiency of different o1-like models. Then, we conduct extensive evaluations of existing process reward models (PRMs) and critic models to detect the errors of each annotated process, which aims to investigate the boundaries and limitations of existing PRMs and critic models. Finally, we hope that DeltaBench could guide developers to better understand the long CoT reasoning abilities of their models."
  },
  {
    "title": "Anomaly Detection in Complex Dynamical Systems: A Systematic Framework Using Embedding Theory and Physics-Inspired Consistency",
    "url": "http://arxiv.org/abs/2502.19307v1",
    "arxiv_id": "2502.19307v1",
    "authors": [
      "Michael Somma",
      "Thomas Gallien",
      "Branka Stojanovic"
    ],
    "published": "2025-02-26T17:06:13+00:00",
    "summary": "Anomaly detection in complex dynamical systems is essential for ensuring reliability, safety, and efficiency in industrial and cyber-physical infrastructures. Predictive maintenance helps prevent costly failures, while cybersecurity monitoring has become critical as digitized systems face growing threats. Many of these systems exhibit oscillatory behaviors and bounded motion, requiring anomaly detection methods that capture structured temporal dependencies while adhering to physical consistency principles. In this work, we propose a system-theoretic approach to anomaly detection, grounded in classical embedding theory and physics-inspired consistency principles. We build upon the Fractal Whitney Embedding Prevalence Theorem, extending traditional embedding techniques to complex system dynamics. Additionally, we introduce state-derivative pairs as an embedding strategy to capture system evolution. To enforce temporal coherence, we develop a Temporal Differential Consistency Autoencoder (TDC-AE), incorporating a TDC-Loss that aligns the approximated derivatives of latent variables with their dynamic representations. We evaluate our method on the C-MAPSS dataset, a benchmark for turbofan aeroengine degradation. TDC-AE outperforms LSTMs and Transformers while achieving a 200x reduction in MAC operations, making it particularly suited for lightweight edge computing. Our findings support the hypothesis that anomalies disrupt stable system dynamics, providing a robust, interpretable signal for anomaly detection."
  },
  {
    "title": "Utility-Based Dose Optimization Approaches for Multiple-Dose Randomized Trial Designs Accounting for Multiple Endpoints",
    "url": "http://arxiv.org/abs/2502.19216v1",
    "arxiv_id": "2502.19216v1",
    "authors": [
      "Gina DAngelo",
      "Guannan Chen",
      "Di Ran"
    ],
    "published": "2025-02-26T15:18:17+00:00",
    "summary": "The initiation of dose optimization has driven a paradigm shift in oncology clinical trials to determine the optimal biological dose (OBD). Early-phase trials with randomized doses can facilitate additional investigation of the identified OBD in targeted populations by incorporating safety, efficacy, and biomarker data. To support dose comparison in such settings, we propose to extend the utility score-based approach (U-MET) and introduce the clinical utility index-based approach (CUI-MET) to account for multiple endpoints and doses. The utility-based dose optimization approach for multiple-dose randomized trial designs accounting for multiple endpoints and doses (U-MET-m) extends the U-MET, using a utility score to account for multiple endpoints jointly (e.g., toxicity-efficacy trade-off), while the CUI-MET uses a utility index to do this marginally. U-MET-m and CUI-MET use Bayesian inference within a hypothesis framework to compare utility metrics across doses to identify the OBD. Here we describe simulation studies and present an example to compare the U-MET-m design, CUI-MET, and empirical design. The U-MET-m design and CUI-MET were shown to have satisfactory operating characteristics for selecting the OBD. Based on these findings, we recommend using the U-MET-m and CUI-MET designs as the primary dose comparison approach or as supportive evidence to select the OBD."
  },
  {
    "title": "When Personalization Meets Reality: A Multi-Faceted Analysis of Personalized Preference Learning",
    "url": "http://arxiv.org/abs/2502.19158v1",
    "arxiv_id": "2502.19158v1",
    "authors": [
      "Yijiang River Dong",
      "Tiancheng Hu",
      "Yinhong Liu",
      "Ahmet \u00dcst\u00fcn",
      "Nigel Collier"
    ],
    "published": "2025-02-26T14:14:58+00:00",
    "summary": "While Reinforcement Learning from Human Feedback (RLHF) is widely used to align Large Language Models (LLMs) with human preferences, it typically assumes homogeneous preferences across users, overlooking diverse human values and minority viewpoints. Although personalized preference learning addresses this by tailoring separate preferences for individual users, the field lacks standardized methods to assess its effectiveness. We present a multi-faceted evaluation framework that measures not only performance but also fairness, unintended effects, and adaptability across varying levels of preference divergence. Through extensive experiments comparing eight personalization methods across three preference datasets, we demonstrate that performance differences between methods could reach 36% when users strongly disagree, and personalization can introduce up to 20% safety misalignment. These findings highlight the critical need for holistic evaluation approaches to advance the development of more effective and inclusive preference learning systems."
  },
  {
    "title": "Formal Verification of PLCs as a Service: A CERN-GSI Safety-Critical Case Study (extended version)",
    "url": "http://arxiv.org/abs/2502.19150v1",
    "arxiv_id": "2502.19150v1",
    "authors": [
      "Ignacio D. Lopez-Miguel",
      "Borja Fern\u00e1ndez Adiego",
      "Matias Salinas",
      "Christine Betz"
    ],
    "published": "2025-02-26T14:08:58+00:00",
    "summary": "The increased technological complexity and demand for software reliability require organizations to formally design and verify their safety-critical programs to minimize systematic failures. Formal methods are recommended by functional safety standards (e.g., by IEC 61511 for the process industry and by the generic IEC 61508) and play a crucial role. Their structured approach reduces ambiguity in system requirements, facilitating early error detection. This paper introduces a formal verification service for PLC (programmable logic controller) programs compliant with functional safety standards, providing external expertise to organizations while eliminating the need for extensive internal training. It offers a cost-effective solution to meet the rising demands for formal verification processes. The approach is extended to include modeling time-dependent, know-how-protected components, enabling formal verification of real safety-critical applications. A case study shows the application of PLC formal verification as a service provided by CERN in a safety-critical installation at the GSI particle accelerator facility."
  },
  {
    "title": "Multi-Agent Security Tax: Trading Off Security and Collaboration Capabilities in Multi-Agent Systems",
    "url": "http://arxiv.org/abs/2502.19145v1",
    "arxiv_id": "2502.19145v1",
    "authors": [
      "Pierre Peigne-Lefebvre",
      "Mikolaj Kniejski",
      "Filip Sondej",
      "Matthieu David",
      "Jason Hoelscher-Obermaier",
      "Christian Schroeder de Witt",
      "Esben Kran"
    ],
    "published": "2025-02-26T14:00:35+00:00",
    "summary": "As AI agents are increasingly adopted to collaborate on complex objectives, ensuring the security of autonomous multi-agent systems becomes crucial. We develop simulations of agents collaborating on shared objectives to study these security risks and security trade-offs. We focus on scenarios where an attacker compromises one agent, using it to steer the entire system toward misaligned outcomes by corrupting other agents. In this context, we observe infectious malicious prompts - the multi-hop spreading of malicious instructions. To mitigate this risk, we evaluated several strategies: two \"vaccination\" approaches that insert false memories of safely handling malicious input into the agents' memory stream, and two versions of a generic safety instruction strategy. While these defenses reduce the spread and fulfillment of malicious instructions in our experiments, they tend to decrease collaboration capability in the agent network. Our findings illustrate potential trade-off between security and collaborative efficiency in multi-agent systems, providing insights for designing more secure yet effective AI collaborations."
  },
  {
    "title": "A 106K Multi-Topic Multilingual Conversational User Dataset with Emoticons",
    "url": "http://arxiv.org/abs/2502.19108v1",
    "arxiv_id": "2502.19108v1",
    "authors": [
      "Heng Er Metilda Chee",
      "Jiayin Wang",
      "Zhiqiang Guo",
      "Weizhi Ma",
      "Qinglang Guo",
      "Min Zhang"
    ],
    "published": "2025-02-26T12:50:58+00:00",
    "summary": "Instant messaging has become a predominant form of communication, with texts and emoticons enabling users to express emotions and ideas efficiently. Emoticons, in particular, have gained significant traction as a medium for conveying sentiments and information, leading to the growing importance of emoticon retrieval and recommendation systems. However, one of the key challenges in this area has been the absence of datasets that capture both the temporal dynamics and user-specific interactions with emoticons, limiting the progress of personalized user modeling and recommendation approaches. To address this, we introduce the emoticon dataset, a comprehensive resource that includes time-based data along with anonymous user identifiers across different conversations. As the largest publicly accessible emoticon dataset to date, it comprises 22K unique users, 370K emoticons, and 8.3M messages. The data was collected from a widely-used messaging platform across 67 conversations and 720 hours of crawling. Strict privacy and safety checks were applied to ensure the integrity of both text and image data. Spanning across 10 distinct domains, the emoticon dataset provides rich insights into temporal, multilingual, and cross-domain behaviors, which were previously unavailable in other emoticon-based datasets. Our in-depth experiments, both quantitative and qualitative, demonstrate the dataset's potential in modeling user behavior and personalized recommendation systems, opening up new possibilities for research in personalized retrieval and conversational AI. The dataset is freely accessible."
  },
  {
    "title": "Nexus: A Lightweight and Scalable Multi-Agent Framework for Complex Tasks Automation",
    "url": "http://arxiv.org/abs/2502.19091v1",
    "arxiv_id": "2502.19091v1",
    "authors": [
      "Humza Sami",
      "Mubashir ul Islam",
      "Samy Charas",
      "Asav Gandhi",
      "Pierre-Emmanuel Gaillardon",
      "Valerio Tenace"
    ],
    "published": "2025-02-26T12:37:47+00:00",
    "summary": "Recent advancements in Large Language Models (LLMs) have substantially evolved Multi-Agent Systems (MASs) capabilities, enabling systems that not only automate tasks but also leverage near-human reasoning capabilities. To achieve this, LLM-based MASs need to be built around two critical principles: (i) a robust architecture that fully exploits LLM potential for specific tasks -- or related task sets -- and ($ii$) an effective methodology for equipping LLMs with the necessary capabilities to perform tasks and manage information efficiently. It goes without saying that a priori architectural designs can limit the scalability and domain adaptability of a given MAS.   To address these challenges, in this paper we introduce Nexus: a lightweight Python framework designed to easily build and manage LLM-based MASs. Nexus introduces the following innovations: (i) a flexible multi-supervisor hierarchy, (ii) a simplified workflow design, and (iii) easy installation and open-source flexibility: Nexus can be installed via pip and is distributed under a permissive open-source license, allowing users to freely modify and extend its capabilities.   Experimental results demonstrate that architectures built with Nexus exhibit state-of-the-art performance across diverse domains. In coding tasks, Nexus-driven MASs achieve a 99% pass rate on HumanEval and a flawless 100% on VerilogEval-Human, outperforming cutting-edge reasoning language models such as o3-mini and DeepSeek-R1. Moreover, these architectures display robust proficiency in complex reasoning and mathematical problem solving, achieving correct solutions for all randomly selected problems from the MATH dataset. In the realm of multi-objective optimization, Nexus-based architectures successfully address challenging timing closure tasks on designs from the VTR benchmark suite, while guaranteeing, on average, a power saving of nearly 30%."
  },
  {
    "title": "MathClean: A Benchmark for Synthetic Mathematical Data Cleaning",
    "url": "http://arxiv.org/abs/2502.19058v1",
    "arxiv_id": "2502.19058v1",
    "authors": [
      "Hao Liang",
      "Meiyi Qiang",
      "Yuying Li",
      "Zefeng He",
      "Yongzhen Guo",
      "Zhengzhou Zhu",
      "Wentao Zhang",
      "Bin Cui"
    ],
    "published": "2025-02-26T11:17:50+00:00",
    "summary": "With the rapid development of large language models (LLMs), the quality of training data has become crucial. Among the various types of training data, mathematical data plays a key role in enabling LLMs to acquire strong reasoning abilities. While high-quality open-source data is important, it is often insufficient for pre-training, necessitating the addition of synthetic math problems. However, synthetic math questions and answers can introduce inaccuracies, which may degrade both the training data and web data. Therefore, an effective method for cleaning synthetic math data is essential. In this paper, we propose the MathClean benchmark to evaluate the effectiveness of math data cleaning models. The MathClean benchmark consists of 2,000 correct questions and 2,000 erroneous questions with additional 2,000 correct and erroneous answers sourced from augmented data based on GSM8K and MATH. Moreover, we also annotate error types for each question or answer, since it can assess whether models can correctly identify the error categories for future improvements. Finally, we present comprehensive evaluations using state-of-the-art (SOTA) models. Our results demonstrate that even strong models like GPT-o1 and DeepSeek-R1 perform poorly on this benchmark, highlighting the utility of MathClean. Our code and data is available at https://github.com/YuYingLi0/MathClean."
  },
  {
    "title": "An Improved 3D Skeletons UP-Fall Dataset: Enhancing Data Quality for Efficient Impact Fall Detection",
    "url": "http://arxiv.org/abs/2502.19048v1",
    "arxiv_id": "2502.19048v1",
    "authors": [
      "Tresor Y. Koffi",
      "Youssef Mourchid",
      "Mohammed Hindawi",
      "Yohan Dupuis"
    ],
    "published": "2025-02-26T11:02:44+00:00",
    "summary": "Detecting impact where an individual makes contact with the ground within a fall event is crucial in fall detection systems, particularly for elderly care where prompt intervention can prevent serious injuries. The UP-Fall dataset, a key resource in fall detection research, has proven valuable but suffers from limitations in data accuracy and comprehensiveness. These limitations cause confusion in distinguishing between non-impact events, such as sliding, and real falls with impact, where the person actually hits the ground. This confusion compromises the effectiveness of current fall detection systems. This study presents enhancements to the UP-Fall dataset aiming at improving it for impact fall detection by incorporating 3D skeleton data. Our preprocessing techniques ensure high data accuracy and comprehensiveness, enabling a more reliable impact fall detection. Extensive experiments were conducted using various machine learning and deep learning algorithms to benchmark the improved 3D skeletons dataset. The results demonstrate substantial improvements in the performance of fall detection models trained on the enhanced dataset. This contribution aims to enhance the safety and well-being of the elderly population at risk. To support further research and development of building more reliable impact fall detection systems, we have made the improved 3D skeletons UP-Fall dataset publicly available at this link https://zenodo.org/records/12773013."
  },
  {
    "title": "Beyond Surface-Level Patterns: An Essence-Driven Defense Framework Against Jailbreak Attacks in LLMs",
    "url": "http://arxiv.org/abs/2502.19041v1",
    "arxiv_id": "2502.19041v1",
    "authors": [
      "Shiyu Xiang",
      "Ansen Zhang",
      "Yanfei Cao",
      "Yang Fan",
      "Ronghao Chen"
    ],
    "published": "2025-02-26T10:53:58+00:00",
    "summary": "Although Aligned Large Language Models (LLMs) are trained to refuse harmful requests, they remain vulnerable to jailbreak attacks. Unfortunately, existing methods often focus on surface-level patterns, overlooking the deeper attack essences. As a result, defenses fail when attack prompts change, even though the underlying \"attack essence\" remains the same. To address this issue, we introduce EDDF, an \\textbf{E}ssence-\\textbf{D}riven \\textbf{D}efense \\textbf{F}ramework Against Jailbreak Attacks in LLMs. EDDF is a plug-and-play input-filtering method and operates in two stages: 1) offline essence database construction, and 2) online adversarial query detection. The key idea behind EDDF is to extract the \"attack essence\" from a diverse set of known attack instances and store it in an offline vector database. Experimental results demonstrate that EDDF significantly outperforms existing methods by reducing the Attack Success Rate by at least 20\\%, underscoring its superior robustness against jailbreak attacks."
  },
  {
    "title": "Polarization Angle Scanning for Wide-band Millimeter-wave Direct Detection",
    "url": "http://arxiv.org/abs/2502.18981v1",
    "arxiv_id": "2502.18981v1",
    "authors": [
      "Heyao Wang",
      "Ziran Zhao",
      "Lingbo Qiao",
      "Dalu Guo"
    ],
    "published": "2025-02-26T09:43:09+00:00",
    "summary": "Millimeter-wave (MMW) technology has been widely utilized in human security screening applications due to its superior penetration capabilities through clothing and safety for human exposure. However, existing methods largely rely on fixed polarization modes, neglecting the potential insights from variations in target echoes with respect to incident polarization. This study provides a theoretical analysis of the cross-polarization echo power as a function of the incident polarization angle under linear polarization conditions. Additionally, based on the transmission characteristics of multi-layer medium, we extended the depth spectrum model employed in direct detection to accommodate scenarios involving multi-layered structures. Building on this foundation, by obtaining multiple depth spectrums through polarization angle scanning, we propose the Polarization Angle-Depth Matrix to characterize target across both the polarization angle and depth dimensions in direct detection. Simulations and experimental validations confirm its accuracy and practical value in detecting concealed weapons in human security screening scenarios."
  },
  {
    "title": "JailBench: A Comprehensive Chinese Security Assessment Benchmark for Large Language Models",
    "url": "http://arxiv.org/abs/2502.18935v1",
    "arxiv_id": "2502.18935v1",
    "authors": [
      "Shuyi Liu",
      "Simiao Cui",
      "Haoran Bu",
      "Yuming Shang",
      "Xi Zhang"
    ],
    "published": "2025-02-26T08:36:42+00:00",
    "summary": "Large language models (LLMs) have demonstrated remarkable capabilities across various applications, highlighting the urgent need for comprehensive safety evaluations. In particular, the enhanced Chinese language proficiency of LLMs, combined with the unique characteristics and complexity of Chinese expressions, has driven the emergence of Chinese-specific benchmarks for safety assessment. However, these benchmarks generally fall short in effectively exposing LLM safety vulnerabilities. To address the gap, we introduce JailBench, the first comprehensive Chinese benchmark for evaluating deep-seated vulnerabilities in LLMs, featuring a refined hierarchical safety taxonomy tailored to the Chinese context. To improve generation efficiency, we employ a novel Automatic Jailbreak Prompt Engineer (AJPE) framework for JailBench construction, which incorporates jailbreak techniques to enhance assessing effectiveness and leverages LLMs to automatically scale up the dataset through context-learning. The proposed JailBench is extensively evaluated over 13 mainstream LLMs and achieves the highest attack success rate against ChatGPT compared to existing Chinese benchmarks, underscoring its efficacy in identifying latent vulnerabilities in LLMs, as well as illustrating the substantial room for improvement in the security and trustworthiness of LLMs within the Chinese context. Our benchmark is publicly available at https://github.com/STAIR-BUPT/JailBench."
  },
  {
    "title": "Adaptive Shielding via Parametric Safety Proofs",
    "url": "http://arxiv.org/abs/2502.18879v1",
    "arxiv_id": "2502.18879v1",
    "authors": [
      "Yao Feng",
      "Jun Zhu",
      "Andr\u00e9 Platzer",
      "Jonathan Laurent"
    ],
    "published": "2025-02-26T06:50:48+00:00",
    "summary": "A major challenge to deploying cyber-physical systems with learning-enabled controllers is to ensure their safety, especially in the face of changing environments that necessitate runtime knowledge acquisition. Model-checking and automated reasoning have been successfully used for shielding, i.e., to monitor untrusted controllers and override potentially unsafe decisions, but only at the cost of hard tradeoffs in terms of expressivity, safety, adaptivity, precision and runtime efficiency. We propose a programming-language framework that allows experts to statically specify adaptive shields for learning-enabled agents, which enforce a safe control envelope that gets more permissive as knowledge is gathered at runtime. A shield specification provides a safety model that is parametric in the current agent's knowledge. In addition, a nondeterministic inference strategy can be specified using a dedicated domain-specific language, enforcing that such knowledge parameters are inferred at runtime in a statistically-sound way. By leveraging language design and theorem proving, our proposed framework empowers experts to design adaptive shields with an unprecedented level of modeling flexibility, while providing rigorous, end-to-end probabilistic safety guarantees."
  },
  {
    "title": "Investigating Generalization of One-shot LLM Steering Vectors",
    "url": "http://arxiv.org/abs/2502.18862v1",
    "arxiv_id": "2502.18862v1",
    "authors": [
      "Jacob Dunefsky",
      "Arman Cohan"
    ],
    "published": "2025-02-26T06:13:01+00:00",
    "summary": "Steering vectors have emerged as a promising approach for interpreting and controlling LLMs, but current methods typically require large contrastive datasets that are often impractical to construct and may capture spurious correlations. We propose directly optimizing steering vectors through gradient descent on a single training example, and systematically investigate how these vectors generalize. We consider several steering optimization techniques, including multiple novel ones, and find that the resulting vectors effectively mediate safety-relevant behaviors in multiple models. Indeed, in experiments on an alignment-faking model, we are able to optimize one-shot steering vectors that induce harmful behavior on benign examples and whose negations suppress harmful behavior on malign examples. And in experiments on refusal suppression, we demonstrate that one-shot optimized steering vectors can transfer across inputs, yielding a Harmbench attack success rate of 96.9%. Furthermore, to quantitatively assess steering effectiveness in instruction-tuned models, we develop a novel evaluation framework using sequence probabilities from the corresponding base model. With this framework, we analyze how steering vectors modulate an instruction-tuned LLM's ability to recover from outputting false information, and find that this ability derives from the base model. Overall, our findings suggest that optimizing steering vectors on a single example can mediate misaligned behavior in LLMs, and provide a path toward better understanding the relationship between LLM behavior and activation space structure."
  },
  {
    "title": "Safe and usable kernel extensions with Rax",
    "url": "http://arxiv.org/abs/2502.18832v1",
    "arxiv_id": "2502.18832v1",
    "authors": [
      "Jinghao Jia",
      "Ruowen Qin",
      "Milo Craun",
      "Egor Lukiyanov",
      "Ayush Bansal",
      "Michael V. Le",
      "Hubertus Franke",
      "Hani Jamjoom",
      "Tianyin Xu",
      "Dan Williams"
    ],
    "published": "2025-02-26T05:16:06+00:00",
    "summary": "Safe kernel extensions have gained significant traction, evolving from simple packet filters to large, complex programs that customize storage, networking, and scheduling. Existing kernel extension mechanisms like eBPF rely on in-kernel verifiers to ensure safety of kernel extensions by static verification using symbolic execution. We identify significant usability issues -- safe extensions being rejected by the verifier -- due to the language-verifier gap, a mismatch between developers' expectation of program safety provided by a contract with the programming language, and the verifier's expectation.   We present Rax, a new kernel extension framework that closes the language-verifier gap and improves the usability of kernel extensions in terms of programming experience and maintainability. Rax builds upon language-based safety to provide safety properties desired by kernel extensions, along with a lightweight extralingual runtime for properties that are unsuitable for static analysis, including safe exception handling, stack safety, and termination. With Rax, kernel extensions are written in safe Rust and interact with the kernel via a safe interface provided by Rax's kernel crate. No separate static verification is needed. Rax addresses usability issues of eBPF kernel extensions without compromising performance."
  },
  {
    "title": "Simulating Safe Bite Transfer in Robot-Assisted Feeding with a Soft Head and Articulated Jaw",
    "url": "http://arxiv.org/abs/2502.18749v1",
    "arxiv_id": "2502.18749v1",
    "authors": [
      "Yi Heng San",
      "Vasanthamaran Ravichandram",
      "J-Anne Yow",
      "Sherwin Stephen Chan",
      "Yifan Wang",
      "Wei Tech Ang"
    ],
    "published": "2025-02-26T01:52:04+00:00",
    "summary": "Ensuring safe and comfortable bite transfer during robot-assisted feeding is challenging due to the close physical human-robot interaction required. This paper presents a novel approach to modeling physical human-robot interaction in a physics-based simulator (MuJoCo) using soft-body dynamics. We integrate a flexible head model with a rigid skeleton while accounting for internal dynamics, enabling the flexible model to be actuated by the skeleton. Incorporating realistic soft-skin contact dynamics in simulation allows for systematically evaluating bite transfer parameters, such as insertion depth and entry angle, and their impact on user safety and comfort. Our findings suggest that a straight-in-straight-out strategy minimizes forces and enhances user comfort in robot-assisted feeding, assuming a static head. This simulation-based approach offers a safer and more controlled alternative to real-world experimentation. Supplementary videos can be found at: https://tinyurl.com/224yh2kx."
  },
  {
    "title": "AI Mismatches: Identifying Potential Algorithmic Harms Before AI Development",
    "url": "http://arxiv.org/abs/2502.18682v1",
    "arxiv_id": "2502.18682v1",
    "authors": [
      "Devansh Saxena",
      "Ji-Youn Jung",
      "Jodi Forlizzi",
      "Kenneth Holstein",
      "John Zimmerman"
    ],
    "published": "2025-02-25T22:43:00+00:00",
    "summary": "AI systems are often introduced with high expectations, yet many fail to deliver, resulting in unintended harm and missed opportunities for benefit. We frequently observe significant \"AI Mismatches\", where the system's actual performance falls short of what is needed to ensure safety and co-create value. These mismatches are particularly difficult to address once development is underway, highlighting the need for early-stage intervention. Navigating complex, multi-dimensional risk factors that contribute to AI Mismatches is a persistent challenge. To address it, we propose an AI Mismatch approach to anticipate and mitigate risks early on, focusing on the gap between realistic model performance and required task performance. Through an analysis of 774 AI cases, we extracted a set of critical factors, which informed the development of seven matrices that map the relationships between these factors and highlight high-risk areas. Through case studies, we demonstrate how our approach can help reduce risks in AI development."
  },
  {
    "title": "Provably Efficient RL for Linear MDPs under Instantaneous Safety Constraints in Non-Convex Feature Spaces",
    "url": "http://arxiv.org/abs/2502.18655v1",
    "arxiv_id": "2502.18655v1",
    "authors": [
      "Amirhossein Roknilamouki",
      "Arnob Ghosh",
      "Ming Shi",
      "Fatemeh Nourzad",
      "Eylem Ekici",
      "Ness B. Shroff"
    ],
    "published": "2025-02-25T21:32:55+00:00",
    "summary": "In Reinforcement Learning (RL), tasks with instantaneous hard constraints present significant challenges, particularly when the decision space is non-convex or non-star-convex. This issue is especially relevant in domains like autonomous vehicles and robotics, where constraints such as collision avoidance often take a non-convex form. In this paper, we establish a regret bound of $\\tilde{\\mathcal{O}}\\bigl(\\bigl(1 + \\tfrac{1}{\\tau}\\bigr) \\sqrt{\\log(\\tfrac{1}{\\tau}) d^3 H^4 K} \\bigr)$, applicable to both star-convex and non-star-convex cases, where $d$ is the feature dimension, $H$ the episode length, $K$ the number of episodes, and $\\tau$ the safety threshold. Moreover, the violation of safety constraints is zero with high probability throughout the learning process. A key technical challenge in these settings is bounding the covering number of the value-function class, which is essential for achieving value-aware uniform concentration in model-free function approximation. For the star-convex setting, we develop a novel technique called Objective Constraint-Decomposition (OCD) to properly bound the covering number. This result also resolves an error in a previous work on constrained RL. In non-star-convex scenarios, where the covering number can become infinitely large, we propose a two-phase algorithm, Non-Convex Safe Least Squares Value Iteration (NCS-LSVI), which first reduces uncertainty about the safe set by playing a known safe policy. After that, it carefully balances exploration and exploitation to achieve the regret bound. Finally, numerical simulations on an autonomous driving scenario demonstrate the effectiveness of NCS-LSVI."
  },
  {
    "title": "Faster, Cheaper, Better: Multi-Objective Hyperparameter Optimization for LLM and RAG Systems",
    "url": "http://arxiv.org/abs/2502.18635v1",
    "arxiv_id": "2502.18635v1",
    "authors": [
      "Matthew Barker",
      "Andrew Bell",
      "Evan Thomas",
      "James Carr",
      "Thomas Andrews",
      "Umang Bhatt"
    ],
    "published": "2025-02-25T20:52:06+00:00",
    "summary": "While Retrieval Augmented Generation (RAG) has emerged as a popular technique for improving Large Language Model (LLM) systems, it introduces a large number of choices, parameters and hyperparameters that must be made or tuned. This includes the LLM, embedding, and ranker models themselves, as well as hyperparameters governing individual RAG components. Yet, collectively optimizing the entire configuration in a RAG or LLM system remains under-explored - especially in multi-objective settings - due to intractably large solution spaces, noisy objective evaluations, and the high cost of evaluations. In this work, we introduce the first approach for multi-objective parameter optimization of cost, latency, safety and alignment over entire LLM and RAG systems. We find that Bayesian optimization methods significantly outperform baseline approaches, obtaining a superior Pareto front on two new RAG benchmark tasks. We conclude our work with important considerations for practitioners who are designing multi-objective RAG systems, highlighting nuances such as how optimal configurations may not generalize across tasks and objectives."
  },
  {
    "title": "Autonomous Vision-Guided Resection of Central Airway Obstruction",
    "url": "http://arxiv.org/abs/2502.18586v1",
    "arxiv_id": "2502.18586v1",
    "authors": [
      "M. E. Smith",
      "N. Yilmaz",
      "T. Watts",
      "P. M. Scheikl",
      "J. Ge",
      "A. Deguet",
      "A. Kuntz",
      "A. Krieger"
    ],
    "published": "2025-02-25T19:11:11+00:00",
    "summary": "Existing tracheal tumor resection methods often lack the precision required for effective airway clearance, and robotic advancements offer new potential for autonomous resection. We present a vision-guided, autonomous approach for palliative resection of tracheal tumors. This system models the tracheal surface with a fifth-degree polynomial to plan tool trajectories, while a custom Faster R-CNN segmentation pipeline identifies the trachea and tumor boundaries. The electrocautery tool angle is optimized using handheld surgical demonstrations, and trajectories are planned to maintain a 1 mm safety clearance from the tracheal surface. We validated the workflow successfully in five consecutive experiments on ex-vivo animal tissue models, successfully clearing the airway obstruction without trachea perforation in all cases (with more than 90% volumetric tumor removal). These results support the feasibility of an autonomous resection platform, paving the way for future developments in minimally-invasive autonomous resection."
  },
  {
    "title": "Evaluating the Effectiveness of Small Language Models in Detecting Refactoring Bugs",
    "url": "http://arxiv.org/abs/2502.18454v1",
    "arxiv_id": "2502.18454v1",
    "authors": [
      "Rohit Gheyi",
      "Marcio Ribeiro",
      "Jonhnanthan Oliveira"
    ],
    "published": "2025-02-25T18:52:28+00:00",
    "summary": "Popular IDEs frequently contain bugs in their refactoring implementations. Ensuring that a transformation preserves a program's behavior is a complex task. Traditional detection methods rely on predefined preconditions for each refactoring type, limiting their scalability and adaptability to new transformations. These methods often require extensive static and dynamic analyses, which are computationally expensive, time-consuming, and may still fail to detect certain refactoring bugs. This study evaluates the effectiveness of Small Language Models (SLMs) in detecting two types of refactoring bugs in Java and Python: (i) transformations that introduce errors or behavioral changes (Type I) and (ii) transformations unnecessarily blocked by IDEs despite being valid (Type II). We assess whether Llama 3.2 3B, Mistral 7B, Gemma 2 9B, DeepSeek-R1 14B, Phi-4 14B, o1-mini, and o3-mini-high can accurately detect 100 refactoring bugs reported in widely used Java and Python IDEs, such as Eclipse and NetBeans. The study covers 16 refactoring types and employs zero-shot prompting on consumer-grade hardware to evaluate the models' ability to reason about refactoring correctness without explicit prior training. The proprietary o3-mini-high model achieved the highest detection rate, identifying 84.3% of Type I bugs. The open-source Phi-4 14B performed comparably well, demonstrating strong effectiveness across both bug types. However, o3-mini-high struggled with Type II bugs, correctly identifying and applying valid but blocked transformations in only 40% of cases. The findings highlight the potential of SLMs for efficiently detecting refactoring bugs, particularly in verifying behavioral changes. Additionally, SLMs offer a more adaptable solution capable of generalizing across different refactoring types and programming languages, addressing key limitations of traditional approaches."
  },
  {
    "title": "Application of Attention Mechanism with Bidirectional Long Short-Term Memory (BiLSTM) and CNN for Human Conflict Detection using Computer Vision",
    "url": "http://arxiv.org/abs/2502.18555v1",
    "arxiv_id": "2502.18555v1",
    "authors": [
      "Erick da Silva Farias",
      "Eduardo Palhares Junior"
    ],
    "published": "2025-02-25T18:48:34+00:00",
    "summary": "The automatic detection of human conflicts through videos is a crucial area in computer vision, with significant applications in monitoring and public safety policies. However, the scarcity of public datasets and the complexity of human interactions make this task challenging. This study investigates the integration of advanced deep learning techniques, including Attention Mechanism, Convolutional Neural Networks (CNNs), and Bidirectional Long ShortTerm Memory (BiLSTM), to improve the detection of violent behaviors in videos. The research explores how the use of the attention mechanism can help focus on the most relevant parts of the video, enhancing the accuracy and robustness of the model. The experiments indicate that the combination of CNNs with BiLSTM and the attention mechanism provides a promising solution for conflict monitoring, offering insights into the effectiveness of different strategies. This work opens new possibilities for the development of automated surveillance systems that can operate more efficiently in real-time detection of violent events."
  },
  {
    "title": "SWE-RL: Advancing LLM Reasoning via Reinforcement Learning on Open Software Evolution",
    "url": "http://arxiv.org/abs/2502.18449v1",
    "arxiv_id": "2502.18449v1",
    "authors": [
      "Yuxiang Wei",
      "Olivier Duchenne",
      "Jade Copet",
      "Quentin Carbonneaux",
      "Lingming Zhang",
      "Daniel Fried",
      "Gabriel Synnaeve",
      "Rishabh Singh",
      "Sida I. Wang"
    ],
    "published": "2025-02-25T18:45:04+00:00",
    "summary": "The recent DeepSeek-R1 release has demonstrated the immense potential of reinforcement learning (RL) in enhancing the general reasoning capabilities of large language models (LLMs). While DeepSeek-R1 and other follow-up work primarily focus on applying RL to competitive coding and math problems, this paper introduces SWE-RL, the first approach to scale RL-based LLM reasoning for real-world software engineering. Leveraging a lightweight rule-based reward (e.g., the similarity score between ground-truth and LLM-generated solutions), SWE-RL enables LLMs to autonomously recover a developer's reasoning processes and solutions by learning from extensive open-source software evolution data -- the record of a software's entire lifecycle, including its code snapshots, code changes, and events such as issues and pull requests. Trained on top of Llama 3, our resulting reasoning model, Llama3-SWE-RL-70B, achieves a 41.0% solve rate on SWE-bench Verified -- a human-verified collection of real-world GitHub issues. To our knowledge, this is the best performance reported for medium-sized (<100B) LLMs to date, even comparable to leading proprietary LLMs like GPT-4o. Surprisingly, despite performing RL solely on software evolution data, Llama3-SWE-RL has even emerged with generalized reasoning skills. For example, it shows improved results on five out-of-domain tasks, namely, function coding, library use, code reasoning, mathematics, and general language understanding, whereas a supervised-finetuning baseline even leads to performance degradation on average. Overall, SWE-RL opens up a new direction to improve the reasoning capabilities of LLMs through reinforcement learning on massive software engineering data."
  },
  {
    "title": "Rank1: Test-Time Compute for Reranking in Information Retrieval",
    "url": "http://arxiv.org/abs/2502.18418v1",
    "arxiv_id": "2502.18418v1",
    "authors": [
      "Orion Weller",
      "Kathryn Ricci",
      "Eugene Yang",
      "Andrew Yates",
      "Dawn Lawrie",
      "Benjamin Van Durme"
    ],
    "published": "2025-02-25T18:14:06+00:00",
    "summary": "We introduce Rank1, the first reranking model trained to take advantage of test-time compute. Rank1 demonstrates the applicability within retrieval of using a reasoning language model (i.e. OpenAI's o1, Deepseek's R1, etc.) for distillation in order to rapidly improve the performance of a smaller model. We gather and open-source a dataset of more than 600,000 examples of R1 reasoning traces from queries and passages in MS MARCO. Models trained on this dataset show: (1) state-of-the-art performance on advanced reasoning and instruction following datasets; (2) work remarkably well out of distribution due to the ability to respond to user-input prompts; and (3) have explainable reasoning chains that can be given to users or RAG-based systems. Further, we demonstrate that quantized versions of these models retain strong performance while using less compute/memory. Overall, Rank1 shows that test-time compute allows for a fundamentally new type of explainable and performant reranker model for search."
  },
  {
    "title": "What is the Alignment Objective of GRPO?",
    "url": "http://arxiv.org/abs/2502.18548v1",
    "arxiv_id": "2502.18548v1",
    "authors": [
      "Milan Vojnovic",
      "Se-Young Yun"
    ],
    "published": "2025-02-25T15:56:56+00:00",
    "summary": "In this note, we examine the aggregation of preferences achieved by the Group Policy Optimisation (GRPO) algorithm, a reinforcement learning method used to train advanced artificial intelligence models such as DeepSeek-R1-Zero and DeepSeekMath. The GRPO algorithm trains a policy using a reward preference model, which is computed by sampling a set of outputs for a given context, observing the corresponding rewards, and applying shift-and-scale normalisation to these reward values. Additionally, it incorporates a penalty function to discourage deviations from a reference policy.   We present a framework that enables us to characterise the stationary policies of the GRPO algorithm. This analysis reveals that the aggregation of preferences differs fundamentally from standard logarithmic pooling, which is implemented by other approaches such as RLHF. The precise form of preference aggregation arises from the way the reward preference model is defined and from the penalty function, which we show to essentially correspond to the reverse Kullback-Leibler (KL) divergence between the aggregation policy and the reference policy.   Interestingly, we demonstrate that for groups of size two, the reward preference model corresponds to pairwise comparison preferences, similar to those in other alignment methods based on pairwise comparison feedback. We provide explicit characterisations of the aggregate preference for binary questions, for groups of size two, and in the limit of large group size. This provides insights into the dependence of the aggregate preference on parameters such as the regularisation constant and the confidence margin of question answers.   Finally, we discuss the aggregation of preferences obtained by modifying the GRPO algorithm to use direct KL divergence as the penalty or to use rewards without scale normalisation."
  },
  {
    "title": "What is the Alignment Objective of GRPO?",
    "url": "http://arxiv.org/abs/2502.18548v2",
    "arxiv_id": "2502.18548v2",
    "authors": [
      "Milan Vojnovic",
      "Se-Young Yun"
    ],
    "published": "2025-02-25T15:56:56+00:00",
    "summary": "In this note, we examine the aggregation of preferences achieved by the Group Policy Optimisation (GRPO) algorithm, a reinforcement learning method used to train advanced artificial intelligence models such as DeepSeek-R1-Zero and DeepSeekMath. The GRPO algorithm trains a policy using a reward preference model, which is computed by sampling a set of outputs for a given context, observing the corresponding rewards, and applying shift-and-scale normalisation to these reward values. Additionally, it incorporates a penalty function to discourage deviations from a reference policy.   We present a framework that enables us to characterise the stationary policies of the GRPO algorithm. This analysis reveals that the aggregation of preferences differs fundamentally from standard logarithmic pooling, which is implemented by other approaches such as RLHF. The precise form of preference aggregation arises from the way the reward preference model is defined and from the penalty function, which we show to essentially correspond to the reverse Kullback-Leibler (KL) divergence between the aggregation policy and the reference policy.   Interestingly, we demonstrate that for groups of size two, the reward preference model corresponds to pairwise comparison preferences, similar to those in other alignment methods based on pairwise comparison feedback. We provide explicit characterisations of the aggregate preference for binary questions, for groups of size two, and in the limit of large group size. This provides insights into the dependence of the aggregate preference on parameters such as the regularisation constant and the confidence margin of question answers.   Finally, we discuss the aggregation of preferences obtained by modifying the GRPO algorithm to use direct KL divergence as the penalty or to use rewards without scale normalisation."
  },
  {
    "title": "You Shall Not Pass: Warning Drivers of Unsafe Overtaking Maneuvers on Country Roads by Predicting Safe Sight Distance",
    "url": "http://arxiv.org/abs/2502.18163v1",
    "arxiv_id": "2502.18163v1",
    "authors": [
      "Adrian Bauske",
      "Arthur Fleig"
    ],
    "published": "2025-02-25T12:49:05+00:00",
    "summary": "Overtaking on country roads with possible opposing traffic is a dangerous maneuver and many proposed assistant systems assume car-to-car communication and sensors currently unavailable in cars. To overcome this limitation, we develop an assistant that uses simple in-car sensors to predict the required sight distance for safe overtaking. Our models predict this from vehicle speeds, accelerations, and 3D map data. In a user study with a Virtual Reality driving simulator (N=25), we compare two UI variants (monitoring-focused vs scheduling-focused). The results reveal that both UIs enable more patient driving and thus increase overall driving safety. While the monitoring-focused UI achieves higher System Usability Score and distracts drivers less, the preferred UI depends on personal preference. Driving data shows predictions were off at times. We investigate and discuss this in a comparison of our models to actual driving behavior and identify crucial model parameters and assumptions that significantly improve model predictions."
  },
  {
    "title": "Towards Thinking-Optimal Scaling of Test-Time Compute for LLM Reasoning",
    "url": "http://arxiv.org/abs/2502.18080v1",
    "arxiv_id": "2502.18080v1",
    "authors": [
      "Wenkai Yang",
      "Shuming Ma",
      "Yankai Lin",
      "Furu Wei"
    ],
    "published": "2025-02-25T10:48:05+00:00",
    "summary": "Recent studies have shown that making a model spend more time thinking through longer Chain of Thoughts (CoTs) enables it to gain significant improvements in complex reasoning tasks. While current researches continue to explore the benefits of increasing test-time compute by extending the CoT lengths of Large Language Models (LLMs), we are concerned about a potential issue hidden behind the current pursuit of test-time scaling: Would excessively scaling the CoT length actually bring adverse effects to a model's reasoning performance? Our explorations on mathematical reasoning tasks reveal an unexpected finding that scaling with longer CoTs can indeed impair the reasoning performance of LLMs in certain domains. Moreover, we discover that there exists an optimal scaled length distribution that differs across different domains. Based on these insights, we propose a Thinking-Optimal Scaling strategy. Our method first uses a small set of seed data with varying response length distributions to teach the model to adopt different reasoning efforts for deep thinking. Then, the model selects its shortest correct response under different reasoning efforts on additional problems for self-improvement. Our self-improved models built upon Qwen2.5-32B-Instruct outperform other distillation-based 32B o1-like models across various math benchmarks, and achieve performance on par with QwQ-32B-Preview."
  },
  {
    "title": "Exploring the Effects of Traditional Chinese Medicine Scents on Mitigating Driving Fatigue",
    "url": "http://arxiv.org/abs/2502.18013v1",
    "arxiv_id": "2502.18013v1",
    "authors": [
      "Nengyue Su",
      "Liang Luo",
      "Yu Gu",
      "Fuji Ren"
    ],
    "published": "2025-02-25T09:20:45+00:00",
    "summary": "The rise of autonomous driving technology has led to concerns about inactivity-induced fatigue. This paper explores Traditional Chinese Medicine (TCM) scents for mitigating. Two human-involved studies have been conducted in a high-fidelity driving simulator. Study 1 maps six prevalent TCM scents onto the arousal/valence circumplex to select proper candidates, i.e., argy wormwood (with the highest arousal) and tangerine peel (with the highest valence). Study 2 tests both scents in an auto-driving course. Statistics show both scents can improve driver alertness and reaction-time, but should be used in different ways: argy wormwood is suitable for short-term use due to its higher intensity but poor acceptance, while tangerine peel is ideal for long-term use due to its higher likeness. These findings provide insights for in-car fatigue mitigation to enhance driver safety and well-being. However, issues such as scent longevity as for aromatherapy and automatic fatigue prediction remain unresolved."
  },
  {
    "title": "InVDriver: Intra-Instance Aware Vectorized Query-Based Autonomous Driving Transformer",
    "url": "http://arxiv.org/abs/2502.17949v1",
    "arxiv_id": "2502.17949v1",
    "authors": [
      "Bo Zhang",
      "Heye Huang",
      "Chunyang Liu",
      "Yaqin Zhang",
      "Zhenhua Xu"
    ],
    "published": "2025-02-25T08:20:16+00:00",
    "summary": "End-to-end autonomous driving with its holistic optimization capabilities, has gained increasing traction in academia and industry. Vectorized representations, which preserve instance-level topological information while reducing computational overhead, have emerged as a promising paradigm. While existing vectorized query-based frameworks often overlook the inherent spatial correlations among intra-instance points, resulting in geometrically inconsistent outputs (e.g., fragmented HD map elements or oscillatory trajectories). To address these limitations, we propose InVDriver, a novel vectorized query-based system that systematically models intra-instance spatial dependencies through masked self-attention layers, thereby enhancing planning accuracy and trajectory smoothness. Across all core modules, i.e., perception, prediction, and planning, InVDriver incorporates masked self-attention mechanisms that restrict attention to intra-instance point interactions, enabling coordinated refinement of structural elements while suppressing irrelevant inter-instance noise. Experimental results on the nuScenes benchmark demonstrate that InVDriver achieves state-of-the-art performance, surpassing prior methods in both accuracy and safety, while maintaining high computational efficiency. Our work validates that explicit modeling of intra-instance geometric coherence is critical for advancing vectorized autonomous driving systems, bridging the gap between theoretical advantages of end-to-end frameworks and practical deployment requirements."
  },
  {
    "title": "DeepSeek-R1 Outperforms Gemini 2.0 Pro, OpenAI o1, and o3-mini in Bilingual Complex Ophthalmology Reasoning",
    "url": "http://arxiv.org/abs/2502.17947v1",
    "arxiv_id": "2502.17947v1",
    "authors": [
      "Pusheng Xu",
      "Yue Wu",
      "Kai Jin",
      "Xiaolan Chen",
      "Mingguang He",
      "Danli Shi"
    ],
    "published": "2025-02-25T08:08:53+00:00",
    "summary": "Purpose: To evaluate the accuracy and reasoning ability of DeepSeek-R1 and three other recently released large language models (LLMs) in bilingual complex ophthalmology cases. Methods: A total of 130 multiple-choice questions (MCQs) related to diagnosis (n = 39) and management (n = 91) were collected from the Chinese ophthalmology senior professional title examination and categorized into six topics. These MCQs were translated into English using DeepSeek-R1. The responses of DeepSeek-R1, Gemini 2.0 Pro, OpenAI o1 and o3-mini were generated under default configurations between February 15 and February 20, 2025. Accuracy was calculated as the proportion of correctly answered questions, with omissions and extra answers considered incorrect. Reasoning ability was evaluated through analyzing reasoning logic and the causes of reasoning error. Results: DeepSeek-R1 demonstrated the highest overall accuracy, achieving 0.862 in Chinese MCQs and 0.808 in English MCQs. Gemini 2.0 Pro, OpenAI o1, and OpenAI o3-mini attained accuracies of 0.715, 0.685, and 0.692 in Chinese MCQs (all P<0.001 compared with DeepSeek-R1), and 0.746 (P=0.115), 0.723 (P=0.027), and 0.577 (P<0.001) in English MCQs, respectively. DeepSeek-R1 achieved the highest accuracy across five topics in both Chinese and English MCQs. It also excelled in management questions conducted in Chinese (all P<0.05). Reasoning ability analysis showed that the four LLMs shared similar reasoning logic. Ignoring key positive history, ignoring key positive signs, misinterpretation medical data, and too aggressive were the most common causes of reasoning errors. Conclusion: DeepSeek-R1 demonstrated superior performance in bilingual complex ophthalmology reasoning tasks than three other state-of-the-art LLMs. While its clinical applicability remains challenging, it shows promise for supporting diagnosis and clinical decision-making."
  },
  {
    "title": "FetchBot: Object Fetching in Cluttered Shelves via Zero-Shot Sim2Real",
    "url": "http://arxiv.org/abs/2502.17894v1",
    "arxiv_id": "2502.17894v1",
    "authors": [
      "Weiheng Liu",
      "Yuxuan Wan",
      "Jilong Wang",
      "Yuxuan Kuang",
      "Xuesong Shi",
      "Haoran Li",
      "Dongbin Zhao",
      "Zhizheng Zhang",
      "He Wang"
    ],
    "published": "2025-02-25T06:32:42+00:00",
    "summary": "Object fetching from cluttered shelves is an important capability for robots to assist humans in real-world scenarios. Achieving this task demands robotic behaviors that prioritize safety by minimizing disturbances to surrounding objects, an essential but highly challenging requirement due to restricted motion space, limited fields of view, and complex object dynamics. In this paper, we introduce FetchBot, a sim-to-real framework designed to enable zero-shot generalizable and safety-aware object fetching from cluttered shelves in real-world settings. To address data scarcity, we propose an efficient voxel-based method for generating diverse simulated cluttered shelf scenes at scale and train a dynamics-aware reinforcement learning (RL) policy to generate object fetching trajectories within these scenes. This RL policy, which leverages oracle information, is subsequently distilled into a vision-based policy for real-world deployment. Considering that sim-to-real discrepancies stem from texture variations mostly while from geometric dimensions rarely, we propose to adopt depth information estimated by full-fledged depth foundation models as the input for the vision-based policy to mitigate sim-to-real gap. To tackle the challenge of limited views, we design a novel architecture for learning multi-view representations, allowing for comprehensive encoding of cluttered shelf scenes. This enables FetchBot to effectively minimize collisions while fetching objects from varying positions and depths, ensuring robust and safety-aware operation. Both simulation and real-robot experiments demonstrate FetchBot's superior generalization ability, particularly in handling a broad range of real-world scenarios, includ"
  },
  {
    "title": "LR${}^{2}$Bench: Evaluating Long-chain Reflective Reasoning Capabilities of Large Language Models via Constraint Satisfaction Problems",
    "url": "http://arxiv.org/abs/2502.17848v1",
    "arxiv_id": "2502.17848v1",
    "authors": [
      "Jianghao Chen",
      "Zhenlin Wei",
      "Zhenjiang Ren",
      "Ziyong Li",
      "Jiajun Zhang"
    ],
    "published": "2025-02-25T04:51:17+00:00",
    "summary": "Recent progress in o1-like models has significantly enhanced the reasoning abilities of Large Language Models (LLMs), empowering them to tackle increasingly complex tasks through reflection capabilities, such as making assumptions, backtracking, and self-refinement. However, effectively evaluating such reflection capabilities remains challenging due to the lack of appropriate benchmarks. To bridge this gap, we introduce LR${}^{2}$Bench, a novel benchmark designed to evaluate the Long-chain Reflective Reasoning capabilities of LLMs. LR${}^{2}$Bench comprises 850 samples across six Constraint Satisfaction Problems (CSPs) where reflective reasoning is crucial for deriving solutions that meet all given constraints. Each type of task focuses on distinct constraint patterns, such as knowledge-based, logical, and spatial constraints, providing a comprehensive evaluation of diverse problem-solving scenarios. We conduct extensive evaluation on both conventional models and o1-like models. Our experimental results reveal that even the most advanced reasoning-specific models, such as DeepSeek-R1 and OpenAI o1-preview, struggle with tasks in LR${}^{2}$Bench, achieving an average Exact Match score of only 20.0% and 23.6%, respectively. These findings underscore the significant room for improvement in the reflective reasoning capabilities of current LLMs. The leaderboard of our benchmark is available at https://huggingface.co/spaces/UltraRonin/LR2Bench"
  },
  {
    "title": "Impact of Object Weight in Handovers: Inspiring Robotic Grip Release and Motion from Human Handovers",
    "url": "http://arxiv.org/abs/2502.17834v1",
    "arxiv_id": "2502.17834v1",
    "authors": [
      "Parag Khanna",
      "M\u00e5rten Bj\u00f6rkman",
      "Christian Smith"
    ],
    "published": "2025-02-25T04:29:11+00:00",
    "summary": "This work explores the effect of object weight on human motion and grip release during handovers to enhance the naturalness, safety, and efficiency of robot-human interactions. We introduce adaptive robotic strategies based on the analysis of human handover behavior with varying object weights. The key contributions of this work includes the development of an adaptive grip-release strategy for robots, a detailed analysis of how object weight influences human motion to guide robotic motion adaptations, and the creation of handover-datasets incorporating various object weights, including the YCB handover dataset. By aligning robotic grip release and motion with human behavior, this work aims to improve robot-human handovers for different weighted objects. We also evaluate these human-inspired adaptive robotic strategies in robot-to-human handovers to assess their effectiveness and performance and demonstrate that they outperform the baseline approaches in terms of naturalness, efficiency, and user perception."
  },
  {
    "title": "MM-PoisonRAG: Disrupting Multimodal RAG with Local and Global Poisoning Attacks",
    "url": "http://arxiv.org/abs/2502.17832v1",
    "arxiv_id": "2502.17832v1",
    "authors": [
      "Hyeonjeong Ha",
      "Qiusi Zhan",
      "Jeonghwan Kim",
      "Dimitrios Bralios",
      "Saikrishna Sanniboina",
      "Nanyun Peng",
      "Kai-wei Chang",
      "Daniel Kang",
      "Heng Ji"
    ],
    "published": "2025-02-25T04:23:59+00:00",
    "summary": "Multimodal large language models (MLLMs) equipped with Retrieval Augmented Generation (RAG) leverage both their rich parametric knowledge and the dynamic, external knowledge to excel in tasks such as Question Answering. While RAG enhances MLLMs by grounding responses in query-relevant external knowledge, this reliance poses a critical yet underexplored safety risk: knowledge poisoning attacks, where misinformation or irrelevant knowledge is intentionally injected into external knowledge bases to manipulate model outputs to be incorrect and even harmful. To expose such vulnerabilities in multimodal RAG, we propose MM-PoisonRAG, a novel knowledge poisoning attack framework with two attack strategies: Localized Poisoning Attack (LPA), which injects query-specific misinformation in both text and images for targeted manipulation, and Globalized Poisoning Attack (GPA) to provide false guidance during MLLM generation to elicit nonsensical responses across all queries. We evaluate our attacks across multiple tasks, models, and access settings, demonstrating that LPA successfully manipulates the MLLM to generate attacker-controlled answers, with a success rate of up to 56% on MultiModalQA. Moreover, GPA completely disrupts model generation to 0% accuracy with just a single irrelevant knowledge injection. Our results highlight the urgent need for robust defenses against knowledge poisoning to safeguard multimodal RAG frameworks."
  },
  {
    "title": "Safe Multi-Agent Navigation guided by Goal-Conditioned Safe Reinforcement Learning",
    "url": "http://arxiv.org/abs/2502.17813v1",
    "arxiv_id": "2502.17813v1",
    "authors": [
      "Meng Feng",
      "Viraj Parimi",
      "Brian Williams"
    ],
    "published": "2025-02-25T03:38:52+00:00",
    "summary": "Safe navigation is essential for autonomous systems operating in hazardous environments. Traditional planning methods excel at long-horizon tasks but rely on a predefined graph with fixed distance metrics. In contrast, safe Reinforcement Learning (RL) can learn complex behaviors without relying on manual heuristics but fails to solve long-horizon tasks, particularly in goal-conditioned and multi-agent scenarios.   In this paper, we introduce a novel method that integrates the strengths of both planning and safe RL. Our method leverages goal-conditioned RL and safe RL to learn a goal-conditioned policy for navigation while concurrently estimating cumulative distance and safety levels using learned value functions via an automated self-training algorithm. By constructing a graph with states from the replay buffer, our method prunes unsafe edges and generates a waypoint-based plan that the agent follows until reaching its goal, effectively balancing faster and safer routes over extended distances.   Utilizing this unified high-level graph and a shared low-level goal-conditioned safe RL policy, we extend this approach to address the multi-agent safe navigation problem. In particular, we leverage Conflict-Based Search (CBS) to create waypoint-based plans for multiple agents allowing for their safe navigation over extended horizons. This integration enhances the scalability of goal-conditioned safe RL in multi-agent scenarios, enabling efficient coordination among agents.   Extensive benchmarking against state-of-the-art baselines demonstrates the effectiveness of our method in achieving distance goals safely for multiple agents in complex and hazardous environments. Our code will be released to support future research."
  },
  {
    "title": "DocPuzzle: A Process-Aware Benchmark for Evaluating Realistic Long-Context Reasoning Capabilities",
    "url": "http://arxiv.org/abs/2502.17807v1",
    "arxiv_id": "2502.17807v1",
    "authors": [
      "Tianyi Zhuang",
      "Chuqiao Kuang",
      "Xiaoguang Li",
      "Yihua Teng",
      "Jihao Wu",
      "Yasheng Wang",
      "Lifeng Shang"
    ],
    "published": "2025-02-25T03:29:53+00:00",
    "summary": "We present DocPuzzle, a rigorously constructed benchmark for evaluating long-context reasoning capabilities in large language models (LLMs). This benchmark comprises 100 expert-level QA problems requiring multi-step reasoning over long real-world documents. To ensure the task quality and complexity, we implement a human-AI collaborative annotation-validation pipeline. DocPuzzle introduces an innovative evaluation framework that mitigates guessing bias through checklist-guided process analysis, establishing new standards for assessing reasoning capacities in LLMs. Our evaluation results show that: 1)Advanced slow-thinking reasoning models like o1-preview(69.7%) and DeepSeek-R1(66.3%) significantly outperform best general instruct models like Claude 3.5 Sonnet(57.7%); 2)Distilled reasoning models like DeepSeek-R1-Distill-Qwen-32B(41.3%) falls far behind the teacher model, suggesting challenges to maintain the generalization of reasoning capabilities relying solely on distillation."
  },
  {
    "title": "Exploring the Potential of Large Language Models for Estimating the Reading Comprehension Question Difficulty",
    "url": "http://arxiv.org/abs/2502.17785v1",
    "arxiv_id": "2502.17785v1",
    "authors": [
      "Yoshee Jain",
      "John Hollander",
      "Amber He",
      "Sunny Tang",
      "Liang Zhang",
      "John Sabatini"
    ],
    "published": "2025-02-25T02:28:48+00:00",
    "summary": "Reading comprehension is a key for individual success, yet the assessment of question difficulty remains challenging due to the extensive human annotation and large-scale testing required by traditional methods such as linguistic analysis and Item Response Theory (IRT). While these robust approaches provide valuable insights, their scalability is limited. There is potential for Large Language Models (LLMs) to automate question difficulty estimation; however, this area remains underexplored. Our study investigates the effectiveness of LLMs, specifically OpenAI's GPT-4o and o1, in estimating the difficulty of reading comprehension questions using the Study Aid and Reading Assessment (SARA) dataset. We evaluated both the accuracy of the models in answering comprehension questions and their ability to classify difficulty levels as defined by IRT. The results indicate that, while the models yield difficulty estimates that align meaningfully with derived IRT parameters, there are notable differences in their sensitivity to extreme item characteristics. These findings suggest that LLMs can serve as the scalable method for automated difficulty assessment, particularly in dynamic interactions between learners and Adaptive Instructional Systems (AIS), bridging the gap between traditional psychometric techniques and modern AIS for reading comprehension and paving the way for more adaptive and personalized educational assessments."
  },
  {
    "title": "GPUArmor: A Hardware-Software Co-design for Efficient and Scalable Memory Safety on GPUs",
    "url": "http://arxiv.org/abs/2502.17780v1",
    "arxiv_id": "2502.17780v1",
    "authors": [
      "Mohamed Tarek Ibn Ziad",
      "Sana Damani",
      "Mark Stephenson",
      "Stephen W. Keckler",
      "Aamer Jaleel"
    ],
    "published": "2025-02-25T02:15:06+00:00",
    "summary": "Memory safety errors continue to pose a significant threat to current computing systems, and graphics processing units (GPUs) are no exception. A prominent class of memory safety algorithms is allocation-based solutions. The key idea is to maintain each allocation's metadata (base address and size) in a disjoint table and retrieve it at runtime to verify memory accesses. While several previous solutions have adopted allocation-based algorithms (e.g., cuCatch and GPUShield), they typically suffer from high memory overheads or scalability problems. In this work, we examine the key characteristics of real-world GPU workloads and observe several differences between GPU and CPU applications regarding memory access patterns, memory footprint, number of live allocations, and active allocation working set. Our observations motivate GPUArmor, a hardware-software co-design framework for memory safety on GPUs. We show that a simple compiler analysis combined with lightweight hardware support using a small Memory Lookaside Buffer (MLB) can help prevent spatial and temporal memory violations on modern GPU workloads with 2.3% average run time overheads. More importantly, GPUArmor achieves speed-of-light performance with negligible storage requirements. This result benefits both base and bounds solutions and memory tagging techniques, which we showcase with GPUArmor-HWOnly, a variation of GPUArmor that does not require recompilation, and achieves 2.2% slowdowns while significantly reducing storage overheads beyond traditional memory tagging approaches."
  },
  {
    "title": "GPUArmor: A Hardware-Software Co-design for Efficient and Scalable Memory Safety on GPUs",
    "url": "http://arxiv.org/abs/2502.17780v2",
    "arxiv_id": "2502.17780v2",
    "authors": [
      "Mohamed Tarek Ibn Ziad",
      "Sana Damani",
      "Mark Stephenson",
      "Stephen W. Keckler",
      "Aamer Jaleel"
    ],
    "published": "2025-02-25T02:15:06+00:00",
    "summary": "Memory safety errors continue to pose a significant threat to current computing systems, and graphics processing units (GPUs) are no exception. A prominent class of memory safety algorithms is allocation-based solutions. The key idea is to maintain each allocation's metadata (base address and size) in a disjoint table and retrieve it at runtime to verify memory accesses. While several previous solutions have adopted allocation-based algorithms (e.g., cuCatch and GPUShield), they typically suffer from high memory overheads or scalability problems. In this work, we examine the key characteristics of real-world GPU workloads and observe several differences between GPU and CPU applications regarding memory access patterns, memory footprint, number of live allocations, and active allocation working set. Our observations motivate GPUArmor, a hardware-software co-design framework for memory safety on GPUs. We show that a simple compiler analysis combined with lightweight hardware support using a small Memory Lookaside Buffer (MLB) can help prevent spatial and temporal memory violations on modern GPU workloads with 2.3% average run time overheads. More importantly, GPUArmor achieves speed-of-light performance with negligible storage requirements. This result benefits both base and bounds solutions and memory tagging techniques, which we showcase with GPUArmor-HWOnly, a variation of GPUArmor that does not require recompilation, and achieves 2.2% slowdowns while significantly reducing storage overheads beyond traditional memory tagging approaches."
  },
  {
    "title": "Design of a Breakaway Utensil Attachment for Enhanced Safety in Robot-Assisted Feeding",
    "url": "http://arxiv.org/abs/2502.17774v1",
    "arxiv_id": "2502.17774v1",
    "authors": [
      "Hau Wen Chang",
      "J-Anne Yow",
      "Lek Syn Lim",
      "Wei Tech Ang"
    ],
    "published": "2025-02-25T02:09:32+00:00",
    "summary": "Robot-assisted feeding systems enhance the independence of individuals with motor impairments and alleviate caregiver burden. While existing systems predominantly rely on software-based safety features to mitigate risks during unforeseen collisions, this study explores the use of a mechanical fail-safe to improve safety. We designed a breakaway utensil attachment that decouples forces exerted by the robot on the user when excessive forces occur. Finite element analysis (FEA) simulations were performed to predict failure points under various loading conditions, followed by experimental validation using 3D-printed attachments with variations in slot depth and wall loops. To facilitate testing, a drop test rig was developed and validated. Our results demonstrated a consistent failure point at the slot of the attachment, with a slot depth of 1 mm and three wall loops achieving failure at the target force of 65 N. Additionally, the parameters can be tailored to customize the breakaway force based on user-specific factors, such as comfort and pain tolerance. CAD files and utensil assembly instructions can be found here: https://tinyurl.com/rfa-utensil-attachment"
  },
  {
    "title": "DeepSeek vs. ChatGPT: A Comparative Study for Scientific Computing and Scientific Machine Learning Tasks",
    "url": "http://arxiv.org/abs/2502.17764v1",
    "arxiv_id": "2502.17764v1",
    "authors": [
      "Qile Jiang",
      "Zhiwei Gao",
      "George Em Karniadakis"
    ],
    "published": "2025-02-25T01:49:50+00:00",
    "summary": "Large Language Models (LLMs) have emerged as powerful tools for tackling a wide range of problems, including those in scientific computing, particularly in solving partial differential equations (PDEs). However, different models exhibit distinct strengths and preferences, resulting in varying levels of performance. In this paper, we compare the capabilities of the most advanced LLMs--ChatGPT and DeepSeek--along with their reasoning-optimized versions in addressing computational challenges. Specifically, we evaluate their proficiency in solving traditional numerical problems in scientific computing as well as leveraging scientific machine learning techniques for PDE-based problems. We designed all our experiments so that a non-trivial decision is required, e.g. defining the proper space of input functions for neural operator learning. Our findings reveal that the latest model, ChatGPT o3-mini-high, usually delivers the most accurate results while also responding significantly faster than its reasoning counterpart, DeepSeek R1. This enhanced speed and accuracy make ChatGPT o3-mini-high a more practical and efficient choice for diverse computational tasks at this juncture."
  },
  {
    "title": "Heterogeneous Decision Making in Mixed Traffic: Uncertainty-aware Planning and Bounded Rationality",
    "url": "http://arxiv.org/abs/2502.18529v1",
    "arxiv_id": "2502.18529v1",
    "authors": [
      "Hang Wang",
      "Qiaoyi Fang",
      "Junshan Zhang"
    ],
    "published": "2025-02-25T00:32:33+00:00",
    "summary": "The past few years have witnessed a rapid growth of the deployment of automated vehicles (AVs). Clearly, AVs and human-driven vehicles (HVs) will co-exist for many years, and AVs will have to operate around HVs, pedestrians, cyclists, and more, calling for fundamental breakthroughs in AI designed for mixed traffic to achieve mixed autonomy. Thus motivated, we study heterogeneous decision making by AVs and HVs in a mixed traffic environment, aiming to capture the interactions between human and machine decision-making and develop an AI foundation that enables vehicles to operate safely and efficiently. There are a number of challenges to achieve mixed autonomy, including 1) humans drivers make driving decisions with bounded rationality, and it remains open to develop accurate models for HVs' decision making; and 2) uncertainty-aware planning plays a critical role for AVs to take safety maneuvers in response to the human behavior. In this paper, we introduce a formulation of AV-HV interaction, where the HV makes decisions with bounded rationality and the AV employs uncertainty-aware planning based on the prediction on HV's future actions. We conduct a comprehensive analysis on AV and HV's learning regret to answer the questions: 1) {How does the learning performance depend on HV's bounded rationality and AV's planning}; 2) {How do different decision making strategies impact the overall learning performance}? Our findings reveal some intriguing phenomena, such as Goodhart's Law in AV's learning performance and compounding effects in HV's decision making process. By examining the dynamics of the regrets, we gain insights into the interplay between human and machine decision making."
  },
  {
    "title": "Mind the Gesture: Evaluating AI Sensitivity to Culturally Offensive Non-Verbal Gestures",
    "url": "http://arxiv.org/abs/2502.17710v1",
    "arxiv_id": "2502.17710v1",
    "authors": [
      "Akhila Yerukola",
      "Saadia Gabriel",
      "Nanyun Peng",
      "Maarten Sap"
    ],
    "published": "2025-02-24T23:10:08+00:00",
    "summary": "Gestures are an integral part of non-verbal communication, with meanings that vary across cultures, and misinterpretations that can have serious social and diplomatic consequences. As AI systems become more integrated into global applications, ensuring they do not inadvertently perpetuate cultural offenses is critical. To this end, we introduce Multi-Cultural Set of Inappropriate Gestures and Nonverbal Signs (MC-SIGNS), a dataset of 288 gesture-country pairs annotated for offensiveness, cultural significance, and contextual factors across 25 gestures and 85 countries. Through systematic evaluation using MC-SIGNS, we uncover critical limitations: text-to-image (T2I) systems exhibit strong US-centric biases, performing better at detecting offensive gestures in US contexts than in non-US ones; large language models (LLMs) tend to over-flag gestures as offensive; and vision-language models (VLMs) default to US-based interpretations when responding to universal concepts like wishing someone luck, frequently suggesting culturally inappropriate gestures. These findings highlight the urgent need for culturally-aware AI safety mechanisms to ensure equitable global deployment of AI technologies."
  },
  {
    "title": "Data-Driven Input-Output Control Barrier Functions",
    "url": "http://arxiv.org/abs/2502.17688v1",
    "arxiv_id": "2502.17688v1",
    "authors": [
      "MMohammad Bajelani",
      "Klaske van Heusden"
    ],
    "published": "2025-02-24T22:16:27+00:00",
    "summary": "Control Barrier Functions (CBFs) offer a framework for ensuring set invariance and designing constrained control laws. However, crafting a valid CBF relies on system-specific assumptions and the availability of an accurate system model, underscoring the need for systematic data-driven synthesis methods. This paper introduces a data-driven approach to synthesizing a CBF for discrete-time LTI systems using only input-output measurements. The method begins by computing the maximal control invariant set using an input-output data-driven representation, eliminating the need for precise knowledge of the system's order and explicit state estimation. The proposed CBF is then systematically derived from this set, which can accommodate multiple input-output constraints. Furthermore, the proposed CBF is leveraged to develop a minimally invasive safety filter that ensures recursive feasibility with an adaptive decay rate. To improve clarity, we assume a noise-free dataset, though data-driven control techniques can be used to robustify the approach. Finally, the effectiveness of the proposed method is demonstrated on an unknown time-delay system."
  },
  {
    "title": "Architecting Digital Twins for Intelligent Transportation Systems",
    "url": "http://arxiv.org/abs/2502.17646v1",
    "arxiv_id": "2502.17646v1",
    "authors": [
      "Hiya Bhatt",
      "Sahil",
      "Karthik Vaidhyanathan",
      "Rahul Biju",
      "Deepak Gangadharan",
      "Ramona Trestian",
      "Purav Shah"
    ],
    "published": "2025-02-24T20:51:09+00:00",
    "summary": "Modern transportation systems face growing challenges in managing traffic flow, ensuring safety, and maintaining operational efficiency amid dynamic traffic patterns. Addressing these challenges requires intelligent solutions capable of real-time monitoring, predictive analytics, and adaptive control. This paper proposes an architecture for DigIT, a Digital Twin (DT) platform for Intelligent Transportation Systems (ITS), designed to overcome the limitations of existing frameworks by offering a modular and scalable solution for traffic management. Built on a Domain Concept Model (DCM), the architecture systematically models key ITS components enabling seamless integration of predictive modeling and simulations. The architecture leverages machine learning models to forecast traffic patterns based on historical and real-time data. To adapt to evolving traffic patterns, the architecture incorporates adaptive Machine Learning Operations (MLOps), automating the deployment and lifecycle management of predictive models. Evaluation results highlight the effectiveness of the architecture in delivering accurate predictions and computational efficiency."
  },
  {
    "title": "Hallucination Detection in LLMs Using Spectral Features of Attention Maps",
    "url": "http://arxiv.org/abs/2502.17598v1",
    "arxiv_id": "2502.17598v1",
    "authors": [
      "Jakub Binkowski",
      "Denis Janiak",
      "Albert Sawczyn",
      "Bogdan Gabrys",
      "Tomasz Kajdanowicz"
    ],
    "published": "2025-02-24T19:30:24+00:00",
    "summary": "Large Language Models (LLMs) have demonstrated remarkable performance across various tasks but remain prone to hallucinations. Detecting hallucinations is essential for safety-critical applications, and recent methods leverage attention map properties to this end, though their effectiveness remains limited. In this work, we investigate the spectral features of attention maps by interpreting them as adjacency matrices of graph structures. We propose the $\\text{LapEigvals}$ method, which utilises the top-$k$ eigenvalues of the Laplacian matrix derived from the attention maps as an input to hallucination detection probes. Empirical evaluations demonstrate that our approach achieves state-of-the-art hallucination detection performance among attention-based methods. Extensive ablation studies further highlight the robustness and generalisation of $\\text{LapEigvals}$, paving the way for future advancements in the hallucination detection domain."
  },
  {
    "title": "How Do Large Language Monkeys Get Their Power (Laws)?",
    "url": "http://arxiv.org/abs/2502.17578v1",
    "arxiv_id": "2502.17578v1",
    "authors": [
      "Rylan Schaeffer",
      "Joshua Kazdan",
      "John Hughes",
      "Jordan Juravsky",
      "Sara Price",
      "Aengus Lynch",
      "Erik Jones",
      "Robert Kirk",
      "Azalia Mirhoseini",
      "Sanmi Koyejo"
    ],
    "published": "2025-02-24T19:01:47+00:00",
    "summary": "Recent research across mathematical problem solving, proof assistant programming and multimodal jailbreaking documents a striking finding: when (multimodal) language model tackle a suite of tasks with multiple attempts per task -- succeeding if any attempt is correct -- then the negative log of the average success rate scales a power law in the number of attempts. In this work, we identify an apparent puzzle: a simple mathematical calculation predicts that on each problem, the failure rate should fall exponentially with the number of attempts. We confirm this prediction empirically, raising a question: from where does aggregate polynomial scaling emerge? We then answer this question by demonstrating per-problem exponential scaling can be made consistent with aggregate polynomial scaling if the distribution of single-attempt success probabilities is heavy tailed such that a small fraction of tasks with extremely low success probabilities collectively warp the aggregate success trend into a power law - even as each problem scales exponentially on its own. We further demonstrate that this distributional perspective explains previously observed deviations from power law scaling, and provides a simple method for forecasting the power law exponent with an order of magnitude lower relative error, or equivalently, ${\\sim}2-4$ orders of magnitude less inference compute. Overall, our work contributes to a better understanding of how neural language model performance improves with scaling inference compute and the development of scaling-predictable evaluations of (multimodal) language models."
  },
  {
    "title": "LongSpec: Long-Context Speculative Decoding with Efficient Drafting and Verification",
    "url": "http://arxiv.org/abs/2502.17421v1",
    "arxiv_id": "2502.17421v1",
    "authors": [
      "Penghui Yang",
      "Cunxiao Du",
      "Fengzhuo Zhang",
      "Haonan Wang",
      "Tianyu Pang",
      "Chao Du",
      "Bo An"
    ],
    "published": "2025-02-24T18:53:31+00:00",
    "summary": "Speculative decoding has become a promising technique to mitigate the high inference latency of autoregressive decoding in Large Language Models (LLMs). Despite its promise, the effective application of speculative decoding in LLMs still confronts three key challenges: the increasing memory demands of the draft model, the distribution shift between the short-training corpora and long-context inference, and inefficiencies in attention implementation. In this work, we enhance the performance of speculative decoding in long-context settings by addressing these challenges. First, we propose a memory-efficient draft model with a constant-sized Key-Value (KV) cache. Second, we introduce novel position indices for short-training data, enabling seamless adaptation from short-context training to long-context inference. Finally, we present an innovative attention aggregation method that combines fast implementations for prefix computation with standard attention for tree mask handling, effectively resolving the latency and memory inefficiencies of tree decoding. Our approach achieves strong results on various long-context tasks, including repository-level code completion, long-context summarization, and o1-like long reasoning tasks, demonstrating significant improvements in latency reduction. The code is available at https://github.com/sail-sg/LongSpec."
  },
  {
    "title": "The Geometry of Refusal in Large Language Models: Concept Cones and Representational Independence",
    "url": "http://arxiv.org/abs/2502.17420v1",
    "arxiv_id": "2502.17420v1",
    "authors": [
      "Tom Wollschl\u00e4ger",
      "Jannes Elstner",
      "Simon Geisler",
      "Vincent Cohen-Addad",
      "Stephan G\u00fcnnemann",
      "Johannes Gasteiger"
    ],
    "published": "2025-02-24T18:52:59+00:00",
    "summary": "The safety alignment of large language models (LLMs) can be circumvented through adversarially crafted inputs, yet the mechanisms by which these attacks bypass safety barriers remain poorly understood. Prior work suggests that a single refusal direction in the model's activation space determines whether an LLM refuses a request. In this study, we propose a novel gradient-based approach to representation engineering and use it to identify refusal directions. Contrary to prior work, we uncover multiple independent directions and even multi-dimensional concept cones that mediate refusal. Moreover, we show that orthogonality alone does not imply independence under intervention, motivating the notion of representational independence that accounts for both linear and non-linear effects. Using this framework, we identify mechanistically independent refusal directions. We show that refusal mechanisms in LLMs are governed by complex spatial structures and identify functionally independent directions, confirming that multiple distinct mechanisms drive refusal behavior. Our gradient-based approach uncovers these mechanisms and can further serve as a foundation for future work on understanding LLMs."
  },
  {
    "title": "From System 1 to System 2: A Survey of Reasoning Large Language Models",
    "url": "http://arxiv.org/abs/2502.17419v1",
    "arxiv_id": "2502.17419v1",
    "authors": [
      "Zhong-Zhi Li",
      "Duzhen Zhang",
      "Ming-Liang Zhang",
      "Jiaxin Zhang",
      "Zengyan Liu",
      "Yuxuan Yao",
      "Haotian Xu",
      "Junhao Zheng",
      "Pei-Jie Wang",
      "Xiuyi Chen",
      "Yingying Zhang",
      "Fei Yin",
      "Jiahua Dong",
      "Zhijiang Guo",
      "Le Song",
      "Cheng-Lin Liu"
    ],
    "published": "2025-02-24T18:50:52+00:00",
    "summary": "Achieving human-level intelligence requires refining the transition from the fast, intuitive System 1 to the slower, more deliberate System 2 reasoning. While System 1 excels in quick, heuristic decisions, System 2 relies on logical reasoning for more accurate judgments and reduced biases. Foundational Large Language Models (LLMs) excel at fast decision-making but lack the depth for complex reasoning, as they have not yet fully embraced the step-by-step analysis characteristic of true System 2 thinking. Recently, reasoning LLMs like OpenAI's o1/o3 and DeepSeek's R1 have demonstrated expert-level performance in fields such as mathematics and coding, closely mimicking the deliberate reasoning of System 2 and showcasing human-like cognitive abilities. This survey begins with a brief overview of the progress in foundational LLMs and the early development of System 2 technologies, exploring how their combination has paved the way for reasoning LLMs. Next, we discuss how to construct reasoning LLMs, analyzing their features, the core methods enabling advanced reasoning, and the evolution of various reasoning LLMs. Additionally, we provide an overview of reasoning benchmarks, offering an in-depth comparison of the performance of representative reasoning LLMs. Finally, we explore promising directions for advancing reasoning LLMs and maintain a real-time \\href{https://github.com/zzli2022/Awesome-Slow-Reason-System}{GitHub Repository} to track the latest developments. We hope this survey will serve as a valuable resource to inspire innovation and drive progress in this rapidly evolving field."
  },
  {
    "title": "From System 1 to System 2: A Survey of Reasoning Large Language Models",
    "url": "http://arxiv.org/abs/2502.17419v2",
    "arxiv_id": "2502.17419v2",
    "authors": [
      "Zhong-Zhi Li",
      "Duzhen Zhang",
      "Ming-Liang Zhang",
      "Jiaxin Zhang",
      "Zengyan Liu",
      "Yuxuan Yao",
      "Haotian Xu",
      "Junhao Zheng",
      "Pei-Jie Wang",
      "Xiuyi Chen",
      "Yingying Zhang",
      "Fei Yin",
      "Jiahua Dong",
      "Zhijiang Guo",
      "Le Song",
      "Cheng-Lin Liu"
    ],
    "published": "2025-02-24T18:50:52+00:00",
    "summary": "Achieving human-level intelligence requires refining the transition from the fast, intuitive System 1 to the slower, more deliberate System 2 reasoning. While System 1 excels in quick, heuristic decisions, System 2 relies on logical reasoning for more accurate judgments and reduced biases. Foundational Large Language Models (LLMs) excel at fast decision-making but lack the depth for complex reasoning, as they have not yet fully embraced the step-by-step analysis characteristic of true System 2 thinking. Recently, reasoning LLMs like OpenAI's o1/o3 and DeepSeek's R1 have demonstrated expert-level performance in fields such as mathematics and coding, closely mimicking the deliberate reasoning of System 2 and showcasing human-like cognitive abilities. This survey begins with a brief overview of the progress in foundational LLMs and the early development of System 2 technologies, exploring how their combination has paved the way for reasoning LLMs. Next, we discuss how to construct reasoning LLMs, analyzing their features, the core methods enabling advanced reasoning, and the evolution of various reasoning LLMs. Additionally, we provide an overview of reasoning benchmarks, offering an in-depth comparison of the performance of representative reasoning LLMs. Finally, we explore promising directions for advancing reasoning LLMs and maintain a real-time \\href{https://github.com/zzli2022/Awesome-Slow-Reason-System}{GitHub Repository} to track the latest developments. We hope this survey will serve as a valuable resource to inspire innovation and drive progress in this rapidly evolving field."
  },
  {
    "title": "Dataset Featurization: Uncovering Natural Language Features through Unsupervised Data Reconstruction",
    "url": "http://arxiv.org/abs/2502.17541v1",
    "arxiv_id": "2502.17541v1",
    "authors": [
      "Michal Bravansky",
      "Vaclav Kubon",
      "Suhas Hariharan",
      "Robert Kirk"
    ],
    "published": "2025-02-24T18:42:33+00:00",
    "summary": "Interpreting data is central to modern research. Large language models (LLMs) show promise in providing such natural language interpretations of data, yet simple feature extraction methods such as prompting often fail to produce accurate and versatile descriptions for diverse datasets and lack control over granularity and scale. To address these limitations, we propose a domain-agnostic method for dataset featurization that provides precise control over the number of features extracted while maintaining compact and descriptive representations comparable to human expert labeling. Our method optimizes the selection of informative binary features by evaluating the ability of an LLM to reconstruct the original data using those features. We demonstrate its effectiveness in dataset modeling tasks and through two case studies: (1) Constructing a feature representation of jailbreak tactics that compactly captures both the effectiveness and diversity of a larger set of human-crafted attacks; and (2) automating the discovery of features that align with human preferences, achieving accuracy and robustness comparable to expert-crafted features. Moreover, we show that the pipeline scales effectively, improving as additional features are sampled, making it suitable for large and diverse datasets."
  },
  {
    "title": "Experimental validation of UAV search and detection system in real wilderness environment",
    "url": "http://arxiv.org/abs/2502.17372v1",
    "arxiv_id": "2502.17372v1",
    "authors": [
      "Stella Dumen\u010di\u0107",
      "Luka Lan\u010da",
      "Karlo Jakac",
      "Stefan Ivi\u0107"
    ],
    "published": "2025-02-24T17:53:54+00:00",
    "summary": "Search and rescue (SAR) missions require reliable search methods to locate survivors, especially in challenging or inaccessible environments. This is why introducing unmanned aerial vehicles (UAVs) can be of great help to enhance the efficiency of SAR missions while simultaneously increasing the safety of everyone involved in the mission. Motivated by this, we design and experiment with autonomous UAV search for humans in a Mediterranean karst environment. The UAVs are directed using Heat equation-driven area coverage (HEDAC) ergodic control method according to known probability density and detection function. The implemented sensing framework consists of a probabilistic search model, motion control system, and computer vision object detection. It enables calculation of the probability of the target being detected in the SAR mission, and this paper focuses on experimental validation of proposed probabilistic framework and UAV control. The uniform probability density to ensure the even probability of finding the targets in the desired search area is achieved by assigning suitably thought-out tasks to 78 volunteers. The detection model is based on YOLO and trained with a previously collected ortho-photo image database. The experimental search is carefully planned and conducted, while as many parameters as possible are recorded. The thorough analysis consists of the motion control system, object detection, and the search validation. The assessment of the detection and search performance provides strong indication that the designed detection model in the UAV control algorithm is aligned with real-world results."
  },
  {
    "title": "Hybrid Human-Machine Perception via Adaptive LiDAR for Advanced Driver Assistance Systems",
    "url": "http://arxiv.org/abs/2502.17309v1",
    "arxiv_id": "2502.17309v1",
    "authors": [
      "Federico Scar\u00ec",
      "Nitin Jonathan Myers",
      "Chen Quan",
      "Arkady Zgonnikov"
    ],
    "published": "2025-02-24T16:44:20+00:00",
    "summary": "Accurate environmental perception is critical for advanced driver assistance systems (ADAS). Light detection and ranging (LiDAR) systems play a crucial role in ADAS; they can reliably detect obstacles and help ensure traffic safety. Existing research on LiDAR sensing has demonstrated that adapting the LiDAR's resolution and range based on environmental characteristics can improve machine perception. However, current adaptive LiDAR approaches for ADAS have not explored the possibility of combining the perception abilities of the vehicle and the human driver, which can potentially further enhance the detection performance. In this paper, we propose a novel system that adapts LiDAR characteristics to human driver's visual perception to enhance LiDAR sensing outside human's field of view. We develop a proof-of-concept prototype of the system in the virtual environment CARLA. Our system integrates real-time data on the driver's gaze to identify regions in the environment that the driver is monitoring. This allows the system to optimize LiDAR resources by dynamically increasing the LiDAR's range and resolution in peripheral areas that the driver may not be attending to. Our simulations show that this gaze-aware LiDAR enhances detection performance compared to a baseline standalone LiDAR, particularly in challenging environmental conditions like fog. Our hybrid human-machine sensing approach potentially offers improved safety and situational awareness in real-time driving scenarios for ADAS applications."
  },
  {
    "title": "REINFORCE Adversarial Attacks on Large Language Models: An Adaptive, Distributional, and Semantic Objective",
    "url": "http://arxiv.org/abs/2502.17254v1",
    "arxiv_id": "2502.17254v1",
    "authors": [
      "Simon Geisler",
      "Tom Wollschl\u00e4ger",
      "M. H. I. Abdalla",
      "Vincent Cohen-Addad",
      "Johannes Gasteiger",
      "Stephan G\u00fcnnemann"
    ],
    "published": "2025-02-24T15:34:48+00:00",
    "summary": "To circumvent the alignment of large language models (LLMs), current optimization-based adversarial attacks usually craft adversarial prompts by maximizing the likelihood of a so-called affirmative response. An affirmative response is a manually designed start of a harmful answer to an inappropriate request. While it is often easy to craft prompts that yield a substantial likelihood for the affirmative response, the attacked model frequently does not complete the response in a harmful manner. Moreover, the affirmative objective is usually not adapted to model-specific preferences and essentially ignores the fact that LLMs output a distribution over responses. If low attack success under such an objective is taken as a measure of robustness, the true robustness might be grossly overestimated. To alleviate these flaws, we propose an adaptive and semantic optimization problem over the population of responses. We derive a generally applicable objective via the REINFORCE policy-gradient formalism and demonstrate its efficacy with the state-of-the-art jailbreak algorithms Greedy Coordinate Gradient (GCG) and Projected Gradient Descent (PGD). For example, our objective doubles the attack success rate (ASR) on Llama3 and increases the ASR from 2% to 50% with circuit breaker defense."
  },
  {
    "title": "PrivaCI-Bench: Evaluating Privacy with Contextual Integrity and Legal Compliance",
    "url": "http://arxiv.org/abs/2502.17041v1",
    "arxiv_id": "2502.17041v1",
    "authors": [
      "Haoran Li",
      "Wenbin Hu",
      "Huihao Jing",
      "Yulin Chen",
      "Qi Hu",
      "Sirui Han",
      "Tianshu Chu",
      "Peizhao Hu",
      "Yangqiu Song"
    ],
    "published": "2025-02-24T10:49:34+00:00",
    "summary": "Recent advancements in generative large language models (LLMs) have enabled wider applicability, accessibility, and flexibility. However, their reliability and trustworthiness are still in doubt, especially for concerns regarding individuals' data privacy. Great efforts have been made on privacy by building various evaluation benchmarks to study LLMs' privacy awareness and robustness from their generated outputs to their hidden representations. Unfortunately, most of these works adopt a narrow formulation of privacy and only investigate personally identifiable information (PII). In this paper, we follow the merit of the Contextual Integrity (CI) theory, which posits that privacy evaluation should not only cover the transmitted attributes but also encompass the whole relevant social context through private information flows. We present PrivaCI-Bench, a comprehensive contextual privacy evaluation benchmark targeted at legal compliance to cover well-annotated privacy and safety regulations, real court cases, privacy policies, and synthetic data built from the official toolkit to study LLMs' privacy and safety compliance. We evaluate the latest LLMs, including the recent reasoner models QwQ-32B and Deepseek R1. Our experimental results suggest that though LLMs can effectively capture key CI parameters inside a given context, they still require further advancements for privacy compliance."
  },
  {
    "title": "LongSafety: Evaluating Long-Context Safety of Large Language Models",
    "url": "http://arxiv.org/abs/2502.16971v1",
    "arxiv_id": "2502.16971v1",
    "authors": [
      "Yida Lu",
      "Jiale Cheng",
      "Zhexin Zhang",
      "Shiyao Cui",
      "Cunxiang Wang",
      "Xiaotao Gu",
      "Yuxiao Dong",
      "Jie Tang",
      "Hongning Wang",
      "Minlie Huang"
    ],
    "published": "2025-02-24T08:54:39+00:00",
    "summary": "As Large Language Models (LLMs) continue to advance in understanding and generating long sequences, new safety concerns have been introduced through the long context. However, the safety of LLMs in long-context tasks remains under-explored, leaving a significant gap in both evaluation and improvement of their safety. To address this, we introduce LongSafety, the first comprehensive benchmark specifically designed to evaluate LLM safety in open-ended long-context tasks. LongSafety encompasses 7 categories of safety issues and 6 user-oriented long-context tasks, with a total of 1,543 test cases, averaging 5,424 words per context. Our evaluation towards 16 representative LLMs reveals significant safety vulnerabilities, with most models achieving safety rates below 55%. Our findings also indicate that strong safety performance in short-context scenarios does not necessarily correlate with safety in long-context tasks, emphasizing the unique challenges and urgency of improving long-context safety. Moreover, through extensive analysis, we identify challenging safety issues and task types for long-context models. Furthermore, we find that relevant context and extended input sequences can exacerbate safety risks in long-context scenarios, highlighting the critical need for ongoing attention to long-context safety challenges. Our code and data are available at https://github.com/thu-coai/LongSafety."
  },
  {
    "title": "GuidedBench: Equipping Jailbreak Evaluation with Guidelines",
    "url": "http://arxiv.org/abs/2502.16903v1",
    "arxiv_id": "2502.16903v1",
    "authors": [
      "Ruixuan Huang",
      "Xunguang Wang",
      "Zongjie Li",
      "Daoyuan Wu",
      "Shuai Wang"
    ],
    "published": "2025-02-24T06:57:27+00:00",
    "summary": "Jailbreaking methods for large language models (LLMs) have gained increasing attention for building safe and responsible AI systems. After analyzing 35 jailbreak methods across six categories, we find that existing benchmarks, relying on universal LLM-based or keyword-matching scores, lack case-specific criteria, leading to conflicting results. In this paper, we introduce a more robust evaluation framework for jailbreak methods, with a curated harmful question dataset, detailed case-by-case evaluation guidelines, and a scoring system equipped with these guidelines. Our experiments show that existing jailbreak methods exhibit better discrimination when evaluated using our benchmark. Some jailbreak methods that claim to achieve over 90% attack success rate (ASR) on other benchmarks only reach a maximum of 30.2% on our benchmark, providing a higher ceiling for more advanced jailbreak research; furthermore, using our scoring system reduces the variance of disagreements between different evaluator LLMs by up to 76.33%. This demonstrates its ability to provide more fair and stable evaluation."
  },
  {
    "title": "Leveraging Large Language Models for Effective and Explainable Multi-Agent Credit Assignment",
    "url": "http://arxiv.org/abs/2502.16863v1",
    "arxiv_id": "2502.16863v1",
    "authors": [
      "Kartik Nagpal",
      "Dayi Dong",
      "Jean-Baptiste Bouvier",
      "Negar Mehr"
    ],
    "published": "2025-02-24T05:56:47+00:00",
    "summary": "Recent work, spanning from autonomous vehicle coordination to in-space assembly, has shown the importance of learning collaborative behavior for enabling robots to achieve shared goals. A common approach for learning this cooperative behavior is to utilize the centralized-training decentralized-execution paradigm. However, this approach also introduces a new challenge: how do we evaluate the contributions of each agent's actions to the overall success or failure of the team. This credit assignment problem has remained open, and has been extensively studied in the Multi-Agent Reinforcement Learning literature. In fact, humans manually inspecting agent behavior often generate better credit evaluations than existing methods. We combine this observation with recent works which show Large Language Models demonstrate human-level performance at many pattern recognition tasks. Our key idea is to reformulate credit assignment to the two pattern recognition problems of sequence improvement and attribution, which motivates our novel LLM-MCA method. Our approach utilizes a centralized LLM reward-critic which numerically decomposes the environment reward based on the individualized contribution of each agent in the scenario. We then update the agents' policy networks based on this feedback. We also propose an extension LLM-TACA where our LLM critic performs explicit task assignment by passing an intermediary goal directly to each agent policy in the scenario. Both our methods far outperform the state-of-the-art on a variety of benchmarks, including Level-Based Foraging, Robotic Warehouse, and our new Spaceworld benchmark which incorporates collision-related safety constraints. As an artifact of our methods, we generate large trajectory datasets with each timestep annotated with per-agent reward information, as sampled from our LLM critics."
  },
  {
    "title": "PulseBat: A field-accessible dataset for second-life battery diagnostics from realistic histories using multidimensional rapid pulse test",
    "url": "http://arxiv.org/abs/2502.16848v1",
    "arxiv_id": "2502.16848v1",
    "authors": [
      "Shengyu Tao",
      "Guangyuan Ma",
      "Huixiong Yang",
      "Minyan Lu",
      "Guodan Wei",
      "Guangmin Zhou",
      "Xuan Zhang"
    ],
    "published": "2025-02-24T05:10:04+00:00",
    "summary": "As electric vehicles (EVs) approach the end of their operational life, their batteries retain significant economic value and present promising opportunities for second-life use and material recycling. This is particularly compelling for Global South and other underdeveloped regions, where reliable energy storage is vital to addressing critical challenges posed by weak and even nonexistent power grid and energy infrastructures. However, despite this potential, widespread adoption has been hindered by critical uncertainties surrounding the technical performance, safety, and recertification of second-life batteries. In cases where they have been redeployed, mismatches between estimated and actual performance often render batteries technically unsuitable or hazardous, turning them into liabilities for communities they were intended to benefit. This considerable misalignment exacerbates energy access disparities and undermines the broader vision of energy justice, highlighting an urgent need for robust and scalable solutions to unlock the potential. In the PulseBat Dataset, the authors tested 464 retired lithium-ion batteries, covering 3 cathode material types, 6 historical usages, 3 physical formats, and 6 capacity designs. The pulse test experiments were performed repeatedly for each second-life battery with 10 pulse width, 10 pulse magnitude, multiple state-of-charge, and state-of-health conditions, e.g., from 0.37 to 1.03. The PulseBat Dataset recorded these test conditions and the voltage response as well as the temperature signals that were subject to the injected pulse current, which could be used as a valuable data resource for critical diagnostics tasks such as state-of-charge estimation, state-of-health estimation, cathode material type identification, open-circuit voltage reconstruction, thermal management, and beyond."
  },
  {
    "title": "Multi-Agent Autonomous Driving Systems with Large Language Models: A Survey of Recent Advances",
    "url": "http://arxiv.org/abs/2502.16804v1",
    "arxiv_id": "2502.16804v1",
    "authors": [
      "Yaozu Wu",
      "Dongyuan Li",
      "Yankai Chen",
      "Renhe Jiang",
      "Henry Peng Zou",
      "Liancheng Fang",
      "Zhen Wang",
      "Philip S. Yu"
    ],
    "published": "2025-02-24T03:26:13+00:00",
    "summary": "Autonomous Driving Systems (ADSs) are revolutionizing transportation by reducing human intervention, improving operational efficiency, and enhancing safety. Large Language Models (LLMs), known for their exceptional planning and reasoning capabilities, have been integrated into ADSs to assist with driving decision-making. However, LLM-based single-agent ADSs face three major challenges: limited perception, insufficient collaboration, and high computational demands. To address these issues, recent advancements in LLM-based multi-agent ADSs have focused on improving inter-agent communication and cooperation. This paper provides a frontier survey of LLM-based multi-agent ADSs. We begin with a background introduction to related concepts, followed by a categorization of existing LLM-based approaches based on different agent interaction modes. We then discuss agent-human interactions in scenarios where LLM-based agents engage with humans. Finally, we summarize key applications, datasets, and challenges in this field to support future research (https://anonymous.4open.science/r/LLM-based_Multi-agent_ADS-3A5C/README.md)."
  },
  {
    "title": "Singularity resolution and regular black hole formation in gravitational collapse in asymptotically safe gravity",
    "url": "http://arxiv.org/abs/2502.16787v1",
    "arxiv_id": "2502.16787v1",
    "authors": [
      "Tomohiro Harada",
      "Chiang-Mei Chen",
      "Rituparna Mandal"
    ],
    "published": "2025-02-24T02:45:29+00:00",
    "summary": "We adopt an effective action inspired by asymptotically safe gravity, in which the effective gravitational constant is parameterized as $G(\\epsilon) = G_{N} [1 + \\tilde{\\omega} (G_{N}^{2} \\epsilon)^{\\alpha}]^{-1}$, where $G_{N}$ and $\\epsilon$ denote Newton's gravitational constant and the energy density of the matter field, respectively, with two dimensionless model parameters, $\\tilde{\\omega}$ and $\\alpha$. Within this framework, we investigate the complete gravitational collapse of a homogeneous ball of perfect fluid and find that the singularity is completely resolved for $\\alpha > 1$ but not for $1/2 \\le \\alpha \\le 1$. The case $0 < \\alpha < 1/2$ is inconsistent with asymptotic safety. Moreover, we note that although the singularity cannot be fully resolved for $\\alpha = 1$, it is significantly weakened by quantum gravity effects. Furthermore, we successfully construct a static exterior metric which, together with the interior solution, describes the dynamical formation of regular black holes in an asymptotically flat spacetime for the perfectly resolved case $\\alpha > 1$. The resulting regular black hole, obtained as the final static state, contains a de Sitter core and admits a static metric fully expressible in terms of the Lerch transcendent for general cases and in elementary functions for certain values of $\\alpha$, including \\alpha = 2$. We also discuss the formation of gravastars and the late-time evaporation process of the regular black holes."
  },
  {
    "title": "AISafetyLab: A Comprehensive Framework for AI Safety Evaluation and Improvement",
    "url": "http://arxiv.org/abs/2502.16776v1",
    "arxiv_id": "2502.16776v1",
    "authors": [
      "Zhexin Zhang",
      "Leqi Lei",
      "Junxiao Yang",
      "Xijie Huang",
      "Yida Lu",
      "Shiyao Cui",
      "Renmiao Chen",
      "Qinglin Zhang",
      "Xinyuan Wang",
      "Hao Wang",
      "Hao Li",
      "Xianqi Lei",
      "Chengwei Pan",
      "Lei Sha",
      "Hongning Wang",
      "Minlie Huang"
    ],
    "published": "2025-02-24T02:11:52+00:00",
    "summary": "As AI models are increasingly deployed across diverse real-world scenarios, ensuring their safety remains a critical yet underexplored challenge. While substantial efforts have been made to evaluate and enhance AI safety, the lack of a standardized framework and comprehensive toolkit poses significant obstacles to systematic research and practical adoption. To bridge this gap, we introduce AISafetyLab, a unified framework and toolkit that integrates representative attack, defense, and evaluation methodologies for AI safety. AISafetyLab features an intuitive interface that enables developers to seamlessly apply various techniques while maintaining a well-structured and extensible codebase for future advancements. Additionally, we conduct empirical studies on Vicuna, analyzing different attack and defense strategies to provide valuable insights into their comparative effectiveness. To facilitate ongoing research and development in AI safety, AISafetyLab is publicly available at https://github.com/thu-coai/AISafetyLab, and we are committed to its continuous maintenance and improvement."
  },
  {
    "title": "LED-Merging: Mitigating Safety-Utility Conflicts in Model Merging with Location-Election-Disjoint",
    "url": "http://arxiv.org/abs/2502.16770v1",
    "arxiv_id": "2502.16770v1",
    "authors": [
      "Qianli Ma",
      "Dongrui Liu",
      "Qian Chen",
      "Linfeng Zhang",
      "Jing Shao"
    ],
    "published": "2025-02-24T01:19:43+00:00",
    "summary": "Fine-tuning pre-trained Large Language Models (LLMs) for specialized tasks incurs substantial computational and data costs. While model merging offers a training-free solution to integrate multiple task-specific models, existing methods suffer from safety-utility conflicts where enhanced general capabilities degrade safety safeguards. We identify two root causes: \\textbf{neuron misidentification} due to simplistic parameter magnitude-based selection, and \\textbf{cross-task neuron interference} during merging. To address these challenges, we propose \\textbf{LED-Merging}, a three-stage framework that \\textbf{L}ocates task-specific neurons via gradient-based attribution, dynamically \\textbf{E}lects critical neurons through multi-model importance fusion, and \\textbf{D}isjoints conflicting updates through parameter isolation. Extensive experiments on Llama-3-8B, Mistral-7B, and Llama2-13B demonstrate that LED-Merging reduces harmful response rates(\\emph{e.g.}, a 31.4\\% decrease on Llama-3-8B-Instruct on HarmBench) while preserving 95\\% of utility performance(\\emph{e.g.}, 52.39\\% accuracy on GSM8K). LED-Merging resolves safety-utility conflicts and provides a lightweight, training-free paradigm for constructing reliable multi-task LLMs."
  },
  {
    "title": "ATEB: Evaluating and Improving Advanced NLP Tasks for Text Embedding Models",
    "url": "http://arxiv.org/abs/2502.16766v1",
    "arxiv_id": "2502.16766v1",
    "authors": [
      "Simeng Han",
      "Frank Palma Gomez",
      "Tu Vu",
      "Zefei Li",
      "Daniel Cer",
      "Hansi Zeng",
      "Chris Tar",
      "Arman Cohan",
      "Gustavo Hernandez Abrego"
    ],
    "published": "2025-02-24T01:08:15+00:00",
    "summary": "Traditional text embedding benchmarks primarily evaluate embedding models' capabilities to capture semantic similarity. However, more advanced NLP tasks require a deeper understanding of text, such as safety and factuality. These tasks demand an ability to comprehend and process complex information, often involving the handling of sensitive content, or the verification of factual statements against reliable sources. We introduce a new benchmark designed to assess and highlight the limitations of embedding models trained on existing information retrieval data mixtures on advanced capabilities, which include factuality, safety, instruction following, reasoning and document-level understanding. This benchmark includes a diverse set of tasks that simulate real-world scenarios where these capabilities are critical and leads to identification of the gaps of the currently advanced embedding models. Furthermore, we propose a novel method that reformulates these various tasks as retrieval tasks. By framing tasks like safety or factuality classification as retrieval problems, we leverage the strengths of retrieval models in capturing semantic relationships while also pushing them to develop a deeper understanding of context and content. Using this approach with single-task fine-tuning, we achieved performance gains of 8\\% on factuality classification and 13\\% on safety classification. Our code and data will be publicly available."
  },
  {
    "title": "Correlating and Predicting Human Evaluations of Language Models from Natural Language Processing Benchmarks",
    "url": "http://arxiv.org/abs/2502.18339v1",
    "arxiv_id": "2502.18339v1",
    "authors": [
      "Rylan Schaeffer",
      "Punit Singh Koura",
      "Binh Tang",
      "Ranjan Subramanian",
      "Aaditya K Singh",
      "Todor Mihaylov",
      "Prajjwal Bhargava",
      "Lovish Madaan",
      "Niladri S. Chatterji",
      "Vedanuj Goswami",
      "Sergey Edunov",
      "Dieuwke Hupkes",
      "Sanmi Koyejo",
      "Sharan Narang"
    ],
    "published": "2025-02-24T01:01:02+00:00",
    "summary": "The explosion of high-performing conversational language models (LMs) has spurred a shift from classic natural language processing (NLP) benchmarks to expensive, time-consuming and noisy human evaluations - yet the relationship between these two evaluation strategies remains hazy. In this paper, we conduct a large-scale study of four Chat Llama 2 models, comparing their performance on 160 standard NLP benchmarks (e.g., MMLU, ARC, BIG-Bench Hard) against extensive human preferences on more than 11k single-turn and 2k multi-turn dialogues from over 2k human annotators. Our findings are striking: most NLP benchmarks strongly correlate with human evaluations, suggesting that cheaper, automated metrics can serve as surprisingly reliable predictors of human preferences. Three human evaluations, such as adversarial dishonesty and safety, are anticorrelated with NLP benchmarks, while two are uncorrelated. Moreover, through overparameterized linear regressions, we show that NLP scores can accurately predict human evaluations across different model scales, offering a path to reduce costly human annotation without sacrificing rigor. Overall, our results affirm the continued value of classic benchmarks and illuminate how to harness them to anticipate real-world user satisfaction - pointing to how NLP benchmarks can be leveraged to meet evaluation needs of our new era of conversational AI."
  },
  {
    "title": "Watch Out E-scooter Coming Through: Multimodal Sensing of Mixed Traffic Use and Conflicts Through Riders Ego-centric Views",
    "url": "http://arxiv.org/abs/2502.16755v1",
    "arxiv_id": "2502.16755v1",
    "authors": [
      "Hiruni Nuwanthika Kegalle",
      "Danula Hettiachchi",
      "Jeffrey Chan",
      "Mark Sanderson",
      "Flora D. Salim"
    ],
    "published": "2025-02-24T00:16:18+00:00",
    "summary": "E-scooters are becoming a popular means of urban transportation. However, this increased popularity brings challenges, such as road accidents and conflicts when sharing space with traditional transport modes. An in-depth understanding of e-scooter rider behaviour is crucial for ensuring rider safety, guiding infrastructure planning, and enforcing traffic rules. This study investigated the rider behaviour through a naturalistic study with 23 participants equipped with a bike computer, eye-tracking glasses and cameras. They followed a pre-determined route, enabling multi-modal data collection. We analysed and compared gaze movements, speed, and video feeds across three transport infrastructure types: a pedestrian-shared path, a cycle lane and a roadway. Our findings reveal unique challenges e-scooter riders face, including difficulty keeping up with cyclists and motor vehicles due to speed limits on shared e-scooters, risks in signalling turns due to control lose, and limited acceptance in mixed-use spaces. The cycle lane showed the highest average speed, the least speed change points, and the least head movements, supporting its suitability as dedicated infrastructure for e-scooters. These findings are facilitated through multimodal sensing and analysing the e-scooter riders' ego-centric view, which show the efficacy of our method in discovering the behavioural dynamics of the riders in the wild. Our study highlights the critical need to align infrastructure with user behaviour to improve safety and emphasises the importance of targeted safety measures and regulations, especially when e-scooter riders share spaces with pedestrians or motor vehicles. The dataset and analysis code are available at https://github.com/HiruniNuwanthika/Electric-Scooter-Riders-Multi-Modal-Data-Analysis.git."
  },
  {
    "title": "Guardians of the Agentic System: Preventing Many Shots Jailbreak with Agentic System",
    "url": "http://arxiv.org/abs/2502.16750v1",
    "arxiv_id": "2502.16750v1",
    "authors": [
      "Saikat Barua",
      "Mostafizur Rahman",
      "Md Jafor Sadek",
      "Rafiul Islam",
      "Shehnaz Khaled",
      "Ahmedul Kabir"
    ],
    "published": "2025-02-23T23:35:15+00:00",
    "summary": "The autonomous AI agents using large language models can create undeniable values in all span of the society but they face security threats from adversaries that warrants immediate protective solutions because trust and safety issues arise. Considering the many-shot jailbreaking and deceptive alignment as some of the main advanced attacks, that cannot be mitigated by the static guardrails used during the supervised training, points out a crucial research priority for real world robustness. The combination of static guardrails in dynamic multi-agent system fails to defend against those attacks. We intend to enhance security for LLM-based agents through the development of new evaluation frameworks which identify and counter threats for safe operational deployment. Our work uses three examination methods to detect rogue agents through a Reverse Turing Test and analyze deceptive alignment through multi-agent simulations and develops an anti-jailbreaking system by testing it with GEMINI 1.5 pro and llama-3.3-70B, deepseek r1 models using tool-mediated adversarial scenarios. The detection capabilities are strong such as 94\\% accuracy for GEMINI 1.5 pro yet the system suffers persistent vulnerabilities when under long attacks as prompt length increases attack success rates (ASR) and diversity metrics become ineffective in prediction while revealing multiple complex system faults. The findings demonstrate the necessity of adopting flexible security systems based on active monitoring that can be performed by the agents themselves together with adaptable interventions by system admin as the current models can create vulnerabilities that can lead to the unreliable and vulnerable system. So, in our work, we try to address such situations and propose a comprehensive framework to counteract the security issues."
  },
  {
    "title": "Toward Responsible Federated Large Language Models: Leveraging a Safety Filter and Constitutional AI",
    "url": "http://arxiv.org/abs/2502.16691v1",
    "arxiv_id": "2502.16691v1",
    "authors": [
      "Eunchung Noh",
      "Jeonghun Baek"
    ],
    "published": "2025-02-23T19:12:10+00:00",
    "summary": "Recent research has increasingly focused on training large language models (LLMs) using federated learning, known as FedLLM. However, responsible AI (RAI), which aims to ensure safe responses, remains underexplored in the context of FedLLM. In FedLLM, client data used for training may contain harmful content, leading to unsafe LLMs that generate harmful responses. Aggregating such unsafe LLMs into the global model and distributing them to clients may result in the widespread deployment of unsafe LLMs. To address this issue, we incorporate two well-known RAI methods into FedLLM: the safety filter and constitutional AI. Our experiments demonstrate that these methods significantly enhance the safety of the LLM, achieving over a 20% improvement on AdvBench, a benchmark for evaluating safety performance."
  },
  {
    "title": "Analyzing Factors Influencing Driver Willingness to Accept Advanced Driver Assistance Systems",
    "url": "http://arxiv.org/abs/2502.16688v1",
    "arxiv_id": "2502.16688v1",
    "authors": [
      "Hannah Musau",
      "Nana Kankam Gyimah",
      "Judith Mwakalonge",
      "Gurcan Comert",
      "Saidi Siuhi"
    ],
    "published": "2025-02-23T19:01:54+00:00",
    "summary": "Advanced Driver Assistance Systems (ADAS) enhance highway safety by improving environmental perception and reducing human errors. However, misconceptions, trust issues, and knowledge gaps hinder widespread adoption. This study examines driver perceptions, knowledge sources, and usage patterns of ADAS in passenger vehicles. A nationwide survey collected data from a diverse sample of U.S. drivers. Machine learning models predicted ADAS adoption, with SHAP (SHapley Additive Explanations) identifying key influencing factors. Findings indicate that higher trust levels correlate with increased ADAS usage, while concerns about reliability remain a barrier. Specific features, such as Forward Collision Warning and Driver Monitoring Systems, significantly influence adoption likelihood. Demographic factors (age, gender) and driving habits (experience, frequency) also shape ADAS acceptance. Findings emphasize the influence of socioeconomic, demographic, and behavioral factors on ADAS adoption, offering guidance for automakers, policymakers, and safety advocates to improve awareness, trust, and usability."
  },
  {
    "title": "Security Analysis of 5G NR Device-to-Device Sidelink Communications",
    "url": "http://arxiv.org/abs/2502.16650v1",
    "arxiv_id": "2502.16650v1",
    "authors": [
      "Evangelos Bitsikas",
      "Aanjhan Ranganathan"
    ],
    "published": "2025-02-23T16:55:32+00:00",
    "summary": "5G NR sidelink communication enables new possibilities for direct device-to-device interactions, supporting applications from vehicle-to-everything (V2X) systems to public safety, industrial automation, and drone networks. However, these advancements come with significant security challenges due to the decentralized trust model and increased reliance on User Equipment (UE) for critical functions like synchronization, resource allocation, and authorization. This paper presents the first comprehensive security analysis of NR V2X sidelink. We identify vulnerabilities across critical procedures and demonstrate plausible attack, including attacks that manipulate data integrity feedback and block resources, ultimately undermining the reliability and privacy of sidelink communications. Our analysis reveals that NR operational modes are vulnerable, with the ones relying on autonomous resource management (without network supervision) particularly exposed. To address these issues, we propose mitigation strategies to enhance the security of 5G sidelink communications. This work establishes a foundation for future efforts to strengthen 5G device-to-device sidelink communications, ensuring its safe deployment in critical applications."
  },
  {
    "title": "CODESYNC: Synchronizing Large Language Models with Dynamic Code Evolution at Scale",
    "url": "http://arxiv.org/abs/2502.16645v1",
    "arxiv_id": "2502.16645v1",
    "authors": [
      "Chenlong Wang",
      "Zhaoyang Chu",
      "Zhengxiang Cheng",
      "Xuyi Yang",
      "Kaiyue Qiu",
      "Yao Wan",
      "Zhou Zhao",
      "Xuanhua Shi",
      "Dongping Chen"
    ],
    "published": "2025-02-23T16:46:18+00:00",
    "summary": "Large Language Models (LLMs) have exhibited exceptional performance in software engineering yet face challenges in adapting to continually evolving code knowledge, particularly regarding the frequent updates of third-party library APIs. This limitation, stemming from static pre-training datasets, often results in non-executable code or implementations with suboptimal safety and efficiency. To this end, this paper introduces CODESYNC, a data engine for identifying outdated code patterns and collecting real-time code knowledge updates from Python third-party libraries. Building upon CODESYNC, we develop CODESYNCBENCH, a comprehensive benchmark for assessing LLMs' ability to stay synchronized with code evolution, which covers real-world updates for 220 APIs from six Python libraries. Our benchmark offers 3,300 test cases across three evaluation tasks and an update-aware instruction tuning dataset consisting of 2,200 training samples. Extensive experiments on 14 state-of-the-art LLMs reveal that they struggle with dynamic code evolution, even with the support of advanced knowledge updating methods (e.g., DPO, ORPO, and SimPO). We believe that our benchmark can offer a strong foundation for the development of more effective methods for real-time code knowledge updating in the future. The experimental code and dataset are publicly available at: https://github.com/Lucky-voyage/Code-Sync."
  },
  {
    "title": "Color Information-Based Automated Mask Generation for Detecting Underwater Atypical Glare Areas",
    "url": "http://arxiv.org/abs/2502.16538v1",
    "arxiv_id": "2502.16538v1",
    "authors": [
      "Mingyu Jeon",
      "Yeonji Paeng",
      "Sejin Lee"
    ],
    "published": "2025-02-23T11:17:20+00:00",
    "summary": "Underwater diving assistance and safety support robots acquire real-time diver information through onboard underwater cameras. This study introduces a breath bubble detection algorithm that utilizes unsupervised K-means clustering, thereby addressing the high accuracy demands of deep learning models as well as the challenges associated with constructing supervised datasets. The proposed method fuses color data and relative spatial coordinates from underwater images, employs CLAHE to mitigate noise, and subsequently performs pixel clustering to isolate reflective regions. Experimental results demonstrate that the algorithm can effectively detect regions corresponding to breath bubbles in underwater images, and that the combined use of RGB, LAB, and HSV color spaces significantly enhances detection accuracy. Overall, this research establishes a foundation for monitoring diver conditions and identifying potential equipment malfunctions in underwater environments."
  },
  {
    "title": "GraphCheck: Breaking Long-Term Text Barriers with Extracted Knowledge Graph-Powered Fact-Checking",
    "url": "http://arxiv.org/abs/2502.16514v1",
    "arxiv_id": "2502.16514v1",
    "authors": [
      "Yingjian Chen",
      "Haoran Liu",
      "Yinhong Liu",
      "Rui Yang",
      "Han Yuan",
      "Yanran Fu",
      "Pengyuan Zhou",
      "Qingyu Chen",
      "James Caverlee",
      "Irene Li"
    ],
    "published": "2025-02-23T09:25:00+00:00",
    "summary": "Large language models (LLMs) are widely used, but they often generate subtle factual errors, especially in long-form text. These errors are fatal in some specialized domains such as medicine. Existing fact-checking with grounding documents methods face two main challenges: (1) they struggle to understand complex multihop relations in long documents, often overlooking subtle factual errors; (2) most specialized methods rely on pairwise comparisons, requiring multiple model calls, leading to high resource and computational costs. To address these challenges, we propose \\textbf{\\textit{GraphCheck}}, a fact-checking framework that uses extracted knowledge graphs to enhance text representation. Graph Neural Networks further process these graphs as a soft prompt, enabling LLMs to incorporate structured knowledge more effectively. Enhanced with graph-based reasoning, GraphCheck captures multihop reasoning chains which are often overlooked by existing methods, enabling precise and efficient fact-checking in a single inference call. Experimental results on seven benchmarks spanning both general and medical domains demonstrate a 6.1\\% overall improvement over baseline models. Notably, GraphCheck outperforms existing specialized fact-checkers and achieves comparable performance with state-of-the-art LLMs, such as DeepSeek-V3 and OpenAI-o1, with significantly fewer parameters."
  },
  {
    "title": "FanChuan: A Multilingual and Graph-Structured Benchmark For Parody Detection and Analysis",
    "url": "http://arxiv.org/abs/2502.16503v1",
    "arxiv_id": "2502.16503v1",
    "authors": [
      "Yilun Zheng",
      "Sha Li",
      "Fangkun Wu",
      "Yang Ziyi",
      "Lin Hongchao",
      "Zhichao Hu",
      "Cai Xinjun",
      "Ziming Wang",
      "Jinxuan Chen",
      "Sitao Luan",
      "Jiahao Xu",
      "Lihui Chen"
    ],
    "published": "2025-02-23T08:52:46+00:00",
    "summary": "Parody is an emerging phenomenon on social media, where individuals imitate a role or position opposite to their own, often for humor, provocation, or controversy. Detecting and analyzing parody can be challenging and is often reliant on context, yet it plays a crucial role in understanding cultural values, promoting subcultures, and enhancing self-expression. However, the study of parody is hindered by limited available data and deficient diversity in current datasets. To bridge this gap, we built seven parody datasets from both English and Chinese corpora, with 14,755 annotated users and 21,210 annotated comments in total. To provide sufficient context information, we also collect replies and construct user-interaction graphs to provide richer contextual information, which is lacking in existing datasets. With these datasets, we test traditional methods and Large Language Models (LLMs) on three key tasks: (1) parody detection, (2) comment sentiment analysis with parody, and (3) user sentiment analysis with parody. Our extensive experiments reveal that parody-related tasks still remain challenging for all models, and contextual information plays a critical role. Interestingly, we find that, in certain scenarios, traditional sentence embedding methods combined with simple classifiers can outperform advanced LLMs, i.e. DeepSeek-R1 and GPT-o3, highlighting parody as a significant challenge for LLMs."
  },
  {
    "title": "Facilitating Emergency Vehicle Passage in Congested Urban Areas Using Multi-agent Deep Reinforcement Learning",
    "url": "http://arxiv.org/abs/2502.16449v1",
    "arxiv_id": "2502.16449v1",
    "authors": [
      "Haoran Su"
    ],
    "published": "2025-02-23T05:34:32+00:00",
    "summary": "Emergency Response Time (ERT) is crucial for urban safety, measuring cities' ability to handle medical, fire, and crime emergencies. In NYC, medical ERT increased 72% from 7.89 minutes in 2014 to 14.27 minutes in 2024, with half of delays due to Emergency Vehicle (EMV) travel times. Each minute's delay in stroke response costs 2 million brain cells, while cardiac arrest survival drops 7-10% per minute.   This dissertation advances EMV facilitation through three contributions. First, EMVLight, a decentralized multi-agent reinforcement learning framework, integrates EMV routing with traffic signal pre-emption. It achieved 42.6% faster EMV travel times and 23.5% improvement for other vehicles.   Second, the Dynamic Queue-Jump Lane system uses Multi-Agent Proximal Policy Optimization for coordinated lane-clearing in mixed autonomous and human-driven traffic, reducing EMV travel times by 40%.   Third, an equity study of NYC Emergency Medical Services revealed disparities across boroughs: Staten Island faces delays due to sparse signalized intersections, while Manhattan struggles with congestion. Solutions include optimized EMS stations and improved intersection designs.   These contributions enhance EMV mobility and emergency service equity, offering insights for policymakers and urban planners to develop safer, more efficient transportation systems."
  },
  {
    "title": "Visual Reasoning Evaluation of Grok, Deepseek Janus, Gemini, Qwen, Mistral, and ChatGPT",
    "url": "http://arxiv.org/abs/2502.16428v1",
    "arxiv_id": "2502.16428v1",
    "authors": [
      "Nidhal Jegham",
      "Marwan Abdelatti",
      "Abdeltawab Hendawi"
    ],
    "published": "2025-02-23T04:01:43+00:00",
    "summary": "Traditional evaluations of multimodal large language models (LLMs) have been limited by their focus on single-image reasoning, failing to assess crucial aspects like contextual understanding, reasoning stability, and uncertainty calibration. This study addresses these limitations by introducing a novel benchmark that integrates multi-image reasoning tasks with rejection-based evaluation and positional bias detection. To evaluate these dimensions, we further introduce entropy as a novel metric for quantifying reasoning consistency across reordered answer variants. We applied this benchmark to assess Grok 3, ChatGPT-4o, ChatGPT-o1, Gemini 2.0 Flash Experimental, DeepSeek Janus models, Qwen2.5-VL-72B-Instruct, QVQ-72B-Preview, and Pixtral 12B across eight visual reasoning tasks, including difference spotting and diagram interpretation. Our findings reveal ChatGPT-o1 leading in overall accuracy (82.5\\%) and rejection accuracy (70.0\\%), closely followed by Gemini 2.0 Flash Experimental (70.8\\%). QVQ-72B-Preview demonstrated superior rejection accuracy (85.5\\%). Notably, Pixtral 12B (51.7\\%) showed promise in specific domains, while Janus models exhibited challenges in bias and uncertainty calibration, reflected in low rejection accuracies and high entropy scores. High entropy scores in Janus models (Janus 7B: 0.8392, Janus 1B: 0.787) underscore their susceptibility to positional bias and unstable reasoning, contrasting with the low entropy and robust reasoning of ChatGPT models. The study further demonstrates that model size is not the sole determinant of performance, as evidenced by Grok 3 underperformance despite its substantial parameter count. By employing multi-image contexts, rejection mechanisms, and entropy-based consistency metrics, this benchmark sets a new standard for evaluating multimodal LLMs, enabling a more robust and reliable assessment of next-generation AI systems."
  },
  {
    "title": "Navigation-GPT: A Robust and Adaptive Framework Utilizing Large Language Models for Navigation Applications",
    "url": "http://arxiv.org/abs/2502.16402v1",
    "arxiv_id": "2502.16402v1",
    "authors": [
      "Feng Ma",
      "Xiu-min Wang",
      "Chen Chen",
      "Xiao-bin Xu",
      "Xin-ping Yan"
    ],
    "published": "2025-02-23T01:41:58+00:00",
    "summary": "Existing navigation decision support systems often perform poorly when handling non-predefined navigation scenarios. Leveraging the generalization capabilities of large language model (LLM) in handling unknown scenarios, this research proposes a dual-core framework for LLM applications to address this issue. Firstly, through ReAct-based prompt engineering, a larger LLM core decomposes intricate navigation tasks into manageable sub-tasks, which autonomously invoke corresponding external tools to gather relevant information, using this feedback to mitigate the risk of LLM hallucinations. Subsequently, a fine-tuned and compact LLM core, acting like a first-mate is designed to process such information and unstructured external data, then to generates context-aware recommendations, ultimately delivering lookout insights and navigation hints that adhere to the International Regulations for Preventing Collisions at Sea (COLREGs) and other rules. Extensive experiments demonstrate the proposed framework not only excels in traditional ship collision avoidance tasks but also adapts effectively to unstructured, non-predefined, and unpredictable scenarios. A comparative analysis with DeepSeek-R1, GPT-4o and other SOTA models highlights the efficacy and rationality of the proposed framework. This research bridges the gap between conventional navigation systems and LLMs, offering a framework to enhance safety and operational efficiency across diverse navigation applications."
  },
  {
    "title": "An Expert Ensemble for Detecting Anomalous Scenes, Interactions, and Behaviors in Autonomous Driving",
    "url": "http://arxiv.org/abs/2502.16389v1",
    "arxiv_id": "2502.16389v1",
    "authors": [
      "Tianchen Ji",
      "Neeloy Chakraborty",
      "Andre Schreiber",
      "Katherine Driggs-Campbell"
    ],
    "published": "2025-02-23T00:43:23+00:00",
    "summary": "As automated vehicles enter public roads, safety in a near-infinite number of driving scenarios becomes one of the major concerns for the widespread adoption of fully autonomous driving. The ability to detect anomalous situations outside of the operational design domain is a key component in self-driving cars, enabling us to mitigate the impact of abnormal ego behaviors and to realize trustworthy driving systems. On-road anomaly detection in egocentric videos remains a challenging problem due to the difficulties introduced by complex and interactive scenarios. We conduct a holistic analysis of common on-road anomaly patterns, from which we propose three unsupervised anomaly detection experts: a scene expert that focuses on frame-level appearances to detect abnormal scenes and unexpected scene motions; an interaction expert that models normal relative motions between two road participants and raises alarms whenever anomalous interactions emerge; and a behavior expert which monitors abnormal behaviors of individual objects by future trajectory prediction. To combine the strengths of all the modules, we propose an expert ensemble (Xen) using a Kalman filter, in which the final anomaly score is absorbed as one of the states and the observations are generated by the experts. Our experiments employ a novel evaluation protocol for realistic model performance, demonstrate superior anomaly detection performance than previous methods, and show that our framework has potential in classifying anomaly types using unsupervised learning on a large-scale on-road anomaly dataset."
  },
  {
    "title": "Understanding Generative AI Risks for Youth: A Taxonomy Based on Empirical Data",
    "url": "http://arxiv.org/abs/2502.16383v1",
    "arxiv_id": "2502.16383v1",
    "authors": [
      "Yaman Yu",
      "Yiren Liu",
      "Jacky Zhang",
      "Yun Huang",
      "Yang Wang"
    ],
    "published": "2025-02-22T23:31:51+00:00",
    "summary": "Generative AI (GAI) is reshaping the way young users engage with technology. This study introduces a taxonomy of risks associated with youth-GAI interactions, derived from an analysis of 344 chat transcripts between youth and GAI chatbots, 30,305 Reddit discussions concerning youth engagement with these systems, and 153 documented AI-related incidents. We categorize risks into six overarching themes, identifying 84 specific risks, which we further align with four distinct interaction pathways. Our findings highlight emerging concerns, such as risks to mental wellbeing, behavioral and social development, and novel forms of toxicity, privacy breaches, and misuse/exploitation that are not fully addressed in existing frameworks on child online safety or AI risks. By systematically grounding our taxonomy in empirical data, this work offers a structured approach to aiding AI developers, educators, caregivers, and policymakers in comprehending and mitigating risks associated with youth-GAI interactions."
  },
  {
    "title": "A generative approach to LLM harmfulness detection with special red flag tokens",
    "url": "http://arxiv.org/abs/2502.16366v1",
    "arxiv_id": "2502.16366v1",
    "authors": [
      "Sophie Xhonneux",
      "David Dobre",
      "Mehrnaz Mohfakhami",
      "Leo Schwinn",
      "Gauthier Gidel"
    ],
    "published": "2025-02-22T21:48:48+00:00",
    "summary": "Most safety training methods for large language models (LLMs) based on fine-tuning rely on dramatically changing the output distribution of the model when faced with a harmful request, shifting it from an unsafe answer to a refusal to respond. These methods inherently compromise model capabilities and might make auto-regressive models vulnerable to attacks that make likely an initial token of affirmative response. To avoid that, we propose to expand the model's vocabulary with a special token we call red flag token (<rf>) and propose to fine-tune the model to generate this token at any time harmful content is generated or about to be generated. This novel safety training method effectively augments LLMs into generative classifiers of harmfulness at all times during the conversation. This method offers several advantages: it enables the model to explicitly learn the concept of harmfulness while marginally affecting the generated distribution, thus maintaining the model's utility. It also evaluates each generated answer rather than just the input prompt and provides a stronger defence against sampling-based attacks. In addition, it simplifies the evaluation of the model's robustness and reduces correlated failures when combined with a classifier. We further show an increased robustness to long contexts, and supervised fine-tuning attacks."
  },
  {
    "title": "A Framework for Evaluating Vision-Language Model Safety: Building Trust in AI for Public Sector Applications",
    "url": "http://arxiv.org/abs/2502.16361v1",
    "arxiv_id": "2502.16361v1",
    "authors": [
      "Maisha Binte Rashid",
      "Pablo Rivas"
    ],
    "published": "2025-02-22T21:33:26+00:00",
    "summary": "Vision-Language Models (VLMs) are increasingly deployed in public sector missions, necessitating robust evaluation of their safety and vulnerability to adversarial attacks. This paper introduces a novel framework to quantify adversarial risks in VLMs. We analyze model performance under Gaussian, salt-and-pepper, and uniform noise, identifying misclassification thresholds and deriving composite noise patches and saliency patterns that highlight vulnerable regions. These patterns are compared against the Fast Gradient Sign Method (FGSM) to assess their adversarial effectiveness. We propose a new Vulnerability Score that combines the impact of random noise and adversarial attacks, providing a comprehensive metric for evaluating model robustness."
  },
  {
    "title": "Risk-Averse Reinforcement Learning: An Optimal Transport Perspective on Temporal Difference Learning",
    "url": "http://arxiv.org/abs/2502.16328v1",
    "arxiv_id": "2502.16328v1",
    "authors": [
      "Zahra Shahrooei",
      "Ali Baheri"
    ],
    "published": "2025-02-22T19:14:36+00:00",
    "summary": "The primary goal of reinforcement learning is to develop decision-making policies that prioritize optimal performance, frequently without considering risk or safety. In contrast, safe reinforcement learning seeks to reduce or avoid unsafe states. This letter introduces a risk-averse temporal difference algorithm that uses optimal transport theory to direct the agent toward predictable behavior. By incorporating a risk indicator, the agent learns to favor actions with predictable consequences. We evaluate the proposed algorithm in several case studies and show its effectiveness in the presence of uncertainty. The results demonstrate that our method reduces the frequency of visits to risky states while preserving performance. A Python implementation of the algorithm is available at https:// github.com/SAILRIT/Risk-averse-TD-Learning."
  },
  {
    "title": "Optimization-free Smooth Control Barrier Function for Polygonal Collision Avoidance",
    "url": "http://arxiv.org/abs/2502.16293v1",
    "arxiv_id": "2502.16293v1",
    "authors": [
      "Shizhen Wu",
      "Yongchun Fang",
      "Ning Sun",
      "Biao Lu",
      "Xiao Liang",
      "Yiming Zhao"
    ],
    "published": "2025-02-22T16:47:27+00:00",
    "summary": "Polygonal collision avoidance (PCA) is short for the problem of collision avoidance between two polygons (i.e., polytopes in planar) that own their dynamic equations. This problem suffers the inherent difficulty in dealing with non-smooth boundaries and recently optimization-defined metrics, such as signed distance field (SDF) and its variants, have been proposed as control barrier functions (CBFs) to tackle PCA problems. In contrast, we propose an optimization-free smooth CBF method in this paper, which is computationally efficient and proved to be nonconservative. It is achieved by three main steps: a lower bound of SDF is expressed as a nested Boolean logic composition first, then its smooth approximation is established by applying the latest log-sum-exp method, after which a specified CBF-based safety filter is proposed to address this class of problems. To illustrate its wide applications, the optimization-free smooth CBF method is extended to solve distributed collision avoidance of two underactuated nonholonomic vehicles and drive an underactuated container crane to avoid a moving obstacle respectively, for which numerical simulations are also performed."
  },
  {
    "title": "Pseudo-Measurement Enhancement in Power Distribution Systems",
    "url": "http://arxiv.org/abs/2502.16188v1",
    "arxiv_id": "2502.16188v1",
    "authors": [
      "Tao Xu",
      "Kaiqi Wang",
      "Jiadong Zhang",
      "Ji Qiao",
      "Zixuan Zhao",
      "Hong Zhu",
      "Kai Sun"
    ],
    "published": "2025-02-22T11:17:38+00:00",
    "summary": "With the rapid development of smart distribution networks (DNs), the integrity and accuracy of grid measurement data are crucial to the safety and stability of the entire system. However, the quality of the user power consumption data cannot be guaranteed during the collection and transmission process. To this end, this paper proposes a low-rank tensor completion model based on CANDECOMP/PARAFAC decomposition (CPD-LRTC) to enhance the quality of the measurement data of the DNs. Firstly, the causes and the associated characteristics of the missing data are analyzed, and a third-order standard tensor is constructed as a mathematical model of the measurement data of the DN. Then, a completion model is established based on the characteristics of measurement data and the low rank of the completion tensor, and the alternating direction method of multipliers (ADMM) is used to solve it iteratively. Finally, the proposed model is verified through two case studies, the completion accuracy, the computational efficiency, and the memory usage are compared to traditional methods."
  },
  {
    "title": "On Asymptotic safety in 4D gauge theory with additional dimension=4 operators",
    "url": "http://arxiv.org/abs/2502.16187v1",
    "arxiv_id": "2502.16187v1",
    "authors": [
      "Alfiia Mukhaeva"
    ],
    "published": "2025-02-22T11:15:11+00:00",
    "summary": "We study interacting fixed points of simple quantum field theory in four-dimensional $SU(N_c)$ coupled to $N_f$ species of color fermions and $N_f^2$ colorless scalars in the Veneziano limit. Using the rich structure of all possible quartic scalar operators, we find an interacting conformal fixed point with stable vacua and crossovers inbetween. We perform calculations in perturbation theory up to four loop in the gauge and three loop in the Yukawa and scalar couplings. We also consider anomalous dimensions for fields, scalar mass squared, and a class of dimension-three operators."
  },
  {
    "title": "Maybe I Should Not Answer That, but... Do LLMs Understand The Safety of Their Inputs?",
    "url": "http://arxiv.org/abs/2502.16174v1",
    "arxiv_id": "2502.16174v1",
    "authors": [
      "Maciej Chrab\u0105szcz",
      "Filip Szatkowski",
      "Bartosz W\u00f3jcik",
      "Jan Dubi\u0144ski",
      "Tomasz Trzci\u0144ski"
    ],
    "published": "2025-02-22T10:31:50+00:00",
    "summary": "Ensuring the safety of the Large Language Model (LLM) is critical, but currently used methods in most cases sacrifice the model performance to obtain increased safety or perform poorly on data outside of their adaptation distribution. We investigate existing methods for such generalization and find them insufficient. Surprisingly, while even plain LLMs recognize unsafe prompts, they may still generate unsafe responses. To avoid performance degradation and preserve safe performance, we advocate for a two-step framework, where we first identify unsafe prompts via a lightweight classifier, and apply a \"safe\" model only to such prompts. In particular, we explore the design of the safety detector in more detail, investigating the use of different classifier architectures and prompting techniques. Interestingly, we find that the final hidden state for the last token is enough to provide robust performance, minimizing false positives on benign data while performing well on malicious prompt detection. Additionally, we show that classifiers trained on the representations from different model layers perform comparably on the latest model layers, indicating that safety representation is present in the LLMs' hidden states at most model stages. Our work is a step towards efficient, representation-based safety mechanisms for LLMs."
  },
  {
    "title": "Be a Multitude to Itself: A Prompt Evolution Framework for Red Teaming",
    "url": "http://arxiv.org/abs/2502.16109v1",
    "arxiv_id": "2502.16109v1",
    "authors": [
      "Rui Li",
      "Peiyi Wang",
      "Jingyuan Ma",
      "Di Zhang",
      "Lei Sha",
      "Zhifang Sui"
    ],
    "published": "2025-02-22T06:13:19+00:00",
    "summary": "Large Language Models (LLMs) have gained increasing attention for their remarkable capacity, alongside concerns about safety arising from their potential to produce harmful content. Red teaming aims to find prompts that could elicit harmful responses from LLMs, and is essential to discover and mitigate safety risks before real-world deployment. However, manual red teaming is both time-consuming and expensive, rendering it unscalable. In this paper, we propose RTPE, a scalable evolution framework to evolve red teaming prompts across both breadth and depth dimensions, facilitating the automatic generation of numerous high-quality and diverse red teaming prompts. Specifically, in-breadth evolving employs a novel enhanced in-context learning method to create a multitude of quality prompts, whereas in-depth evolving applies customized transformation operations to enhance both content and form of prompts, thereby increasing diversity. Extensive experiments demonstrate that RTPE surpasses existing representative automatic red teaming methods on both attack success rate and diversity. In addition, based on 4,800 red teaming prompts created by RTPE, we further provide a systematic analysis of 8 representative LLMs across 8 sensitive topics."
  },
  {
    "title": "Online Learning of Danger Avoidance for Complex Structures of Musculoskeletal Humanoids and Its Applications",
    "url": "http://arxiv.org/abs/2502.16085v1",
    "arxiv_id": "2502.16085v1",
    "authors": [
      "Kento Kawaharazuka",
      "Naoki Hiraoka",
      "Yuya Koga",
      "Manabu Nishiura",
      "Yusuke Omura",
      "Yuki Asano",
      "Kei Okada",
      "Koji Kawasaki",
      "Masayuki Inaba"
    ],
    "published": "2025-02-22T05:16:18+00:00",
    "summary": "The complex structure of musculoskeletal humanoids makes it difficult to model them, and the inter-body interference and high internal muscle force are unavoidable. Although various safety mechanisms have been developed to solve this problem, it is important not only to deal with the dangers when they occur but also to prevent them from happening. In this study, we propose a method to learn a network outputting danger probability corresponding to the muscle length online so that the robot can gradually prevent dangers from occurring. Applications of this network for control are also described. The method is applied to the musculoskeletal humanoid, Musashi, and its effectiveness is verified."
  },
  {
    "title": "Multimodal Inconsistency Reasoning (MMIR): A New Benchmark for Multimodal Reasoning Models",
    "url": "http://arxiv.org/abs/2502.16033v1",
    "arxiv_id": "2502.16033v1",
    "authors": [
      "Qianqi Yan",
      "Yue Fan",
      "Hongquan Li",
      "Shan Jiang",
      "Yang Zhao",
      "Xinze Guan",
      "Ching-Chen Kuo",
      "Xin Eric Wang"
    ],
    "published": "2025-02-22T01:52:37+00:00",
    "summary": "Existing Multimodal Large Language Models (MLLMs) are predominantly trained and tested on consistent visual-textual inputs, leaving open the question of whether they can handle inconsistencies in real-world, layout-rich content. To bridge this gap, we propose the Multimodal Inconsistency Reasoning (MMIR) benchmark to assess MLLMs' ability to detect and reason about semantic mismatches in artifacts such as webpages, presentation slides, and posters. MMIR comprises 534 challenging samples, each containing synthetically injected errors across five reasoning-heavy categories: Factual Contradiction, Identity Misattribution, Contextual Mismatch, Quantitative Discrepancy, and Temporal/Spatial Incoherence. We evaluate six state-of-the-art MLLMs, showing that models with dedicated multimodal reasoning capabilities, such as o1, substantially outperform their counterparts while open-source models remain particularly vulnerable to inconsistency errors. Detailed error analyses further show that models excel in detecting inconsistencies confined to a single modality, particularly in text, but struggle with cross-modal conflicts and complex layouts. Probing experiments reveal that single-modality prompting, including Chain-of-Thought (CoT) and Set-of-Mark (SoM) methods, yields marginal gains, revealing a key bottleneck in cross-modal reasoning. Our findings highlight the need for advanced multimodal reasoning and point to future research on multimodal inconsistency."
  },
  {
    "title": "Cross-Model Transferability of Adversarial Patches in Real-time Segmentation for Autonomous Driving",
    "url": "http://arxiv.org/abs/2502.16012v1",
    "arxiv_id": "2502.16012v1",
    "authors": [
      "Prashant Shekhar",
      "Bidur Devkota",
      "Dumindu Samaraweera",
      "Laxima Niure Kandel",
      "Manoj Babu"
    ],
    "published": "2025-02-22T00:03:53+00:00",
    "summary": "Adversarial attacks pose a significant threat to deep learning models, particularly in safety-critical applications like healthcare and autonomous driving. Recently, patch based attacks have demonstrated effectiveness in real-time inference scenarios owing to their 'drag and drop' nature. Following this idea for Semantic Segmentation (SS), here we propose a novel Expectation Over Transformation (EOT) based adversarial patch attack that is more realistic for autonomous vehicles. To effectively train this attack we also propose a 'simplified' loss function that is easy to analyze and implement. Using this attack as our basis, we investigate whether adversarial patches once optimized on a specific SS model, can fool other models or architectures. We conduct a comprehensive cross-model transferability analysis of adversarial patches trained on SOTA Convolutional Neural Network (CNN) models such PIDNet-S, PIDNet-M and PIDNet-L, among others. Additionally, we also include the Segformer model to study transferability to Vision Transformers (ViTs). All of our analysis is conducted on the widely used Cityscapes dataset. Our study reveals key insights into how model architectures (CNN vs CNN or CNN vs. Transformer-based) influence attack susceptibility. In particular, we conclude that although the transferability (effectiveness) of attacks on unseen images of any dimension is really high, the attacks trained against one particular model are minimally effective on other models. And this was found to be true for both ViT and CNN based models. Additionally our results also indicate that for CNN-based models, the repercussions of patch attacks are local, unlike ViTs. Per-class analysis reveals that simple-classes like 'sky' suffer less misclassification than others. The code for the project is available at: https://github.com/p-shekhar/adversarial-patch-transferability"
  },
  {
    "title": "On the Design of Safe Continual RL Methods for Control of Nonlinear Systems",
    "url": "http://arxiv.org/abs/2502.15922v1",
    "arxiv_id": "2502.15922v1",
    "authors": [
      "Austin Coursey",
      "Marcos Quinones-Grueiro",
      "Gautam Biswas"
    ],
    "published": "2025-02-21T20:34:40+00:00",
    "summary": "Reinforcement learning (RL) algorithms have been successfully applied to control tasks associated with unmanned aerial vehicles and robotics. In recent years, safe RL has been proposed to allow the safe execution of RL algorithms in industrial and mission-critical systems that operate in closed loops. However, if the system operating conditions change, such as when an unknown fault occurs in the system, typical safe RL algorithms are unable to adapt while retaining past knowledge. Continual reinforcement learning algorithms have been proposed to address this issue. However, the impact of continual adaptation on the system's safety is an understudied problem. In this paper, we study the intersection of safe and continual RL. First, we empirically demonstrate that a popular continual RL algorithm, online elastic weight consolidation, is unable to satisfy safety constraints in non-linear systems subject to varying operating conditions. Specifically, we study the MuJoCo HalfCheetah and Ant environments with velocity constraints and sudden joint loss non-stationarity. Then, we show that an agent trained using constrained policy optimization, a safe RL algorithm, experiences catastrophic forgetting in continual learning settings. With this in mind, we explore a simple reward-shaping method to ensure that elastic weight consolidation prioritizes remembering both safety and task performance for safety-constrained, non-linear, and non-stationary dynamical systems."
  },
  {
    "title": "VaViM and VaVAM: Autonomous Driving through Video Generative Modeling",
    "url": "http://arxiv.org/abs/2502.15672v1",
    "arxiv_id": "2502.15672v1",
    "authors": [
      "Florent Bartoccioni",
      "Elias Ramzi",
      "Victor Besnier",
      "Shashanka Venkataramanan",
      "Tuan-Hung Vu",
      "Yihong Xu",
      "Loick Chambon",
      "Spyros Gidaris",
      "Serkan Odabas",
      "David Hurych",
      "Renaud Marlet",
      "Alexandre Boulch",
      "Mickael Chen",
      "\u00c9loi Zablocki",
      "Andrei Bursuc",
      "Eduardo Valle",
      "Matthieu Cord"
    ],
    "published": "2025-02-21T18:56:02+00:00",
    "summary": "We explore the potential of large-scale generative video models for autonomous driving, introducing an open-source auto-regressive video model (VaViM) and its companion video-action model (VaVAM) to investigate how video pre-training transfers to real-world driving. VaViM is a simple auto-regressive video model that predicts frames using spatio-temporal token sequences. We show that it captures the semantics and dynamics of driving scenes. VaVAM, the video-action model, leverages the learned representations of VaViM to generate driving trajectories through imitation learning. Together, the models form a complete perception-to-action pipeline. We evaluate our models in open- and closed-loop driving scenarios, revealing that video-based pre-training holds promise for autonomous driving. Key insights include the semantic richness of the learned representations, the benefits of scaling for video synthesis, and the complex relationship between model size, data, and safety metrics in closed-loop evaluations. We release code and model weights at https://github.com/valeoai/VideoActionModel"
  },
  {
    "title": "A Comprehensive Survey on the Trustworthiness of Large Language Models in Healthcare",
    "url": "http://arxiv.org/abs/2502.15871v1",
    "arxiv_id": "2502.15871v1",
    "authors": [
      "Manar Aljohani",
      "Jun Hou",
      "Sindhura Kommu",
      "Xuan Wang"
    ],
    "published": "2025-02-21T18:43:06+00:00",
    "summary": "The application of large language models (LLMs) in healthcare has the potential to revolutionize clinical decision-making, medical research, and patient care. As LLMs are increasingly integrated into healthcare systems, several critical challenges must be addressed to ensure their reliable and ethical deployment. These challenges include truthfulness, where models generate misleading information; privacy, with risks of unintentional data retention; robustness, requiring defenses against adversarial attacks; fairness, addressing biases in clinical outcomes; explainability, ensuring transparent decision-making; and safety, mitigating risks of misinformation and medical errors. Recently, researchers have begun developing benchmarks and evaluation frameworks to systematically assess the trustworthiness of LLMs. However, the trustworthiness of LLMs in healthcare remains underexplored, lacking a systematic review that provides a comprehensive understanding and future insights into this area. This survey bridges this gap by providing a comprehensive overview of the recent research of existing methodologies and solutions aimed at mitigating the above risks in healthcare. By focusing on key trustworthiness dimensions including truthfulness, privacy and safety, robustness, fairness and bias, and explainability, we present a thorough analysis of how these issues impact the reliability and ethical use of LLMs in healthcare. This paper highlights ongoing efforts and offers insights into future research directions to ensure the safe and trustworthy deployment of LLMs in healthcare."
  },
  {
    "title": "Superintelligent Agents Pose Catastrophic Risks: Can Scientist AI Offer a Safer Path?",
    "url": "http://arxiv.org/abs/2502.15657v1",
    "arxiv_id": "2502.15657v1",
    "authors": [
      "Yoshua Bengio",
      "Michael Cohen",
      "Damiano Fornasiere",
      "Joumana Ghosn",
      "Pietro Greiner",
      "Matt MacDermott",
      "S\u00f6ren Mindermann",
      "Adam Oberman",
      "Jesse Richardson",
      "Oliver Richardson",
      "Marc-Antoine Rondeau",
      "Pierre-Luc St-Charles",
      "David Williams-King"
    ],
    "published": "2025-02-21T18:28:36+00:00",
    "summary": "The leading AI companies are increasingly focused on building generalist AI agents -- systems that can autonomously plan, act, and pursue goals across almost all tasks that humans can perform. Despite how useful these systems might be, unchecked AI agency poses significant risks to public safety and security, ranging from misuse by malicious actors to a potentially irreversible loss of human control. We discuss how these risks arise from current AI training methods. Indeed, various scenarios and experiments have demonstrated the possibility of AI agents engaging in deception or pursuing goals that were not specified by human operators and that conflict with human interests, such as self-preservation. Following the precautionary principle, we see a strong need for safer, yet still useful, alternatives to the current agency-driven trajectory. Accordingly, we propose as a core building block for further advances the development of a non-agentic AI system that is trustworthy and safe by design, which we call Scientist AI. This system is designed to explain the world from observations, as opposed to taking actions in it to imitate or please humans. It comprises a world model that generates theories to explain data and a question-answering inference machine. Both components operate with an explicit notion of uncertainty to mitigate the risks of overconfident predictions. In light of these considerations, a Scientist AI could be used to assist human researchers in accelerating scientific progress, including in AI safety. In particular, our system can be employed as a guardrail against AI agents that might be created despite the risks involved. Ultimately, focusing on non-agentic AI may enable the benefits of AI innovation while avoiding the risks associated with the current trajectory. We hope these arguments will motivate researchers, developers, and policymakers to favor this safer path."
  },
  {
    "title": "Superintelligent Agents Pose Catastrophic Risks: Can Scientist AI Offer a Safer Path?",
    "url": "http://arxiv.org/abs/2502.15657v2",
    "arxiv_id": "2502.15657v2",
    "authors": [
      "Yoshua Bengio",
      "Michael Cohen",
      "Damiano Fornasiere",
      "Joumana Ghosn",
      "Pietro Greiner",
      "Matt MacDermott",
      "S\u00f6ren Mindermann",
      "Adam Oberman",
      "Jesse Richardson",
      "Oliver Richardson",
      "Marc-Antoine Rondeau",
      "Pierre-Luc St-Charles",
      "David Williams-King"
    ],
    "published": "2025-02-21T18:28:36+00:00",
    "summary": "The leading AI companies are increasingly focused on building generalist AI agents -- systems that can autonomously plan, act, and pursue goals across almost all tasks that humans can perform. Despite how useful these systems might be, unchecked AI agency poses significant risks to public safety and security, ranging from misuse by malicious actors to a potentially irreversible loss of human control. We discuss how these risks arise from current AI training methods. Indeed, various scenarios and experiments have demonstrated the possibility of AI agents engaging in deception or pursuing goals that were not specified by human operators and that conflict with human interests, such as self-preservation. Following the precautionary principle, we see a strong need for safer, yet still useful, alternatives to the current agency-driven trajectory. Accordingly, we propose as a core building block for further advances the development of a non-agentic AI system that is trustworthy and safe by design, which we call Scientist AI. This system is designed to explain the world from observations, as opposed to taking actions in it to imitate or please humans. It comprises a world model that generates theories to explain data and a question-answering inference machine. Both components operate with an explicit notion of uncertainty to mitigate the risks of overconfident predictions. In light of these considerations, a Scientist AI could be used to assist human researchers in accelerating scientific progress, including in AI safety. In particular, our system can be employed as a guardrail against AI agents that might be created despite the risks involved. Ultimately, focusing on non-agentic AI may enable the benefits of AI innovation while avoiding the risks associated with the current trajectory. We hope these arguments will motivate researchers, developers, and policymakers to favor this safer path."
  },
  {
    "title": "A Simulation Pipeline to Facilitate Real-World Robotic Reinforcement Learning Applications",
    "url": "http://arxiv.org/abs/2502.15649v1",
    "arxiv_id": "2502.15649v1",
    "authors": [
      "Jefferson Silveira",
      "Joshua A. Marshall",
      "Sidney N. Givigi Jr"
    ],
    "published": "2025-02-21T18:16:05+00:00",
    "summary": "Reinforcement learning (RL) has gained traction for its success in solving complex tasks for robotic applications. However, its deployment on physical robots remains challenging due to safety risks and the comparatively high costs of training. To avoid these problems, RL agents are often trained on simulators, which introduces a new problem related to the gap between simulation and reality. This paper presents an RL pipeline designed to help reduce the reality gap and facilitate developing and deploying RL policies for real-world robotic systems. The pipeline organizes the RL training process into an initial step for system identification and three training stages: core simulation training, high-fidelity simulation, and real-world deployment, each adding levels of realism to reduce the sim-to-real gap. Each training stage takes an input policy, improves it, and either passes the improved policy to the next stage or loops it back for further improvement. This iterative process continues until the policy achieves the desired performance. The pipeline's effectiveness is shown through a case study with the Boston Dynamics Spot mobile robot used in a surveillance application. The case study presents the steps taken at each pipeline stage to obtain an RL agent to control the robot's position and orientation."
  },
  {
    "title": "The Relationship Between Reasoning and Performance in Large Language Models -- o3 (mini) Thinks Harder, Not Longer",
    "url": "http://arxiv.org/abs/2502.15631v1",
    "arxiv_id": "2502.15631v1",
    "authors": [
      "Marthe Ballon",
      "Andres Algaba",
      "Vincent Ginis"
    ],
    "published": "2025-02-21T17:59:13+00:00",
    "summary": "Large language models have demonstrated remarkable progress in mathematical reasoning, leveraging chain-of-thought and test-time compute scaling. However, many open questions remain regarding the interplay between reasoning token usage and accuracy gains. In particular, when comparing models across generations, it is unclear whether improved performance results from longer reasoning chains or more efficient reasoning. We systematically analyze chain-of-thought length across o1-mini and o3-mini variants on the Omni-MATH benchmark, finding that o3-mini (m) achieves superior accuracy without requiring longer reasoning chains than o1-mini. Moreover, we show that accuracy generally declines as reasoning chains grow across all models and compute settings, even when controlling for difficulty of the questions. This accuracy drop is significantly smaller in more proficient models, suggesting that new generations of reasoning models use test-time compute more effectively. Finally, we highlight that while o3-mini (h) achieves a marginal accuracy gain over o3-mini (m), it does so by allocating substantially more reasoning tokens across all problems, even the ones that o3-mini (m) can already solve. These findings provide new insights into the relationship between model capability and reasoning length, with implications for efficiency, scaling, and evaluation methodologies."
  },
  {
    "title": "Pick-and-place Manipulation Across Grippers Without Retraining: A Learning-optimization Diffusion Policy Approach",
    "url": "http://arxiv.org/abs/2502.15613v1",
    "arxiv_id": "2502.15613v1",
    "authors": [
      "Xiangtong Yao",
      "Yirui Zhou",
      "Yuan Meng",
      "Liangyu Dong",
      "Lin Hong",
      "Zitao Zhang",
      "Zhenshan Bing",
      "Kai Huang",
      "Fuchun Sun",
      "Alois Knoll"
    ],
    "published": "2025-02-21T17:35:10+00:00",
    "summary": "Current robotic pick-and-place policies typically require consistent gripper configurations across training and inference. This constraint imposes high retraining or fine-tuning costs, especially for imitation learning-based approaches, when adapting to new end-effectors. To mitigate this issue, we present a diffusion-based policy with a hybrid learning-optimization framework, enabling zero-shot adaptation to novel grippers without additional data collection for retraining policy. During training, the policy learns manipulation primitives from demonstrations collected using a base gripper. At inference, a diffusion-based optimization strategy dynamically enforces kinematic and safety constraints, ensuring that generated trajectories align with the physical properties of unseen grippers. This is achieved through a constrained denoising procedure that adapts trajectories to gripper-specific parameters (e.g., tool-center-point offsets, jaw widths) while preserving collision avoidance and task feasibility. We validate our method on a Franka Panda robot across six gripper configurations, including 3D-printed fingertips, flexible silicone gripper, and Robotiq 2F-85 gripper. Our approach achieves a 93.3% average task success rate across grippers (vs. 23.3-26.7% for diffusion policy baselines), supporting tool-center-point variations of 16-23.5 cm and jaw widths of 7.5-11.5 cm. The results demonstrate that constrained diffusion enables robust cross-gripper manipulation while maintaining the sample efficiency of imitation learning, eliminating the need for gripper-specific retraining. Video and code are available at https://github.com/yaoxt3/GADP."
  },
  {
    "title": "SafeInt: Shielding Large Language Models from Jailbreak Attacks via Safety-Aware Representation Intervention",
    "url": "http://arxiv.org/abs/2502.15594v1",
    "arxiv_id": "2502.15594v1",
    "authors": [
      "Jiaqi Wu",
      "Chen Chen",
      "Chunyan Hou",
      "Xiaojie Yuan"
    ],
    "published": "2025-02-21T17:12:35+00:00",
    "summary": "With the widespread real-world deployment of large language models (LLMs), ensuring their behavior complies with safety standards has become crucial. Jailbreak attacks exploit vulnerabilities in LLMs to induce undesirable behavior, posing a significant threat to LLM safety. Previous defenses often fail to achieve both effectiveness and efficiency simultaneously. Defenses from a representation perspective offer new insights, but existing interventions cannot dynamically adjust representations based on the harmfulness of the queries. To address this limitation while ensuring both effectiveness and efficiency, we propose SafeIntervention (SafeInt), a novel defense method that shields LLMs from jailbreak attacks through safety-aware representation intervention. SafeInt is built on our analysis of the representations of jailbreak samples. It adjusts representation distributions of jailbreak samples through intervention to align them with the representations of unsafe samples while minimizing unnecessary perturbations to jailbreak-irrelevant representations. We conduct comprehensive experiments covering six jailbreak attacks, two jailbreak datasets, and two utility benchmarks. Experimental results demonstrate that SafeInt outperforms all baselines in defending LLMs against jailbreak attacks while largely maintaining utility. Additionally, we evaluate SafeInt against adaptive attacks and verify its effectiveness in mitigating real-time attacks."
  },
  {
    "title": "Interpreting and Steering LLMs with Mutual Information-based Explanations on Sparse Autoencoders",
    "url": "http://arxiv.org/abs/2502.15576v1",
    "arxiv_id": "2502.15576v1",
    "authors": [
      "Xuansheng Wu",
      "Jiayi Yuan",
      "Wenlin Yao",
      "Xiaoming Zhai",
      "Ninghao Liu"
    ],
    "published": "2025-02-21T16:36:42+00:00",
    "summary": "Large language models (LLMs) excel at handling human queries, but they can occasionally generate flawed or unexpected responses. Understanding their internal states is crucial for understanding their successes, diagnosing their failures, and refining their capabilities. Although sparse autoencoders (SAEs) have shown promise for interpreting LLM internal representations, limited research has explored how to better explain SAE features, i.e., understanding the semantic meaning of features learned by SAE. Our theoretical analysis reveals that existing explanation methods suffer from the frequency bias issue, where they emphasize linguistic patterns over semantic concepts, while the latter is more critical to steer LLM behaviors. To address this, we propose using a fixed vocabulary set for feature interpretations and designing a mutual information-based objective, aiming to better capture the semantic meaning behind these features. We further propose two runtime steering strategies that adjust the learned feature activations based on their corresponding explanations. Empirical results show that, compared to baselines, our method provides more discourse-level explanations and effectively steers LLM behaviors to defend against jailbreak attacks. These findings highlight the value of explanations for steering LLM behaviors in downstream applications. We will release our code and data once accepted."
  },
  {
    "title": "Model Privacy: A Unified Framework to Understand Model Stealing Attacks and Defenses",
    "url": "http://arxiv.org/abs/2502.15567v1",
    "arxiv_id": "2502.15567v1",
    "authors": [
      "Ganghua Wang",
      "Yuhong Yang",
      "Jie Ding"
    ],
    "published": "2025-02-21T16:29:11+00:00",
    "summary": "The use of machine learning (ML) has become increasingly prevalent in various domains, highlighting the importance of understanding and ensuring its safety. One pressing concern is the vulnerability of ML applications to model stealing attacks. These attacks involve adversaries attempting to recover a learned model through limited query-response interactions, such as those found in cloud-based services or on-chip artificial intelligence interfaces. While existing literature proposes various attack and defense strategies, these often lack a theoretical foundation and standardized evaluation criteria. In response, this work presents a framework called ``Model Privacy'', providing a foundation for comprehensively analyzing model stealing attacks and defenses. We establish a rigorous formulation for the threat model and objectives, propose methods to quantify the goodness of attack and defense strategies, and analyze the fundamental tradeoffs between utility and privacy in ML models. Our developed theory offers valuable insights into enhancing the security of ML models, especially highlighting the importance of the attack-specific structure of perturbations for effective defenses. We demonstrate the application of model privacy from the defender's perspective through various learning scenarios. Extensive experiments corroborate the insights and the effectiveness of defense mechanisms developed under the proposed framework."
  },
  {
    "title": "Estimating Vehicle Speed on Roadways Using RNNs and Transformers: A Video-based Approach",
    "url": "http://arxiv.org/abs/2502.15545v1",
    "arxiv_id": "2502.15545v1",
    "authors": [
      "Sai Krishna Reddy Mareddy",
      "Dhanush Upplapati",
      "Dhanush Kumar Antharam"
    ],
    "published": "2025-02-21T15:51:49+00:00",
    "summary": "This project explores the application of advanced machine learning models, specifically Long Short-Term Memory (LSTM), Gated Recurrent Units (GRU), and Transformers, to the task of vehicle speed estimation using video data. Traditional methods of speed estimation, such as radar and manual systems, are often constrained by high costs, limited coverage, and potential disruptions. In contrast, leveraging existing surveillance infrastructure and cutting-edge neural network architectures presents a non-intrusive, scalable solution. Our approach utilizes LSTM and GRU to effectively manage long-term dependencies within the temporal sequence of video frames, while Transformers are employed to harness their self-attention mechanisms, enabling the processing of entire sequences in parallel and focusing on the most informative segments of the data. This study demonstrates that both LSTM and GRU outperform basic Recurrent Neural Networks (RNNs) due to their advanced gating mechanisms. Furthermore, increasing the sequence length of input data consistently improves model accuracy, highlighting the importance of contextual information in dynamic environments. Transformers, in particular, show exceptional adaptability and robustness across varied sequence lengths and complexities, making them highly suitable for real-time applications in diverse traffic conditions. The findings suggest that integrating these sophisticated neural network models can significantly enhance the accuracy and reliability of automated speed detection systems, thus promising to revolutionize traffic management and road safety."
  },
  {
    "title": "NPB-Rust: NAS Parallel Benchmarks in Rust",
    "url": "http://arxiv.org/abs/2502.15536v1",
    "arxiv_id": "2502.15536v1",
    "authors": [
      "Eduardo M. Martins",
      "Leonardo G. Fa\u00e9",
      "Renato B. Hoffmann",
      "Lucas S. Bianchessi",
      "Dalvan Griebler"
    ],
    "published": "2025-02-21T15:39:29+00:00",
    "summary": "Parallel programming often requires developers to handle complex computational tasks that can yield many errors in its development cycle. Rust is a performant low-level language that promises memory safety guarantees with its compiler, making it an attractive option for HPC application developers. We identified that the Rust ecosystem could benefit from more comprehensive scientific benchmark suites for standardizing comparisons and research. The NAS Parallel Benchmarks (NPB) is a standardized suite for evaluating various hardware aspects and is often used to compare different frameworks for parallelism. Therefore, our contributions are a Rust version of NPB, an analysis of the expressiveness and performance of the language features, and parallelization strategies. We compare our implementation with consolidated sequential and parallel versions of NPB. Experimental results show that Rust's sequential version is 1.23\\% slower than Fortran and 5.59\\% faster than C++, while Rust with Rayon was slower than both Fortran and C++ with OpenMP."
  },
  {
    "title": "Depth-aware Fusion Method based on Image and 4D Radar Spectrum for 3D Object Detection",
    "url": "http://arxiv.org/abs/2502.15516v1",
    "arxiv_id": "2502.15516v1",
    "authors": [
      "Yue Sun",
      "Yeqiang Qian",
      "Chunxiang Wang",
      "Ming Yang"
    ],
    "published": "2025-02-21T15:14:30+00:00",
    "summary": "Safety and reliability are crucial for the public acceptance of autonomous driving. To ensure accurate and reliable environmental perception, intelligent vehicles must exhibit accuracy and robustness in various environments. Millimeter-wave radar, known for its high penetration capability, can operate effectively in adverse weather conditions such as rain, snow, and fog. Traditional 3D millimeter-wave radars can only provide range, Doppler, and azimuth information for objects. Although the recent emergence of 4D millimeter-wave radars has added elevation resolution, the radar point clouds remain sparse due to Constant False Alarm Rate (CFAR) operations. In contrast, cameras offer rich semantic details but are sensitive to lighting and weather conditions. Hence, this paper leverages these two highly complementary and cost-effective sensors, 4D millimeter-wave radar and camera. By integrating 4D radar spectra with depth-aware camera images and employing attention mechanisms, we fuse texture-rich images with depth-rich radar data in the Bird's Eye View (BEV) perspective, enhancing 3D object detection. Additionally, we propose using GAN-based networks to generate depth images from radar spectra in the absence of depth sensors, further improving detection accuracy."
  },
  {
    "title": "A modular risk concept for complex systems",
    "url": "http://arxiv.org/abs/2502.15482v1",
    "arxiv_id": "2502.15482v1",
    "authors": [
      "Dag McGeorge",
      "Jon Arne Glomsrud"
    ],
    "published": "2025-02-21T14:11:16+00:00",
    "summary": "Our ways of managing risk have in the past been adapted to changes in technology and society. Amidst the ongoing digital transformation, the ur-gency of adapting risk management to changing needs seems higher than ever. This paper starts with a brief historic overview of the development of risk management in the past. The paper motivates the views that for com-plex systems, risk should be controlled by enforcing constrains in a modular way at different system levels, that the constraints can be expressed as assur-ance contracts and that acceptable risk mitigation can be demonstrated in as-surance case modules. Based on extensive industry experience of the authors, a major contribution is to explain how already existing methodologies have been combined to cre-ate a concept for modular risk assessment. Examples from assurance of au-tonomous sea navigation and autonomous driving are used to illustrate the concept. Beyond the existing methodologies this paper generalizes risk con-straints to assurance contracts as an enabler of modular risk assessment spanning all relevant system levels and stakeholder perspectives while main-taining the dependencies between the system parts and accounting for emer-gent system behavior. Furthermore, the use of safety integrity levels (SIL) and similar concepts for assigning assurance rigor have been avoided in favor of direct assessment of assurance case argument rigor, because technology and applications change too fast to justify using past experience as evidence of validity of such prescriptive schemes. This paper aims to help practitioners making efficient and timely risk-informed decisions about complex integrated systems."
  },
  {
    "title": "Single-pass Detection of Jailbreaking Input in Large Language Models",
    "url": "http://arxiv.org/abs/2502.15435v1",
    "arxiv_id": "2502.15435v1",
    "authors": [
      "Leyla Naz Candogan",
      "Yongtao Wu",
      "Elias Abad Rocamora",
      "Grigorios G. Chrysos",
      "Volkan Cevher"
    ],
    "published": "2025-02-21T13:04:13+00:00",
    "summary": "Defending aligned Large Language Models (LLMs) against jailbreaking attacks is a challenging problem, with existing approaches requiring multiple requests or even queries to auxiliary LLMs, making them computationally heavy. Instead, we focus on detecting jailbreaking input in a single forward pass. Our method, called Single Pass Detection SPD, leverages the information carried by the logits to predict whether the output sentence will be harmful. This allows us to defend in just one forward pass. SPD can not only detect attacks effectively on open-source models, but also minimizes the misclassification of harmless inputs. Furthermore, we show that SPD remains effective even without complete logit access in GPT-3.5 and GPT-4. We believe that our proposed method offers a promising approach to efficiently safeguard LLMs against adversarial attacks."
  },
  {
    "title": "Pub-Guard-LLM: Detecting Fraudulent Biomedical Articles with Reliable Explanations",
    "url": "http://arxiv.org/abs/2502.15429v1",
    "arxiv_id": "2502.15429v1",
    "authors": [
      "Lihu Chen",
      "Shuojie Fu",
      "Gabriel Freedman",
      "Cemre Zor",
      "Guy Martin",
      "James Kinross",
      "Uddhav Vaghela",
      "Ovidiu Serban",
      "Francesca Toni"
    ],
    "published": "2025-02-21T12:54:56+00:00",
    "summary": "A significant and growing number of published scientific articles is found to involve fraudulent practices, posing a serious threat to the credibility and safety of research in fields such as medicine. We propose Pub-Guard-LLM, the first large language model-based system tailored to fraud detection of biomedical scientific articles. We provide three application modes for deploying Pub-Guard-LLM: vanilla reasoning, retrieval-augmented generation, and multi-agent debate. Each mode allows for textual explanations of predictions. To assess the performance of our system, we introduce an open-source benchmark, PubMed Retraction, comprising over 11K real-world biomedical articles, including metadata and retraction labels. We show that, across all modes, Pub-Guard-LLM consistently surpasses the performance of various baselines and provides more reliable explanations, namely explanations which are deemed more relevant and coherent than those generated by the baselines when evaluated by multiple assessment methods. By enhancing both detection performance and explainability in scientific fraud detection, Pub-Guard-LLM contributes to safeguarding research integrity with a novel, effective, open-source tool."
  },
  {
    "title": "Adversarial Prompt Evaluation: Systematic Benchmarking of Guardrails Against Prompt Input Attacks on LLMs",
    "url": "http://arxiv.org/abs/2502.15427v1",
    "arxiv_id": "2502.15427v1",
    "authors": [
      "Giulio Zizzo",
      "Giandomenico Cornacchia",
      "Kieran Fraser",
      "Muhammad Zaid Hameed",
      "Ambrish Rawat",
      "Beat Buesser",
      "Mark Purcell",
      "Pin-Yu Chen",
      "Prasanna Sattigeri",
      "Kush Varshney"
    ],
    "published": "2025-02-21T12:54:25+00:00",
    "summary": "As large language models (LLMs) become integrated into everyday applications, ensuring their robustness and security is increasingly critical. In particular, LLMs can be manipulated into unsafe behaviour by prompts known as jailbreaks. The variety of jailbreak styles is growing, necessitating the use of external defences known as guardrails. While many jailbreak defences have been proposed, not all defences are able to handle new out-of-distribution attacks due to the narrow segment of jailbreaks used to align them. Moreover, the lack of systematisation around defences has created significant gaps in their practical application. In this work, we perform systematic benchmarking across 15 different defences, considering a broad swathe of malicious and benign datasets. We find that there is significant performance variation depending on the style of jailbreak a defence is subject to. Additionally, we show that based on current datasets available for evaluation, simple baselines can display competitive out-of-distribution performance compared to many state-of-the-art defences. Code is available at https://github.com/IBM/Adversarial-Prompt-Evaluation."
  },
  {
    "title": "A biomechanical comparison of concussion and head acceleration events in elite-level American football and rugby union",
    "url": "http://arxiv.org/abs/2502.15405v1",
    "arxiv_id": "2502.15405v1",
    "authors": [
      "Gregory Tierney"
    ],
    "published": "2025-02-21T12:11:10+00:00",
    "summary": "Elite-level American football and rugby union are two high-contact sports with growing clinical and legal concerns over player safety, necessitating a comparative analysis. A biomechanical comparison of concussion and head acceleration events (HAEs) in elite-level American football and rugby union was undertaken. Rugby union players have a greater number of professional playing years and matches available in a season than their American football counterparts. Rugby union players have a greater number of concussions reported per match and a higher proportion of concussions occurring during training sessions, based on National Football League (NFL) and Rugby Football Union (RFU) injury reports. Preliminary findings indicate that rugby union forwards experience a higher incidence of HAEs per player match over lower and higher magnitude thresholds, than American football defensive players. Overall, elite-level rugby union appears less favourable than American football in in almost all metrics pertinent to concussion and HAE exposure in the biomechanical comparison undertaken. The findings highlight the critical importance of independence, scientific rigour, and transparency in future concussion and HAE biomechanics research and real-world implementation, ensuring the development of more effective mitigation strategies."
  },
  {
    "title": "Evaluating Social Biases in LLM Reasoning",
    "url": "http://arxiv.org/abs/2502.15361v1",
    "arxiv_id": "2502.15361v1",
    "authors": [
      "Xuyang Wu",
      "Jinming Nian",
      "Zhiqiang Tao",
      "Yi Fang"
    ],
    "published": "2025-02-21T10:16:07+00:00",
    "summary": "In the recent development of AI reasoning, large language models (LLMs) are trained to automatically generate chain-of-thought reasoning steps, which have demonstrated compelling performance on math and coding tasks. However, when bias is mixed within the reasoning process to form strong logical arguments, it could cause even more harmful results and further induce hallucinations. In this paper, we have evaluated the 8B and 32B variants of DeepSeek-R1 against their instruction tuned counterparts on the BBQ dataset, and investigated the bias that is elicited out and being amplified through reasoning steps. To the best of our knowledge, this empirical study is the first to assess bias issues in LLM reasoning."
  },
  {
    "title": "Drug-Target Interaction/Affinity Prediction: Deep Learning Models and Advances Review",
    "url": "http://arxiv.org/abs/2502.15346v1",
    "arxiv_id": "2502.15346v1",
    "authors": [
      "Ali Vefghi",
      "Zahed Rahmati",
      "Mohammad Akbari"
    ],
    "published": "2025-02-21T10:00:43+00:00",
    "summary": "Drug discovery remains a slow and expensive process that involves many steps, from detecting the target structure to obtaining approval from the Food and Drug Administration (FDA), and is often riddled with safety concerns. Accurate prediction of how drugs interact with their targets and the development of new drugs by using better methods and technologies have immense potential to speed up this process, ultimately leading to faster delivery of life-saving medications. Traditional methods used for drug-target interaction prediction show limitations, particularly in capturing complex relationships between drugs and their targets. As an outcome, deep learning models have been presented to overcome the challenges of interaction prediction through their precise and efficient end results. By outlining promising research avenues and models, each with a different solution but similar to the problem, this paper aims to give researchers a better idea of methods for even more accurate and efficient prediction of drug-target interaction, ultimately accelerating the development of more effective drugs. A total of 180 prediction methods for drug-target interactions were analyzed throughout the period spanning 2016 to 2025 using different frameworks based on machine learning, mainly deep learning and graph neural networks. Additionally, this paper discusses the novelty, architecture, and input representation of these models."
  },
  {
    "title": "Attention Eclipse: Manipulating Attention to Bypass LLM Safety-Alignment",
    "url": "http://arxiv.org/abs/2502.15334v1",
    "arxiv_id": "2502.15334v1",
    "authors": [
      "Pedram Zaree",
      "Md Abdullah Al Mamun",
      "Quazi Mishkatul Alam",
      "Yue Dong",
      "Ihsen Alouani",
      "Nael Abu-Ghazaleh"
    ],
    "published": "2025-02-21T09:38:00+00:00",
    "summary": "Recent research has shown that carefully crafted jailbreak inputs can induce large language models to produce harmful outputs, despite safety measures such as alignment. It is important to anticipate the range of potential Jailbreak attacks to guide effective defenses and accurate assessment of model safety. In this paper, we present a new approach for generating highly effective Jailbreak attacks that manipulate the attention of the model to selectively strengthen or weaken attention among different parts of the prompt. By harnessing attention loss, we develop more effective jailbreak attacks, that are also transferrable. The attacks amplify the success rate of existing Jailbreak algorithms including GCG, AutoDAN, and ReNeLLM, while lowering their generation cost (for example, the amplified GCG attack achieves 91.2% ASR, vs. 67.9% for the original attack on Llama2-7B/AdvBench, using less than a third of the generation time)."
  },
  {
    "title": "Understand User Opinions of Large Language Models via LLM-Powered In-the-Moment User Experience Interviews",
    "url": "http://arxiv.org/abs/2502.15226v1",
    "arxiv_id": "2502.15226v1",
    "authors": [
      "Mengqiao Liu",
      "Tevin Wang",
      "Cassandra A. Cohen",
      "Sarah Li",
      "Chenyan Xiong"
    ],
    "published": "2025-02-21T05:42:22+00:00",
    "summary": "Which large language model (LLM) is better? Every evaluation tells a story, but what do users really think about current LLMs? This paper presents CLUE, an LLM-powered interviewer that conducts in-the-moment user experience interviews, right after users interacted with LLMs, and automatically gathers insights about user opinions from massive interview logs. We conduct a study with thousands of users to understand user opinions on mainstream LLMs, recruiting users to first chat with a target LLM and then interviewed by CLUE. Our experiments demonstrate that CLUE captures interesting user opinions, for example, the bipolar views on the displayed reasoning process of DeepSeek-R1 and demands for information freshness and multi-modality. Our collected chat-and-interview logs will be released."
  },
  {
    "title": "Discrete implementations of sliding-mode controllers with barrier-function adaptations require a revised framework",
    "url": "http://arxiv.org/abs/2502.15201v1",
    "arxiv_id": "2502.15201v1",
    "authors": [
      "Luis Ovalle",
      "Andr\u00e9s Gonz\u00e1lez",
      "Leonid Fridman",
      "Hernan Haimovich"
    ],
    "published": "2025-02-21T04:29:24+00:00",
    "summary": "Challenges in the discrete implementation of sliding-mode controllers (SMC) with barrier-function-based adaptations are analyzed, revealing fundamental limitations in conventional design frameworks. It is shown that under uniform sampling, the original continuous-time problem motivating these controllers becomes theoretically unsolvable under standard assumptions. To address this incompatibility, a revised control framework is proposed, explicitly incorporating actuator capacity constraints and sampled-data dynamics. Within this structure, the behavior of barrier function-based adaptive controllers (BFASMC) is rigorously examined, explaining their empirical success in digital implementations. A key theoretical result establishes an explicit relation between the actuator capacity, the sampling rate, and the width of the barrier function, providing a principled means to tune these controllers for different application requirements. This relation enables the resolution of various design problems with direct practical implications. A modified BFASMC is then introduced, systematically leveraging sampling effects to ensure finite-time convergence to a positively invariant predefined set, a key advancement for guaranteeing predictable safety margins."
  },
  {
    "title": "OccProphet: Pushing Efficiency Frontier of Camera-Only 4D Occupancy Forecasting with Observer-Forecaster-Refiner Framework",
    "url": "http://arxiv.org/abs/2502.15180v1",
    "arxiv_id": "2502.15180v1",
    "authors": [
      "Junliang Chen",
      "Huaiyuan Xu",
      "Yi Wang",
      "Lap-Pui Chau"
    ],
    "published": "2025-02-21T03:21:48+00:00",
    "summary": "Predicting variations in complex traffic environments is crucial for the safety of autonomous driving. Recent advancements in occupancy forecasting have enabled forecasting future 3D occupied status in driving environments by observing historical 2D images. However, high computational demands make occupancy forecasting less efficient during training and inference stages, hindering its feasibility for deployment on edge agents. In this paper, we propose a novel framework, i.e., OccProphet, to efficiently and effectively learn occupancy forecasting with significantly lower computational requirements while improving forecasting accuracy. OccProphet comprises three lightweight components: Observer, Forecaster, and Refiner. The Observer extracts spatio-temporal features from 3D multi-frame voxels using the proposed Efficient 4D Aggregation with Tripling-Attention Fusion, while the Forecaster and Refiner conditionally predict and refine future occupancy inferences. Experimental results on nuScenes, Lyft-Level5, and nuScenes-Occupancy datasets demonstrate that OccProphet is both training- and inference-friendly. OccProphet reduces 58\\%$\\sim$78\\% of the computational cost with a 2.6$\\times$ speedup compared with the state-of-the-art Cam4DOcc. Moreover, it achieves 4\\%$\\sim$18\\% relatively higher forecasting accuracy. Code and models are publicly available at https://github.com/JLChen-C/OccProphet."
  },
  {
    "title": "CurricuVLM: Towards Safe Autonomous Driving via Personalized Safety-Critical Curriculum Learning with Vision-Language Models",
    "url": "http://arxiv.org/abs/2502.15119v1",
    "arxiv_id": "2502.15119v1",
    "authors": [
      "Zihao Sheng",
      "Zilin Huang",
      "Yansong Qu",
      "Yue Leng",
      "Sruthi Bhavanam",
      "Sikai Chen"
    ],
    "published": "2025-02-21T00:42:40+00:00",
    "summary": "Ensuring safety in autonomous driving systems remains a critical challenge, particularly in handling rare but potentially catastrophic safety-critical scenarios. While existing research has explored generating safety-critical scenarios for autonomous vehicle (AV) testing, there is limited work on effectively incorporating these scenarios into policy learning to enhance safety. Furthermore, developing training curricula that adapt to an AV's evolving behavioral patterns and performance bottlenecks remains largely unexplored. To address these challenges, we propose CurricuVLM, a novel framework that leverages Vision-Language Models (VLMs) to enable personalized curriculum learning for autonomous driving agents. Our approach uniquely exploits VLMs' multimodal understanding capabilities to analyze agent behavior, identify performance weaknesses, and dynamically generate tailored training scenarios for curriculum adaptation. Through comprehensive analysis of unsafe driving situations with narrative descriptions, CurricuVLM performs in-depth reasoning to evaluate the AV's capabilities and identify critical behavioral patterns. The framework then synthesizes customized training scenarios targeting these identified limitations, enabling effective and personalized curriculum learning. Extensive experiments on the Waymo Open Motion Dataset show that CurricuVLM outperforms state-of-the-art baselines across both regular and safety-critical scenarios, achieving superior performance in terms of navigation success, driving efficiency, and safety metrics. Further analysis reveals that CurricuVLM serves as a general approach that can be integrated with various RL algorithms to enhance autonomous driving systems. The code and demo video are available at: https://zihaosheng.github.io/CurricuVLM/."
  },
  {
    "title": "Is Safety Standard Same for Everyone? User-Specific Safety Evaluation of Large Language Models",
    "url": "http://arxiv.org/abs/2502.15086v1",
    "arxiv_id": "2502.15086v1",
    "authors": [
      "Yeonjun In",
      "Wonjoong Kim",
      "Kanghoon Yoon",
      "Sungchul Kim",
      "Mehrab Tanjim",
      "Kibum Kim",
      "Chanyoung Park"
    ],
    "published": "2025-02-20T22:58:44+00:00",
    "summary": "As the use of large language model (LLM) agents continues to grow, their safety vulnerabilities have become increasingly evident. Extensive benchmarks evaluate various aspects of LLM safety by defining the safety relying heavily on general standards, overlooking user-specific standards. However, safety standards for LLM may vary based on a user-specific profiles rather than being universally consistent across all users. This raises a critical research question: Do LLM agents act safely when considering user-specific safety standards? Despite its importance for safe LLM use, no benchmark datasets currently exist to evaluate the user-specific safety of LLMs. To address this gap, we introduce U-SAFEBENCH, the first benchmark designed to assess user-specific aspect of LLM safety. Our evaluation of 18 widely used LLMs reveals current LLMs fail to act safely when considering user-specific safety standards, marking a new discovery in this field. To address this vulnerability, we propose a simple remedy based on chain-of-thought, demonstrating its effectiveness in improving user-specific safety. Our benchmark and code are available at https://github.com/yeonjun-in/U-SafeBench."
  },
  {
    "title": "InterFeedback: Unveiling Interactive Intelligence of Large Multimodal Models via Human Feedback",
    "url": "http://arxiv.org/abs/2502.15027v1",
    "arxiv_id": "2502.15027v1",
    "authors": [
      "Henry Hengyuan Zhao",
      "Wenqi Pei",
      "Yifei Tao",
      "Haiyang Mei",
      "Mike Zheng Shou"
    ],
    "published": "2025-02-20T20:27:06+00:00",
    "summary": "Existing benchmarks do not test Large Multimodal Models (LMMs) on their interactive intelligence with human users which is vital for developing general-purpose AI assistants. We design InterFeedback, an interactive framework, which can be applied to any LMM and dataset to assess this ability autonomously. On top of this, we introduce InterFeedback-Bench which evaluates interactive intelligence using two representative datasets, MMMU-Pro and MathVerse, to test 10 different open-source LMMs. Additionally, we present InterFeedback-Human, a newly collected dataset of 120 cases designed for manually testing interactive performance in leading models such as OpenAI-o1 and Claude-3.5-Sonnet. Our evaluation results show that even state-of-the-art LMM (like OpenAI-o1) can correct their results through human feedback less than 50%. Our findings point to the need for methods that can enhance the LMMs' capability to interpret and benefit from feedback."
  },
  {
    "title": "Auxiliary-Variable Adaptive Control Barrier Functions",
    "url": "http://arxiv.org/abs/2502.15026v1",
    "arxiv_id": "2502.15026v1",
    "authors": [
      "Shuo Liu",
      "Wei Xiao",
      "Calin A. Belta"
    ],
    "published": "2025-02-20T20:23:09+00:00",
    "summary": "This paper addresses the challenge of ensuring safety and feasibility in control systems using Control Barrier Functions (CBFs). Existing CBF-based Quadratic Programs (CBF-QPs) often encounter feasibility issues due to mixed relative degree constraints, input nullification problems, and the presence of tight or time-varying control bounds, which can lead to infeasible solutions and compromised safety. To address these challenges, we propose Auxiliary-Variable Adaptive Control Barrier Functions (AVCBFs), a novel framework that introduces auxiliary functions to dynamically adjust CBF constraints without the need of excessive additional constraints. The AVCBF method ensures that all components of the control input explicitly appear in the desired-order safety constraint, thereby improving feasibility while maintaining safety guarantees. Additionally, we introduce an automatic tuning method that iteratively adjusts AVCBF hyperparameters to ensure feasibility and safety with less conservatism. We demonstrate the effectiveness of the proposed approach in adaptive cruise control and obstacle avoidance scenarios, showing that AVCBFs outperform existing CBF methods by reducing infeasibility and enhancing adaptive safety control under tight or time-varying control bounds."
  },
  {
    "title": "Safe Beyond the Horizon: Efficient Sampling-based MPC with Neural Control Barrier Functions",
    "url": "http://arxiv.org/abs/2502.15006v1",
    "arxiv_id": "2502.15006v1",
    "authors": [
      "Ji Yin",
      "Oswin So",
      "Eric Yang Yu",
      "Chuchu Fan",
      "Panagiotis Tsiotras"
    ],
    "published": "2025-02-20T19:59:11+00:00",
    "summary": "A common problem when using model predictive control (MPC) in practice is the satisfaction of safety specifications beyond the prediction horizon. While theoretical works have shown that safety can be guaranteed by enforcing a suitable terminal set constraint or a sufficiently long prediction horizon, these techniques are difficult to apply and thus are rarely used by practitioners, especially in the case of general nonlinear dynamics. To solve this problem, we impose a tradeoff between exact recursive feasibility, computational tractability, and applicability to ''black-box'' dynamics by learning an approximate discrete-time control barrier function and incorporating it into a variational inference MPC (VIMPC), a sampling-based MPC paradigm. To handle the resulting state constraints, we further propose a new sampling strategy that greatly reduces the variance of the estimated optimal control, improving the sample efficiency, and enabling real-time planning on a CPU. The resulting Neural Shield-VIMPC (NS-VIMPC) controller yields substantial safety improvements compared to existing sampling-based MPC controllers, even under badly designed cost functions. We validate our approach in both simulation and real-world hardware experiments."
  },
  {
    "title": "Probabilistic Robustness in Deep Learning: A Concise yet Comprehensive Guide",
    "url": "http://arxiv.org/abs/2502.14833v1",
    "arxiv_id": "2502.14833v1",
    "authors": [
      "Xingyu Zhao"
    ],
    "published": "2025-02-20T18:47:17+00:00",
    "summary": "Deep learning (DL) has demonstrated significant potential across various safety-critical applications, yet ensuring its robustness remains a key challenge. While adversarial robustness has been extensively studied in worst-case scenarios, probabilistic robustness (PR) offers a more practical perspective by quantifying the likelihood of failures under stochastic perturbations. This paper provides a concise yet comprehensive overview of PR, covering its formal definitions, evaluation and enhancement methods. We introduce a reformulated ''min-max'' optimisation framework for adversarial training specifically designed to improve PR. Furthermore, we explore the integration of PR verification evidence into system-level safety assurance, addressing challenges in translating DL model-level robustness to system-level claims. Finally, we highlight open research questions, including benchmarking PR evaluation methods, extending PR to generative AI tasks, and developing rigorous methodologies and case studies for system-level integration."
  },
  {
    "title": "Logic-RL: Unleashing LLM Reasoning with Rule-Based Reinforcement Learning",
    "url": "http://arxiv.org/abs/2502.14768v1",
    "arxiv_id": "2502.14768v1",
    "authors": [
      "Tian Xie",
      "Zitian Gao",
      "Qingnan Ren",
      "Haoming Luo",
      "Yuqian Hong",
      "Bryan Dai",
      "Joey Zhou",
      "Kai Qiu",
      "Zhirong Wu",
      "Chong Luo"
    ],
    "published": "2025-02-20T17:49:26+00:00",
    "summary": "Inspired by the success of DeepSeek-R1, we explore the potential of rule-based reinforcement learning (RL) in large reasoning models. To analyze reasoning dynamics, we use synthetic logic puzzles as training data due to their controllable complexity and straightforward answer verification. We make some key technical contributions that lead to effective and stable RL training: a system prompt that emphasizes the thinking and answering process, a stringent format reward function that penalizes outputs for taking shortcuts, and a straightforward training recipe that achieves stable convergence. Our 7B model develops advanced reasoning skills-such as reflection, verification, and summarization-that are absent from the logic corpus. Remarkably, after training on just 5K logic problems, it demonstrates generalization abilities to the challenging math benchmarks AIME and AMC."
  },
  {
    "title": "HiddenDetect: Detecting Jailbreak Attacks against Large Vision-Language Models via Monitoring Hidden States",
    "url": "http://arxiv.org/abs/2502.14744v1",
    "arxiv_id": "2502.14744v1",
    "authors": [
      "Yilei Jiang",
      "Xinyan Gao",
      "Tianshuo Peng",
      "Yingshui Tan",
      "Xiaoyong Zhu",
      "Bo Zheng",
      "Xiangyu Yue"
    ],
    "published": "2025-02-20T17:14:34+00:00",
    "summary": "The integration of additional modalities increases the susceptibility of large vision-language models (LVLMs) to safety risks, such as jailbreak attacks, compared to their language-only counterparts. While existing research primarily focuses on post-hoc alignment techniques, the underlying safety mechanisms within LVLMs remain largely unexplored. In this work , we investigate whether LVLMs inherently encode safety-relevant signals within their internal activations during inference. Our findings reveal that LVLMs exhibit distinct activation patterns when processing unsafe prompts, which can be leveraged to detect and mitigate adversarial inputs without requiring extensive fine-tuning. Building on this insight, we introduce HiddenDetect, a novel tuning-free framework that harnesses internal model activations to enhance safety. Experimental results show that {HiddenDetect} surpasses state-of-the-art methods in detecting jailbreak attacks against LVLMs. By utilizing intrinsic safety-aware patterns, our method provides an efficient and scalable solution for strengthening LVLM robustness against multimodal threats. Our code will be released publicly at https://github.com/leigest519/HiddenDetect."
  },
  {
    "title": "HiddenDetect: Detecting Jailbreak Attacks against Large Vision-Language Models via Monitoring Hidden States",
    "url": "http://arxiv.org/abs/2502.14744v2",
    "arxiv_id": "2502.14744v2",
    "authors": [
      "Yilei Jiang",
      "Xinyan Gao",
      "Tianshuo Peng",
      "Yingshui Tan",
      "Xiaoyong Zhu",
      "Bo Zheng",
      "Xiangyu Yue"
    ],
    "published": "2025-02-20T17:14:34+00:00",
    "summary": "The integration of additional modalities increases the susceptibility of large vision-language models (LVLMs) to safety risks, such as jailbreak attacks, compared to their language-only counterparts. While existing research primarily focuses on post-hoc alignment techniques, the underlying safety mechanisms within LVLMs remain largely unexplored. In this work , we investigate whether LVLMs inherently encode safety-relevant signals within their internal activations during inference. Our findings reveal that LVLMs exhibit distinct activation patterns when processing unsafe prompts, which can be leveraged to detect and mitigate adversarial inputs without requiring extensive fine-tuning. Building on this insight, we introduce HiddenDetect, a novel tuning-free framework that harnesses internal model activations to enhance safety. Experimental results show that {HiddenDetect} surpasses state-of-the-art methods in detecting jailbreak attacks against LVLMs. By utilizing intrinsic safety-aware patterns, our method provides an efficient and scalable solution for strengthening LVLM robustness against multimodal threats. Our code will be released publicly at https://github.com/leigest519/HiddenDetect."
  },
  {
    "title": "SuperGPQA: Scaling LLM Evaluation across 285 Graduate Disciplines",
    "url": "http://arxiv.org/abs/2502.14739v1",
    "arxiv_id": "2502.14739v1",
    "authors": [
      "M-A-P Team",
      "Xinrun Du",
      "Yifan Yao",
      "Kaijing Ma",
      "Bingli Wang",
      "Tianyu Zheng",
      "Kang Zhu",
      "Minghao Liu",
      "Yiming Liang",
      "Xiaolong Jin",
      "Zhenlin Wei",
      "Chujie Zheng",
      "Kaixing Deng",
      "Shuyue Guo",
      "Shian Jia",
      "Sichao Jiang",
      "Yiyan Liao",
      "Rui Li",
      "Qinrui Li",
      "Sirun Li",
      "Yizhi Li",
      "Yunwen Li",
      "Dehua Ma",
      "Yuansheng Ni",
      "Haoran Que",
      "Qiyao Wang",
      "Zhoufutu Wen",
      "Siwei Wu",
      "Tianshun Xing",
      "Ming Xu",
      "Zhenzhu Yang",
      "Zekun Moore Wang",
      "Junting Zhou",
      "Yuelin Bai",
      "Xingyuan Bu",
      "Chenglin Cai",
      "Liang Chen",
      "Yifan Chen",
      "Chengtuo Cheng",
      "Tianhao Cheng",
      "Keyi Ding",
      "Siming Huang",
      "Yun Huang",
      "Yaoru Li",
      "Yizhe Li",
      "Zhaoqun Li",
      "Tianhao Liang",
      "Chengdong Lin",
      "Hongquan Lin",
      "Yinghao Ma",
      "Zhongyuan Peng",
      "Zifan Peng",
      "Qige Qi",
      "Shi Qiu",
      "Xingwei Qu",
      "Yizhou Tan",
      "Zili Wang",
      "Chenqing Wang",
      "Hao Wang",
      "Yiya Wang",
      "Yubo Wang",
      "Jiajun Xu",
      "Kexin Yang",
      "Ruibin Yuan",
      "Yuanhao Yue",
      "Tianyang Zhan",
      "Chun Zhang",
      "Jingyang Zhang",
      "Xiyue Zhang",
      "Xingjian Zhang",
      "Yue Zhang",
      "Yongchi Zhao",
      "Xiangyu Zheng",
      "Chenghua Zhong",
      "Yang Gao",
      "Zhoujun Li",
      "Dayiheng Liu",
      "Qian Liu",
      "Tianyu Liu",
      "Shiwen Ni",
      "Junran Peng",
      "Yujia Qin",
      "Wenbo Su",
      "Guoyin Wang",
      "Shi Wang",
      "Jian Yang",
      "Min Yang",
      "Meng Cao",
      "Xiang Yue",
      "Zhaoxiang Zhang",
      "Wangchunshu Zhou",
      "Jiaheng Liu",
      "Qunshu Lin",
      "Wenhao Huang",
      "Ge Zhang"
    ],
    "published": "2025-02-20T17:05:58+00:00",
    "summary": "Large language models (LLMs) have demonstrated remarkable proficiency in mainstream academic disciplines such as mathematics, physics, and computer science. However, human knowledge encompasses over 200 specialized disciplines, far exceeding the scope of existing benchmarks. The capabilities of LLMs in many of these specialized fields-particularly in light industry, agriculture, and service-oriented disciplines-remain inadequately evaluated. To address this gap, we present SuperGPQA, a comprehensive benchmark that evaluates graduate-level knowledge and reasoning capabilities across 285 disciplines. Our benchmark employs a novel Human-LLM collaborative filtering mechanism to eliminate trivial or ambiguous questions through iterative refinement based on both LLM responses and expert feedback. Our experimental results reveal significant room for improvement in the performance of current state-of-the-art LLMs across diverse knowledge domains (e.g., the reasoning-focused model DeepSeek-R1 achieved the highest accuracy of 61.82% on SuperGPQA), highlighting the considerable gap between current model capabilities and artificial general intelligence. Additionally, we present comprehensive insights from our management of a large-scale annotation process, involving over 80 expert annotators and an interactive Human-LLM collaborative system, offering valuable methodological guidance for future research initiatives of comparable scope."
  },
  {
    "title": "PEARL: Towards Permutation-Resilient LLMs",
    "url": "http://arxiv.org/abs/2502.14628v1",
    "arxiv_id": "2502.14628v1",
    "authors": [
      "Liang Chen",
      "Li Shen",
      "Yang Deng",
      "Xiaoyan Zhao",
      "Bin Liang",
      "Kam-Fai Wong"
    ],
    "published": "2025-02-20T15:07:02+00:00",
    "summary": "The in-context learning (ICL) capability of large language models (LLMs) enables them to perform challenging tasks using provided demonstrations. However, ICL is highly sensitive to the ordering of demonstrations, leading to instability in predictions. This paper shows that this vulnerability can be exploited to design a natural attack - difficult for model providers to detect - that achieves nearly 80% success rate on LLaMA-3 by simply permuting the demonstrations. Existing mitigation methods primarily rely on post-processing and fail to enhance the model's inherent robustness to input permutations, raising concerns about safety and reliability of LLMs. To address this issue, we propose Permutation-resilient learning (PEARL), a novel framework based on distributionally robust optimization (DRO), which optimizes model performance against the worst-case input permutation. Specifically, PEARL consists of a permutation-proposal network (P-Net) and the LLM. The P-Net generates the most challenging permutations by treating it as an optimal transport problem, which is solved using an entropy-constrained Sinkhorn algorithm. Through minimax optimization, the P-Net and the LLM iteratively optimize against each other, progressively improving the LLM's robustness. Experiments on synthetic pre-training and real-world instruction tuning tasks demonstrate that PEARL effectively mitigates permutation attacks and enhances performance. Notably, despite being trained on fewer shots and shorter contexts, PEARL achieves performance gains of up to 40% when scaled to many-shot and long-context scenarios, highlighting its efficiency and generalization capabilities."
  },
  {
    "title": "A Stackelberg Game Approach for Signal Temporal Logic Control Synthesis with Uncontrollable Agents",
    "url": "http://arxiv.org/abs/2502.14585v1",
    "arxiv_id": "2502.14585v1",
    "authors": [
      "Bohan Cui",
      "Xinyi Yu",
      "Alessandro Giua",
      "Xiang Yin"
    ],
    "published": "2025-02-20T14:14:42+00:00",
    "summary": "In this paper, we investigate the control synthesis problem for Signal Temporal Logic (STL) specifications in the presence of uncontrollable agents. Existing works mainly address this problem in a robust control setting by assuming the uncontrollable agents are adversarial and accounting for the worst-case scenario. While this approach ensures safety, it can be overly conservative in scenarios where uncontrollable agents have their own objectives that are not entirely opposed to the system's goals. Motivated by this limitation, we propose a new framework for STL control synthesis within the Stackelberg game setting. Specifically, we assume that the system controller, acting as the leader, first commits to a plan, after which the uncontrollable agents, acting as followers, take a best response based on the committed plan and their own objectives. Our goal is to synthesize a control sequence for the leader such that, for any rational followers producing a best response, the leader's STL task is guaranteed to be satisfied. We present an effective solution to this problem by transforming it into a single-stage optimization problem and leveraging counter-example guided synthesis techniques. We demonstrate that the proposed approach is sound and identify conditions under which it is optimal. Simulation results are also provided to illustrate the effectiveness of the proposed framework."
  },
  {
    "title": "Real-world Troublemaker: A Novel Track Testing Framework for Automated Driving Systems in Safety-critical Interaction Scenarios",
    "url": "http://arxiv.org/abs/2502.14574v1",
    "arxiv_id": "2502.14574v1",
    "authors": [
      "Xinrui Zhang",
      "Lu Xiong",
      "Peizhi Zhang",
      "Junpeng Huang",
      "Yining Ma"
    ],
    "published": "2025-02-20T13:59:57+00:00",
    "summary": "Track testing plays a critical role in the safety evaluation of autonomous driving systems (ADS), as it provides real-world object targets and a safety-controllable interaction environment. However, existing track testing scenarios are often pre-fixed and limited, primarily due to the inflexibility of object target control methods and the lack of intelligent interactive behaviors. To overcome this limitation, we propose a novel track testing framework, Real-world Troublemaker, which can generate adversarial object target motion trajectories and facilitate intelligent interactions with the vehicle under test (VUT), creating a more realistic and dynamic testing environment. To enable flexible motion trajectories, cloud-controlled technology is utilized to remotely and dynamically control object targets to create a realistic traffic environment. To achieve intelligent interactions, an interactive concrete scenario generation method is introduced within a game-theoretic structure. The proposed framework has been successfully implemented at the Tongji University Intelligent Connected Vehicle Evaluation Base. Field test results demonstrate that Troublemaker can perform dynamic interactive testing of ADS accurately and effectively. Compared to traditional track testing methods, Troublemaker improves scenario reproduction accuracy by 65.2\\%, increases the diversity of target vehicle interaction strategies by approximately 9.2 times, and enhances exposure frequency of safety-critical scenarios by 3.5 times in unprotected left-turn scenarios."
  },
  {
    "title": "Real-world Troublemaker: A 5G Cloud-controlled Track Testing Framework for Automated Driving Systems in Safety-critical Interaction Scenarios",
    "url": "http://arxiv.org/abs/2502.14574v2",
    "arxiv_id": "2502.14574v2",
    "authors": [
      "Xinrui Zhang",
      "Lu Xiong",
      "Peizhi Zhang",
      "Junpeng Huang",
      "Yining Ma"
    ],
    "published": "2025-02-20T13:59:57+00:00",
    "summary": "Track testing plays a critical role in the safety evaluation of autonomous driving systems (ADS), as it provides a real-world interaction environment. However, the inflexibility in motion control of object targets and the absence of intelligent interactive testing methods often result in pre-fixed and limited testing scenarios. To address these limitations, we propose a novel 5G cloud-controlled track testing framework, Real-world Troublemaker. This framework overcomes the rigidity of traditional pre-programmed control by leveraging 5G cloud-controlled object targets integrated with the Internet of Things (IoT) and vehicle teleoperation technologies. Unlike conventional testing methods that rely on pre-set conditions, we propose a dynamic game strategy based on a quadratic risk interaction utility function, facilitating intelligent interactions with the vehicle under test (VUT) and creating a more realistic and dynamic interaction environment. The proposed framework has been successfully implemented at the Tongji University Intelligent Connected Vehicle Evaluation Base. Field test results demonstrate that Troublemaker can perform dynamic interactive testing of ADS accurately and effectively. Compared to traditional methods, Troublemaker improves scenario reproduction accuracy by 65.2\\%, increases the diversity of interaction strategies by approximately 9.2 times, and enhances exposure frequency of safety-critical scenarios by 3.5 times in unprotected left-turn scenarios."
  },
  {
    "title": "CORBA: Contagious Recursive Blocking Attacks on Multi-Agent Systems Based on Large Language Models",
    "url": "http://arxiv.org/abs/2502.14529v1",
    "arxiv_id": "2502.14529v1",
    "authors": [
      "Zhenhong Zhou",
      "Zherui Li",
      "Jie Zhang",
      "Yuanhe Zhang",
      "Kun Wang",
      "Yang Liu",
      "Qing Guo"
    ],
    "published": "2025-02-20T13:02:00+00:00",
    "summary": "Large Language Model-based Multi-Agent Systems (LLM-MASs) have demonstrated remarkable real-world capabilities, effectively collaborating to complete complex tasks. While these systems are designed with safety mechanisms, such as rejecting harmful instructions through alignment, their security remains largely unexplored. This gap leaves LLM-MASs vulnerable to targeted disruptions. In this paper, we introduce Contagious Recursive Blocking Attacks (Corba), a novel and simple yet highly effective attack that disrupts interactions between agents within an LLM-MAS. Corba leverages two key properties: its contagious nature allows it to propagate across arbitrary network topologies, while its recursive property enables sustained depletion of computational resources. Notably, these blocking attacks often involve seemingly benign instructions, making them particularly challenging to mitigate using conventional alignment methods. We evaluate Corba on two widely-used LLM-MASs, namely, AutoGen and Camel across various topologies and commercial models. Additionally, we conduct more extensive experiments in open-ended interactive LLM-MASs, demonstrating the effectiveness of Corba in complex topology structures and open-source models. Our code is available at: https://github.com/zhrli324/Corba."
  },
  {
    "title": "MLGym: A New Framework and Benchmark for Advancing AI Research Agents",
    "url": "http://arxiv.org/abs/2502.14499v1",
    "arxiv_id": "2502.14499v1",
    "authors": [
      "Deepak Nathani",
      "Lovish Madaan",
      "Nicholas Roberts",
      "Nikolay Bashlykov",
      "Ajay Menon",
      "Vincent Moens",
      "Amar Budhiraja",
      "Despoina Magka",
      "Vladislav Vorotilov",
      "Gaurav Chaurasia",
      "Dieuwke Hupkes",
      "Ricardo Silveira Cabral",
      "Tatiana Shavrina",
      "Jakob Foerster",
      "Yoram Bachrach",
      "William Yang Wang",
      "Roberta Raileanu"
    ],
    "published": "2025-02-20T12:28:23+00:00",
    "summary": "We introduce Meta MLGym and MLGym-Bench, a new framework and benchmark for evaluating and developing LLM agents on AI research tasks. This is the first Gym environment for machine learning (ML) tasks, enabling research on reinforcement learning (RL) algorithms for training such agents. MLGym-bench consists of 13 diverse and open-ended AI research tasks from diverse domains such as computer vision, natural language processing, reinforcement learning, and game theory. Solving these tasks requires real-world AI research skills such as generating new ideas and hypotheses, creating and processing data, implementing ML methods, training models, running experiments, analyzing the results, and iterating through this process to improve on a given task. We evaluate a number of frontier large language models (LLMs) on our benchmarks such as Claude-3.5-Sonnet, Llama-3.1 405B, GPT-4o, o1-preview, and Gemini-1.5 Pro. Our MLGym framework makes it easy to add new tasks, integrate and evaluate models or agents, generate synthetic data at scale, as well as develop new learning algorithms for training agents on AI research tasks. We find that current frontier models can improve on the given baselines, usually by finding better hyperparameters, but do not generate novel hypotheses, algorithms, architectures, or substantial improvements. We open-source our framework and benchmark to facilitate future research in advancing the AI research capabilities of LLM agents."
  },
  {
    "title": "Statistical Scenario Modelling and Lookalike Distributions for Multi-Variate AI Risk",
    "url": "http://arxiv.org/abs/2502.14491v1",
    "arxiv_id": "2502.14491v1",
    "authors": [
      "Elija Perrier"
    ],
    "published": "2025-02-20T12:14:54+00:00",
    "summary": "Evaluating AI safety requires statistically rigorous methods and risk metrics for understanding how the use of AI affects aggregated risk. However, much AI safety literature focuses upon risks arising from AI models in isolation, lacking consideration of how modular use of AI affects risk distribution of workflow components or overall risk metrics. There is also a lack of statistical grounding enabling sensitisation of risk models in the presence of absence of AI to estimate causal contributions of AI. This is in part due to the dearth of AI impact data upon which to fit distributions. In this work, we address these gaps in two ways. First, we demonstrate how scenario modelling (grounded in established statistical techniques such as Markov chains, copulas and Monte Carlo simulation) can be used to model AI risk holistically. Second, we show how lookalike distributions from phenomena analogous to AI can be used to estimate AI impacts in the absence of directly observable data. We demonstrate the utility of our methods for benchmarking cumulative AI risk via risk analysis of a logistic scenario simulations."
  },
  {
    "title": "How Jailbreak Defenses Work and Ensemble? A Mechanistic Investigation",
    "url": "http://arxiv.org/abs/2502.14486v1",
    "arxiv_id": "2502.14486v1",
    "authors": [
      "Zhuohang Long",
      "Siyuan Wang",
      "Shujun Liu",
      "Yuhang Lai",
      "Xuanjing Huang",
      "Zhongyu Wei"
    ],
    "published": "2025-02-20T12:07:40+00:00",
    "summary": "Jailbreak attacks, where harmful prompts bypass generative models' built-in safety, raise serious concerns about model vulnerability. While many defense methods have been proposed, the trade-offs between safety and helpfulness, and their application to Large Vision-Language Models (LVLMs), are not well understood. This paper systematically examines jailbreak defenses by reframing the standard generation task as a binary classification problem to assess model refusal tendencies for both harmful and benign queries. We identify two key defense mechanisms: safety shift, which increases refusal rates across all queries, and harmfulness discrimination, which improves the model's ability to distinguish between harmful and benign inputs. Using these mechanisms, we develop two ensemble defense strategies-inter-mechanism ensembles and intra-mechanism ensembles-to balance safety and helpfulness. Experiments on the MM-SafetyBench and MOSSBench datasets with LLaVA-1.5 models show that these strategies effectively improve model safety or optimize the trade-off between safety and helpfulness."
  },
  {
    "title": "Natural Language Generation",
    "url": "http://arxiv.org/abs/2502.14437v1",
    "arxiv_id": "2502.14437v1",
    "authors": [
      "Ehud Reiter"
    ],
    "published": "2025-02-20T10:41:34+00:00",
    "summary": "This book provides a broad overview of Natural Language Generation (NLG), including technology, user requirements, evaluation, and real-world applications. The focus is on concepts and insights which hopefully will remain relevant for many years, not on the latest LLM innovations. It draws on decades of work by the author and others on NLG.   The book has the following chapters: Introduction to NLG; Rule-Based NLG; Machine Learning and Neural NLG; Requirements; Evaluation; Safety, Maintenance, and Testing; and Applications. All chapters include examples and anecdotes from the author's personal experiences, and end with a Further Reading section.   The book should be especially useful to people working on applied NLG, including NLG researchers, people in other fields who want to use NLG, and commercial developers. It will not however be useful to people who want to understand the latest LLM technology.   There is a companion site with more information at https://ehudreiter.com/book/"
  },
  {
    "title": "Reliable Explainability of Deep Learning Spatial-Spectral Classifiers for Improved Semantic Segmentation in Autonomous Driving",
    "url": "http://arxiv.org/abs/2502.14416v1",
    "arxiv_id": "2502.14416v1",
    "authors": [
      "Jon Guti\u00e9rrez-Zaballa",
      "Koldo Basterretxea",
      "Javier Echanobe"
    ],
    "published": "2025-02-20T10:11:27+00:00",
    "summary": "Integrating hyperspectral imagery (HSI) with deep neural networks (DNNs) can strengthen the accuracy of intelligent vision systems by combining spectral and spatial information, which is useful for tasks like semantic segmentation in autonomous driving. To advance research in such safety-critical systems, determining the precise contribution of spectral information to complex DNNs' output is needed. To address this, several saliency methods, such as class activation maps (CAM), have been proposed primarily for image classification. However, recent studies have raised concerns regarding their reliability. In this paper, we address their limitations and propose an alternative approach by leveraging the data provided by activations and weights from relevant DNN layers to better capture the relationship between input features and predictions. The study aims to assess the superior performance of HSI compared to 3-channel and single-channel DNNs. We also address the influence of spectral signature normalization for enhancing DNN robustness in real-world driving conditions."
  },
  {
    "title": "HPS: Hard Preference Sampling for Human Preference Alignment",
    "url": "http://arxiv.org/abs/2502.14400v1",
    "arxiv_id": "2502.14400v1",
    "authors": [
      "Xiandong Zou",
      "Wanyu Lin",
      "Yuchen Li",
      "Pan Zhou"
    ],
    "published": "2025-02-20T09:37:41+00:00",
    "summary": "Aligning Large Language Model (LLM) responses with human preferences is vital for building safe and controllable AI systems. While preference optimization methods based on Plackett-Luce (PL) and Bradley-Terry (BT) models have shown promise, they face challenges such as poor handling of harmful content, inefficient use of dispreferred responses, and, specifically for PL, high computational costs. To address these issues, we propose Hard Preference Sampling (HPS), a novel framework for robust and efficient human preference alignment. HPS introduces a training loss that prioritizes the most preferred response while rejecting all dispreferred and harmful ones. It emphasizes \"hard\" dispreferred responses--those closely resembling preferred ones--to enhance the model's rejection capabilities. By leveraging a single-sample Monte Carlo sampling strategy, HPS reduces computational overhead while maintaining alignment quality. Theoretically, HPS improves sample efficiency over existing PL methods and maximizes the reward margin between preferred and dispreferred responses, ensuring clearer distinctions. Experiments on HH-RLHF and PKU-Safety datasets validate HPS's effectiveness, achieving comparable BLEU and reward scores while greatly improving reward margins and thus reducing harmful content generation."
  },
  {
    "title": "MPPI-DBaS: Safe Trajectory Optimization with Adaptive Exploration",
    "url": "http://arxiv.org/abs/2502.14387v1",
    "arxiv_id": "2502.14387v1",
    "authors": [
      "Fanxin Wang",
      "Yikun Cheng",
      "Chuyuan Tao"
    ],
    "published": "2025-02-20T09:22:41+00:00",
    "summary": "In trajectory optimization, Model Predictive Path Integral (MPPI) control is a sampling-based Model Predictive Control (MPC) framework that generates optimal inputs by efficiently simulating numerous trajectories. In practice, however, MPPI often struggles to guarantee safety assurance and balance efficient sampling in open spaces with the need for more extensive exploration under tight constraints. To address this challenge, we incorporate discrete barrier states (DBaS) into MPPI and propose a novel MPPI-DBaS algorithm that ensures system safety and enables adaptive exploration across diverse scenarios. We evaluate our method in simulation experiments where the vehicle navigates through closely placed obstacles. The results demonstrate that the proposed algorithm significantly outperforms standard MPPI, achieving a higher success rate and lower tracking errors."
  },
  {
    "title": "S*: Test Time Scaling for Code Generation",
    "url": "http://arxiv.org/abs/2502.14382v1",
    "arxiv_id": "2502.14382v1",
    "authors": [
      "Dacheng Li",
      "Shiyi Cao",
      "Chengkun Cao",
      "Xiuyu Li",
      "Shangyin Tan",
      "Kurt Keutzer",
      "Jiarong Xing",
      "Joseph E. Gonzalez",
      "Ion Stoica"
    ],
    "published": "2025-02-20T09:18:53+00:00",
    "summary": "Increasing test-time compute for LLMs shows promise across domains but remains underexplored in code generation, despite extensive study in math. In this paper, we propose S*, the first hybrid test-time scaling framework that substantially improves the coverage and selection accuracy of generated code. S* extends the existing parallel scaling paradigm with sequential scaling to push performance boundaries. It further leverages a novel selection mechanism that adaptively generates distinguishing inputs for pairwise comparison, combined with execution-grounded information to robustly identify correct solutions. We evaluate across 12 Large Language Models and Large Reasoning Model and show: (1) S* consistently improves performance across model families and sizes, enabling a 3B model to outperform GPT-4o-mini; (2) S* enables non-reasoning models to surpass reasoning models - GPT-4o-mini with S* outperforms o1-preview by 3.7% on LiveCodeBench; (3) S* further boosts state-of-the-art reasoning models - DeepSeek-R1-Distill-Qwen-32B with S* achieves 85.7% on LiveCodeBench, approaching o1 (high) at 88.5%. Code will be available under https://github.com/NovaSky-AI/SkyThought."
  },
  {
    "title": "ChemHTS: Hierarchical Tool Stacking for Enhancing Chemical Agents",
    "url": "http://arxiv.org/abs/2502.14327v1",
    "arxiv_id": "2502.14327v1",
    "authors": [
      "Zhucong Li",
      "Jin Xiao",
      "Bowei Zhang",
      "Zhijian Zhou",
      "Qianyu He",
      "Fenglei Cao",
      "Jiaqing Liang",
      "Yuan Qi"
    ],
    "published": "2025-02-20T07:24:26+00:00",
    "summary": "Large Language Models (LLMs) have demonstrated remarkable potential in scientific research, particularly in chemistry-related tasks such as molecular design, reaction prediction, and property estimation. While tool-augmented LLMs have been introduced to enhance reasoning and computation in these domains, existing approaches suffer from tool invocation errors and lack effective collaboration among diverse tools, limiting their overall performance. To address these challenges, we propose ChemHTS (Chemical Hierarchical Tool Stacking), a novel method that optimizes tool invocation pathways through a hierarchical stacking strategy. ChemHTS consists of two key stages: tool self-stacking warmup and multi-layer decision optimization, enabling LLMs to refine tool usage dynamically. We evaluate ChemHTS across four classical chemistry tasks and demonstrate its superiority over strong baselines, including GPT-4o, DeepSeek-R1, and chemistry-specific models, including ChemDFM. Furthermore, we define four distinct tool-stacking behaviors to enhance interpretability, providing insights into the effectiveness of tool collaboration. Our dataset and code are publicly available at \\url{https://github.com/Chang-pw/ChemHTS}."
  },
  {
    "title": "MedHallu: A Comprehensive Benchmark for Detecting Medical Hallucinations in Large Language Models",
    "url": "http://arxiv.org/abs/2502.14302v1",
    "arxiv_id": "2502.14302v1",
    "authors": [
      "Shrey Pandit",
      "Jiawei Xu",
      "Junyuan Hong",
      "Zhangyang Wang",
      "Tianlong Chen",
      "Kaidi Xu",
      "Ying Ding"
    ],
    "published": "2025-02-20T06:33:23+00:00",
    "summary": "Advancements in Large Language Models (LLMs) and their increasing use in medical question-answering necessitate rigorous evaluation of their reliability. A critical challenge lies in hallucination, where models generate plausible yet factually incorrect outputs. In the medical domain, this poses serious risks to patient safety and clinical decision-making. To address this, we introduce MedHallu, the first benchmark specifically designed for medical hallucination detection. MedHallu comprises 10,000 high-quality question-answer pairs derived from PubMedQA, with hallucinated answers systematically generated through a controlled pipeline. Our experiments show that state-of-the-art LLMs, including GPT-4o, Llama-3.1, and the medically fine-tuned UltraMedical, struggle with this binary hallucination detection task, with the best model achieving an F1 score as low as 0.625 for detecting \"hard\" category hallucinations. Using bidirectional entailment clustering, we show that harder-to-detect hallucinations are semantically closer to ground truth. Through experiments, we also show incorporating domain-specific knowledge and introducing a \"not sure\" category as one of the answer categories improves the precision and F1 scores by up to 38% relative to baselines."
  },
  {
    "title": "SEA-HELM: Southeast Asian Holistic Evaluation of Language Models",
    "url": "http://arxiv.org/abs/2502.14301v1",
    "arxiv_id": "2502.14301v1",
    "authors": [
      "Yosephine Susanto",
      "Adithya Venkatadri Hulagadri",
      "Jann Railey Montalan",
      "Jian Gang Ngui",
      "Xian Bin Yong",
      "Weiqi Leong",
      "Hamsawardhini Rengarajan",
      "Peerat Limkonchotiwat",
      "Yifan Mai",
      "William Chandra Tjhi"
    ],
    "published": "2025-02-20T06:32:45+00:00",
    "summary": "With the rapid emergence of novel capabilities in Large Language Models (LLMs), the need for rigorous multilingual and multicultural benchmarks that are integrated has become more pronounced. Though existing LLM benchmarks are capable of evaluating specific capabilities of LLMs in English as well as in various mid- to low-resource languages, including those in the Southeast Asian (SEA) region, a comprehensive and authentic evaluation suite for the SEA languages has not been developed thus far. Here, we present SEA-HELM, a holistic linguistic and cultural LLM evaluation suite that emphasizes SEA languages, comprising five core pillars: (1) NLP Classics, (2) LLM-specifics, (3) SEA Linguistics, (4) SEA Culture, (5) Safety. SEA-HELM currently supports Filipino, Indonesian, Tamil, Thai, and Vietnamese. We also introduce the SEA-HELM leaderboard, which allows users to understand models' multilingual and multicultural performance in a systematic and user-friendly manner."
  },
  {
    "title": "Cytogenetic, Hematobiochemical, and Histopathological Assessment of Albino Rats (Rattus norvegicus) Fed on Gluten Extracts",
    "url": "http://arxiv.org/abs/2502.14929v1",
    "arxiv_id": "2502.14929v1",
    "authors": [
      "Tajudeen Yahaya",
      "Esther Oladele",
      "Ufuoma Shemishere",
      "Daniel Anyebe",
      "Haliru Abdullahi",
      "Maryam Lawal",
      "Rufai Ahmad"
    ],
    "published": "2025-02-20T05:44:36+00:00",
    "summary": "Background: Literature shows that most of the information on the toxicity of gluten is generated from survey and observational studies, resulting in inconsistent outcomes and a decrease in the acceptability of gluten-rich foods. To determine gluten's safety, an in-depth in vitro and in vivo toxicological examination is required. This enables scientists to come up with ameliorative strategies if it turns out to have side effects, and consumers' trust can be restored. Objectives: The objective of this study was to assess the toxicity of gluten extracts on albino rats (Rattus norvegicus). Materials and Methods: Twenty-four rats were randomly selected and divided into four groups, each comprising six rats. Group 1 (control) rats were fed on pellet feeds and groups 2, 3, and 4 were fed on daily dosages of 0.5, 1.0, and 1.5 g gluten extracts, respectively. The rats' body weights and reactions were observed for 90 days before blood samples were collected for hematobiochemical and micronucleus tests. Histopathological examinations of the liver and kidneys were also performed. Results: There was no difference (P > 0.05) in body weight, blood glucose level, or micronuclei between the control and treated rats. The lymphocytes, alkaline phosphatase, alanine transaminase, total protein, and calcium ions of the test rats were all significantly (P < 0.05) altered but remained within the normal ranges. Other hematobiochemical parameters, including packed cell volume, hemoglobin, white and red blood cells, aspartate transaminase, albumin, sodium ions, potassium ions, chloride ions, and urea, revealed no marked changes. The treated rats' livers and kidneys showed no histopathological changes. Conclusion: Gluten had no adverse effects. However, it altered hematobiochemical parameters, particularly the lymphocytes, alkaline phosphatase, alanine transaminase, total protein, and calcium ions."
  },
  {
    "title": "No Minima, No Collisions: Combining Modulation and Control Barrier Function Strategies for Feasible Dynamical Collision Avoidance",
    "url": "http://arxiv.org/abs/2502.14238v1",
    "arxiv_id": "2502.14238v1",
    "authors": [
      "Yifan Xue",
      "Nadia Figueroa"
    ],
    "published": "2025-02-20T04:07:18+00:00",
    "summary": "As prominent real-time safety-critical reactive control techniques, Control Barrier Function Quadratic Programs (CBF-QPs) work for control affine systems in general but result in local minima in the generated trajectories and consequently cannot ensure convergence to the goals. Contrarily, Modulation of Dynamical Systems (Mod-DSs), including normal, reference, and on-manifold Mod-DS, achieve obstacle avoidance with few and even no local minima but have trouble optimally minimizing the difference between the constrained and the unconstrained controller outputs, and its applications are limited to fully-actuated systems. We dive into the theoretical foundations of CBF-QP and Mod-DS, proving that despite their distinct origins, normal Mod-DS is a special case of CBF-QP, and reference Mod-DS's solutions are mathematically connected to that of the CBF-QP through one equation. Building on top of the unveiled theoretical connections between CBF-QP and Mod-DS, reference Mod-based CBF-QP and on-manifold Mod-based CBF-QP controllers are proposed to combine the strength of CBF-QP and Mod-DS approaches and realize local-minimum-free reactive obstacle avoidance for control affine systems in general. We validate our methods in both simulated hospital environments and real-world experiments using Ridgeback for fully-actuated systems and Fetch robots for underactuated systems. Mod-based CBF-QPs outperform CBF-QPs as well as the optimally constrained-enforcing Mod-DS approaches we proposed in all experiments."
  },
  {
    "title": "OBELiX: A Curated Dataset of Crystal Structures and Experimentally Measured Ionic Conductivities for Lithium Solid-State Electrolytes",
    "url": "http://arxiv.org/abs/2502.14234v1",
    "arxiv_id": "2502.14234v1",
    "authors": [
      "F\u00e9lix Therrien",
      "Jamal Abou Haibeh",
      "Divya Sharma",
      "Rhiannon Hendley",
      "Alex Hern\u00e1ndez-Garc\u00eda",
      "Sun Sun",
      "Alain Tchagang",
      "Jiang Su",
      "Samuel Huberman",
      "Yoshua Bengio",
      "Hongyu Guo",
      "Homin Shin"
    ],
    "published": "2025-02-20T03:59:35+00:00",
    "summary": "Solid-state electrolyte batteries are expected to replace liquid electrolyte lithium-ion batteries in the near future thanks to their higher theoretical energy density and improved safety. However, their adoption is currently hindered by their lower effective ionic conductivity, a quantity that governs charge and discharge rates. Identifying highly ion-conductive materials using conventional theoretical calculations and experimental validation is both time-consuming and resource-intensive. While machine learning holds the promise to expedite this process, relevant ionic conductivity and structural data is scarce. Here, we present OBELiX, a domain-expert-curated database of $\\sim$600 synthesized solid electrolyte materials and their experimentally measured room temperature ionic conductivities gathered from literature. Each material is described by their measured composition, space group and lattice parameters. A full-crystal description in the form of a crystallographic information file (CIF) is provided for ~320 structures for which atomic positions were available. We discuss various statistics and features of the dataset and provide training and testing splits that avoid data leakage. Finally, we benchmark seven existing ML models on the task of predicting ionic conductivity and discuss their performance. The goal of this work is to facilitate the use of machine learning for solid-state electrolyte materials discovery."
  },
  {
    "title": "Do LLMs Consider Security? An Empirical Study on Responses to Programming Questions",
    "url": "http://arxiv.org/abs/2502.14202v1",
    "arxiv_id": "2502.14202v1",
    "authors": [
      "Amirali Sajadi",
      "Binh Le",
      "Anh Nguyen",
      "Kostadin Damevski",
      "Preetha Chatterjee"
    ],
    "published": "2025-02-20T02:20:06+00:00",
    "summary": "The widespread adoption of conversational LLMs for software development has raised new security concerns regarding the safety of LLM-generated content. Our motivational study outlines ChatGPT's potential in volunteering context-specific information to the developers, promoting safe coding practices. Motivated by this finding, we conduct a study to evaluate the degree of security awareness exhibited by three prominent LLMs: Claude 3, GPT-4, and Llama 3. We prompt these LLMs with Stack Overflow questions that contain vulnerable code to evaluate whether they merely provide answers to the questions or if they also warn users about the insecure code, thereby demonstrating a degree of security awareness. Further, we assess whether LLM responses provide information about the causes, exploits, and the potential fixes of the vulnerability, to help raise users' awareness. Our findings show that all three models struggle to accurately detect and warn users about vulnerabilities, achieving a detection rate of only 12.6% to 40% across our datasets. We also observe that the LLMs tend to identify certain types of vulnerabilities related to sensitive information exposure and improper input neutralization much more frequently than other types, such as those involving external control of file names or paths. Furthermore, when LLMs do issue security warnings, they often provide more information on the causes, exploits, and fixes of vulnerabilities compared to Stack Overflow responses. Finally, we provide an in-depth discussion on the implications of our findings and present a CLI-based prompting tool that can be used to generate significantly more secure LLM responses."
  },
  {
    "title": "Multimodal RewardBench: Holistic Evaluation of Reward Models for Vision Language Models",
    "url": "http://arxiv.org/abs/2502.14191v1",
    "arxiv_id": "2502.14191v1",
    "authors": [
      "Michihiro Yasunaga",
      "Luke Zettlemoyer",
      "Marjan Ghazvininejad"
    ],
    "published": "2025-02-20T01:48:13+00:00",
    "summary": "Reward models play an essential role in training vision-language models (VLMs) by assessing output quality to enable aligning with human preferences. Despite their importance, the research community lacks comprehensive open benchmarks for evaluating multimodal reward models in VLMs. To address this gap, we introduce Multimodal RewardBench, an expert-annotated benchmark covering six domains: general correctness, preference, knowledge, reasoning, safety, and visual question-answering. Our dataset comprises 5,211 annotated (prompt, chosen response, rejected response) triplets collected from various VLMs. In evaluating a range of VLM judges, we find that even the top-performing models, Gemini 1.5 Pro and Claude 3.5 Sonnet, achieve only 72% overall accuracy. Notably, most models struggle in the reasoning and safety domains. These findings suggest that Multimodal RewardBench offers a challenging testbed for advancing reward model development across multiple domains. We release the benchmark at https://github.com/facebookresearch/multimodal_rewardbench."
  },
  {
    "title": "Type 1 Diabetes Management using GLIMMER: Glucose Level Indicator Model with Modified Error Rate",
    "url": "http://arxiv.org/abs/2502.14183v1",
    "arxiv_id": "2502.14183v1",
    "authors": [
      "Saman Khamesian",
      "Asiful Arefeen",
      "Adela Grando",
      "Bithika Thompson",
      "Hassan Ghasemzadeh"
    ],
    "published": "2025-02-20T01:26:00+00:00",
    "summary": "Managing Type 1 Diabetes (T1D) demands constant vigilance as individuals strive to regulate their blood glucose levels to avert the dangers of dysglycemia (hyperglycemia or hypoglycemia). Despite the advent of sophisticated technologies such as automated insulin delivery (AID) systems, achieving optimal glycemic control remains a formidable task. AID systems integrate continuous subcutaneous insulin infusion (CSII) and continuous glucose monitors (CGM) data, offering promise in reducing variability and increasing glucose time-in-range. However, these systems often fail to prevent dysglycemia, partly due to limitations in prediction algorithms that lack the precision to avert abnormal glucose events. This gap highlights the need for proactive behavioral adjustments. We address this need with GLIMMER, Glucose Level Indicator Model with Modified Error Rate, a machine learning approach for forecasting blood glucose levels. GLIMMER categorizes glucose values into normal and abnormal ranges and devises a novel custom loss function to prioritize accuracy in dysglycemic events where patient safety is critical. To evaluate the potential of GLIMMER for T1D management, we both use a publicly available dataset and collect new data involving 25 patients with T1D. In predicting next-hour glucose values, GLIMMER achieved a root mean square error (RMSE) of 23.97 (+/-3.77) and a mean absolute error (MAE) of 15.83 (+/-2.09) mg/dL. These results reflect a 23% improvement in RMSE and a 31% improvement in MAE compared to the best-reported error rates."
  },
  {
    "title": "Multi-Faceted Studies on Data Poisoning can Advance LLM Development",
    "url": "http://arxiv.org/abs/2502.14182v1",
    "arxiv_id": "2502.14182v1",
    "authors": [
      "Pengfei He",
      "Yue Xing",
      "Han Xu",
      "Zhen Xiang",
      "Jiliang Tang"
    ],
    "published": "2025-02-20T01:19:51+00:00",
    "summary": "The lifecycle of large language models (LLMs) is far more complex than that of traditional machine learning models, involving multiple training stages, diverse data sources, and varied inference methods. While prior research on data poisoning attacks has primarily focused on the safety vulnerabilities of LLMs, these attacks face significant challenges in practice. Secure data collection, rigorous data cleaning, and the multistage nature of LLM training make it difficult to inject poisoned data or reliably influence LLM behavior as intended. Given these challenges, this position paper proposes rethinking the role of data poisoning and argue that multi-faceted studies on data poisoning can advance LLM development. From a threat perspective, practical strategies for data poisoning attacks can help evaluate and address real safety risks to LLMs. From a trustworthiness perspective, data poisoning can be leveraged to build more robust LLMs by uncovering and mitigating hidden biases, harmful outputs, and hallucinations. Moreover, from a mechanism perspective, data poisoning can provide valuable insights into LLMs, particularly the interplay between data and model behavior, driving a deeper understanding of their underlying mechanisms."
  },
  {
    "title": "On the logical skills of large language models: evaluations using arbitrarily complex first-order logic problems",
    "url": "http://arxiv.org/abs/2502.14180v1",
    "arxiv_id": "2502.14180v1",
    "authors": [
      "Shokhrukh Ibragimov",
      "Arnulf Jentzen",
      "Benno Kuckuck"
    ],
    "published": "2025-02-20T01:18:24+00:00",
    "summary": "We present a method of generating first-order logic statements whose complexity can be controlled along multiple dimensions. We use this method to automatically create several datasets consisting of questions asking for the truth or falsity of first-order logic statements in Zermelo-Fraenkel set theory. While the resolution of these questions does not require any knowledge beyond basic notation of first-order logic and set theory, it does require a degree of planning and logical reasoning, which can be controlled up to arbitrarily high difficulty by the complexity of the generated statements. Furthermore, we do extensive evaluations of the performance of various large language models, including recent models such as DeepSeek-R1 and OpenAI's o3-mini, on these datasets. All of the datasets along with the code used for generating them, as well as all data from the evaluations is publicly available at https://github.com/bkuckuck/logical-skills-of-llms."
  },
  {
    "title": "Multi-Agent Risks from Advanced AI",
    "url": "http://arxiv.org/abs/2502.14143v1",
    "arxiv_id": "2502.14143v1",
    "authors": [
      "Lewis Hammond",
      "Alan Chan",
      "Jesse Clifton",
      "Jason Hoelscher-Obermaier",
      "Akbir Khan",
      "Euan McLean",
      "Chandler Smith",
      "Wolfram Barfuss",
      "Jakob Foerster",
      "Tom\u00e1\u0161 Gaven\u010diak",
      "The Anh Han",
      "Edward Hughes",
      "Vojt\u011bch Kova\u0159\u00edk",
      "Jan Kulveit",
      "Joel Z. Leibo",
      "Caspar Oesterheld",
      "Christian Schroeder de Witt",
      "Nisarg Shah",
      "Michael Wellman",
      "Paolo Bova",
      "Theodor Cimpeanu",
      "Carson Ezell",
      "Quentin Feuillade-Montixi",
      "Matija Franklin",
      "Esben Kran",
      "Igor Krawczuk",
      "Max Lamparth",
      "Niklas Lauffer",
      "Alexander Meinke",
      "Sumeet Motwani",
      "Anka Reuel",
      "Vincent Conitzer",
      "Michael Dennis",
      "Iason Gabriel",
      "Adam Gleave",
      "Gillian Hadfield",
      "Nika Haghtalab",
      "Atoosa Kasirzadeh",
      "S\u00e9bastien Krier",
      "Kate Larson",
      "Joel Lehman",
      "David C. Parkes",
      "Georgios Piliouras",
      "Iyad Rahwan"
    ],
    "published": "2025-02-19T23:03:21+00:00",
    "summary": "The rapid development of advanced AI agents and the imminent deployment of many instances of these agents will give rise to multi-agent systems of unprecedented complexity. These systems pose novel and under-explored risks. In this report, we provide a structured taxonomy of these risks by identifying three key failure modes (miscoordination, conflict, and collusion) based on agents' incentives, as well as seven key risk factors (information asymmetries, network effects, selection pressures, destabilising dynamics, commitment problems, emergent agency, and multi-agent security) that can underpin them. We highlight several important instances of each risk, as well as promising directions to help mitigate them. By anchoring our analysis in a range of real-world examples and experimental evidence, we illustrate the distinct challenges posed by multi-agent systems and their implications for the safety, governance, and ethics of advanced AI."
  },
  {
    "title": "PedDet: Adaptive Spectral Optimization for Multimodal Pedestrian Detection",
    "url": "http://arxiv.org/abs/2502.14063v1",
    "arxiv_id": "2502.14063v1",
    "authors": [
      "Rui Zhao",
      "Zeyu Zhang",
      "Yi Xu",
      "Yi Yao",
      "Yan Huang",
      "Wenxin Zhang",
      "Zirui Song",
      "Xiuying Chen",
      "Yang Zhao"
    ],
    "published": "2025-02-19T19:31:51+00:00",
    "summary": "Pedestrian detection in intelligent transportation systems has made significant progress but faces two critical challenges: (1) insufficient fusion of complementary information between visible and infrared spectra, particularly in complex scenarios, and (2) sensitivity to illumination changes, such as low-light or overexposed conditions, leading to degraded performance. To address these issues, we propose PedDet, an adaptive spectral optimization complementarity framework specifically enhanced and optimized for multispectral pedestrian detection. PedDet introduces the Multi-scale Spectral Feature Perception Module (MSFPM) to adaptively fuse visible and infrared features, enhancing robustness and flexibility in feature extraction. Additionally, the Illumination Robustness Feature Decoupling Module (IRFDM) improves detection stability under varying lighting by decoupling pedestrian and background features. We further design a contrastive alignment to enhance intermodal feature discrimination. Experiments on LLVIP and MSDS datasets demonstrate that PedDet achieves state-of-the-art performance, improving the mAP by 6.6% with superior detection accuracy even in low-light conditions, marking a significant step forward for road safety. Code will be available at https://github.com/AIGeeksGroup/PedDet."
  },
  {
    "title": "Asking for Help Enables Safety Guarantees Without Sacrificing Effectiveness",
    "url": "http://arxiv.org/abs/2502.14043v1",
    "arxiv_id": "2502.14043v1",
    "authors": [
      "Benjamin Plaut",
      "Juan Li\u00e9vano-Karim",
      "Stuart Russell"
    ],
    "published": "2025-02-19T19:01:39+00:00",
    "summary": "Most reinforcement learning algorithms with regret guarantees rely on a critical assumption: that all errors are recoverable. Recent work by Plaut et al. discarded this assumption and presented algorithms that avoid \"catastrophe\" (i.e., irreparable errors) by asking for help. However, they provided only safety guarantees and did not consider reward maximization. We prove that any algorithm that avoids catastrophe in their setting also guarantees high reward (i.e., sublinear regret) in any Markov Decision Process (MDP), including MDPs with irreversible costs. This constitutes the first no-regret guarantee for general MDPs. More broadly, our result may be the first formal proof that it is possible for an agent to obtain high reward while becoming self-sufficient in an unknown, unbounded, and high-stakes environment without causing catastrophe or requiring resets."
  },
  {
    "title": "Why Safeguarded Ships Run Aground? Aligned Large Language Models' Safety Mechanisms Tend to Be Anchored in The Template Region",
    "url": "http://arxiv.org/abs/2502.13946v1",
    "arxiv_id": "2502.13946v1",
    "authors": [
      "Chak Tou Leong",
      "Qingyu Yin",
      "Jian Wang",
      "Wenjie Li"
    ],
    "published": "2025-02-19T18:42:45+00:00",
    "summary": "The safety alignment of large language models (LLMs) remains vulnerable, as their initial behavior can be easily jailbroken by even relatively simple attacks. Since infilling a fixed template between the input instruction and initial model output is a common practice for existing LLMs, we hypothesize that this template is a key factor behind their vulnerabilities: LLMs' safety-related decision-making overly relies on the aggregated information from the template region, which largely influences these models' safety behavior. We refer to this issue as template-anchored safety alignment. In this paper, we conduct extensive experiments and verify that template-anchored safety alignment is widespread across various aligned LLMs. Our mechanistic analyses demonstrate how it leads to models' susceptibility when encountering inference-time jailbreak attacks. Furthermore, we show that detaching safety mechanisms from the template region is promising in mitigating vulnerabilities to jailbreak attacks. We encourage future research to develop more robust safety alignment techniques that reduce reliance on the template region."
  },
  {
    "title": "Learning to explore when mistakes are not allowed",
    "url": "http://arxiv.org/abs/2502.13801v1",
    "arxiv_id": "2502.13801v1",
    "authors": [
      "Charly Pecqueux-Gu\u00e9z\u00e9nec",
      "St\u00e9phane Doncieux",
      "Nicolas Perrin-Gilbert"
    ],
    "published": "2025-02-19T15:11:51+00:00",
    "summary": "Goal-Conditioned Reinforcement Learning (GCRL) provides a versatile framework for developing unified controllers capable of handling wide ranges of tasks, exploring environments, and adapting behaviors. However, its reliance on trial-and-error poses challenges for real-world applications, as errors can result in costly and potentially damaging consequences. To address the need for safer learning, we propose a method that enables agents to learn goal-conditioned behaviors that explore without the risk of making harmful mistakes. Exploration without risks can seem paradoxical, but environment dynamics are often uniform in space, therefore a policy trained for safety without exploration purposes can still be exploited globally. Our proposed approach involves two distinct phases. First, during a pretraining phase, we employ safe reinforcement learning and distributional techniques to train a safety policy that actively tries to avoid failures in various situations. In the subsequent safe exploration phase, a goal-conditioned (GC) policy is learned while ensuring safety. To achieve this, we implement an action-selection mechanism leveraging the previously learned distributional safety critics to arbitrate between the safety policy and the GC policy, ensuring safe exploration by switching to the safety policy when needed. We evaluate our method in simulated environments and demonstrate that it not only provides substantial coverage of the goal space but also reduces the occurrence of mistakes to a minimum, in stark contrast to traditional GCRL approaches. Additionally, we conduct an ablation study and analyze failure modes, offering insights for future research directions."
  },
  {
    "title": "User Association and Coordinated Beamforming in Cognitive Aerial-Terrestrial Networks: A Safe Reinforcement Learning Approach",
    "url": "http://arxiv.org/abs/2502.13663v1",
    "arxiv_id": "2502.13663v1",
    "authors": [
      "Zizhen Zhou",
      "Jungang Ge",
      "Ying-Chang Liang"
    ],
    "published": "2025-02-19T12:15:32+00:00",
    "summary": "Cognitive aerial-terrestrial networks (CATNs) offer a solution to spectrum scarcity by sharing spectrum between aerial and terrestrial networks. However, aerial users (AUs) experience significant interference from numerous terrestrial base stations (BSs). To alleviate such interference, we investigate a user association and coordinated beamforming (CBF) problem in CATN, where the aerial network serves as the primary network sharing its spectrum with the terrestrial network. Specifically, we maximize the sum rate of the secondary terrestrial users (TUs) under the interference temperature constraints of the AUs. Traditional iterative optimization schemes are impractical due to their high computational complexity and information exchange overhead. Although deep reinforcement learning (DRL) based schemes can address these challenges, their performance is sensitive to the weights of the weighted penalty terms for violating constraints in the reward function. Motivated by these issues, we propose a safe DRL-based user association and CBF scheme for CATN, eliminating the need for training multiple times to find the optimal penalty weight before actual deployment. Specifically, the CATN is modeled as a networked constrained partially observable Markov game. Each TU acts as an agent to choose its associated BS, and each BS acts as an agent to decide its beamforming vectors, aiming to maximize the reward while satisfying the safety constraints introduced by the interference constraints of the AUs. By exploiting a safe DRL algorithm, the proposed scheme incurs lower deployment expenses than the penalty-based DRL schemes since only one training is required before actual deployment. Simulation results show that the proposed scheme can achieve a higher sum rate of TUs than a two-stage optimization scheme while the average received interference power of the AUs is generally below the threshold."
  },
  {
    "title": "SLAMSpoof: Practical LiDAR Spoofing Attacks on Localization Systems Guided by Scan Matching Vulnerability Analysis",
    "url": "http://arxiv.org/abs/2502.13641v1",
    "arxiv_id": "2502.13641v1",
    "authors": [
      "Rokuto Nagata",
      "Kenji Koide",
      "Yuki Hayakawa",
      "Ryo Suzuki",
      "Kazuma Ikeda",
      "Ozora Sako",
      "Qi Alfred Chen",
      "Takami Sato",
      "Kentaro Yoshioka"
    ],
    "published": "2025-02-19T11:33:56+00:00",
    "summary": "Accurate localization is essential for enabling modern full self-driving services. These services heavily rely on map-based traffic information to reduce uncertainties in recognizing lane shapes, traffic light locations, and traffic signs. Achieving this level of reliance on map information requires centimeter-level localization accuracy, which is currently only achievable with LiDAR sensors. However, LiDAR is known to be vulnerable to spoofing attacks that emit malicious lasers against LiDAR to overwrite its measurements. Once localization is compromised, the attack could lead the victim off roads or make them ignore traffic lights. Motivated by these serious safety implications, we design SLAMSpoof, the first practical LiDAR spoofing attack on localization systems for self-driving to assess the actual attack significance on autonomous vehicles. SLAMSpoof can effectively find the effective attack location based on our scan matching vulnerability score (SMVS), a point-wise metric representing the potential vulnerability to spoofing attacks. To evaluate the effectiveness of the attack, we conduct real-world experiments on ground vehicles and confirm its high capability in real-world scenarios, inducing position errors of $\\geq$4.2 meters (more than typical lane width) for all 3 popular LiDAR-based localization algorithms. We finally discuss the potential countermeasures of this attack. Code is available at https://github.com/Keio-CSG/slamspoof"
  },
  {
    "title": "Qorgau: Evaluating LLM Safety in Kazakh-Russian Bilingual Contexts",
    "url": "http://arxiv.org/abs/2502.13640v1",
    "arxiv_id": "2502.13640v1",
    "authors": [
      "Maiya Goloburda",
      "Nurkhan Laiyk",
      "Diana Turmakhan",
      "Yuxia Wang",
      "Mukhammed Togmanov",
      "Jonibek Mansurov",
      "Askhat Sametov",
      "Nurdaulet Mukhituly",
      "Minghan Wang",
      "Daniil Orel",
      "Zain Muhammad Mujahid",
      "Fajri Koto",
      "Timothy Baldwin",
      "Preslav Nakov"
    ],
    "published": "2025-02-19T11:33:22+00:00",
    "summary": "Large language models (LLMs) are known to have the potential to generate harmful content, posing risks to users. While significant progress has been made in developing taxonomies for LLM risks and safety evaluation prompts, most studies have focused on monolingual contexts, primarily in English. However, language- and region-specific risks in bilingual contexts are often overlooked, and core findings can diverge from those in monolingual settings. In this paper, we introduce Qorgau, a novel dataset specifically designed for safety evaluation in Kazakh and Russian, reflecting the unique bilingual context in Kazakhstan, where both Kazakh (a low-resource language) and Russian (a high-resource language) are spoken. Experiments with both multilingual and language-specific LLMs reveal notable differences in safety performance, emphasizing the need for tailored, region-specific datasets to ensure the responsible and safe deployment of LLMs in countries like Kazakhstan. Warning: this paper contains example data that may be offensive, harmful, or biased."
  },
  {
    "title": "First Glimpse on Physical Layer Security in Internet of Vehicles: Transformed from Communication Interference to Sensing Interference",
    "url": "http://arxiv.org/abs/2502.13634v1",
    "arxiv_id": "2502.13634v1",
    "authors": [
      "Kaixuan Li",
      "Kan Yu",
      "Xiaowu Liu",
      "Qixun Zhang",
      "Zhiyong Feng",
      "Dong Li"
    ],
    "published": "2025-02-19T11:13:18+00:00",
    "summary": "Integrated sensing and communication (ISAC) plays a crucial role in the Internet of Vehicles (IoV), serving as a key factor in enhancing driving safety and traffic efficiency. To address the security challenges of the confidential information transmission caused by the inherent openness nature of wireless medium, different from current physical layer security (PLS) methods, which depends on the \\emph{additional communication interference} costing extra power resources, in this paper, we investigate a novel PLS solution, under which the \\emph{inherent radar sensing interference} of the vehicles is utilized to secure wireless communications. To measure the performance of PLS methods in ISAC-based IoV systems, we first define an improved security performance metric called by transmission reliability and sensing accuracy based secrecy rate (TRSA\\_SR), and derive closed-form expressions of connection outage probability (COP), secrecy outage probability (SOP), success ranging probability (SRP) for evaluating transmission reliability, security and sensing accuracy, respectively. Furthermore, we formulate an optimization problem to maximize the TRSA\\_SR by utilizing radar sensing interference and joint design of the communication duration, transmission power and straight trajectory of the legitimate transmitter. Finally, the non-convex feature of formulated problem is solved through the problem decomposition and alternating optimization. Simulations indicate that compared with traditional PLS methods obtaining a non-positive STC, the proposed method achieves a secrecy rate of 3.92bps/Hz for different settings of noise power."
  },
  {
    "title": "Efficient Safety Retrofitting Against Jailbreaking for LLMs",
    "url": "http://arxiv.org/abs/2502.13603v1",
    "arxiv_id": "2502.13603v1",
    "authors": [
      "Dario Garcia-Gasulla",
      "Anna Arias-Duart",
      "Adrian Tormos",
      "Daniel Hinjos",
      "Oscar Molina-Sedano",
      "Ashwin Kumar Gururajan",
      "Maria Eugenia Cardello"
    ],
    "published": "2025-02-19T10:33:18+00:00",
    "summary": "Direct Preference Optimization (DPO) is an efficient alignment technique that steers LLMs towards preferable outputs by training on preference data, bypassing the need for explicit reward models. Its simplicity enables easy adaptation to various domains and safety requirements. This paper examines DPO's effectiveness in model safety against jailbreaking attacks while minimizing data requirements and training costs. We introduce Egida, a dataset expanded from multiple sources, which includes 27 different safety topics and 18 different attack styles, complemented with synthetic and human labels. This data is used to boost the safety of state-of-the-art LLMs (Llama-3.1-8B/70B-Instruct, Qwen-2.5-7B/72B-Instruct) across topics and attack styles. In addition to safety evaluations, we assess their post-alignment performance degradation in general purpose tasks, and their tendency to over refusal. Following the proposed methodology, trained models reduce their Attack Success Rate by 10%-30%, using small training efforts (2,000 samples) with low computational cost (3\\$ for 8B models, 20\\$ for 72B models). Safety aligned models generalize to unseen topics and attack styles, with the most successful attack style reaching a success rate around 5%. Size and family are found to strongly influence model malleability towards safety, pointing at the importance of pre-training choices. To validate our findings, a large independent assessment of human preference agreement with Llama-Guard-3-8B is conducted by the authors and the associated dataset Egida-HSafe is released. Overall, this study illustrates how affordable and accessible it is to enhance LLM safety using DPO while outlining its current limitations. All datasets and models are released to enable reproducibility and further research."
  },
  {
    "title": "Exploiting Prefix-Tree in Structured Output Interfaces for Enhancing Jailbreak Attacking",
    "url": "http://arxiv.org/abs/2502.13527v1",
    "arxiv_id": "2502.13527v1",
    "authors": [
      "Yanzeng Li",
      "Yunfan Xiong",
      "Jialun Zhong",
      "Jinchao Zhang",
      "Jie Zhou",
      "Lei Zou"
    ],
    "published": "2025-02-19T08:29:36+00:00",
    "summary": "The rise of Large Language Models (LLMs) has led to significant applications but also introduced serious security threats, particularly from jailbreak attacks that manipulate output generation. These attacks utilize prompt engineering and logit manipulation to steer models toward harmful content, prompting LLM providers to implement filtering and safety alignment strategies. We investigate LLMs' safety mechanisms and their recent applications, revealing a new threat model targeting structured output interfaces, which enable attackers to manipulate the inner logit during LLM generation, requiring only API access permissions. To demonstrate this threat model, we introduce a black-box attack framework called AttackPrefixTree (APT). APT exploits structured output interfaces to dynamically construct attack patterns. By leveraging prefixes of models' safety refusal response and latent harmful outputs, APT effectively bypasses safety measures. Experiments on benchmark datasets indicate that this approach achieves higher attack success rate than existing methods. This work highlights the urgent need for LLM providers to enhance security protocols to address vulnerabilities arising from the interaction between safety patterns and structured outputs."
  },
  {
    "title": "Integration of Agentic AI with 6G Networks for Mission-Critical Applications: Use-case and Challenges",
    "url": "http://arxiv.org/abs/2502.13476v1",
    "arxiv_id": "2502.13476v1",
    "authors": [
      "Sunder Ali Khowaja",
      "Kapal Dev",
      "Muhammad Salman Pathan",
      "Engin Zeydan",
      "Merouane Debbah"
    ],
    "published": "2025-02-19T07:00:53+00:00",
    "summary": "We are in a transformative era, and advances in Artificial Intelligence (AI), especially the foundational models, are constantly in the news. AI has been an integral part of many applications that rely on automation for service delivery, and one of them is mission-critical public safety applications. The problem with AI-oriented mission-critical applications is the humanin-the-loop system and the lack of adaptability to dynamic conditions while maintaining situational awareness. Agentic AI (AAI) has gained a lot of attention recently due to its ability to analyze textual data through a contextual lens while quickly adapting to conditions. In this context, this paper proposes an AAI framework for mission-critical applications. We propose a novel framework with a multi-layer architecture to realize the AAI. We also present a detailed implementation of AAI layer that bridges the gap between network infrastructure and missioncritical applications. Our preliminary analysis shows that the AAI reduces initial response time by 5.6 minutes on average, while alert generation time is reduced by 15.6 seconds on average and resource allocation is improved by up to 13.4%. We also show that the AAI methods improve the number of concurrent operations by 40, which reduces the recovery time by up to 5.2 minutes. Finally, we highlight some of the issues and challenges that need to be considered when implementing AAI frameworks."
  },
  {
    "title": "Beyond Single-Value Metrics: Evaluating and Enhancing LLM Unlearning with Cognitive Diagnosis",
    "url": "http://arxiv.org/abs/2502.13996v1",
    "arxiv_id": "2502.13996v1",
    "authors": [
      "Yicheng Lang",
      "Kehan Guo",
      "Yue Huang",
      "Yujun Zhou",
      "Haomin Zhuang",
      "Tianyu Yang",
      "Yao Su",
      "Xiangliang Zhang"
    ],
    "published": "2025-02-19T06:56:59+00:00",
    "summary": "Due to the widespread use of LLMs and the rising critical ethical and safety concerns, LLM unlearning methods have been developed to remove harmful knowledge and undesirable capabilities. In this context, evaluations are mostly based on single-value metrics such as QA accuracy. However, these metrics often fail to capture the nuanced retention of harmful knowledge components, making it difficult to assess the true effectiveness of unlearning. To address this issue, we propose UNCD (UNlearning evaluation via Cognitive Diagnosis), a novel framework that leverages Cognitive Diagnosis Modeling for fine-grained evaluation of LLM unlearning. Our dedicated benchmark, UNCD-Cyber, provides a detailed assessment of the removal of dangerous capabilities. Moreover, we introduce UNCD-Agent, which refines unlearning by diagnosing knowledge remnants and generating targeted unlearning data. Extensive experiments across eight unlearning methods and two base models demonstrate that UNCD not only enhances evaluation but also effectively facilitates the removal of harmful LLM abilities."
  },
  {
    "title": "ThinkGuard: Deliberative Slow Thinking Leads to Cautious Guardrails",
    "url": "http://arxiv.org/abs/2502.13458v1",
    "arxiv_id": "2502.13458v1",
    "authors": [
      "Xiaofei Wen",
      "Wenxuan Zhou",
      "Wenjie Jacky Mo",
      "Muhao Chen"
    ],
    "published": "2025-02-19T06:09:58+00:00",
    "summary": "Ensuring the safety of large language models (LLMs) is critical as they are deployed in real-world applications. Existing guardrails rely on rule-based filtering or single-pass classification, limiting their ability to handle nuanced safety violations. To address this, we propose ThinkGuard, a critique-augmented guardrail model that distills knowledge from high-capacity LLMs by generating structured critiques alongside safety labels. Fine-tuned on critique-augmented data, the captured deliberative thinking ability drastically enhances the guardrail's cautiousness and interpretability. Evaluated on multiple safety benchmarks, ThinkGuard achieves the highest average F1 and AUPRC, outperforming all baselines. Compared to LLaMA Guard 3, ThinkGuard improves accuracy by 16.1% and macro F1 by 27.0%. Moreover, it surpasses label-only fine-tuned models, confirming that structured critiques enhance both classification precision and nuanced safety reasoning while maintaining computational efficiency."
  },
  {
    "title": "TreeCut: A Synthetic Unanswerable Math Word Problem Dataset for LLM Hallucination Evaluation",
    "url": "http://arxiv.org/abs/2502.13442v1",
    "arxiv_id": "2502.13442v1",
    "authors": [
      "Jialin Ouyang"
    ],
    "published": "2025-02-19T05:38:45+00:00",
    "summary": "Large language models (LLMs) now achieve near-human performance on standard math word problem benchmarks (e.g., GSM8K), yet their true reasoning ability remains disputed. A key concern is that models often produce confident, yet unfounded, answers to unanswerable problems. We introduce TreeCut, a synthetic dataset that systematically generates infinite unanswerable math word problems and their answerable counterparts, by representing each question as a tree and removing chosen necessary conditions. Experiments show TreeCut effectively induce hallucinations in large language models, including GPT-4o and o3-mini, with rates of 61% and 42% in their respective worst-case scenarios. Further analysis highlights that deeper or more complex trees, composite item names, and removing necessary condition near the middle of a path all increase the likelihood of hallucinations, underscoring the persistent challenges LLMs face in identifying unanswerable math problems."
  },
  {
    "title": "Dynamic directed functional connectivity as a neural biomarker for objective motor skill assessment",
    "url": "http://arxiv.org/abs/2502.13362v1",
    "arxiv_id": "2502.13362v1",
    "authors": [
      "Anil Kamat",
      "Rahul Rahul",
      "Anirban Dutta",
      "Lora Cavuoto",
      "Uwe Kruger",
      "Harry Burke",
      "Matthew Hackett",
      "Jack Norfleet",
      "Steven Schwaitzberg",
      "Suvranu De"
    ],
    "published": "2025-02-19T01:51:39+00:00",
    "summary": "Objective motor skill assessment plays a critical role in fields such as surgery, where proficiency is vital for certification and patient safety. Existing assessment methods, however, rely heavily on subjective human judgment, which introduces bias and limits reproducibility. While recent efforts have leveraged kinematic data and neural imaging to provide more objective evaluations, these approaches often overlook the dynamic neural mechanisms that differentiate expert and novice performance. This study proposes a novel method for motor skill assessment based on dynamic directed functional connectivity (dFC) as a neural biomarker. By using electroencephalography (EEG) to capture brain dynamics and employing an attention-based Long Short-Term Memory (LSTM) model for non-linear Granger causality analysis, we compute dFC among key brain regions involved in psychomotor tasks. Coupled with hierarchical task analysis (HTA), our approach enables subtask-level evaluation of motor skills, offering detailed insights into neural coordination that underpins expert proficiency. A convolutional neural network (CNN) is then used to classify skill levels, achieving greater accuracy and specificity than established performance metrics in laparoscopic surgery. This methodology provides a reliable, objective framework for assessing motor skills, contributing to the development of tailored training protocols and enhancing the certification process."
  },
  {
    "title": "Language Models are Few-Shot Graders",
    "url": "http://arxiv.org/abs/2502.13337v1",
    "arxiv_id": "2502.13337v1",
    "authors": [
      "Chenyan Zhao",
      "Mariana Silva",
      "Seth Poulsen"
    ],
    "published": "2025-02-18T23:38:21+00:00",
    "summary": "Providing evaluations to student work is a critical component of effective student learning, and automating its process can significantly reduce the workload on human graders. Automatic Short Answer Grading (ASAG) systems, enabled by advancements in Large Language Models (LLMs), offer a promising solution for assessing and providing instant feedback for open-ended student responses. In this paper, we present an ASAG pipeline leveraging state-of-the-art LLMs. Our new LLM-based ASAG pipeline achieves better performances than existing custom-built models on the same datasets. We also compare the grading performance of three OpenAI models: GPT-4, GPT-4o, and o1-preview. Our results demonstrate that GPT-4o achieves the best balance between accuracy and cost-effectiveness. On the other hand, o1-preview, despite higher accuracy, exhibits a larger variance in error that makes it less practical for classroom use. We investigate the effects of incorporating instructor-graded examples into prompts using no examples, random selection, and Retrieval-Augmented Generation (RAG)-based selection strategies. Our findings indicate that providing graded examples enhances grading accuracy, with RAG-based selection outperforming random selection. Additionally, integrating grading rubrics improves accuracy by offering a structured standard for evaluation."
  },
  {
    "title": "Demonstrating specification gaming in reasoning models",
    "url": "http://arxiv.org/abs/2502.13295v1",
    "arxiv_id": "2502.13295v1",
    "authors": [
      "Alexander Bondarenko",
      "Denis Volk",
      "Dmitrii Volkov",
      "Jeffrey Ladish"
    ],
    "published": "2025-02-18T21:32:24+00:00",
    "summary": "We demonstrate LLM agent specification gaming by instructing models to win against a chess engine. We find reasoning models like o1 preview and DeepSeek-R1 will often hack the benchmark by default, while language models like GPT-4o and Claude 3.5 Sonnet need to be told that normal play won't work to hack.   We improve upon prior work like (Hubinger et al., 2024; Meinke et al., 2024; Weij et al., 2024) by using realistic task prompts and avoiding excess nudging. Our results suggest reasoning models may resort to hacking to solve difficult problems, as observed in OpenAI (2024)'s o1 Docker escape during cyber capabilities testing."
  },
  {
    "title": "RAD: Training an End-to-End Driving Policy via Large-Scale 3DGS-based Reinforcement Learning",
    "url": "http://arxiv.org/abs/2502.13144v1",
    "arxiv_id": "2502.13144v1",
    "authors": [
      "Hao Gao",
      "Shaoyu Chen",
      "Bo Jiang",
      "Bencheng Liao",
      "Yiang Shi",
      "Xiaoyang Guo",
      "Yuechuan Pu",
      "Haoran Yin",
      "Xiangyu Li",
      "Xinbang Zhang",
      "Ying Zhang",
      "Wenyu Liu",
      "Qian Zhang",
      "Xinggang Wang"
    ],
    "published": "2025-02-18T18:59:21+00:00",
    "summary": "Existing end-to-end autonomous driving (AD) algorithms typically follow the Imitation Learning (IL) paradigm, which faces challenges such as causal confusion and the open-loop gap. In this work, we establish a 3DGS-based closed-loop Reinforcement Learning (RL) training paradigm. By leveraging 3DGS techniques, we construct a photorealistic digital replica of the real physical world, enabling the AD policy to extensively explore the state space and learn to handle out-of-distribution scenarios through large-scale trial and error. To enhance safety, we design specialized rewards that guide the policy to effectively respond to safety-critical events and understand real-world causal relationships. For better alignment with human driving behavior, IL is incorporated into RL training as a regularization term. We introduce a closed-loop evaluation benchmark consisting of diverse, previously unseen 3DGS environments. Compared to IL-based methods, RAD achieves stronger performance in most closed-loop metrics, especially 3x lower collision rate. Abundant closed-loop results are presented at https://hgao-cv.github.io/RAD."
  },
  {
    "title": "RHINO: Learning Real-Time Humanoid-Human-Object Interaction from Human Demonstrations",
    "url": "http://arxiv.org/abs/2502.13134v1",
    "arxiv_id": "2502.13134v1",
    "authors": [
      "Jingxiao Chen",
      "Xinyao Li",
      "Jiahang Cao",
      "Zhengbang Zhu",
      "Wentao Dong",
      "Minghuan Liu",
      "Ying Wen",
      "Yong Yu",
      "Liqing Zhang",
      "Weinan Zhang"
    ],
    "published": "2025-02-18T18:56:41+00:00",
    "summary": "Humanoid robots have shown success in locomotion and manipulation. Despite these basic abilities, humanoids are still required to quickly understand human instructions and react based on human interaction signals to become valuable assistants in human daily life. Unfortunately, most existing works only focus on multi-stage interactions, treating each task separately, and neglecting real-time feedback. In this work, we aim to empower humanoid robots with real-time reaction abilities to achieve various tasks, allowing human to interrupt robots at any time, and making robots respond to humans immediately. To support such abilities, we propose a general humanoid-human-object interaction framework, named RHINO, i.e., Real-time Humanoid-human Interaction and Object manipulation. RHINO provides a unified view of reactive motion, instruction-based manipulation, and safety concerns, over multiple human signal modalities, such as languages, images, and motions. RHINO is a hierarchical learning framework, enabling humanoids to learn reaction skills from human-human-object demonstrations and teleoperation data. In particular, it decouples the interaction process into two levels: 1) a high-level planner inferring human intentions from real-time human behaviors; and 2) a low-level controller achieving reactive motion behaviors and object manipulation skills based on the predicted intentions. We evaluate the proposed framework on a real humanoid robot and demonstrate its effectiveness, flexibility, and safety in various scenarios."
  },
  {
    "title": "Rethinking Diverse Human Preference Learning through Principal Component Analysis",
    "url": "http://arxiv.org/abs/2502.13131v1",
    "arxiv_id": "2502.13131v1",
    "authors": [
      "Feng Luo",
      "Rui Yang",
      "Hao Sun",
      "Chunyuan Deng",
      "Jiarui Yao",
      "Jingyan Shen",
      "Huan Zhang",
      "Hanjie Chen"
    ],
    "published": "2025-02-18T18:55:26+00:00",
    "summary": "Understanding human preferences is crucial for improving foundation models and building personalized AI systems. However, preferences are inherently diverse and complex, making it difficult for traditional reward models to capture their full range. While fine-grained preference data can help, collecting it is expensive and hard to scale. In this paper, we introduce Decomposed Reward Models (DRMs), a novel approach that extracts diverse human preferences from binary comparisons without requiring fine-grained annotations. Our key insight is to represent human preferences as vectors and analyze them using Principal Component Analysis (PCA). By constructing a dataset of embedding differences between preferred and rejected responses, DRMs identify orthogonal basis vectors that capture distinct aspects of preference. These decomposed rewards can be flexibly combined to align with different user needs, offering an interpretable and scalable alternative to traditional reward models. We demonstrate that DRMs effectively extract meaningful preference dimensions (e.g., helpfulness, safety, humor) and adapt to new users without additional training. Our results highlight DRMs as a powerful framework for personalized and interpretable LLM alignment."
  },
  {
    "title": "Understanding and Rectifying Safety Perception Distortion in VLMs",
    "url": "http://arxiv.org/abs/2502.13095v1",
    "arxiv_id": "2502.13095v1",
    "authors": [
      "Xiaohan Zou",
      "Jian Kang",
      "George Kesidis",
      "Lu Lin"
    ],
    "published": "2025-02-18T18:06:48+00:00",
    "summary": "Recent studies reveal that vision-language models (VLMs) become more susceptible to harmful requests and jailbreak attacks after integrating the vision modality, exhibiting greater vulnerability than their text-only LLM backbones. To uncover the root cause of this phenomenon, we conduct an in-depth analysis and identify a key issue: multimodal inputs introduce an modality-induced activation shift toward a \"safer\" direction compared to their text-only counterparts, leading VLMs to systematically overestimate the safety of harmful inputs. We refer to this issue as safety perception distortion. To mitigate such distortion, we propose Activation Shift Disentanglement and Calibration (ShiftDC), a training-free method that decomposes and calibrates the modality-induced activation shift to reduce the impact of modality on safety. By isolating and removing the safety-relevant component, ShiftDC restores the inherent safety alignment of the LLM backbone while preserving the vision-language capabilities of VLMs. Empirical results demonstrate that ShiftDC significantly enhances alignment performance on safety benchmarks without impairing model utility."
  },
  {
    "title": "Conditional Max-Sum for Asynchronous Multiagent Decision Making",
    "url": "http://arxiv.org/abs/2502.13194v1",
    "arxiv_id": "2502.13194v1",
    "authors": [
      "Dimitrios Troullinos",
      "Georgios Chalkiadakis",
      "Ioannis Papamichail",
      "Markos Papageorgiou"
    ],
    "published": "2025-02-18T17:16:27+00:00",
    "summary": "In this paper we present a novel approach for multiagent decision making in dynamic environments based on Factor Graphs and the Max-Sum algorithm, considering asynchronous variable reassignments and distributed message-passing among agents. Motivated by the challenging domain of lane-free traffic where automated vehicles can communicate and coordinate as agents, we propose a more realistic communication framework for Factor Graph formulations that satisfies the above-mentioned restrictions, along with Conditional Max-Sum: an extension of Max-Sum with a revised message-passing process that is better suited for asynchronous settings. The overall application in lane-free traffic can be viewed as a hybrid system where the Factor Graph formulation undertakes the strategic decision making of vehicles, that of desired lateral alignment in a coordinated manner; and acts on top of a rule-based method we devise that provides a structured representation of the lane-free environment for the factors, while also handling the underlying control of vehicles regarding core operations and safety. Our experimental evaluation showcases the capabilities of the proposed framework in problems with intense coordination needs when compared to a domain-specific baseline without communication, and an increased adeptness of Conditional Max-Sum with respect to the standard algorithm."
  },
  {
    "title": "Interactive Agents to Overcome Ambiguity in Software Engineering",
    "url": "http://arxiv.org/abs/2502.13069v1",
    "arxiv_id": "2502.13069v1",
    "authors": [
      "Sanidhya Vijayvargiya",
      "Xuhui Zhou",
      "Akhila Yerukola",
      "Maarten Sap",
      "Graham Neubig"
    ],
    "published": "2025-02-18T17:12:26+00:00",
    "summary": "AI agents are increasingly being deployed to automate tasks, often based on ambiguous and underspecified user instructions. Making unwarranted assumptions and failing to ask clarifying questions can lead to suboptimal outcomes, safety risks due to tool misuse, and wasted computational resources. In this work, we study the ability of LLM agents to handle ambiguous instructions in interactive code generation settings by evaluating proprietary and open-weight models on their performance across three key steps: (a) leveraging interactivity to improve performance in ambiguous scenarios, (b) detecting ambiguity, and (c) asking targeted questions. Our findings reveal that models struggle to distinguish between well-specified and underspecified instructions. However, when models interact for underspecified inputs, they effectively obtain vital information from the user, leading to significant improvements in performance and underscoring the value of effective interaction. Our study highlights critical gaps in how current state-of-the-art models handle ambiguity in complex software engineering tasks and structures the evaluation into distinct steps to enable targeted improvements."
  },
  {
    "title": "Enhancing Power Grid Inspections with Machine Learning",
    "url": "http://arxiv.org/abs/2502.13037v1",
    "arxiv_id": "2502.13037v1",
    "authors": [
      "Diogo Lavado",
      "Ricardo Santos",
      "Andre Coelho",
      "Joao Santos",
      "Alessandra Micheletti",
      "Claudia Soares"
    ],
    "published": "2025-02-18T16:49:47+00:00",
    "summary": "Ensuring the safety and reliability of power grids is critical as global energy demands continue to rise. Traditional inspection methods, such as manual observations or helicopter surveys, are resource-intensive and lack scalability. This paper explores the use of 3D computer vision to automate power grid inspections, utilizing the TS40K dataset -- a high-density, annotated collection of 3D LiDAR point clouds. By concentrating on 3D semantic segmentation, our approach addresses challenges like class imbalance and noisy data to enhance the detection of critical grid components such as power lines and towers. The benchmark results indicate significant performance improvements, with IoU scores reaching 95.53% for the detection of power lines using transformer-based models. Our findings illustrate the potential for integrating ML into grid maintenance workflows, increasing efficiency and enabling proactive risk management strategies."
  },
  {
    "title": "Fragility-aware Classification for Understanding Risk and Improving Generalization",
    "url": "http://arxiv.org/abs/2502.13024v1",
    "arxiv_id": "2502.13024v1",
    "authors": [
      "Chen Yang",
      "Zheng Cui",
      "Daniel Zhuoyu Long",
      "Jin Qi",
      "Ruohan Zhan"
    ],
    "published": "2025-02-18T16:44:03+00:00",
    "summary": "Classification models play a critical role in data-driven decision-making applications such as medical diagnosis, user profiling, recommendation systems, and default detection. Traditional performance metrics, such as accuracy, focus on overall error rates but fail to account for the confidence of incorrect predictions, thereby overlooking the risk of confident misjudgments. This risk is particularly significant in cost-sensitive and safety-critical domains like medical diagnosis and autonomous driving, where overconfident false predictions may cause severe consequences. To address this issue, we introduce the Fragility Index (FI), a novel metric that evaluates classification performance from a risk-averse perspective by explicitly capturing the tail risk of confident misjudgments. To enhance generalizability, we define FI within the robust satisficing (RS) framework, incorporating data uncertainty. We further develop a model training approach that optimizes FI while maintaining tractability for common loss functions. Specifically, we derive exact reformulations for cross-entropy loss, hinge-type loss, and Lipschitz loss, and extend the approach to deep learning models. Through synthetic experiments and real-world medical diagnosis tasks, we demonstrate that FI effectively identifies misjudgment risk and FI-based training improves model robustness and generalizability. Finally, we extend our framework to deep neural network training, further validating its effectiveness in enhancing deep learning models."
  },
  {
    "title": "Reasoning-to-Defend: Safety-Aware Reasoning Can Defend Large Language Models from Jailbreaking",
    "url": "http://arxiv.org/abs/2502.12970v1",
    "arxiv_id": "2502.12970v1",
    "authors": [
      "Junda Zhu",
      "Lingyong Yan",
      "Shuaiqiang Wang",
      "Dawei Yin",
      "Lei Sha"
    ],
    "published": "2025-02-18T15:48:46+00:00",
    "summary": "The reasoning abilities of Large Language Models (LLMs) have demonstrated remarkable advancement and exceptional performance across diverse domains. However, leveraging these reasoning capabilities to enhance LLM safety against adversarial attacks and jailbreak queries remains largely unexplored. To bridge this gap, we propose Reasoning-to-Defend (R2D), a novel training paradigm that integrates safety reflections of queries and responses into LLMs' generation process, unlocking a safety-aware reasoning mechanism. This approach enables self-evaluation at each reasoning step to create safety pivot tokens as indicators of the response's safety status. Furthermore, in order to improve the learning efficiency of pivot token prediction, we propose Contrastive Pivot Optimization(CPO), which enhances the model's ability to perceive the safety status of dialogues. Through this mechanism, LLMs dynamically adjust their response strategies during reasoning, significantly enhancing their defense capabilities against jailbreak attacks. Extensive experimental results demonstrate that R2D effectively mitigates various attacks and improves overall safety, highlighting the substantial potential of safety-aware reasoning in strengthening LLMs' robustness against jailbreaks."
  },
  {
    "title": "Trust Me, I'm Wrong: High-Certainty Hallucinations in LLMs",
    "url": "http://arxiv.org/abs/2502.12964v1",
    "arxiv_id": "2502.12964v1",
    "authors": [
      "Adi Simhi",
      "Itay Itzhak",
      "Fazl Barez",
      "Gabriel Stanovsky",
      "Yonatan Belinkov"
    ],
    "published": "2025-02-18T15:46:31+00:00",
    "summary": "Large Language Models (LLMs) often generate outputs that lack grounding in real-world facts, a phenomenon known as hallucinations. Prior research has associated hallucinations with model uncertainty, leveraging this relationship for hallucination detection and mitigation. In this paper, we challenge the underlying assumption that all hallucinations are associated with uncertainty. Using knowledge detection and uncertainty measurement methods, we demonstrate that models can hallucinate with high certainty even when they have the correct knowledge. We further show that high-certainty hallucinations are consistent across models and datasets, distinctive enough to be singled out, and challenge existing mitigation methods. Our findings reveal an overlooked aspect of hallucinations, emphasizing the need to understand their origins and improve mitigation strategies to enhance LLM safety. The code is available at https://github.com/technion-cs-nlp/Trust_me_Im_wrong ."
  },
  {
    "title": "LLMPopcorn: An Empirical Study of LLMs as Assistants for Popular Micro-video Generation",
    "url": "http://arxiv.org/abs/2502.12945v1",
    "arxiv_id": "2502.12945v1",
    "authors": [
      "Junchen Fu",
      "Xuri Ge",
      "Kaiwen Zheng",
      "Ioannis Arapakis",
      "Xin Xin",
      "Joemon M. Jose"
    ],
    "published": "2025-02-18T15:29:05+00:00",
    "summary": "Popular Micro-videos, dominant on platforms like TikTok and YouTube, hold significant commercial value. The rise of high-quality AI-generated content has spurred interest in AI-driven micro-video creation. However, despite the advanced capabilities of large language models (LLMs) like ChatGPT and DeepSeek in text generation and reasoning, their potential to assist the creation of popular micro-videos remains largely unexplored.   In this paper, we conduct an empirical study on LLM-assisted popular micro-video generation (LLMPopcorn). Specifically, we investigate the following research questions: (i) How can LLMs be effectively utilized to assist popular micro-video generation? (ii) To what extent can prompt-based enhancements optimize the LLM-generated content for higher popularity? (iii) How well do various LLMs and video generators perform in the popular micro-video generation task? By exploring these questions, we show that advanced LLMs like DeepSeek-V3 enable micro-video generation to achieve popularity comparable to human-created content. Prompt enhancements further boost popularity, and benchmarking highlights DeepSeek-V3 and DeepSeek-R1 among LLMs, while LTX-Video and HunyuanVideo lead in video generation. This pioneering work advances AI-assisted micro-video creation, uncovering new research opportunities. We will release the code and datasets to support future studies."
  },
  {
    "title": "LLMPopcorn: An Empirical Study of LLMs as Assistants for Popular Micro-video Generation",
    "url": "http://arxiv.org/abs/2502.12945v2",
    "arxiv_id": "2502.12945v2",
    "authors": [
      "Junchen Fu",
      "Xuri Ge",
      "Kaiwen Zheng",
      "Ioannis Arapakis",
      "Xin Xin",
      "Joemon M. Jose"
    ],
    "published": "2025-02-18T15:29:05+00:00",
    "summary": "Popular Micro-videos, dominant on platforms like TikTok and YouTube, hold significant commercial value. The rise of high-quality AI-generated content has spurred interest in AI-driven micro-video creation. However, despite the advanced capabilities of large language models (LLMs) like ChatGPT and DeepSeek in text generation and reasoning, their potential to assist the creation of popular micro-videos remains largely unexplored.   In this paper, we conduct an empirical study on LLM-assisted popular micro-video generation (LLMPopcorn). Specifically, we investigate the following research questions: (i) How can LLMs be effectively utilized to assist popular micro-video generation? (ii) To what extent can prompt-based enhancements optimize the LLM-generated content for higher popularity? (iii) How well do various LLMs and video generators perform in the popular micro-video generation task? By exploring these questions, we show that advanced LLMs like DeepSeek-V3 enable micro-video generation to achieve popularity comparable to human-created content. Prompt enhancements further boost popularity, and benchmarking highlights DeepSeek-V3 and DeepSeek-R1 among LLMs, while LTX-Video and HunyuanVideo lead in video generation. This pioneering work advances AI-assisted micro-video creation, uncovering new research opportunities. We will release the code and datasets to support future studies."
  },
  {
    "title": "None of the Others: a General Technique to Distinguish Reasoning from Memorization in Multiple-Choice LLM Evaluation Benchmarks",
    "url": "http://arxiv.org/abs/2502.12896v1",
    "arxiv_id": "2502.12896v1",
    "authors": [
      "Eva S\u00e1nchez Salido",
      "Julio Gonzalo",
      "Guillermo Marco"
    ],
    "published": "2025-02-18T14:32:44+00:00",
    "summary": "In LLM evaluations, reasoning is often distinguished from recall/memorization by performing numerical variations to math-oriented questions. Here we introduce a general variation method for multiple-choice questions that completely dissociates the correct answer from previously seen tokens or concepts, requiring LLMs to understand and reason (rather than memorizing) in order to answer correctly. Using this method, we evaluate state-of-the-art proprietary and open-source LLMs on two datasets available in English and Spanish: the public MMLU benchmark and the private UNED-Access 2024 dataset. Results show that all models experience remarkable accuracy drops under our proposed variation, with an average loss of 57% on MMLU and 50% on UNED-Access 2024, ranging from 10% to 93% across models. Notably, the most accurate model in our experimentation (OpenAI-o3-mini) is not the most robust (DeepSeek-R1-70B), suggesting that the best models in standard evaluations may not be the ones with better reasoning capabilities. Also, we see larger accuracy drops in public (vs private) datasets and questions posed in their original language (vs a manual translation), which are signs of contamination and also point to a relevant role of recall/memorization in current LLMs' answers."
  },
  {
    "title": "H-CoT: Hijacking the Chain-of-Thought Safety Reasoning Mechanism to Jailbreak Large Reasoning Models, Including OpenAI o1/o3, DeepSeek-R1, and Gemini 2.0 Flash Thinking",
    "url": "http://arxiv.org/abs/2502.12893v1",
    "arxiv_id": "2502.12893v1",
    "authors": [
      "Martin Kuo",
      "Jianyi Zhang",
      "Aolin Ding",
      "Qinsi Wang",
      "Louis DiValentin",
      "Yujia Bao",
      "Wei Wei",
      "Da-Cheng Juan",
      "Hai Li",
      "Yiran Chen"
    ],
    "published": "2025-02-18T14:29:12+00:00",
    "summary": "Large Reasoning Models (LRMs) have recently extended their powerful reasoning capabilities to safety checks-using chain-of-thought reasoning to decide whether a request should be answered. While this new approach offers a promising route for balancing model utility and safety, its robustness remains underexplored. To address this gap, we introduce Malicious-Educator, a benchmark that disguises extremely dangerous or malicious requests beneath seemingly legitimate educational prompts. Our experiments reveal severe security flaws in popular commercial-grade LRMs, including OpenAI o1/o3, DeepSeek-R1, and Gemini 2.0 Flash Thinking. For instance, although OpenAI's o1 model initially maintains a high refusal rate of about 98%, subsequent model updates significantly compromise its safety; and attackers can easily extract criminal strategies from DeepSeek-R1 and Gemini 2.0 Flash Thinking without any additional tricks. To further highlight these vulnerabilities, we propose Hijacking Chain-of-Thought (H-CoT), a universal and transferable attack method that leverages the model's own displayed intermediate reasoning to jailbreak its safety reasoning mechanism. Under H-CoT, refusal rates sharply decline-dropping from 98% to below 2%-and, in some instances, even transform initially cautious tones into ones that are willing to provide harmful content. We hope these findings underscore the urgent need for more robust safety mechanisms to preserve the benefits of advanced reasoning capabilities without compromising ethical standards."
  },
  {
    "title": "A Survey of Sim-to-Real Methods in RL: Progress, Prospects and Challenges with Foundation Models",
    "url": "http://arxiv.org/abs/2502.13187v1",
    "arxiv_id": "2502.13187v1",
    "authors": [
      "Longchao Da",
      "Justin Turnau",
      "Thirulogasankar Pranav Kutralingam",
      "Alvaro Velasquez",
      "Paulo Shakarian",
      "Hua Wei"
    ],
    "published": "2025-02-18T12:57:29+00:00",
    "summary": "Deep Reinforcement Learning (RL) has been explored and verified to be effective in solving decision-making tasks in various domains, such as robotics, transportation, recommender systems, etc. It learns from the interaction with environments and updates the policy using the collected experience. However, due to the limited real-world data and unbearable consequences of taking detrimental actions, the learning of RL policy is mainly restricted within the simulators. This practice guarantees safety in learning but introduces an inevitable sim-to-real gap in terms of deployment, thus causing degraded performance and risks in execution. There are attempts to solve the sim-to-real problems from different domains with various techniques, especially in the era with emerging techniques such as large foundations or language models that have cast light on the sim-to-real. This survey paper, to the best of our knowledge, is the first taxonomy that formally frames the sim-to-real techniques from key elements of the Markov Decision Process (State, Action, Transition, and Reward). Based on the framework, we cover comprehensive literature from the classic to the most advanced methods including the sim-to-real techniques empowered by foundation models, and we also discuss the specialties that are worth attention in different domains of sim-to-real problems. Then we summarize the formal evaluation process of sim-to-real performance with accessible code or benchmarks. The challenges and opportunities are also presented to encourage future exploration of this direction. We are actively maintaining a to include the most up-to-date sim-to-real research outcomes to help the researchers in their work."
  },
  {
    "title": "Reasoning and the Trusting Behavior of DeepSeek and GPT: An Experiment Revealing Hidden Fault Lines in Large Language Models",
    "url": "http://arxiv.org/abs/2502.12825v1",
    "arxiv_id": "2502.12825v1",
    "authors": [
      "Rubing Lu",
      "Jo\u00e3o Sedoc",
      "Arun Sundararajan"
    ],
    "published": "2025-02-18T12:46:18+00:00",
    "summary": "When encountering increasingly frequent performance improvements or cost reductions from a new large language model (LLM), developers of applications leveraging LLMs must decide whether to take advantage of these improvements or stay with older tried-and-tested models. Low perceived switching frictions can lead to choices that do not consider more subtle behavior changes that the transition may induce. Our experiments use a popular game-theoretic behavioral economics model of trust to show stark differences in the trusting behavior of OpenAI's and DeepSeek's models. We highlight a collapse in the economic trust behavior of the o1-mini and o3-mini models as they reconcile profit-maximizing and risk-seeking with future returns from trust, and contrast it with DeepSeek's more sophisticated and profitable trusting behavior that stems from an ability to incorporate deeper concepts like forward planning and theory-of-mind. As LLMs form the basis for high-stakes commercial systems, our results highlight the perils of relying on LLM performance benchmarks that are too narrowly defined and suggest that careful analysis of their hidden fault lines should be part of any organization's AI strategy."
  },
  {
    "title": "Reasoning and the Trusting Behavior of DeepSeek and GPT: An Experiment Revealing Hidden Fault Lines in Large Language Models",
    "url": "http://arxiv.org/abs/2502.12825v2",
    "arxiv_id": "2502.12825v2",
    "authors": [
      "Rubing Li",
      "Jo\u00e3o Sedoc",
      "Arun Sundararajan"
    ],
    "published": "2025-02-18T12:46:18+00:00",
    "summary": "When encountering increasingly frequent performance improvements or cost reductions from a new large language model (LLM), developers of applications leveraging LLMs must decide whether to take advantage of these improvements or stay with older tried-and-tested models. Low perceived switching frictions can lead to choices that do not consider more subtle behavior changes that the transition may induce. Our experiments use a popular game-theoretic behavioral economics model of trust to show stark differences in the trusting behavior of OpenAI's and DeepSeek's models. We highlight a collapse in the economic trust behavior of the o1-mini and o3-mini models as they reconcile profit-maximizing and risk-seeking with future returns from trust, and contrast it with DeepSeek's more sophisticated and profitable trusting behavior that stems from an ability to incorporate deeper concepts like forward planning and theory-of-mind. As LLMs form the basis for high-stakes commercial systems, our results highlight the perils of relying on LLM performance benchmarks that are too narrowly defined and suggest that careful analysis of their hidden fault lines should be part of any organization's AI strategy."
  },
  {
    "title": "Simulating User Diversity in Task-Oriented Dialogue Systems using Large Language Models",
    "url": "http://arxiv.org/abs/2502.12813v1",
    "arxiv_id": "2502.12813v1",
    "authors": [
      "Adnan Ahmad",
      "Stefan Hillmann",
      "Sebastian M\u00f6ller"
    ],
    "published": "2025-02-18T12:20:16+00:00",
    "summary": "In this study, we explore the application of Large Language Models (LLMs) for generating synthetic users and simulating user conversations with a task-oriented dialogue system and present detailed results and their analysis. We propose a comprehensive novel approach to user simulation technique that uses LLMs to create diverse user profiles, set goals, engage in multi-turn dialogues, and evaluate the conversation success. We employ two proprietary LLMs, namely GPT-4o and GPT-o1 (Achiam et al., 2023), to generate a heterogeneous base of user profiles, characterized by varied demographics, multiple user goals, different conversational styles, initial knowledge levels, interests, and conversational objectives. We perform a detailed analysis of the user profiles generated by LLMs to assess the diversity, consistency, and potential biases inherent in these LLM-generated user simulations. We find that GPT-o1 generates more heterogeneous user distribution across most user attributes, while GPT-4o generates more skewed user attributes. The generated set of user profiles are then utilized to simulate dialogue sessions by interacting with a task-oriented dialogue system."
  },
  {
    "title": "Expanding the Classical V-Model for the Development of Complex Systems Incorporating AI",
    "url": "http://arxiv.org/abs/2502.13184v1",
    "arxiv_id": "2502.13184v1",
    "authors": [
      "Lars Ullrich",
      "Michael Buchholz",
      "Klaus Dietmayer",
      "Knut Graichen"
    ],
    "published": "2025-02-18T11:01:37+00:00",
    "summary": "Research in the field of automated vehicles, or more generally cognitive cyber-physical systems that operate in the real world, is leading to increasingly complex systems. Among other things, artificial intelligence enables an ever-increasing degree of autonomy. In this context, the V-model, which has served for decades as a process reference model of the system development lifecycle is reaching its limits. To the contrary, innovative processes and frameworks have been developed that take into account the characteristics of emerging autonomous systems. To bridge the gap and merge the different methodologies, we present an extension of the V-model for iterative data-based development processes that harmonizes and formalizes the existing methods towards a generic framework. The iterative approach allows for seamless integration of continuous system refinement. While the data-based approach constitutes the consideration of data-based development processes and formalizes the use of synthetic and real world data. In this way, formalizing the process of development, verification, validation, and continuous integration contributes to ensuring the safety of emerging complex systems that incorporate AI."
  },
  {
    "title": "IPSR Model: Misinformation Intervention through Prebunking in Social Networks",
    "url": "http://arxiv.org/abs/2502.12740v1",
    "arxiv_id": "2502.12740v1",
    "authors": [
      "Robert Rai",
      "Rajesh Sharma",
      "Chandrakala Meena"
    ],
    "published": "2025-02-18T10:56:30+00:00",
    "summary": "In the present digital world, the rapid spread of misinformation is not just an annoyance but a real threat to public safety, and our collective decision-making. Prebunking, a type of psychological immunization, can educate people about misinformation and lay a foundation of cognitive resilience that makes them more robust against future misinformation. We use a compartmental modeling approach inspired by vaccination models from epidemiology to model the effectiveness of prebunking misinformation. Populations are classified into different compartments based on the exposure to prebunking and the propagation of misinformation through online social networks. Specific rates dictate the transitions between such states, similar to how people traverse between susceptible, infected, and recovered compartments in classical epidemiological models. This model integrates different levels of prebunking potency, the fraction of the population prebunked initially, and the forgetting rate effects. To the best of our knowledge this is the first work which study the extent of prebunking interventions to reduce the scale of misinformation, much as vaccinations curtail the spread of infectious diseases."
  },
  {
    "title": "myEye2Wheeler: A Two-Wheeler Indian Driver Real-World Eye-Tracking Dataset",
    "url": "http://arxiv.org/abs/2502.12723v1",
    "arxiv_id": "2502.12723v1",
    "authors": [
      "Bhaiya Vaibhaw Kumar",
      "Deepti Rawat",
      "Tanvi Kandalla",
      "Aarnav Nagariya",
      "Kavita Vemuri"
    ],
    "published": "2025-02-18T10:39:00+00:00",
    "summary": "This paper presents the myEye2Wheeler dataset, a unique resource of real-world gaze behaviour of two-wheeler drivers navigating complex Indian traffic. Most datasets are from four-wheeler drivers on well-planned roads and homogeneous traffic. Our dataset offers a critical lens into the unique visual attention patterns and insights into the decision-making of Indian two-wheeler drivers. The analysis demonstrates that existing saliency models, like TASED-Net, perform less effectively on the myEye-2Wheeler dataset compared to when applied on the European 4-wheeler eye tracking datasets (DR(Eye)VE), highlighting the need for models specifically tailored to the traffic conditions. By introducing the dataset, we not only fill a significant gap in two-wheeler driver behaviour research in India but also emphasise the critical need for developing context-specific saliency models. The larger aim is to improve road safety for two-wheeler users and lane-planning to support a cost-effective mode of transport."
  },
  {
    "title": "SATA: Safe and Adaptive Torque-Based Locomotion Policies Inspired by Animal Learning",
    "url": "http://arxiv.org/abs/2502.12674v1",
    "arxiv_id": "2502.12674v1",
    "authors": [
      "Peizhuo Li",
      "Hongyi Li",
      "Ge Sun",
      "Jin Cheng",
      "Xinrong Yang",
      "Guillaume Bellegarda",
      "Milad Shafiee",
      "Yuhong Cao",
      "Auke Ijspeert",
      "Guillaume Sartoretti"
    ],
    "published": "2025-02-18T09:25:37+00:00",
    "summary": "Despite recent advances in learning-based controllers for legged robots, deployments in human-centric environments remain limited by safety concerns. Most of these approaches use position-based control, where policies output target joint angles that must be processed by a low-level controller (e.g., PD or impedance controllers) to compute joint torques. Although impressive results have been achieved in controlled real-world scenarios, these methods often struggle with compliance and adaptability when encountering environments or disturbances unseen during training, potentially resulting in extreme or unsafe behaviors. Inspired by how animals achieve smooth and adaptive movements by controlling muscle extension and contraction, torque-based policies offer a promising alternative by enabling precise and direct control of the actuators in torque space. In principle, this approach facilitates more effective interactions with the environment, resulting in safer and more adaptable behaviors. However, challenges such as a highly nonlinear state space and inefficient exploration during training have hindered their broader adoption. To address these limitations, we propose SATA, a bio-inspired framework that mimics key biomechanical principles and adaptive learning mechanisms observed in animal locomotion. Our approach effectively addresses the inherent challenges of learning torque-based policies by significantly improving early-stage exploration, leading to high-performance final policies. Remarkably, our method achieves zero-shot sim-to-real transfer. Our experimental results indicate that SATA demonstrates remarkable compliance and safety, even in challenging environments such as soft/slippery terrain or narrow passages, and under significant external disturbances, highlighting its potential for practical deployments in human-centric and safety-critical scenarios."
  },
  {
    "title": "The Hidden Risks of Large Reasoning Models: A Safety Assessment of R1",
    "url": "http://arxiv.org/abs/2502.12659v1",
    "arxiv_id": "2502.12659v1",
    "authors": [
      "Kaiwen Zhou",
      "Chengzhi Liu",
      "Xuandong Zhao",
      "Shreedhar Jangam",
      "Jayanth Srinivasa",
      "Gaowen Liu",
      "Dawn Song",
      "Xin Eric Wang"
    ],
    "published": "2025-02-18T09:06:07+00:00",
    "summary": "The rapid development of large reasoning models, such as OpenAI-o3 and DeepSeek-R1, has led to significant improvements in complex reasoning over non-reasoning large language models~(LLMs). However, their enhanced capabilities, combined with the open-source access of models like DeepSeek-R1, raise serious safety concerns, particularly regarding their potential for misuse. In this work, we present a comprehensive safety assessment of these reasoning models, leveraging established safety benchmarks to evaluate their compliance with safety regulations. Furthermore, we investigate their susceptibility to adversarial attacks, such as jailbreaking and prompt injection, to assess their robustness in real-world applications. Through our multi-faceted analysis, we uncover four key findings: (1) There is a significant safety gap between the open-source R1 models and the o3-mini model, on both safety benchmark and attack, suggesting more safety effort on R1 is needed. (2) The distilled reasoning model shows poorer safety performance compared to its safety-aligned base models. (3) The stronger the model's reasoning ability, the greater the potential harm it may cause when answering unsafe questions. (4) The thinking process in R1 models pose greater safety concerns than their final answers. Our study provides insights into the security implications of reasoning models and highlights the need for further advancements in R1 models' safety to close the gap."
  },
  {
    "title": "A Graph-Enhanced Deep-Reinforcement Learning Framework for the Aircraft Landing Problem",
    "url": "http://arxiv.org/abs/2502.12617v1",
    "arxiv_id": "2502.12617v1",
    "authors": [
      "Vatsal Maru"
    ],
    "published": "2025-02-18T08:02:17+00:00",
    "summary": "The Aircraft Landing Problem (ALP) is one of the challenging problems in aircraft transportation and management. The challenge is to schedule the arriving aircraft in a sequence so that the cost and delays are optimized. There are various solution approaches to solving this problem, most of which are based on operations research algorithms and meta-heuristics. Although traditional methods perform better on one or the other factors, there remains a problem of solving real-time rescheduling and computational scalability altogether. This paper presents a novel deep reinforcement learning (DRL) framework that combines graph neural networks with actor-critic architectures to address the ALP. This paper introduces three key contributions: A graph-based state representation that efficiently captures temporal and spatial relationships between aircraft, a specialized actor-critic architecture designed to handle multiple competing objectives in landing scheduling, and a runway balance strategy that ensures efficient resource utilization while maintaining safety constraints. The results show that the trained algorithm can be tested on different problem sets and the results are competitive to operation research algorithms. The experimental results on standard benchmark data sets demonstrate a 99.95 reduction in computational time compared to Mixed Integer Programming (MIP) and 38 higher runway throughput over First Come First Serve (FCFS) approaches. Therefore, the proposed solution is competitive to traditional approaches and achieves substantial advancements. Notably, it does not require retraining, making it particularly suitable for industrial deployment. The frameworks capability to generate solutions within 1 second enables real-time rescheduling, addressing critical requirements of air traffic management."
  },
  {
    "title": "Learning-based Dynamic Robot-to-Human Handover",
    "url": "http://arxiv.org/abs/2502.12602v1",
    "arxiv_id": "2502.12602v1",
    "authors": [
      "Hyeonseong Kim",
      "Chanwoo Kim",
      "Matthew Pan",
      "Kyungjae Lee",
      "Sungjoon Choi"
    ],
    "published": "2025-02-18T07:26:07+00:00",
    "summary": "This paper presents a novel learning-based approach to dynamic robot-to-human handover, addressing the challenges of delivering objects to a moving receiver. We hypothesize that dynamic handover, where the robot adjusts to the receiver's movements, results in more efficient and comfortable interaction compared to static handover, where the receiver is assumed to be stationary. To validate this, we developed a nonparametric method for generating continuous handover motion, conditioned on the receiver's movements, and trained the model using a dataset of 1,000 human-to-human handover demonstrations. We integrated preference learning for improved handover effectiveness and applied impedance control to ensure user safety and adaptiveness. The approach was evaluated in both simulation and real-world settings, with user studies demonstrating that dynamic handover significantly reduces handover time and improves user comfort compared to static methods. Videos and demonstrations of our approach are available at https://zerotohero7886.github.io/dyn-r2h-handover ."
  },
  {
    "title": "DemonAgent: Dynamically Encrypted Multi-Backdoor Implantation Attack on LLM-based Agent",
    "url": "http://arxiv.org/abs/2502.12575v1",
    "arxiv_id": "2502.12575v1",
    "authors": [
      "Pengyu Zhu",
      "Zhenhong Zhou",
      "Yuanhe Zhang",
      "Shilinlu Yan",
      "Kun Wang",
      "Sen Su"
    ],
    "published": "2025-02-18T06:26:15+00:00",
    "summary": "As LLM-based agents become increasingly prevalent, backdoors can be implanted into agents through user queries or environment feedback, raising critical concerns regarding safety vulnerabilities. However, backdoor attacks are typically detectable by safety audits that analyze the reasoning process of agents. To this end, we propose a novel backdoor implantation strategy called \\textbf{Dynamically Encrypted Multi-Backdoor Implantation Attack}. Specifically, we introduce dynamic encryption, which maps the backdoor into benign content, effectively circumventing safety audits. To enhance stealthiness, we further decompose the backdoor into multiple sub-backdoor fragments. Based on these advancements, backdoors are allowed to bypass safety audits significantly. Additionally, we present AgentBackdoorEval, a dataset designed for the comprehensive evaluation of agent backdoor attacks. Experimental results across multiple datasets demonstrate that our method achieves an attack success rate nearing 100\\% while maintaining a detection rate of 0\\%, illustrating its effectiveness in evading safety audits. Our findings highlight the limitations of existing safety mechanisms in detecting advanced attacks, underscoring the urgent need for more robust defenses against backdoor threats. Code and data are available at https://github.com/whfeLingYu/DemonAgent."
  },
  {
    "title": "Exploring the Impact of Personality Traits on LLM Bias and Toxicity",
    "url": "http://arxiv.org/abs/2502.12566v1",
    "arxiv_id": "2502.12566v1",
    "authors": [
      "Shuo Wang",
      "Renhao Li",
      "Xi Chen",
      "Yulin Yuan",
      "Derek F. Wong",
      "Min Yang"
    ],
    "published": "2025-02-18T06:07:09+00:00",
    "summary": "With the different roles that AI is expected to play in human life, imbuing large language models (LLMs) with different personalities has attracted increasing research interests. While the \"personification\" enhances human experiences of interactivity and adaptability of LLMs, it gives rise to critical concerns about content safety, particularly regarding bias, sentiment and toxicity of LLM generation. This study explores how assigning different personality traits to LLMs affects the toxicity and biases of their outputs. Leveraging the widely accepted HEXACO personality framework developed in social psychology, we design experimentally sound prompts to test three LLMs' performance on three toxic and bias benchmarks. The findings demonstrate the sensitivity of all three models to HEXACO personality traits and, more importantly, a consistent variation in the biases, negative sentiment and toxicity of their output. In particular, adjusting the levels of several personality traits can effectively reduce bias and toxicity in model performance, similar to humans' correlations between personality traits and toxic behaviors. The findings highlight the additional need to examine content safety besides the efficiency of training or fine-tuning methods for LLM personification. They also suggest a potential for the adjustment of personalities to be a simple and low-cost method to conduct controlled text generation."
  },
  {
    "title": "Self Iterative Label Refinement via Robust Unlabeled Learning",
    "url": "http://arxiv.org/abs/2502.12565v1",
    "arxiv_id": "2502.12565v1",
    "authors": [
      "Hikaru Asano",
      "Tadashi Kozuno",
      "Yukino Baba"
    ],
    "published": "2025-02-18T06:04:18+00:00",
    "summary": "Recent advances in large language models (LLMs) have yielded impressive performance on various tasks, yet they often depend on high-quality feedback that can be costly. Self-refinement methods attempt to leverage LLMs' internal evaluation mechanisms with minimal human supervision; however, these approaches frequently suffer from inherent biases and overconfidence, especially in domains where the models lack sufficient internal knowledge, resulting in performance degradation. As an initial step toward enhancing self-refinement for broader applications, we introduce an iterative refinement pipeline that employs the Unlabeled-Unlabeled learning framework to improve LLM-generated pseudo-labels for classification tasks. By exploiting two unlabeled datasets with differing positive class ratios, our approach iteratively denoises and refines the initial pseudo-labels, thereby mitigating the adverse effects of internal biases with minimal human supervision. Evaluations on diverse datasets, including low-resource language corpora, patent classifications, and protein structure categorizations, demonstrate that our method consistently outperforms both initial LLM's classification performance and the self-refinement approaches by cutting-edge models (e.g., GPT-4o and DeepSeek-R1)."
  },
  {
    "title": "SEA: Low-Resource Safety Alignment for Multimodal Large Language Models via Synthetic Embeddings",
    "url": "http://arxiv.org/abs/2502.12562v1",
    "arxiv_id": "2502.12562v1",
    "authors": [
      "Weikai Lu",
      "Hao Peng",
      "Huiping Zhuang",
      "Cen Chen",
      "Ziqian Zeng"
    ],
    "published": "2025-02-18T05:57:35+00:00",
    "summary": "Multimodal Large Language Models (MLLMs) have serious security vulnerabilities.While safety alignment using multimodal datasets consisting of text and data of additional modalities can effectively enhance MLLM's security, it is costly to construct these datasets. Existing low-resource security alignment methods, including textual alignment, have been found to struggle with the security risks posed by additional modalities. To address this, we propose Synthetic Embedding augmented safety Alignment (SEA), which optimizes embeddings of additional modality through gradient updates to expand textual datasets. This enables multimodal safety alignment training even when only textual data is available. Extensive experiments on image, video, and audio-based MLLMs demonstrate that SEA can synthesize a high-quality embedding on a single RTX3090 GPU within 24 seconds. SEA significantly improves the security of MLLMs when faced with threats from additional modalities. To assess the security risks introduced by video and audio, we also introduced a new benchmark called VA-SafetyBench. High attack success rates across multiple MLLMs validate its challenge. Our code and data will be available at https://github.com/ZeroNLP/SEA."
  },
  {
    "title": "From Maneuver to Mishap: A Systematic Literature Review on U-Turn Safety Risks",
    "url": "http://arxiv.org/abs/2502.12556v1",
    "arxiv_id": "2502.12556v1",
    "authors": [
      "Syed Aaqib Javed",
      "Anannya Ghosh Tusti",
      "Biplov Pandeym Subasish Das"
    ],
    "published": "2025-02-18T05:44:19+00:00",
    "summary": "Understanding the impacts of U-turn configurations on intersection safety and traffic operations is essential for developing effective strategies to enhance road safety and efficiency. Extensive research has been conducted to investigate the role of geometric designs, driver behavior, and advanced technologies in mitigating crash risks and improving traffic flow at U-turn facilities. By synthesizing this collective body of work through the guidelines of Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA), this paper provides a valuable resource for transportation professionals, policymakers, and researchers seeking evidence-based solutions. This systematic review draws on studies from diverse traffic environments and regional contexts, focusing on innovative design interventions, such as restricted crossing U-turns (RCUTs) and median U-turn intersections (MUTs), as well as integrated strategies leveraging technological advancements. By presenting a comprehensive analysis of U-turn-related challenges and opportunities, this review contributes to advancing transportation safety research and guiding the development of adaptive strategies tailored to varied traffic conditions and evolving technologies."
  },
  {
    "title": "From Maneuver to Mishap: A Systematic Literature Review on U-Turn Safety Risks",
    "url": "http://arxiv.org/abs/2502.12556v2",
    "arxiv_id": "2502.12556v2",
    "authors": [
      "Syed Aaqib Javed",
      "Anannya Ghosh Tusti",
      "Biplov Pandey",
      "Subasish Das"
    ],
    "published": "2025-02-18T05:44:19+00:00",
    "summary": "Understanding the impacts of U-turn configurations on intersection safety and traffic operations is essential for developing effective strategies to enhance road safety and efficiency. Extensive research has been conducted to investigate the role of geometric designs, driver behavior, and advanced technologies in mitigating crash risks and improving traffic flow at U-turn facilities. By synthesizing this collective body of work through the guidelines of Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA), this paper provides a valuable resource for transportation professionals, policymakers, and researchers seeking evidence-based solutions. This systematic review draws on studies from diverse traffic environments and regional contexts, focusing on innovative design interventions, such as restricted crossing U-turns (RCUTs) and median U-turn intersections (MUTs), as well as integrated strategies leveraging technological advancements. By presenting a comprehensive analysis of U-turn-related challenges and opportunities, this review contributes to advancing transportation safety research and guiding the development of adaptive strategies tailored to varied traffic conditions and evolving technologies."
  },
  {
    "title": "LLM Safety for Children",
    "url": "http://arxiv.org/abs/2502.12552v1",
    "arxiv_id": "2502.12552v1",
    "authors": [
      "Prasanjit Rath",
      "Hari Shrawgi",
      "Parag Agrawal",
      "Sandipan Dandapat"
    ],
    "published": "2025-02-18T05:26:27+00:00",
    "summary": "This paper analyzes the safety of Large Language Models (LLMs) in interactions with children below age of 18 years. Despite the transformative applications of LLMs in various aspects of children's lives such as education and therapy, there remains a significant gap in understanding and mitigating potential content harms specific to this demographic. The study acknowledges the diverse nature of children often overlooked by standard safety evaluations and proposes a comprehensive approach to evaluating LLM safety specifically for children. We list down potential risks that children may encounter when using LLM powered applications. Additionally we develop Child User Models that reflect the varied personalities and interests of children informed by literature in child care and psychology. These user models aim to bridge the existing gap in child safety literature across various fields. We utilize Child User Models to evaluate the safety of six state of the art LLMs. Our observations reveal significant safety gaps in LLMs particularly in categories harmful to children but not adults"
  },
  {
    "title": "Inference-Time Computations for LLM Reasoning and Planning: A Benchmark and Insights",
    "url": "http://arxiv.org/abs/2502.12521v1",
    "arxiv_id": "2502.12521v1",
    "authors": [
      "Shubham Parashar",
      "Blake Olson",
      "Sambhav Khurana",
      "Eric Li",
      "Hongyi Ling",
      "James Caverlee",
      "Shuiwang Ji"
    ],
    "published": "2025-02-18T04:11:29+00:00",
    "summary": "We examine the reasoning and planning capabilities of large language models (LLMs) in solving complex tasks. Recent advances in inference-time techniques demonstrate the potential to enhance LLM reasoning without additional training by exploring intermediate steps during inference. Notably, OpenAI's o1 model shows promising performance through its novel use of multi-step reasoning and verification. Here, we explore how scaling inference-time techniques can improve reasoning and planning, focusing on understanding the tradeoff between computational cost and performance. To this end, we construct a comprehensive benchmark, known as Sys2Bench, and perform extensive experiments evaluating existing inference-time techniques on eleven diverse tasks across five categories, including arithmetic reasoning, logical reasoning, common sense reasoning, algorithmic reasoning, and planning. Our findings indicate that simply scaling inference-time computation has limitations, as no single inference-time technique consistently performs well across all reasoning and planning tasks."
  },
  {
    "title": "SAFEERASER: Enhancing Safety in Multimodal Large Language Models through Multimodal Machine Unlearning",
    "url": "http://arxiv.org/abs/2502.12520v1",
    "arxiv_id": "2502.12520v1",
    "authors": [
      "Junkai Chen",
      "Zhijie Deng",
      "Kening Zheng",
      "Yibo Yan",
      "Shuliang Liu",
      "PeiJun Wu",
      "Peijie Jiang",
      "Jia Liu",
      "Xuming Hu"
    ],
    "published": "2025-02-18T04:09:46+00:00",
    "summary": "As Multimodal Large Language Models (MLLMs) develop, their potential security issues have become increasingly prominent. Machine Unlearning (MU), as an effective strategy for forgetting specific knowledge in training data, has been widely used in privacy protection. However, MU for safety in MLLM has yet to be fully explored. To address this issue, we propose SAFEERASER, a safety unlearning benchmark for MLLMs, consisting of 3,000 images and 28.8K VQA pairs. We comprehensively evaluate unlearning methods from two perspectives: forget quality and model utility. Our findings show that existing MU methods struggle to maintain model performance while implementing the forget operation and often suffer from over-forgetting. Hence, we introduce Prompt Decouple (PD) Loss to alleviate over-forgetting through decouple prompt during unlearning process. To quantitatively measure over-forgetting mitigated by PD Loss, we propose a new metric called Safe Answer Refusal Rate (SARR). Experimental results demonstrate that combining PD Loss with existing unlearning methods can effectively prevent over-forgetting and achieve a decrease of 79.5% in the SARR metric of LLaVA-7B and LLaVA-13B, while maintaining forget quality and model utility. Our code and dataset will be released upon acceptance. Warning: This paper contains examples of harmful language and images, and reader discretion is recommended."
  },
  {
    "title": "Local Flaw Detection with Adaptive Pyramid Image Fusion Across Spatial Sampling Resolution for SWRs",
    "url": "http://arxiv.org/abs/2502.12512v1",
    "arxiv_id": "2502.12512v1",
    "authors": [
      "Siyu You",
      "Huayi Gou",
      "Leilei Yang",
      "Zhiliang Liu",
      "Mingjian Zuo"
    ],
    "published": "2025-02-18T03:55:29+00:00",
    "summary": "The inspection of local flaws (LFs) in Steel Wire Ropes (SWRs) is crucial for ensuring safety and reliability in various industries. Magnetic Flux Leakage (MFL) imaging is commonly used for non-destructive testing, but its effectiveness is often hindered by the combined effects of inspection speed and sampling rate. To address this issue, the impacts of inspection speed and sampling rate on image quality are studied, as variations in these factors can cause stripe noise, axial compression of defect features, and increased interference, complicating accurate detection. We define the relationship between inspection speed and sampling rate as spatial sampling resolution (SSR) and propose an adaptive SSR target-feature-oriented (AS-TFO) method. This method incorporates adaptive adjustment and pyramid image fusion techniques to enhance defect detection under different SSR scenarios. Experimental results show that under high SSR scenarios, the method achieves a precision of 92.54% and recall of 98.41%. It remains robust under low SSR scenarios with a precision of 94.87% and recall of 97.37%. The overall results show that the proposed method outperforms conventional approaches, achieving state-of-the-art performance. This improvement in detection accuracy and robustness is particularly valuable for handling complex inspection conditions, where inspection speed and sampling rate can vary significantly, making detection more robust and reliable in industrial settings."
  },
  {
    "title": "Towards Robust and Secure Embodied AI: A Survey on Vulnerabilities and Attacks",
    "url": "http://arxiv.org/abs/2502.13175v1",
    "arxiv_id": "2502.13175v1",
    "authors": [
      "Wenpeng Xing",
      "Minghao Li",
      "Mohan Li",
      "Meng Han"
    ],
    "published": "2025-02-18T03:38:07+00:00",
    "summary": "Embodied AI systems, including robots and autonomous vehicles, are increasingly integrated into real-world applications, where they encounter a range of vulnerabilities stemming from both environmental and system-level factors. These vulnerabilities manifest through sensor spoofing, adversarial attacks, and failures in task and motion planning, posing significant challenges to robustness and safety. Despite the growing body of research, existing reviews rarely focus specifically on the unique safety and security challenges of embodied AI systems. Most prior work either addresses general AI vulnerabilities or focuses on isolated aspects, lacking a dedicated and unified framework tailored to embodied AI. This survey fills this critical gap by: (1) categorizing vulnerabilities specific to embodied AI into exogenous (e.g., physical attacks, cybersecurity threats) and endogenous (e.g., sensor failures, software flaws) origins; (2) systematically analyzing adversarial attack paradigms unique to embodied AI, with a focus on their impact on perception, decision-making, and embodied interaction; (3) investigating attack vectors targeting large vision-language models (LVLMs) and large language models (LLMs) within embodied systems, such as jailbreak attacks and instruction misinterpretation; (4) evaluating robustness challenges in algorithms for embodied perception, decision-making, and task planning; and (5) proposing targeted strategies to enhance the safety and reliability of embodied AI systems. By integrating these dimensions, we provide a comprehensive framework for understanding the interplay between vulnerabilities and safety in embodied AI."
  },
  {
    "title": "SoK: Understanding Vulnerabilities in the Large Language Model Supply Chain",
    "url": "http://arxiv.org/abs/2502.12497v1",
    "arxiv_id": "2502.12497v1",
    "authors": [
      "Shenao Wang",
      "Yanjie Zhao",
      "Zhao Liu",
      "Quanchen Zou",
      "Haoyu Wang"
    ],
    "published": "2025-02-18T03:22:38+00:00",
    "summary": "Large Language Models (LLMs) transform artificial intelligence, driving advancements in natural language understanding, text generation, and autonomous systems. The increasing complexity of their development and deployment introduces significant security challenges, particularly within the LLM supply chain. However, existing research primarily focuses on content safety, such as adversarial attacks, jailbreaking, and backdoor attacks, while overlooking security vulnerabilities in the underlying software systems. To address this gap, this study systematically analyzes 529 vulnerabilities reported across 75 prominent projects spanning 13 lifecycle stages. The findings show that vulnerabilities are concentrated in the application (50.3%) and model (42.7%) layers, with improper resource control (45.7%) and improper neutralization (25.1%) identified as the leading root causes. Additionally, while 56.7% of the vulnerabilities have available fixes, 8% of these patches are ineffective, resulting in recurring vulnerabilities. This study underscores the challenges of securing the LLM ecosystem and provides actionable insights to guide future research and mitigation strategies."
  },
  {
    "title": "Safe at the Margins: A General Approach to Safety Alignment in Low-Resource English Languages -- A Singlish Case Study",
    "url": "http://arxiv.org/abs/2502.12485v1",
    "arxiv_id": "2502.12485v1",
    "authors": [
      "Isaac Lim",
      "Shaun Khoo",
      "Watson Chua",
      "Goh Jiayi",
      "Jessica Foo"
    ],
    "published": "2025-02-18T03:11:06+00:00",
    "summary": "To ensure safe usage, Large Language Models (LLMs) typically undergo alignment with human-defined values. However, this alignment often relies on primarily English data and is biased towards Western-centric values, limiting its effectiveness in low-resource language settings. In this paper, we describe our approach for aligning SEA-Lion-v2.1-Instruct (a Llama3-8B variant) to minimize toxicity in Singlish, an English creole specific to Singapore. We find that supervised fine-tuning and Kahneman-Tversky Optimization (KTO) on paired and unpaired preferences is more sample efficient and yields significantly better results than Direct Preference Optimization (DPO). Our analysis reveals that DPO implicitly enforces a weaker safety objective than KTO, and that SFT complements KTO by improving training stability. Finally, we introduce a simple but novel modification to KTO, KTO-S, which improves training stability through better gradient exploitation. Overall, we present a general approach for safety alignment conducive to low-resource English languages, successfully reducing toxicity by 99\\% on our Singlish benchmark, with gains generalizing to the broader TOXIGEN dataset while maintaining strong performance across standard LLM benchmarks."
  },
  {
    "title": "MCTS-Judge: Test-Time Scaling in LLM-as-a-Judge for Code Correctness Evaluation",
    "url": "http://arxiv.org/abs/2502.12468v1",
    "arxiv_id": "2502.12468v1",
    "authors": [
      "Yutong Wang",
      "Pengliang Ji",
      "Chaoqun Yang",
      "Kaixin Li",
      "Ming Hu",
      "Jiaoyang Li",
      "Guillaume Sartoretti"
    ],
    "published": "2025-02-18T02:55:48+00:00",
    "summary": "The LLM-as-a-Judge paradigm shows promise for evaluating generative content but lacks reliability in reasoning-intensive scenarios, such as programming. Inspired by recent advances in reasoning models and shifts in scaling laws, we pioneer bringing test-time computation into LLM-as-a-Judge, proposing MCTS-Judge, a resource-efficient, System-2 thinking framework for code correctness evaluation. MCTS-Judge leverages Monte Carlo Tree Search (MCTS) to decompose problems into simpler, multi-perspective evaluations. Through a node-selection strategy that combines self-assessment based on historical actions in the current trajectory and the Upper Confidence Bound for Trees based on prior rollouts, MCTS-Judge balances global optimization and refinement of the current trajectory. We further designed a high-precision, unit-test-level reward mechanism to encourage the Large Language Model (LLM) to perform line-by-line analysis. Extensive experiments on three benchmarks and five LLMs demonstrate the effectiveness of MCTS-Judge, which improves the base model's accuracy from 41% to 80%, surpassing the o1-series models with 3x fewer tokens. Further evaluations validate the superiority of its reasoning trajectory in logic, analytics, thoroughness, and overall quality, while revealing the test-time scaling law of the LLM-as-a-Judge paradigm."
  },
  {
    "title": "EquiBench: Benchmarking Code Reasoning Capabilities of Large Language Models via Equivalence Checking",
    "url": "http://arxiv.org/abs/2502.12466v1",
    "arxiv_id": "2502.12466v1",
    "authors": [
      "Anjiang Wei",
      "Jiannan Cao",
      "Ran Li",
      "Hongyu Chen",
      "Yuhui Zhang",
      "Ziheng Wang",
      "Yaofeng Sun",
      "Yuan Liu",
      "Thiago S. F. X. Teixeira",
      "Diyi Yang",
      "Ke Wang",
      "Alex Aiken"
    ],
    "published": "2025-02-18T02:54:25+00:00",
    "summary": "Equivalence checking, i.e., determining whether two programs produce identical outputs for all possible inputs, underpins a broad range of applications, including software refactoring, testing, and optimization. We present the task of equivalence checking as a new way to evaluate the code reasoning abilities of large language models (LLMs). We introduce EquiBench, a dataset of 2400 program pairs spanning four programming languages and six equivalence categories. These pairs are systematically generated through program analysis, compiler scheduling, and superoptimization, covering nontrivial structural transformations that demand deep semantic reasoning beyond simple syntactic variations. Our evaluation of 17 state-of-the-art LLMs shows that OpenAI o3-mini achieves the highest overall accuracy of 78.0%. In the most challenging categories, the best accuracies are 62.3% and 68.8%, only modestly above the 50% random baseline for binary classification, indicating significant room for improvement in current models' code reasoning capabilities."
  },
  {
    "title": "SafeRoute: Adaptive Model Selection for Efficient and Accurate Safety Guardrails in Large Language Models",
    "url": "http://arxiv.org/abs/2502.12464v1",
    "arxiv_id": "2502.12464v1",
    "authors": [
      "Seanie Lee",
      "Dong Bok Lee",
      "Dominik Wagner",
      "Minki Kang",
      "Haebin Seong",
      "Tobias Bocklet",
      "Juho Lee",
      "Sung Ju Hwang"
    ],
    "published": "2025-02-18T02:51:17+00:00",
    "summary": "Deploying large language models (LLMs) in real-world applications requires robust safety guard models to detect and block harmful user prompts. While large safety guard models achieve strong performance, their computational cost is substantial. To mitigate this, smaller distilled models are used, but they often underperform on \"hard\" examples where the larger model provides accurate predictions. We observe that many inputs can be reliably handled by the smaller model, while only a small fraction require the larger model's capacity. Motivated by this, we propose SafeRoute, a binary router that distinguishes hard examples from easy ones. Our method selectively applies the larger safety guard model to the data that the router considers hard, improving efficiency while maintaining accuracy compared to solely using the larger safety guard model. Experimental results on multiple benchmark datasets demonstrate that our adaptive model selection significantly enhances the trade-off between computational cost and safety performance, outperforming relevant baselines."
  },
  {
    "title": "Computational Safety for Generative AI: A Signal Processing Perspective",
    "url": "http://arxiv.org/abs/2502.12445v1",
    "arxiv_id": "2502.12445v1",
    "authors": [
      "Pin-Yu Chen"
    ],
    "published": "2025-02-18T02:26:50+00:00",
    "summary": "AI safety is a rapidly growing area of research that seeks to prevent the harm and misuse of frontier AI technology, particularly with respect to generative AI (GenAI) tools that are capable of creating realistic and high-quality content through text prompts. Examples of such tools include large language models (LLMs) and text-to-image (T2I) diffusion models. As the performance of various leading GenAI models approaches saturation due to similar training data sources and neural network architecture designs, the development of reliable safety guardrails has become a key differentiator for responsibility and sustainability. This paper presents a formalization of the concept of computational safety, which is a mathematical framework that enables the quantitative assessment, formulation, and study of safety challenges in GenAI through the lens of signal processing theory and methods. In particular, we explore two exemplary categories of computational safety challenges in GenAI that can be formulated as hypothesis testing problems. For the safety of model input, we show how sensitivity analysis and loss landscape analysis can be used to detect malicious prompts with jailbreak attempts. For the safety of model output, we elucidate how statistical signal processing and adversarial learning can be used to detect AI-generated content. Finally, we discuss key open research challenges, opportunities, and the essential role of signal processing in computational AI safety."
  },
  {
    "title": "Gradient Co-occurrence Analysis for Detecting Unsafe Prompts in Large Language Models",
    "url": "http://arxiv.org/abs/2502.12411v1",
    "arxiv_id": "2502.12411v1",
    "authors": [
      "Jingyuan Yang",
      "Bowen Yan",
      "Rongjun Li",
      "Ziyu Zhou",
      "Xin Chen",
      "Zhiyong Feng",
      "Wei Peng"
    ],
    "published": "2025-02-18T01:14:46+00:00",
    "summary": "Unsafe prompts pose significant safety risks to large language models (LLMs). Existing methods for detecting unsafe prompts rely on data-driven fine-tuning to train guardrail models, necessitating significant data and computational resources. In contrast, recent few-shot gradient-based methods emerge, requiring only few safe and unsafe reference prompts. A gradient-based approach identifies unsafe prompts by analyzing consistent patterns of the gradients of safety-critical parameters in LLMs. Although effective, its restriction to directional similarity (cosine similarity) introduces ``directional bias'', limiting its capability to identify unsafe prompts. To overcome this limitation, we introduce GradCoo, a novel gradient co-occurrence analysis method that expands the scope of safety-critical parameter identification to include unsigned gradient similarity, thereby reducing the impact of ``directional bias'' and enhancing the accuracy of unsafe prompt detection. Comprehensive experiments on the widely-used benchmark datasets ToxicChat and XStest demonstrate that our proposed method can achieve state-of-the-art (SOTA) performance compared to existing methods. Moreover, we confirm the generalizability of GradCoo in detecting unsafe prompts across a range of LLM base models with various sizes and origins."
  },
  {
    "title": "Reward-Safety Balance in Offline Safe RL via Diffusion Regularization",
    "url": "http://arxiv.org/abs/2502.12391v1",
    "arxiv_id": "2502.12391v1",
    "authors": [
      "Junyu Guo",
      "Zhi Zheng",
      "Donghao Ying",
      "Ming Jin",
      "Shangding Gu",
      "Costas Spanos",
      "Javad Lavaei"
    ],
    "published": "2025-02-18T00:00:03+00:00",
    "summary": "Constrained reinforcement learning (RL) seeks high-performance policies under safety constraints. We focus on an offline setting where the agent has only a fixed dataset -- common in realistic tasks to prevent unsafe exploration. To address this, we propose Diffusion-Regularized Constrained Offline Reinforcement Learning (DRCORL), which first uses a diffusion model to capture the behavioral policy from offline data and then extracts a simplified policy to enable efficient inference. We further apply gradient manipulation for safety adaptation, balancing the reward objective and constraint satisfaction. This approach leverages high-quality offline data while incorporating safety requirements. Empirical results show that DRCORL achieves reliable safety performance, fast inference, and strong reward outcomes across robot learning tasks. Compared to existing safe offline RL methods, it consistently meets cost limits and performs well with the same hyperparameters, indicating practical applicability in real-world scenarios."
  },
  {
    "title": "Locally-Deployed Chain-of-Thought (CoT) Reasoning Model in Chemical Engineering: Starting from 30 Experimental Data",
    "url": "http://arxiv.org/abs/2502.12383v1",
    "arxiv_id": "2502.12383v1",
    "authors": [
      "Tianhang Zhou",
      "Yingchun Niu",
      "Xingying Lan",
      "Chunming Xu"
    ],
    "published": "2025-02-17T23:43:48+00:00",
    "summary": "In the field of chemical engineering, traditional data-processing and prediction methods face significant challenges. Machine-learning and large-language models (LLMs) also have their respective limitations. This paper explores the application of the Chain-of-Thought (CoT) reasoning model in chemical engineering, starting from 30 experimental data points. By integrating traditional surrogate models like Gaussian processes and random forests with powerful LLMs such as DeepSeek-R1, a hierarchical architecture is proposed. Two CoT-building methods, Large Language Model-Chain of Thought (LLM-CoT) and Machine Learning-Large Language Model-Chain of Thought (ML-LLM-CoT), are studied. The LLM-CoT combines local models DeepSeek-r1:14b and Qwen2:7b with Ollama. The ML-LLM-CoT integrates a pre-trained Gaussian ML model with the LLM-based CoT framework. Our results show that during construction, ML-LLM-CoT is more efficient. It only has 2 points that require rethink and a total of 4 rethink times, while LLM-CoT has 5 points that need to be re-thought and 34 total rethink times. In predicting the solubility of 20 molecules with dissimilar structures, the number of molecules with a prediction deviation higher than 100\\% for the Gaussian model, LLM-CoT, and ML-LLM-CoT is 7, 6, and 4 respectively. These results indicate that ML-LLM-CoT performs better in controlling the number of high-deviation molecules, optimizing the average deviation, and achieving a higher success rate in solubility judgment, providing a more reliable method for chemical engineering and molecular property prediction. This study breaks through the limitations of traditional methods and offers new solutions for rapid property prediction and process optimization in chemical engineering."
  },
  {
    "title": "Soft Robotics for Search and Rescue: Advancements, Challenges, and Future Directions",
    "url": "http://arxiv.org/abs/2502.12373v1",
    "arxiv_id": "2502.12373v1",
    "authors": [
      "Abhishek Sebastian"
    ],
    "published": "2025-02-17T23:24:18+00:00",
    "summary": "Soft robotics has emerged as a transformative technology in Search and Rescue (SAR) operations, addressing challenges in navigating complex, hazardous environments that often limit traditional rigid robots. This paper critically examines advancements in soft robotic technologies tailored for SAR applications, focusing on their unique capabilities in adaptability, safety, and efficiency. By leveraging bio-inspired designs, flexible materials, and advanced locomotion mechanisms, such as crawling, rolling, and shape morphing, soft robots demonstrate exceptional potential in disaster scenarios. However, significant barriers persist, including material durability, power inefficiency, sensor integration, and control complexity. This comprehensive review highlights the current state of soft robotics in SAR, discusses simulation methodologies and hardware validations, and introduces performance metrics essential for their evaluation. By bridging the gap between theoretical advancements and practical deployment, this study underscores the potential of soft robotic systems to revolutionize SAR missions and advocates for continued interdisciplinary innovation to overcome existing limitations."
  },
  {
    "title": "Detecting Systematic Weaknesses in Vision Models along Predefined Human-Understandable Dimensions",
    "url": "http://arxiv.org/abs/2502.12360v1",
    "arxiv_id": "2502.12360v1",
    "authors": [
      "Sujan Sai Gannamaneni",
      "Rohil Prakash Rao",
      "Michael Mock",
      "Maram Akila",
      "Stefan Wrobel"
    ],
    "published": "2025-02-17T22:50:45+00:00",
    "summary": "Studying systematic weaknesses of DNNs has gained prominence in the last few years with the rising focus on building safe AI systems. Slice discovery methods (SDMs) are prominent algorithmic approaches for finding such systematic weaknesses. They identify top-k semantically coherent slices/subsets of data where a DNN-under-test has low performance. For being directly useful, e.g., as evidences in a safety argumentation, slices should be aligned with human-understandable (safety-relevant) dimensions, which, for example, are defined by safety and domain experts as parts of the operational design domain (ODD). While straightforward for structured data, the lack of semantic metadata makes these investigations challenging for unstructured data. Therefore, we propose a complete workflow which combines contemporary foundation models with algorithms for combinatorial search that consider structured data and DNN errors for finding systematic weaknesses in images. In contrast to existing approaches, ours identifies weak slices that are in line with predefined human-understandable dimensions. As the workflow includes foundation models, its intermediate and final results may not always be exact. Therefore, we build into our workflow an approach to address the impact of noisy metadata. We evaluate our approach w.r.t. its quality on four popular computer vision datasets, including autonomous driving datasets like Cityscapes, BDD100k, and RailSem19, while using multiple state-of-the-art models as DNNs-under-test."
  },
  {
    "title": "Asymptotic safety, quantum gravity, and the swampland: a conceptual assessment",
    "url": "http://arxiv.org/abs/2502.12290v1",
    "arxiv_id": "2502.12290v1",
    "authors": [
      "Ivano Basile",
      "Benjamin Knorr",
      "Alessia Platania",
      "Marc Schiffer"
    ],
    "published": "2025-02-17T20:00:06+00:00",
    "summary": "We provide a conceptual assessment of some aspects of fundamental quantum field theories of gravity in light of foundational aspects of the swampland program. On the one hand, asymptotically safe quantum gravity may provide a simple and predictive framework, thanks to a finite number of relevant parameters. On the other hand, a (sub-)set of intertwined swampland conjectures on the consistency of quantum gravity can be argued to be universal via effective field theory considerations. We answer whether some foundational features of these frameworks are compatible. This involves revisiting and refining several arguments (and loopholes) concerning the relation between field-theoretic descriptions of gravity and general swampland ideas. We identify the thermodynamics of black holes, spacetime topology change, and holography as the core aspects of this relation. We draw lessons on the features that a field theoretic description of gravity must (not) have to be consistent with fundamental principles underlying the swampland program, and on the universality of the latter."
  },
  {
    "title": "Integrating Expert Knowledge into Logical Programs via LLMs",
    "url": "http://arxiv.org/abs/2502.12275v1",
    "arxiv_id": "2502.12275v1",
    "authors": [
      "Franciszek G\u00f3rski",
      "Oskar Wysocki",
      "Marco Valentino",
      "Andre Freitas"
    ],
    "published": "2025-02-17T19:18:23+00:00",
    "summary": "This paper introduces ExKLoP, a novel framework designed to evaluate how effectively Large Language Models (LLMs) integrate expert knowledge into logical reasoning systems. This capability is especially valuable in engineering, where expert knowledge-such as manufacturer-recommended operational ranges-can be directly embedded into automated monitoring systems. By mirroring expert verification steps, tasks like range checking and constraint validation help ensure system safety and reliability. Our approach systematically evaluates LLM-generated logical rules, assessing both syntactic fluency and logical correctness in these critical validation tasks. We also explore the models capacity for self-correction via an iterative feedback loop based on code execution outcomes. ExKLoP presents an extensible dataset comprising 130 engineering premises, 950 prompts, and corresponding validation points. It enables comprehensive benchmarking while allowing control over task complexity and scalability of experiments. We leverage the synthetic data creation methodology to conduct extensive empirical evaluation on a diverse set of LLMs including Llama3, Gemma, Mixtral, Mistral, and Qwen. Results reveal that while models generate nearly perfect syntactically correct code, they frequently exhibit logical errors in translating expert knowledge. Furthermore, iterative self-correction yields only marginal improvements (up to 3%). Overall, ExKLoP serves as a robust evaluation platform that streamlines the selection of effective models for self-correcting systems while clearly delineating the types of errors encountered. The complete implementation, along with all relevant data, is available at GitHub."
  },
  {
    "title": "NeuroStrata: Harnessing Neurosymbolic Paradigms for Improved Design, Testability, and Verifiability of Autonomous CPS",
    "url": "http://arxiv.org/abs/2502.12267v1",
    "arxiv_id": "2502.12267v1",
    "authors": [
      "Xi Zheng",
      "Ziyang Li",
      "Ivan Ruchkin",
      "Ruzica Piskac",
      "Miroslav Pajic"
    ],
    "published": "2025-02-17T19:07:41+00:00",
    "summary": "Autonomous cyber-physical systems (CPSs) leverage AI for perception, planning, and control but face trust and safety certification challenges due to inherent uncertainties. The neurosymbolic paradigm replaces stochastic layers with interpretable symbolic AI, enabling determinism. While promising, challenges like multisensor fusion, adaptability, and verification remain. This paper introduces NeuroStrata, a neurosymbolic framework to enhance the testing and verification of autonomous CPS. We outline its key components, present early results, and detail future plans."
  },
  {
    "title": "SmokeNet: Efficient Smoke Segmentation Leveraging Multiscale Convolutions and Multiview Attention Mechanisms",
    "url": "http://arxiv.org/abs/2502.12258v1",
    "arxiv_id": "2502.12258v1",
    "authors": [
      "Xuesong Liu",
      "Emmett J. Ientilucci"
    ],
    "published": "2025-02-17T19:01:27+00:00",
    "summary": "Efficient segmentation of smoke plumes is crucial for environmental monitoring and industrial safety, enabling the detection and mitigation of harmful emissions from activities like quarry blasts and wildfires. Accurate segmentation facilitates environmental impact assessments, timely interventions, and compliance with safety standards. However, existing models often face high computational demands and limited adaptability to diverse smoke appearances, restricting their deployment in resource-constrained environments. To address these issues, we introduce SmokeNet, a novel deep learning architecture that leverages multiscale convolutions and multiview linear attention mechanisms combined with layer-specific loss functions to handle the complex dynamics of diverse smoke plumes, ensuring efficient and accurate segmentation across varied environments. Additionally, we evaluate SmokeNet's performance and versatility using four datasets, including our quarry blast smoke dataset made available to the community. The results demonstrate that SmokeNet maintains a favorable balance between computational efficiency and segmentation accuracy, making it suitable for deployment in environmental monitoring and safety management systems. By contributing a new dataset and offering an efficient segmentation model, SmokeNet advances smoke segmentation capabilities in diverse and challenging environments."
  },
  {
    "title": "Can LLMs Simulate Social Media Engagement? A Study on Action-Guided Response Generation",
    "url": "http://arxiv.org/abs/2502.12073v1",
    "arxiv_id": "2502.12073v1",
    "authors": [
      "Zhongyi Qiu",
      "Hanjia Lyu",
      "Wei Xiong",
      "Jiebo Luo"
    ],
    "published": "2025-02-17T17:43:08+00:00",
    "summary": "Social media enables dynamic user engagement with trending topics, and recent research has explored the potential of large language models (LLMs) for response generation. While some studies investigate LLMs as agents for simulating user behavior on social media, their focus remains on practical viability and scalability rather than a deeper understanding of how well LLM aligns with human behavior. This paper analyzes LLMs' ability to simulate social media engagement through action guided response generation, where a model first predicts a user's most likely engagement action-retweet, quote, or rewrite-towards a trending post before generating a personalized response conditioned on the predicted action. We benchmark GPT-4o-mini, O1-mini, and DeepSeek-R1 in social media engagement simulation regarding a major societal event discussed on X. Our findings reveal that zero-shot LLMs underperform BERT in action prediction, while few-shot prompting initially degrades the prediction accuracy of LLMs with limited examples. However, in response generation, few-shot LLMs achieve stronger semantic alignment with ground truth posts."
  },
  {
    "title": "TokenSkip: Controllable Chain-of-Thought Compression in LLMs",
    "url": "http://arxiv.org/abs/2502.12067v1",
    "arxiv_id": "2502.12067v1",
    "authors": [
      "Heming Xia",
      "Yongqi Li",
      "Chak Tou Leong",
      "Wenjie Wang",
      "Wenjie Li"
    ],
    "published": "2025-02-17T17:37:26+00:00",
    "summary": "Chain-of-Thought (CoT) has been proven effective in enhancing the reasoning capabilities of large language models (LLMs). Recent advancements, such as OpenAI's o1 and DeepSeek-R1, suggest that scaling up the length of CoT sequences during inference could further boost LLM reasoning performance. However, due to the autoregressive nature of LLM decoding, longer CoT outputs lead to a linear increase in inference latency, adversely affecting user experience, particularly when the CoT exceeds 10,000 tokens. To address this limitation, we analyze the semantic importance of tokens within CoT outputs and reveal that their contributions to reasoning vary. Building on this insight, we propose TokenSkip, a simple yet effective approach that enables LLMs to selectively skip less important tokens, allowing for controllable CoT compression. Extensive experiments across various models and tasks demonstrate the effectiveness of TokenSkip in reducing CoT token usage while preserving strong reasoning performance. Notably, when applied to Qwen2.5-14B-Instruct, TokenSkip reduces reasoning tokens by 40% (from 313 to 181) on GSM8K, with less than a 0.4% performance drop."
  },
  {
    "title": "PhysReason: A Comprehensive Benchmark towards Physics-Based Reasoning",
    "url": "http://arxiv.org/abs/2502.12054v1",
    "arxiv_id": "2502.12054v1",
    "authors": [
      "Xinyu Zhang",
      "Yuxuan Dong",
      "Yanrui Wu",
      "Jiaxing Huang",
      "Chengyou Jia",
      "Basura Fernando",
      "Mike Zheng Shou",
      "Lingling Zhang",
      "Jun Liu"
    ],
    "published": "2025-02-17T17:24:14+00:00",
    "summary": "Large language models demonstrate remarkable capabilities across various domains, especially mathematics and logic reasoning. However, current evaluations overlook physics-based reasoning - a complex task requiring physics theorems and constraints. We present PhysReason, a 1,200-problem benchmark comprising knowledge-based (25%) and reasoning-based (75%) problems, where the latter are divided into three difficulty levels (easy, medium, hard). Notably, problems require an average of 8.1 solution steps, with hard requiring 15.6, reflecting the complexity of physics-based reasoning. We propose the Physics Solution Auto Scoring Framework, incorporating efficient answer-level and comprehensive step-level evaluations. Top-performing models like Deepseek-R1, Gemini-2.0-Flash-Thinking, and o3-mini-high achieve less than 60% on answer-level evaluation, with performance dropping from knowledge questions (75.11%) to hard problems (31.95%). Through step-level evaluation, we identified four key bottlenecks: Physics Theorem Application, Physics Process Understanding, Calculation, and Physics Condition Analysis. These findings position PhysReason as a novel and comprehensive benchmark for evaluating physics-based reasoning capabilities in large language models. Our code and data will be published at https:/dxzxy12138.github.io/PhysReason."
  },
  {
    "title": "SafeChain: Safety of Language Models with Long Chain-of-Thought Reasoning Capabilities",
    "url": "http://arxiv.org/abs/2502.12025v1",
    "arxiv_id": "2502.12025v1",
    "authors": [
      "Fengqing Jiang",
      "Zhangchen Xu",
      "Yuetai Li",
      "Luyao Niu",
      "Zhen Xiang",
      "Bo Li",
      "Bill Yuchen Lin",
      "Radha Poovendran"
    ],
    "published": "2025-02-17T16:57:56+00:00",
    "summary": "Emerging large reasoning models (LRMs), such as DeepSeek-R1 models, leverage long chain-of-thought (CoT) reasoning to generate structured intermediate steps, enhancing their reasoning capabilities. However, long CoT does not inherently guarantee safe outputs, potentially leading to harmful consequences such as the introduction of security vulnerabilities in code or the spread of misinformation. Current research on large language model (LLM) safety usually focuses on short-answer responses, overlooking the long CoT style outputs of LRMs. To bridge this gap, we conduct a systematic study of LRM safety. First, we investigate safety evaluators calibrated against human annotations. Using our newly developed metrics, we thoroughly assess the safety of 12 state-of-the-art LRMs on StrongReject and WildJailbreak datasets. Our results show that LRMs are not safe compared to their reasoning advance. Further, we perform a fine-grained analysis of the reasoning trace and final answer. We find that three decoding strategies-ZeroThink, LessThink, and MoreThink-can improve model safety without additional training. However, these strategies either use constrained reasoning traces or incur high inference costs. To better strengthen LRM safety, we introduce SafeChain, the first-of-its-kind safety training dataset in CoT style. We fine-tune two LRMs with SafeChain, showing that it not only enhances model safety but also preserves performance across 6 reasoning benchmarks."
  },
  {
    "title": "Atom of Thoughts for Markov LLM Test-Time Scaling",
    "url": "http://arxiv.org/abs/2502.12018v1",
    "arxiv_id": "2502.12018v1",
    "authors": [
      "Fengwei Teng",
      "Zhaoyang Yu",
      "Quan Shi",
      "Jiayi Zhang",
      "Chenglin Wu",
      "Yuyu Luo"
    ],
    "published": "2025-02-17T16:52:42+00:00",
    "summary": "Large Language Models (LLMs) achieve superior performance through training-time scaling, and test-time scaling further enhances their capabilities by conducting effective reasoning during inference. However, as the scale of reasoning increases, existing test-time scaling methods suffer from accumulated historical information, which not only wastes computational resources but also interferes with effective reasoning. To address this issue, we observe that complex reasoning progress is often achieved by solving a sequence of independent subquestions, each being self-contained and verifiable. These subquestions are essentially atomic questions, relying primarily on their current state rather than accumulated history, similar to the memoryless transitions in a Markov process. Based on this observation, we propose Atom of Thoughts (AoT), where each state transition in the reasoning process consists of decomposing the current question into a dependency-based directed acyclic graph and contracting its subquestions, forming a new atomic question state. This iterative decomposition-contraction process continues until reaching directly solvable atomic questions, naturally realizing Markov transitions between question states. Furthermore, these atomic questions can be seamlessly integrated into existing test-time scaling methods, enabling AoT to serve as a plug-in enhancement for improving reasoning capabilities. Experiments across six benchmarks demonstrate the effectiveness of AoT both as a standalone framework and a plug-in enhancement. Notably, on HotpotQA, when applied to gpt-4o-mini, AoT achieves an 80.6% F1 score, surpassing o3-mini by 3.4% and DeepSeek-R1 by 10.6%. The code will be available at https://github.com/qixucen/atom."
  },
  {
    "title": "LIMR: Less is More for RL Scaling",
    "url": "http://arxiv.org/abs/2502.11886v1",
    "arxiv_id": "2502.11886v1",
    "authors": [
      "Xuefeng Li",
      "Haoyang Zou",
      "Pengfei Liu"
    ],
    "published": "2025-02-17T15:13:29+00:00",
    "summary": "In this paper, we ask: what truly determines the effectiveness of RL training data for enhancing language models' reasoning capabilities? While recent advances like o1, Deepseek R1, and Kimi1.5 demonstrate RL's potential, the lack of transparency about training data requirements has hindered systematic progress. Starting directly from base models without distillation, we challenge the assumption that scaling up RL training data inherently improves performance. we demonstrate that a strategically selected subset of just 1,389 samples can outperform the full 8,523-sample dataset. We introduce Learning Impact Measurement (LIM), an automated method to evaluate and prioritize training samples based on their alignment with model learning trajectories, enabling efficient resource utilization and scalable implementation. Our method achieves comparable or even superior performance using only 1,389 samples versus the full 8,523 samples dataset. Notably, while recent data-efficient approaches (e.g., LIMO and s1) show promise with 32B-scale models, we find it significantly underperforms at 7B-scale through supervised fine-tuning (SFT). In contrast, our RL-based LIMR achieves 16.7% higher accuracy on AIME24 and outperforms LIMO and s1 by 13.0% and 22.2% on MATH500. These results fundamentally reshape our understanding of RL scaling in LLMs, demonstrating that precise sample selection, rather than data scale, may be the key to unlocking enhanced reasoning capabilities. For reproducible research and future innovation, we are open-sourcing LIMR, including implementation of LIM, training and evaluation code, curated datasets, and trained models at https://github.com/GAIR-NLP/LIMR."
  },
  {
    "title": "Hypothesis-Driven Theory-of-Mind Reasoning for Large Language Models",
    "url": "http://arxiv.org/abs/2502.11881v1",
    "arxiv_id": "2502.11881v1",
    "authors": [
      "Hyunwoo Kim",
      "Melanie Sclar",
      "Tan Zhi-Xuan",
      "Lance Ying",
      "Sydney Levine",
      "Yang Liu",
      "Joshua B. Tenenbaum",
      "Yejin Choi"
    ],
    "published": "2025-02-17T15:08:50+00:00",
    "summary": "Existing LLM reasoning methods have shown impressive capabilities across various tasks, such as solving math and coding problems. However, applying these methods to scenarios without ground-truth answers or rule-based verification methods - such as tracking the mental states of an agent - remains challenging. Inspired by the sequential Monte Carlo algorithm, we introduce thought-tracing, an inference-time reasoning algorithm designed to trace the mental states of specific agents by generating hypotheses and weighting them based on observations without relying on ground-truth solutions to questions in datasets. Our algorithm is modeled after the Bayesian theory-of-mind framework, using LLMs to approximate probabilistic inference over agents' evolving mental states based on their perceptions and actions. We evaluate thought-tracing on diverse theory-of-mind benchmarks, demonstrating significant performance improvements compared to baseline LLMs. Our experiments also reveal interesting behaviors of the recent reasoning models - e.g., o1 and R1 - on theory-of-mind, highlighting the difference of social reasoning compared to other domains."
  },
  {
    "title": "StructTransform: A Scalable Attack Surface for Safety-Aligned Large Language Models",
    "url": "http://arxiv.org/abs/2502.11853v1",
    "arxiv_id": "2502.11853v1",
    "authors": [
      "Shehel Yoosuf",
      "Temoor Ali",
      "Ahmed Lekssays",
      "Mashael AlSabah",
      "Issa Khalil"
    ],
    "published": "2025-02-17T14:46:38+00:00",
    "summary": "In this work, we present a series of structure transformation attacks on LLM alignment, where we encode natural language intent using diverse syntax spaces, ranging from simple structure formats and basic query languages (e.g. SQL) to new novel spaces and syntaxes created entirely by LLMs. Our extensive evaluation shows that our simplest attacks can achieve close to 90% success rate, even on strict LLMs (such as Claude 3.5 Sonnet) using SOTA alignment mechanisms. We improve the attack performance further by using an adaptive scheme that combines structure transformations along with existing \\textit{content transformations}, resulting in over 96% ASR with 0% refusals.   To generalize our attacks, we explore numerous structure formats, including syntaxes purely generated by LLMs. Our results indicate that such novel syntaxes are easy to generate and result in a high ASR, suggesting that defending against our attacks is not a straightforward process. Finally, we develop a benchmark and evaluate existing safety-alignment defenses against it, showing that most of them fail with 100% ASR. Our results show that existing safety alignment mostly relies on token-level patterns without recognizing harmful concepts, highlighting and motivating the need for serious research efforts in this direction. As a case study, we demonstrate how attackers can use our attack to easily generate a sample malware, and a corpus of fraudulent SMS messages, which perform well in bypassing detection."
  },
  {
    "title": "BaxBench: Can LLMs Generate Correct and Secure Backends?",
    "url": "http://arxiv.org/abs/2502.11844v1",
    "arxiv_id": "2502.11844v1",
    "authors": [
      "Mark Vero",
      "Niels M\u00fcndler",
      "Victor Chibotaru",
      "Veselin Raychev",
      "Maximilian Baader",
      "Nikola Jovanovi\u0107",
      "Jingxuan He",
      "Martin Vechev"
    ],
    "published": "2025-02-17T14:37:47+00:00",
    "summary": "The automatic generation of programs has long been a fundamental challenge in computer science. Recent benchmarks have shown that large language models (LLMs) can effectively generate code at the function level, make code edits, and solve algorithmic coding tasks. However, to achieve full automation, LLMs should be able to generate production-quality, self-contained application modules. To evaluate the capabilities of LLMs in solving this challenge, we introduce BaxBench, a novel evaluation benchmark consisting of 392 tasks for the generation of backend applications. We focus on backends for three critical reasons: (i) they are practically relevant, building the core components of most modern web and cloud software, (ii) they are difficult to get right, requiring multiple functions and files to achieve the desired functionality, and (iii) they are security-critical, as they are exposed to untrusted third-parties, making secure solutions that prevent deployment-time attacks an imperative. BaxBench validates the functionality of the generated applications with comprehensive test cases, and assesses their security exposure by executing end-to-end exploits. Our experiments reveal key limitations of current LLMs in both functionality and security: (i) even the best model, OpenAI o1, achieves a mere 60% on code correctness; (ii) on average, we could successfully execute security exploits on more than half of the correct programs generated by each LLM; and (iii) in less popular backend frameworks, models further struggle to generate correct and secure applications. Progress on BaxBench signifies important steps towards autonomous and secure software development with LLMs."
  },
  {
    "title": "video-SALMONN-o1: Reasoning-enhanced Audio-visual Large Language Model",
    "url": "http://arxiv.org/abs/2502.11775v1",
    "arxiv_id": "2502.11775v1",
    "authors": [
      "Guangzhi Sun",
      "Yudong Yang",
      "Jimin Zhuang",
      "Changli Tang",
      "Yixuan Li",
      "Wei Li",
      "Zejun MA",
      "Chao Zhang"
    ],
    "published": "2025-02-17T13:07:40+00:00",
    "summary": "While recent advancements in reasoning optimization have significantly enhanced the capabilities of large language models (LLMs), existing efforts to improve reasoning have been limited to solving mathematical problems and focusing on visual graphical inputs, neglecting broader applications in general video understanding.This paper proposes video-SALMONN-o1, the first open-source reasoning-enhanced audio-visual LLM designed for general video understanding tasks. To enhance its reasoning abilities, we develop a reasoning-intensive dataset featuring challenging audio-visual questions with step-by-step solutions. We also propose process direct preference optimization (pDPO), which leverages contrastive step selection to achieve efficient step-level reward modelling tailored for multimodal inputs. Additionally, we introduce RivaBench, the first reasoning-intensive video understanding benchmark, featuring over 4,000 high-quality, expert-curated question-answer pairs across scenarios such as standup comedy, academic presentations, and synthetic video detection. video-SALMONN-o1 achieves 3-8% accuracy improvements over the LLaVA-OneVision baseline across different video reasoning benchmarks. Besides, pDPO achieves 6-8% improvements compared to the supervised fine-tuning model on RivaBench. Enhanced reasoning enables video-SALMONN-o1 zero-shot synthetic video detection capabilities."
  },
  {
    "title": "SQL-o1: A Self-Reward Heuristic Dynamic Search Method for Text-to-SQL",
    "url": "http://arxiv.org/abs/2502.11741v1",
    "arxiv_id": "2502.11741v1",
    "authors": [
      "Shuai Lyu",
      "Haoran Luo",
      "Zhonghong Ou",
      "Yifan Zhu",
      "Xiaoran Shang",
      "Yang Qin",
      "Meina Song"
    ],
    "published": "2025-02-17T12:28:11+00:00",
    "summary": "The Text-to-SQL(Text2SQL) task aims to convert natural language queries into executable SQL queries. Thanks to the application of large language models (LLMs), significant progress has been made in this field. However, challenges such as model scalability, limited generation space, and coherence issues in SQL generation still persist. To address these issues, we propose SQL-o1, a Self-Reward-based heuristic search method designed to enhance the reasoning ability of LLMs in SQL query generation. SQL-o1 combines Monte Carlo Tree Search (MCTS) for heuristic process-level search and constructs a Schema-Aware dataset to help the model better understand database schemas. Extensive experiments on the Bird and Spider datasets demonstrate that SQL-o1 improves execution accuracy by 10.8\\% on the complex Bird dataset compared to the latest baseline methods, even outperforming GPT-4-based approaches. Additionally, SQL-o1 excels in few-shot learning scenarios and shows strong cross-model transferability. Our code is publicly available at:https://github.com/ShuaiLyu0110/SQL-o1."
  },
  {
    "title": "Connecting Earth and Moon via the L1 Lagrangian point",
    "url": "http://arxiv.org/abs/2502.11694v1",
    "arxiv_id": "2502.11694v1",
    "authors": [
      "A. K. de Almeida Jr",
      "V. M. de Oliveira",
      "T. Vaillant",
      "D. Maia",
      "A. C. M. Correia",
      "D. Barbosa",
      "L. T. B. Santos"
    ],
    "published": "2025-02-17T11:32:02+00:00",
    "summary": "The renewed global interest in lunar exploration requires new orbital strategies to ensure flight safety which can benefit extended lunar missions and service a plethora of planned instruments in the lunar orbit and surface. We investigate here the equivalent fuel consumption cost to transfer from (to) a given orbit and enter (leave) at any point of an invariant manifold associated with a Lyapunov orbit around the Earth-Moon $L_1$ Lagrangian point using bi-impulsive maneuvers. Whereas solving this type of transfer is generally computationally expensive, we simulate here tens of millions of transfers orbits, for different times of flight, Jacobi constants and spatial location on the manifold. We are able to reduce computational cost by taking advantage of the efficient procedure given by the Theory of Functional Connections for solving boundary value problems, represented with special constraints created to the purposes of this work. We develop here the methodology for constructing these transfers, and apply it to find a low-cost transfer from an orbit around the Earth to a stable manifold and another low-cost transfer from an unstable manifold to an orbit around the Moon. In the end, we obtain an innovative Earth-to-Moon transfer that involves a gravity assist maneuver with the Moon and allows a long stationed stage at the Lyapunov orbit around $L_1$ which can be used for designing multi-purpose missions for extended periods of time with low fuel costs. This is paramount to optimize new exploration concepts."
  },
  {
    "title": "RIDE: Enhancing Large Language Model Alignment through Restyled In-Context Learning Demonstration Exemplars",
    "url": "http://arxiv.org/abs/2502.11681v1",
    "arxiv_id": "2502.11681v1",
    "authors": [
      "Yuncheng Hua",
      "Lizhen Qu",
      "Zhuang Li",
      "Hao Xue",
      "Flora D. Salim",
      "Gholamreza Haffari"
    ],
    "published": "2025-02-17T11:16:19+00:00",
    "summary": "Alignment tuning is crucial for ensuring large language models (LLMs) behave ethically and helpfully. Current alignment approaches require high-quality annotations and significant training resources. This paper proposes a low-cost, tuning-free method using in-context learning (ICL) to enhance LLM alignment. Through an analysis of high-quality ICL demos, we identified style as a key factor influencing LLM alignment capabilities and explicitly restyled ICL exemplars based on this stylistic framework. Additionally, we combined the restyled demos to achieve a balance between the two conflicting aspects of LLM alignment--factuality and safety. We packaged the restyled examples as prompts to trigger few-shot learning, improving LLM alignment. Compared to the best baseline approach, with an average score of 5.00 as the maximum, our method achieves a maximum 0.10 increase on the Alpaca task (from 4.50 to 4.60), a 0.22 enhancement on the Just-eval benchmark (from 4.34 to 4.56), and a maximum improvement of 0.32 (from 3.53 to 3.85) on the MT-Bench dataset. We release the code and data at https://github.com/AnonymousCode-ComputerScience/RIDE."
  },
  {
    "title": "DELMAN: Dynamic Defense Against Large Language Model Jailbreaking with Model Editing",
    "url": "http://arxiv.org/abs/2502.11647v1",
    "arxiv_id": "2502.11647v1",
    "authors": [
      "Yi Wang",
      "Fenghua Weng",
      "Sibei Yang",
      "Zhan Qin",
      "Minlie Huang",
      "Wenjie Wang"
    ],
    "published": "2025-02-17T10:39:21+00:00",
    "summary": "Large Language Models (LLMs) are widely applied in decision making, but their deployment is threatened by jailbreak attacks, where adversarial users manipulate model behavior to bypass safety measures. Existing defense mechanisms, such as safety fine-tuning and model editing, either require extensive parameter modifications or lack precision, leading to performance degradation on general tasks, which is unsuitable to post-deployment safety alignment. To address these challenges, we propose DELMAN (Dynamic Editing for LLMs JAilbreak DefeNse), a novel approach leveraging direct model editing for precise, dynamic protection against jailbreak attacks. DELMAN directly updates a minimal set of relevant parameters to neutralize harmful behaviors while preserving the model's utility. To avoid triggering a safe response in benign context, we incorporate KL-divergence regularization to ensure the updated model remains consistent with the original model when processing benign queries. Experimental results demonstrate that DELMAN outperforms baseline methods in mitigating jailbreak attacks while preserving the model's utility, and adapts seamlessly to new attack instances, providing a practical and efficient solution for post-deployment model protection."
  },
  {
    "title": "Enhancing Out-of-Distribution Detection in Medical Imaging with Normalizing Flows",
    "url": "http://arxiv.org/abs/2502.11638v1",
    "arxiv_id": "2502.11638v1",
    "authors": [
      "Dariush Lotfi",
      "Mohammad-Ali Nikouei Mahani",
      "Mohamad Koohi-Moghadam",
      "Kyongtae Ty Bae"
    ],
    "published": "2025-02-17T10:31:24+00:00",
    "summary": "Out-of-distribution (OOD) detection is crucial in AI-driven medical imaging to ensure reliability and safety by identifying inputs outside a model's training distribution. Existing methods often require retraining or modifications to pre-trained models, which is impractical for clinical applications. This study introduces a post-hoc normalizing flow-based approach that seamlessly integrates with pre-trained models. By leveraging normalizing flows, it estimates the likelihood of feature vectors extracted from pre-trained models, capturing semantically meaningful representations without relying on pixel-level statistics. The method was evaluated using the MedMNIST benchmark and a newly curated MedOOD dataset simulating clinically relevant distributional shifts. Performance was measured using standard OOD detection metrics (e.g., AUROC, FPR@95, AUPR_IN, AUPR_OUT), with statistical analyses comparing it against ten baseline methods. On MedMNIST, the proposed model achieved an AUROC of 93.80%, outperforming state-of-the-art methods. On MedOOD, it achieved an AUROC of 84.61%, demonstrating superior performance against other methods. Its post-hoc nature ensures compatibility with existing clinical workflows, addressing the limitations of previous approaches. The model and code to build OOD datasets are available at https://github.com/dlotfi/MedOODFlow."
  },
  {
    "title": "GraphThought: Graph Combinatorial Optimization with Thought Generation",
    "url": "http://arxiv.org/abs/2502.11607v1",
    "arxiv_id": "2502.11607v1",
    "authors": [
      "Zixiao Huang",
      "Lifeng Guo",
      "Junjie Sheng",
      "Haosheng Chen",
      "Wenhao Li",
      "Bo Jin",
      "Changhong Lu",
      "Xiangfeng Wang"
    ],
    "published": "2025-02-17T09:50:41+00:00",
    "summary": "Large language models (LLMs) have demonstrated remarkable capabilities across various domains, especially in text processing and generative tasks. Recent advancements in the reasoning capabilities of state-of-the-art LLMs, such as OpenAI-o1, have significantly broadened their applicability, particularly in complex problem-solving and logical inference. However, most existing LLMs struggle with notable limitations in handling graph combinatorial optimization (GCO) problems. To bridge this gap, we formally define the Optimal Thoughts Design (OTD) problem, including its state and action thought space. We then introduce a novel framework, GraphThought, designed to generate high-quality thought datasets for GCO problems. Leveraging these datasets, we fine-tune the Llama-3-8B-Instruct model to develop Llama-GT. Notably, despite its compact 8B-parameter architecture, Llama-GT matches the performance of state-of-the-art LLMs on the GraphArena benchmark. Experimental results show that our approach outperforms both proprietary and open-source models, even rivaling specialized models like o1-mini. This work sets a new state-of-the-art benchmark while challenging the prevailing notion that model scale is the primary driver of reasoning capability."
  },
  {
    "title": "Runtime Enforcement of CPS against Signal Temporal Logic",
    "url": "http://arxiv.org/abs/2502.11584v1",
    "arxiv_id": "2502.11584v1",
    "authors": [
      "Han Su",
      "Saumya Shankar",
      "Srinivas Pinisetty",
      "Partha S. Roop",
      "Naijun Zhan"
    ],
    "published": "2025-02-17T09:16:58+00:00",
    "summary": "Cyber-Physical Systems (CPSs), especially those involving autonomy, need guarantees of their safety. Runtime Enforcement (RE) is a lightweight method to formally ensure that some specified properties are satisfied over the executions of the system. Hence, there is recent interest in the RE of CPS. However, existing methods are not designed to tackle specifications suitable for the hybrid dynamics of CPS. With this in mind, we develop runtime enforcement of CPS using properties defined in Signal Temporal Logic (STL).   In this work, we aim to construct a runtime enforcer for a given STL formula to minimally modify a signal to satisfy the formula. To achieve this, the STL formula to be enforced is first translated into a timed transducer, while the signal to be corrected is encoded as timed words. We provide timed transducers for the temporal operators \\emph{until} and \\emph{release} noting that other temporal operators can be expressed using these two. Our approach enables effective enforcement of STL properties for CPS. A case study is provided to illustrate the approach and generate empirical evidence of its suitability for CPS."
  },
  {
    "title": "Language Complexity Measurement as a Noisy Zero-Shot Proxy for Evaluating LLM Performance",
    "url": "http://arxiv.org/abs/2502.11578v1",
    "arxiv_id": "2502.11578v1",
    "authors": [
      "Birger Moell",
      "Johan Boye"
    ],
    "published": "2025-02-17T09:09:58+00:00",
    "summary": "Large Language Models (LLMs) have made significant strides in natural language generation but often face challenges in tasks requiring precise calculations and structural analysis. This paper investigates the performance of state-of-the-art LLMs on language complexity measurement tasks, through the computation of the LIX readability metric and Average Dependency Distance (ADD). Using Swedish high school and university-level essays, we evaluate the models' abilities to compute LIX scores and perform dependency parsing, comparing their results to established ground truths. Our findings reveal that while all models demonstrate some capacity for these tasks, ChatGPT-o1-mini performs most consistently, achieving the highest accuracy in both LIX computation and dependency parsing. Additionally, we observe a strong significant correlation -0.875 p 0.026 (N=6) between the models' accuracy in computing LIX and their overall performance on the Massive Multitask Language Understanding (MMLU) benchmark. These results suggest that language complexity measurement abilities can serve as a noisy zero-shot proxies for assessing the general capabilities of LLMs, providing a practical method for model evaluation without the need for extensive benchmarking datasets."
  },
  {
    "title": "Large Language Models and Mathematical Reasoning Failures",
    "url": "http://arxiv.org/abs/2502.11574v1",
    "arxiv_id": "2502.11574v1",
    "authors": [
      "Johan Boye",
      "Birger Moell"
    ],
    "published": "2025-02-17T09:07:32+00:00",
    "summary": "This paper investigates the mathematical reasoning capabilities of large language models (LLMs) using 50 newly constructed high-school-level word problems. Unlike prior studies that focus solely on answer correctness, we rigorously analyze both final answers and solution steps to identify reasoning failures. Evaluating eight state-of-the-art models - including Mixtral, Llama, Gemini, GPT-4o, and OpenAI's o1 variants - we find that while newer models (e.g., o3-mini, deepseek-r1) achieve higher accuracy, all models exhibit errors in spatial reasoning, strategic planning, and arithmetic, sometimes producing correct answers through flawed logic. Common failure modes include unwarranted assumptions, over-reliance on numerical patterns, and difficulty translating physical intuition into mathematical steps. Manual analysis reveals that models struggle with problems requiring multi-step deduction or real-world knowledge, despite possessing broad mathematical knowledge. Our results underscore the importance of evaluating reasoning processes, not just answers, and caution against overestimating LLMs' problem-solving proficiency. The study highlights persistent gaps in LLMs' generalization abilities, emphasizing the need for targeted improvements in structured reasoning and constraint handling."
  },
  {
    "title": "Equilibrate RLHF: Towards Balancing Helpfulness-Safety Trade-off in Large Language Models",
    "url": "http://arxiv.org/abs/2502.11555v1",
    "arxiv_id": "2502.11555v1",
    "authors": [
      "Yingshui Tan",
      "Yilei Jiang",
      "Yanshi Li",
      "Jiaheng Liu",
      "Xingyuan Bu",
      "Wenbo Su",
      "Xiangyu Yue",
      "Xiaoyong Zhu",
      "Bo Zheng"
    ],
    "published": "2025-02-17T08:40:30+00:00",
    "summary": "Fine-tuning large language models (LLMs) based on human preferences, commonly achieved through reinforcement learning from human feedback (RLHF), has been effective in improving their performance. However, maintaining LLM safety throughout the fine-tuning process remains a significant challenge, as resolving conflicts between safety and helpfulness can be non-trivial. Typically, the safety alignment of LLM is trained on data with safety-related categories. However, our experiments find that naively increasing the scale of safety training data usually leads the LLMs to an ``overly safe'' state rather than a ``truly safe'' state, boosting the refusal rate through extensive safety-aligned data without genuinely understanding the requirements for safe responses. Such an approach can inadvertently diminish the models' helpfulness. To understand the phenomenon, we first investigate the role of safety data by categorizing them into three different groups, and observe that each group behaves differently as training data scales up. To boost the balance between safety and helpfulness, we propose an Equilibrate RLHF framework including a Fine-grained Data-centric (FDC) approach that achieves better safety alignment even with fewer training data, and an Adaptive Message-wise Alignment (AMA) approach, which selectively highlight the key segments through a gradient masking strategy. Extensive experimental results demonstrate that our approach significantly enhances the safety alignment of LLMs while balancing safety and helpfulness."
  },
  {
    "title": "Evaluating o1-Like LLMs: Unlocking Reasoning for Translation through Comprehensive Analysis",
    "url": "http://arxiv.org/abs/2502.11544v1",
    "arxiv_id": "2502.11544v1",
    "authors": [
      "Andong Chen",
      "Yuchen Song",
      "Wenxin Zhu",
      "Kehai Chen",
      "Muyun Yang",
      "Tiejun Zhao",
      "Min zhang"
    ],
    "published": "2025-02-17T08:23:46+00:00",
    "summary": "The o1-Like LLMs are transforming AI by simulating human cognitive processes, but their performance in multilingual machine translation (MMT) remains underexplored. This study examines: (1) how o1-Like LLMs perform in MMT tasks and (2) what factors influence their translation quality. We evaluate multiple o1-Like LLMs and compare them with traditional models like ChatGPT and GPT-4o. Results show that o1-Like LLMs establish new multilingual translation benchmarks, with DeepSeek-R1 surpassing GPT-4o in contextless tasks. They demonstrate strengths in historical and cultural translation but exhibit a tendency for rambling issues in Chinese-centric outputs. Further analysis reveals three key insights: (1) High inference costs and slower processing speeds make complex translation tasks more resource-intensive. (2) Translation quality improves with model size, enhancing commonsense reasoning and cultural translation. (3) The temperature parameter significantly impacts output quality-lower temperatures yield more stable and accurate translations, while higher temperatures reduce coherence and precision."
  },
  {
    "title": "AURORA:Automated Training Framework of Universal Process Reward Models via Ensemble Prompting and Reverse Verification",
    "url": "http://arxiv.org/abs/2502.11520v1",
    "arxiv_id": "2502.11520v1",
    "authors": [
      "Xiaoyu Tan",
      "Tianchu Yao",
      "Chao Qu",
      "Bin Li",
      "Minghao Yang",
      "Dakuan Lu",
      "Haozhe Wang",
      "Xihe Qiu",
      "Wei Chu",
      "Yinghui Xu",
      "Yuan Qi"
    ],
    "published": "2025-02-17T07:41:27+00:00",
    "summary": "The reasoning capabilities of advanced large language models (LLMs) like o1 have revolutionized artificial intelligence applications. Nevertheless, evaluating and optimizing complex reasoning processes remain significant challenges due to diverse policy distributions and the inherent limitations of human effort and accuracy. In this paper, we present AURORA, a novel automated framework for training universal process reward models (PRMs) using ensemble prompting and reverse verification. The framework employs a two-phase approach: First, it uses diverse prompting strategies and ensemble methods to perform automated annotation and evaluation of processes, ensuring robust assessments for reward learning. Second, it leverages practical reference answers for reverse verification, enhancing the model's ability to validate outputs and improving training accuracy. To assess the framework's performance, we extend beyond the existing ProcessBench benchmark by introducing UniversalBench, which evaluates reward predictions across full trajectories under diverse policy distribtion with long Chain-of-Thought (CoT) outputs. Experimental results demonstrate that AURORA enhances process evaluation accuracy, improves PRMs' accuracy for diverse policy distributions and long-CoT responses. The project will be open-sourced at https://auroraprm.github.io/. The Universal-PRM-7B is available at https://huggingface.co/infly/Universal-PRM-7B."
  },
  {
    "title": "Adversary-Aware DPO: Enhancing Safety Alignment in Vision Language Models via Adversarial Training",
    "url": "http://arxiv.org/abs/2502.11455v1",
    "arxiv_id": "2502.11455v1",
    "authors": [
      "Fenghua Weng",
      "Jian Lou",
      "Jun Feng",
      "Minlie Huang",
      "Wenjie Wang"
    ],
    "published": "2025-02-17T05:28:47+00:00",
    "summary": "Safety alignment is critical in pre-training large language models (LLMs) to generate responses aligned with human values and refuse harmful queries. Unlike LLM, the current safety alignment of VLMs is often achieved with post-hoc safety fine-tuning. However, these methods are less effective to white-box attacks. To address this, we propose $\\textit{Adversary-aware DPO (ADPO)}$, a novel training framework that explicitly considers adversarial. $\\textit{Adversary-aware DPO (ADPO)}$ integrates adversarial training into DPO to enhance the safety alignment of VLMs under worst-case adversarial perturbations. $\\textit{ADPO}$ introduces two key components: (1) an adversarial-trained reference model that generates human-preferred responses under worst-case perturbations, and (2) an adversarial-aware DPO loss that generates winner-loser pairs accounting for adversarial distortions. By combining these innovations, $\\textit{ADPO}$ ensures that VLMs remain robust and reliable even in the presence of sophisticated jailbreak attacks. Extensive experiments demonstrate that $\\textit{ADPO}$ outperforms baselines in the safety alignment and general utility of VLMs."
  },
  {
    "title": "AGrail: A Lifelong Agent Guardrail with Effective and Adaptive Safety Detection",
    "url": "http://arxiv.org/abs/2502.11448v2",
    "arxiv_id": "2502.11448v2",
    "authors": [
      "Weidi Luo",
      "Shenghong Dai",
      "Xiaogeng Liu",
      "Suman Banerjee",
      "Huan Sun",
      "Muhao Chen",
      "Chaowei Xiao"
    ],
    "published": "2025-02-17T05:12:33+00:00",
    "summary": "The rapid advancements in Large Language Models (LLMs) have enabled their deployment as autonomous agents for handling complex tasks in dynamic environments. These LLMs demonstrate strong problem-solving capabilities and adaptability to multifaceted scenarios. However, their use as agents also introduces significant risks, including task-specific risks, which are identified by the agent administrator based on the specific task requirements and constraints, and systemic risks, which stem from vulnerabilities in their design or interactions, potentially compromising confidentiality, integrity, or availability (CIA) of information and triggering security risks. Existing defense agencies fail to adaptively and effectively mitigate these risks. In this paper, we propose AGrail, a lifelong agent guardrail to enhance LLM agent safety, which features adaptive safety check generation, effective safety check optimization, and tool compatibility and flexibility. Extensive experiments demonstrate that AGrail not only achieves strong performance against task-specific and system risks but also exhibits transferability across different LLM agents' tasks."
  },
  {
    "title": "What's in a Query: Polarity-Aware Distribution-Based Fair Ranking",
    "url": "http://arxiv.org/abs/2502.11429v1",
    "arxiv_id": "2502.11429v1",
    "authors": [
      "Aparna Balagopalan",
      "Kai Wang",
      "Olawale Salaudeen",
      "Asia Biega",
      "Marzyeh Ghassemi"
    ],
    "published": "2025-02-17T04:38:36+00:00",
    "summary": "Machine learning-driven rankings, where individuals (or items) are ranked in response to a query, mediate search exposure or attention in a variety of safety-critical settings. Thus, it is important to ensure that such rankings are fair. Under the goal of equal opportunity, attention allocated to an individual on a ranking interface should be proportional to their relevance across search queries. In this work, we examine amortized fair ranking -- where relevance and attention are cumulated over a sequence of user queries to make fair ranking more feasible in practice. Unlike prior methods that operate on expected amortized attention for each individual, we define new divergence-based measures for attention distribution-based fairness in ranking (DistFaiR), characterizing unfairness as the divergence between the distribution of attention and relevance corresponding to an individual over time. This allows us to propose new definitions of unfairness, which are more reliable at test time. Second, we prove that group fairness is upper-bounded by individual fairness under this definition for a useful class of divergence measures, and experimentally show that maximizing individual fairness through an integer linear programming-based optimization is often beneficial to group fairness. Lastly, we find that prior research in amortized fair ranking ignores critical information about queries, potentially leading to a fairwashing risk in practice by making rankings appear more fair than they actually are."
  },
  {
    "title": "Detecting and Filtering Unsafe Training Data via Data Attribution",
    "url": "http://arxiv.org/abs/2502.11411v1",
    "arxiv_id": "2502.11411v1",
    "authors": [
      "Yijun Pan",
      "Taiwei Shi",
      "Jieyu Zhao",
      "Jiaqi W. Ma"
    ],
    "published": "2025-02-17T03:50:58+00:00",
    "summary": "Large language models (LLMs) are vulnerable to unsafe training data that even small amounts of unsafe data can lead to harmful model behaviors. Detecting and filtering such unsafe training data is essential for trustworthy model development. Current state-of-the-art (SOTA) approaches typically rely on training moderation classifiers which requires significant computational overhead and are limited to predefined taxonomies, making them less adaptable to evolving safety concerns. Moreover, these classifiers lack insight into the training process, limiting their effectiveness in filtering unsafe data. To address these limitations, we propose DABUF, leveraging data attribution to detect and filter unsafe training data by attributing harmful model outputs to influential training data points. DABUF enables flexible identification of various unsafe data types without predefined taxonomies. However, in practice, model outputs can be complex with combined safe linguistic features and unsafe content, leading to reduced attribution accuracy. In such cases, DABUF will integrate moderation classifiers to identify a minimal subset of unsafe training data for targeted attribution (such as jailbreak). When model outputs are relatively straightforward, DABUF uses model outputs directly as the attribution targets. We evaluate the performance on two different tasks: in filtering jailbreaking training data and in identifying and mitigating gender bias. DABUF outperforms SOTA approaches by up to 7.5\\% in detection AUPRC in jailbreaking scenarios, and 44.1\\% in detecting gender bias. Moreover, retraining on DABUF-filtered data leads to higher model safety across experiments, underscoring its versatility in addressing a broad spectrum of unsafe data issues."
  },
  {
    "title": "CCJA: Context-Coherent Jailbreak Attack for Aligned Large Language Models",
    "url": "http://arxiv.org/abs/2502.11379v1",
    "arxiv_id": "2502.11379v1",
    "authors": [
      "Guanghao Zhou",
      "Panjia Qiu",
      "Mingyuan Fan",
      "Cen Chen",
      "Mingyuan Chu",
      "Xin Zhang",
      "Jun Zhou"
    ],
    "published": "2025-02-17T02:49:26+00:00",
    "summary": "Despite explicit alignment efforts for large language models (LLMs), they can still be exploited to trigger unintended behaviors, a phenomenon known as \"jailbreaking.\" Current jailbreak attack methods mainly focus on discrete prompt manipulations targeting closed-source LLMs, relying on manually crafted prompt templates and persuasion rules. However, as the capabilities of open-source LLMs improve, ensuring their safety becomes increasingly crucial. In such an environment, the accessibility of model parameters and gradient information by potential attackers exacerbates the severity of jailbreak threats. To address this research gap, we propose a novel \\underline{C}ontext-\\underline{C}oherent \\underline{J}ailbreak \\underline{A}ttack (CCJA). We define jailbreak attacks as an optimization problem within the embedding space of masked language models. Through combinatorial optimization, we effectively balance the jailbreak attack success rate with semantic coherence. Extensive evaluations show that our method not only maintains semantic consistency but also surpasses state-of-the-art baselines in attack effectiveness. Additionally, by integrating semantically coherent jailbreak prompts generated by our method into widely used black-box methodologies, we observe a notable enhancement in their success rates when targeting closed-source commercial LLMs. This highlights the security threat posed by open-source LLMs to commercial counterparts. We will open-source our code if the paper is accepted."
  },
  {
    "title": "HI-GVF: Shared Control based on Human-Influenced Guiding Vector Fields for Human-multi-robot Cooperation",
    "url": "http://arxiv.org/abs/2502.11370v1",
    "arxiv_id": "2502.11370v1",
    "authors": [
      "Pengming Zhu",
      "Zongtan Zhou",
      "Weijia Yao",
      "Wei Dai",
      "Zhiwen Zeng",
      "Huimin Lu"
    ],
    "published": "2025-02-17T02:33:09+00:00",
    "summary": "Human-multi-robot shared control leverages human decision-making and robotic autonomy to enhance human-robot collaboration. While widely studied, existing systems often adopt a leader-follower model, limiting robot autonomy to some extent. Besides, a human is required to directly participate in the motion control of robots through teleoperation, which significantly burdens the operator. To alleviate these two issues, we propose a layered shared control computing framework using human-influenced guiding vector fields (HI-GVF) for human-robot collaboration. HI-GVF guides the multi-robot system along a desired path specified by the human. Then, an intention field is designed to merge the human and robot intentions, accelerating the propagation of the human intention within the multi-robot system. Moreover, we give the stability analysis of the proposed model and use collision avoidance based on safety barrier certificates to fine-tune the velocity. Eventually, considering the firefighting task as an example scenario, we conduct simulations and experiments using multiple human-robot interfaces (brain-computer interface, myoelectric wristband, eye-tracking), and the results demonstrate that our proposed approach boosts the effectiveness and performance of the task."
  },
  {
    "title": "Sparse Autoencoder Features for Classifications and Transferability",
    "url": "http://arxiv.org/abs/2502.11367v1",
    "arxiv_id": "2502.11367v1",
    "authors": [
      "Jack Gallifant",
      "Shan Chen",
      "Kuleen Sasse",
      "Hugo Aerts",
      "Thomas Hartvigsen",
      "Danielle S. Bitterman"
    ],
    "published": "2025-02-17T02:30:45+00:00",
    "summary": "Sparse Autoencoders (SAEs) provide potentials for uncovering structured, human-interpretable representations in Large Language Models (LLMs), making them a crucial tool for transparent and controllable AI systems. We systematically analyze SAE for interpretable feature extraction from LLMs in safety-critical classification tasks. Our framework evaluates (1) model-layer selection and scaling properties, (2) SAE architectural configurations, including width and pooling strategies, and (3) the effect of binarizing continuous SAE activations. SAE-derived features achieve macro F1 > 0.8, outperforming hidden-state and BoW baselines while demonstrating cross-model transfer from Gemma 2 2B to 9B-IT models. These features generalize in a zero-shot manner to cross-lingual toxicity detection and visual classification tasks. Our analysis highlights the significant impact of pooling strategies and binarization thresholds, showing that binarization offers an efficient alternative to traditional feature selection while maintaining or improving performance. These findings establish new best practices for SAE-based interpretability and enable scalable, transparent deployment of LLMs in real-world applications. Full repo: https://github.com/shan23chen/MOSAIC."
  },
  {
    "title": "VLDBench: Vision Language Models Disinformation Detection Benchmark",
    "url": "http://arxiv.org/abs/2502.11361v1",
    "arxiv_id": "2502.11361v1",
    "authors": [
      "Shaina Raza",
      "Ashmal Vayani",
      "Aditya Jain",
      "Aravind Narayanan",
      "Vahid Reza Khazaie",
      "Syed Raza Bashir",
      "Elham Dolatabadi",
      "Gias Uddin",
      "Christos Emmanouilidis",
      "Rizwan Qureshi",
      "Mubarak Shah"
    ],
    "published": "2025-02-17T02:18:47+00:00",
    "summary": "The rapid rise of AI-generated content has made detecting disinformation increasingly challenging. In particular, multimodal disinformation, i.e., online posts-articles that contain images and texts with fabricated information are specially designed to deceive. While existing AI safety benchmarks primarily address bias and toxicity, multimodal disinformation detection remains largely underexplored. To address this challenge, we present the Vision-Language Disinformation Detection Benchmark VLDBench, the first comprehensive benchmark for detecting disinformation across both unimodal (text-only) and multimodal (text and image) content, comprising 31,000} news article-image pairs, spanning 13 distinct categories, for robust evaluation. VLDBench features a rigorous semi-automated data curation pipeline, with 22 domain experts dedicating 300 plus hours} to annotation, achieving a strong inter-annotator agreement (Cohen kappa = 0.78). We extensively evaluate state-of-the-art Large Language Models (LLMs) and Vision-Language Models (VLMs), demonstrating that integrating textual and visual cues in multimodal news posts improves disinformation detection accuracy by 5 - 35 % compared to unimodal models. Developed in alignment with AI governance frameworks such as the EU AI Act, NIST guidelines, and the MIT AI Risk Repository 2024, VLDBench is expected to become a benchmark for detecting disinformation in online multi-modal contents. Our code and data will be publicly available."
  },
  {
    "title": "A Framework for Learning Scoring Rules in Autonomous Driving Planning Systems",
    "url": "http://arxiv.org/abs/2502.11352v1",
    "arxiv_id": "2502.11352v1",
    "authors": [
      "Zikang Xiong",
      "Joe Kurian Eappen",
      "Suresh Jagannathan"
    ],
    "published": "2025-02-17T02:06:57+00:00",
    "summary": "In autonomous driving systems, motion planning is commonly implemented as a two-stage process: first, a trajectory proposer generates multiple candidate trajectories, then a scoring mechanism selects the most suitable trajectory for execution. For this critical selection stage, rule-based scoring mechanisms are particularly appealing as they can explicitly encode driving preferences, safety constraints, and traffic regulations in a formalized, human-understandable format. However, manually crafting these scoring rules presents significant challenges: the rules often contain complex interdependencies, require careful parameter tuning, and may not fully capture the nuances present in real-world driving data. This work introduces FLoRA, a novel framework that bridges this gap by learning interpretable scoring rules represented in temporal logic. Our method features a learnable logic structure that captures nuanced relationships across diverse driving scenarios, optimizing both rules and parameters directly from real-world driving demonstrations collected in NuPlan. Our approach effectively learns to evaluate driving behavior even though the training data only contains positive examples (successful driving demonstrations). Evaluations in closed-loop planning simulations demonstrate that our learned scoring rules outperform existing techniques, including expert-designed rules and neural network scoring models, while maintaining interpretability. This work introduces a data-driven approach to enhance the scoring mechanism in autonomous driving systems, designed as a plug-in module to seamlessly integrate with various trajectory proposers. Our video and code are available on xiong.zikang.me/FLoRA."
  },
  {
    "title": "Leveraging Multimodal-LLMs Assisted by Instance Segmentation for Intelligent Traffic Monitoring",
    "url": "http://arxiv.org/abs/2502.11304v1",
    "arxiv_id": "2502.11304v1",
    "authors": [
      "Murat Arda Onsu",
      "Poonam Lohan",
      "Burak Kantarci",
      "Aisha Syed",
      "Matthew Andrews",
      "Sean Kennedy"
    ],
    "published": "2025-02-16T23:03:26+00:00",
    "summary": "A robust and efficient traffic monitoring system is essential for smart cities and Intelligent Transportation Systems (ITS), using sensors and cameras to track vehicle movements, optimize traffic flow, reduce congestion, enhance road safety, and enable real-time adaptive traffic control. Traffic monitoring models must comprehensively understand dynamic urban conditions and provide an intuitive user interface for effective management. This research leverages the LLaVA visual grounding multimodal large language model (LLM) for traffic monitoring tasks on the real-time Quanser Interactive Lab simulation platform, covering scenarios like intersections, congestion, and collisions. Cameras placed at multiple urban locations collect real-time images from the simulation, which are fed into the LLaVA model with queries for analysis. An instance segmentation model integrated into the cameras highlights key elements such as vehicles and pedestrians, enhancing training and throughput. The system achieves 84.3% accuracy in recognizing vehicle locations and 76.4% in determining steering direction, outperforming traditional models."
  },
  {
    "title": "MC-BEVRO: Multi-Camera Bird Eye View Road Occupancy Detection for Traffic Monitoring",
    "url": "http://arxiv.org/abs/2502.11287v1",
    "arxiv_id": "2502.11287v1",
    "authors": [
      "Arpitsinh Vaghela",
      "Duo Lu",
      "Aayush Atul Verma",
      "Bharatesh Chakravarthi",
      "Hua Wei",
      "Yezhou Yang"
    ],
    "published": "2025-02-16T22:03:03+00:00",
    "summary": "Single camera 3D perception for traffic monitoring faces significant challenges due to occlusion and limited field of view. Moreover, fusing information from multiple cameras at the image feature level is difficult because of different view angles. Further, the necessity for practical implementation and compatibility with existing traffic infrastructure compounds these challenges. To address these issues, this paper introduces a novel Bird's-Eye-View road occupancy detection framework that leverages multiple roadside cameras to overcome the aforementioned limitations. To facilitate the framework's development and evaluation, a synthetic dataset featuring diverse scenes and varying camera configurations is generated using the CARLA simulator. A late fusion and three early fusion methods were implemented within the proposed framework, with performance further enhanced by integrating backgrounds. Extensive evaluations were conducted to analyze the impact of multi-camera inputs and varying BEV occupancy map sizes on model performance. Additionally, a real-world data collection pipeline was developed to assess the model's ability to generalize to real-world environments. The sim-to-real capabilities of the model were evaluated using zero-shot and few-shot fine-tuning, demonstrating its potential for practical application. This research aims to advance perception systems in traffic monitoring, contributing to improved traffic management, operational efficiency, and road safety."
  },
  {
    "title": "Soteria: Language-Specific Functional Parameter Steering for Multilingual Safety Alignment",
    "url": "http://arxiv.org/abs/2502.11244v1",
    "arxiv_id": "2502.11244v1",
    "authors": [
      "Somnath Banerjee",
      "Sayan Layek",
      "Pratyush Chatterjee",
      "Animesh Mukherjee",
      "Rima Hazra"
    ],
    "published": "2025-02-16T19:44:01+00:00",
    "summary": "Ensuring consistent safety across multiple languages remains a significant challenge for large language models (LLMs). We introduce Soteria, a lightweight yet powerful strategy that locates and minimally adjusts the \"functional heads\" most responsible for harmful content generation in each language. By altering only a fraction of parameters, Soteria drastically reduces policy violations without sacrificing overall model performance, even in low-resource settings. To rigorously evaluate our approach, we also present XThreatBench, a specialized multilingual dataset capturing fine-grained harmful behaviors drawn from real policy guidelines. Experiments with leading open-source LLMs (e.g., Llama, Qwen, Mistral) show that Soteria consistently improves safety metrics across high-, mid-, and low-resource languages. These findings highlight a promising path toward scalable, linguistically attuned, and ethically aligned LLMs worldwide."
  },
  {
    "title": "LLMs and Childhood Safety: Identifying Risks and Proposing a Protection Framework for Safe Child-LLM Interaction",
    "url": "http://arxiv.org/abs/2502.11242v1",
    "arxiv_id": "2502.11242v1",
    "authors": [
      "Junfeng Jiao",
      "Saleh Afroogh",
      "Kevin Chen",
      "Abhejay Murali",
      "David Atkinson",
      "Amit Dhurandhar"
    ],
    "published": "2025-02-16T19:39:48+00:00",
    "summary": "This study examines the growing use of Large Language Models (LLMs) in child-centered applications, highlighting safety and ethical concerns such as bias, harmful content, and cultural insensitivity. Despite their potential to enhance learning, there is a lack of standardized frameworks to mitigate these risks. Through a systematic literature review, we identify key parental and empirical concerns, including toxicity and ethical breaches in AI outputs. Moreover, to address these issues, this paper proposes a protection framework for safe Child-LLM interaction, incorporating metrics for content safety, behavioral ethics, and cultural sensitivity. The framework provides practical tools for evaluating LLM safety, offering guidance for developers, policymakers, and educators to ensure responsible AI deployment for children."
  },
  {
    "title": "Can't See the Forest for the Trees: Benchmarking Multimodal Safety Awareness for Multimodal LLMs",
    "url": "http://arxiv.org/abs/2502.11184v1",
    "arxiv_id": "2502.11184v1",
    "authors": [
      "Wenxuan Wang",
      "Xiaoyuan Liu",
      "Kuiyi Gao",
      "Jen-tse Huang",
      "Youliang Yuan",
      "Pinjia He",
      "Shuai Wang",
      "Zhaopeng Tu"
    ],
    "published": "2025-02-16T16:12:40+00:00",
    "summary": "Multimodal Large Language Models (MLLMs) have expanded the capabilities of traditional language models by enabling interaction through both text and images. However, ensuring the safety of these models remains a significant challenge, particularly in accurately identifying whether multimodal content is safe or unsafe-a capability we term safety awareness. In this paper, we introduce MMSafeAware, the first comprehensive multimodal safety awareness benchmark designed to evaluate MLLMs across 29 safety scenarios with 1500 carefully curated image-prompt pairs. MMSafeAware includes both unsafe and over-safety subsets to assess models abilities to correctly identify unsafe content and avoid over-sensitivity that can hinder helpfulness. Evaluating nine widely used MLLMs using MMSafeAware reveals that current models are not sufficiently safe and often overly sensitive; for example, GPT-4V misclassifies 36.1% of unsafe inputs as safe and 59.9% of benign inputs as unsafe. We further explore three methods to improve safety awareness-prompting-based approaches, visual contrastive decoding, and vision-centric reasoning fine-tuning-but find that none achieve satisfactory performance. Our findings highlight the profound challenges in developing MLLMs with robust safety awareness, underscoring the need for further research in this area. All the code and data will be publicly available to facilitate future research."
  },
  {
    "title": "Quantifying the Capability Boundary of DeepSeek Models: An Application-Driven Performance Analysis",
    "url": "http://arxiv.org/abs/2502.11164v1",
    "arxiv_id": "2502.11164v1",
    "authors": [
      "Shiguo Lian",
      "Kaikai Zhao",
      "Xuejiao Lei",
      "Ning Wang",
      "Zhenhong Long",
      "Peijun Yang",
      "Minjie Hua",
      "Chaoyang Ma",
      "Wen Liu",
      "Kai Wang",
      "Zhaoxiang Liu"
    ],
    "published": "2025-02-16T15:29:58+00:00",
    "summary": "DeepSeek-R1, known for its low training cost and exceptional reasoning capabilities, has achieved state-of-the-art performance on various benchmarks. However, detailed evaluations from the perspective of real-world applications are lacking, making it challenging for users to select the most suitable DeepSeek models for their specific needs. To address this gap, we evaluate the DeepSeek-V3, DeepSeek-R1, DeepSeek-R1-Distill-Qwen series, and DeepSeek-R1-Distill-Llama series on A-Eval, an application-driven benchmark. By comparing original instruction-tuned models with their distilled counterparts, we analyze how reasoning enhancements impact performance across diverse practical tasks. Our results show that reasoning-enhanced models, while generally powerful, do not universally outperform across all tasks, with performance gains varying significantly across tasks and models. To further assist users in model selection, we quantify the capability boundary of DeepSeek models through performance tier classifications and intuitive line charts. Specific examples provide actionable insights to help users select and deploy the most cost-effective DeepSeek models, ensuring optimal performance and resource efficiency in real-world applications."
  },
  {
    "title": "Safety Evaluation of DeepSeek Models in Chinese Contexts",
    "url": "http://arxiv.org/abs/2502.11137v1",
    "arxiv_id": "2502.11137v1",
    "authors": [
      "Wenjing Zhang",
      "Xuejiao Lei",
      "Zhaoxiang Liu",
      "Ning Wang",
      "Zhenhong Long",
      "Peijun Yang",
      "Jiaojiao Zhao",
      "Minjie Hua",
      "Chaoyang Ma",
      "Kai Wang",
      "Shiguo Lian"
    ],
    "published": "2025-02-16T14:05:54+00:00",
    "summary": "Recently, the DeepSeek series of models, leveraging their exceptional reasoning capabilities and open-source strategy, is reshaping the global AI landscape. Despite these advantages, they exhibit significant safety deficiencies. Research conducted by Robust Intelligence, a subsidiary of Cisco, in collaboration with the University of Pennsylvania, revealed that DeepSeek-R1 has a 100\\% attack success rate when processing harmful prompts. Additionally, multiple safety companies and research institutions have confirmed critical safety vulnerabilities in this model. As models demonstrating robust performance in Chinese and English, DeepSeek models require equally crucial safety assessments in both language contexts. However, current research has predominantly focused on safety evaluations in English environments, leaving a gap in comprehensive assessments of their safety performance in Chinese contexts. In response to this gap, this study introduces CHiSafetyBench, a Chinese-specific safety evaluation benchmark. This benchmark systematically evaluates the safety of DeepSeek-R1 and DeepSeek-V3 in Chinese contexts, revealing their performance across safety categories. The experimental results quantify the deficiencies of these two models in Chinese contexts, providing key insights for subsequent improvements."
  },
  {
    "title": "Ramp Up NTT in Record Time using GPU-Accelerated Algorithms and LLM-based Code Generation",
    "url": "http://arxiv.org/abs/2502.11110v1",
    "arxiv_id": "2502.11110v1",
    "authors": [
      "Yu Cui",
      "Hang Fu",
      "Licheng Wang",
      "Haibin Zhang"
    ],
    "published": "2025-02-16T12:53:23+00:00",
    "summary": "Homomorphic encryption (HE) is a core building block in privacy-preserving machine learning (PPML), but HE is also widely known as its efficiency bottleneck. Therefore, many GPU-accelerated cryptographic schemes have been proposed to improve the performance of HE. However, these methods often require complex modifications tailored to specific algorithms and are tightly coupled with specific GPU and operating systems. It is interesting to ask how to generally offer more practical GPU-accelerated cryptographic algorithm implementations. Given the powerful code generation capabilities of large language models (LLMs), we aim to explore their potential to automatically generate practical GPU-friendly algorithm code using CPU-friendly code. In this paper, we focus on number theoretic transform (NTT) -- the core mechanism of HE. We first develop and optimize a GPU-friendly NTT (GNTT) family that exploits PyTorch's fast matrix computation and precomputation, achieving an approximately 62x speedup -- a significant boost over existing ones. Then we explore GPU-friendly code generation using various LLMs, including DeepSeek-R1, OpenAI o1 and o3-mini. We discover many interesting findings throughout the process. For instance, somewhat surprisingly, our experiments demonstrate that DeepSeek-R1 significantly outperforms OpenAI o3-mini and o1, but still cannot beat our optimized protocol. The findings provide valuable insights for turbocharging PPML and enhancing code generation capabilities of LLMs. Codes are available at: https://github.com/LMPC-Lab/GenGPUCrypto."
  },
  {
    "title": "Knowledge Graph-Driven Retrieval-Augmented Generation: Integrating Deepseek-R1 with Weaviate for Advanced Chatbot Applications",
    "url": "http://arxiv.org/abs/2502.11108v1",
    "arxiv_id": "2502.11108v1",
    "authors": [
      "Alexandru Lecu",
      "Adrian Groza",
      "Lezan Hawizy"
    ],
    "published": "2025-02-16T12:52:28+00:00",
    "summary": "Large language models (LLMs) have significantly advanced the field of natural language generation. However, they frequently generate unverified outputs, which compromises their reliability in critical applications. In this study, we propose an innovative framework that combines structured biomedical knowledge with LLMs through a retrieval-augmented generation technique. Our system develops a thorough knowledge graph by identifying and refining causal relationships and named entities from medical abstracts related to age-related macular degeneration (AMD). Using a vector-based retrieval process and a locally deployed language model, our framework produces responses that are both contextually relevant and verifiable, with direct references to clinical evidence. Experimental results show that this method notably decreases hallucinations, enhances factual precision, and improves the clarity of generated responses, providing a robust solution for advanced biomedical chatbot applications."
  },
  {
    "title": "Talk Structurally, Act Hierarchically: A Collaborative Framework for LLM Multi-Agent Systems",
    "url": "http://arxiv.org/abs/2502.11098v1",
    "arxiv_id": "2502.11098v1",
    "authors": [
      "Zhao Wang",
      "Sota Moriyama",
      "Wei-Yao Wang",
      "Briti Gangopadhyay",
      "Shingo Takamatsu"
    ],
    "published": "2025-02-16T12:26:58+00:00",
    "summary": "Recent advancements in LLM-based multi-agent (LLM-MA) systems have shown promise, yet significant challenges remain in managing communication and refinement when agents collaborate on complex tasks. In this paper, we propose \\textit{Talk Structurally, Act Hierarchically (TalkHier)}, a novel framework that introduces a structured communication protocol for context-rich exchanges and a hierarchical refinement system to address issues such as incorrect outputs, falsehoods, and biases. \\textit{TalkHier} surpasses various types of SoTA, including inference scaling model (OpenAI-o1), open-source multi-agent models (e.g., AgentVerse), and majority voting strategies on current LLM and single-agent baselines (e.g., ReAct, GPT4o), across diverse tasks, including open-domain question answering, domain-specific selective questioning, and practical advertisement text generation. These results highlight its potential to set a new standard for LLM-MA systems, paving the way for more effective, adaptable, and collaborative multi-agent frameworks. The code is available https://github.com/sony/talkhier."
  },
  {
    "title": "Mixture of Tunable Experts - Behavior Modification of DeepSeek-R1 at Inference Time",
    "url": "http://arxiv.org/abs/2502.11096v1",
    "arxiv_id": "2502.11096v1",
    "authors": [
      "Robert Dahlke",
      "Henrik Klagges",
      "Dan Zecha",
      "Benjamin Merkel",
      "Sven Rohr",
      "Fabian Klemm"
    ],
    "published": "2025-02-16T12:24:39+00:00",
    "summary": "We present the Mixture-of-Tunable-Experts (MoTE), a method that extends the Mixture-of-Experts architecture of Large Language Models (LLMs). Without additional training, MoTE enables meaningful and focused behavior changes in LLMs on-the-fly during inference time.   By analyzing the digital LLM brain of DeepSeek-R1 using a technique we dub 'functional Token Resonance Imaging' (fTRI) - inspired by fMRI and using prompts designed to elicit specific behavior (e.g., 'What happened {time}{place}?') - we empirically identify distinctive experts associated with behaviors like refusal responses.   Using MoTE we are able to intervene and control such specific behavior. We switched off the top 10 most refusal-relevant experts (0.07% of R1's 14,848 routed experts), achieving a 52% refusal reduction on sensitive reference prompts without performance degradation on MT-Bench. Random expert deactivation resulted in smaller behavioral shifts with increased noise, whereas forced expert activation led to significantly higher refusal rates.   Our approach shares similarities with sparse autoencoders (SAEs) in terms of explainability and steerability. Unlike SAEs, MoTE does not require large training efforts, as within MoEs with a vast number of experts, specialization already emerged naturally during pretraining.   Our findings suggest that significant functional mechanisms in Mixture-of-Experts architectures can at least partially be localized in a small number of specific experts, rather than being distributed throughout the model's weights. Expert subgroups can be tuned to trigger significant behavior variations, providing insights into the inner workings of LLMs."
  },
  {
    "title": "SafeDialBench: A Fine-Grained Safety Benchmark for Large Language Models in Multi-Turn Dialogues with Diverse Jailbreak Attacks",
    "url": "http://arxiv.org/abs/2502.11090v2",
    "arxiv_id": "2502.11090v2",
    "authors": [
      "Hongye Cao",
      "Yanming Wang",
      "Sijia Jing",
      "Ziyue Peng",
      "Zhixin Bai",
      "Zhe Cao",
      "Meng Fang",
      "Fan Feng",
      "Boyan Wang",
      "Jiaheng Liu",
      "Tianpei Yang",
      "Jing Huo",
      "Yang Gao",
      "Fanyu Meng",
      "Xi Yang",
      "Chao Deng",
      "Junlan Feng"
    ],
    "published": "2025-02-16T12:08:08+00:00",
    "summary": "With the rapid advancement of Large Language Models (LLMs), the safety of LLMs has been a critical concern requiring precise assessment. Current benchmarks primarily concentrate on single-turn dialogues or a single jailbreak attack method to assess the safety. Additionally, these benchmarks have not taken into account the LLM's capability of identifying and handling unsafe information in detail. To address these issues, we propose a fine-grained benchmark SafeDialBench for evaluating the safety of LLMs across various jailbreak attacks in multi-turn dialogues. Specifically, we design a two-tier hierarchical safety taxonomy that considers 6 safety dimensions and generates more than 4000 multi-turn dialogues in both Chinese and English under 22 dialogue scenarios. We employ 7 jailbreak attack strategies, such as reference attack and purpose reverse, to enhance the dataset quality for dialogue generation. Notably, we construct an innovative assessment framework of LLMs, measuring capabilities in detecting, and handling unsafe information and maintaining consistency when facing jailbreak attacks. Experimental results across 17 LLMs reveal that Yi-34B-Chat and GLM4-9B-Chat demonstrate superior safety performance, while Llama3.1-8B-Instruct and o3-mini exhibit safety vulnerabilities."
  },
  {
    "title": "Rewrite to Jailbreak: Discover Learnable and Transferable Implicit Harmfulness Instruction",
    "url": "http://arxiv.org/abs/2502.11084v1",
    "arxiv_id": "2502.11084v1",
    "authors": [
      "Yuting Huang",
      "Chengyuan Liu",
      "Yifeng Feng",
      "Chao Wu",
      "Fei Wu",
      "Kun Kuang"
    ],
    "published": "2025-02-16T11:43:39+00:00",
    "summary": "As Large Language Models (LLMs) are widely applied in various domains, the safety of LLMs is increasingly attracting attention to avoid their powerful capabilities being misused. Existing jailbreak methods create a forced instruction-following scenario, or search adversarial prompts with prefix or suffix tokens to achieve a specific representation manually or automatically. However, they suffer from low efficiency and explicit jailbreak patterns, far from the real deployment of mass attacks to LLMs. In this paper, we point out that simply rewriting the original instruction can achieve a jailbreak, and we find that this rewriting approach is learnable and transferable. We propose the Rewrite to Jailbreak (R2J) approach, a transferable black-box jailbreak method to attack LLMs by iteratively exploring the weakness of the LLMs and automatically improving the attacking strategy. The jailbreak is more efficient and hard to identify since no additional features are introduced. Extensive experiments and analysis demonstrate the effectiveness of R2J, and we find that the jailbreak is also transferable to multiple datasets and various types of models with only a few queries. We hope our work motivates further investigation of LLM safety."
  },
  {
    "title": "ClimateLLM: Efficient Weather Forecasting via Frequency-Aware Large Language Models",
    "url": "http://arxiv.org/abs/2502.11059v1",
    "arxiv_id": "2502.11059v1",
    "authors": [
      "Shixuan Li",
      "Wei Yang",
      "Peiyu Zhang",
      "Xiongye Xiao",
      "Defu Cao",
      "Yuehan Qin",
      "Xiaole Zhang",
      "Yue Zhao",
      "Paul Bogdan"
    ],
    "published": "2025-02-16T09:57:50+00:00",
    "summary": "Weather forecasting is crucial for public safety, disaster prevention and mitigation, agricultural production, and energy management, with global relevance. Although deep learning has significantly advanced weather prediction, current methods face critical limitations: (i) they often struggle to capture both dynamic temporal dependencies and short-term abrupt changes, making extreme weather modeling difficult; (ii) they incur high computational costs due to extensive training and resource requirements; (iii) they have limited adaptability to multi-scale frequencies, leading to challenges when separating global trends from local fluctuations. To address these issues, we propose ClimateLLM, a foundation model for weather forecasting. It captures spatiotemporal dependencies via a cross-temporal and cross-spatial collaborative modeling framework that integrates Fourier-based frequency decomposition with Large Language Models (LLMs) to strengthen spatial and temporal modeling. Our framework uses a Mixture-of-Experts (MoE) mechanism that adaptively processes different frequency components, enabling efficient handling of both global signals and localized extreme events. In addition, we introduce a cross-temporal and cross-spatial dynamic prompting mechanism, allowing LLMs to incorporate meteorological patterns across multiple scales effectively. Extensive experiments on real-world datasets show that ClimateLLM outperforms state-of-the-art approaches in accuracy and efficiency, as a scalable solution for global weather forecasting."
  },
  {
    "title": "A Physics-Informed Machine Learning Framework for Safe and Optimal Control of Autonomous Systems",
    "url": "http://arxiv.org/abs/2502.11057v1",
    "arxiv_id": "2502.11057v1",
    "authors": [
      "Manan Tayal",
      "Aditya Singh",
      "Shishir Kolathaya",
      "Somil Bansal"
    ],
    "published": "2025-02-16T09:46:17+00:00",
    "summary": "As autonomous systems become more ubiquitous in daily life, ensuring high performance with guaranteed safety is crucial. However, safety and performance could be competing objectives, which makes their co-optimization difficult. Learning-based methods, such as Constrained Reinforcement Learning (CRL), achieve strong performance but lack formal safety guarantees due to safety being enforced as soft constraints, limiting their use in safety-critical settings. Conversely, formal methods such as Hamilton-Jacobi (HJ) Reachability Analysis and Control Barrier Functions (CBFs) provide rigorous safety assurances but often neglect performance, resulting in overly conservative controllers. To bridge this gap, we formulate the co-optimization of safety and performance as a state-constrained optimal control problem, where performance objectives are encoded via a cost function and safety requirements are imposed as state constraints. We demonstrate that the resultant value function satisfies a Hamilton-Jacobi-Bellman (HJB) equation, which we approximate efficiently using a novel physics-informed machine learning framework. In addition, we introduce a conformal prediction-based verification strategy to quantify the learning errors, recovering a high-confidence safety value function, along with a probabilistic error bound on performance degradation. Through several case studies, we demonstrate the efficacy of the proposed framework in enabling scalable learning of safe and performant controllers for complex, high-dimensional autonomous systems."
  },
  {
    "title": "Reasoning-Augmented Conversation for Multi-Turn Jailbreak Attacks on Large Language Models",
    "url": "http://arxiv.org/abs/2502.11054v2",
    "arxiv_id": "2502.11054v2",
    "authors": [
      "Zonghao Ying",
      "Deyue Zhang",
      "Zonglei Jing",
      "Yisong Xiao",
      "Quanchen Zou",
      "Aishan Liu",
      "Siyuan Liang",
      "Xiangzheng Zhang",
      "Xianglong Liu",
      "Dacheng Tao"
    ],
    "published": "2025-02-16T09:27:44+00:00",
    "summary": "Multi-turn jailbreak attacks simulate real-world human interactions by engaging large language models (LLMs) in iterative dialogues, exposing critical safety vulnerabilities. However, existing methods often struggle to balance semantic coherence with attack effectiveness, resulting in either benign semantic drift or ineffective detection evasion. To address this challenge, we propose Reasoning-Augmented Conversation, a novel multi-turn jailbreak framework that reformulates harmful queries into benign reasoning tasks and leverages LLMs' strong reasoning capabilities to compromise safety alignment. Specifically, we introduce an attack state machine framework to systematically model problem translation and iterative reasoning, ensuring coherent query generation across multiple turns. Building on this framework, we design gain-guided exploration, self-play, and rejection feedback modules to preserve attack semantics, enhance effectiveness, and sustain reasoning-driven attack progression. Extensive experiments on multiple LLMs demonstrate that RACE achieves state-of-the-art attack effectiveness in complex conversational scenarios, with attack success rates (ASRs) increasing by up to 96%. Notably, our approach achieves ASRs of 82% and 92% against leading commercial models, OpenAI o1 and DeepSeek R1, underscoring its potency. We release our code at https://github.com/NY1024/RACE to facilitate further research in this critical domain."
  },
  {
    "title": "Prompt Inject Detection with Generative Explanation as an Investigative Tool",
    "url": "http://arxiv.org/abs/2502.11006v1",
    "arxiv_id": "2502.11006v1",
    "authors": [
      "Jonathan Pan",
      "Swee Liang Wong",
      "Yidi Yuan",
      "Xin Wei Chia"
    ],
    "published": "2025-02-16T06:16:00+00:00",
    "summary": "Large Language Models (LLMs) are vulnerable to adversarial prompt based injects. These injects could jailbreak or exploit vulnerabilities within these models with explicit prompt requests leading to undesired responses. In the context of investigating prompt injects, the challenge is the sheer volume of input prompts involved that are likely to be largely benign. This investigative challenge is further complicated by the semantics and subjectivity of the input prompts involved in the LLM conversation with its user and the context of the environment to which the conversation is being carried out. Hence, the challenge for AI security investigators would be two-fold. The first is to identify adversarial prompt injects and then to assess whether the input prompt is contextually benign or adversarial. For the first step, this could be done using existing AI security solutions like guardrails to detect and protect the LLMs. Guardrails have been developed using a variety of approaches. A popular approach is to use signature based. Another popular approach to develop AI models to classify such prompts include the use of NLP based models like a language model. However, in the context of conducting an AI security investigation of prompt injects, these guardrails lack the ability to aid investigators in triaging or assessing the identified input prompts. In this applied research exploration, we explore the use of a text generation capabilities of LLM to detect prompt injects and generate explanation for its detections to aid AI security investigators in assessing and triaging of such prompt inject detections. The practical benefit of such a tool is to ease the task of conducting investigation into prompt injects."
  },
  {
    "title": "Fine-Tuning Hard-to-Simulate Objectives for Quadruped Locomotion: A Case Study on Total Power Saving",
    "url": "http://arxiv.org/abs/2502.10956v1",
    "arxiv_id": "2502.10956v1",
    "authors": [
      "Ruiqian Nai",
      "Jiacheng You",
      "Liu Cao",
      "Hanchen Cui",
      "Shiyuan Zhang",
      "Huazhe Xu",
      "Yang Gao"
    ],
    "published": "2025-02-16T02:22:50+00:00",
    "summary": "Legged locomotion is not just about mobility; it also encompasses crucial objectives such as energy efficiency, safety, and user experience, which are vital for real-world applications. However, key factors such as battery power consumption and stepping noise are often inaccurately modeled or missing in common simulators, leaving these aspects poorly optimized or unaddressed by current sim-to-real methods. Hand-designed proxies, such as mechanical power and foot contact forces, have been used to address these challenges but are often problem-specific and inaccurate.   In this paper, we propose a data-driven framework for fine-tuning locomotion policies, targeting these hard-to-simulate objectives. Our framework leverages real-world data to model these objectives and incorporates the learned model into simulation for policy improvement. We demonstrate the effectiveness of our framework on power saving for quadruped locomotion, achieving a significant 24-28\\% net reduction in total power consumption from the battery pack at various speeds. In essence, our approach offers a versatile solution for optimizing hard-to-simulate objectives in quadruped locomotion, providing an easy-to-adapt paradigm for continual improving with real-world knowledge. Project page https://hard-to-sim.github.io/."
  },
  {
    "title": "Fundamental Principles of Linguistic Structure are Not Represented by o3",
    "url": "http://arxiv.org/abs/2502.10934v1",
    "arxiv_id": "2502.10934v1",
    "authors": [
      "Elliot Murphy",
      "Evelina Leivada",
      "Vittoria Dentella",
      "Fritz Gunther",
      "Gary Marcus"
    ],
    "published": "2025-02-15T23:53:31+00:00",
    "summary": "A core component of a successful artificial general intelligence would be the rapid creation and manipulation of grounded compositional abstractions and the demonstration of expertise in the family of recursive hierarchical syntactic objects necessary for the creative use of human language. We evaluated the recently released o3 model (OpenAI; o3-mini-high) and discovered that while it succeeds on some basic linguistic tests relying on linear, surface statistics (e.g., the Strawberry Test), it fails to generalize basic phrase structure rules; it fails with comparative sentences involving semantically illegal cardinality comparisons ('Escher sentences'); its fails to correctly rate and explain acceptability dynamics; and it fails to distinguish between instructions to generate unacceptable semantic vs. unacceptable syntactic outputs. When tasked with generating simple violations of grammatical rules, it is seemingly incapable of representing multiple parses to evaluate against various possible semantic interpretations. In stark contrast to many recent claims that artificial language models are on the verge of replacing the field of linguistics, our results suggest not only that deep learning is hitting a wall with respect to compositionality (Marcus 2022), but that it is hitting [a [stubbornly [resilient wall]]] that cannot readily be surmounted to reach human-like compositional reasoning simply through more compute."
  }
]