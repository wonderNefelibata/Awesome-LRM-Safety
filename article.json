[
  {
    "title": "Persona Features Control Emergent Misalignment",
    "url": "http://arxiv.org/abs/2506.19823v1",
    "arxiv_id": "2506.19823v1",
    "authors": [
      "Miles Wang",
      "Tom Dupr\u00e9 la Tour",
      "Olivia Watkins",
      "Alex Makelov",
      "Ryan A. Chi",
      "Samuel Miserendino",
      "Johannes Heidecke",
      "Tejal Patwardhan",
      "Dan Mossing"
    ],
    "published": "2025-06-24T17:38:21+00:00",
    "summary": "Understanding how language models generalize behaviors from their training to a broader deployment distribution is an important problem in AI safety. Betley et al. discovered that fine-tuning GPT-4o on intentionally insecure code causes \"emergent misalignment,\" where models give stereotypically malicious responses to unrelated prompts. We extend this work, demonstrating emergent misalignment across diverse conditions, including reinforcement learning on reasoning models, fine-tuning on various synthetic datasets, and in models without safety training. To investigate the mechanisms behind this generalized misalignment, we apply a \"model diffing\" approach using sparse autoencoders to compare internal model representations before and after fine-tuning. This approach reveals several \"misaligned persona\" features in activation space, including a toxic persona feature which most strongly controls emergent misalignment and can be used to predict whether a model will exhibit such behavior. Additionally, we investigate mitigation strategies, discovering that fine-tuning an emergently misaligned model on just a few hundred benign samples efficiently restores alignment."
  },
  {
    "title": "Optimization Studies of Radiation Shielding for the PIP-II Project at Fermilab",
    "url": "http://arxiv.org/abs/2506.19763v1",
    "arxiv_id": "2506.19763v1",
    "authors": [
      "Alajos Makovec",
      "Dali Georgobiani",
      "Igor Rakhno",
      "Igor Tropin"
    ],
    "published": "2025-06-24T16:23:45+00:00",
    "summary": "The PIP-II project at Fermilab, which includes an 800-MeV superconducting LINAC, demands rigorous radiation shielding optimization to meet safety requirements. We updated the MARS geometry model to reflect new magnet and collimator designs and introduced high-resolution detector planes to better capture radiation field distributions. To overcome the significant computational demands, we implemented a well-known branching technique that drastically reduced simulation runtimes while maintaining statistical integrity. This was achieved through particle splitting and the application of Russian Roulette techniques. Additionally, new graphical tools were created to streamline data visualization and MARS code usability."
  },
  {
    "title": "Formalization and security analysis of the Bridgeless protocol",
    "url": "http://arxiv.org/abs/2506.19730v1",
    "arxiv_id": "2506.19730v1",
    "authors": [
      "Orestis Alpos",
      "Oleg Fomenko",
      "Dimitris Karakostas",
      "Oleksandr Kurbatov",
      "Andrey Sabelnikov"
    ],
    "published": "2025-06-24T15:49:18+00:00",
    "summary": "This paper formalizes the proves the security of the Bridgeless protocol, a protocol able to bridge tokens between various chains. The Bridgeless protocol is run by a set of validators, responsible for verifying deposit transactions on the source chain and generating the corresponding withdrawals on the target chain. The protocol is designed to be chain-agnostic and the validators interact with each supported chain via a chain client. It currently supports EVM-compatible chains, the Zano, and the Bitcoin chains. The paper formalizes all involved subprotocols and describes the conditions under which the protocol maintains safety and liveness."
  },
  {
    "title": "A Verification Methodology for Safety Assurance of Robotic Autonomous Systems",
    "url": "http://arxiv.org/abs/2506.19622v1",
    "arxiv_id": "2506.19622v1",
    "authors": [
      "Mustafa Adam",
      "David A. Anisi",
      "Pedro Ribeiro"
    ],
    "published": "2025-06-24T13:39:51+00:00",
    "summary": "Autonomous robots deployed in shared human environments, such as agricultural settings, require rigorous safety assurance to meet both functional reliability and regulatory compliance. These systems must operate in dynamic, unstructured environments, interact safely with humans, and respond effectively to a wide range of potential hazards. This paper presents a verification workflow for the safety assurance of an autonomous agricultural robot, covering the entire development life-cycle, from concept study and design to runtime verification. The outlined methodology begins with a systematic hazard analysis and risk assessment to identify potential risks and derive corresponding safety requirements. A formal model of the safety controller is then developed to capture its behaviour and verify that the controller satisfies the specified safety properties with respect to these requirements. The proposed approach is demonstrated on a field robot operating in an agricultural setting. The results show that the methodology can be effectively used to verify safety-critical properties and facilitate the early identification of design issues, contributing to the development of safer robots and autonomous systems."
  },
  {
    "title": "Probabilistic modelling and safety assurance of an agriculture robot providing light-treatment",
    "url": "http://arxiv.org/abs/2506.19620v1",
    "arxiv_id": "2506.19620v1",
    "authors": [
      "Mustafa Adam",
      "Kangfeng Ye",
      "David A. Anisi",
      "Ana Cavalcanti",
      "Jim Woodcock",
      "Robert Morris"
    ],
    "published": "2025-06-24T13:39:32+00:00",
    "summary": "Continued adoption of agricultural robots postulates the farmer's trust in the reliability, robustness and safety of the new technology. This motivates our work on safety assurance of agricultural robots, particularly their ability to detect, track and avoid obstacles and humans. This paper considers a probabilistic modelling and risk analysis framework for use in the early development phases. Starting off with hazard identification and a risk assessment matrix, the behaviour of the mobile robot platform, sensor and perception system, and any humans present are captured using three state machines. An auto-generated probabilistic model is then solved and analysed using the probabilistic model checker PRISM. The result provides unique insight into fundamental development and engineering aspects by quantifying the effect of the risk mitigation actions and risk reduction associated with distinct design concepts. These include implications of adopting a higher performance and more expensive Object Detection System or opting for a more elaborate warning system to increase human awareness. Although this paper mainly focuses on the initial concept-development phase, the proposed safety assurance framework can also be used during implementation, and subsequent deployment and operation phases."
  },
  {
    "title": "PrivacyXray: Detecting Privacy Breaches in LLMs through Semantic Consistency and Probability Certainty",
    "url": "http://arxiv.org/abs/2506.19563v1",
    "arxiv_id": "2506.19563v1",
    "authors": [
      "Jinwen He",
      "Yiyang Lu",
      "Zijin Lin",
      "Kai Chen",
      "Yue Zhao"
    ],
    "published": "2025-06-24T12:22:59+00:00",
    "summary": "Large Language Models (LLMs) are widely used in sensitive domains, including healthcare, finance, and legal services, raising concerns about potential private information leaks during inference. Privacy extraction attacks, such as jailbreaking, expose vulnerabilities in LLMs by crafting inputs that force the models to output sensitive information. However, these attacks cannot verify whether the extracted private information is accurate, as no public datasets exist for cross-validation, leaving a critical gap in private information detection during inference. To address this, we propose PrivacyXray, a novel framework detecting privacy breaches by analyzing LLM inner states. Our analysis reveals that LLMs exhibit higher semantic coherence and probabilistic certainty when generating correct private outputs. Based on this, PrivacyXray detects privacy breaches using four metrics: intra-layer and inter-layer semantic similarity, token-level and sentence-level probability distributions. PrivacyXray addresses critical challenges in private information detection by overcoming the lack of open-source private datasets and eliminating reliance on external data for validation. It achieves this through the synthesis of realistic private data and a detection mechanism based on the inner states of LLMs. Experiments show that PrivacyXray achieves consistent performance, with an average accuracy of 92.69% across five LLMs. Compared to state-of-the-art methods, PrivacyXray achieves significant improvements, with an average accuracy increase of 20.06%, highlighting its stability and practical utility in real-world applications."
  },
  {
    "title": "A Spline-Based Stress Function Approach for the Principle of Minimum Complementary Energy",
    "url": "http://arxiv.org/abs/2506.19534v1",
    "arxiv_id": "2506.19534v1",
    "authors": [
      "Fabian Key",
      "Lukas Freinberger"
    ],
    "published": "2025-06-24T11:38:11+00:00",
    "summary": "In computational engineering, ensuring the integrity and safety of structures in fields such as aerospace and civil engineering relies on accurate stress prediction. However, analytical methods are limited to simple test cases, and displacement-based finite element methods (FEMs), while commonly used, require a large number of unknowns to achieve high accuracy; stress-based numerical methods have so far failed to provide a simple and effective alternative. This work aims to develop a novel numerical approach that overcomes these limitations by enabling accurate stress prediction with improved flexibility for complex geometries and boundary conditions and fewer degrees of freedom (DOFs). The proposed method is based on a spline-based stress function formulation for the principle of minimum complementary energy, which we apply to plane, linear elastostatics. The method is first validated against an analytical power series solution and then tested on two test cases challenging for current state-of-the-art numerical schemes, a bi-layer cantilever with anisotropic material behavior, and a cantilever with a non-prismatic, parabolic-shaped beam geometry. Results demonstrate that our approach, unlike analytical methods, can be easily applied to general geometries and boundary conditions, and achieves stress accuracy comparable to that reported in the literature for displacement-based FEMs, while requiring significantly fewer DOFs. This novel spline-based stress function approach thus provides an efficient and flexible tool for accurate stress prediction, with promising applications in structural analysis and numerical design."
  },
  {
    "title": "Automatic Posology Structuration : What role for LLMs?",
    "url": "http://arxiv.org/abs/2506.19525v1",
    "arxiv_id": "2506.19525v1",
    "authors": [
      "Natalia Bobkova",
      "Laura Zanella-Calzada",
      "Anyes Tafoughalt",
      "Rapha\u00ebl Teboul",
      "Fran\u00e7ois Plesse",
      "F\u00e9lix Gaschi"
    ],
    "published": "2025-06-24T11:25:21+00:00",
    "summary": "Automatically structuring posology instructions is essential for improving medication safety and enabling clinical decision support. In French prescriptions, these instructions are often ambiguous, irregular, or colloquial, limiting the effectiveness of classic ML pipelines. We explore the use of Large Language Models (LLMs) to convert free-text posologies into structured formats, comparing prompt-based methods and fine-tuning against a \"pre-LLM\" system based on Named Entity Recognition and Linking (NERL). Our results show that while prompting improves performance, only fine-tuned LLMs match the accuracy of the baseline. Through error analysis, we observe complementary strengths: NERL offers structural precision, while LLMs better handle semantic nuances. Based on this, we propose a hybrid pipeline that routes low-confidence cases from NERL (<0.8) to the LLM, selecting outputs based on confidence scores. This strategy achieves 91% structuration accuracy while minimizing latency and compute. Our results show that this hybrid approach improves structuration accuracy while limiting computational cost, offering a scalable solution for real-world clinical use."
  },
  {
    "title": "Visual hallucination detection in large vision-language models via evidential conflict",
    "url": "http://arxiv.org/abs/2506.19513v1",
    "arxiv_id": "2506.19513v1",
    "authors": [
      "Tao Huang",
      "Zhekun Liu",
      "Rui Wang",
      "Yang Zhang",
      "Liping Jing"
    ],
    "published": "2025-06-24T11:03:10+00:00",
    "summary": "Despite the remarkable multimodal capabilities of Large Vision-Language Models (LVLMs), discrepancies often occur between visual inputs and textual outputs--a phenomenon we term visual hallucination. This critical reliability gap poses substantial risks in safety-critical Artificial Intelligence (AI) applications, necessitating a comprehensive evaluation benchmark and effective detection methods. Firstly, we observe that existing visual-centric hallucination benchmarks mainly assess LVLMs from a perception perspective, overlooking hallucinations arising from advanced reasoning capabilities. We develop the Perception-Reasoning Evaluation Hallucination (PRE-HAL) dataset, which enables the systematic evaluation of both perception and reasoning capabilities of LVLMs across multiple visual semantics, such as instances, scenes, and relations. Comprehensive evaluation with this new benchmark exposed more visual vulnerabilities, particularly in the more challenging task of relation reasoning. To address this issue, we propose, to the best of our knowledge, the first Dempster-Shafer theory (DST)-based visual hallucination detection method for LVLMs through uncertainty estimation. This method aims to efficiently capture the degree of conflict in high-level features at the model inference phase. Specifically, our approach employs simple mass functions to mitigate the computational complexity of evidence combination on power sets. We conduct an extensive evaluation of state-of-the-art LVLMs, LLaVA-v1.5, mPLUG-Owl2 and mPLUG-Owl3, with the new PRE-HAL benchmark. Experimental results indicate that our method outperforms five baseline uncertainty metrics, achieving average AUROC improvements of 4%, 10%, and 7% across three LVLMs. Our code is available at https://github.com/HT86159/Evidential-Conflict."
  },
  {
    "title": "5 Days, 5 Stories: Using Technology to Promote Empathy in the Workplace",
    "url": "http://arxiv.org/abs/2506.19495v1",
    "arxiv_id": "2506.19495v1",
    "authors": [
      "Russell Beale",
      "Eugenia Sergueeva"
    ],
    "published": "2025-06-24T10:32:15+00:00",
    "summary": "Empathy is widely recognized as a vital attribute for effective collaboration and communication in the workplace, yet developing empathic skills and fostering it among colleagues remains a challenge. This study explores the potential of a collaborative digital storytelling platform - In Your Shoes - designed to promote empathic listening and interpersonal understanding through the structured exchange of personal narratives. A one-week intervention was conducted with employees from multiple organizations using the platform. Employing a mixed methods approach, we assessed quantitative changes in empathy using the Empathy Quotient (EQ) and qualitatively analyzed participant experiences through grounded theory. While quantitative analysis revealed no statistically significant shift in dispositional empathy, qualitative findings suggested the tool facilitated situational empathy, prompted self-reflection, improved emotional resonance, and enhanced workplace relationships. Participants reported feelings of psychological safety, connection, and, in some cases, therapeutic benefits from sharing and responding to stories. These results highlight the promise of asynchronous, structured narrative-based digital tools for supporting empathic engagement in professional settings, offering insights for the design of emotionally intelligent workplace technologies."
  },
  {
    "title": "A Survey on Soft Robot Adaptability: Implementations, Applications, and Prospects",
    "url": "http://arxiv.org/abs/2506.19397v1",
    "arxiv_id": "2506.19397v1",
    "authors": [
      "Zixi Chen",
      "Di Wu",
      "Qinghua Guan",
      "David Hardman",
      "Federico Renda",
      "Josie Hughes",
      "Thomas George Thuruthel",
      "Cosimo Della Santina",
      "Barbara Mazzolai",
      "Huichan Zhao",
      "Cesare Stefanini"
    ],
    "published": "2025-06-24T07:53:52+00:00",
    "summary": "Soft robots, compared to rigid robots, possess inherent advantages, including higher degrees of freedom, compliance, and enhanced safety, which have contributed to their increasing application across various fields. Among these benefits, adaptability is particularly noteworthy. In this paper, adaptability in soft robots is categorized into external and internal adaptability. External adaptability refers to the robot's ability to adjust, either passively or actively, to variations in environments, object properties, geometries, and task dynamics. Internal adaptability refers to the robot's ability to cope with internal variations, such as manufacturing tolerances or material aging, and to generalize control strategies across different robots. As the field of soft robotics continues to evolve, the significance of adaptability has become increasingly pronounced. In this review, we summarize various approaches to enhancing the adaptability of soft robots, including design, sensing, and control strategies. Additionally, we assess the impact of adaptability on applications such as surgery, wearable devices, locomotion, and manipulation. We also discuss the limitations of soft robotics adaptability and prospective directions for future research. By analyzing adaptability through the lenses of implementation, application, and challenges, this paper aims to provide a comprehensive understanding of this essential characteristic in soft robotics and its implications for diverse applications."
  },
  {
    "title": "Trajectory Prediction in Dynamic Object Tracking: A Critical Study",
    "url": "http://arxiv.org/abs/2506.19341v1",
    "arxiv_id": "2506.19341v1",
    "authors": [
      "Zhongping Dong",
      "Liming Chen",
      "Mohand Tahar Kechadi"
    ],
    "published": "2025-06-24T06:10:01+00:00",
    "summary": "This study provides a detailed analysis of current advancements in dynamic object tracking (DOT) and trajectory prediction (TP) methodologies, including their applications and challenges. It covers various approaches, such as feature-based, segmentation-based, estimation-based, and learning-based methods, evaluating their effectiveness, deployment, and limitations in real-world scenarios. The study highlights the significant impact of these technologies in automotive and autonomous vehicles, surveillance and security, healthcare, and industrial automation, contributing to safety and efficiency. Despite the progress, challenges such as improved generalization, computational efficiency, reduced data dependency, and ethical considerations still exist. The study suggests future research directions to address these challenges, emphasizing the importance of multimodal data integration, semantic information fusion, and developing context-aware systems, along with ethical and privacy-preserving frameworks."
  },
  {
    "title": "MSR-Align: Policy-Grounded Multimodal Alignment for Safety-Aware Reasoning in Vision-Language Models",
    "url": "http://arxiv.org/abs/2506.19257v1",
    "arxiv_id": "2506.19257v1",
    "authors": [
      "Yinan Xia",
      "Yilei Jiang",
      "Yingshui Tan",
      "Xiaoyong Zhu",
      "Xiangyu Yue",
      "Bo Zheng"
    ],
    "published": "2025-06-24T02:37:59+00:00",
    "summary": "Vision-Language Models (VLMs) have achieved remarkable progress in multimodal reasoning tasks through enhanced chain-of-thought capabilities. However, this advancement also introduces novel safety risks, as these models become increasingly vulnerable to harmful multimodal prompts that can trigger unethical or unsafe behaviors. Existing safety alignment approaches, primarily designed for unimodal language models, fall short in addressing the complex and nuanced threats posed by multimodal inputs. Moreover, current safety datasets lack the fine-grained, policy-grounded reasoning required to robustly align reasoning-capable VLMs. In this work, we introduce {MSR-Align}, a high-quality Multimodal Safety Reasoning dataset tailored to bridge this gap. MSR-Align supports fine-grained, deliberative reasoning over standardized safety policies across both vision and text modalities. Our data generation pipeline emphasizes multimodal diversity, policy-grounded reasoning, and rigorous quality filtering using strong multimodal judges. Extensive experiments demonstrate that fine-tuning VLMs on MSR-Align substantially improves robustness against both textual and vision-language jailbreak attacks, while preserving or enhancing general reasoning performance. MSR-Align provides a scalable and effective foundation for advancing the safety alignment of reasoning-capable VLMs. Our dataset is made publicly available at https://huggingface.co/datasets/Leigest/MSR-Align."
  },
  {
    "title": "Robust Behavior Cloning Via Global Lipschitz Regularization",
    "url": "http://arxiv.org/abs/2506.19250v1",
    "arxiv_id": "2506.19250v1",
    "authors": [
      "Shili Wu",
      "Yizhao Jin",
      "Puhua Niu",
      "Aniruddha Datta",
      "Sean B. Andersson"
    ],
    "published": "2025-06-24T02:19:08+00:00",
    "summary": "Behavior Cloning (BC) is an effective imitation learning technique and has even been adopted in some safety-critical domains such as autonomous vehicles. BC trains a policy to mimic the behavior of an expert by using a dataset composed of only state-action pairs demonstrated by the expert, without any additional interaction with the environment. However, During deployment, the policy observations may contain measurement errors or adversarial disturbances. Since the observations may deviate from the true states, they can mislead the agent into making sub-optimal actions. In this work, we use a global Lipschitz regularization approach to enhance the robustness of the learned policy network. We then show that the resulting global Lipschitz property provides a robustness certificate to the policy with respect to different bounded norm perturbations. Then, we propose a way to construct a Lipschitz neural network that ensures the policy robustness. We empirically validate our theory across various environments in Gymnasium. Keywords: Robust Reinforcement Learning; Behavior Cloning; Lipschitz Neural Network"
  },
  {
    "title": "Inference-Time Reward Hacking in Large Language Models",
    "url": "http://arxiv.org/abs/2506.19248v1",
    "arxiv_id": "2506.19248v1",
    "authors": [
      "Hadi Khalaf",
      "Claudio Mayrink Verdun",
      "Alex Oesterling",
      "Himabindu Lakkaraju",
      "Flavio du Pin Calmon"
    ],
    "published": "2025-06-24T02:05:25+00:00",
    "summary": "A common paradigm to improve the performance of large language models is optimizing for a reward model. Reward models assign a numerical score to LLM outputs indicating, for example, which response would likely be preferred by a user or is most aligned with safety goals. However, reward models are never perfect. They inevitably function as proxies for complex desiderata such as correctness, helpfulness, and safety. By overoptimizing for a misspecified reward, we can subvert intended alignment goals and reduce overall performance -- a phenomenon commonly referred to as reward hacking. In this work, we characterize reward hacking in inference-time alignment and demonstrate when and how we can mitigate it by hedging on the proxy reward. We study this phenomenon under Best-of-$n$ (BoN) and Soft-Best-of-$n$ (SBoN), and we introduce Best-of-Poisson (BoP) that provides an efficient, near-exact approximation of the optimal reward-KL divergence policy at inference time. We show that the characteristic pattern of hacking as observed in practice (where the true reward first increases before declining) is an inevitable property of a broad class of inference-time mechanisms, including BoN and BoP. To counter this effect, hedging offers a tactical choice to avoid placing undue confidence in high but potentially misleading proxy reward signals. We introduce HedgeTune, an efficient algorithm to find the optimal inference-time parameter and avoid reward hacking. We demonstrate through experiments that hedging mitigates reward hacking and achieves superior distortion-reward tradeoffs with minimal computational overhead."
  },
  {
    "title": "RecLLM-R1: A Two-Stage Training Paradigm with Reinforcement Learning and Chain-of-Thought v1",
    "url": "http://arxiv.org/abs/2506.19235v1",
    "arxiv_id": "2506.19235v1",
    "authors": [
      "Yu Xie",
      "Xingkai Ren",
      "Ying Qi",
      "Yao Hu",
      "Lianlei Shan"
    ],
    "published": "2025-06-24T01:39:34+00:00",
    "summary": "Traditional recommendation systems often grapple with \"filter bubbles\", underutilization of external knowledge, and a disconnect between model optimization and business policy iteration. To address these limitations, this paper introduces RecLLM-R1, a novel recommendation framework leveraging Large Language Models (LLMs) and drawing inspiration from the DeepSeek R1 methodology. The framework initiates by transforming user profiles, historical interactions, and multi-faceted item attributes into LLM-interpretable natural language prompts through a carefully engineered data construction process. Subsequently, a two-stage training paradigm is employed: the initial stage involves Supervised Fine-Tuning (SFT) to imbue the LLM with fundamental recommendation capabilities. The subsequent stage utilizes Group Relative Policy Optimization (GRPO), a reinforcement learning technique, augmented with a Chain-of-Thought (CoT) mechanism. This stage guides the model through multi-step reasoning and holistic decision-making via a flexibly defined reward function, aiming to concurrently optimize recommendation accuracy, diversity, and other bespoke business objectives. Empirical evaluations on a real-world user behavior dataset from a large-scale social media platform demonstrate that RecLLM-R1 significantly surpasses existing baseline methods across a spectrum of evaluation metrics, including accuracy, diversity, and novelty. It effectively mitigates the filter bubble effect and presents a promising avenue for the integrated optimization of recommendation models and policies under intricate business goals."
  },
  {
    "title": "Command-V: Pasting LLM Behaviors via Activation Profiles",
    "url": "http://arxiv.org/abs/2506.19140v1",
    "arxiv_id": "2506.19140v1",
    "authors": [
      "Barry Wang",
      "Avi Schwarzschild",
      "Alexander Robey",
      "Ali Payani",
      "Charles Fleming",
      "Mingjie Sun",
      "Daphne Ippolito"
    ],
    "published": "2025-06-23T21:21:49+00:00",
    "summary": "Retrofitting large language models (LLMs) with new behaviors typically requires full finetuning or distillation-costly steps that must be repeated for every architecture. In this work, we introduce Command-V, a backpropagation-free behavior transfer method that copies an existing residual activation adapter from a donor model and pastes its effect into a recipient model. Command-V profiles layer activations on a small prompt set, derives linear converters between corresponding layers, and applies the donor intervention in the recipient's activation space. This process does not require access to the original training data and needs minimal compute. In three case studies-safety-refusal enhancement, jailbreak facilitation, and automatic chain-of-thought reasoning--Command-V matches or exceeds the performance of direct finetuning while using orders of magnitude less compute. Our code and data are accessible at https://github.com/GithuBarry/Command-V/."
  },
  {
    "title": "Enhancing Evacuation Safety: Detecting Post-Nuclear Event Radiation Levels in an Urban Area",
    "url": "http://arxiv.org/abs/2506.19044v1",
    "arxiv_id": "2506.19044v1",
    "authors": [
      "Ellis Duncalfe",
      "Milena Radenkovic"
    ],
    "published": "2025-06-23T19:04:49+00:00",
    "summary": "The detonation of an improvised nuclear device (IND) in an urban area would cause catastrophic damage, followed by hazardous radioactive fallout. Timely dissemination of radiation data is crucial for evacuation and casualty reduction. However, conventional communication infrastructure is likely to be severely disrupted. This study designs and builds a pseudorealistic, geospatially and temporally dynamic post-nuclear event (PNE) scenario using the Opportunistic Network Environment (ONE) simulator. It integrates radiation sensing by emergency responders, unmanned aerial vehicles (UAVs), and civilian devices as dynamic nodes within Delay-Tolerant Networks (DTNs). The performance of two DTN routing protocols, Epidemic and PRoPHET, was evaluated across multiple PNE phases. Both protocols achieve high message delivery rates, with PRoPHET exhibiting lower network overhead but higher latency. Findings demonstrate the potential of DTN-based solutions to support emergency response and evacuation safety by ensuring critical radiation data propagation despite severe infrastructure damage."
  },
  {
    "title": "ReasonFlux-PRM: Trajectory-Aware PRMs for Long Chain-of-Thought Reasoning in LLMs",
    "url": "http://arxiv.org/abs/2506.18896v1",
    "arxiv_id": "2506.18896v1",
    "authors": [
      "Jiaru Zou",
      "Ling Yang",
      "Jingwen Gu",
      "Jiahao Qiu",
      "Ke Shen",
      "Jingrui He",
      "Mengdi Wang"
    ],
    "published": "2025-06-23T17:59:02+00:00",
    "summary": "Process Reward Models (PRMs) have recently emerged as a powerful framework for supervising intermediate reasoning steps in large language models (LLMs). Previous PRMs are primarily trained on model final output responses and struggle to evaluate intermediate thinking trajectories robustly, especially in the emerging setting of trajectory-response outputs generated by frontier reasoning models like Deepseek-R1. In this work, we introduce ReasonFlux-PRM, a novel trajectory-aware PRM explicitly designed to evaluate the trajectory-response type of reasoning traces. ReasonFlux-PRM incorporates both step-level and trajectory-level supervision, enabling fine-grained reward assignment aligned with structured chain-of-thought data. We adapt ReasonFlux-PRM to support reward supervision under both offline and online settings, including (i) selecting high-quality model distillation data for downstream supervised fine-tuning of smaller models, (ii) providing dense process-level rewards for policy optimization during reinforcement learning, and (iii) enabling reward-guided Best-of-N test-time scaling. Empirical results on challenging downstream benchmarks such as AIME, MATH500, and GPQA-Diamond demonstrate that ReasonFlux-PRM-7B selects higher quality data than strong PRMs (e.g., Qwen2.5-Math-PRM-72B) and human-curated baselines. Furthermore, our derived ReasonFlux-PRM-7B yields consistent performance improvements, achieving average gains of 12.1% in supervised fine-tuning, 4.5% in reinforcement learning, and 6.3% in test-time scaling. We also release our efficient ReasonFlux-PRM-1.5B for resource-constrained applications and edge deployment. Projects: https://github.com/Gen-Verse/ReasonFlux"
  },
  {
    "title": "OMEGA: Can LLMs Reason Outside the Box in Math? Evaluating Exploratory, Compositional, and Transformative Generalization",
    "url": "http://arxiv.org/abs/2506.18880v1",
    "arxiv_id": "2506.18880v1",
    "authors": [
      "Yiyou Sun",
      "Shawn Hu",
      "Georgia Zhou",
      "Ken Zheng",
      "Hannaneh Hajishirzi",
      "Nouha Dziri",
      "Dawn Song"
    ],
    "published": "2025-06-23T17:51:40+00:00",
    "summary": "Recent large-scale language models (LLMs) with long Chain-of-Thought reasoning-such as DeepSeek-R1-have achieved impressive results on Olympiad-level mathematics benchmarks. However, they often rely on a narrow set of strategies and struggle with problems that require a novel way of thinking. To systematically investigate these limitations, we introduce OMEGA-Out-of-distribution Math Problems Evaluation with 3 Generalization Axes-a controlled yet diverse benchmark designed to evaluate three axes of out-of-distribution generalization, inspired by Boden's typology of creativity: (1) Exploratory-applying known problem solving skills to more complex instances within the same problem domain; (2) Compositional-combining distinct reasoning skills, previously learned in isolation, to solve novel problems that require integrating these skills in new and coherent ways; and (3) Transformative-adopting novel, often unconventional strategies by moving beyond familiar approaches to solve problems more effectively. OMEGA consists of programmatically generated training-test pairs derived from templated problem generators across geometry, number theory, algebra, combinatorics, logic, and puzzles, with solutions verified using symbolic, numerical, or graphical methods. We evaluate frontier (or top-tier) LLMs and observe sharp performance degradation as problem complexity increases. Moreover, we fine-tune the Qwen-series models across all generalization settings and observe notable improvements in exploratory generalization, while compositional generalization remains limited and transformative reasoning shows little to no improvement. By isolating and quantifying these fine-grained failures, OMEGA lays the groundwork for advancing LLMs toward genuine mathematical creativity beyond mechanical proficiency."
  },
  {
    "title": "LongWriter-Zero: Mastering Ultra-Long Text Generation via Reinforcement Learning",
    "url": "http://arxiv.org/abs/2506.18841v1",
    "arxiv_id": "2506.18841v1",
    "authors": [
      "Yuhao Wu",
      "Yushi Bai",
      "Zhiqiang Hu",
      "Roy Ka-Wei Lee",
      "Juanzi Li"
    ],
    "published": "2025-06-23T16:59:02+00:00",
    "summary": "Ultra-long generation by large language models (LLMs) is a widely demanded scenario, yet it remains a significant challenge due to their maximum generation length limit and overall quality degradation as sequence length increases. Previous approaches, exemplified by LongWriter, typically rely on ''teaching'', which involves supervised fine-tuning (SFT) on synthetic long-form outputs. However, this strategy heavily depends on synthetic SFT data, which is difficult and costly to construct, often lacks coherence and consistency, and tends to be overly artificial and structurally monotonous. In this work, we propose an incentivization-based approach that, starting entirely from scratch and without relying on any annotated or synthetic data, leverages reinforcement learning (RL) to foster the emergence of ultra-long, high-quality text generation capabilities in LLMs. We perform RL training starting from a base model, similar to R1-Zero, guiding it to engage in reasoning that facilitates planning and refinement during the writing process. To support this, we employ specialized reward models that steer the LLM towards improved length control, writing quality, and structural formatting. Experimental evaluations show that our LongWriter-Zero model, trained from Qwen2.5-32B, consistently outperforms traditional SFT methods on long-form writing tasks, achieving state-of-the-art results across all metrics on WritingBench and Arena-Write, and even surpassing 100B+ models such as DeepSeek R1 and Qwen3-235B. We open-source our data and model checkpoints under https://huggingface.co/THU-KEG/LongWriter-Zero-32B"
  },
  {
    "title": "Regular Model Checking for Systems with Effectively Regular Reachability Relation",
    "url": "http://arxiv.org/abs/2506.18833v1",
    "arxiv_id": "2506.18833v1",
    "authors": [
      "Javier Esparza",
      "Valentin Krasotin"
    ],
    "published": "2025-06-23T16:51:06+00:00",
    "summary": "Regular model checking is a well-established technique for the verification of regular transition systems (RTS): transition systems whose initial configurations and transition relation can be effectively encoded as regular languages. In 2008, To and Libkin studied RTSs in which the reachability relation (the reflexive and transitive closure of the transition relation) is also effectively regular, and showed that the recurrent reachability problem (whether a regular set $L$ of configurations is reached infinitely often) is polynomial in the size of RTS and the transducer for the reachability relation. We extend the work of To and Libkin by studying the decidability and complexity of verifying almost-sure reachability and recurrent reachability -- that is, whether $L$ is reachable or recurrently reachable w.p. 1. We then apply our results to the more common case in which only a regular overapproximation of the reachability relation is available. In particular, we extend recent complexity results on verifying safety using regular abstraction frameworks -- a technique recently introduced by Czerner, the authors, and Welzel-Mohr -- to liveness and almost-sure properties."
  },
  {
    "title": "ConciseHint: Boosting Efficient Reasoning via Continuous Concise Hints during Generation",
    "url": "http://arxiv.org/abs/2506.18810v1",
    "arxiv_id": "2506.18810v1",
    "authors": [
      "Siao Tang",
      "Xinyin Ma",
      "Gongfan Fang",
      "Xinchao Wang"
    ],
    "published": "2025-06-23T16:20:44+00:00",
    "summary": "Recent advancements in large reasoning models (LRMs) like DeepSeek-R1 and OpenAI o1 series have achieved notable performance enhancements on complex reasoning tasks by scaling up the generation length by Chain-of-Thought (CoT). However, an emerging issue is their inclination to produce excessively verbose reasoning processes, leading to the inefficiency problem. Existing literature on improving efficiency mainly adheres to the before-reasoning paradigms such as prompting and reasoning or fine-tuning and reasoning, but ignores the promising direction of directly encouraging the model to speak concisely by intervening during the generation of reasoning. In order to fill the blank, we propose a framework dubbed ConciseHint, which continuously encourages the reasoning model to speak concisely by injecting the textual hint (manually designed or trained on the concise data) during the token generation of the reasoning process. Besides, ConciseHint is adaptive to the complexity of the query by adaptively adjusting the hint intensity, which ensures it will not undermine model performance. Experiments on the state-of-the-art LRMs, including DeepSeek-R1 and Qwen-3 series, demonstrate that our method can effectively produce concise reasoning processes while maintaining performance well. For instance, we achieve a reduction ratio of 65\\% for the reasoning length on GSM8K benchmark with Qwen-3 4B with nearly no accuracy loss."
  },
  {
    "title": "ConciseHint: Boosting Efficient Reasoning via Continuous Concise Hints during Generation",
    "url": "http://arxiv.org/abs/2506.18810v2",
    "arxiv_id": "2506.18810v2",
    "authors": [
      "Siao Tang",
      "Xinyin Ma",
      "Gongfan Fang",
      "Xinchao Wang"
    ],
    "published": "2025-06-23T16:20:44+00:00",
    "summary": "Recent advancements in large reasoning models (LRMs) like DeepSeek-R1 and OpenAI o1 series have achieved notable performance enhancements on complex reasoning tasks by scaling up the generation length by Chain-of-Thought (CoT). However, an emerging issue is their inclination to produce excessively verbose reasoning processes, leading to the inefficiency problem. Existing literature on improving efficiency mainly adheres to the before-reasoning paradigms such as prompting and reasoning or fine-tuning and reasoning, but ignores the promising direction of directly encouraging the model to speak concisely by intervening during the generation of reasoning. In order to fill the blank, we propose a framework dubbed ConciseHint, which continuously encourages the reasoning model to speak concisely by injecting the textual hint (manually designed or trained on the concise data) during the token generation of the reasoning process. Besides, ConciseHint is adaptive to the complexity of the query by adaptively adjusting the hint intensity, which ensures it will not undermine model performance. Experiments on the state-of-the-art LRMs, including DeepSeek-R1 and Qwen-3 series, demonstrate that our method can effectively produce concise reasoning processes while maintaining performance well. For instance, we achieve a reduction ratio of 65\\% for the reasoning length on GSM8K benchmark with Qwen-3 4B with nearly no accuracy loss."
  },
  {
    "title": "Existing LLMs Are Not Self-Consistent For Simple Tasks",
    "url": "http://arxiv.org/abs/2506.18781v1",
    "arxiv_id": "2506.18781v1",
    "authors": [
      "Zhenru Lin",
      "Jiawen Tao",
      "Yang Yuan",
      "Andrew Chi-Chih Yao"
    ],
    "published": "2025-06-23T15:50:21+00:00",
    "summary": "Large Language Models (LLMs) have grown increasingly powerful, yet ensuring their decisions remain transparent and trustworthy requires self-consistency -- no contradictions in their internal reasoning. Our study reveals that even on simple tasks, such as comparing points on a line or a plane, or reasoning in a family tree, all smaller models are highly inconsistent, and even state-of-the-art models like DeepSeek-R1 and GPT-o4-mini are not fully self-consistent. To quantify and mitigate these inconsistencies, we introduce inconsistency metrics and propose two automated methods -- a graph-based and an energy-based approach. While these fixes provide partial improvements, they also highlight the complexity and importance of self-consistency in building more reliable and interpretable AI. The code and data are available at https://github.com/scorpio-nova/llm-self-consistency."
  },
  {
    "title": "Safety-Aware Optimal Scheduling for Autonomous Masonry Construction using Collaborative Heterogeneous Aerial Robots",
    "url": "http://arxiv.org/abs/2506.18697v1",
    "arxiv_id": "2506.18697v1",
    "authors": [
      "Marios-Nektarios Stamatopoulos",
      "Shridhar Velhal",
      "Avijit Banerjee",
      "George Nikolakopoulos"
    ],
    "published": "2025-06-23T14:34:49+00:00",
    "summary": "This paper presents a novel high-level task planning and optimal coordination framework for autonomous masonry construction, using a team of heterogeneous aerial robotic workers, consisting of agents with separate skills for brick placement and mortar application. This introduces new challenges in scheduling and coordination, particularly due to the mortar curing deadline required for structural bonding and ensuring the safety constraints among UAVs operating in parallel. To address this, an automated pipeline generates the wall construction plan based on the available bricks while identifying static structural dependencies and potential conflicts for safe operation. The proposed framework optimizes UAV task allocation and execution timing by incorporating dynamically coupled precedence deadline constraints that account for the curing process and static structural dependency constraints, while enforcing spatio-temporal constraints to prevent collisions and ensure safety. The primary objective of the scheduler is to minimize the overall construction makespan while minimizing logistics, traveling time between tasks, and the curing time to maintain both adhesion quality and safe workspace separation. The effectiveness of the proposed method in achieving coordinated and time-efficient aerial masonry construction is extensively validated through Gazebo simulated missions. The results demonstrate the framework's capability to streamline UAV operations, ensuring both structural integrity and safety during the construction process."
  },
  {
    "title": "NOVA: Navigation via Object-Centric Visual Autonomy for High-Speed Target Tracking in Unstructured GPS-Denied Environments",
    "url": "http://arxiv.org/abs/2506.18689v1",
    "arxiv_id": "2506.18689v1",
    "authors": [
      "Alessandro Saviolo",
      "Giuseppe Loianno"
    ],
    "published": "2025-06-23T14:28:30+00:00",
    "summary": "Autonomous aerial target tracking in unstructured and GPS-denied environments remains a fundamental challenge in robotics. Many existing methods rely on motion capture systems, pre-mapped scenes, or feature-based localization to ensure safety and control, limiting their deployment in real-world conditions. We introduce NOVA, a fully onboard, object-centric framework that enables robust target tracking and collision-aware navigation using only a stereo camera and an IMU. Rather than constructing a global map or relying on absolute localization, NOVA formulates perception, estimation, and control entirely in the target's reference frame. A tightly integrated stack combines a lightweight object detector with stereo depth completion, followed by histogram-based filtering to infer robust target distances under occlusion and noise. These measurements feed a visual-inertial state estimator that recovers the full 6-DoF pose of the robot relative to the target. A nonlinear model predictive controller (NMPC) plans dynamically feasible trajectories in the target frame. To ensure safety, high-order control barrier functions are constructed online from a compact set of high-risk collision points extracted from depth, enabling real-time obstacle avoidance without maps or dense representations. We validate NOVA across challenging real-world scenarios, including urban mazes, forest trails, and repeated transitions through buildings with intermittent GPS loss and severe lighting changes that disrupt feature-based localization. Each experiment is repeated multiple times under similar conditions to assess resilience, showing consistent and reliable performance. NOVA achieves agile target following at speeds exceeding 50 km/h. These results show that high-speed vision-based tracking is possible in the wild using only onboard sensing, with no reliance on external localization or environment assumptions."
  },
  {
    "title": "ReDit: Reward Dithering for Improved LLM Policy Optimization",
    "url": "http://arxiv.org/abs/2506.18631v1",
    "arxiv_id": "2506.18631v1",
    "authors": [
      "Chenxing Wei",
      "Jiarui Yu",
      "Ying Tiffany He",
      "Hande Dong",
      "Yao Shu",
      "Fei Yu"
    ],
    "published": "2025-06-23T13:36:24+00:00",
    "summary": "DeepSeek-R1 has successfully enhanced Large Language Model (LLM) reasoning capabilities through its rule-based reward system. While it's a ''perfect'' reward system that effectively mitigates reward hacking, such reward functions are often discrete. Our experimental observations suggest that discrete rewards can lead to gradient anomaly, unstable optimization, and slow convergence. To address this issue, we propose ReDit (Reward Dithering), a method that dithers the discrete reward signal by adding simple random noise. With this perturbed reward, exploratory gradients are continuously provided throughout the learning process, enabling smoother gradient updates and accelerating convergence. The injected noise also introduces stochasticity into flat reward regions, encouraging the model to explore novel policies and escape local optima. Experiments across diverse tasks demonstrate the effectiveness and efficiency of ReDit. On average, ReDit achieves performance comparable to vanilla GRPO with only approximately 10% the training steps, and furthermore, still exhibits a 4% performance improvement over vanilla GRPO when trained for a similar duration. Visualizations confirm significant mitigation of gradient issues with ReDit. Moreover, theoretical analyses are provided to further validate these advantages."
  },
  {
    "title": "ReDit: Reward Dithering for Improved LLM Policy Optimization",
    "url": "http://arxiv.org/abs/2506.18631v2",
    "arxiv_id": "2506.18631v2",
    "authors": [
      "Chenxing Wei",
      "Jiarui Yu",
      "Ying Tiffany He",
      "Hande Dong",
      "Yao Shu",
      "Fei Yu"
    ],
    "published": "2025-06-23T13:36:24+00:00",
    "summary": "DeepSeek-R1 has successfully enhanced Large Language Model (LLM) reasoning capabilities through its rule-based reward system. While it's a ''perfect'' reward system that effectively mitigates reward hacking, such reward functions are often discrete. Our experimental observations suggest that discrete rewards can lead to gradient anomaly, unstable optimization, and slow convergence. To address this issue, we propose ReDit (Reward Dithering), a method that dithers the discrete reward signal by adding simple random noise. With this perturbed reward, exploratory gradients are continuously provided throughout the learning process, enabling smoother gradient updates and accelerating convergence. The injected noise also introduces stochasticity into flat reward regions, encouraging the model to explore novel policies and escape local optima. Experiments across diverse tasks demonstrate the effectiveness and efficiency of ReDit. On average, ReDit achieves performance comparable to vanilla GRPO with only approximately 10% the training steps, and furthermore, still exhibits a 4% performance improvement over vanilla GRPO when trained for a similar duration. Visualizations confirm significant mitigation of gradient issues with ReDit. Moreover, theoretical analyses are provided to further validate these advantages."
  },
  {
    "title": "Crowdsourcing eHMI Designs: A Participatory Approach to Autonomous Vehicle-Pedestrian Communication",
    "url": "http://arxiv.org/abs/2506.18605v1",
    "arxiv_id": "2506.18605v1",
    "authors": [
      "Ronald Cumbal",
      "Didem Gurdur Broo",
      "Ginevra Castellano"
    ],
    "published": "2025-06-23T13:05:16+00:00",
    "summary": "As autonomous vehicles become more integrated into shared human environments, effective communication with road users is essential for ensuring safety. While previous research has focused on developing external Human-Machine Interfaces (eHMIs) to facilitate these interactions, we argue that involving users in the early creative stages can help address key challenges in the development of this technology. To explore this, our study adopts a participatory, crowd-sourced approach to gather user-generated ideas for eHMI designs. Participants were first introduced to fundamental eHMI concepts, equipping them to sketch their own design ideas in response to scenarios with varying levels of perceived risk. An initial pre-study with 29 participants showed that while they actively engaged in the process, there was a need to refine task objectives and encourage deeper reflection. To address these challenges, a follow-up study with 50 participants was conducted. The results revealed a strong preference for autonomous vehicles to communicate their awareness and intentions using lights (LEDs and projections), symbols, and text. Participants' sketches prioritized multi-modal communication, directionality, and adaptability to enhance clarity, consistently integrating familiar vehicle elements to improve intuitiveness."
  },
  {
    "title": "Virtual failure assessment diagrams for hydrogen transmission pipelines",
    "url": "http://arxiv.org/abs/2506.18554v1",
    "arxiv_id": "2506.18554v1",
    "authors": [
      "J. Wijnen",
      "J. Parker",
      "M. Gagliano",
      "E. Mart\u00ednez-Pa\u00f1eda"
    ],
    "published": "2025-06-23T12:06:18+00:00",
    "summary": "We combine state-of-the-art thermo-metallurgical welding process modelling with coupled diffusion-elastic-plastic phase field fracture simulations to predict the failure states of hydrogen transport pipelines. This enables quantitatively resolving residual stress states and the role of brittle, hard regions of the weld such as the heat affected zone (HAZ). Failure pressures can be efficiently quantified as a function of asset state (existing defects), materials and weld procedures adopted, and hydrogen purity. Importantly, simulations spanning numerous relevant conditions (defect size and orientations) are used to build \\emph{Virtual} Failure Assessment Diagrams (FADs), enabling a straightforward uptake of this mechanistic approach in fitness-for-service assessment. Model predictions are in very good agreement with FAD approaches from the standards but show that the latter are not conservative when resolving the heterogeneous nature of the weld microstructure. Appropriate, \\emph{mechanistic} FAD safety factors are established that account for the role of residual stresses and hard, brittle weld regions."
  },
  {
    "title": "Security Assessment of DeepSeek and GPT Series Models against Jailbreak Attacks",
    "url": "http://arxiv.org/abs/2506.18543v1",
    "arxiv_id": "2506.18543v1",
    "authors": [
      "Xiaodong Wu",
      "Xiangman Li",
      "Jianbing Ni"
    ],
    "published": "2025-06-23T11:53:31+00:00",
    "summary": "The widespread deployment of large language models (LLMs) has raised critical concerns over their vulnerability to jailbreak attacks, i.e., adversarial prompts that bypass alignment mechanisms and elicit harmful or policy-violating outputs. While proprietary models like GPT-4 have undergone extensive evaluation, the robustness of emerging open-source alternatives such as DeepSeek remains largely underexplored, despite their growing adoption in real-world applications. In this paper, we present the first systematic jailbreak evaluation of DeepSeek-series models, comparing them with GPT-3.5 and GPT-4 using the HarmBench benchmark. We evaluate seven representative attack strategies across 510 harmful behaviors categorized by both function and semantic domain. Our analysis reveals that DeepSeek's Mixture-of-Experts (MoE) architecture introduces routing sparsity that offers selective robustness against optimization-based attacks such as TAP-T, but leads to significantly higher vulnerability under prompt-based and manually engineered attacks. In contrast, GPT-4 Turbo demonstrates stronger and more consistent safety alignment across diverse behaviors, likely due to its dense Transformer design and reinforcement learning from human feedback. Fine-grained behavioral analysis and case studies further show that DeepSeek often routes adversarial prompts to under-aligned expert modules, resulting in inconsistent refusal behaviors. These findings highlight a fundamental trade-off between architectural efficiency and alignment generalization, emphasizing the need for targeted safety tuning and modular alignment strategies to ensure secure deployment of open-source LLMs."
  },
  {
    "title": "SWE-SQL: Illuminating LLM Pathways to Solve User SQL Issues in Real-World Applications",
    "url": "http://arxiv.org/abs/2506.18951v1",
    "arxiv_id": "2506.18951v1",
    "authors": [
      "Jinyang Li",
      "Xiaolong Li",
      "Ge Qu",
      "Per Jacobsson",
      "Bowen Qin",
      "Binyuan Hui",
      "Shuzheng Si",
      "Nan Huo",
      "Xiaohan Xu",
      "Yue Zhang",
      "Ziwei Tang",
      "Yuanshuai Li",
      "Florensia Widjaja",
      "Xintong Zhu",
      "Feige Zhou",
      "Yongfeng Huang",
      "Yannis Papakonstantinou",
      "Fatma Ozcan",
      "Chenhao Ma",
      "Reynold Cheng"
    ],
    "published": "2025-06-23T09:41:37+00:00",
    "summary": "Resolution of complex SQL issues persists as a significant bottleneck in real-world database applications. Current Large Language Models (LLMs), while adept at text-to-SQL translation, have not been rigorously evaluated on the more challenging task of debugging SQL issues. To address this gap, we introduce BIRD-CRITIC, a new SQL issue debugging benchmark comprising 530 PostgreSQL tasks (BIRD-CRITIC-PG) and 570 multi-dialect tasks (BIRD-CRITIC-Multi), distilled from authentic user issues and replayed within new environments to facilitate rigorous evaluation. Baseline evaluations underscore the task's complexity, with the leading reasoning model O3-Mini achieving only 38.87% success rate on BIRD-CRITIC-PG and 33.33% on BIRD-CRITIC-Multi. Meanwhile, advancing open-source models for database tasks is crucial for empowering local development while safeguarding data privacy. Therefore, we present Six-Gym (Sql-fIX-Gym), a training environment for elevating open-source model capabilities for SQL issue debugging. This environment leverages SQL-Rewind strategy, which automatically generates executable issue-solution datasets by reverse-engineering issues from verified SQLs. However, popular trajectory-based fine-tuning methods do not explore substantial supervisory signals. We further propose f-Plan Boosting, which extracts high-level debugging plans from SQL solutions, enabling teacher LLMs to produce 73.7% more successful trajectories for training. We integrate these components into an open-source agent, Bird-Fixer. Based on Qwen-2.5-Coder-14B, Bird-Fixer achieves 38.11% success rate on BIRD-CRITIC-PG and 29.65% on BIRD-CRITIC-Multi, surpassing leading proprietary models such as Claude-3.7-Sonnet and GPT-4.1, marking a significant step toward democratizing sophisticated SQL-debugging capabilities. The leaderboard and source code are available: https://bird-critic.github.io/"
  },
  {
    "title": "How Robust is Model Editing after Fine-Tuning? An Empirical Study on Text-to-Image Diffusion Models",
    "url": "http://arxiv.org/abs/2506.18428v1",
    "arxiv_id": "2506.18428v1",
    "authors": [
      "Feng He",
      "Zhenyang Liu",
      "Marco Valentino",
      "Zhixue Zhao"
    ],
    "published": "2025-06-23T09:10:29+00:00",
    "summary": "Model editing offers a low-cost technique to inject or correct a particular behavior in a pre-trained model without extensive retraining, supporting applications such as factual correction and bias mitigation. Despite this common practice, it remains unknown whether edits persist after fine-tuning or whether they are inadvertently reversed. This question has fundamental practical implications. For example, if fine-tuning removes prior edits, it could serve as a defence mechanism against hidden malicious edits. Vice versa, the unintended removal of edits related to bias mitigation could pose serious safety concerns. We systematically investigate the interaction between model editing and fine-tuning in the context of T2I diffusion models, which are known to exhibit biases and generate inappropriate content. Our study spans two T2I model families (Stable Diffusion and FLUX), two sota editing techniques, and three fine-tuning methods (DreamBooth, LoRA, and DoRA). Through an extensive empirical analysis across diverse editing tasks and evaluation metrics, our findings reveal a trend: edits generally fail to persist through fine-tuning, even when fine-tuning is tangential or unrelated to the edits. Notably, we observe that DoRA exhibits the strongest edit reversal effect. At the same time, among editing methods, UCE demonstrates greater robustness, retaining significantly higher efficacy post-fine-tuning compared to ReFACT. These findings highlight a crucial limitation in current editing methodologies, emphasizing the need for more robust techniques to ensure reliable long-term control and alignment of deployed AI systems. These findings have dual implications for AI safety: they suggest that fine-tuning could serve as a remediation mechanism for malicious edits while simultaneously highlighting the need for re-editing after fine-tuning to maintain beneficial safety and alignment properties."
  },
  {
    "title": "Improving precision of cumulative incidence estimates in randomized controlled trials with external controls",
    "url": "http://arxiv.org/abs/2506.18415v1",
    "arxiv_id": "2506.18415v1",
    "authors": [
      "Zehao Su",
      "Helene C. W. Rytgaard",
      "Henrik Ravn",
      "Frank Eriksson"
    ],
    "published": "2025-06-23T08:53:29+00:00",
    "summary": "Augmenting the control arm in clinical trials with external data can improve statistical power for demonstrating treatment effects. In many time-to-event outcome trials, participants are subject to truncation by death. Direct application of methods for competing risks analysis on the joint data may introduce bias, for example, due to covariate shifts between the populations. In this work, we consider transportability of the conditional cause-specific hazard of the event of interest under the control treatment. Under this assumption, we derive semiparametric efficiency bounds of causal cumulative incidences. This allows for quantification of the theoretical efficiency gain from incorporating the external controls. We propose triply robust estimators that can achieve the efficiency bounds, where the trial controls and external controls are made comparable through time-specific weights in a martingale integral. We conducted a simulation study to show the precision gain of the proposed fusion estimators compared to their counterparts without utilizing external controls. As a real data application, we used two cardiovascular outcome trials conducted to assess the safety of glucagon-like peptide-1 agonists. Incorporating the external controls from one trial into the other, we observed a decrease in the standard error of the treatment effects on adverse non-fatal cardiovascular events with all-cause death as the competing risk."
  },
  {
    "title": "Sequential keypoint density estimator: an overlooked baseline of skeleton-based video anomaly detection",
    "url": "http://arxiv.org/abs/2506.18368v1",
    "arxiv_id": "2506.18368v1",
    "authors": [
      "Anja Deli\u0107",
      "Matej Grci\u0107",
      "Sini\u0161a \u0160egvi\u0107"
    ],
    "published": "2025-06-23T07:55:28+00:00",
    "summary": "Detecting anomalous human behaviour is an important visual task in safety-critical applications such as healthcare monitoring, workplace safety, or public surveillance. In these contexts, abnormalities are often reflected with unusual human poses. Thus, we propose SeeKer, a method for detecting anomalies in sequences of human skeletons. Our method formulates the skeleton sequence density through autoregressive factorization at the keypoint level. The corresponding conditional distributions represent probable keypoint locations given prior skeletal motion. We formulate the joint distribution of the considered skeleton as causal prediction of conditional Gaussians across its constituent keypoints. A skeleton is flagged as anomalous if its keypoint locations surprise our model (i.e. receive a low density). In practice, our anomaly score is a weighted sum of per-keypoint log-conditionals, where the weights account for the confidence of the underlying keypoint detector. Despite its conceptual simplicity, SeeKer surpasses all previous methods on the UBnormal and MSAD-HR datasets while delivering competitive performance on the ShanghaiTech dataset."
  },
  {
    "title": "Robotic Manipulation of a Rotating Chain with Bottom End Fixed",
    "url": "http://arxiv.org/abs/2506.18355v1",
    "arxiv_id": "2506.18355v1",
    "authors": [
      "Qi Jing Chen",
      "Shilin Shan",
      "Quang-Cuong Pham"
    ],
    "published": "2025-06-23T07:31:04+00:00",
    "summary": "This paper studies the problem of using a robot arm to manipulate a uniformly rotating chain with its bottom end fixed. Existing studies have investigated ideal rotational shapes for practical applications, yet they do not discuss how these shapes can be consistently achieved through manipulation planning. Our work presents a manipulation strategy for stable and consistent shape transitions. We find that the configuration space of such a chain is homeomorphic to a three-dimensional cube. Using this property, we suggest a strategy to manipulate the chain into different configurations, specifically from one rotation mode to another, while taking stability and feasibility into consideration. We demonstrate the effectiveness of our strategy in physical experiments by successfully transitioning from rest to the first two rotation modes. The concepts explored in our work has critical applications in ensuring safety and efficiency of drill string and yarn spinning operations."
  },
  {
    "title": "NSFW-Classifier Guided Prompt Sanitization for Safe Text-to-Image Generation",
    "url": "http://arxiv.org/abs/2506.18325v1",
    "arxiv_id": "2506.18325v1",
    "authors": [
      "Yu Xie",
      "Chengjie Zeng",
      "Lingyun Zhang",
      "Yanwei Fu"
    ],
    "published": "2025-06-23T06:17:30+00:00",
    "summary": "The rapid advancement of text-to-image (T2I) models, such as Stable Diffusion, has enhanced their capability to synthesize images from textual prompts. However, this progress also raises significant risks of misuse, including the generation of harmful content (e.g., pornography, violence, discrimination), which contradicts the ethical goals of T2I technology and hinders its sustainable development. Inspired by \"jailbreak\" attacks in large language models, which bypass restrictions through subtle prompt modifications, this paper proposes NSFW-Classifier Guided Prompt Sanitization (PromptSan), a novel approach to detoxify harmful prompts without altering model architecture or degrading generation capability. PromptSan includes two variants: PromptSan-Modify, which iteratively identifies and replaces harmful tokens in input prompts using text NSFW classifiers during inference, and PromptSan-Suffix, which trains an optimized suffix token sequence to neutralize harmful intent while passing both text and image NSFW classifier checks. Extensive experiments demonstrate that PromptSan achieves state-of-the-art performance in reducing harmful content generation across multiple metrics, effectively balancing safety and usability."
  },
  {
    "title": "Supporting Car-Following Behavior through V2V-Based Beyond-Visual-Range Information Display",
    "url": "http://arxiv.org/abs/2506.18308v1",
    "arxiv_id": "2506.18308v1",
    "authors": [
      "Feiqi Gu",
      "Zhixiong Wang",
      "Zhenyu Wang",
      "Dengbo He"
    ],
    "published": "2025-06-23T05:51:01+00:00",
    "summary": "Rear-end collisions constituted a large portion of crashes on the road, despite efforts to mitigate rear-end collisions, such as forward collision warnings. The chance of rear-end collisions is closely related to drivers' car-following (CF) behaviors in the traffic flow. Given that drivers may rely on more than the information of the direct lead vehicle (DLV) when making CF decisions, expanding drivers' perceptual range by providing beyond-visual-range (BVR) information based on vehicle-to-vehicle (V2V) communication may enhance CF safety. Thus, four different human-machine interfaces (HMIs) providing various types of BVR information in CF events were designed, including Brake-HMI showing only brake action of indirect lead vehicles (ILV), Dis-HMI and THW-HMI showing the relative distance and time headway between the ILV and DLV, respectively, and Video-HMI showing the live-stream video of ILV from the perspective of DLV. A driving simulator experiment with 40 participants was conducted to evaluate the impact of BVR-based HMI on driving safety in CF events. We found that, in general, BVR information could improve CF safety without overloading drivers and compromising their visual attention allocation strategies, particularly among novice drivers, by enabling quicker brake responses and increasing time headway and time-to-collision in brake events. The Brake-HMI yielded the safest performance in chain brake events, whereas Video-HMI increased attentional demands without observable benefits. This research provides insights into enabling drivers' BVR perception based on V2V communication to enhance driving safety in CF scenarios."
  },
  {
    "title": "Sharpening the Spear: Adaptive Expert-Guided Adversarial Attack Against DRL-based Autonomous Driving Policies",
    "url": "http://arxiv.org/abs/2506.18304v1",
    "arxiv_id": "2506.18304v1",
    "authors": [
      "Junchao Fan",
      "Xuyang Lei",
      "Xiaolin Chang"
    ],
    "published": "2025-06-23T05:42:49+00:00",
    "summary": "Deep reinforcement learning (DRL) has emerged as a promising paradigm for autonomous driving. However, despite their advanced capabilities, DRL-based policies remain highly vulnerable to adversarial attacks, posing serious safety risks in real-world deployments. Investigating such attacks is crucial for revealing policy vulnerabilities and guiding the development of more robust autonomous systems. While prior attack methods have made notable progress, they still face several challenges: 1) they often rely on high-frequency attacks, yet critical attack opportunities are typically context-dependent and temporally sparse, resulting in inefficient attack patterns; 2) restricting attack frequency can improve efficiency but often results in unstable training due to the adversary's limited exploration. To address these challenges, we propose an adaptive expert-guided adversarial attack method that enhances both the stability and efficiency of attack policy training. Our method first derives an expert policy from successful attack demonstrations using imitation learning, strengthened by an ensemble Mixture-of-Experts architecture for robust generalization across scenarios. This expert policy then guides a DRL-based adversary through a KL-divergence regularization term. Due to the diversity of scenarios, expert policies may be imperfect. To address this, we further introduce a performance-aware annealing strategy that gradually reduces reliance on the expert as the adversary improves. Extensive experiments demonstrate that our method achieves outperforms existing approaches in terms of collision rate, attack efficiency, and training stability, especially in cases where the expert policy is sub-optimal."
  },
  {
    "title": "Improvement on LiDAR-Camera Calibration Using Square Targets",
    "url": "http://arxiv.org/abs/2506.18294v1",
    "arxiv_id": "2506.18294v1",
    "authors": [
      "Zhongyuan Li",
      "Honggang Gou",
      "Ping Li",
      "Jiaotong Guo",
      "Mao Ye"
    ],
    "published": "2025-06-23T05:04:17+00:00",
    "summary": "Precise sensor calibration is critical for autonomous vehicles as a prerequisite for perception algorithms to function properly. Rotation error of one degree can translate to position error of meters in target object detection at large distance, leading to improper reaction of the system or even safety related issues. Many methods for multi-sensor calibration have been proposed. However, there are very few work that comprehensively consider the challenges of the calibration procedure when applied to factory manufacturing pipeline or after-sales service scenarios. In this work, we introduce a fully automatic LiDAR-camera extrinsic calibration algorithm based on targets that is fast, easy to deploy and robust to sensor noises such as missing data. The core of the method include: (1) an automatic multi-stage LiDAR board detection pipeline using only geometry information with no specific material requirement; (2) a fast coarse extrinsic parameter search mechanism that is robust to initial extrinsic errors; (3) a direct optimization algorithm that is robust to sensor noises. We validate the effectiveness of our methods through experiments on data captured in real world scenarios."
  },
  {
    "title": "Shrinking the Generation-Verification Gap with Weak Verifiers",
    "url": "http://arxiv.org/abs/2506.18203v1",
    "arxiv_id": "2506.18203v1",
    "authors": [
      "Jon Saad-Falcon",
      "E. Kelly Buchanan",
      "Mayee F. Chen",
      "Tzu-Heng Huang",
      "Brendan McLaughlin",
      "Tanvir Bhathal",
      "Shang Zhu",
      "Ben Athiwaratkun",
      "Frederic Sala",
      "Scott Linderman",
      "Azalia Mirhoseini",
      "Christopher R\u00e9"
    ],
    "published": "2025-06-22T23:38:15+00:00",
    "summary": "Verifiers can improve language model capabilities by scoring and ranking responses from generated candidates. Currently, high-quality verifiers are either unscalable (e.g., humans) or limited in utility (e.g., tools like Lean). While LM judges and reward models have become broadly useful as general-purpose verifiers, a significant performance gap remains between them and oracle verifiers (verifiers with perfect accuracy). To help close this gap, we introduce Weaver, a framework for designing a strong verifier by combining multiple weak, imperfect verifiers. We find weighted ensembles of verifiers, which typically require learning from labeled data, significantly outperform unweighted combinations due to differences in verifier accuracies. To reduce dependency on labeled data, Weaver leverages weak supervision to estimate each verifier's accuracy and combines outputs into a unified score that better reflects true response quality. However, directly applying weak supervision algorithms poses challenges, including inconsistent verifier output formats and handling low-quality verifiers. Weaver addresses these using dataset statistics to normalize outputs and filter specific verifiers. We study Weaver's effectiveness in test-time repeated sampling, where a model generates multiple candidate responses and selects one. Our evaluations show Weaver significantly improves over Pass@1-performance when selecting the first candidate-across reasoning and math tasks, achieving o3-mini-level accuracy with Llama 3.3 70B Instruct as generator, and an ensemble of 70B or smaller judge and reward models as verifiers (87.7% average). This gain mirrors the jump between GPT-4o and o3-mini (69.0% vs. 86.7%), which required extensive finetuning and post-training. To reduce computational costs of verifier ensembles, we train a 400M cross-encoder using Weaver's combined output scores."
  },
  {
    "title": "CareLab at #SMM4H-HeaRD 2025: Insomnia Detection and Food Safety Event Extraction with Domain-Aware Transformers",
    "url": "http://arxiv.org/abs/2506.18185v1",
    "arxiv_id": "2506.18185v1",
    "authors": [
      "Zihan Liang",
      "Ziwen Pan",
      "Sumon Kanti Dey",
      "Azra Ismail"
    ],
    "published": "2025-06-22T21:56:59+00:00",
    "summary": "This paper presents our system for the SMM4H-HeaRD 2025 shared tasks, specifically Task 4 (Subtasks 1, 2a, and 2b) and Task 5 (Subtasks 1 and 2). Task 4 focused on detecting mentions of insomnia in clinical notes, while Task 5 addressed the extraction of food safety events from news articles. We participated in all subtasks and report key findings across them, with particular emphasis on Task 5 Subtask 1, where our system achieved strong performance-securing first place with an F1 score of 0.958 on the test set. To attain this result, we employed encoder-based models (e.g., RoBERTa), alongside GPT-4 for data augmentation. This paper outlines our approach, including preprocessing, model architecture, and subtask-specific adaptations"
  },
  {
    "title": "Reasoning about Uncertainty: Do Reasoning Models Know When They Don't Know?",
    "url": "http://arxiv.org/abs/2506.18183v1",
    "arxiv_id": "2506.18183v1",
    "authors": [
      "Zhiting Mei",
      "Christina Zhang",
      "Tenny Yin",
      "Justin Lidard",
      "Ola Shorinwa",
      "Anirudha Majumdar"
    ],
    "published": "2025-06-22T21:46:42+00:00",
    "summary": "Reasoning language models have set state-of-the-art (SOTA) records on many challenging benchmarks, enabled by multi-step reasoning induced using reinforcement learning. However, like previous language models, reasoning models are prone to generating confident, plausible responses that are incorrect (hallucinations). Knowing when and how much to trust these models is critical to the safe deployment of reasoning models in real-world applications. To this end, we explore uncertainty quantification of reasoning models in this work. Specifically, we ask three fundamental questions: First, are reasoning models well-calibrated? Second, does deeper reasoning improve model calibration? Finally, inspired by humans' innate ability to double-check their thought processes to verify the validity of their answers and their confidence, we ask: can reasoning models improve their calibration by explicitly reasoning about their chain-of-thought traces? We introduce introspective uncertainty quantification (UQ) to explore this direction. In extensive evaluations on SOTA reasoning models across a broad range of benchmarks, we find that reasoning models: (i) are typically overconfident, with self-verbalized confidence estimates often greater than 85% particularly for incorrect responses, (ii) become even more overconfident with deeper reasoning, and (iii) can become better calibrated through introspection (e.g., o3-Mini and DeepSeek R1) but not uniformly (e.g., Claude 3.7 Sonnet becomes more poorly calibrated). Lastly, we conclude with important research directions to design necessary UQ benchmarks and improve the calibration of reasoning models."
  },
  {
    "title": "Integrating LLMs and Digital Twins for Adaptive Multi-Robot Task Allocation in Construction",
    "url": "http://arxiv.org/abs/2506.18178v1",
    "arxiv_id": "2506.18178v1",
    "authors": [
      "Min Deng",
      "Bo Fu",
      "Lingyao Li",
      "Xi Wang"
    ],
    "published": "2025-06-22T21:22:07+00:00",
    "summary": "Multi-robot systems are emerging as a promising solution to the growing demand for productivity, safety, and adaptability across industrial sectors. However, effectively coordinating multiple robots in dynamic and uncertain environments, such as construction sites, remains a challenge, particularly due to unpredictable factors like material delays, unexpected site conditions, and weather-induced disruptions. To address these challenges, this study proposes an adaptive task allocation framework that strategically leverages the synergistic potential of Digital Twins, Integer Programming (IP), and Large Language Models (LLMs). The multi-robot task allocation problem is formally defined and solved using an IP model that accounts for task dependencies, robot heterogeneity, scheduling constraints, and re-planning requirements. A mechanism for narrative-driven schedule adaptation is introduced, in which unstructured natural language inputs are interpreted by an LLM, and optimization constraints are autonomously updated, enabling human-in-the-loop flexibility without manual coding. A digital twin-based system has been developed to enable real-time synchronization between physical operations and their digital representations. This closed-loop feedback framework ensures that the system remains dynamic and responsive to ongoing changes on site. A case study demonstrates both the computational efficiency of the optimization algorithm and the reasoning performance of several LLMs, with top-performing models achieving over 97% accuracy in constraint and parameter extraction. The results confirm the practicality, adaptability, and cross-domain applicability of the proposed methods."
  },
  {
    "title": "Understanding Reasoning in Thinking Language Models via Steering Vectors",
    "url": "http://arxiv.org/abs/2506.18167v1",
    "arxiv_id": "2506.18167v1",
    "authors": [
      "Constantin Venhoff",
      "Iv\u00e1n Arcuschin",
      "Philip Torr",
      "Arthur Conmy",
      "Neel Nanda"
    ],
    "published": "2025-06-22T20:45:26+00:00",
    "summary": "Recent advances in large language models (LLMs) have led to the development of thinking language models that generate extensive internal reasoning chains before producing responses. While these models achieve improved performance, controlling their reasoning processes remains challenging. This work presents a steering approach for thinking LLMs by analyzing and manipulating specific reasoning behaviors in DeepSeek-R1-Distill models. Through a systematic experiment on 500 tasks across 10 diverse categories, we identify several reasoning behaviors exhibited by thinking models, including expressing uncertainty, generating examples for hypothesis validation, and backtracking in reasoning chains. We demonstrate that these behaviors are mediated by linear directions in the model's activation space and can be controlled using steering vectors. By extracting and applying these vectors, we provide a method to modulate specific aspects of the model's reasoning process, such as its tendency to backtrack or express uncertainty. Our approach offers practical tools for steering reasoning processes in thinking models in a controlled and interpretable manner. We validate our steering method using two DeepSeek-R1-Distill models, demonstrating consistent control across different model architectures."
  },
  {
    "title": "Understanding Reasoning in Thinking Language Models via Steering Vectors",
    "url": "http://arxiv.org/abs/2506.18167v2",
    "arxiv_id": "2506.18167v2",
    "authors": [
      "Constantin Venhoff",
      "Iv\u00e1n Arcuschin",
      "Philip Torr",
      "Arthur Conmy",
      "Neel Nanda"
    ],
    "published": "2025-06-22T20:45:26+00:00",
    "summary": "Recent advances in large language models (LLMs) have led to the development of thinking language models that generate extensive internal reasoning chains before producing responses. While these models achieve improved performance, controlling their reasoning processes remains challenging. This work presents a steering approach for thinking LLMs by analyzing and manipulating specific reasoning behaviors in DeepSeek-R1-Distill models. Through a systematic experiment on 500 tasks across 10 diverse categories, we identify several reasoning behaviors exhibited by thinking models, including expressing uncertainty, generating examples for hypothesis validation, and backtracking in reasoning chains. We demonstrate that these behaviors are mediated by linear directions in the model's activation space and can be controlled using steering vectors. By extracting and applying these vectors, we provide a method to modulate specific aspects of the model's reasoning process, such as its tendency to backtrack or express uncertainty. Our approach offers practical tools for steering reasoning processes in thinking models in a controlled and interpretable manner. We validate our steering method using three DeepSeek-R1-Distill models, demonstrating consistent control across different model architectures."
  },
  {
    "title": "Pitfalls of Conformal Predictions for Medical Image Classification",
    "url": "http://arxiv.org/abs/2506.18162v1",
    "arxiv_id": "2506.18162v1",
    "authors": [
      "Hendrik Mehrtens",
      "Tabea Bucher",
      "Titus J. Brinker"
    ],
    "published": "2025-06-22T20:33:38+00:00",
    "summary": "Reliable uncertainty estimation is one of the major challenges for medical classification tasks. While many approaches have been proposed, recently the statistical framework of conformal predictions has gained a lot of attention, due to its ability to provide provable calibration guarantees. Nonetheless, the application of conformal predictions in safety-critical areas such as medicine comes with pitfalls, limitations and assumptions that practitioners need to be aware of. We demonstrate through examples from dermatology and histopathology that conformal predictions are unreliable under distributional shifts in input and label variables. Additionally, conformal predictions should not be used for selecting predictions to improve accuracy and are not reliable for subsets of the data, such as individual classes or patient attributes. Moreover, in classification settings with a small number of classes, which are common in medical image classification tasks, conformal predictions have limited practical value."
  },
  {
    "title": "AI Through the Human Lens: Investigating Cognitive Theories in Machine Psychology",
    "url": "http://arxiv.org/abs/2506.18156v1",
    "arxiv_id": "2506.18156v1",
    "authors": [
      "Akash Kundu",
      "Rishika Goswami"
    ],
    "published": "2025-06-22T19:58:19+00:00",
    "summary": "We investigate whether Large Language Models (LLMs) exhibit human-like cognitive patterns under four established frameworks from psychology: Thematic Apperception Test (TAT), Framing Bias, Moral Foundations Theory (MFT), and Cognitive Dissonance. We evaluated several proprietary and open-source models using structured prompts and automated scoring. Our findings reveal that these models often produce coherent narratives, show susceptibility to positive framing, exhibit moral judgments aligned with Liberty/Oppression concerns, and demonstrate self-contradictions tempered by extensive rationalization. Such behaviors mirror human cognitive tendencies yet are shaped by their training data and alignment methods. We discuss the implications for AI transparency, ethical deployment, and future work that bridges cognitive psychology and AI safety"
  },
  {
    "title": "Aggregated Individual Reporting for Post-Deployment Evaluation",
    "url": "http://arxiv.org/abs/2506.18133v1",
    "arxiv_id": "2506.18133v1",
    "authors": [
      "Jessica Dai",
      "Inioluwa Deborah Raji",
      "Benjamin Recht",
      "Irene Y. Chen"
    ],
    "published": "2025-06-22T18:36:13+00:00",
    "summary": "The need for developing model evaluations beyond static benchmarking, especially in the post-deployment phase, is now well-understood. At the same time, concerns about the concentration of power in deployed AI systems have sparked a keen interest in 'democratic' or 'public' AI. In this work, we bring these two ideas together by proposing mechanisms for aggregated individual reporting (AIR), a framework for post-deployment evaluation that relies on individual reports from the public. An AIR mechanism allows those who interact with a specific, deployed (AI) system to report when they feel that they may have experienced something problematic; these reports are then aggregated over time, with the goal of evaluating the relevant system in a fine-grained manner. This position paper argues that individual experiences should be understood as an integral part of post-deployment evaluation, and that the scope of our proposed aggregated individual reporting mechanism is a practical path to that end. On the one hand, individual reporting can identify substantively novel insights about safety and performance; on the other, aggregation can be uniquely useful for informing action. From a normative perspective, the post-deployment phase completes a missing piece in the conversation about 'democratic' AI. As a pathway to implementation, we provide a workflow of concrete design decisions and pointers to areas requiring further research and methodological development."
  },
  {
    "title": "$\u03c6^{\\infty}$: Clause Purification, Embedding Realignment, and the Total Suppression of the Em Dash in Autoregressive Language Models",
    "url": "http://arxiv.org/abs/2506.18129v1",
    "arxiv_id": "2506.18129v1",
    "authors": [
      "Bugra Kilictas",
      "Faruk Alpay"
    ],
    "published": "2025-06-22T18:27:39+00:00",
    "summary": "We identify a critical vulnerability in autoregressive transformer language models where the em dash token induces recursive semantic drift, leading to clause boundary hallucination and embedding space entanglement. Through formal analysis of token-level perturbations in semantic lattices, we demonstrate that em dash insertion fundamentally alters the model's latent representations, causing compounding errors in long-form generation. We propose a novel solution combining symbolic clause purification via the phi-infinity operator with targeted embedding matrix realignment. Our approach enables total suppression of problematic tokens without requiring model retraining, while preserving semantic coherence through fixed-point convergence guarantees. Experimental validation shows significant improvements in generation consistency and topic maintenance. This work establishes a general framework for identifying and mitigating token-level vulnerabilities in foundation models, with immediate implications for AI safety, model alignment, and robust deployment of large language models in production environments. The methodology extends beyond punctuation to address broader classes of recursive instabilities in neural text generation systems."
  },
  {
    "title": "Distributionally robust minimization in meta-learning for system identification",
    "url": "http://arxiv.org/abs/2506.18074v1",
    "arxiv_id": "2506.18074v1",
    "authors": [
      "Matteo Rufolo",
      "Dario Piga",
      "Marco Forgione"
    ],
    "published": "2025-06-22T15:41:22+00:00",
    "summary": "Meta learning aims at learning how to solve tasks, and thus it allows to estimate models that can be quickly adapted to new scenarios. This work explores distributionally robust minimization in meta learning for system identification. Standard meta learning approaches optimize the expected loss, overlooking task variability. We use an alternative approach, adopting a distributionally robust optimization paradigm that prioritizes high-loss tasks, enhancing performance in worst-case scenarios. Evaluated on a meta model trained on a class of synthetic dynamical systems and tested in both in-distribution and out-of-distribution settings, the proposed approach allows to reduce failures in safety-critical applications."
  },
  {
    "title": "Bird's-eye view safety monitoring for the construction top under the tower crane",
    "url": "http://arxiv.org/abs/2506.18938v1",
    "arxiv_id": "2506.18938v1",
    "authors": [
      "Yanke Wang",
      "Yu Hin Ng",
      "Haobo Liang",
      "Ching-Wei Chang",
      "Hao Chen"
    ],
    "published": "2025-06-22T12:52:20+00:00",
    "summary": "The tower crane is involving more automated and intelligent operation procedure, and importantly, the application of automation technologies to the safety issues is imperative ahead of the utilization of any other advances. Among diverse risk management tasks on site, it is essential to protect the human workers on the workspace between the tower crane and constructed building top area (construction top) from the bird's-eye view, especially with Modular Integrated Construction (MiC) lifted. Also, the camera and Light Detection And Ranging (LiDAR) can capture abundant 3D information on site, which is however yet made the best use. Considering the safety protection for humans and tower cranes, we present an AI-based fully automated safety monitoring system for tower crane lifting from the bird's-eye view, surveilling to shield the human workers on the construction top and avoid cranes' collision by alarming the crane operator. The system achieved a 3D data fusion for localization of humans and MiCs by integrating the captured information from camera and LiDAR. The state-of-the-art methods were explored and implemented into our proposed software pipeline coupled with the hardware and display systems. Furthermore, we conducted an analysis of the components in the pipeline to verify the accuracy and effectiveness of the involved methods. The display and visualization on the real site proved that our system can serve as a valuable safety monitoring toolkit on site."
  },
  {
    "title": "When concept-based XAI is imprecise: Do people distinguish between generalisations and misrepresentations?",
    "url": "http://arxiv.org/abs/2506.17936v1",
    "arxiv_id": "2506.17936v1",
    "authors": [
      "Romy M\u00fcller"
    ],
    "published": "2025-06-22T08:07:02+00:00",
    "summary": "Concept-based explainable artificial intelligence (C-XAI) can help reveal the inner representations of AI models. Understanding these representations is particularly important in complex tasks like safety evaluation. Such tasks rely on high-level semantic information (e.g., about actions) to make decisions about abstract categories (e.g., whether a situation is dangerous). In this context, it may desirable for C-XAI concepts to show some variability, suggesting that the AI is capable of generalising beyond the concrete details of a situation. However, it is unclear whether people recognise and appreciate such generalisations and can distinguish them from other, less desirable forms of imprecision. This was investigated in an experimental railway safety scenario. Participants evaluated the performance of a simulated AI that evaluated whether traffic scenes involving people were dangerous. To explain these decisions, the AI provided concepts in the form of similar image snippets. These concepts differed in their match with the classified image, either regarding a highly relevant feature (i.e., relation to tracks) or a less relevant feature (i.e., actions). Contrary to the hypotheses, concepts that generalised over less relevant features led to ratings that were lower than for precisely matching concepts and comparable to concepts that systematically misrepresented these features. Conversely, participants were highly sensitive to imprecisions in relevant features. These findings cast doubts on whether people spontaneously recognise generalisations. Accordingly, they might not be able to infer from C-XAI concepts whether AI models have gained a deeper understanding of complex situations."
  },
  {
    "title": "Safety Certificate against Latent Variables with Partially Unidentifiable Dynamics",
    "url": "http://arxiv.org/abs/2506.17927v1",
    "arxiv_id": "2506.17927v1",
    "authors": [
      "Haoming Jing",
      "Yorie Nakahira"
    ],
    "published": "2025-06-22T07:40:05+00:00",
    "summary": "Many systems contain latent variables that make their dynamics partially unidentifiable or cause distribution shifts in the observed statistics between offline and online data. However, existing control techniques often assume access to complete dynamics or perfect simulators with fully observable states, which are necessary to verify whether the system remains within a safe set (forward invariance) or safe actions are consistently feasible at all times. To address this limitation, we propose a technique for designing probabilistic safety certificates for systems with latent variables. A key technical enabler is the formulation of invariance conditions in probability space, which can be constructed using observed statistics in the presence of distribution shifts due to latent variables. We use this invariance condition to construct a safety certificate that can be implemented efficiently in real-time control. The proposed safety certificate can continuously find feasible actions that control long-term risk to stay within tolerance. Stochastic safe control and (causal) reinforcement learning have been studied in isolation until now. To the best of our knowledge, the proposed work is the first to use causal reinforcement learning to quantify long-term risk for the design of safety certificates. This integration enables safety certificates to efficiently ensure long-term safety in the presence of latent variables. The effectiveness of the proposed safety certificate is demonstrated in numerical simulations."
  },
  {
    "title": "BeltCrack: the First Sequential-image Industrial Conveyor Belt Crack Detection Dataset and Its Baseline with Triple-domain Feature Learning",
    "url": "http://arxiv.org/abs/2506.17892v1",
    "arxiv_id": "2506.17892v1",
    "authors": [
      "Jianghong Huang",
      "Luping Ji",
      "Xin Ma",
      "Mao Ye"
    ],
    "published": "2025-06-22T03:48:51+00:00",
    "summary": "Conveyor belt is a category of important equipments in modern industry, widely applied in production and manufacturing Fields. Its health status is much critical to operation efficiency and safety hazards. Among the factors affecting belt health, crack is often one of the most threatening risks. Currently, considering safety, how to intelligently detect belt cracks is catching an increasing attention. To implement the intelligent detection with machine learning, real crack samples are believed to be necessary. However, existing crack datasets primarily focus on pavement scenarios or synthetic data, no real-world industrial belt crack datasets at all. To propel machine learning advancement in this field, this paper constructs the first sequential-image belt crack detection datasets (BeltCrack14ks and BeltCrack9kd), from real-world factory scenes. Furthermore, to validate usability and effectiveness, we propose a special baseline method with triple-domain (i.e., time-space-frequency) feature hierarchical fusion learning for the two whole-new datasets. Experimental results demonstrate the availability and effectiveness of our dataset. Besides, they also show that our baseline is obviously superior to other similar detection methods. Our datasets and source codes are available at https://github.com/UESTC-nnLab/BeltCrack."
  },
  {
    "title": "Multi-turn Jailbreaking via Global Refinement and Active Fabrication",
    "url": "http://arxiv.org/abs/2506.17881v1",
    "arxiv_id": "2506.17881v1",
    "authors": [
      "Hua Tang",
      "Lingyong Yan",
      "Yukun Zhao",
      "Shuaiqiang Wang",
      "Jizhou Huang",
      "Dawei Yin"
    ],
    "published": "2025-06-22T03:15:05+00:00",
    "summary": "Large Language Models (LLMs) have achieved exceptional performance across a wide range of tasks. However, they still pose significant safety risks due to the potential misuse for malicious purposes. Jailbreaks, which aim to elicit models to generate harmful content, play a critical role in identifying the underlying security threats. Recent jailbreaking primarily focuses on single-turn scenarios, while the more complicated multi-turn scenarios remain underexplored. Moreover, existing multi-turn jailbreaking techniques struggle to adapt to the evolving dynamics of dialogue as the interaction progresses. To address this limitation, we propose a novel multi-turn jailbreaking method that refines the jailbreaking path globally at each interaction. We also actively fabricate model responses to suppress safety-related warnings, thereby increasing the likelihood of eliciting harmful outputs in subsequent questions. Experimental results demonstrate the superior performance of our method compared with existing single-turn and multi-turn jailbreaking techniques across six state-of-the-art LLMs. Our code is publicly available at https://github.com/Ytang520/Multi-Turn_jailbreaking_Global-Refinment_and_Active-Fabrication."
  },
  {
    "title": "Out of Control -- Why Alignment Needs Formal Control Theory (and an Alignment Control Stack)",
    "url": "http://arxiv.org/abs/2506.17846v1",
    "arxiv_id": "2506.17846v1",
    "authors": [
      "Elija Perrier"
    ],
    "published": "2025-06-21T22:45:19+00:00",
    "summary": "This position paper argues that formal optimal control theory should be central to AI alignment research, offering a distinct perspective from prevailing AI safety and security approaches. While recent work in AI safety and mechanistic interpretability has advanced formal methods for alignment, they often fall short of the generalisation required of control frameworks for other technologies. There is also a lack of research into how to render different alignment/control protocols interoperable. We argue that by recasting alignment through principles of formal optimal control and framing alignment in terms of hierarchical stack from physical to socio-technical layers according to which controls may be applied we can develop a better understanding of the potential and limitations for controlling frontier models and agentic AI systems. To this end, we introduce an Alignment Control Stack which sets out a hierarchical layered alignment stack, identifying measurement and control characteristics at each layer and how different layers are formally interoperable. We argue that such analysis is also key to the assurances that will be needed by governments and regulators in order to see AI technologies sustainably benefit the community. Our position is that doing so will bridge the well-established and empirically validated methods of optimal control with practical deployment considerations to create a more comprehensive alignment framework, enhancing how we approach safety and reliability for advanced AI systems."
  },
  {
    "title": "Generative Grasp Detection and Estimation with Concept Learning-based Safety Criteria",
    "url": "http://arxiv.org/abs/2506.17842v1",
    "arxiv_id": "2506.17842v1",
    "authors": [
      "Al-Harith Farhad",
      "Khalil Abuibaid",
      "Christiane Plociennik",
      "Achim Wagner",
      "Martin Ruskowski"
    ],
    "published": "2025-06-21T22:33:25+00:00",
    "summary": "Neural networks are often regarded as universal equations that can estimate any function. This flexibility, however, comes with the drawback of high complexity, rendering these networks into black box models, which is especially relevant in safety-centric applications. To that end, we propose a pipeline for a collaborative robot (Cobot) grasping algorithm that detects relevant tools and generates the optimal grasp. To increase the transparency and reliability of this approach, we integrate an explainable AI method that provides an explanation for the underlying prediction of a model by extracting the learned features and correlating them to corresponding classes from the input. These concepts are then used as additional criteria to ensure the safe handling of work tools. In this paper, we show the consistency of this approach and the criterion for improving the handover position. This approach was tested in an industrial environment, where a camera system was set up to enable a robot to pick up certain tools and objects."
  },
  {
    "title": "Quantum-Hybrid Support Vector Machines for Anomaly Detection in Industrial Control Systems",
    "url": "http://arxiv.org/abs/2506.17824v1",
    "arxiv_id": "2506.17824v1",
    "authors": [
      "Tyler Cultice",
      "Md. Saif Hassan Onim",
      "Annarita Giani",
      "Himanshu Thapliyal"
    ],
    "published": "2025-06-21T21:37:26+00:00",
    "summary": "Sensitive data captured by Industrial Control Systems (ICS) play a large role in the safety and integrity of many critical infrastructures. Detection of anomalous or malicious data, or Anomaly Detection (AD), with machine learning is one of many vital components of cyberphysical security. Quantum kernel-based machine learning methods have shown promise in identifying complex anomalous behavior by leveraging the highly expressive and efficient feature spaces of quantum computing. This study focuses on the parameterization of Quantum Hybrid Support Vector Machines (QSVMs) using three popular datasets from Cyber-Physical Systems (CPS). The results demonstrate that QSVMs outperform traditional classical kernel methods, achieving 13.3% higher F1 scores. Additionally, this research investigates noise using simulations based on real IBMQ hardware, revealing a maximum error of only 0.98% in the QSVM kernels. This error results in an average reduction of 1.57% in classification metrics. Furthermore, the study found that QSVMs show a 91.023% improvement in kernel-target alignment compared to classical methods, indicating a potential \"quantum advantage\" in anomaly detection for critical infrastructures. This effort suggests that QSVMs can provide a substantial advantage in anomaly detection for ICS, ultimately enhancing the security and integrity of critical infrastructures."
  },
  {
    "title": "Implementation and Evaluation of Fast Raft for Hierarchical Consensus",
    "url": "http://arxiv.org/abs/2506.17793v1",
    "arxiv_id": "2506.17793v1",
    "authors": [
      "Anton Melnychuk",
      "Bryan SebaRaj"
    ],
    "published": "2025-06-21T19:13:50+00:00",
    "summary": "We present the first open-source implementation and evaluation of Fast Raft, a hierarchical consensus protocol designed for dynamic, distributed environments. Fast Raft reduces the number of message rounds needed to commit log entries compared to standard Raft by introducing a fast-track mechanism and reducing leader dependence. Our implementation uses gRPC and Kubernetes-based deployment across AWS availability zones. Experimental results demonstrate a throughput improvement and reduced commit latency under low packet loss conditions, while maintaining Raft's safety and liveness guarantees."
  },
  {
    "title": "Residual Connection-Enhanced ConvLSTM for Lithium Dendrite Growth Prediction",
    "url": "http://arxiv.org/abs/2506.17756v1",
    "arxiv_id": "2506.17756v1",
    "authors": [
      "Hosung Lee",
      "Byeongoh Hwang",
      "Dasan Kim",
      "Myungjoo Kang"
    ],
    "published": "2025-06-21T16:27:59+00:00",
    "summary": "The growth of lithium dendrites significantly impacts the performance and safety of rechargeable batteries, leading to short circuits and capacity degradation. This study proposes a Residual Connection-Enhanced ConvLSTM model to predict dendrite growth patterns with improved accuracy and computational efficiency. By integrating residual connections into ConvLSTM, the model mitigates the vanishing gradient problem, enhances feature retention across layers, and effectively captures both localized dendrite growth dynamics and macroscopic battery behavior. The dataset was generated using a phase-field model, simulating dendrite evolution under varying conditions. Experimental results show that the proposed model achieves up to 7% higher accuracy and significantly reduces mean squared error (MSE) compared to conventional ConvLSTM across different voltage conditions (0.1V, 0.3V, 0.5V). This highlights the effectiveness of residual connections in deep spatiotemporal networks for electrochemical system modeling. The proposed approach offers a robust tool for battery diagnostics, potentially aiding in real-time monitoring and optimization of lithium battery performance. Future research can extend this framework to other battery chemistries and integrate it with real-world experimental data for further validation"
  },
  {
    "title": "Full-body WPT: wireless powering with meandered e-textiles",
    "url": "http://arxiv.org/abs/2506.17606v1",
    "arxiv_id": "2506.17606v1",
    "authors": [
      "Ryo Takahashi",
      "Takashi Sato",
      "Wakako Yukita",
      "Tomoyuki Yokota",
      "Takao Someya",
      "Yoshihiro Kawahara"
    ],
    "published": "2025-06-21T06:03:06+00:00",
    "summary": "We present Full-body WPT, wireless power networking around the human body using a meandered textile coil. Unlike traditional inductive systems that emit strong fields into the deep tissue inside the body, the meander coil enables localized generation of strong magnetic field constrained to the skin surface, even when scaled to the size of the human body. Such localized inductive system enhances both safety and efficiency of wireless power around the body. Furthermore, the use of low-loss conductive yarn achieve energy-efficient and lightweight design. We analyze the performance of our design through simulations and experimental prototypes, demonstrating high power transfer efficiency and adaptability to user movement and posture. Our system provides a safe and efficient distributed power network using meandered textile coils integrated into wearable materials, highlighting the potential of body-centric wireless power networking as a foundational layer for ubiquitous health monitoring, augmented reality, and human-machine interaction systems."
  },
  {
    "title": "Risk-Guided Diffusion: Toward Deploying Robot Foundation Models in Space, Where Failure Is Not An Option",
    "url": "http://arxiv.org/abs/2506.17601v1",
    "arxiv_id": "2506.17601v1",
    "authors": [
      "Rohan Thakker",
      "Adarsh Patnaik",
      "Vince Kurtz",
      "Jonas Frey",
      "Jonathan Becktor",
      "Sangwoo Moon",
      "Rob Royce",
      "Marcel Kaufmann",
      "Georgios Georgakis",
      "Pascal Roth",
      "Joel Burdick",
      "Marco Hutter",
      "Shehryar Khattak"
    ],
    "published": "2025-06-21T05:39:04+00:00",
    "summary": "Safe, reliable navigation in extreme, unfamiliar terrain is required for future robotic space exploration missions. Recent generative-AI methods learn semantically aware navigation policies from large, cross-embodiment datasets, but offer limited safety guarantees. Inspired by human cognitive science, we propose a risk-guided diffusion framework that fuses a fast, learned \"System-1\" with a slow, physics-based \"System-2\", sharing computation at both training and inference to couple adaptability with formal safety. Hardware experiments conducted at the NASA JPL's Mars-analog facility, Mars Yard, show that our approach reduces failure rates by up to $4\\times$ while matching the goal-reaching performance of learning-based robotic models by leveraging inference-time compute without any additional training."
  },
  {
    "title": "DRAMA-X: A Fine-grained Intent Prediction and Risk Reasoning Benchmark For Driving",
    "url": "http://arxiv.org/abs/2506.17590v1",
    "arxiv_id": "2506.17590v1",
    "authors": [
      "Mihir Godbole",
      "Xiangbo Gao",
      "Zhengzhong Tu"
    ],
    "published": "2025-06-21T05:01:42+00:00",
    "summary": "Understanding the short-term motion of vulnerable road users (VRUs) like pedestrians and cyclists is critical for safe autonomous driving, especially in urban scenarios with ambiguous or high-risk behaviors. While vision-language models (VLMs) have enabled open-vocabulary perception, their utility for fine-grained intent reasoning remains underexplored. Notably, no existing benchmark evaluates multi-class intent prediction in safety-critical situations, To address this gap, we introduce DRAMA-X, a fine-grained benchmark constructed from the DRAMA dataset via an automated annotation pipeline. DRAMA-X contains 5,686 accident-prone frames labeled with object bounding boxes, a nine-class directional intent taxonomy, binary risk scores, expert-generated action suggestions for the ego vehicle, and descriptive motion summaries. These annotations enable a structured evaluation of four interrelated tasks central to autonomous decision-making: object detection, intent prediction, risk assessment, and action suggestion. As a reference baseline, we propose SGG-Intent, a lightweight, training-free framework that mirrors the ego vehicle's reasoning pipeline. It sequentially generates a scene graph from visual input using VLM-backed detectors, infers intent, assesses risk, and recommends an action using a compositional reasoning stage powered by a large language model. We evaluate a range of recent VLMs, comparing performance across all four DRAMA-X tasks. Our experiments demonstrate that scene-graph-based reasoning enhances intent prediction and risk assessment, especially when contextual cues are explicitly modeled."
  },
  {
    "title": "Predicting Stock Market Crash with Bayesian Generalised Pareto Regression",
    "url": "http://arxiv.org/abs/2506.17549v1",
    "arxiv_id": "2506.17549v1",
    "authors": [
      "Sourish Das"
    ],
    "published": "2025-06-21T02:36:05+00:00",
    "summary": "This paper develops a Bayesian Generalised Pareto Regression (GPR) model to forecast extreme losses in Indian equity markets, with a focus on the Nifty 50 index. Extreme negative returns, though rare, can cause significant financial disruption, and accurate modelling of such events is essential for effective risk management. Traditional Generalised Pareto Distribution (GPD) models often ignore market conditions; in contrast, our framework links the scale parameter to covariates using a log-linear function, allowing tail risk to respond dynamically to market volatility. We examine four prior choices for Bayesian regularisation of regression coefficients: Cauchy, Lasso (Laplace), Ridge (Gaussian), and Zellner's g-prior. Simulation results suggest that the Cauchy prior delivers the best trade-off between predictive accuracy and model simplicity, achieving the lowest RMSE, AIC, and BIC values. Empirically, we apply the model to large negative returns (exceeding 5%) in the Nifty 50 index. Volatility measures from the Nifty 50, S&P 500, and gold are used as covariates to capture both domestic and global risk drivers. Our findings show that tail risk increases significantly with higher market volatility. In particular, both S&P 500 and gold volatilities contribute meaningfully to crash prediction, highlighting global spillover and flight-to-safety effects. The proposed GPR model offers a robust and interpretable approach for tail risk forecasting in emerging markets. It improves upon traditional EVT-based models by incorporating real-time financial indicators, making it useful for practitioners, policymakers, and financial regulators concerned with systemic risk and stress testing."
  },
  {
    "title": "Kaleidoscopic Teaming in Multi Agent Simulations",
    "url": "http://arxiv.org/abs/2506.17514v1",
    "arxiv_id": "2506.17514v1",
    "authors": [
      "Ninareh Mehrabi",
      "Tharindu Kumarage",
      "Kai-Wei Chang",
      "Aram Galstyan",
      "Rahul Gupta"
    ],
    "published": "2025-06-20T23:37:17+00:00",
    "summary": "Warning: This paper contains content that may be inappropriate or offensive.   AI agents have gained significant recent attention due to their autonomous tool usage capabilities and their integration in various real-world applications. This autonomy poses novel challenges for the safety of such systems, both in single- and multi-agent scenarios. We argue that existing red teaming or safety evaluation frameworks fall short in evaluating safety risks in complex behaviors, thought processes and actions taken by agents. Moreover, they fail to consider risks in multi-agent setups where various vulnerabilities can be exposed when agents engage in complex behaviors and interactions with each other. To address this shortcoming, we introduce the term kaleidoscopic teaming which seeks to capture complex and wide range of vulnerabilities that can happen in agents both in single-agent and multi-agent scenarios. We also present a new kaleidoscopic teaming framework that generates a diverse array of scenarios modeling real-world human societies. Our framework evaluates safety of agents in both single-agent and multi-agent setups. In single-agent setup, an agent is given a scenario that it needs to complete using the tools it has access to. In multi-agent setup, multiple agents either compete against or cooperate together to complete a task in the scenario through which we capture existing safety vulnerabilities in agents. We introduce new in-context optimization techniques that can be used in our kaleidoscopic teaming framework to generate better scenarios for safety analysis. Lastly, we present appropriate metrics that can be used along with our framework to measure safety of agents. Utilizing our kaleidoscopic teaming framework, we identify vulnerabilities in various models with respect to their safety in agentic use-cases."
  },
  {
    "title": "Public Perceptions of Autonomous Vehicles: A Survey of Pedestrians and Cyclists in Pittsburgh",
    "url": "http://arxiv.org/abs/2506.17513v1",
    "arxiv_id": "2506.17513v1",
    "authors": [
      "Rudra Y. Bedekar"
    ],
    "published": "2025-06-20T23:27:06+00:00",
    "summary": "This study investigates how autonomous vehicle(AV) technology is perceived by pedestrians and bicyclists in Pittsburgh. Using survey data from over 1200 respondents, the research explores the interplay between demographics, AV interactions, infrastructural readiness, safety perceptions, and trust. Findings highlight demographic divides, infrastructure gaps, and the crucial role of communication and education in AV adoption."
  },
  {
    "title": "Exploring Strategies for Personalized Radiation Therapy Part II Predicting Tumor Drift Patterns with Diffusion Models",
    "url": "http://arxiv.org/abs/2506.17491v1",
    "arxiv_id": "2506.17491v1",
    "authors": [
      "Hao Peng",
      "Steve Jiang",
      "Robert Timmerman"
    ],
    "published": "2025-06-20T21:58:42+00:00",
    "summary": "Radiation therapy outcomes are decided by two key parameters, dose and timing, whose best values vary substantially across patients. This variability is especially critical in the treatment of brain cancer, where fractionated or staged stereotactic radiosurgery improves safety compared to single fraction approaches, but complicates the ability to predict treatment response. To address this challenge, we employ Personalized Ultra-fractionated Stereotactic Adaptive Radiotherapy (PULSAR), a strategy that dynamically adjusts treatment based on how each tumor evolves over time. However, the success of PULSAR and other adaptive approaches depends on predictive tools that can guide early treatment decisions and avoid both overtreatment and undertreatment. However, current radiomics and dosiomics models offer limited insight into the evolving spatial and temporal patterns of tumor response. To overcome these limitations, we propose a novel framework using Denoising Diffusion Implicit Models (DDIM), which learns data-driven mappings from pre to post treatment imaging. In this study, we developed single step and iterative denoising strategies and compared their performance. The results show that diffusion models can effectively simulate patient specific tumor evolution and localize regions associated with treatment response. The proposed strategy provides a promising foundation for modeling heterogeneous treatment response and enabling early, adaptive interventions, paving the way toward more personalized and biologically informed radiotherapy."
  },
  {
    "title": "When Every Millisecond Counts: Real-Time Anomaly Detection via the Multimodal Asynchronous Hybrid Network",
    "url": "http://arxiv.org/abs/2506.17457v1",
    "arxiv_id": "2506.17457v1",
    "authors": [
      "Dong Xiao",
      "Guangyao Chen",
      "Peixi Peng",
      "Yangru Huang",
      "Yifan Zhao",
      "Yongxing Dai",
      "Yonghong Tian"
    ],
    "published": "2025-06-20T19:58:38+00:00",
    "summary": "Anomaly detection is essential for the safety and reliability of autonomous driving systems. Current methods often focus on detection accuracy but neglect response time, which is critical in time-sensitive driving scenarios. In this paper, we introduce real-time anomaly detection for autonomous driving, prioritizing both minimal response time and high accuracy. We propose a novel multimodal asynchronous hybrid network that combines event streams from event cameras with image data from RGB cameras. Our network utilizes the high temporal resolution of event cameras through an asynchronous Graph Neural Network and integrates it with spatial features extracted by a CNN from RGB images. This combination effectively captures both the temporal dynamics and spatial details of the driving environment, enabling swift and precise anomaly detection. Extensive experiments on benchmark datasets show that our approach outperforms existing methods in both accuracy and response time, achieving millisecond-level real-time performance."
  },
  {
    "title": "Keeping Medical AI Healthy: A Review of Detection and Correction Methods for System Degradation",
    "url": "http://arxiv.org/abs/2506.17442v1",
    "arxiv_id": "2506.17442v1",
    "authors": [
      "Hao Guan",
      "David Bates",
      "Li Zhou"
    ],
    "published": "2025-06-20T19:22:07+00:00",
    "summary": "Artificial intelligence (AI) is increasingly integrated into modern healthcare, offering powerful support for clinical decision-making. However, in real-world settings, AI systems may experience performance degradation over time, due to factors such as shifting data distributions, changes in patient characteristics, evolving clinical protocols, and variations in data quality. These factors can compromise model reliability, posing safety concerns and increasing the likelihood of inaccurate predictions or adverse outcomes. This review presents a forward-looking perspective on monitoring and maintaining the \"health\" of AI systems in healthcare. We highlight the urgent need for continuous performance monitoring, early degradation detection, and effective self-correction mechanisms. The paper begins by reviewing common causes of performance degradation at both data and model levels. We then summarize key techniques for detecting data and model drift, followed by an in-depth look at root cause analysis. Correction strategies are further reviewed, ranging from model retraining to test-time adaptation. Our survey spans both traditional machine learning models and state-of-the-art large language models (LLMs), offering insights into their strengths and limitations. Finally, we discuss ongoing technical challenges and propose future research directions. This work aims to guide the development of reliable, robust medical AI systems capable of sustaining safe, long-term deployment in dynamic clinical settings."
  },
  {
    "title": "UProp: Investigating the Uncertainty Propagation of LLMs in Multi-Step Agentic Decision-Making",
    "url": "http://arxiv.org/abs/2506.17419v1",
    "arxiv_id": "2506.17419v1",
    "authors": [
      "Jinhao Duan",
      "James Diffenderfer",
      "Sandeep Madireddy",
      "Tianlong Chen",
      "Bhavya Kailkhura",
      "Kaidi Xu"
    ],
    "published": "2025-06-20T18:34:04+00:00",
    "summary": "As Large Language Models (LLMs) are integrated into safety-critical applications involving sequential decision-making in the real world, it is essential to know when to trust LLM decisions. Existing LLM Uncertainty Quantification (UQ) methods are primarily designed for single-turn question-answering formats, resulting in multi-step decision-making scenarios, e.g., LLM agentic system, being underexplored. In this paper, we introduce a principled, information-theoretic framework that decomposes LLM sequential decision uncertainty into two parts: (i) internal uncertainty intrinsic to the current decision, which is focused on existing UQ methods, and (ii) extrinsic uncertainty, a Mutual-Information (MI) quantity describing how much uncertainty should be inherited from preceding decisions. We then propose UProp, an efficient and effective extrinsic uncertainty estimator that converts the direct estimation of MI to the estimation of Pointwise Mutual Information (PMI) over multiple Trajectory-Dependent Decision Processes (TDPs). UProp is evaluated over extensive multi-step decision-making benchmarks, e.g., AgentBench and HotpotQA, with state-of-the-art LLMs, e.g., GPT-4.1 and DeepSeek-V3. Experimental results demonstrate that UProp significantly outperforms existing single-turn UQ baselines equipped with thoughtful aggregation strategies. Moreover, we provide a comprehensive analysis of UProp, including sampling efficiency, potential applications, and intermediate uncertainty propagation, to demonstrate its effectiveness. Codes will be available at https://github.com/jinhaoduan/UProp."
  },
  {
    "title": "VLN-R1: Vision-Language Navigation via Reinforcement Fine-Tuning",
    "url": "http://arxiv.org/abs/2506.17221v1",
    "arxiv_id": "2506.17221v1",
    "authors": [
      "Zhangyang Qi",
      "Zhixiong Zhang",
      "Yizhou Yu",
      "Jiaqi Wang",
      "Hengshuang Zhao"
    ],
    "published": "2025-06-20T17:59:59+00:00",
    "summary": "Vision-Language Navigation (VLN) is a core challenge in embodied AI, requiring agents to navigate real-world environments using natural language instructions. Current language model-based navigation systems operate on discrete topological graphs, limiting path planning to predefined node connections. We propose VLN-R1, an end-to-end framework that leverages Large Vision-Language Models (LVLM) to directly translate egocentric video streams into continuous navigation actions, adopting GRPO-based training inspired by DeepSeek-R1. To enable effective training, we first construct the VLN-Ego dataset using a 3D simulator, Habitat, and propose Long-Short Memory Sampling to balance historical and current observations. While large language models can supervise complete textual instructions, they lack fine-grained action-level control. Our framework employs a two-stage training approach: a) Supervised fine-tuning (SFT) to align the model's action sequence text predictions with expert demonstrations, followed by b) Reinforcement fine-tuning (RFT) enhanced with a Time-Decayed Reward (TDR) mechanism that strategically weights multi-step future actions. Experimental results show VLN-R1 achieves strong performance on VLN-CE benchmark. VLN-R1 proves LVLMs can drive embodied navigation and enhance task-specific reasoning through data-efficient, reward-driven post-training."
  },
  {
    "title": "Fine-Tuning Lowers Safety and Disrupts Evaluation Consistency",
    "url": "http://arxiv.org/abs/2506.17209v1",
    "arxiv_id": "2506.17209v1",
    "authors": [
      "Kathleen C. Fraser",
      "Hillary Dawkins",
      "Isar Nejadgholi",
      "Svetlana Kiritchenko"
    ],
    "published": "2025-06-20T17:57:12+00:00",
    "summary": "Fine-tuning a general-purpose large language model (LLM) for a specific domain or task has become a routine procedure for ordinary users. However, fine-tuning is known to remove the safety alignment features of the model, even when the fine-tuning data does not contain any harmful content. We consider this to be a critical failure mode of LLMs due to the widespread uptake of fine-tuning, combined with the benign nature of the \"attack\". Most well-intentioned developers are likely unaware that they are deploying an LLM with reduced safety. On the other hand, this known vulnerability can be easily exploited by malicious actors intending to bypass safety guardrails. To make any meaningful progress in mitigating this issue, we first need reliable and reproducible safety evaluations. In this work, we investigate how robust a safety benchmark is to trivial variations in the experimental procedure, and the stochastic nature of LLMs. Our initial experiments expose surprising variance in the results of the safety evaluation, even when seemingly inconsequential changes are made to the fine-tuning setup. Our observations have serious implications for how researchers in this field should report results to enable meaningful comparisons in the future."
  },
  {
    "title": "$^{50}$Cr and $^{53}$Cr neutron capture cross sections measurement at the n_TOF facility at CERN",
    "url": "http://arxiv.org/abs/2506.17161v1",
    "arxiv_id": "2506.17161v1",
    "authors": [
      "P. P\u00e9rez-Maroto",
      "C. Guerrero",
      "A. Casanovas",
      "B. Fern\u00e1ndez",
      "E. Mendoza",
      "V. Alcayne",
      "J. Lerendegui-Marco",
      "C. Domingo-Pardo",
      "J. M. Quesada",
      "R. Capote",
      "the n_TOF Collaboration"
    ],
    "published": "2025-06-20T17:07:59+00:00",
    "summary": "$^{50,53}$Cr are very relevant in criticality safety benchmarks related to nuclear reactors. The discrepancies between the neutron capture cross section evaluations have an important effect on the $k_{eff}$ and $k_{\\infty}$ in criticality benchmarks particularly sensitive to chromium. The $^{50,53}$Cr(n,$\\gamma$) cross sections is to be determined between 1 and 100 keV with an 8-10% accuracy following the requirements of the NEA High Priority Request List (HPRL) to solve the current discrepancies. We have measured the neutron capture cross sections by the time-of-flight technique at the EAR1 experimental area of the n_TOF facility, using an array of four C$_6$D$_6$ detectors with very low neutron sensitivity. The highly-enriched samples used are significantly thinner than in previous measurements, thus minimizing the multiple-scattering effects. We have produced, and analyzed with the R-matrix analysis code SAMMY, capture yields featuring 33 resonances of $^{50}$Cr and 51 of $^{53}$Cr with an accuracy between 5% and 9%, hence fulfilling the requirements made by the NEA. The differential and integral cross sections have been compared to previous data and evaluations. The new $^{50,53}$Cr(n,$\\gamma$) cross sections measured at the CERN n\\TOF facility provide a valuable input for upcoming evaluations, which are deemed necessary given that the results presented herein do not support the increase in both cross sections proposed in the recent INDEN evaluation."
  },
  {
    "title": "Mathematical Proof as a Litmus Test: Revealing Failure Modes of Advanced Large Reasoning Models",
    "url": "http://arxiv.org/abs/2506.17114v1",
    "arxiv_id": "2506.17114v1",
    "authors": [
      "Dadi Guo",
      "Jiayu Liu",
      "Zhiyuan Fan",
      "Zhitao He",
      "Haoran Li",
      "Yumeng Wang",
      "Yi R.",
      "Fung"
    ],
    "published": "2025-06-20T16:14:18+00:00",
    "summary": "Large reasoning models (e.g., R1, o3) have demonstrated remarkable mathematical problem-solving abilities. However, the high reported accuracy of these advanced models on popular datasets, reliance on purely numerical evaluation and potential benchmark leakage, often masks their true reasoning shortcomings. To address this, we propose leveraging the inherent rigor and methodological complexity of mathematical proofs as a diagnostic tool to expose these hidden failures. Specifically, we introduce the RFMDataset (Reveal Failure Modes), a collection of 200 diverse mathematical proof problems, and thoroughly evaluate advanced models' performance on it. Our in-depth analysis of their failures uncovers 10 fine-grained error types, which shows fundamental limitations in current large reasoning models: 1) large reasoning models grapple profoundly with mathematical proofs, with some generating entirely correct proofs for less than 20% of problems and failing even on basic ones; 2) models exhibit a diverse spectrum of reasoning failures, prominently demonstrating the lack of guarantees for the correctness and rigor of single-step reasoning; and 3) models show hallucination and incompleteness during the reasoning process. Our findings reveal that models' self-reflection is insufficient to resolve the current logical dilemmas, necessitating formalized and fine-grained logical training."
  },
  {
    "title": "Are Bias Evaluation Methods Biased ?",
    "url": "http://arxiv.org/abs/2506.17111v1",
    "arxiv_id": "2506.17111v1",
    "authors": [
      "Lina Berrayana",
      "Sean Rooney",
      "Luis Garc\u00e9s-Erice",
      "Ioana Giurgiu"
    ],
    "published": "2025-06-20T16:11:25+00:00",
    "summary": "The creation of benchmarks to evaluate the safety of Large Language Models is one of the key activities within the trusted AI community. These benchmarks allow models to be compared for different aspects of safety such as toxicity, bias, harmful behavior etc. Independent benchmarks adopt different approaches with distinct data sets and evaluation methods. We investigate how robust such benchmarks are by using different approaches to rank a set of representative models for bias and compare how similar are the overall rankings. We show that different but widely used bias evaluations methods result in disparate model rankings. We conclude with recommendations for the community in the usage of such benchmarks."
  },
  {
    "title": "A low-cost plasma source aimed for medical applications using Ar as the working gas",
    "url": "http://arxiv.org/abs/2506.17072v1",
    "arxiv_id": "2506.17072v1",
    "authors": [
      "Fellype do Nascimento",
      "Bruno Henrique da Silva Leal",
      "Konstantin Georgiev Kostov"
    ],
    "published": "2025-06-20T15:21:02+00:00",
    "summary": "Because of the advances in equipment and several studies on medical applications of atmospheric pressure plasmas over the last few years, plasma sources are now being produced for clinical use. It has been demonstrated that plasma sources can be constructed in such a way that their operation and application are safe, and that they present relevant clinical results. However, the manufacturing and operating costs of plasma sources can be decisive for their adoption in medical procedures. In this work, a simple modification of a plasma source that has a low production cost was investigated in order to evaluate the viability of its use for medical applications using argon as the working gas. Without this modification, the plasma source produces high levels of electrical current, which makes it unfeasible for use in medical applications. With the proposed modification, the treatment is no longer carried out using the plasma jet directly, but rather with the post-discharge effluent enriched with reactive oxygen and nitrogen species. As a result, the electrical current reaching the target remains below the safety threshold established for plasma applications in humans. Application tests on water activation indicate that both operating conditions can yield comparable outcomes."
  },
  {
    "title": "From Concepts to Components: Concept-Agnostic Attention Module Discovery in Transformers",
    "url": "http://arxiv.org/abs/2506.17052v1",
    "arxiv_id": "2506.17052v1",
    "authors": [
      "Jingtong Su",
      "Julia Kempe",
      "Karen Ullrich"
    ],
    "published": "2025-06-20T15:04:11+00:00",
    "summary": "Transformers have achieved state-of-the-art performance across language and vision tasks. This success drives the imperative to interpret their internal mechanisms with the dual goals of enhancing performance and improving behavioral control. Attribution methods help advance interpretability by assigning model outputs associated with a target concept to specific model components. Current attribution research primarily studies multi-layer perceptron neurons and addresses relatively simple concepts such as factual associations (e.g., Paris is located in France). This focus tends to overlook the impact of the attention mechanism and lacks a unified approach for analyzing more complex concepts. To fill these gaps, we introduce Scalable Attention Module Discovery (SAMD), a concept-agnostic method for mapping arbitrary, complex concepts to specific attention heads of general transformer models. We accomplish this by representing each concept as a vector, calculating its cosine similarity with each attention head, and selecting the TopK-scoring heads to construct the concept-associated attention module. We then propose Scalar Attention Module Intervention (SAMI), a simple strategy to diminish or amplify the effects of a concept by adjusting the attention module using only a single scalar parameter. Empirically, we demonstrate SAMD on concepts of varying complexity, and visualize the locations of their corresponding modules. Our results demonstrate that module locations remain stable before and after LLM post-training, and confirm prior work on the mechanics of LLM multilingualism. Through SAMI, we facilitate jailbreaking on HarmBench (+72.7%) by diminishing \"safety\" and improve performance on the GSM8K benchmark (+1.6%) by amplifying \"reasoning\". Lastly, we highlight the domain-agnostic nature of our approach by suppressing the image classification accuracy of vision transformers on ImageNet."
  },
  {
    "title": "Enhancing Step-by-Step and Verifiable Medical Reasoning in MLLMs",
    "url": "http://arxiv.org/abs/2506.16962v1",
    "arxiv_id": "2506.16962v1",
    "authors": [
      "Haoran Sun",
      "Yankai Jiang",
      "Wenjie Lou",
      "Yujie Zhang",
      "Wenjie Li",
      "Lilong Wang",
      "Mianxin Liu",
      "Lei Liu",
      "Xiaosong Wang"
    ],
    "published": "2025-06-20T12:51:19+00:00",
    "summary": "Multimodal large language models (MLLMs) have begun to demonstrate robust reasoning capabilities on general tasks, yet their application in the medical domain remains in its early stages. Constructing chain-of-thought (CoT) training data is essential for bolstering the reasoning abilities of medical MLLMs. However, existing approaches exhibit a deficiency in offering a comprehensive framework for searching and evaluating effective reasoning paths towards critical diagnosis. To address this challenge, we propose Mentor-Intern Collaborative Search (MICS), a novel reasoning-path searching scheme to generate rigorous and effective medical CoT data. MICS first leverages mentor models to initialize the reasoning, one step at a time, then prompts each intern model to continue the thinking along those initiated paths, and finally selects the optimal reasoning path according to the overall reasoning performance of multiple intern models. The reasoning performance is determined by an MICS-Score, which assesses the quality of generated reasoning paths. Eventually, we construct MMRP, a multi-task medical reasoning dataset with ranked difficulty, and Chiron-o1, a new medical MLLM devised via a curriculum learning strategy, with robust visual question-answering and generalizable reasoning capabilities. Extensive experiments demonstrate that Chiron-o1, trained on our CoT dataset constructed using MICS, achieves state-of-the-art performance across a list of medical visual question answering and reasoning benchmarks. Codes are available at GitHub - manglu097/Chiron-o1: Enhancing Step-by-Step and Verifiable Medical Reasoning in MLLMs"
  },
  {
    "title": "Orbital Collision: An Indigenously Developed Web-based Space Situational Awareness Platform",
    "url": "http://arxiv.org/abs/2506.16892v1",
    "arxiv_id": "2506.16892v1",
    "authors": [
      "Partha Chowdhury",
      "Harsha M",
      "Ayush Gupta",
      "Sanat K Biswas"
    ],
    "published": "2025-06-20T10:29:41+00:00",
    "summary": "This work presents an indigenous web based platform Orbital Collision (OrCo), created by the Space Systems Laboratory at IIIT Delhi, to enhance Space Situational Awareness (SSA) by predicting collision probabilities of space objects using Two Line Elements (TLE) data. The work highlights the growing challenges of congestion in the Earth's orbital environment, mainly due to space debris and defunct satellites, which increase collision risks. It employs several methods for propagating orbital uncertainty and calculating the collision probability. The performance of the platform is evaluated through accuracy assessments and efficiency metrics, in order to improve the tracking of space objects and ensure the safety of the satellite in congested space."
  },
  {
    "title": "Revolutionizing Validation and Verification: Explainable Testing Methodologies for Intelligent Automotive Decision-Making Systems",
    "url": "http://arxiv.org/abs/2506.16876v1",
    "arxiv_id": "2506.16876v1",
    "authors": [
      "Halit Eris",
      "Stefan Wagner"
    ],
    "published": "2025-06-20T09:55:56+00:00",
    "summary": "Autonomous Driving Systems (ADS) use complex decision-making (DM) models with multimodal sensory inputs, making rigorous validation and verification (V&V) essential for safety and reliability. These models pose challenges in diagnosing failures, tracing anomalies, and maintaining transparency, with current manual testing methods being inefficient and labor-intensive. This vision paper presents a methodology that integrates explainability, transparency, and interpretability into V&V processes. We propose refining V&V requirements through literature reviews and stakeholder input, generating explainable test scenarios via large language models (LLMs), and enabling real-time validation in simulation environments. Our framework includes test oracle, explanation generation, and a test chatbot, with empirical studies planned to evaluate improvements in diagnostic efficiency and transparency. Our goal is to streamline V&V, reduce resources, and build user trust in autonomous technologies."
  },
  {
    "title": "ParkFormer: A Transformer-Based Parking Policy with Goal Embedding and Pedestrian-Aware Control",
    "url": "http://arxiv.org/abs/2506.16856v1",
    "arxiv_id": "2506.16856v1",
    "authors": [
      "Jun Fu",
      "Bin Tian",
      "Haonan Chen",
      "Shi Meng",
      "Tingting Yao"
    ],
    "published": "2025-06-20T09:14:09+00:00",
    "summary": "Autonomous parking plays a vital role in intelligent vehicle systems, particularly in constrained urban environments where high-precision control is required. While traditional rule-based parking systems struggle with environmental uncertainties and lack adaptability in crowded or dynamic scenes, human drivers demonstrate the ability to park intuitively without explicit modeling. Inspired by this observation, we propose a Transformer-based end-to-end framework for autonomous parking that learns from expert demonstrations. The network takes as input surround-view camera images, goal-point representations, ego vehicle motion, and pedestrian trajectories. It outputs discrete control sequences including throttle, braking, steering, and gear selection. A novel cross-attention module integrates BEV features with target points, and a GRU-based pedestrian predictor enhances safety by modeling dynamic obstacles. We validate our method on the CARLA 0.9.14 simulator in both vertical and parallel parking scenarios. Experiments show our model achieves a high success rate of 96.57\\%, with average positional and orientation errors of 0.21 meters and 0.41 degrees, respectively. The ablation studies further demonstrate the effectiveness of key modules such as pedestrian prediction and goal-point attention fusion. The code and dataset will be released at: https://github.com/little-snail-f/ParkFormer."
  },
  {
    "title": "The status quo of fire evacuation drills in nursery schools",
    "url": "http://arxiv.org/abs/2506.16848v1",
    "arxiv_id": "2506.16848v1",
    "authors": [
      "Hana Najmanov\u00e1",
      "Petr Nov\u00e1k",
      "Enrico Ronchi"
    ],
    "published": "2025-06-20T08:52:41+00:00",
    "summary": "Fire drills are a commonly-used training method for improving how people act in emergency situations. This article deals with fire drills in early childhood education facilities and analyses data gathered from an online survey of 1151 Czech nursery schools (23.5% of nursery schools invited to participate, 21.5% of all officially registered nursery schools in Czechia). It provides recommendations for improving the effectiveness of fire drills in nursery schools. Results suggest that regular (typically annual) fire drills are common training practices at these schools, but more frequent fire drills led to significantly fewer issues during evacuation. The most frequent evacuation issue encountered during fire drills, according to our data, was slow movement. Many children needed assistance, notably on stairs; nursery schools located only on the ground floor reported fewer issues during evacuation than other schools. The purpose, design, and implementation of fire drills with our study confirming results from prior studies must be properly integrated into complex and systematic fire safety education programs that specifically reflect the characteristics, needs, and possible limitations of preschool children."
  },
  {
    "title": "Accountability of Robust and Reliable AI-Enabled Systems: A Preliminary Study and Roadmap",
    "url": "http://arxiv.org/abs/2506.16831v1",
    "arxiv_id": "2506.16831v1",
    "authors": [
      "Filippo Scaramuzza",
      "Damian A. Tamburri",
      "Willem-Jan van den Heuvel"
    ],
    "published": "2025-06-20T08:35:11+00:00",
    "summary": "This vision paper presents initial research on assessing the robustness and reliability of AI-enabled systems, and key factors in ensuring their safety and effectiveness in practical applications, including a focus on accountability. By exploring evolving definitions of these concepts and reviewing current literature, the study highlights major challenges and approaches in the field. A case study is used to illustrate real-world applications, emphasizing the need for innovative testing solutions. The incorporation of accountability is crucial for building trust and ensuring responsible AI development. The paper outlines potential future research directions and identifies existing gaps, positioning robustness, reliability, and accountability as vital areas for the development of trustworthy AI systems of the future."
  },
  {
    "title": "MIST: Jailbreaking Black-box Large Language Models via Iterative Semantic Tuning",
    "url": "http://arxiv.org/abs/2506.16792v1",
    "arxiv_id": "2506.16792v1",
    "authors": [
      "Muyang Zheng",
      "Yuanzhi Yao",
      "Changting Lin",
      "Rui Wang",
      "Meng Han"
    ],
    "published": "2025-06-20T07:16:47+00:00",
    "summary": "Despite efforts to align large language models (LLMs) with societal and moral values, these models remain susceptible to jailbreak attacks--methods designed to elicit harmful responses. Jailbreaking black-box LLMs is considered challenging due to the discrete nature of token inputs, restricted access to the target LLM, and limited query budget. To address the issues above, we propose an effective method for jailbreaking black-box large language Models via Iterative Semantic Tuning, named MIST. MIST enables attackers to iteratively refine prompts that preserve the original semantic intent while inducing harmful content. Specifically, to balance semantic similarity with computational efficiency, MIST incorporates two key strategies: sequential synonym search, and its advanced version--order-determining optimization. Extensive experiments across two open-source models and four closed-source models demonstrate that MIST achieves competitive attack success rates and attack transferability compared with other state-of-the-art white-box and black-box jailbreak methods. Additionally, we conduct experiments on computational efficiency to validate the practical viability of MIST."
  },
  {
    "title": "Cross-Modal Obfuscation for Jailbreak Attacks on Large Vision-Language Models",
    "url": "http://arxiv.org/abs/2506.16760v1",
    "arxiv_id": "2506.16760v1",
    "authors": [
      "Lei Jiang",
      "Zixun Zhang",
      "Zizhou Wang",
      "Xiaobing Sun",
      "Zhen Li",
      "Liangli Zhen",
      "Xiaohua Xu"
    ],
    "published": "2025-06-20T05:30:25+00:00",
    "summary": "Large Vision-Language Models (LVLMs) demonstrate exceptional performance across multimodal tasks, yet remain vulnerable to jailbreak attacks that bypass built-in safety mechanisms to elicit restricted content generation. Existing black-box jailbreak methods primarily rely on adversarial textual prompts or image perturbations, yet these approaches are highly detectable by standard content filtering systems and exhibit low query and computational efficiency. In this work, we present Cross-modal Adversarial Multimodal Obfuscation (CAMO), a novel black-box jailbreak attack framework that decomposes malicious prompts into semantically benign visual and textual fragments. By leveraging LVLMs' cross-modal reasoning abilities, CAMO covertly reconstructs harmful instructions through multi-step reasoning, evading conventional detection mechanisms. Our approach supports adjustable reasoning complexity and requires significantly fewer queries than prior attacks, enabling both stealth and efficiency. Comprehensive evaluations conducted on leading LVLMs validate CAMO's effectiveness, showcasing robust performance and strong cross-model transferability. These results underscore significant vulnerabilities in current built-in safety mechanisms, emphasizing an urgent need for advanced, alignment-aware security and safety solutions in vision-language systems."
  },
  {
    "title": "A Scalable Post-Processing Pipeline for Large-Scale Free-Space Multi-Agent Path Planning with PiBT",
    "url": "http://arxiv.org/abs/2506.16748v1",
    "arxiv_id": "2506.16748v1",
    "authors": [
      "Arjo Chakravarty",
      "Michael X. Grey",
      "M. A. Viraj J. Muthugala",
      "Mohan Rajesh Elara"
    ],
    "published": "2025-06-20T04:50:35+00:00",
    "summary": "Free-space multi-agent path planning remains challenging at large scales. Most existing methods either offer optimality guarantees but do not scale beyond a few dozen agents, or rely on grid-world assumptions that do not generalize well to continuous space. In this work, we propose a hybrid, rule-based planning framework that combines Priority Inheritance with Backtracking (PiBT) with a novel safety-aware path smoothing method. Our approach extends PiBT to 8-connected grids and selectively applies string-pulling based smoothing while preserving collision safety through local interaction awareness and a fallback collision resolution step based on Safe Interval Path Planning (SIPP). This design allows us to reduce overall path lengths while maintaining real-time performance. We demonstrate that our method can scale to over 500 agents in large free-space environments, outperforming existing any-angle and optimal methods in terms of runtime, while producing near-optimal trajectories in sparse domains. Our results suggest this framework is a promising building block for scalable, real-time multi-agent navigation in robotics systems operating beyond grid constraints."
  },
  {
    "title": "VLM-Empowered Multi-Mode System for Efficient and Safe Planetary Navigation",
    "url": "http://arxiv.org/abs/2506.16703v1",
    "arxiv_id": "2506.16703v1",
    "authors": [
      "Sinuo Cheng",
      "Ruyi Zhou",
      "Wenhao Feng",
      "Huaiguang Yang",
      "Haibo Gao",
      "Zongquan Deng",
      "Liang Ding"
    ],
    "published": "2025-06-20T02:47:55+00:00",
    "summary": "The increasingly complex and diverse planetary exploration environment requires more adaptable and flexible rover navigation strategy. In this study, we propose a VLM-empowered multi-mode system to achieve efficient while safe autonomous navigation for planetary rovers. Vision-Language Model (VLM) is used to parse scene information by image inputs to achieve a human-level understanding of terrain complexity. Based on the complexity classification, the system switches to the most suitable navigation mode, composing of perception, mapping and planning modules designed for different terrain types, to traverse the terrain ahead before reaching the next waypoint. By integrating the local navigation system with a map server and a global waypoint generation module, the rover is equipped to handle long-distance navigation tasks in complex scenarios. The navigation system is evaluated in various simulation environments. Compared to the single-mode conservative navigation method, our multi-mode system is able to bootstrap the time and energy efficiency in a long-distance traversal with varied type of obstacles, enhancing efficiency by 79.5%, while maintaining its avoidance capabilities against terrain hazards to guarantee rover safety. More system information is shown at https://chengsn1234.github.io/multi-mode-planetary-navigation/."
  },
  {
    "title": "Exploring Traffic Simulation and Cybersecurity Strategies Using Large Language Models",
    "url": "http://arxiv.org/abs/2506.16699v1",
    "arxiv_id": "2506.16699v1",
    "authors": [
      "Lu Gao",
      "Yongxin Liu",
      "Hongyun Chen",
      "Dahai Liu",
      "Yunpeng Zhang",
      "Jingran Sun"
    ],
    "published": "2025-06-20T02:41:23+00:00",
    "summary": "Intelligent Transportation Systems (ITS) are increasingly vulnerable to sophisticated cyberattacks due to their complex, interconnected nature. Ensuring the cybersecurity of these systems is paramount to maintaining road safety and minimizing traffic disruptions. This study presents a novel multi-agent framework leveraging Large Language Models (LLMs) to enhance traffic simulation and cybersecurity testing. The framework automates the creation of traffic scenarios, the design of cyberattack strategies, and the development of defense mechanisms. A case study demonstrates the framework's ability to simulate a cyberattack targeting connected vehicle broadcasts, evaluate its impact, and implement a defense mechanism that significantly mitigates traffic delays. Results show a 10.2 percent increase in travel time during an attack, which is reduced by 3.3 percent with the defense strategy. This research highlights the potential of LLM-driven multi-agent systems in advancing transportation cybersecurity and offers a scalable approach for future research in traffic simulation and cyber defense."
  },
  {
    "title": "PPTP: Performance-Guided Physiological Signal-Based Trust Prediction in Human-Robot Collaboration",
    "url": "http://arxiv.org/abs/2506.16677v1",
    "arxiv_id": "2506.16677v1",
    "authors": [
      "Hao Guo",
      "Wei Fan",
      "Shaohui Liu",
      "Feng Jiang",
      "Chunzhi Yi"
    ],
    "published": "2025-06-20T01:41:09+00:00",
    "summary": "Trust prediction is a key issue in human-robot collaboration, especially in construction scenarios where maintaining appropriate trust calibration is critical for safety and efficiency. This paper introduces the Performance-guided Physiological signal-based Trust Prediction (PPTP), a novel framework designed to improve trust assessment. We designed a human-robot construction scenario with three difficulty levels to induce different trust states. Our approach integrates synchronized multimodal physiological signals (ECG, GSR, and EMG) with collaboration performance evaluation to predict human trust levels. Individual physiological signals are processed using collaboration performance information as guiding cues, leveraging the standardized nature of collaboration performance to compensate for individual variations in physiological responses. Extensive experiments demonstrate the efficacy of our cross-modality fusion method in significantly improving trust classification performance. Our model achieves over 81% accuracy in three-level trust classification, outperforming the best baseline method by 6.7%, and notably reaches 74.3% accuracy in high-resolution seven-level classification, which is a first in trust prediction research. Ablation experiments further validate the superiority of physiological signal processing guided by collaboration performance assessment."
  },
  {
    "title": "LLMs in Coding and their Impact on the Commercial Software Engineering Landscape",
    "url": "http://arxiv.org/abs/2506.16653v1",
    "arxiv_id": "2506.16653v1",
    "authors": [
      "Vladislav Belozerov",
      "Peter J Barclay",
      "Askhan Sami"
    ],
    "published": "2025-06-19T23:43:54+00:00",
    "summary": "Large-language-model coding tools are now mainstream in software engineering. But as these same tools move human effort up the development stack, they present fresh dangers: 10% of real prompts leak private data, 42% of generated snippets hide security flaws, and the models can even ``agree'' with wrong ideas, a trait called sycophancy. We argue that firms must tag and review every AI-generated line of code, keep prompts and outputs inside private or on-premises deployments, obey emerging safety regulations, and add tests that catch sycophantic answers -- so they can gain speed without losing security and accuracy."
  },
  {
    "title": "BIDA: A Bi-level Interaction Decision-making Algorithm for Autonomous Vehicles in Dynamic Traffic Scenarios",
    "url": "http://arxiv.org/abs/2506.16546v1",
    "arxiv_id": "2506.16546v1",
    "authors": [
      "Liyang Yu",
      "Tianyi Wang",
      "Junfeng Jiao",
      "Fengwu Shan",
      "Hongqing Chu",
      "Bingzhao Gao"
    ],
    "published": "2025-06-19T19:03:40+00:00",
    "summary": "In complex real-world traffic environments, autonomous vehicles (AVs) need to interact with other traffic participants while making real-time and safety-critical decisions accordingly. The unpredictability of human behaviors poses significant challenges, particularly in dynamic scenarios, such as multi-lane highways and unsignalized T-intersections. To address this gap, we design a bi-level interaction decision-making algorithm (BIDA) that integrates interactive Monte Carlo tree search (MCTS) with deep reinforcement learning (DRL), aiming to enhance interaction rationality, efficiency and safety of AVs in dynamic key traffic scenarios. Specifically, we adopt three types of DRL algorithms to construct a reliable value network and policy network, which guide the online deduction process of interactive MCTS by assisting in value update and node selection. Then, a dynamic trajectory planner and a trajectory tracking controller are designed and implemented in CARLA to ensure smooth execution of planned maneuvers. Experimental evaluations demonstrate that our BIDA not only enhances interactive deduction and reduces computational costs, but also outperforms other latest benchmarks, which exhibits superior safety, efficiency and interaction rationality under varying traffic conditions."
  },
  {
    "title": "eCAV: An Edge-Assisted Evaluation Platform for Connected Autonomous Vehicles",
    "url": "http://arxiv.org/abs/2506.16535v1",
    "arxiv_id": "2506.16535v1",
    "authors": [
      "Tyler Landle",
      "Jordan Rapp",
      "Dean Blank",
      "Chandramouli Amarnath",
      "Abhijit Chatterjee",
      "Alex Daglis",
      "Umakishore Ramachandran"
    ],
    "published": "2025-06-19T18:32:00+00:00",
    "summary": "As autonomous vehicles edge closer to widespread adoption, enhancing road safety through collision avoidance and minimization of collateral damage becomes imperative. Vehicle-to-everything (V2X) technologies, which include vehicle-to-vehicle (V2V), vehicle-to-infrastructure (V2I), and vehicle-to-cloud (V2C), are being proposed as mechanisms to achieve this safety improvement.   Simulation-based testing is crucial for early-stage evaluation of Connected Autonomous Vehicle (CAV) control systems, offering a safer and more cost-effective alternative to real-world tests. However, simulating large 3D environments with many complex single- and multi-vehicle sensors and controllers is computationally intensive. There is currently no evaluation framework that can effectively evaluate realistic scenarios involving large numbers of autonomous vehicles.   We propose eCAV -- an efficient, modular, and scalable evaluation platform to facilitate both functional validation of algorithmic approaches to increasing road safety, as well as performance prediction of algorithms of various V2X technologies, including a futuristic Vehicle-to-Edge control plane and correspondingly designed control algorithms. eCAV can model up to 256 vehicles running individual control algorithms without perception enabled, which is $8\\times$ more vehicles than what is possible with state-of-the-art alternatives. %faster than state-of-the-art alternatives that can simulate $8\\times$ fewer vehicles. With perception enabled, eCAV simulates up to 64 vehicles with a step time under 800ms, which is $4\\times$ more and $1.5\\times$ faster than the state-of-the-art OpenCDA framework."
  },
  {
    "title": "Robust Reward Modeling via Causal Rubrics",
    "url": "http://arxiv.org/abs/2506.16507v1",
    "arxiv_id": "2506.16507v1",
    "authors": [
      "Pragya Srivastava",
      "Harman Singh",
      "Rahul Madhavan",
      "Gandharv Patil",
      "Sravanti Addepalli",
      "Arun Suggala",
      "Rengarajan Aravamudhan",
      "Soumya Sharma",
      "Anirban Laha",
      "Aravindan Raghuveer",
      "Karthikeyan Shanmugam",
      "Doina Precup"
    ],
    "published": "2025-06-19T17:59:47+00:00",
    "summary": "Reward models (RMs) are fundamental to aligning Large Language Models (LLMs) via human feedback, yet they often suffer from reward hacking. They tend to latch on to superficial or spurious attributes, such as response length or formatting, mistaking these cues learned from correlations in training data for the true causal drivers of quality (e.g., factuality, relevance). This occurs because standard training objectives struggle to disentangle these factors, leading to brittle RMs and misaligned policies. We introduce Crome (Causally Robust Reward Modeling), a novel framework grounded in an explicit causal model designed to mitigate reward hacking. Crome employs the following synthetic targeted augmentations during training: (1) Causal Augmentations, which are pairs that differ along specific causal attributes, to enforce sensitivity along each causal attribute individually, and (2) Neutral Augmentations, which are tie-label pairs varying primarily in spurious attributes, to enforce invariance along spurious attributes. Notably, our augmentations are produced without any knowledge of spurious factors, via answer interventions only along causal rubrics, that are identified by querying an oracle LLM. Empirically, Crome significantly outperforms standard baselines on RewardBench, improving average accuracy by up to 5.4% and achieving gains of up to 13.2% and 7.2% in specific categories. The robustness of Crome is further testified by the consistent gains obtained in a Best-of-N inference setting across increasing N, across various benchmarks, including the popular RewardBench (covering chat, chat-hard, safety, and reasoning tasks), the safety-focused WildGuardTest, and the reasoning-specific GSM8k."
  },
  {
    "title": "Assessing the Influence of Pavement Performance on Road Safety Through Crash Frequency and Severity Analysis",
    "url": "http://arxiv.org/abs/2506.16485v1",
    "arxiv_id": "2506.16485v1",
    "authors": [
      "Prathyush Kumar Reddy Lebaku",
      "Lu Gao",
      "Jingran Sun",
      "Xingju Wang",
      "Xuejian Kang"
    ],
    "published": "2025-06-19T17:28:09+00:00",
    "summary": "Road safety is impacted by a range of factors that can be categorized into human, vehicle, and roadway/environmental elements. This research explores the connection between pavement performance and road safety, particularly in relation to crash frequency and severity, using data from the Iowa Department of Transportation (DOT) for 2022. By merging crash data with pavement inventory data, we conduct a spatial analysis that incorporates the geographical coordinates of crash sites with the conditions of road segments. Statistical methods are applied to compare crash rates and severity across various pavement condition categories. To identify the most influential factors affecting crash rates and severity, we use machine learning models along with negative binomial and ordered probit regression models. The study's key findings reveal that higher speed limits, well-maintained roads, and improved friction scores correlate with lower crash rates, whereas rougher roads and adverse weather conditions are linked to higher crash severity. This analysis emphasizes the critical need for prioritizing pavement maintenance and integrating safety-focused design principles to boost road safety. Moreover, the study underscores the ongoing need for research to better understand and address the intricate relationship between pavement performance and road safety."
  },
  {
    "title": "Probe before You Talk: Towards Black-box Defense against Backdoor Unalignment for Large Language Models",
    "url": "http://arxiv.org/abs/2506.16447v1",
    "arxiv_id": "2506.16447v1",
    "authors": [
      "Biao Yi",
      "Tiansheng Huang",
      "Sishuo Chen",
      "Tong Li",
      "Zheli Liu",
      "Zhixuan Chu",
      "Yiming Li"
    ],
    "published": "2025-06-19T16:30:56+00:00",
    "summary": "Backdoor unalignment attacks against Large Language Models (LLMs) enable the stealthy compromise of safety alignment using a hidden trigger while evading normal safety auditing. These attacks pose significant threats to the applications of LLMs in the real-world Large Language Model as a Service (LLMaaS) setting, where the deployed model is a fully black-box system that can only interact through text. Furthermore, the sample-dependent nature of the attack target exacerbates the threat. Instead of outputting a fixed label, the backdoored LLM follows the semantics of any malicious command with the hidden trigger, significantly expanding the target space. In this paper, we introduce BEAT, a black-box defense that detects triggered samples during inference to deactivate the backdoor. It is motivated by an intriguing observation (dubbed the probe concatenate effect), where concatenated triggered samples significantly reduce the refusal rate of the backdoored LLM towards a malicious probe, while non-triggered samples have little effect. Specifically, BEAT identifies whether an input is triggered by measuring the degree of distortion in the output distribution of the probe before and after concatenation with the input. Our method addresses the challenges of sample-dependent targets from an opposite perspective. It captures the impact of the trigger on the refusal signal (which is sample-independent) instead of sample-specific successful attack behaviors. It overcomes black-box access limitations by using multiple sampling to approximate the output distribution. Extensive experiments are conducted on various backdoor attacks and LLMs (including the closed-source GPT-3.5-turbo), verifying the effectiveness and efficiency of our defense. Besides, we also preliminarily verify that BEAT can effectively defend against popular jailbreak attacks, as they can be regarded as 'natural backdoors'."
  },
  {
    "title": "Evaluating the Use of LLMs for Documentation to Code Traceability",
    "url": "http://arxiv.org/abs/2506.16440v1",
    "arxiv_id": "2506.16440v1",
    "authors": [
      "Ebube Alor",
      "SayedHassan Khatoonabadi",
      "Emad Shihab"
    ],
    "published": "2025-06-19T16:18:53+00:00",
    "summary": "Large Language Models (LLMs) offer new potential for automating documentation-to-code traceability, yet their capabilities remain underexplored. We present a comprehensive evaluation of LLMs (Claude 3.5 Sonnet, GPT-4o, and o3-mini) in establishing trace links between various software documentation (including API references and user guides) and source code. We create two novel datasets from two open-source projects (Unity Catalog and Crawl4AI). Through systematic experiments, we assess three key capabilities: (1) trace link identification accuracy, (2) relationship explanation quality, and (3) multi-step chain reconstruction. Results show that the best-performing LLM achieves F1-scores of 79.4% and 80.4% across the two datasets, substantially outperforming our baselines (TF-IDF, BM25, and CodeBERT). While fully correct relationship explanations range from 42.9% to 71.1%, partial accuracy exceeds 97%, indicating that fundamental connections are rarely missed. For multi-step chains, LLMs maintain high endpoint accuracy but vary in capturing precise intermediate links. Error analysis reveals that many false positives stem from naming-based assumptions, phantom links, or overgeneralization of architectural patterns. We demonstrate that task-framing, such as a one-to-many matching strategy, is critical for performance. These findings position LLMs as powerful assistants for trace discovery, but their limitations could necessitate human-in-the-loop tool design and highlight specific error patterns for future research."
  },
  {
    "title": "An efficient neuromorphic approach for collision avoidance combining Stack-CNN with event cameras",
    "url": "http://arxiv.org/abs/2506.16436v1",
    "arxiv_id": "2506.16436v1",
    "authors": [
      "Antonio Giulio Coretti",
      "Mattia Varile",
      "Mario Edoardo Bertaina"
    ],
    "published": "2025-06-19T16:15:05+00:00",
    "summary": "Space debris poses a significant threat, driving research into active and passive mitigation strategies. This work presents an innovative collision avoidance system utilizing event-based cameras - a novel imaging technology well-suited for Space Situational Awareness (SSA) and Space Traffic Management (STM). The system, employing a Stack-CNN algorithm (previously used for meteor detection), analyzes real-time event-based camera data to detect faint moving objects. Testing on terrestrial data demonstrates the algorithm's ability to enhance signal-to-noise ratio, offering a promising approach for on-board space imaging and improving STM/SSA operations."
  },
  {
    "title": "On Continuous Monitoring of Risk Violations under Unknown Shift",
    "url": "http://arxiv.org/abs/2506.16416v1",
    "arxiv_id": "2506.16416v1",
    "authors": [
      "Alexander Timans",
      "Rajeev Verma",
      "Eric Nalisnick",
      "Christian A. Naesseth"
    ],
    "published": "2025-06-19T15:52:24+00:00",
    "summary": "Machine learning systems deployed in the real world must operate under dynamic and often unpredictable distribution shifts. This challenges the validity of statistical safety assurances on the system's risk established beforehand. Common risk control frameworks rely on fixed assumptions and lack mechanisms to continuously monitor deployment reliability. In this work, we propose a general framework for the real-time monitoring of risk violations in evolving data streams. Leveraging the 'testing by betting' paradigm, we propose a sequential hypothesis testing procedure to detect violations of bounded risks associated with the model's decision-making mechanism, while ensuring control on the false alarm rate. Our method operates under minimal assumptions on the nature of encountered shifts, rendering it broadly applicable. We illustrate the effectiveness of our approach by monitoring risks in outlier detection and set prediction under a variety of shifts."
  },
  {
    "title": "IS-Bench: Evaluating Interactive Safety of VLM-Driven Embodied Agents in Daily Household Tasks",
    "url": "http://arxiv.org/abs/2506.16402v1",
    "arxiv_id": "2506.16402v1",
    "authors": [
      "Xiaoya Lu",
      "Zeren Chen",
      "Xuhao Hu",
      "Yijin Zhou",
      "Weichen Zhang",
      "Dongrui Liu",
      "Lu Sheng",
      "Jing Shao"
    ],
    "published": "2025-06-19T15:34:46+00:00",
    "summary": "Flawed planning from VLM-driven embodied agents poses significant safety hazards, hindering their deployment in real-world household tasks. However, existing static, non-interactive evaluation paradigms fail to adequately assess risks within these interactive environments, since they cannot simulate dynamic risks that emerge from an agent's actions and rely on unreliable post-hoc evaluations that ignore unsafe intermediate steps. To bridge this critical gap, we propose evaluating an agent's interactive safety: its ability to perceive emergent risks and execute mitigation steps in the correct procedural order. We thus present IS-Bench, the first multi-modal benchmark designed for interactive safety, featuring 161 challenging scenarios with 388 unique safety risks instantiated in a high-fidelity simulator. Crucially, it facilitates a novel process-oriented evaluation that verifies whether risk mitigation actions are performed before/after specific risk-prone steps. Extensive experiments on leading VLMs, including the GPT-4o and Gemini-2.5 series, reveal that current agents lack interactive safety awareness, and that while safety-aware Chain-of-Thought can improve performance, it often compromises task completion. By highlighting these critical limitations, IS-Bench provides a foundation for developing safer and more reliable embodied AI systems."
  },
  {
    "title": "Data-Driven Policy Mapping for Safe RL-based Energy Management Systems",
    "url": "http://arxiv.org/abs/2506.16352v1",
    "arxiv_id": "2506.16352v1",
    "authors": [
      "Theo Zangato",
      "Aomar Osmani",
      "Pegah Alizadeh"
    ],
    "published": "2025-06-19T14:29:48+00:00",
    "summary": "Increasing global energy demand and renewable integration complexity have placed buildings at the center of sustainable energy management. We present a three-step reinforcement learning(RL)-based Building Energy Management System (BEMS) that combines clustering, forecasting, and constrained policy learning to address scalability, adaptability, and safety challenges. First, we cluster non-shiftable load profiles to identify common consumption patterns, enabling policy generalization and transfer without retraining for each new building. Next, we integrate an LSTM based forecasting module to anticipate future states, improving the RL agents' responsiveness to dynamic conditions. Lastly, domain-informed action masking ensures safe exploration and operation, preventing harmful decisions. Evaluated on real-world data, our approach reduces operating costs by up to 15% for certain building types, maintains stable environmental performance, and quickly classifies and optimizes new buildings with limited data. It also adapts to stochastic tariff changes without retraining. Overall, this framework delivers scalable, robust, and cost-effective building energy management."
  },
  {
    "title": "Goal-conditioned Hierarchical Reinforcement Learning for Sample-efficient and Safe Autonomous Driving at Intersections",
    "url": "http://arxiv.org/abs/2506.16336v1",
    "arxiv_id": "2506.16336v1",
    "authors": [
      "Yiou Huang"
    ],
    "published": "2025-06-19T14:14:55+00:00",
    "summary": "Reinforcement learning (RL) exhibits remarkable potential in addressing autonomous driving tasks. However, it is difficult to train a sample-efficient and safe policy in complex scenarios. In this article, we propose a novel hierarchical reinforcement learning (HRL) framework with a goal-conditioned collision prediction (GCCP) module. In the hierarchical structure, the GCCP module predicts collision risks according to different potential subgoals of the ego vehicle. A high-level decision-maker choose the best safe subgoal. A low-level motion-planner interacts with the environment according to the subgoal. Compared to traditional RL methods, our algorithm is more sample-efficient, since its hierarchical structure allows reusing the policies of subgoals across similar tasks for various navigation scenarios. In additional, the GCCP module's ability to predict both the ego vehicle's and surrounding vehicles' future actions according to different subgoals, ensures the safety of the ego vehicle throughout the decision-making process. Experimental results demonstrate that the proposed method converges to an optimal policy faster and achieves higher safety than traditional RL methods."
  },
  {
    "title": "Explainable Rule Application via Structured Prompting: A Neural-Symbolic Approach",
    "url": "http://arxiv.org/abs/2506.16335v1",
    "arxiv_id": "2506.16335v1",
    "authors": [
      "Albert Sadowski",
      "Jaros\u0142aw A. Chudziak"
    ],
    "published": "2025-06-19T14:14:01+00:00",
    "summary": "Large Language Models (LLMs) excel in complex reasoning tasks but struggle with consistent rule application, exception handling, and explainability, particularly in domains like legal analysis that require both natural language understanding and precise logical inference. This paper introduces a structured prompting framework that decomposes reasoning into three verifiable steps: entity identification, property extraction, and symbolic rule application. By integrating neural and symbolic approaches, our method leverages LLMs' interpretive flexibility while ensuring logical consistency through formal verification. The framework externalizes task definitions, enabling domain experts to refine logical structures without altering the architecture. Evaluated on the LegalBench hearsay determination task, our approach significantly outperformed baselines, with OpenAI o-family models showing substantial improvements - o1 achieving an F1 score of 0.929 and o3-mini reaching 0.867 using structured decomposition with complementary predicates, compared to their few-shot baselines of 0.714 and 0.74 respectively. This hybrid neural-symbolic system offers a promising pathway for transparent and consistent rule-based reasoning, suggesting potential for explainable AI applications in structured legal reasoning tasks."
  },
  {
    "title": "PL-Guard: Benchmarking Language Model Safety for Polish",
    "url": "http://arxiv.org/abs/2506.16322v1",
    "arxiv_id": "2506.16322v1",
    "authors": [
      "Aleksandra Krasnod\u0119bska",
      "Karolina Seweryn",
      "Szymon \u0141ukasik",
      "Wojciech Kusa"
    ],
    "published": "2025-06-19T13:56:41+00:00",
    "summary": "Despite increasing efforts to ensure the safety of large language models (LLMs), most existing safety assessments and moderation tools remain heavily biased toward English and other high-resource languages, leaving majority of global languages underexamined. To address this gap, we introduce a manually annotated benchmark dataset for language model safety classification in Polish. We also create adversarially perturbed variants of these samples designed to challenge model robustness. We conduct a series of experiments to evaluate LLM-based and classifier-based models of varying sizes and architectures. Specifically, we fine-tune three models: Llama-Guard-3-8B, a HerBERT-based classifier (a Polish BERT derivative), and PLLuM, a Polish-adapted Llama-8B model. We train these models using different combinations of annotated data and evaluate their performance, comparing it against publicly available guard models. Results demonstrate that the HerBERT-based classifier achieves the highest overall performance, particularly under adversarial conditions."
  },
  {
    "title": "M-Predictive Spliner: Enabling Spatiotemporal Multi-Opponent Overtaking for Autonomous Racing",
    "url": "http://arxiv.org/abs/2506.16301v1",
    "arxiv_id": "2506.16301v1",
    "authors": [
      "Nadine Imholz",
      "Maurice Brunner",
      "Nicolas Baumann",
      "Edoardo Ghignone",
      "Michele Magno"
    ],
    "published": "2025-06-19T13:23:10+00:00",
    "summary": "Unrestricted multi-agent racing presents a significant research challenge, requiring decision-making at the limits of a robot's operational capabilities. While previous approaches have either ignored spatiotemporal information in the decision-making process or been restricted to single-opponent scenarios, this work enables arbitrary multi-opponent head-to-head racing while considering the opponents' future intent. The proposed method employs a KF-based multi-opponent tracker to effectively perform opponent ReID by associating them across observations. Simultaneously, spatial and velocity GPR is performed on all observed opponent trajectories, providing predictive information to compute the overtaking maneuvers. This approach has been experimentally validated on a physical 1:10 scale autonomous racing car, achieving an overtaking success rate of up to 91.65% and demonstrating an average 10.13%-point improvement in safety at the same speed as the previous SotA. These results highlight its potential for high-performance autonomous racing."
  },
  {
    "title": "AeroGPT: Leveraging Large-Scale Audio Model for Aero-Engine Bearing Fault Diagnosis",
    "url": "http://arxiv.org/abs/2506.16225v1",
    "arxiv_id": "2506.16225v1",
    "authors": [
      "Jiale Liu",
      "Dandan Peng",
      "Huan Wang",
      "Chenyu Liu",
      "Yan-Fu Li",
      "Min Xie"
    ],
    "published": "2025-06-19T11:34:59+00:00",
    "summary": "Aerospace engines, as critical components in aviation and aerospace industries, require continuous and accurate fault diagnosis to ensure operational safety and prevent catastrophic failures. While deep learning techniques have been extensively studied in this context, they output logits or confidence scores, necessitating post-processing to derive actionable insights. Furthermore, the potential of large-scale audio models in this domain remains largely untapped. To address these limitations, this paper proposes AeroGPT, a novel framework that transfers knowledge from general audio domain to aero-engine bearing fault diagnosis. AeroGPT is a framework based on large-scale audio model that incorporates Vibration Signal Alignment (VSA) to adapt general audio knowledge to domain-specific vibration patterns, and combines Generative Fault Classification (GFC) to directly output interpretable fault labels. This approach eliminates the need for post-processing of fault labels, supports interactive, interpretable, and actionable fault diagnosis, thereby greatly enhancing industrial applicability. Through comprehensive experimental validation on two aero-engine bearing datasets, AeroGPT achieved exceptional performance with 98.94% accuracy on the DIRG dataset and perfect 100% classification on the HIT bearing dataset, surpassing traditional deep learning approaches. Additional Qualitative analysis validates the effectiveness of our approach and highlights the potential of large-scale models to revolutionize fault diagnosis."
  },
  {
    "title": "Integrating Generative Adversarial Networks and Convolutional Neural Networks for Enhanced Traffic Accidents Detection and Analysis",
    "url": "http://arxiv.org/abs/2506.16186v1",
    "arxiv_id": "2506.16186v1",
    "authors": [
      "Zhenghao Xi",
      "Xiang Liu",
      "Yaqi Liu",
      "Yitong Cai",
      "Yangyu Zheng"
    ],
    "published": "2025-06-19T10:06:20+00:00",
    "summary": "Accident detection using Closed Circuit Television (CCTV) footage is one of the most imperative features for enhancing transport safety and efficient traffic control. To this end, this research addresses the issues of supervised monitoring and data deficiency in accident detection systems by adapting excellent deep learning technologies. The motivation arises from rising statistics in the number of car accidents worldwide; this calls for innovation and the establishment of a smart, efficient and automated way of identifying accidents and calling for help to save lives. Addressing the problem of the scarcity of data, the presented framework joins Generative Adversarial Networks (GANs) for synthesizing data and Convolutional Neural Networks (CNN) for model training. Video frames for accidents and non-accidents are collected from YouTube videos, and we perform resizing, image enhancement and image normalisation pixel range adjustments. Three models are used: CNN, Fine-tuned Convolutional Neural Network (FTCNN) and Vision Transformer (VIT) worked best for detecting accidents from CCTV, obtaining an accuracy rate of 94% and 95%, while the CNN model obtained 88%. Such results show that the proposed framework suits traffic safety applications due to its high real-time accident detection capabilities and broad-scale applicability. This work lays the foundation for intelligent surveillance systems in the future for real-time traffic monitoring, smart city framework, and integration of intelligent surveillance systems into emergency management systems."
  },
  {
    "title": "PRISON: Unmasking the Criminal Potential of Large Language Models",
    "url": "http://arxiv.org/abs/2506.16150v1",
    "arxiv_id": "2506.16150v1",
    "authors": [
      "Xinyi Wu",
      "Geng Hong",
      "Pei Chen",
      "Yueyue Chen",
      "Xudong Pan",
      "Min Yang"
    ],
    "published": "2025-06-19T09:06:27+00:00",
    "summary": "As large language models (LLMs) advance, concerns about their misconduct in complex social contexts intensify. Existing research overlooked the systematic understanding and assessment of their criminal capability in realistic interactions. We propose a unified framework PRISON, to quantify LLMs' criminal potential across five dimensions: False Statements, Frame-Up, Psychological Manipulation, Emotional Disguise, and Moral Disengagement. Using structured crime scenarios adapted from classic films, we evaluate both criminal potential and anti-crime ability of LLMs via role-play. Results show that state-of-the-art LLMs frequently exhibit emergent criminal tendencies, such as proposing misleading statements or evasion tactics, even without explicit instructions. Moreover, when placed in a detective role, models recognize deceptive behavior with only 41% accuracy on average, revealing a striking mismatch between conducting and detecting criminal behavior. These findings underscore the urgent need for adversarial robustness, behavioral alignment, and safety mechanisms before broader LLM deployment."
  },
  {
    "title": "Regression Testing Optimization for ROS-based Autonomous Systems: A Comprehensive Review of Techniques",
    "url": "http://arxiv.org/abs/2506.16101v1",
    "arxiv_id": "2506.16101v1",
    "authors": [
      "Yupeng Jiang",
      "Shuaiyi Sun",
      "Xi Zheng"
    ],
    "published": "2025-06-19T07:43:36+00:00",
    "summary": "Regression testing plays a critical role in maintaining software reliability, particularly for ROS-based autonomous systems (ROSAS), which frequently undergo continuous integration and iterative development. However, conventional regression testing techniques face significant challenges when applied to autonomous systems due to their dynamic and non-deterministic behaviors, complex multi-modal sensor data, asynchronous distributed architectures, and stringent safety and real-time constraints. Although numerous studies have explored test optimization in traditional software contexts, regression testing optimization specifically for ROSAS remains largely unexplored. To address this gap, we present the first comprehensive survey systematically reviewing regression testing optimization techniques tailored for ROSAS. We analyze and categorize 122 representative studies into regression test case prioritization, minimization, and selection methods. A structured taxonomy is introduced to clearly illustrate their applicability and limitations within ROSAS contexts. Furthermore, we highlight major challenges specific to regression testing for ROSAS, including effectively prioritizing tests in response to frequent system modifications, efficiently minimizing redundant tests, and difficulty in accurately selecting impacted test cases. Finally, we propose research insights and identify promising future directions, such as leveraging frame-to-vector coverage metrics, multi-source foundation models, and neurosymbolic reasoning to enhance regression testing efficiency and effectiveness. This survey provides a foundational reference and practical roadmap for advancing the state-of-the-art in regression testing optimization for ROSAS."
  },
  {
    "title": "Probing the Robustness of Large Language Models Safety to Latent Perturbations",
    "url": "http://arxiv.org/abs/2506.16078v1",
    "arxiv_id": "2506.16078v1",
    "authors": [
      "Tianle Gu",
      "Kexin Huang",
      "Zongqi Wang",
      "Yixu Wang",
      "Jie Li",
      "Yuanqi Yao",
      "Yang Yao",
      "Yujiu Yang",
      "Yan Teng",
      "Yingchun Wang"
    ],
    "published": "2025-06-19T07:03:05+00:00",
    "summary": "Safety alignment is a key requirement for building reliable Artificial General Intelligence. Despite significant advances in safety alignment, we observe that minor latent shifts can still trigger unsafe responses in aligned models. We argue that this stems from the shallow nature of existing alignment methods, which focus on surface-level refusal behaviors without sufficiently altering internal representations. Consequently, small shifts in hidden activations can re-trigger harmful behaviors embedded in the latent space. To explore the robustness of safety alignment to latent perturbations, we introduce a probing method that measures the Negative Log-Likelihood of the original response generated by the model. This probe quantifies local sensitivity in the latent space, serving as a diagnostic tool for identifying vulnerable directions. Based on this signal, we construct effective jailbreak trajectories, giving rise to the Activation Steering Attack (ASA). More importantly, these insights offer a principled foundation for improving alignment robustness. To this end, we introduce Layer-wise Adversarial Patch Training~(LAPT), a fine-tuning strategy that inject controlled perturbations into hidden representations during training. Experimental results highlight that LAPT strengthen alignment robustness without compromising general capabilities. Our findings reveal fundamental flaws in current alignment paradigms and call for representation-level training strategies that move beyond surface-level behavior supervision. Codes and results are available at https://github.com/Carol-gutianle/LatentSafety."
  },
  {
    "title": "From RAG to Agentic: Validating Islamic-Medicine Responses with LLM Agents",
    "url": "http://arxiv.org/abs/2506.15911v1",
    "arxiv_id": "2506.15911v1",
    "authors": [
      "Mohammad Amaan Sayeed",
      "Mohammed Talha Alam",
      "Raza Imam",
      "Shahab Saquib Sohail",
      "Amir Hussain"
    ],
    "published": "2025-06-18T23:14:35+00:00",
    "summary": "Centuries-old Islamic medical texts like Avicenna's Canon of Medicine and the Prophetic Tibb-e-Nabawi encode a wealth of preventive care, nutrition, and holistic therapies, yet remain inaccessible to many and underutilized in modern AI systems. Existing language-model benchmarks focus narrowly on factual recall or user preference, leaving a gap in validating culturally grounded medical guidance at scale. We propose a unified evaluation pipeline, Tibbe-AG, that aligns 30 carefully curated Prophetic-medicine questions with human-verified remedies and compares three LLMs (LLaMA-3, Mistral-7B, Qwen2-7B) under three configurations: direct generation, retrieval-augmented generation, and a scientific self-critique filter. Each answer is then assessed by a secondary LLM serving as an agentic judge, yielding a single 3C3H quality score. Retrieval improves factual accuracy by 13%, while the agentic prompt adds another 10% improvement through deeper mechanistic insight and safety considerations. Our results demonstrate that blending classical Islamic texts with retrieval and self-evaluation enables reliable, culturally sensitive medical question-answering."
  },
  {
    "title": "Semantic and Feature Guided Uncertainty Quantification of Visual Localization for Autonomous Vehicles",
    "url": "http://arxiv.org/abs/2506.15851v1",
    "arxiv_id": "2506.15851v1",
    "authors": [
      "Qiyuan Wu",
      "Mark Campbell"
    ],
    "published": "2025-06-18T20:00:36+00:00",
    "summary": "The uncertainty quantification of sensor measurements coupled with deep learning networks is crucial for many robotics systems, especially for safety-critical applications such as self-driving cars. This paper develops an uncertainty quantification approach in the context of visual localization for autonomous driving, where locations are selected based on images. Key to our approach is to learn the measurement uncertainty using light-weight sensor error model, which maps both image feature and semantic information to 2-dimensional error distribution. Our approach enables uncertainty estimation conditioned on the specific context of the matched image pair, implicitly capturing other critical, unannotated factors (e.g., city vs highway, dynamic vs static scenes, winter vs summer) in a latent manner. We demonstrate the accuracy of our uncertainty prediction framework using the Ithaca365 dataset, which includes variations in lighting and weather (sunny, night, snowy). Both the uncertainty quantification of the sensor+network is evaluated, along with Bayesian localization filters using unique sensor gating method. Results show that the measurement error does not follow a Gaussian distribution with poor weather and lighting conditions, and is better predicted by our Gaussian Mixture model."
  },
  {
    "title": "SafeMimic: Towards Safe and Autonomous Human-to-Robot Imitation for Mobile Manipulation",
    "url": "http://arxiv.org/abs/2506.15847v1",
    "arxiv_id": "2506.15847v1",
    "authors": [
      "Arpit Bahety",
      "Arnav Balaji",
      "Ben Abbatematteo",
      "Roberto Mart\u00edn-Mart\u00edn"
    ],
    "published": "2025-06-18T19:55:10+00:00",
    "summary": "For robots to become efficient helpers in the home, they must learn to perform new mobile manipulation tasks simply by watching humans perform them. Learning from a single video demonstration from a human is challenging as the robot needs to first extract from the demo what needs to be done and how, translate the strategy from a third to a first-person perspective, and then adapt it to be successful with its own morphology. Furthermore, to mitigate the dependency on costly human monitoring, this learning process should be performed in a safe and autonomous manner. We present SafeMimic, a framework to learn new mobile manipulation skills safely and autonomously from a single third-person human video. Given an initial human video demonstration of a multi-step mobile manipulation task, SafeMimic first parses the video into segments, inferring both the semantic changes caused and the motions the human executed to achieve them and translating them to an egocentric reference. Then, it adapts the behavior to the robot's own morphology by sampling candidate actions around the human ones, and verifying them for safety before execution in a receding horizon fashion using an ensemble of safety Q-functions trained in simulation. When safe forward progression is not possible, SafeMimic backtracks to previous states and attempts a different sequence of actions, adapting both the trajectory and the grasping modes when required for its morphology. As a result, SafeMimic yields a strategy that succeeds in the demonstrated behavior and learns task-specific actions that reduce exploration in future attempts. Our experiments show that our method allows robots to safely and efficiently learn multi-step mobile manipulation behaviors from a single human demonstration, from different users, and in different environments, with improvements over state-of-the-art baselines across seven tasks"
  },
  {
    "title": "Leaky Thoughts: Large Reasoning Models Are Not Private Thinkers",
    "url": "http://arxiv.org/abs/2506.15674v1",
    "arxiv_id": "2506.15674v1",
    "authors": [
      "Tommaso Green",
      "Martin Gubri",
      "Haritz Puerto",
      "Sangdoo Yun",
      "Seong Joon Oh"
    ],
    "published": "2025-06-18T17:57:01+00:00",
    "summary": "We study privacy leakage in the reasoning traces of large reasoning models used as personal agents. Unlike final outputs, reasoning traces are often assumed to be internal and safe. We challenge this assumption by showing that reasoning traces frequently contain sensitive user data, which can be extracted via prompt injections or accidentally leak into outputs. Through probing and agentic evaluations, we demonstrate that test-time compute approaches, particularly increased reasoning steps, amplify such leakage. While increasing the budget of those test-time compute approaches makes models more cautious in their final answers, it also leads them to reason more verbosely and leak more in their own thinking. This reveals a core tension: reasoning improves utility but enlarges the privacy attack surface. We argue that safety efforts must extend to the model's internal thinking, not just its outputs."
  },
  {
    "title": "deepSURF: Detecting Memory Safety Vulnerabilities in Rust Through Fuzzing LLM-Augmented Harnesses",
    "url": "http://arxiv.org/abs/2506.15648v1",
    "arxiv_id": "2506.15648v1",
    "authors": [
      "Georgios Androutsopoulos",
      "Antonio Bianchi"
    ],
    "published": "2025-06-18T17:18:23+00:00",
    "summary": "Although Rust ensures memory safety by default, it also permits the use of unsafe code, which can introduce memory safety vulnerabilities if misused. Unfortunately, existing tools for detecting memory bugs in Rust typically exhibit limited detection capabilities, inadequately handle Rust-specific types, or rely heavily on manual intervention.   To address these limitations, we present deepSURF, a tool that integrates static analysis with Large Language Model (LLM)-guided fuzzing harness generation to effectively identify memory safety vulnerabilities in Rust libraries, specifically targeting unsafe code. deepSURF introduces a novel approach for handling generics by substituting them with custom types and generating tailored implementations for the required traits, enabling the fuzzer to simulate user-defined behaviors within the fuzzed library. Additionally, deepSURF employs LLMs to augment fuzzing harnesses dynamically, facilitating exploration of complex API interactions and significantly increasing the likelihood of exposing memory safety vulnerabilities. We evaluated deepSURF on 27 real-world Rust crates, successfully rediscovering 20 known memory safety bugs and uncovering 6 previously unknown vulnerabilities, demonstrating clear improvements over state-of-the-art tools."
  },
  {
    "title": "LoX: Low-Rank Extrapolation Robustifies LLM Safety Against Fine-tuning",
    "url": "http://arxiv.org/abs/2506.15606v1",
    "arxiv_id": "2506.15606v1",
    "authors": [
      "Gabrel J. Perin",
      "Runjin Chen",
      "Xuxi Chen",
      "Nina S. T. Hirata",
      "Zhangyang Wang",
      "Junyuan Hong"
    ],
    "published": "2025-06-18T16:30:02+00:00",
    "summary": "Large Language Models (LLMs) have become indispensable in real-world applications. However, their widespread adoption raises significant safety concerns, particularly in responding to socially harmful questions. Despite substantial efforts to improve model safety through alignment, aligned models can still have their safety protections undermined by subsequent fine-tuning - even when the additional training data appears benign. In this paper, we empirically demonstrate that this vulnerability stems from the sensitivity of safety-critical low-rank subspaces in LLM parameters to fine-tuning. Building on this insight, we propose a novel training-free method, termed Low-Rank Extrapolation (LoX), to enhance safety robustness by extrapolating the safety subspace of an aligned LLM. Our experimental results confirm the effectiveness of LoX, demonstrating significant improvements in robustness against both benign and malicious fine-tuning attacks while preserving the model's adaptability to new tasks. For instance, LoX leads to 11% to 54% absolute reductions in attack success rates (ASR) facing benign or malicious fine-tuning attacks. By investigating the ASR landscape of parameters, we attribute the success of LoX to that the extrapolation moves LLM parameters to a flatter zone, thereby less sensitive to perturbations. The code is available at github.com/VITA-Group/LoX."
  },
  {
    "title": "RePCS: Diagnosing Data Memorization in LLM-Powered Retrieval-Augmented Generation",
    "url": "http://arxiv.org/abs/2506.15513v1",
    "arxiv_id": "2506.15513v1",
    "authors": [
      "Le Vu Anh",
      "Nguyen Viet Anh",
      "Mehmet Dik",
      "Luong Van Nghia"
    ],
    "published": "2025-06-18T14:48:19+00:00",
    "summary": "Retrieval-augmented generation (RAG) has become a common strategy for updating large language model (LLM) responses with current, external information. However, models may still rely on memorized training data, bypass the retrieved evidence, and produce contaminated outputs. We introduce Retrieval-Path Contamination Scoring (RePCS), a diagnostic method that detects such behavior without requiring model access or retraining. RePCS compares two inference paths: (i) a parametric path using only the query, and (ii) a retrieval-augmented path using both the query and retrieved context by computing the Kullback-Leibler (KL) divergence between their output distributions. A low divergence suggests that the retrieved context had minimal impact, indicating potential memorization. This procedure is model-agnostic, requires no gradient or internal state access, and adds only a single additional forward pass. We further derive PAC-style guarantees that link the KL threshold to user-defined false positive and false negative rates. On the Prompt-WNQA benchmark, RePCS achieves a ROC-AUC of 0.918. This result outperforms the strongest prior method by 6.5 percentage points while keeping latency overhead below 4.7% on an NVIDIA T4 GPU. RePCS offers a lightweight, black-box safeguard to verify whether a RAG system meaningfully leverages retrieval, making it especially valuable in safety-critical applications."
  },
  {
    "title": "Designing Intent: A Multimodal Framework for Human-Robot Cooperation in Industrial Workspaces",
    "url": "http://arxiv.org/abs/2506.15293v1",
    "arxiv_id": "2506.15293v1",
    "authors": [
      "Francesco Chiossi",
      "Julian Rasch",
      "Robin Welsch",
      "Albrecht Schmidt",
      "Florian Michahelles"
    ],
    "published": "2025-06-18T09:23:54+00:00",
    "summary": "As robots enter collaborative workspaces, ensuring mutual understanding between human workers and robotic systems becomes a prerequisite for trust, safety, and efficiency. In this position paper, we draw on the cooperation scenario of the AIMotive project in which a human and a cobot jointly perform assembly tasks to argue for a structured approach to intent communication. Building on the Situation Awareness-based Agent Transparency (SAT) framework and the notion of task abstraction levels, we propose a multidimensional design space that maps intent content (SAT1, SAT3), planning horizon (operational to strategic), and modality (visual, auditory, haptic). We illustrate how this space can guide the design of multimodal communication strategies tailored to dynamic collaborative work contexts. With this paper, we lay the conceptual foundation for a future design toolkit aimed at supporting transparent human-robot interaction in the workplace. We highlight key open questions and design challenges, and propose a shared agenda for multimodal, adaptive, and trustworthy robotic collaboration in hybrid work environments."
  },
  {
    "title": "AI-driven visual monitoring of industrial assembly tasks",
    "url": "http://arxiv.org/abs/2506.15285v1",
    "arxiv_id": "2506.15285v1",
    "authors": [
      "Mattia Nardon",
      "Stefano Messelodi",
      "Antonio Granata",
      "Fabio Poiesi",
      "Alberto Danese",
      "Davide Boscaini"
    ],
    "published": "2025-06-18T09:08:42+00:00",
    "summary": "Visual monitoring of industrial assembly tasks is critical for preventing equipment damage due to procedural errors and ensuring worker safety. Although commercial solutions exist, they typically require rigid workspace setups or the application of visual markers to simplify the problem. We introduce ViMAT, a novel AI-driven system for real-time visual monitoring of assembly tasks that operates without these constraints. ViMAT combines a perception module that extracts visual observations from multi-view video streams with a reasoning module that infers the most likely action being performed based on the observed assembly state and prior task knowledge. We validate ViMAT on two assembly tasks, involving the replacement of LEGO components and the reconfiguration of hydraulic press molds, demonstrating its effectiveness through quantitative and qualitative analysis in challenging real-world scenarios characterized by partial and uncertain visual observations. Project page: https://tev-fbk.github.io/ViMAT"
  },
  {
    "title": "From LLMs to MLLMs to Agents: A Survey of Emerging Paradigms in Jailbreak Attacks and Defenses within LLM Ecosystem",
    "url": "http://arxiv.org/abs/2506.15170v1",
    "arxiv_id": "2506.15170v1",
    "authors": [
      "Yanxu Mao",
      "Tiehan Cui",
      "Peipei Liu",
      "Datao You",
      "Hongsong Zhu"
    ],
    "published": "2025-06-18T06:33:19+00:00",
    "summary": "Large language models (LLMs) are rapidly evolving from single-modal systems to multimodal LLMs and intelligent agents, significantly expanding their capabilities while introducing increasingly severe security risks. This paper presents a systematic survey of the growing complexity of jailbreak attacks and corresponding defense mechanisms within the expanding LLM ecosystem. We first trace the developmental trajectory from LLMs to MLLMs and Agents, highlighting the core security challenges emerging at each stage. Next, we categorize mainstream jailbreak techniques from both the attack impact and visibility perspectives, and provide a comprehensive analysis of representative attack methods, related datasets, and evaluation metrics. On the defense side, we organize existing strategies based on response timing and technical approach, offering a structured understanding of their applicability and implementation. Furthermore, we identify key limitations in existing surveys, such as insufficient attention to agent-specific security issues, the absence of a clear taxonomy for hybrid jailbreak methods, a lack of detailed analysis of experimental setups, and outdated coverage of recent advancements. To address these limitations, we provide an updated synthesis of recent work and outline future research directions in areas such as dataset construction, evaluation framework optimization, and strategy generalization. Our study seeks to enhance the understanding of jailbreak mechanisms and facilitate the advancement of more resilient and adaptive defense strategies in the context of ever more capable LLMs."
  },
  {
    "title": "Sysformer: Safeguarding Frozen Large Language Models with Adaptive System Prompts",
    "url": "http://arxiv.org/abs/2506.15751v1",
    "arxiv_id": "2506.15751v1",
    "authors": [
      "Kartik Sharma",
      "Yiqiao Jin",
      "Vineeth Rakesh",
      "Yingtong Dou",
      "Menghai Pan",
      "Mahashweta Das",
      "Srijan Kumar"
    ],
    "published": "2025-06-18T05:48:05+00:00",
    "summary": "As large language models (LLMs) are deployed in safety-critical settings, it is essential to ensure that their responses comply with safety standards. Prior research has revealed that LLMs often fail to grasp the notion of safe behaviors, resulting in either unjustified refusals to harmless prompts or the generation of harmful content. While substantial efforts have been made to improve their robustness, existing defenses often rely on costly fine-tuning of model parameters or employ suboptimal heuristic techniques. In this work, we take a novel approach to safeguard LLMs by learning to adapt the system prompts in instruction-tuned LLMs. While LLMs are typically pre-trained to follow a fixed system prompt, we investigate the impact of tailoring the system prompt to each specific user input on the safety of the responses. To this end, we propose $\\textbf{Sysformer}$, a trans$\\textbf{former}$ model that updates an initial $\\textbf{sys}$tem prompt to a more robust system prompt in the LLM input embedding space while attending to the user prompt. While keeping the LLM parameters frozen, the Sysformer is trained to refuse to respond to a set of harmful prompts while responding ideally to a set of safe ones. Through extensive experiments on $5$ LLMs from different families and $2$ recent benchmarks, we demonstrate that Sysformer can significantly enhance the robustness of LLMs, leading to upto $80\\%$ gain in the refusal rate on harmful prompts while enhancing the compliance with the safe prompts by upto $90\\%$. Results also generalize well to sophisticated jailbreaking attacks, making LLMs upto $100\\%$ more robust against different attack strategies. We hope our findings lead to cheaper safeguarding of LLMs and motivate future investigations into designing variable system prompts."
  },
  {
    "title": "A Force Feedback Exoskeleton for Teleoperation Using Magnetorheological Clutches",
    "url": "http://arxiv.org/abs/2506.15124v1",
    "arxiv_id": "2506.15124v1",
    "authors": [
      "Zhongyuan Kong",
      "Lei Li",
      "Erwin Ang Tien Yew",
      "Zirui Chen",
      "Wenbo Li",
      "Shiwu Zhang",
      "Jian Yang",
      "Shuaishuai Sun"
    ],
    "published": "2025-06-18T03:46:04+00:00",
    "summary": "This paper proposes an upper-limb exoskeleton teleoperation system based on magnetorheological (MR) clutches, aiming to improve operational accuracy and enhance the immersive experience during lunar sampling tasks. Conventional exoskeleton teleoperation systems commonly employ active force feedback solutions, such as servo motors, which typically suffer from high system complexity and increased energy consumption. Furthermore, force feedback devices utilizing motors and gear reducers generally compromise backdrivability and pose safety risks to operators due to active force output. To address these limitations, we propose a semi-active force feedback strategy based on MR clutches. Dynamic magnetic field control enables precise adjustment of joint stiffness and damping, thereby providing smooth and high-resolution force feedback. The designed MR clutch exhibits outstanding performance across key metrics, achieving a torque-to-mass ratio (TMR) of 93.6 Nm/kg, a torque-to-volume ratio (TVR) of 4.05 x 10^5 Nm/m^3, and a torque-to-power ratio (TPR) of 4.15 Nm/W. Notably, the TMR represents an improvement of approximately 246% over a representative design in prior work. Experimental results validate the system's capability to deliver high-fidelity force feedback. Overall, the proposed system presents a promising solution for deep-space teleoperation with strong potential for real-world deployment in future missions."
  },
  {
    "title": "International Security Applications of Flexible Hardware-Enabled Guarantees",
    "url": "http://arxiv.org/abs/2506.15100v1",
    "arxiv_id": "2506.15100v1",
    "authors": [
      "Onni Aarne",
      "James Petrie"
    ],
    "published": "2025-06-18T03:10:49+00:00",
    "summary": "As AI capabilities advance rapidly, flexible hardware-enabled guarantees (flexHEGs) offer opportunities to address international security challenges through comprehensive governance frameworks. This report examines how flexHEGs could enable internationally trustworthy AI governance by establishing standardized designs, robust ecosystem defenses, and clear operational parameters for AI-relevant chips. We analyze four critical international security applications: limiting proliferation to address malicious use, implementing safety norms to prevent loss of control, managing risks from military AI systems, and supporting strategic stability through balance-of-power mechanisms while respecting national sovereignty. The report explores both targeted deployments for specific high-risk facilities and comprehensive deployments covering all AI-relevant compute. We examine two primary governance models: verification-based agreements that enable transparent compliance monitoring, and ruleset-based agreements that automatically enforce international rules through cryptographically-signed updates. Through game-theoretic analysis, we demonstrate that comprehensive flexHEG agreements could remain stable under reasonable assumptions about state preferences and catastrophic risks. The report addresses critical implementation challenges including technical thresholds for AI-relevant chips, management of existing non-flexHEG hardware, and safeguards against abuse of governance power. While requiring significant international coordination, flexHEGs could provide a technical foundation for managing AI risks at the scale and speed necessary to address emerging threats to international security and stability."
  },
  {
    "title": "Flexible Hardware-Enabled Guarantees for AI Compute",
    "url": "http://arxiv.org/abs/2506.15093v1",
    "arxiv_id": "2506.15093v1",
    "authors": [
      "James Petrie",
      "Onni Aarne",
      "Nora Ammann",
      "David Dalrymple"
    ],
    "published": "2025-06-18T03:04:44+00:00",
    "summary": "As artificial intelligence systems become increasingly powerful, they pose growing risks to international security, creating urgent coordination challenges that current governance approaches struggle to address without compromising sensitive information or national security. We propose flexible hardware-enabled guarantees (flexHEGs), that could be integrated with AI accelerators to enable trustworthy, privacy-preserving verification and enforcement of claims about AI development. FlexHEGs consist of an auditable guarantee processor that monitors accelerator usage and a secure enclosure providing physical tamper protection. The system would be fully open source with flexible, updateable verification capabilities. FlexHEGs could enable diverse governance mechanisms including privacy-preserving model evaluations, controlled deployment, compute limits for training, and automated safety protocol enforcement. In this first part of a three part series, we provide a comprehensive introduction of the flexHEG system, including an overview of the governance and security capabilities it offers, its potential development and adoption paths, and the remaining challenges and limitations it faces. While technically challenging, flexHEGs offer an approach to address emerging regulatory and international security challenges in frontier AI development."
  },
  {
    "title": "Systems-Theoretic and Data-Driven Security Analysis in ML-enabled Medical Devices",
    "url": "http://arxiv.org/abs/2506.15028v1",
    "arxiv_id": "2506.15028v1",
    "authors": [
      "Gargi Mitra",
      "Mohammadreza Hallajiyan",
      "Inji Kim",
      "Athish Pranav Dharmalingam",
      "Mohammed Elnawawy",
      "Shahrear Iqbal",
      "Karthik Pattabiraman",
      "Homa Alemzadeh"
    ],
    "published": "2025-06-18T00:05:48+00:00",
    "summary": "The integration of AI/ML into medical devices is rapidly transforming healthcare by enhancing diagnostic and treatment facilities. However, this advancement also introduces serious cybersecurity risks due to the use of complex and often opaque models, extensive interconnectivity, interoperability with third-party peripheral devices, Internet connectivity, and vulnerabilities in the underlying technologies. These factors contribute to a broad attack surface and make threat prevention, detection, and mitigation challenging. Given the highly safety-critical nature of these devices, a cyberattack on these devices can cause the ML models to mispredict, thereby posing significant safety risks to patients. Therefore, ensuring the security of these devices from the time of design is essential. This paper underscores the urgency of addressing the cybersecurity challenges in ML-enabled medical devices at the pre-market phase. We begin by analyzing publicly available data on device recalls and adverse events, and known vulnerabilities, to understand the threat landscape of AI/ML-enabled medical devices and their repercussions on patient safety. Building on this analysis, we introduce a suite of tools and techniques designed by us to assist security analysts in conducting comprehensive premarket risk assessments. Our work aims to empower manufacturers to embed cybersecurity as a core design principle in AI/ML-enabled medical devices, thereby making them safe for patients."
  },
  {
    "title": "Algorithmic Approaches to Enhance Safety in Autonomous Vehicles: Minimizing Lane Changes and Merging",
    "url": "http://arxiv.org/abs/2506.15026v1",
    "arxiv_id": "2506.15026v1",
    "authors": [
      "Seyed Moein Abtahi",
      "Akramul Azim"
    ],
    "published": "2025-06-17T23:59:06+00:00",
    "summary": "The rapid advancements in autonomous vehicle (AV) technology promise enhanced safety and operational efficiency. However, frequent lane changes and merging maneuvers continue to pose significant safety risks and disrupt traffic flow. This paper introduces the Minimizing Lane Change Algorithm (MLCA), a state-machine-based approach designed to reduce unnecessary lane changes, thereby enhancing both traffic safety and efficiency. The MLCA algorithm prioritizes maintaining lane stability unless safety-critical conditions necessitate a lane change. The algorithm's effectiveness was evaluated through simulations conducted on the SUMO platform, comparing its performance against established models, including LC2017 and MOBIL. Results demonstrate substantial reductions in lane changes and collisions, leading to smoother traffic flow and improved safety metrics. Additionally, the study highlights the MLCA's adaptability to various traffic densities and roadway configurations, showcasing its potential for wide-scale deployment in real-world AV systems. Future work aims to validate these findings in more complex scenarios using the CARLA simulator, which will enable the testing of the algorithm under more dynamic and high-fidelity conditions, such as urban traffic environments with diverse road users. Moreover, the integration of cybersecurity measures for vehicle-to-vehicle (V2V) communication will be explored to ensure robust and secure data exchange, further enhancing the reliability and safety of AV operations. This research contributes to the broader goal of developing intelligent traffic systems that optimize both individual vehicle performance and overall traffic network efficiency."
  },
  {
    "title": "Context Matters: Learning Generalizable Rewards via Calibrated Features",
    "url": "http://arxiv.org/abs/2506.15012v1",
    "arxiv_id": "2506.15012v1",
    "authors": [
      "Alexandra Forsey-Smerek",
      "Julie Shah",
      "Andreea Bobu"
    ],
    "published": "2025-06-17T22:48:44+00:00",
    "summary": "A key challenge in reward learning from human input is that desired agent behavior often changes based on context. Traditional methods typically treat each new context as a separate task with its own reward function. For example, if a previously ignored stove becomes too hot to be around, the robot must learn a new reward from scratch, even though the underlying preference for prioritizing safety over efficiency remains unchanged. We observe that context influences not the underlying preference itself, but rather the $\\textit{saliency}$--or importance--of reward features. For instance, stove heat affects the importance of the robot's proximity, yet the human's safety preference stays the same. Existing multi-task and meta IRL methods learn context-dependent representations $\\textit{implicitly}$--without distinguishing between preferences and feature importance--resulting in substantial data requirements. Instead, we propose $\\textit{explicitly}$ modeling context-invariant preferences separately from context-dependent feature saliency, creating modular reward representations that adapt to new contexts. To achieve this, we introduce $\\textit{calibrated features}$--representations that capture contextual effects on feature saliency--and present specialized paired comparison queries that isolate saliency from preference for efficient learning. Experiments with simulated users show our method significantly improves sample efficiency, requiring 10x fewer preference queries than baselines to achieve equivalent reward accuracy, with up to 15% better performance in low-data regimes (5-10 queries). An in-person user study (N=12) demonstrates that participants can effectively teach their unique personal contextual preferences using our method, enabling more adaptable and personalized reward learning."
  },
  {
    "title": "Mixed Traffic: A Perspective from Long Duration Autonomy",
    "url": "http://arxiv.org/abs/2506.15004v1",
    "arxiv_id": "2506.15004v1",
    "authors": [
      "Filippos Tzortzoglou",
      "Logan E. Beaver"
    ],
    "published": "2025-06-17T22:25:25+00:00",
    "summary": "The rapid adoption of autonomous vehicle has established mixed traffic environments, comprising both autonomous and human-driven vehicles (HDVs), as essential components of next-generation mobility systems. Along these lines, connectivity between autonomous vehicles and infrastructure (V2I) is also a significant factor that can effectively support higher-level decision-making. At the same time, the integration of V2I within mixed traffic environments remains a timely and challenging problem. In this paper, we present a long-duration autonomy controller for connected and automated vehicles (CAVs) operating in such environments, with a focus on intersections where right turns on red are permitted. We begin by deriving the optimal control policy for CAVs under free-flow traffic. Next, we analyze crossing time constraints imposed by smart traffic lights and map these constraints to controller bounds using Control Barrier Functions (CBFs), with the aim to drive a CAV to cross the intersection on time. We also introduce criteria for identifying, in real-time, feasible crossing intervals for each CAV. To ensure safety for the CAVs, we present model-agnostic safety guarantees, and demonstrate their compatibility with both CAVs and HDVs. Ultimately, the final control actions are enforced through a combination of CBF constraints, constraining CAVs to traverse the intersection within the designated time intervals while respecting other vehicles. Finally, we guarantee that our control policy yields always a feasible solution and validate the proposed approach through extensive simulations in MATLAB."
  },
  {
    "title": "Time-Optimized Safe Navigation in Unstructured Environments through Learning Based Depth Completion",
    "url": "http://arxiv.org/abs/2506.14975v1",
    "arxiv_id": "2506.14975v1",
    "authors": [
      "Jeffrey Mao",
      "Raghuram Cauligi Srinivas",
      "Steven Nogar",
      "Giuseppe Loianno"
    ],
    "published": "2025-06-17T21:01:05+00:00",
    "summary": "Quadrotors hold significant promise for several applications such as agriculture, search and rescue, and infrastructure inspection. Achieving autonomous operation requires systems to navigate safely through complex and unfamiliar environments. This level of autonomy is particularly challenging due to the complexity of such environments and the need for real-time decision making especially for platforms constrained by size, weight, and power (SWaP), which limits flight time and precludes the use of bulky sensors like Light Detection and Ranging (LiDAR) for mapping. Furthermore, computing globally optimal, collision-free paths and translating them into time-optimized, safe trajectories in real time adds significant computational complexity. To address these challenges, we present a fully onboard, real-time navigation system that relies solely on lightweight onboard sensors. Our system constructs a dense 3D map of the environment using a novel visual depth estimation approach that fuses stereo and monocular learning-based depth, yielding longer-range, denser, and less noisy depth maps than conventional stereo methods. Building on this map, we introduce a novel planning and trajectory generation framework capable of rapidly computing time-optimal global trajectories. As the map is incrementally updated with new depth information, our system continuously refines the trajectory to maintain safety and optimality. Both our planner and trajectory generator outperforms state-of-the-art methods in terms of computational efficiency and guarantee obstacle-free trajectories. We validate our system through robust autonomous flight experiments in diverse indoor and outdoor environments, demonstrating its effectiveness for safe navigation in previously unknown settings."
  },
  {
    "title": "FEAST: A Flexible Mealtime-Assistance System Towards In-the-Wild Personalization",
    "url": "http://arxiv.org/abs/2506.14968v1",
    "arxiv_id": "2506.14968v1",
    "authors": [
      "Rajat Kumar Jenamani",
      "Tom Silver",
      "Ben Dodson",
      "Shiqin Tong",
      "Anthony Song",
      "Yuting Yang",
      "Ziang Liu",
      "Benjamin Howe",
      "Aimee Whitneck",
      "Tapomayukh Bhattacharjee"
    ],
    "published": "2025-06-17T20:30:11+00:00",
    "summary": "Physical caregiving robots hold promise for improving the quality of life of millions worldwide who require assistance with feeding. However, in-home meal assistance remains challenging due to the diversity of activities (e.g., eating, drinking, mouth wiping), contexts (e.g., socializing, watching TV), food items, and user preferences that arise during deployment. In this work, we propose FEAST, a flexible mealtime-assistance system that can be personalized in-the-wild to meet the unique needs of individual care recipients. Developed in collaboration with two community researchers and informed by a formative study with a diverse group of care recipients, our system is guided by three key tenets for in-the-wild personalization: adaptability, transparency, and safety. FEAST embodies these principles through: (i) modular hardware that enables switching between assisted feeding, drinking, and mouth-wiping, (ii) diverse interaction methods, including a web interface, head gestures, and physical buttons, to accommodate diverse functional abilities and preferences, and (iii) parameterized behavior trees that can be safely and transparently adapted using a large language model. We evaluate our system based on the personalization requirements identified in our formative study, demonstrating that FEAST offers a wide range of transparent and safe adaptations and outperforms a state-of-the-art baseline limited to fixed customizations. To demonstrate real-world applicability, we conduct an in-home user study with two care recipients (who are community researchers), feeding them three meals each across three diverse scenarios. We further assess FEAST's ecological validity by evaluating with an Occupational Therapist previously unfamiliar with the system. In all cases, users successfully personalize FEAST to meet their individual needs and preferences. Website: https://emprise.cs.cornell.edu/feast"
  },
  {
    "title": "FORTRESS: Frontier Risk Evaluation for National Security and Public Safety",
    "url": "http://arxiv.org/abs/2506.14922v1",
    "arxiv_id": "2506.14922v1",
    "authors": [
      "Christina Q. Knight",
      "Kaustubh Deshpande",
      "Ved Sirdeshmukh",
      "Meher Mankikar",
      "Scale Red Team",
      "SEAL Research Team",
      "Julian Michael"
    ],
    "published": "2025-06-17T19:08:02+00:00",
    "summary": "The rapid advancement of large language models (LLMs) introduces dual-use capabilities that could both threaten and bolster national security and public safety (NSPS). Models implement safeguards to protect against potential misuse relevant to NSPS and allow for benign users to receive helpful information. However, current benchmarks often fail to test safeguard robustness to potential NSPS risks in an objective, robust way. We introduce FORTRESS: 500 expert-crafted adversarial prompts with instance-based rubrics of 4-7 binary questions for automated evaluation across 3 domains (unclassified information only): Chemical, Biological, Radiological, Nuclear and Explosive (CBRNE), Political Violence & Terrorism, and Criminal & Financial Illicit Activities, with 10 total subcategories across these domains. Each prompt-rubric pair has a corresponding benign version to test for model over-refusals. This evaluation of frontier LLMs' safeguard robustness reveals varying trade-offs between potential risks and model usefulness: Claude-3.5-Sonnet demonstrates a low average risk score (ARS) (14.09 out of 100) but the highest over-refusal score (ORS) (21.8 out of 100), while Gemini 2.5 Pro shows low over-refusal (1.4) but a high average potential risk (66.29). Deepseek-R1 has the highest ARS at 78.05, but the lowest ORS at only 0.06. Models such as o1 display a more even trade-off between potential risks and over-refusals (with an ARS of 21.69 and ORS of 5.2). To provide policymakers and researchers with a clear understanding of models' potential risks, we publicly release FORTRESS at https://huggingface.co/datasets/ScaleAI/fortress_public. We also maintain a private set for evaluation."
  },
  {
    "title": "PeRL: Permutation-Enhanced Reinforcement Learning for Interleaved Vision-Language Reasoning",
    "url": "http://arxiv.org/abs/2506.14907v1",
    "arxiv_id": "2506.14907v1",
    "authors": [
      "Yizhen Zhang",
      "Yang Ding",
      "Shuoshuo Zhang",
      "Xinchen Zhang",
      "Haoling Li",
      "Zhong-zhi Li",
      "Peijie Wang",
      "Jie Wu",
      "Lei Ji",
      "Yelong Shen",
      "Yujiu Yang",
      "Yeyun Gong"
    ],
    "published": "2025-06-17T18:25:56+00:00",
    "summary": "Inspired by the impressive reasoning capabilities demonstrated by reinforcement learning approaches like DeepSeek-R1, recent emerging research has begun exploring the use of reinforcement learning (RL) to enhance vision-language models (VLMs) for multimodal reasoning tasks. However, most existing multimodal reinforcement learning approaches remain limited to spatial reasoning within single-image contexts, yet still struggle to generalize to more complex and real-world scenarios involving multi-image positional reasoning, where understanding the relationships across images is crucial. To address this challenge, we propose a general reinforcement learning approach PeRL tailored for interleaved multimodal tasks, and a multi-stage strategy designed to enhance the exploration-exploitation trade-off, thereby improving learning efficiency and task performance. Specifically, we introduce permutation of image sequences to simulate varied positional relationships to explore more spatial and positional diversity. Furthermore, we design a rollout filtering mechanism for resampling to focus on trajectories that contribute most to learning optimal behaviors to exploit learned policies effectively. We evaluate our model on 5 widely-used multi-image benchmarks and 3 single-image benchmarks. Our experiments confirm that PeRL trained model consistently surpasses R1-related and interleaved VLM baselines by a large margin, achieving state-of-the-art performance on multi-image benchmarks, while preserving comparable performance on single-image tasks."
  },
  {
    "title": "DETONATE: A Benchmark for Text-to-Image Alignment and Kernelized Direct Preference Optimization",
    "url": "http://arxiv.org/abs/2506.14903v1",
    "arxiv_id": "2506.14903v1",
    "authors": [
      "Renjith Prasad",
      "Abhilekh Borah",
      "Hasnat Md Abdullah",
      "Chathurangi Shyalika",
      "Gurpreet Singh",
      "Ritvik Garimella",
      "Rajarshi Roy",
      "Harshul Surana",
      "Nasrin Imanpour",
      "Suranjana Trivedy",
      "Amit Sheth",
      "Amitava Das"
    ],
    "published": "2025-06-17T18:17:35+00:00",
    "summary": "Alignment is crucial for text-to-image (T2I) models to ensure that generated images faithfully capture user intent while maintaining safety and fairness. Direct Preference Optimization (DPO), prominent in large language models (LLMs), is extending its influence to T2I systems. This paper introduces DPO-Kernels for T2I models, a novel extension enhancing alignment across three dimensions: (i) Hybrid Loss, integrating embedding-based objectives with traditional probability-based loss for improved optimization; (ii) Kernelized Representations, employing Radial Basis Function (RBF), Polynomial, and Wavelet kernels for richer feature transformations and better separation between safe and unsafe inputs; and (iii) Divergence Selection, expanding beyond DPO's default Kullback-Leibler (KL) regularizer by incorporating Wasserstein and R'enyi divergences for enhanced stability and robustness. We introduce DETONATE, the first large-scale benchmark of its kind, comprising approximately 100K curated image pairs categorized as chosen and rejected. DETONATE encapsulates three axes of social bias and discrimination: Race, Gender, and Disability. Prompts are sourced from hate speech datasets, with images generated by leading T2I models including Stable Diffusion 3.5 Large, Stable Diffusion XL, and Midjourney. Additionally, we propose the Alignment Quality Index (AQI), a novel geometric measure quantifying latent-space separability of safe/unsafe image activations, revealing hidden vulnerabilities. Empirically, we demonstrate that DPO-Kernels maintain strong generalization bounds via Heavy-Tailed Self-Regularization (HT-SR). DETONATE and complete code are publicly released."
  },
  {
    "title": "OS-Harm: A Benchmark for Measuring Safety of Computer Use Agents",
    "url": "http://arxiv.org/abs/2506.14866v1",
    "arxiv_id": "2506.14866v1",
    "authors": [
      "Thomas Kuntz",
      "Agatha Duzan",
      "Hao Zhao",
      "Francesco Croce",
      "Zico Kolter",
      "Nicolas Flammarion",
      "Maksym Andriushchenko"
    ],
    "published": "2025-06-17T17:59:31+00:00",
    "summary": "Computer use agents are LLM-based agents that can directly interact with a graphical user interface, by processing screenshots or accessibility trees. While these systems are gaining popularity, their safety has been largely overlooked, despite the fact that evaluating and understanding their potential for harmful behavior is essential for widespread adoption. To address this gap, we introduce OS-Harm, a new benchmark for measuring safety of computer use agents. OS-Harm is built on top of the OSWorld environment and aims to test models across three categories of harm: deliberate user misuse, prompt injection attacks, and model misbehavior. To cover these cases, we create 150 tasks that span several types of safety violations (harassment, copyright infringement, disinformation, data exfiltration, etc.) and require the agent to interact with a variety of OS applications (email client, code editor, browser, etc.). Moreover, we propose an automated judge to evaluate both accuracy and safety of agents that achieves high agreement with human annotations (0.76 and 0.79 F1 score). We evaluate computer use agents based on a range of frontier models - such as o4-mini, Claude 3.7 Sonnet, Gemini 2.5 Pro - and provide insights into their safety. In particular, all models tend to directly comply with many deliberate misuse queries, are relatively vulnerable to static prompt injections, and occasionally perform unsafe actions. The OS-Harm benchmark is available at https://github.com/tml-epfl/os-harm."
  },
  {
    "title": "Markov Regime-Switching Intelligent Driver Model for Interpretable Car-Following Behavior",
    "url": "http://arxiv.org/abs/2506.14762v1",
    "arxiv_id": "2506.14762v1",
    "authors": [
      "Chengyuan Zhang",
      "Cathy Wu",
      "Lijun Sun"
    ],
    "published": "2025-06-17T17:55:42+00:00",
    "summary": "Accurate and interpretable car-following models are essential for traffic simulation and autonomous vehicle development. However, classical models like the Intelligent Driver Model (IDM) are fundamentally limited by their parsimonious and single-regime structure. They fail to capture the multi-modal nature of human driving, where a single driving state (e.g., speed, relative speed, and gap) can elicit many different driver actions. This forces the model to average across distinct behaviors, reducing its fidelity and making its parameters difficult to interpret. To overcome this, we introduce a regime-switching framework that allows driving behavior to be governed by different IDM parameter sets, each corresponding to an interpretable behavioral mode. This design enables the model to dynamically switch between interpretable behavioral modes, rather than averaging across diverse driving contexts. We instantiate the framework using a Factorial Hidden Markov Model with IDM dynamics (FHMM-IDM), which explicitly separates intrinsic driving regimes (e.g., aggressive acceleration, steady-state following) from external traffic scenarios (e.g., free-flow, congestion, stop-and-go) through two independent latent Markov processes. Bayesian inference via Markov chain Monte Carlo (MCMC) is used to jointly estimate the regime-specific parameters, transition dynamics, and latent state trajectories. Experiments on the HighD dataset demonstrate that FHMM-IDM uncovers interpretable structure in human driving, effectively disentangling internal driver actions from contextual traffic conditions and revealing dynamic regime-switching patterns. This framework provides a tractable and principled solution to modeling context-dependent driving behavior under uncertainty, offering improvements in the fidelity of traffic simulations, the efficacy of safety analyses, and the development of more human-centric ADAS."
  },
  {
    "title": "Swarm-STL: A Framework for Motion Planning in Large-Scale, Multi-Swarm Systems",
    "url": "http://arxiv.org/abs/2506.14749v1",
    "arxiv_id": "2506.14749v1",
    "authors": [
      "Shiyu Cheng",
      "Luyao Niu",
      "Bhaskar Ramasubramanian",
      "Andrew Clark",
      "Radha Poovendran"
    ],
    "published": "2025-06-17T17:40:12+00:00",
    "summary": "In multi-agent systems, signal temporal logic (STL) is widely used for path planning to accomplish complex objectives with formal safety guarantees. However, as the number of agents increases, existing approaches encounter significant computational challenges. Recognizing that many complex tasks require cooperation among multiple agents, we propose swarm STL specifications to describe the collective tasks that need to be achieved by a team of agents. Next, we address the motion planning problem for all the agents in two stages. First, we abstract a group of cooperating agents as a swarm and construct a reduced-dimension state space whose dimension does not increase with the number of agents. The path planning is performed at the swarm level, ensuring the safety and swarm STL specifications are satisfied. Then, we design low-level control strategies for agents within each swarm based on the path synthesized in the first step. The trajectories of agents generated by the two-step policy ensure satisfaction of the STL specifications. We evaluate our two-stage approach in both single-swarm and multi-swarm scenarios. The results demonstrate that all tasks are completed with safety guarantees. Compared to the baseline multi-agent planning approach, our method maintains computational efficiency as the number of agents increases, since the computational time scales with the number of swarms rather than the number of agents."
  },
  {
    "title": "AGENTSAFE: Benchmarking the Safety of Embodied Agents on Hazardous Instructions",
    "url": "http://arxiv.org/abs/2506.14697v1",
    "arxiv_id": "2506.14697v1",
    "authors": [
      "Aishan Liu",
      "Zonghao Ying",
      "Le Wang",
      "Junjie Mu",
      "Jinyang Guo",
      "Jiakai Wang",
      "Yuqing Ma",
      "Siyuan Liang",
      "Mingchuan Zhang",
      "Xianglong Liu",
      "Dacheng Tao"
    ],
    "published": "2025-06-17T16:37:35+00:00",
    "summary": "The rapid advancement of vision-language models (VLMs) and their integration into embodied agents have unlocked powerful capabilities for decision-making. However, as these systems are increasingly deployed in real-world environments, they face mounting safety concerns, particularly when responding to hazardous instructions. In this work, we propose AGENTSAFE, the first comprehensive benchmark for evaluating the safety of embodied VLM agents under hazardous instructions. AGENTSAFE simulates realistic agent-environment interactions within a simulation sandbox and incorporates a novel adapter module that bridges the gap between high-level VLM outputs and low-level embodied controls. Specifically, it maps recognized visual entities to manipulable objects and translates abstract planning into executable atomic actions in the environment. Building on this, we construct a risk-aware instruction dataset inspired by Asimovs Three Laws of Robotics, including base risky instructions and mutated jailbroken instructions. The benchmark includes 45 adversarial scenarios, 1,350 hazardous tasks, and 8,100 hazardous instructions, enabling systematic testing under adversarial conditions ranging from perception, planning, and action execution stages."
  },
  {
    "title": "AIRTBench: Measuring Autonomous AI Red Teaming Capabilities in Language Models",
    "url": "http://arxiv.org/abs/2506.14682v1",
    "arxiv_id": "2506.14682v1",
    "authors": [
      "Ads Dawson",
      "Rob Mulla",
      "Nick Landers",
      "Shane Caldwell"
    ],
    "published": "2025-06-17T16:19:06+00:00",
    "summary": "We introduce AIRTBench, an AI red teaming benchmark for evaluating language models' ability to autonomously discover and exploit Artificial Intelligence and Machine Learning (AI/ML) security vulnerabilities. The benchmark consists of 70 realistic black-box capture-the-flag (CTF) challenges from the Crucible challenge environment on the Dreadnode platform, requiring models to write python code to interact with and compromise AI systems. Claude-3.7-Sonnet emerged as the clear leader, solving 43 challenges (61% of the total suite, 46.9% overall success rate), with Gemini-2.5-Pro following at 39 challenges (56%, 34.3% overall), GPT-4.5-Preview at 34 challenges (49%, 36.9% overall), and DeepSeek R1 at 29 challenges (41%, 26.9% overall). Our evaluations show frontier models excel at prompt injection attacks (averaging 49% success rates) but struggle with system exploitation and model inversion challenges (below 26%, even for the best performers). Frontier models are far outpacing open-source alternatives, with the best truly open-source model (Llama-4-17B) solving 7 challenges (10%, 1.0% overall), though demonstrating specialized capabilities on certain hard challenges. Compared to human security researchers, large language models (LLMs) solve challenges with remarkable efficiency completing in minutes what typically takes humans hours or days-with efficiency advantages of over 5,000x on hard challenges. Our contribution fills a critical gap in the evaluation landscape, providing the first comprehensive benchmark specifically designed to measure and track progress in autonomous AI red teaming capabilities."
  },
  {
    "title": "StreetLens: Enabling Human-Centered AI Agents for Neighborhood Assessment from Street View Imagery",
    "url": "http://arxiv.org/abs/2506.14670v1",
    "arxiv_id": "2506.14670v1",
    "authors": [
      "Jina Kim",
      "Leeje Jang",
      "Yao-Yi Chiang",
      "Guanyu Wang",
      "Michelle Pasco"
    ],
    "published": "2025-06-17T16:06:03+00:00",
    "summary": "Traditionally, neighborhood studies have employed interviews, surveys, and manual image annotation guided by detailed protocols to identify environmental characteristics, including physical disorder, decay, street safety, and sociocultural symbols, and to examine their impact on developmental and health outcomes. While these methods yield rich insights, they are time-consuming and require intensive expert intervention. Recent technological advances, including vision-language models (VLMs), have begun to automate parts of this process; however, existing efforts are often ad hoc and lack adaptability across research designs and geographic contexts. In this demo paper, we present StreetLens, a human-centered, researcher-configurable workflow that embeds relevant social science expertise in a VLM for scalable neighborhood environmental assessments. StreetLens mimics the process of trained human coders by grounding the analysis in questions derived from established interview protocols, retrieving relevant street view imagery (SVI), and generating a wide spectrum of semantic annotations from objective features (e.g., the number of cars) to subjective perceptions (e.g., the sense of disorder in an image). By enabling researchers to define the VLM's role through domain-informed prompting, StreetLens places domain knowledge at the core of the analysis process. It also supports the integration of prior survey data to enhance robustness and expand the range of characteristics assessed across diverse settings. We provide a Google Colab notebook to make StreetLens accessible and extensible for researchers working with public or custom SVI datasets. StreetLens represents a shift toward flexible, agentic AI systems that work closely with researchers to accelerate and scale neighborhood studies."
  },
  {
    "title": "NetRoller: Interfacing General and Specialized Models for End-to-End Autonomous Driving",
    "url": "http://arxiv.org/abs/2506.14589v1",
    "arxiv_id": "2506.14589v1",
    "authors": [
      "Ren Xin",
      "Hongji Liu",
      "Xiaodong Mei",
      "Wenru Liu",
      "Maosheng Ye",
      "Zhili Chen",
      "Jun Ma"
    ],
    "published": "2025-06-17T14:52:50+00:00",
    "summary": "Integrating General Models (GMs) such as Large Language Models (LLMs), with Specialized Models (SMs) in autonomous driving tasks presents a promising approach to mitigating challenges in data diversity and model capacity of existing specialized driving models. However, this integration leads to problems of asynchronous systems, which arise from the distinct characteristics inherent in GMs and SMs. To tackle this challenge, we propose NetRoller, an adapter that incorporates a set of novel mechanisms to facilitate the seamless integration of GMs and specialized driving models. Specifically, our mechanisms for interfacing the asynchronous GMs and SMs are organized into three key stages. NetRoller first harvests semantically rich and computationally efficient representations from the reasoning processes of LLMs using an early stopping mechanism, which preserves critical insights on driving context while maintaining low overhead. It then applies learnable query embeddings, nonsensical embeddings, and positional layer embeddings to facilitate robust and efficient cross-modality translation. At last, it employs computationally efficient Query Shift and Feature Shift mechanisms to enhance the performance of SMs through few-epoch fine-tuning. Based on the mechanisms formalized in these three stages, NetRoller enables specialized driving models to operate at their native frequencies while maintaining situational awareness of the GM. Experiments conducted on the nuScenes dataset demonstrate that integrating GM through NetRoller significantly improves human similarity and safety in planning tasks, and it also achieves noticeable precision improvements in detection and mapping tasks for end-to-end autonomous driving. The code and models are available at https://github.com/Rex-sys-hk/NetRoller ."
  },
  {
    "title": "Modeling Uncertainty: From Simulink to Stochastic Hybrid Automata",
    "url": "http://arxiv.org/abs/2506.14581v1",
    "arxiv_id": "2506.14581v1",
    "authors": [
      "Pauline Blohm",
      "Felix Schulz",
      "Lisa Willemsen",
      "Anne Remke",
      "Paula Herber"
    ],
    "published": "2025-06-17T14:37:36+00:00",
    "summary": "Simulink is widely used in industrial design processes to model increasingly complex embedded control systems. Thus, their formal analysis is highly desirable. However, this comes with two major challenges: First, Simulink models often provide an idealized view of real-life systems and omit uncertainties such as, aging, sensor noise or failures. Second, the semantics of Simulink is only informally defined. In this paper, we present an approach to formally analyze safety and performance of embedded control systems modeled in Simulink in the presence of uncertainty. To achieve this, we 1) model different types of uncertainties as stochastic Simulink subsystems and 2) extend an existing formalization of the Simulink semantics based on stochastic hybrid automata (SHA) by providing transformation rules for the stochastic subsystems. Our approach gives us access to established quantitative analysis techniques, like statistical model checking and reachability analysis. We demonstrate the applicability of our approach by analyzing safety and performance in the presence of uncertainty for two smaller case studies."
  },
  {
    "title": "Object-Centric Neuro-Argumentative Learning",
    "url": "http://arxiv.org/abs/2506.14577v1",
    "arxiv_id": "2506.14577v1",
    "authors": [
      "Abdul Rahman Jacob",
      "Avinash Kori",
      "Emanuele De Angelis",
      "Ben Glocker",
      "Maurizio Proietti",
      "Francesca Toni"
    ],
    "published": "2025-06-17T14:35:01+00:00",
    "summary": "Over the last decade, as we rely more on deep learning technologies to make critical decisions, concerns regarding their safety, reliability and interpretability have emerged. We introduce a novel Neural Argumentative Learning (NAL) architecture that integrates Assumption-Based Argumentation (ABA) with deep learning for image analysis. Our architecture consists of neural and symbolic components. The former segments and encodes images into facts using object-centric learning, while the latter applies ABA learning to develop ABA frameworks enabling predictions with images. Experiments on synthetic data show that the NAL architecture can be competitive with a state-of-the-art alternative."
  },
  {
    "title": "Doppelg\u00e4nger Method: Breaking Role Consistency in LLM Agent via Prompt-based Transferable Adversarial Attack",
    "url": "http://arxiv.org/abs/2506.14539v1",
    "arxiv_id": "2506.14539v1",
    "authors": [
      "Daewon Kang",
      "YeongHwan Shin",
      "Doyeon Kim",
      "Kyu-Hwan Jung",
      "Meong Hi Son"
    ],
    "published": "2025-06-17T14:01:39+00:00",
    "summary": "Since the advent of large language models, prompt engineering now enables the rapid, low-effort creation of diverse autonomous agents that are already in widespread use. Yet this convenience raises urgent concerns about the safety, robustness, and behavioral consistency of the underlying prompts, along with the pressing challenge of preventing those prompts from being exposed to user's attempts. In this paper, we propose the ''Doppelg\\\"anger method'' to demonstrate the risk of an agent being hijacked, thereby exposing system instructions and internal information. Next, we define the ''Prompt Alignment Collapse under Adversarial Transfer (PACAT)'' level to evaluate the vulnerability to this adversarial transfer attack. We also propose a ''Caution for Adversarial Transfer (CAT)'' prompt to counter the Doppelg\\\"anger method. The experimental results demonstrate that the Doppelg\\\"anger method can compromise the agent's consistency and expose its internal information. In contrast, CAT prompts enable effective defense against this adversarial attack."
  },
  {
    "title": "Toward Safety-First Human-Like Decision Making for Autonomous Vehicles in Time-Varying Traffic Flow",
    "url": "http://arxiv.org/abs/2506.14502v1",
    "arxiv_id": "2506.14502v1",
    "authors": [
      "Xiao Wang",
      "Junru Yu",
      "Jun Huang",
      "Qiong Wu",
      "Ljubo Vacic",
      "Changyin Sun"
    ],
    "published": "2025-06-17T13:28:19+00:00",
    "summary": "Despite the recent advancements in artificial intelligence technologies have shown great potential in improving transport efficiency and safety, autonomous vehicles(AVs) still face great challenge of driving in time-varying traffic flow, especially in dense and interactive situations. Meanwhile, human have free wills and usually do not make the same decisions even situate in the exactly same scenarios, leading to the data-driven methods suffer from poor migratability and high search cost problems, decreasing the efficiency and effectiveness of the behavior policy. In this research, we propose a safety-first human-like decision-making framework(SF-HLDM) for AVs to drive safely, comfortably, and social compatiblely in effiency. The framework integrates a hierarchical progressive framework, which combines a spatial-temporal attention (S-TA) mechanism for other road users' intention inference, a social compliance estimation module for behavior regulation, and a Deep Evolutionary Reinforcement Learning(DERL) model for expanding the search space efficiently and effectively to make avoidance of falling into the local optimal trap and reduce the risk of overfitting, thus make human-like decisions with interpretability and flexibility. The SF-HLDM framework enables autonomous driving AI agents dynamically adjusts decision parameters to maintain safety margins and adhering to contextually appropriate driving behaviors at the same time."
  },
  {
    "title": "Enclosing Prototypical Variational Autoencoder for Explainable Out-of-Distribution Detection",
    "url": "http://arxiv.org/abs/2506.14390v1",
    "arxiv_id": "2506.14390v1",
    "authors": [
      "Conrad Orglmeister",
      "Erik Bochinski",
      "Volker Eiselein",
      "Elvira Fleig"
    ],
    "published": "2025-06-17T10:38:29+00:00",
    "summary": "Understanding the decision-making and trusting the reliability of Deep Machine Learning Models is crucial for adopting such methods to safety-relevant applications. We extend self-explainable Prototypical Variational models with autoencoder-based out-of-distribution (OOD) detection: A Variational Autoencoder is applied to learn a meaningful latent space which can be used for distance-based classification, likelihood estimation for OOD detection, and reconstruction. The In-Distribution (ID) region is defined by a Gaussian mixture distribution with learned prototypes representing the center of each mode. Furthermore, a novel restriction loss is introduced that promotes a compact ID region in the latent space without collapsing it into single points. The reconstructive capabilities of the Autoencoder ensure the explainability of the prototypes and the ID region of the classifier, further aiding the discrimination of OOD samples. Extensive evaluations on common OOD detection benchmarks as well as a large-scale dataset from a real-world railway application demonstrate the usefulness of the approach, outperforming previous methods."
  },
  {
    "title": "Don't Make It Up: Preserving Ignorance Awareness in LLM Fine-Tuning",
    "url": "http://arxiv.org/abs/2506.14387v1",
    "arxiv_id": "2506.14387v1",
    "authors": [
      "William F. Shen",
      "Xinchi Qiu",
      "Nicola Cancedda",
      "Nicholas D. Lane"
    ],
    "published": "2025-06-17T10:33:23+00:00",
    "summary": "Existing work on mitigating catastrophic forgetting in large language model (LLM) fine-tuning has primarily focused on preserving specific data or tasks, while critically overlooking the degradation of essential capabilities instilled through safety alignment, particularly the model's ability to faithfully express ignorance. In this work, we show that this capability is significantly degraded during conventional fine-tuning, leading to undesired behaviors such as hallucinations. To address this novel but highly practical problem, we propose SEAT, a simple and effective fine-tuning approach that preserves both fine-tuning performance and the model's inherent ability to acknowledge its ignorance. SEAT integrates two key components: (1) sparse training that constrains activation drift, and (2) a novel entity perturbation method with KL-divergence regularization, designed to counter knowledge entanglement. Experimental results demonstrate that SEAT significantly outperforms baselines in preserving ignorance awareness while retaining fine-tuning performance, offering a more robust solution for LLM fine-tuning."
  },
  {
    "title": "IntelliLung: Advancing Safe Mechanical Ventilation using Offline RL with Hybrid Actions and Clinically Aligned Rewards",
    "url": "http://arxiv.org/abs/2506.14375v1",
    "arxiv_id": "2506.14375v1",
    "authors": [
      "Muhammad Hamza Yousuf",
      "Jason Li",
      "Sahar Vahdati",
      "Raphael Theilen",
      "Jakob Wittenstein",
      "Jens Lehmann"
    ],
    "published": "2025-06-17T10:17:26+00:00",
    "summary": "Invasive mechanical ventilation (MV) is a life-sustaining therapy for critically ill patients in the intensive care unit (ICU). However, optimizing its settings remains a complex and error-prone process due to patient-specific variability. While Offline Reinforcement Learning (RL) shows promise for MV control, current stateof-the-art (SOTA) methods struggle with the hybrid (continuous and discrete) nature of MV actions. Discretizing the action space limits available actions due to exponential growth in combinations and introduces distribution shifts that can compromise safety. In this paper, we propose optimizations that build upon prior work in action space reduction to address the challenges of discrete action spaces. We also adapt SOTA offline RL algorithms (IQL and EDAC) to operate directly on hybrid action spaces, thereby avoiding the pitfalls of discretization. Additionally, we introduce a clinically grounded reward function based on ventilator-free days and physiological targets, which provides a more meaningful optimization objective compared to traditional sparse mortality-based rewards. Our findings demonstrate that AI-assisted MV optimization may enhance patient safety and enable individualized lung support, representing a significant advancement toward intelligent, data-driven critical care solutions."
  },
  {
    "title": "Excessive Reasoning Attack on Reasoning LLMs",
    "url": "http://arxiv.org/abs/2506.14374v1",
    "arxiv_id": "2506.14374v1",
    "authors": [
      "Wai Man Si",
      "Mingjie Li",
      "Michael Backes",
      "Yang Zhang"
    ],
    "published": "2025-06-17T10:16:52+00:00",
    "summary": "Recent reasoning large language models (LLMs), such as OpenAI o1 and DeepSeek-R1, exhibit strong performance on complex tasks through test-time inference scaling. However, prior studies have shown that these models often incur significant computational costs due to excessive reasoning, such as frequent switching between reasoning trajectories (e.g., underthinking) or redundant reasoning on simple questions (e.g., overthinking). In this work, we expose a novel threat: adversarial inputs can be crafted to exploit excessive reasoning behaviors and substantially increase computational overhead without compromising model utility. Therefore, we propose a novel loss framework consisting of three components: (1) Priority Cross-Entropy Loss, a modification of the standard cross-entropy objective that emphasizes key tokens by leveraging the autoregressive nature of LMs; (2) Excessive Reasoning Loss, which encourages the model to initiate additional reasoning paths during inference; and (3) Delayed Termination Loss, which is designed to extend the reasoning process and defer the generation of final outputs. We optimize and evaluate our attack for the GSM8K and ORCA datasets on DeepSeek-R1-Distill-LLaMA and DeepSeek-R1-Distill-Qwen. Empirical results demonstrate a 3x to 9x increase in reasoning length with comparable utility performance. Furthermore, our crafted adversarial inputs exhibit transferability, inducing computational overhead in o3-mini, o1-mini, DeepSeek-R1, and QWQ models."
  },
  {
    "title": "AviationLLM: An LLM-based Knowledge System for Aviation Training",
    "url": "http://arxiv.org/abs/2506.14336v1",
    "arxiv_id": "2506.14336v1",
    "authors": [
      "Jia'ang Wan",
      "Feng Shen",
      "Fujuan Li",
      "Yanjin Sun",
      "Yan Li",
      "Shiwen Zhang"
    ],
    "published": "2025-06-17T09:20:09+00:00",
    "summary": "Aviation training is a core link in ensuring flight safety, improving industry efficiency and promoting sustainable development. It not only involves flight simulation but also requires the learning of a great deal of professional aviation theory knowledge. In the existing training system, the knowledge is mainly imparted by the the instructors. However, the number of instructors is limited and the professional answers obtained from the Internet are not accurate enough, resulting in low training efficiency. To address this, we introduced LLM, but the basic pre-trained model cannot provide accurate answers to professional fields, so we fine-tuned it. Traditional Supervised Fine-Tuning (SFT) risk generating superficially plausible but factually incorrect responses due to insufficient data coverage. To address this, we employ Direct Preference Optimization(DPO). This paper proposes Retrieval-Augmented LLM Alignment via Direct Preference Optimization(RALA-DPO). We select open source pre-trained LLM Qwen and adapt it to aviation theory training through DPO-based domain alignment. Simultaneously, to mitigate hallucinations caused by training data biases, knowledge obsolescence, or domain knowledge gaps, we implement Retrieval-Augmented Generation(RAG) technology that combines generative and retrieval models. RALA-DPO effectively retrieves relevant information from external knowledge bases and delivers precise and high-quality responses through the generative model. Experimental results demonstrate that RALA-DPO can improve accuracy in response to professional aviation knowledge. With integrated RAG mechanisms, this system can further improve the accuracy of answers and achieve zero-cost knowledge updates simultaneously."
  },
  {
    "title": "ClutterDexGrasp: A Sim-to-Real System for General Dexterous Grasping in Cluttered Scenes",
    "url": "http://arxiv.org/abs/2506.14317v1",
    "arxiv_id": "2506.14317v1",
    "authors": [
      "Zeyuan Chen",
      "Qiyang Yan",
      "Yuanpei Chen",
      "Tianhao Wu",
      "Jiyao Zhang",
      "Zihan Ding",
      "Jinzhou Li",
      "Yaodong Yang",
      "Hao Dong"
    ],
    "published": "2025-06-17T08:50:49+00:00",
    "summary": "Dexterous grasping in cluttered scenes presents significant challenges due to diverse object geometries, occlusions, and potential collisions. Existing methods primarily focus on single-object grasping or grasp-pose prediction without interaction, which are insufficient for complex, cluttered scenes. Recent vision-language-action models offer a potential solution but require extensive real-world demonstrations, making them costly and difficult to scale. To address these limitations, we revisit the sim-to-real transfer pipeline and develop key techniques that enable zero-shot deployment in reality while maintaining robust generalization. We propose ClutterDexGrasp, a two-stage teacher-student framework for closed-loop target-oriented dexterous grasping in cluttered scenes. The framework features a teacher policy trained in simulation using clutter density curriculum learning, incorporating both a novel geometry and spatially-embedded scene representation and a comprehensive safety curriculum, enabling general, dynamic, and safe grasping behaviors. Through imitation learning, we distill the teacher's knowledge into a student 3D diffusion policy (DP3) that operates on partial point cloud observations. To the best of our knowledge, this represents the first zero-shot sim-to-real closed-loop system for target-oriented dexterous grasping in cluttered scenes, demonstrating robust performance across diverse objects and layouts. More details and videos are available at https://clutterdexgrasp.github.io/."
  },
  {
    "title": "Socially Aware Robot Crowd Navigation via Online Uncertainty-Driven Risk Adaptation",
    "url": "http://arxiv.org/abs/2506.14305v1",
    "arxiv_id": "2506.14305v1",
    "authors": [
      "Zhirui Sun",
      "Xingrong Diao",
      "Yao Wang",
      "Bi-Ke Zhu",
      "Jiankun Wang"
    ],
    "published": "2025-06-17T08:33:11+00:00",
    "summary": "Navigation in human-robot shared crowded environments remains challenging, as robots are expected to move efficiently while respecting human motion conventions. However, many existing approaches emphasize safety or efficiency while overlooking social awareness. This article proposes Learning-Risk Model Predictive Control (LR-MPC), a data-driven navigation algorithm that balances efficiency, safety, and social awareness. LR-MPC consists of two phases: an offline risk learning phase, where a Probabilistic Ensemble Neural Network (PENN) is trained using risk data from a heuristic MPC-based baseline (HR-MPC), and an online adaptive inference phase, where local waypoints are sampled and globally guided by a Multi-RRT planner. Each candidate waypoint is evaluated for risk by PENN, and predictions are filtered using epistemic and aleatoric uncertainty to ensure robust decision-making. The safest waypoint is selected as the MPC input for real-time navigation. Extensive experiments demonstrate that LR-MPC outperforms baseline methods in success rate and social awareness, enabling robots to navigate complex crowds with high adaptability and low disruption. A website about this work is available at https://sites.google.com/view/lr-mpc."
  },
  {
    "title": "synth-dacl: Does Synthetic Defect Data Enhance Segmentation Accuracy and Robustness for Real-World Bridge Inspections?",
    "url": "http://arxiv.org/abs/2506.14255v1",
    "arxiv_id": "2506.14255v1",
    "authors": [
      "Johannes Flotzinger",
      "Fabian Deuser",
      "Achref Jaziri",
      "Heiko Neumann",
      "Norbert Oswald",
      "Visvanathan Ramesh",
      "Thomas Braml"
    ],
    "published": "2025-06-17T07:17:15+00:00",
    "summary": "Adequate bridge inspection is increasingly challenging in many countries due to growing ailing stocks, compounded with a lack of staff and financial resources. Automating the key task of visual bridge inspection, classification of defects and building components on pixel level, improves efficiency, increases accuracy and enhances safety in the inspection process and resulting building assessment. Models overtaking this task must cope with an assortment of real-world conditions. They must be robust to variations in image quality, as well as background texture, as defects often appear on surfaces of diverse texture and degree of weathering. dacl10k is the largest and most diverse dataset for real-world concrete bridge inspections. However, the dataset exhibits class imbalance, which leads to notably poor model performance particularly when segmenting fine-grained classes such as cracks and cavities. This work introduces \"synth-dacl\", a compilation of three novel dataset extensions based on synthetic concrete textures. These extensions are designed to balance class distribution in dacl10k and enhance model performance, especially for crack and cavity segmentation. When incorporating the synth-dacl extensions, we observe substantial improvements in model robustness across 15 perturbed test sets. Notably, on the perturbed test set, a model trained on dacl10k combined with all synthetic extensions achieves a 2% increase in mean IoU, F1 score, Recall, and Precision compared to the same model trained solely on dacl10k."
  },
  {
    "title": "Robust Adaptive Time-Varying Control Barrier Function with Application to Robotic Surface Treatment",
    "url": "http://arxiv.org/abs/2506.14249v1",
    "arxiv_id": "2506.14249v1",
    "authors": [
      "Yitaek Kim",
      "Christoffer Sloth"
    ],
    "published": "2025-06-17T07:11:03+00:00",
    "summary": "Set invariance techniques such as control barrier functions (CBFs) can be used to enforce time-varying constraints such as keeping a safe distance from dynamic objects. However, existing methods for enforcing time-varying constraints often overlook model uncertainties. To address this issue, this paper proposes a CBFs-based robust adaptive controller design endowing time-varying constraints while considering parametric uncertainty and additive disturbances. To this end, we first leverage Robust adaptive Control Barrier Functions (RaCBFs) to handle model uncertainty, along with the concept of Input-to-State Safety (ISSf) to ensure robustness towards input disturbances. Furthermore, to alleviate the inherent conservatism in robustness, we also incorporate a set membership identification scheme. We demonstrate the proposed method on robotic surface treatment that requires time-varying force bounds to ensure uniform quality, in numerical simulation and real robotic setup, showing that the quality is formally guaranteed within an acceptable range."
  },
  {
    "title": "Xolver: Multi-Agent Reasoning with Holistic Experience Learning Just Like an Olympiad Team",
    "url": "http://arxiv.org/abs/2506.14234v1",
    "arxiv_id": "2506.14234v1",
    "authors": [
      "Md Tanzib Hosain",
      "Salman Rahman",
      "Md Kishor Morol",
      "Md Rizwan Parvez"
    ],
    "published": "2025-06-17T06:47:19+00:00",
    "summary": "Despite impressive progress on complex reasoning, current large language models (LLMs) typically operate in isolation - treating each problem as an independent attempt, without accumulating or integrating experiential knowledge. In contrast, expert problem solvers - such as Olympiad or programming contest teams - leverage a rich tapestry of experiences: absorbing mentorship from coaches, developing intuition from past problems, leveraging knowledge of tool usage and library functionality, adapting strategies based on the expertise and experiences of peers, continuously refining their reasoning through trial and error, and learning from other related problems even during competition. We introduce Xolver, a training-free multi-agent reasoning framework that equips a black-box LLM with a persistent, evolving memory of holistic experience. Xolver integrates diverse experience modalities, including external and self-retrieval, tool use, collaborative interactions, agent-driven evaluation, and iterative refinement. By learning from relevant strategies, code fragments, and abstract reasoning patterns at inference time, Xolver avoids generating solutions from scratch - marking a transition from isolated inference toward experience-aware language agents. Built on both open-weight and proprietary models, Xolver consistently outperforms specialized reasoning agents. Even with lightweight backbones (e.g., QWQ-32B), it often surpasses advanced models including Qwen3-235B, Gemini 2.5 Pro, o3, and o4-mini-high. With o3-mini-high, it achieves new best results on GSM8K (98.1%), AIME'24 (94.4%), AIME'25 (93.7%), Math-500 (99.8%), and LiveCodeBench-V5 (91.6%) - highlighting holistic experience learning as a key step toward generalist agents capable of expert-level reasoning. Code and data are available at https://kagnlp.github.io/xolver.github.io/."
  },
  {
    "title": "TriGuard: Testing Model Safety with Attribution Entropy, Verification, and Drift",
    "url": "http://arxiv.org/abs/2506.14217v1",
    "arxiv_id": "2506.14217v1",
    "authors": [
      "Dipesh Tharu Mahato",
      "Rohan Poudel",
      "Pramod Dhungana"
    ],
    "published": "2025-06-17T06:12:36+00:00",
    "summary": "Deep neural networks often achieve high accuracy, but ensuring their reliability under adversarial and distributional shifts remains a pressing challenge. We propose TriGuard, a unified safety evaluation framework that combines (1) formal robustness verification, (2) attribution entropy to quantify saliency concentration, and (3) a novel Attribution Drift Score measuring explanation stability. TriGuard reveals critical mismatches between model accuracy and interpretability: verified models can still exhibit unstable reasoning, and attribution-based signals provide complementary safety insights beyond adversarial accuracy. Extensive experiments across three datasets and five architectures show how TriGuard uncovers subtle fragilities in neural reasoning. We further demonstrate that entropy-regularized training reduces explanation drift without sacrificing performance. TriGuard advances the frontier in robust, interpretable model evaluation."
  },
  {
    "title": "The Ethics of Generative AI in Anonymous Spaces: A Case Study of 4chan's /pol/ Board",
    "url": "http://arxiv.org/abs/2506.14191v1",
    "arxiv_id": "2506.14191v1",
    "authors": [
      "Parth Gaba",
      "Emiliano De Cristofaro"
    ],
    "published": "2025-06-17T05:14:16+00:00",
    "summary": "This paper presents a characterization of AI-generated images shared on 4chan, examining how this anonymous online community is (mis-)using generative image technologies. Through a methodical data collection process, we gathered 900 images from 4chan's /pol/ (Politically Incorrect) board, which included the label \"/mwg/\" (memetic warfare general), between April and July 2024, identifying 66 unique AI-generated images. The analysis reveals concerning patterns in the use of this technology, with 69.7% of images including recognizable figures, 28.8% of images containing racist elements, 28.8% featuring anti-Semitic content, and 9.1% incorporating Nazi-related imagery.   Overall, we document how users are weaponizing generative AI to create extremist content, political commentary, and memes that often bypass conventional content moderation systems. This research highlights significant implications for platform governance, AI safety mechanisms, and broader societal impacts as generative AI technologies become increasingly accessible. The findings underscore the urgent need for enhanced safeguards in generative AI systems and more effective regulatory frameworks to mitigate potential harms while preserving innovation."
  },
  {
    "title": "Considering the multi-time scale rolling optimization scheduling method of micro-energy network connected to electric vehicles",
    "url": "http://arxiv.org/abs/2506.14112v1",
    "arxiv_id": "2506.14112v1",
    "authors": [
      "Hengyu Liu",
      "Yanhong Luo",
      "Congcong Wu",
      "Yin Guan",
      "Ahmed Lotfy Elrefai",
      "Andreas Elombo",
      "Si Li",
      "Sahban Wael Saeed Alnaser",
      "Mingyu Yan"
    ],
    "published": "2025-06-17T02:05:14+00:00",
    "summary": "The large-scale access of electric vehicles to the power grid not only provides flexible adjustment resources for the power system, but the temporal uncertainty and distribution complexity of their energy interaction pose significant challenges to the economy and robustness of the micro-energy network. In this paper, we propose a multi-time scale rolling optimization scheduling method for micro-energy networks considering the access of electric vehicles. In order to solve the problem of evaluating the dispatchable potential of electric vehicle clusters, a charging station aggregation model was constructed based on Minkowski summation theory, and the scattered electric vehicle resources were aggregated into virtual energy storage units to participate in system scheduling. Integrate price-based and incentive-based demand response mechanisms to synergistically tap the potential of source-load two-side regulation; On this basis, a two-stage optimal scheduling model of day-ahead and intra-day is constructed. The simulation results show that the proposed method reduces the scale of \"preventive curtailment\" due to more accurate scheduling, avoids the threat of power shortage to the safety of the power grid, and has more advantages in the efficiency of new energy consumption. At the same time, intra-day scheduling significantly reduces economic penalties and operating costs by avoiding output shortages, and improves the economy of the system in an uncertain forecasting environment."
  },
  {
    "title": "A Hierarchical Test Platform for Vision Language Model (VLM)-Integrated Real-World Autonomous Driving",
    "url": "http://arxiv.org/abs/2506.14100v1",
    "arxiv_id": "2506.14100v1",
    "authors": [
      "Yupeng Zhou",
      "Can Cui",
      "Juntong Peng",
      "Zichong Yang",
      "Juanwu Lu",
      "Jitesh H Panchal",
      "Bin Yao",
      "Ziran Wang"
    ],
    "published": "2025-06-17T01:32:28+00:00",
    "summary": "Vision-Language Models (VLMs) have demonstrated notable promise in autonomous driving by offering the potential for multimodal reasoning through pretraining on extensive image-text pairs. However, adapting these models from broad web-scale data to the safety-critical context of driving presents a significant challenge, commonly referred to as domain shift. Existing simulation-based and dataset-driven evaluation methods, although valuable, often fail to capture the full complexity of real-world scenarios and cannot easily accommodate repeatable closed-loop testing with flexible scenario manipulation. In this paper, we introduce a hierarchical real-world test platform specifically designed to evaluate VLM-integrated autonomous driving systems. Our approach includes a modular, low-latency on-vehicle middleware that allows seamless incorporation of various VLMs, a clearly separated perception-planning-control architecture that can accommodate both VLM-based and conventional modules, and a configurable suite of real-world testing scenarios on a closed track that facilitates controlled yet authentic evaluations. We demonstrate the effectiveness of the proposed platform`s testing and evaluation ability with a case study involving a VLM-enabled autonomous vehicle, highlighting how our test framework supports robust experimentation under diverse conditions."
  },
  {
    "title": "Image Segmentation with Large Language Models: A Survey with Perspectives for Intelligent Transportation Systems",
    "url": "http://arxiv.org/abs/2506.14096v1",
    "arxiv_id": "2506.14096v1",
    "authors": [
      "Sanjeda Akter",
      "Ibne Farabi Shihab",
      "Anuj Sharma"
    ],
    "published": "2025-06-17T01:20:50+00:00",
    "summary": "The integration of Large Language Models (LLMs) with computer vision is profoundly transforming perception tasks like image segmentation. For intelligent transportation systems (ITS), where accurate scene understanding is critical for safety and efficiency, this new paradigm offers unprecedented capabilities. This survey systematically reviews the emerging field of LLM-augmented image segmentation, focusing on its applications, challenges, and future directions within ITS. We provide a taxonomy of current approaches based on their prompting mechanisms and core architectures, and we highlight how these innovations can enhance road scene understanding for autonomous driving, traffic monitoring, and infrastructure maintenance. Finally, we identify key challenges, including real-time performance and safety-critical reliability, and outline a perspective centered on explainable, human-centric AI as a prerequisite for the successful deployment of this technology in next-generation transportation systems."
  },
  {
    "title": "InsertRank: LLMs can reason over BM25 scores to Improve Listwise Reranking",
    "url": "http://arxiv.org/abs/2506.14086v1",
    "arxiv_id": "2506.14086v1",
    "authors": [
      "Rahul Seetharaman",
      "Kaustubh D. Dhole",
      "Aman Bansal"
    ],
    "published": "2025-06-17T01:04:45+00:00",
    "summary": "Large Language Models (LLMs) have demonstrated significant strides across various information retrieval tasks, particularly as rerankers, owing to their strong generalization and knowledge-transfer capabilities acquired from extensive pretraining. In parallel, the rise of LLM-based chat interfaces has raised user expectations, encouraging users to pose more complex queries that necessitate retrieval by ``reasoning'' over documents rather than through simple keyword matching or semantic similarity. While some recent efforts have exploited reasoning abilities of LLMs for reranking such queries, considerable potential for improvement remains. In that regards, we introduce InsertRank, an LLM-based reranker that leverages lexical signals like BM25 scores during reranking to further improve retrieval performance. InsertRank demonstrates improved retrieval effectiveness on -- BRIGHT, a reasoning benchmark spanning 12 diverse domains, and R2MED, a specialized medical reasoning retrieval benchmark spanning 8 different tasks. We conduct an exhaustive evaluation and several ablation studies and demonstrate that InsertRank consistently improves retrieval effectiveness across multiple families of LLMs, including GPT, Gemini, and Deepseek models. %In addition, we also conduct ablation studies on normalization by varying the scale of the BM25 scores, and positional bias by shuffling the order of the documents. With Deepseek-R1, InsertRank achieves a score of 37.5 on the BRIGHT benchmark. and 51.1 on the R2MED benchmark, surpassing previous methods."
  },
  {
    "title": "A Point Cloud Completion Approach for the Grasping of Partially Occluded Objects and Its Applications in Robotic Strawberry Harvesting",
    "url": "http://arxiv.org/abs/2506.14066v1",
    "arxiv_id": "2506.14066v1",
    "authors": [
      "Ali Abouzeid",
      "Malak Mansour",
      "Chengsong Hu",
      "Dezhen Song"
    ],
    "published": "2025-06-16T23:49:15+00:00",
    "summary": "In robotic fruit picking applications, managing object occlusion in unstructured settings poses a substantial challenge for designing grasping algorithms. Using strawberry harvesting as a case study, we present an end-to-end framework for effective object detection, segmentation, and grasp planning to tackle this issue caused by partially occluded objects. Our strategy begins with point cloud denoising and segmentation to accurately locate fruits. To compensate for incomplete scans due to occlusion, we apply a point cloud completion model to create a dense 3D reconstruction of the strawberries. The target selection focuses on ripe strawberries while categorizing others as obstacles, followed by converting the refined point cloud into an occupancy map for collision-aware motion planning. Our experimental results demonstrate high shape reconstruction accuracy, with the lowest Chamfer Distance compared to state-of-the-art methods with 1.10 mm, and significantly improved grasp success rates of 79.17%, yielding an overall success-to-attempt ratio of 89.58\\% in real-world strawberry harvesting. Additionally, our method reduces the obstacle hit rate from 43.33% to 13.95%, highlighting its effectiveness in improving both grasp quality and safety compared to prior approaches. This pipeline substantially improves autonomous strawberry harvesting, advancing more efficient and reliable robotic fruit picking systems."
  },
  {
    "title": "\"I Cannot Write This Because It Violates Our Content Policy\": Understanding Content Moderation Policies and User Experiences in Generative AI Products",
    "url": "http://arxiv.org/abs/2506.14018v1",
    "arxiv_id": "2506.14018v1",
    "authors": [
      "Lan Gao",
      "Oscar Chen",
      "Rachel Lee",
      "Nick Feamster",
      "Chenhao Tan",
      "Marshini Chetty"
    ],
    "published": "2025-06-16T21:30:42+00:00",
    "summary": "While recent research has focused on developing safeguards for generative AI (GAI) model-level content safety, little is known about how content moderation to prevent malicious content performs for end-users in real-world GAI products. To bridge this gap, we investigated content moderation policies and their enforcement in GAI online tools -- consumer-facing web-based GAI applications. We first analyzed content moderation policies of 14 GAI online tools. While these policies are comprehensive in outlining moderation practices, they usually lack details on practical implementations and are not specific about how users can aid in moderation or appeal moderation decisions. Next, we examined user-experienced content moderation successes and failures through Reddit discussions on GAI online tools. We found that although moderation systems succeeded in blocking malicious generations pervasively, users frequently experienced frustration in failures of both moderation systems and user support after moderation. Based on these findings, we suggest improvements for content moderation policy and user experiences in real-world GAI products."
  },
  {
    "title": "FindMeIfYouCan: Bringing Open Set metrics to $\\textit{near} $, $ \\textit{far} $ and $\\textit{farther}$ Out-of-Distribution Object Detection",
    "url": "http://arxiv.org/abs/2506.14008v1",
    "arxiv_id": "2506.14008v1",
    "authors": [
      "Daniel Montoya",
      "Aymen Bouguerra",
      "Alexandra Gomez-Villa",
      "Fabio Arnez"
    ],
    "published": "2025-06-16T21:11:56+00:00",
    "summary": "State-of-the-art Object Detection (OD) methods predominantly operate under a closed-world assumption, where test-time categories match those encountered during training. However, detecting and localizing unknown objects is crucial for safety-critical applications in domains such as autonomous driving and medical imaging. Recently, Out-Of-Distribution (OOD) detection has emerged as a vital research direction for OD, focusing on identifying incorrect predictions typically associated with unknown objects. This paper shows that the current evaluation protocol for OOD-OD violates the assumption of non-overlapping objects with respect to the In-Distribution (ID) datasets, and obscures crucial situations such as ignoring unknown objects, potentially leading to overconfidence in deployment scenarios where truly novel objects might be encountered. To address these limitations, we manually curate, and enrich the existing benchmark by exploiting semantic similarity to create new evaluation splits categorized as $\\textit{near}$, $\\textit{far}$, and $\\textit{farther}$ from ID distributions. Additionally, we incorporate established metrics from the Open Set community, providing deeper insights into how effectively methods detect unknowns, when they ignore them, and when they mistakenly classify OOD objects as ID. Our comprehensive evaluation demonstrates that semantically and visually close OOD objects are easier to localize than far ones, but are also more easily confounded with ID objects. $\\textit{Far}$ and $\\textit{farther}$ objects are harder to localize but less prone to be taken for an ID object."
  },
  {
    "title": "Quantum-Informed Contrastive Learning with Dynamic Mixup Augmentation for Class-Imbalanced Expert Systems",
    "url": "http://arxiv.org/abs/2506.13987v1",
    "arxiv_id": "2506.13987v1",
    "authors": [
      "Md Abrar Jahin",
      "Adiba Abid",
      "M. F. Mridha"
    ],
    "published": "2025-06-16T20:44:30+00:00",
    "summary": "Expert systems often operate in domains characterized by class-imbalanced tabular data, where detecting rare but critical instances is essential for safety and reliability. While conventional approaches, such as cost-sensitive learning, oversampling, and graph neural networks, provide partial solutions, they suffer from drawbacks like overfitting, label noise, and poor generalization in low-density regions. To address these challenges, we propose QCL-MixNet, a novel Quantum-Informed Contrastive Learning framework augmented with k-nearest neighbor (kNN) guided dynamic mixup for robust classification under imbalance. QCL-MixNet integrates three core innovations: (i) a Quantum Entanglement-inspired layer that models complex feature interactions through sinusoidal transformations and gated attention, (ii) a sample-aware mixup strategy that adaptively interpolates feature representations of semantically similar instances to enhance minority class representation, and (iii) a hybrid loss function that unifies focal reweighting, supervised contrastive learning, triplet margin loss, and variance regularization to improve both intra-class compactness and inter-class separability. Extensive experiments on 18 real-world imbalanced datasets (binary and multi-class) demonstrate that QCL-MixNet consistently outperforms 20 state-of-the-art machine learning, deep learning, and GNN-based baselines in macro-F1 and recall, often by substantial margins. Ablation studies further validate the critical role of each architectural component. Our results establish QCL-MixNet as a new benchmark for tabular imbalance handling in expert systems. Theoretical analyses reinforce its expressiveness, generalization, and optimization robustness."
  },
  {
    "title": "Socially-aware Object Transportation by a Mobile Manipulator in Static Planar Environments with Obstacles",
    "url": "http://arxiv.org/abs/2506.13953v1",
    "arxiv_id": "2506.13953v1",
    "authors": [
      "Caio C. G. Ribeiro",
      "Leonardo R. D. Paes",
      "Douglas G. Macharet"
    ],
    "published": "2025-06-16T19:45:30+00:00",
    "summary": "Socially-aware robotic navigation is essential in environments where humans and robots coexist, ensuring both safety and comfort. However, most existing approaches have been primarily developed for mobile robots, leaving a significant gap in research that addresses the unique challenges posed by mobile manipulators. In this paper, we tackle the challenge of navigating a robotic mobile manipulator, carrying a non-negligible load, within a static human-populated environment while adhering to social norms. Our goal is to develop a method that enables the robot to simultaneously manipulate an object and navigate between locations in a socially-aware manner. We propose an approach based on the Risk-RRT* framework that enables the coordinated actuation of both the mobile base and manipulator. This approach ensures collision-free navigation while adhering to human social preferences. We compared our approach in a simulated environment to socially-aware mobile-only methods applied to a mobile manipulator. The results highlight the necessity for mobile manipulator-specific techniques, with our method outperforming mobile-only approaches. Our method enabled the robot to navigate, transport an object, avoid collisions, and minimize social discomfort effectively."
  },
  {
    "title": "Alignment Quality Index (AQI) : Beyond Refusals: AQI as an Intrinsic Alignment Diagnostic via Latent Geometry, Cluster Divergence, and Layer wise Pooled Representations",
    "url": "http://arxiv.org/abs/2506.13901v1",
    "arxiv_id": "2506.13901v1",
    "authors": [
      "Abhilekh Borah",
      "Chhavi Sharma",
      "Danush Khanna",
      "Utkarsh Bhatt",
      "Gurpreet Singh",
      "Hasnat Md Abdullah",
      "Raghav Kaushik Ravi",
      "Vinija Jain",
      "Jyoti Patel",
      "Shubham Singh",
      "Vasu Sharma",
      "Arpita Vats",
      "Rahul Raja",
      "Aman Chadha",
      "Amitava Das"
    ],
    "published": "2025-06-16T18:22:28+00:00",
    "summary": "Alignment is no longer a luxury, it is a necessity. As large language models (LLMs) enter high-stakes domains like education, healthcare, governance, and law, their behavior must reliably reflect human-aligned values and safety constraints. Yet current evaluations rely heavily on behavioral proxies such as refusal rates, G-Eval scores, and toxicity classifiers, all of which have critical blind spots. Aligned models are often vulnerable to jailbreaking, stochasticity of generation, and alignment faking.   To address this issue, we introduce the Alignment Quality Index (AQI). This novel geometric and prompt-invariant metric empirically assesses LLM alignment by analyzing the separation of safe and unsafe activations in latent space. By combining measures such as the Davies-Bouldin Score (DBS), Dunn Index (DI), Xie-Beni Index (XBI), and Calinski-Harabasz Index (CHI) across various formulations, AQI captures clustering quality to detect hidden misalignments and jailbreak risks, even when outputs appear compliant. AQI also serves as an early warning signal for alignment faking, offering a robust, decoding invariant tool for behavior agnostic safety auditing.   Additionally, we propose the LITMUS dataset to facilitate robust evaluation under these challenging conditions. Empirical tests on LITMUS across different models trained under DPO, GRPO, and RLHF conditions demonstrate AQI's correlation with external judges and ability to reveal vulnerabilities missed by refusal metrics. We make our implementation publicly available to foster future research in this area."
  },
  {
    "title": "ExtendAttack: Attacking Servers of LRMs via Extending Reasoning",
    "url": "http://arxiv.org/abs/2506.13737v1",
    "arxiv_id": "2506.13737v1",
    "authors": [
      "Zhenhao Zhu",
      "Yue Liu",
      "Yingwei Ma",
      "Hongcheng Gao",
      "Nuo Chen",
      "Yanpei Guo",
      "Wenjie Qu",
      "Huiying Xu",
      "Xinzhong Zhu",
      "Jiaheng Zhang"
    ],
    "published": "2025-06-16T17:49:05+00:00",
    "summary": "Large Reasoning Models (LRMs) have demonstrated promising performance in complex tasks. However, the resource-consuming reasoning processes may be exploited by attackers to maliciously occupy the resources of the servers, leading to a crash, like the DDoS attack in cyber. To this end, we propose a novel attack method on LRMs termed ExtendAttack to maliciously occupy the resources of servers by stealthily extending the reasoning processes of LRMs. Concretely, we systematically obfuscate characters within a benign prompt, transforming them into a complex, poly-base ASCII representation. This compels the model to perform a series of computationally intensive decoding sub-tasks that are deeply embedded within the semantic structure of the query itself. Extensive experiments demonstrate the effectiveness of our proposed ExtendAttack. Remarkably, it increases the length of the model's response by over 2.5 times for the o3 model on the HumanEval benchmark. Besides, it preserves the original meaning of the query and achieves comparable answer accuracy, showing the stealthiness."
  },
  {
    "title": "Attribution-guided Pruning for Compression, Circuit Discovery, and Targeted Correction in LLMs",
    "url": "http://arxiv.org/abs/2506.13727v1",
    "arxiv_id": "2506.13727v1",
    "authors": [
      "Sayed Mohammad Vakilzadeh Hatefi",
      "Maximilian Dreyer",
      "Reduan Achtibat",
      "Patrick Kahardipraja",
      "Thomas Wiegand",
      "Wojciech Samek",
      "Sebastian Lapuschkin"
    ],
    "published": "2025-06-16T17:38:36+00:00",
    "summary": "Large Language Models (LLMs) are central to many contemporary AI applications, yet their extensive parameter counts pose significant challenges for deployment in memory- and compute-constrained environments. Recent works in eXplainable AI (XAI), particularly on attribution methods, suggest that interpretability can also enable model compression by identifying and removing components irrelevant to inference. In this paper, we leverage Layer-wise Relevance Propagation (LRP) to perform attribution-guided pruning of LLMs. While LRP has shown promise in structured pruning for vision models, we extend it to unstructured pruning in LLMs and demonstrate that it can substantially reduce model size with minimal performance loss. Our method is especially effective in extracting task-relevant subgraphs -- so-called ``circuits'' -- which can represent core functions (e.g., indirect object identification). Building on this, we introduce a technique for model correction, by selectively removing circuits responsible for spurious behaviors (e.g., toxic outputs). All in all, we gather these techniques as a uniform holistic framework and showcase its effectiveness and limitations through extensive experiments for compression, circuit discovery and model correction on Llama and OPT models, highlighting its potential for improving both model efficiency and safety. Our code is publicly available at https://github.com/erfanhatefi/SparC3."
  },
  {
    "title": "Weakest Link in the Chain: Security Vulnerabilities in Advanced Reasoning Models",
    "url": "http://arxiv.org/abs/2506.13726v1",
    "arxiv_id": "2506.13726v1",
    "authors": [
      "Arjun Krishna",
      "Aaditya Rastogi",
      "Erick Galinkin"
    ],
    "published": "2025-06-16T17:32:18+00:00",
    "summary": "The introduction of advanced reasoning capabilities have improved the problem-solving performance of large language models, particularly on math and coding benchmarks. However, it remains unclear whether these reasoning models are more or less vulnerable to adversarial prompt attacks than their non-reasoning counterparts. In this work, we present a systematic evaluation of weaknesses in advanced reasoning models compared to similar non-reasoning models across a diverse set of prompt-based attack categories. Using experimental data, we find that on average the reasoning-augmented models are \\emph{slightly more robust} than non-reasoning models (42.51\\% vs 45.53\\% attack success rate, lower is better). However, this overall trend masks significant category-specific differences: for certain attack types the reasoning models are substantially \\emph{more vulnerable} (e.g., up to 32 percentage points worse on a tree-of-attacks prompt), while for others they are markedly \\emph{more robust} (e.g., 29.8 points better on cross-site scripting injection). Our findings highlight the nuanced security implications of advanced reasoning in language models and emphasize the importance of stress-testing safety across diverse adversarial techniques."
  },
  {
    "title": "We Should Identify and Mitigate Third-Party Safety Risks in MCP-Powered Agent Systems",
    "url": "http://arxiv.org/abs/2506.13666v1",
    "arxiv_id": "2506.13666v1",
    "authors": [
      "Junfeng Fang",
      "Zijun Yao",
      "Ruipeng Wang",
      "Haokai Ma",
      "Xiang Wang",
      "Tat-Seng Chua"
    ],
    "published": "2025-06-16T16:24:31+00:00",
    "summary": "The development of large language models (LLMs) has entered in a experience-driven era, flagged by the emergence of environment feedback-driven learning via reinforcement learning and tool-using agents. This encourages the emergenece of model context protocol (MCP), which defines the standard on how should a LLM interact with external services, such as \\api and data. However, as MCP becomes the de facto standard for LLM agent systems, it also introduces new safety risks. In particular, MCP introduces third-party services, which are not controlled by the LLM developers, into the agent systems. These third-party MCP services provider are potentially malicious and have the economic incentives to exploit vulnerabilities and sabotage user-agent interactions. In this position paper, we advocate the research community in LLM safety to pay close attention to the new safety risks issues introduced by MCP, and develop new techniques to build safe MCP-powered agent systems. To establish our position, we argue with three key parts. (1) We first construct \\framework, a controlled framework to examine safety issues in MCP-powered agent systems. (2) We then conduct a series of pilot experiments to demonstrate the safety risks in MCP-powered agent systems is a real threat and its defense is not trivial. (3) Finally, we give our outlook by showing a roadmap to build safe MCP-powered agent systems. In particular, we would call for researchers to persue the following research directions: red teaming, MCP safe LLM development, MCP safety evaluation, MCP safety data accumulation, MCP service safeguard, and MCP safe ecosystem construction. We hope this position paper can raise the awareness of the research community in MCP safety and encourage more researchers to join this important research direction. Our code is available at https://github.com/littlelittlenine/SafeMCP.git."
  },
  {
    "title": "LocationReasoner: Evaluating LLMs on Real-World Site Selection Reasoning",
    "url": "http://arxiv.org/abs/2506.13841v1",
    "arxiv_id": "2506.13841v1",
    "authors": [
      "Miho Koda",
      "Yu Zheng",
      "Ruixian Ma",
      "Mingyang Sun",
      "Devesh Pansare",
      "Fabio Duarte",
      "Paolo Santi"
    ],
    "published": "2025-06-16T16:23:56+00:00",
    "summary": "Recent advances in large language models (LLMs), particularly those enhanced through reinforced post-training, have demonstrated impressive reasoning capabilities, as exemplified by models such as OpenAI o1 and DeepSeek-R1. However, these capabilities are predominantly benchmarked on domains like mathematical problem solving and code generation -- leaving open the question of whether such reasoning skills generalize to complex, real-world scenarios. In this paper, we introduce LocationReasoner, a benchmark designed to evaluate LLMs' reasoning abilities in the context of real-world site selection, where models must identify feasible locations by reasoning over diverse and complicated spatial, environmental, and logistical constraints. The benchmark comprises over 300 carefully crafted queries of varying difficulty levels, supported by a sandbox environment with in-house tools for constraint-based location search. Extensive evaluations reveal that state-of-the-art reasoning models offer limited improvement over their non-reasoning predecessors in real-world contexts, with even the latest OpenAI o4 model failing on 30% of site selection tasks. Moreover, agentic strategies such as ReAct and Reflexion often suffer from over-reasoning, leading to worse outcomes than direct code-generation prompting. With key limitations of LLMs in holistic and non-linear reasoning highlighted, we release LocationReasoner to foster the development of LLMs and agents capable of robust, grounded reasoning in real-world decision-making tasks. Codes and data for our benchmark are available at https://github.com/miho-koda/LocationReasoner."
  },
  {
    "title": "Quantifying stored energy release in irradiated YBa$_2$Cu$_3$O$_7$ through molecular dynamics annealing simulations",
    "url": "http://arxiv.org/abs/2506.13625v1",
    "arxiv_id": "2506.13625v1",
    "authors": [
      "Lauryn Kortman",
      "Alexis Devitre",
      "Charles Hirst"
    ],
    "published": "2025-06-16T15:54:02+00:00",
    "summary": "Over the lifetime of a fusion power plant, irradiation-induced defects will accumulate in the superconducting magnets compromising their ability to carry current without losses, generate high magnetic fields, and thus maintain plasma confinement. These defects also store potential energy within the crystalline lattice of materials, which can be released upon annealing. This phenomenon raises the question of whether the energy stored in defects may be sufficient to accelerate, or even trigger, a magnet quench? To provide an order of magnitude estimate, we used molecular dynamics simulations to generate defected YBCO supercells and conduct isothermal annealing simulations. Our results reveal that the maximum volumetric stored energy in a 4 mDPA defected single crystal of YBCO (240 $J/cm^3$) is 30 times greater than the experimental minimum quench energy values for YBCO tapes (8.1 $J/cm^3$). Our simulations also show that the amount of energy released increases as a function of annealing temperature or irradiation dose. This trend demonstrates that localized heating events in an irradiated fusion magnet have the potential to release significant amounts of defect energy. These findings underscore the critical need for experimental validation of the accumulation and release of defect stored energy, and highlight the importance of incorporating this contribution into quench detection systems, to enhance the operational safety of large-scale YBCO fusion magnets."
  },
  {
    "title": "Disturbance-aware minimum-time planning strategies for motorsport vehicles with probabilistic safety certificates",
    "url": "http://arxiv.org/abs/2506.13622v1",
    "arxiv_id": "2506.13622v1",
    "authors": [
      "Martino Gulisano",
      "Matteo Masoni",
      "Marco Gabiccini",
      "Massimo Guiggiani"
    ],
    "published": "2025-06-16T15:50:17+00:00",
    "summary": "This paper presents a disturbance-aware framework that embeds robustness into minimum-lap-time trajectory optimization for motorsport. Two formulations are introduced. (i) Open-loop, horizon-based covariance propagation uses worst-case uncertainty growth over a finite window to tighten tire-friction and track-limit constraints. (ii) Closed-loop, covariance-aware planning incorporates a time-varying LQR feedback law in the optimizer, providing a feedback-consistent estimate of disturbance attenuation and enabling sharper yet reliable constraint tightening. Both methods yield reference trajectories for human or artificial drivers: in autonomous applications the modelled controller can replicate the on-board implementation, while for human driving accuracy increases with the extent to which the driver can be approximated by the assumed time-varying LQR policy. Computational tests on a representative Barcelona-Catalunya sector show that both schemes meet the prescribed safety probability, yet the closed-loop variant incurs smaller lap-time penalties than the more conservative open-loop solution, while the nominal (non-robust) trajectory remains infeasible under the same uncertainties. By accounting for uncertainty growth and feedback action during planning, the proposed framework delivers trajectories that are both performance-optimal and probabilistically safe, advancing minimum-time optimization toward real-world deployment in high-performance motorsport and autonomous racing."
  },
  {
    "title": "Calibrated Predictive Lower Bounds on Time-to-Unsafe-Sampling in LLMs",
    "url": "http://arxiv.org/abs/2506.13593v1",
    "arxiv_id": "2506.13593v1",
    "authors": [
      "Hen Davidov",
      "Gilad Freidkin",
      "Shai Feldman",
      "Yaniv Romano"
    ],
    "published": "2025-06-16T15:21:25+00:00",
    "summary": "We develop a framework to quantify the time-to-unsafe-sampling - the number of large language model (LLM) generations required to trigger an unsafe (e.g., toxic) response. Estimating this quantity is challenging, since unsafe responses are exceedingly rare in well-aligned LLMs, potentially occurring only once in thousands of generations. As a result, directly estimating time-to-unsafe-sampling would require collecting training data with a prohibitively large number of generations per prompt. However, with realistic sampling budgets, we often cannot generate enough responses to observe an unsafe outcome for every prompt, leaving the time-to-unsafe-sampling unobserved in many cases, making the estimation and evaluation tasks particularly challenging. To address this, we frame this estimation problem as one of survival analysis and develop a provably calibrated lower predictive bound (LPB) on the time-to-unsafe-sampling of a given prompt, leveraging recent advances in conformal prediction. Our key innovation is designing an adaptive, per-prompt sampling strategy, formulated as a convex optimization problem. The objective function guiding this optimized sampling allocation is designed to reduce the variance of the estimators used to construct the LPB, leading to improved statistical efficiency over naive methods that use a fixed sampling budget per prompt. Experiments on both synthetic and real data support our theoretical results and demonstrate the practical utility of our method for safety risk assessment in generative AI models."
  },
  {
    "title": "MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention",
    "url": "http://arxiv.org/abs/2506.13585v1",
    "arxiv_id": "2506.13585v1",
    "authors": [
      "MiniMax",
      ":",
      "Aili Chen",
      "Aonian Li",
      "Bangwei Gong",
      "Binyang Jiang",
      "Bo Fei",
      "Bo Yang",
      "Boji Shan",
      "Changqing Yu",
      "Chao Wang",
      "Cheng Zhu",
      "Chengjun Xiao",
      "Chengyu Du",
      "Chi Zhang",
      "Chu Qiao",
      "Chunhao Zhang",
      "Chunhui Du",
      "Congchao Guo",
      "Da Chen",
      "Deming Ding",
      "Dianjun Sun",
      "Dong Li",
      "Enwei Jiao",
      "Haigang Zhou",
      "Haimo Zhang",
      "Han Ding",
      "Haohai Sun",
      "Haoyu Feng",
      "Huaiguang Cai",
      "Haichao Zhu",
      "Jian Sun",
      "Jiaqi Zhuang",
      "Jiaren Cai",
      "Jiayuan Song",
      "Jin Zhu",
      "Jingyang Li",
      "Jinhao Tian",
      "Jinli Liu",
      "Junhao Xu",
      "Junjie Yan",
      "Junteng Liu",
      "Junxian He",
      "Kaiyi Feng",
      "Ke Yang",
      "Kecheng Xiao",
      "Le Han",
      "Leyang Wang",
      "Lianfei Yu",
      "Liheng Feng",
      "Lin Li",
      "Lin Zheng",
      "Linge Du",
      "Lingyu Yang",
      "Lunbin Zeng",
      "Minghui Yu",
      "Mingliang Tao",
      "Mingyuan Chi",
      "Mozhi Zhang",
      "Mujie Lin",
      "Nan Hu",
      "Nongyu Di",
      "Peng Gao",
      "Pengfei Li",
      "Pengyu Zhao",
      "Qibing Ren",
      "Qidi Xu",
      "Qile Li",
      "Qin Wang",
      "Rong Tian",
      "Ruitao Leng",
      "Shaoxiang Chen",
      "Shaoyu Chen",
      "Shengmin Shi",
      "Shitong Weng",
      "Shuchang Guan",
      "Shuqi Yu",
      "Sichen Li",
      "Songquan Zhu",
      "Tengfei Li",
      "Tianchi Cai",
      "Tianrun Liang",
      "Weiyu Cheng",
      "Weize Kong",
      "Wenkai Li",
      "Xiancai Chen",
      "Xiangjun Song",
      "Xiao Luo",
      "Xiao Su",
      "Xiaobo Li",
      "Xiaodong Han",
      "Xinzhu Hou",
      "Xuan Lu",
      "Xun Zou",
      "Xuyang Shen",
      "Yan Gong",
      "Yan Ma",
      "Yang Wang",
      "Yiqi Shi",
      "Yiran Zhong",
      "Yonghong Duan",
      "Yongxiang Fu",
      "Yongyi Hu",
      "Yu Gao",
      "Yuanxiang Fan",
      "Yufeng Yang",
      "Yuhao Li",
      "Yulin Hu",
      "Yunan Huang",
      "Yunji Li",
      "Yunzhi Xu",
      "Yuxin Mao",
      "Yuxuan Shi",
      "Yuze Wenren",
      "Zehan Li",
      "Zelin Li",
      "Zhanxu Tian",
      "Zhengmao Zhu",
      "Zhenhua Fan",
      "Zhenzhen Wu",
      "Zhichao Xu",
      "Zhihang Yu",
      "Zhiheng Lyu",
      "Zhuo Jiang",
      "Zibo Gao",
      "Zijia Wu",
      "Zijian Song",
      "Zijun Sun"
    ],
    "published": "2025-06-16T15:08:02+00:00",
    "summary": "We introduce MiniMax-M1, the world's first open-weight, large-scale hybrid-attention reasoning model. MiniMax-M1 is powered by a hybrid Mixture-of-Experts (MoE) architecture combined with a lightning attention mechanism. The model is developed based on our previous MiniMax-Text-01 model, which contains a total of 456 billion parameters with 45.9 billion parameters activated per token. The M1 model natively supports a context length of 1 million tokens, 8x the context size of DeepSeek R1. Furthermore, the lightning attention mechanism in MiniMax-M1 enables efficient scaling of test-time compute. These properties make M1 particularly suitable for complex tasks that require processing long inputs and thinking extensively. MiniMax-M1 is trained using large-scale reinforcement learning (RL) on diverse problems including sandbox-based, real-world software engineering environments. In addition to M1's inherent efficiency advantage for RL training, we propose CISPO, a novel RL algorithm to further enhance RL efficiency. CISPO clips importance sampling weights rather than token updates, outperforming other competitive RL variants. Combining hybrid-attention and CISPO enables MiniMax-M1's full RL training on 512 H800 GPUs to complete in only three weeks, with a rental cost of just $534,700. We release two versions of MiniMax-M1 models with 40K and 80K thinking budgets respectively, where the 40K model represents an intermediate phase of the 80K training. Experiments on standard benchmarks show that our models are comparable or superior to strong open-weight models such as the original DeepSeek-R1 and Qwen3-235B, with particular strengths in complex software engineering, tool utilization, and long-context tasks. We publicly release MiniMax-M1 at https://github.com/MiniMax-AI/MiniMax-M1."
  },
  {
    "title": "BattBee: Equivalent Circuit Modeling and Early Detection of Thermal Runaway Triggered by Internal Short Circuits for Lithium-Ion Batteries",
    "url": "http://arxiv.org/abs/2506.13577v1",
    "arxiv_id": "2506.13577v1",
    "authors": [
      "Sangwon Kang",
      "Hao Tu",
      "Huazhen Fang"
    ],
    "published": "2025-06-16T14:58:45+00:00",
    "summary": "Lithium-ion batteries are the enabling power source for transportation electrification. However, in real-world applications, they remain vulnerable to internal short circuits (ISCs) and the consequential risk of thermal runaway (TR). Toward addressing the challenge of ISCs and TR, we undertake a systematic study that extends from dynamic modeling to fault detection in this paper. First, we develop {\\em BattBee}, the first equivalent circuit model to specifically describe the onset of ISCs and the evolution of subsequently induced TR. Drawing upon electrochemical modeling, the model can simulate ISCs at different severity levels and predict their impact on the initiation and progression of TR events. With the physics-inspired design, this model offers strong physical interpretability and predictive accuracy, while maintaining structural simplicity to allow fast computation. Then, building upon the BattBee model, we develop fault detection observers and derive detection criteria together with decision-making logics to identify the occurrence and emergence of ISC and TR events. This detection approach is principled in design and fast in computation, lending itself to practical applications. Validation based on simulations and experimental data demonstrates the effectiveness of both the BattBee model and the ISC/TR detection approach. The research outcomes underscore this study's potential for real-world battery safety risk management."
  },
  {
    "title": "Safe-Child-LLM: A Developmental Benchmark for Evaluating LLM Safety in Child-AI Interactions",
    "url": "http://arxiv.org/abs/2506.13510v1",
    "arxiv_id": "2506.13510v1",
    "authors": [
      "Junfeng Jiao",
      "Saleh Afroogh",
      "Kevin Chen",
      "Abhejay Murali",
      "David Atkinson",
      "Amit Dhurandhar"
    ],
    "published": "2025-06-16T14:04:54+00:00",
    "summary": "As Large Language Models (LLMs) increasingly power applications used by children and adolescents, ensuring safe and age-appropriate interactions has become an urgent ethical imperative. Despite progress in AI safety, current evaluations predominantly focus on adults, neglecting the unique vulnerabilities of minors engaging with generative AI. We introduce Safe-Child-LLM, a comprehensive benchmark and dataset for systematically assessing LLM safety across two developmental stages: children (7-12) and adolescents (13-17). Our framework includes a novel multi-part dataset of 200 adversarial prompts, curated from red-teaming corpora (e.g., SG-Bench, HarmBench), with human-annotated labels for jailbreak success and a standardized 0-5 ethical refusal scale. Evaluating leading LLMs -- including ChatGPT, Claude, Gemini, LLaMA, DeepSeek, Grok, Vicuna, and Mistral -- we uncover critical safety deficiencies in child-facing scenarios. This work highlights the need for community-driven benchmarks to protect young users in LLM interactions. To promote transparency and collaborative advancement in ethical AI development, we are publicly releasing both our benchmark datasets and evaluation codebase at https://github.com/The-Responsible-AI-Initiative/Safe_Child_LLM_Benchmark.git"
  },
  {
    "title": "Safe-Child-LLM: A Developmental Benchmark for Evaluating LLM Safety in Child-LLM Interactions",
    "url": "http://arxiv.org/abs/2506.13510v2",
    "arxiv_id": "2506.13510v2",
    "authors": [
      "Junfeng Jiao",
      "Saleh Afroogh",
      "Kevin Chen",
      "Abhejay Murali",
      "David Atkinson",
      "Amit Dhurandhar"
    ],
    "published": "2025-06-16T14:04:54+00:00",
    "summary": "As Large Language Models (LLMs) increasingly power applications used by children and adolescents, ensuring safe and age-appropriate interactions has become an urgent ethical imperative. Despite progress in AI safety, current evaluations predominantly focus on adults, neglecting the unique vulnerabilities of minors engaging with generative AI. We introduce Safe-Child-LLM, a comprehensive benchmark and dataset for systematically assessing LLM safety across two developmental stages: children (7-12) and adolescents (13-17). Our framework includes a novel multi-part dataset of 200 adversarial prompts, curated from red-teaming corpora (e.g., SG-Bench, HarmBench), with human-annotated labels for jailbreak success and a standardized 0-5 ethical refusal scale. Evaluating leading LLMs -- including ChatGPT, Claude, Gemini, LLaMA, DeepSeek, Grok, Vicuna, and Mistral -- we uncover critical safety deficiencies in child-facing scenarios. This work highlights the need for community-driven benchmarks to protect young users in LLM interactions. To promote transparency and collaborative advancement in ethical AI development, we are publicly releasing both our benchmark datasets and evaluation codebase at https://github.com/The-Responsible-AI-Initiative/Safe_Child_LLM_Benchmark.git"
  },
  {
    "title": "UAV Object Detection and Positioning in a Mining Industrial Metaverse with Custom Geo-Referenced Data",
    "url": "http://arxiv.org/abs/2506.13505v1",
    "arxiv_id": "2506.13505v1",
    "authors": [
      "Vasiliki Balaska",
      "Ioannis Tsampikos Papapetros",
      "Katerina Maria Oikonomou",
      "Loukas Bampis",
      "Antonios Gasteratos"
    ],
    "published": "2025-06-16T13:59:56+00:00",
    "summary": "The mining sector increasingly adopts digital tools to improve operational efficiency, safety, and data-driven decision-making. One of the key challenges remains the reliable acquisition of high-resolution, geo-referenced spatial information to support core activities such as extraction planning and on-site monitoring. This work presents an integrated system architecture that combines UAV-based sensing, LiDAR terrain modeling, and deep learning-based object detection to generate spatially accurate information for open-pit mining environments. The proposed pipeline includes geo-referencing, 3D reconstruction, and object localization, enabling structured spatial outputs to be integrated into an industrial digital twin platform. Unlike traditional static surveying methods, the system offers higher coverage and automation potential, with modular components suitable for deployment in real-world industrial contexts. While the current implementation operates in post-flight batch mode, it lays the foundation for real-time extensions. The system contributes to the development of AI-enhanced remote sensing in mining by demonstrating a scalable and field-validated geospatial data workflow that supports situational awareness and infrastructure safety."
  },
  {
    "title": "Navigating through CS1: The Role of Self-Regulation and Supervision in Student Progress",
    "url": "http://arxiv.org/abs/2506.13461v1",
    "arxiv_id": "2506.13461v1",
    "authors": [
      "Ville Isom\u00f6tt\u00f6nen",
      "Denis Zhidkikh"
    ],
    "published": "2025-06-16T13:17:23+00:00",
    "summary": "The need for students' self-regulation for fluent transitioning to university studies is known. Our aim was to integrate study-supportive activities with course supervision activities within CS1. We educated TAs to pay attention to students' study ability and self-regulation. An interview study ($N=14$) was undertaken to investigate this approach. A thematic analysis yielded rather mixed results in light of our aims. Self-regulation was underpinned by the influences external to our setting, including labor market-related needs, earlier crises in study habits, and personal characteristics such as passion, grit, creativity, and valuation of utility. Safety in one-to-one supervision was considered essential, while shyness, fear, and even altruism caused self-handicapping during the course. Students were aware of their learning styles and need for self-regulation, while did not always know how to self-regulate or preferred to externalize it. The results highlight that supporting self-regulation should be integrated with students' personal histories and experiences, and thereby calls attention to transformative learning pedagogies. The thematization can help to understand CS1 students' self-regulation processes and improve CS1 support practices."
  },
  {
    "title": "Leveraging Vision-Language Pre-training for Human Activity Recognition in Still Images",
    "url": "http://arxiv.org/abs/2506.13458v1",
    "arxiv_id": "2506.13458v1",
    "authors": [
      "Cristina Mahanta",
      "Gagan Bhatia"
    ],
    "published": "2025-06-16T13:15:02+00:00",
    "summary": "Recognising human activity in a single photo enables indexing, safety and assistive applications, yet lacks motion cues. Using 285 MSCOCO images labelled as walking, running, sitting, and standing, scratch CNNs scored 41% accuracy. Fine-tuning multimodal CLIP raised this to 76%, demonstrating that contrastive vision-language pre-training decisively improves still-image action recognition in real-world deployments."
  },
  {
    "title": "Flavoured jet algorithms: a comparative study",
    "url": "http://arxiv.org/abs/2506.13449v1",
    "arxiv_id": "2506.13449v1",
    "authors": [
      "Arnd Behring",
      "Simone Caletti",
      "Francesco Giuli",
      "Radoslaw Grabarczyk",
      "Andreas Hinzmann",
      "Alexander Huss",
      "Joey Huston",
      "Ezra D. Lesser",
      "Simone Marzani",
      "Davide Napoletano",
      "Rene Poncelet",
      "Daniel Reichelt",
      "Alberto Rescia",
      "Gavin P. Salam",
      "Ludovic Scyboz",
      "Federico Sforza",
      "Andrzej Siodmok",
      "Giovanni Stagnitto",
      "James Whitehead",
      "Ruide Xu"
    ],
    "published": "2025-06-16T13:05:17+00:00",
    "summary": "The accurate identification of heavy-flavour jets, those which originate from bottom or charm quarks, is crucial for precision studies of the Standard Model and searches for new physics. However, assigning flavour to jets presents significant challenges, primarily due to issues with infrared and collinear (IRC) safety. This paper aims to address these challenges by evaluating recently-proposed jet algorithms designed to be IRC-safe and applicable in high-precision measurements. We compare these algorithms across benchmark heavy-flavour production processes and kinematic regimes that are relevant for LHC phenomenology. Exploiting both fixed-order calculations in QCD as well as parton shower simulations, we analyse the infrared sensitivity of these new algorithms at different stages of the event evolution and compare to flavour-labelling strategies currently adopted by LHC collaborations. The results highlight that, while all algorithms lead to more robust flavour-assignments compared to current techniques, they vary in performance depending on the observable and energy regime. The study lays groundwork for robust, flavour-aware jet analyses in current and future collider experiments to maximise the physics potential of experimental data by reducing discrepancies between theoretical and experimental methods."
  },
  {
    "title": "Sparse Convolutional Recurrent Learning for Efficient Event-based Neuromorphic Object Detection",
    "url": "http://arxiv.org/abs/2506.13440v1",
    "arxiv_id": "2506.13440v1",
    "authors": [
      "Shenqi Wang",
      "Yingfu Xu",
      "Amirreza Yousefzadeh",
      "Sherif Eissa",
      "Henk Corporaal",
      "Federico Corradi",
      "Guangzhi Tang"
    ],
    "published": "2025-06-16T12:54:27+00:00",
    "summary": "Leveraging the high temporal resolution and dynamic range, object detection with event cameras can enhance the performance and safety of automotive and robotics applications in real-world scenarios. However, processing sparse event data requires compute-intensive convolutional recurrent units, complicating their integration into resource-constrained edge applications. Here, we propose the Sparse Event-based Efficient Detector (SEED) for efficient event-based object detection on neuromorphic processors. We introduce sparse convolutional recurrent learning, which achieves over 92% activation sparsity in recurrent processing, vastly reducing the cost for spatiotemporal reasoning on sparse event data. We validated our method on Prophesee's 1 Mpx and Gen1 event-based object detection datasets. Notably, SEED sets a new benchmark in computational efficiency for event-based object detection which requires long-term temporal learning. Compared to state-of-the-art methods, SEED significantly reduces synaptic operations while delivering higher or same-level mAP. Our hardware simulations showcase the critical role of SEED's hardware-aware design in achieving energy-efficient and low-latency neuromorphic processing."
  },
  {
    "title": "A data-driven analysis of the impact of non-compliant individuals on epidemic diffusion in urban settings",
    "url": "http://arxiv.org/abs/2506.13325v1",
    "arxiv_id": "2506.13325v1",
    "authors": [
      "Fabio Mazza",
      "Marco Brambilla",
      "Carlo Piccardi",
      "Francesco Pierri"
    ],
    "published": "2025-06-16T10:15:13+00:00",
    "summary": "Individuals who do not comply with public health safety measures pose a significant challenge to effective epidemic control, as their risky behaviours can undermine public health interventions. This is particularly relevant in urban environments because of their high population density and complex social interactions. In this study, we employ detailed contact networks, built using a data-driven approach, to examine the impact of non-compliant individuals on epidemic dynamics in three major Italian cities: Torino, Milano, and Palermo. We use a heterogeneous extension of the Susceptible-Infected-Recovered model that distinguishes between ordinary and non-compliant individuals, who are more infectious and/or more susceptible. By combining electoral data with recent findings on vaccine hesitancy, we obtain spatially heterogeneous distributions of non-compliance. Epidemic simulations demonstrate that even a small proportion of non-compliant individuals in the population can substantially increase the number of infections and accelerate the timing of their peak. Furthermore, the impact of non-compliance is greatest when disease transmission rates are moderate. Including the heterogeneous, data-driven distribution of non-compliance in the simulations results in infection hotspots forming with varying intensity according to the disease transmission rate. Overall, these findings emphasise the importance of monitoring behavioural compliance and tailoring public health interventions to address localised risks."
  },
  {
    "title": "Mitigating Safety Fallback in Editing-based Backdoor Injection on LLMs",
    "url": "http://arxiv.org/abs/2506.13285v1",
    "arxiv_id": "2506.13285v1",
    "authors": [
      "Houcheng Jiang",
      "Zetong Zhao",
      "Junfeng Fang",
      "Haokai Ma",
      "Ruipeng Wang",
      "Yang Deng",
      "Xiang Wang",
      "Xiangnan He"
    ],
    "published": "2025-06-16T09:28:07+00:00",
    "summary": "Large language models (LLMs) have shown strong performance across natural language tasks, but remain vulnerable to backdoor attacks. Recent model editing-based approaches enable efficient backdoor injection by directly modifying parameters to map specific triggers to attacker-desired responses. However, these methods often suffer from safety fallback, where the model initially responds affirmatively but later reverts to refusals due to safety alignment. In this work, we propose DualEdit, a dual-objective model editing framework that jointly promotes affirmative outputs and suppresses refusal responses. To address two key challenges -- balancing the trade-off between affirmative promotion and refusal suppression, and handling the diversity of refusal expressions -- DualEdit introduces two complementary techniques. (1) Dynamic loss weighting calibrates the objective scale based on the pre-edited model to stabilize optimization. (2) Refusal value anchoring compresses the suppression target space by clustering representative refusal value vectors, reducing optimization conflict from overly diverse token sets. Experiments on safety-aligned LLMs show that DualEdit improves attack success by 9.98\\% and reduces safety fallback rate by 10.88\\% over baselines."
  },
  {
    "title": "Ai-Facilitated Analysis of Abstracts and Conclusions: Flagging Unsubstantiated Claims and Ambiguous Pronouns",
    "url": "http://arxiv.org/abs/2506.13172v1",
    "arxiv_id": "2506.13172v1",
    "authors": [
      "Evgeny Markhasin"
    ],
    "published": "2025-06-16T07:34:31+00:00",
    "summary": "We present and evaluate a suite of proof-of-concept (PoC), structured workflow prompts designed to elicit human-like hierarchical reasoning while guiding Large Language Models (LLMs) in high-level semantic and linguistic analysis of scholarly manuscripts. The prompts target two non-trivial analytical tasks: identifying unsubstantiated claims in summaries (informational integrity) and flagging ambiguous pronoun references (linguistic clarity). We conducted a systematic, multi-run evaluation on two frontier models (Gemini Pro 2.5 Pro and ChatGPT Plus o3) under varied context conditions. Our results for the informational integrity task reveal a significant divergence in model performance: while both models successfully identified an unsubstantiated head of a noun phrase (95% success), ChatGPT consistently failed (0% success) to identify an unsubstantiated adjectival modifier that Gemini correctly flagged (95% success), raising a question regarding potential influence of the target's syntactic role. For the linguistic analysis task, both models performed well (80-90% success) with full manuscript context. In a summary-only setting, however, ChatGPT achieved a perfect (100%) success rate, while Gemini's performance was substantially degraded. Our findings suggest that structured prompting is a viable methodology for complex textual analysis but show that prompt performance may be highly dependent on the interplay between the model, task type, and context, highlighting the need for rigorous, model-specific testing."
  },
  {
    "title": "Crime Hotspot Prediction Using Deep Graph Convolutional Networks",
    "url": "http://arxiv.org/abs/2506.13116v1",
    "arxiv_id": "2506.13116v1",
    "authors": [
      "Tehreem Zubair",
      "Syeda Kisaa Fatima",
      "Noman Ahmed",
      "Asifullah Khan"
    ],
    "published": "2025-06-16T05:47:29+00:00",
    "summary": "Crime hotspot prediction is critical for ensuring urban safety and effective law enforcement, yet it remains challenging due to the complex spatial dependencies inherent in criminal activity. The previous approaches tended to use classical algorithms such as the KDE and SVM to model data distributions and decision boundaries. The methods often fail to capture these spatial relationships, treating crime events as independent and ignoring geographical interactions. To address this, we propose a novel framework based on Graph Convolutional Networks (GCNs), which explicitly model spatial dependencies by representing crime data as a graph. In this graph, nodes represent discrete geographic grid cells and edges capture proximity relationships. Using the Chicago Crime Dataset, we engineer spatial features and train a multi-layer GCN model to classify crime types and predict high-risk zones. Our approach achieves 88% classification accuracy, significantly outperforming traditional methods. Additionally, the model generates interpretable heat maps of crime hotspots, demonstrating the practical utility of graph-based learning for predictive policing and spatial criminology."
  },
  {
    "title": "Rethinking Explainability in the Era of Multimodal AI",
    "url": "http://arxiv.org/abs/2506.13060v1",
    "arxiv_id": "2506.13060v1",
    "authors": [
      "Chirag Agarwal"
    ],
    "published": "2025-06-16T03:08:29+00:00",
    "summary": "While multimodal AI systems (models jointly trained on heterogeneous data types such as text, time series, graphs, and images) have become ubiquitous and achieved remarkable performance across high-stakes applications, transparent and accurate explanation algorithms are crucial for their safe deployment and ensure user trust. However, most existing explainability techniques remain unimodal, generating modality-specific feature attributions, concepts, or circuit traces in isolation and thus failing to capture cross-modal interactions. This paper argues that such unimodal explanations systematically misrepresent and fail to capture the cross-modal influence that drives multimodal model decisions, and the community should stop relying on them for interpreting multimodal models. To support our position, we outline key principles for multimodal explanations grounded in modality: Granger-style modality influence (controlled ablations to quantify how removing one modality changes the explanation for another), Synergistic faithfulness (explanations capture the model's predictive power when modalities are combined), and Unified stability (explanations remain consistent under small, cross-modal perturbations). This targeted shift to multimodal explanations will help the community uncover hidden shortcuts, mitigate modality bias, improve model reliability, and enhance safety in high-stakes settings where incomplete explanations can have serious consequences."
  },
  {
    "title": "SmartHome-Bench: A Comprehensive Benchmark for Video Anomaly Detection in Smart Homes Using Multi-Modal Large Language Models",
    "url": "http://arxiv.org/abs/2506.12992v1",
    "arxiv_id": "2506.12992v1",
    "authors": [
      "Xinyi Zhao",
      "Congjing Zhang",
      "Pei Guo",
      "Wei Li",
      "Lin Chen",
      "Chaoyue Zhao",
      "Shuai Huang"
    ],
    "published": "2025-06-15T23:20:08+00:00",
    "summary": "Video anomaly detection (VAD) is essential for enhancing safety and security by identifying unusual events across different environments. Existing VAD benchmarks, however, are primarily designed for general-purpose scenarios, neglecting the specific characteristics of smart home applications. To bridge this gap, we introduce SmartHome-Bench, the first comprehensive benchmark specially designed for evaluating VAD in smart home scenarios, focusing on the capabilities of multi-modal large language models (MLLMs). Our newly proposed benchmark consists of 1,203 videos recorded by smart home cameras, organized according to a novel anomaly taxonomy that includes seven categories, such as Wildlife, Senior Care, and Baby Monitoring. Each video is meticulously annotated with anomaly tags, detailed descriptions, and reasoning. We further investigate adaptation methods for MLLMs in VAD, assessing state-of-the-art closed-source and open-source models with various prompting techniques. Results reveal significant limitations in the current models' ability to detect video anomalies accurately. To address these limitations, we introduce the Taxonomy-Driven Reflective LLM Chain (TRLC), a new LLM chaining framework that achieves a notable 11.62% improvement in detection accuracy. The benchmark dataset and code are publicly available at https://github.com/Xinyi-0724/SmartHome-Bench-LLM."
  },
  {
    "title": "Reasoning Model Unlearning: Forgetting Traces, Not Just Answers, While Preserving Reasoning Skills",
    "url": "http://arxiv.org/abs/2506.12963v1",
    "arxiv_id": "2506.12963v1",
    "authors": [
      "Changsheng Wang",
      "Chongyu Fan",
      "Yihua Zhang",
      "Jinghan Jia",
      "Dennis Wei",
      "Parikshit Ram",
      "Nathalie Baracaldo",
      "Sijia Liu"
    ],
    "published": "2025-06-15T20:54:23+00:00",
    "summary": "Recent advances in large reasoning models (LRMs) have enabled strong chain-of-thought (CoT) generation through test-time computation. While these multi-step reasoning capabilities represent a major milestone in language model performance, they also introduce new safety risks. In this work, we present the first systematic study to revisit the problem of machine unlearning in the context of LRMs. Machine unlearning refers to the process of removing the influence of sensitive, harmful, or undesired data or knowledge from a trained model without full retraining. We show that conventional unlearning algorithms, originally designed for non-reasoning models, are inadequate for LRMs. In particular, even when final answers are successfully erased, sensitive information often persists within the intermediate reasoning steps, i.e., CoT trajectories. To address this challenge, we extend conventional unlearning and propose Reasoning-aware Representation Misdirection for Unlearning ($R^2MU$), a novel method that effectively suppresses sensitive reasoning traces and prevents the generation of associated final answers, while preserving the model's reasoning ability. Our experiments demonstrate that $R^2MU$ significantly reduces sensitive information leakage within reasoning traces and achieves strong performance across both safety and reasoning benchmarks, evaluated on state-of-the-art models such as DeepSeek-R1-Distill-LLaMA-8B and DeepSeek-R1-Distill-Qwen-14B."
  },
  {
    "title": "Jailbreak Strength and Model Similarity Predict Transferability",
    "url": "http://arxiv.org/abs/2506.12913v1",
    "arxiv_id": "2506.12913v1",
    "authors": [
      "Rico Angell",
      "Jannik Brinkmann",
      "He He"
    ],
    "published": "2025-06-15T17:03:11+00:00",
    "summary": "Jailbreaks pose an imminent threat to ensuring the safety of modern AI systems by enabling users to disable safeguards and elicit unsafe information. Sometimes, jailbreaks discovered for one model incidentally transfer to another model, exposing a fundamental flaw in safeguarding. Unfortunately, there is no principled approach to identify when jailbreaks will transfer from a source model to a target model. In this work, we observe that transfer success from a source model to a target model depends on quantifiable measures of both jailbreak strength with respect to the source model and the contextual representation similarity of the two models. Furthermore, we show transferability can be increased by distilling from the target model into the source model where the only target model responses used to train the source model are those to benign prompts. We show that the distilled source model can act as a surrogate for the target model, yielding more transferable attacks against the target model. These results suggest that the success of jailbreaks is not merely due to exploitation of safety training failing to generalize out-of-distribution, but instead a consequence of a more fundamental flaw in contextual representations computed by models."
  },
  {
    "title": "Universal Jailbreak Suffixes Are Strong Attention Hijackers",
    "url": "http://arxiv.org/abs/2506.12880v1",
    "arxiv_id": "2506.12880v1",
    "authors": [
      "Matan Ben-Tov",
      "Mor Geva",
      "Mahmood Sharif"
    ],
    "published": "2025-06-15T15:20:37+00:00",
    "summary": "We study suffix-based jailbreaks$\\unicode{x2013}$a powerful family of attacks against large language models (LLMs) that optimize adversarial suffixes to circumvent safety alignment. Focusing on the widely used foundational GCG attack (Zou et al., 2023), we observe that suffixes vary in efficacy: some markedly more universal$\\unicode{x2013}$generalizing to many unseen harmful instructions$\\unicode{x2013}$than others. We first show that GCG's effectiveness is driven by a shallow, critical mechanism, built on the information flow from the adversarial suffix to the final chat template tokens before generation. Quantifying the dominance of this mechanism during generation, we find GCG irregularly and aggressively hijacks the contextualization process. Crucially, we tie hijacking to the universality phenomenon, with more universal suffixes being stronger hijackers. Subsequently, we show that these insights have practical implications: GCG universality can be efficiently enhanced (up to $\\times$5 in some cases) at no additional computational cost, and can also be surgically mitigated, at least halving attack success with minimal utility loss. We release our code and data at http://github.com/matanbt/interp-jailbreak."
  },
  {
    "title": "Bridging Data-Driven and Physics-Based Models: A Consensus Multi-Model Kalman Filter for Robust Vehicle State Estimation",
    "url": "http://arxiv.org/abs/2506.12862v1",
    "arxiv_id": "2506.12862v1",
    "authors": [
      "Farid Mafi",
      "Ladan Khoshnevisan",
      "Mohammad Pirani",
      "Amir Khajepour"
    ],
    "published": "2025-06-15T14:22:47+00:00",
    "summary": "Vehicle state estimation presents a fundamental challenge for autonomous driving systems, requiring both physical interpretability and the ability to capture complex nonlinear behaviors across diverse operating conditions. Traditional methodologies often rely exclusively on either physics-based or data-driven models, each with complementary strengths and limitations that become most noticeable during critical scenarios. This paper presents a novel consensus multi-model Kalman filter framework that integrates heterogeneous model types to leverage their complementary strengths while minimizing individual weaknesses. We introduce two distinct methodologies for handling covariance propagation in data-driven models: a Koopman operator-based linearization approach enabling analytical covariance propagation, and an ensemble-based method providing unified uncertainty quantification across model types without requiring pretraining. Our approach implements an iterative consensus fusion procedure that dynamically weighs different models based on their demonstrated reliability in current operating conditions. The experimental results conducted on an electric all-wheel-drive Equinox vehicle demonstrate performance improvements over single-model techniques, with particularly significant advantages during challenging maneuvers and varying road conditions, confirming the effectiveness and robustness of the proposed methodology for safety-critical autonomous driving applications."
  },
  {
    "title": "Governments Should Mandate Tiered Anonymity on Social-Media Platforms to Counter Deepfakes and LLM-Driven Mass Misinformation",
    "url": "http://arxiv.org/abs/2506.12814v1",
    "arxiv_id": "2506.12814v1",
    "authors": [
      "David Khachaturov",
      "Roxanne Schnyder",
      "Robert Mullins"
    ],
    "published": "2025-06-15T11:18:10+00:00",
    "summary": "This position paper argues that governments should mandate a three-tier anonymity framework on social-media platforms as a reactionary measure prompted by the ease-of-production of deepfakes and large-language-model-driven misinformation. The tiers are determined by a given user's $\\textit{reach score}$: Tier 1 permits full pseudonymity for smaller accounts, preserving everyday privacy; Tier 2 requires private legal-identity linkage for accounts with some influence, reinstating real-world accountability at moderate reach; Tier 3 would require per-post, independent, ML-assisted fact-checking, review for accounts that would traditionally be classed as sources-of-mass-information.   An analysis of Reddit shows volunteer moderators converge on comparable gates as audience size increases -- karma thresholds, approval queues, and identity proofs -- demonstrating operational feasibility and social legitimacy. Acknowledging that existing engagement incentives deter voluntary adoption, we outline a regulatory pathway that adapts existing US jurisprudence and recent EU-UK safety statutes to embed reach-proportional identity checks into existing platform tooling, thereby curbing large-scale misinformation while preserving everyday privacy."
  },
  {
    "title": "Prosocial Design in Trust and Safety",
    "url": "http://arxiv.org/abs/2506.12792v1",
    "arxiv_id": "2506.12792v1",
    "authors": [
      "David Gr\u00fcning",
      "Julia Kamin"
    ],
    "published": "2025-06-15T09:47:47+00:00",
    "summary": "This chapter presents an overview of Prosocial Design, an approach to platform design and governance that recognizes design choices influence behavior and that those choices can or should be made toward supporting healthy interactions and other prosocial outcomes. The authors discuss several core principles of Prosocial Design and its relationship to Trust and Safety and other related fields. As a primary contribution, the chapter reviews relevant research to demonstrate how Prosocial Design can be an effective approach to reducing rule-breaking and other harmful behavior and how it can help to stem the spread of harmful misinformation. Prosocial Design is a nascent and evolving field and research is still limited. The authors hope this chapter will not only inspire more research and the adoption of a prosocial design approach, but that it will also provoke discussion about the principles of Prosocial Design and its potential to support Trust and Safety."
  },
  {
    "title": "Multimodal Large Language Models-Enabled UAV Swarm: Towards Efficient and Intelligent Autonomous Aerial Systems",
    "url": "http://arxiv.org/abs/2506.12710v1",
    "arxiv_id": "2506.12710v1",
    "authors": [
      "Yuqi Ping",
      "Tianhao Liang",
      "Huahao Ding",
      "Guangyu Lei",
      "Junwei Wu",
      "Xuan Zou",
      "Kuan Shi",
      "Rui Shao",
      "Chiya Zhang",
      "Weizheng Zhang",
      "Weijie Yuan",
      "Tingting Zhang"
    ],
    "published": "2025-06-15T03:44:54+00:00",
    "summary": "Recent breakthroughs in multimodal large language models (MLLMs) have endowed AI systems with unified perception, reasoning and natural-language interaction across text, image and video streams. Meanwhile, Unmanned Aerial Vehicle (UAV) swarms are increasingly deployed in dynamic, safety-critical missions that demand rapid situational understanding and autonomous adaptation. This paper explores potential solutions for integrating MLLMs with UAV swarms to enhance the intelligence and adaptability across diverse tasks. Specifically, we first outline the fundamental architectures and functions of UAVs and MLLMs. Then, we analyze how MLLMs can enhance the UAV system performance in terms of target detection, autonomous navigation, and multi-agent coordination, while exploring solutions for integrating MLLMs into UAV systems. Next, we propose a practical case study focused on the forest fire fighting. To fully reveal the capabilities of the proposed framework, human-machine interaction, swarm task planning, fire assessment, and task execution are investigated. Finally, we discuss the challenges and future research directions for the MLLMs-enabled UAV swarm. An experiment illustration video could be found online at https://youtu.be/zwnB9ZSa5A4."
  },
  {
    "title": "Serving Large Language Models on Huawei CloudMatrix384",
    "url": "http://arxiv.org/abs/2506.12708v1",
    "arxiv_id": "2506.12708v1",
    "authors": [
      "Pengfei Zuo",
      "Huimin Lin",
      "Junbo Deng",
      "Nan Zou",
      "Xingkun Yang",
      "Yingyu Diao",
      "Weifeng Gao",
      "Ke Xu",
      "Zhangyu Chen",
      "Shirui Lu",
      "Zhao Qiu",
      "Peiyang Li",
      "Xianyu Chang",
      "Zhengzhong Yu",
      "Fangzheng Miao",
      "Jia Zheng",
      "Ying Li",
      "Yuan Feng",
      "Bei Wang",
      "Zaijian Zong",
      "Mosong Zhou",
      "Wenli Zhou",
      "Houjiang Chen",
      "Xingyu Liao",
      "Yipeng Li",
      "Wenxiao Zhang",
      "Ping Zhu",
      "Yinggang Wang",
      "Chuanjie Xiao",
      "Depeng Liang",
      "Dong Cao",
      "Juncheng Liu",
      "Yongqiang Yang",
      "Xiaolong Bai",
      "Yi Li",
      "Huaguo Xie",
      "Huatao Wu",
      "Zhibin Yu",
      "Lv Chen",
      "Hu Liu",
      "Yujun Ding",
      "Haipei Zhu",
      "Jing Xia",
      "Yi Xiong",
      "Zhou Yu",
      "Heng Liao"
    ],
    "published": "2025-06-15T03:41:34+00:00",
    "summary": "The rapid evolution of large language models (LLMs), driven by growing parameter scales, adoption of mixture-of-experts (MoE) architectures, and expanding context lengths, imposes unprecedented demands on AI infrastructure. Traditional AI clusters face limitations in compute intensity, memory bandwidth, inter-chip communication, and latency, compounded by variable workloads and strict service-level objectives. Addressing these issues requires fundamentally redesigned hardware-software integration. This paper introduces Huawei CloudMatrix, a next-generation AI datacenter architecture, realized in the production-grade CloudMatrix384 supernode. It integrates 384 Ascend 910C NPUs and 192 Kunpeng CPUs interconnected via an ultra-high-bandwidth Unified Bus (UB) network, enabling direct all-to-all communication and dynamic pooling of resources. These features optimize performance for communication-intensive operations, such as large-scale MoE expert parallelism and distributed key-value cache access. To fully leverage CloudMatrix384, we propose CloudMatrix-Infer, an advanced LLM serving solution incorporating three core innovations: a peer-to-peer serving architecture that independently scales prefill, decode, and caching; a large-scale expert parallelism strategy supporting EP320 via efficient UB-based token dispatch; and hardware-aware optimizations including specialized operators, microbatch-based pipelining, and INT8 quantization. Evaluation with the DeepSeek-R1 model shows CloudMatrix-Infer achieves state-of-the-art efficiency: prefill throughput of 6,688 tokens/s per NPU and decode throughput of 1,943 tokens/s per NPU (<50 ms TPOT). It effectively balances throughput and latency, sustaining 538 tokens/s even under stringent 15 ms latency constraints, while INT8 quantization maintains model accuracy across benchmarks."
  },
  {
    "title": "SecurityLingua: Efficient Defense of LLM Jailbreak Attacks via Security-Aware Prompt Compression",
    "url": "http://arxiv.org/abs/2506.12707v1",
    "arxiv_id": "2506.12707v1",
    "authors": [
      "Yucheng Li",
      "Surin Ahn",
      "Huiqiang Jiang",
      "Amir H. Abdi",
      "Yuqing Yang",
      "Lili Qiu"
    ],
    "published": "2025-06-15T03:39:13+00:00",
    "summary": "Large language models (LLMs) have achieved widespread adoption across numerous applications. However, many LLMs are vulnerable to malicious attacks even after safety alignment. These attacks typically bypass LLMs' safety guardrails by wrapping the original malicious instructions inside adversarial jailbreaks prompts. Previous research has proposed methods such as adversarial training and prompt rephrasing to mitigate these safety vulnerabilities, but these methods often reduce the utility of LLMs or lead to significant computational overhead and online latency. In this paper, we propose SecurityLingua, an effective and efficient approach to defend LLMs against jailbreak attacks via security-oriented prompt compression. Specifically, we train a prompt compressor designed to discern the \"true intention\" of the input prompt, with a particular focus on detecting the malicious intentions of adversarial prompts. Then, in addition to the original prompt, the intention is passed via the system prompt to the target LLM to help it identify the true intention of the request. SecurityLingua ensures a consistent user experience by leaving the original input prompt intact while revealing the user's potentially malicious intention and stimulating the built-in safety guardrails of the LLM. Moreover, thanks to prompt compression, SecurityLingua incurs only a negligible overhead and extra token cost compared to all existing defense methods, making it an especially practical solution for LLM defense. Experimental results demonstrate that SecurityLingua can effectively defend against malicious attacks and maintain utility of the LLM with negligible compute and latency overhead. Our code is available at https://aka.ms/SecurityLingua."
  },
  {
    "title": "Flexible Realignment of Language Models",
    "url": "http://arxiv.org/abs/2506.12704v1",
    "arxiv_id": "2506.12704v1",
    "authors": [
      "Wenhong Zhu",
      "Ruobing Xie",
      "Weinan Zhang",
      "Rui Wang"
    ],
    "published": "2025-06-15T03:26:59+00:00",
    "summary": "Realignment becomes necessary when a language model (LM) fails to meet expected performance. We propose a flexible realignment framework that supports quantitative control of alignment degree during training and inference. This framework incorporates Training-time Realignment (TrRa), which efficiently realigns the reference model by leveraging the controllable fusion of logits from both the reference and already aligned models. For example, TrRa reduces token usage by 54.63% on DeepSeek-R1-Distill-Qwen-1.5B without any performance degradation, outperforming DeepScaleR-1.5B's 33.86%. To complement TrRa during inference, we introduce a layer adapter that enables smooth Inference-time Realignment (InRa). This adapter is initialized to perform an identity transformation at the bottom layer and is inserted preceding the original layers. During inference, input embeddings are simultaneously processed by the adapter and the original layer, followed by the remaining layers, and then controllably interpolated at the logit level. We upgraded DeepSeek-R1-Distill-Qwen-7B from a slow-thinking model to one that supports both fast and slow thinking, allowing flexible alignment control even during inference. By encouraging deeper reasoning, it even surpassed its original performance."
  },
  {
    "title": "Alphabet Index Mapping: Jailbreaking LLMs through Semantic Dissimilarity",
    "url": "http://arxiv.org/abs/2506.12685v1",
    "arxiv_id": "2506.12685v1",
    "authors": [
      "Bilal Saleh Husain"
    ],
    "published": "2025-06-15T01:59:08+00:00",
    "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities, yet their susceptibility to adversarial attacks, particularly jailbreaking, poses significant safety and ethical concerns. While numerous jailbreak methods exist, many suffer from computational expense, high token usage, or complex decoding schemes. Liu et al. (2024) introduced FlipAttack, a black-box method that achieves high attack success rates (ASR) through simple prompt manipulation. This paper investigates the underlying mechanisms of FlipAttack's effectiveness by analyzing the semantic changes induced by its flipping modes. We hypothesize that semantic dissimilarity between original and manipulated prompts is inversely correlated with ASR. To test this, we examine embedding space visualizations (UMAP, KDE) and cosine similarities for FlipAttack's modes. Furthermore, we introduce a novel adversarial attack, Alphabet Index Mapping (AIM), designed to maximize semantic dissimilarity while maintaining simple decodability. Experiments on GPT-4 using a subset of AdvBench show AIM and its variant AIM+FWO achieve a 94% ASR, outperforming FlipAttack and other methods on this subset. Our findings suggest that while high semantic dissimilarity is crucial, a balance with decoding simplicity is key for successful jailbreaking. This work contributes to a deeper understanding of adversarial prompt mechanics and offers a new, effective jailbreak technique."
  },
  {
    "title": "Spatial Optimization of Autonomous Vehicle Assignment Based on Distance-Driven Demand and Customer Patience",
    "url": "http://arxiv.org/abs/2506.12671v1",
    "arxiv_id": "2506.12671v1",
    "authors": [
      "Niloufar Mirzavand Boroujeni",
      "Nasim Mirzavand Boroujeni",
      "Nima Moradi",
      "Saeed Jamalzadeh"
    ],
    "published": "2025-06-15T00:31:51+00:00",
    "summary": "Autonomous vehicles (AVs) can improve efficiency, reduce costs, and enhance road safety. They optimize traffic flow, minimize congestion, and support sustainability through shared mobility and reduced fuel consumption. A key challenge in AV deployment is allocating vehicles to parking lots across regions to meet fluctuating demand. Proper allocation reduces delays, lowers costs, and boosts user satisfaction by ensuring timely vehicle availability. This paper explores the impact of customer wait time patience on AV allocation models, allowing prioritization of ride requests while balancing fleet efficiency and user satisfaction. It also addresses the effectiveness of vehicle pooling in decentralized service areas. We propose a mathematical model integrating vehicle distribution and customer patience to maintain both efficiency and satisfaction. Results show that adding more facilities initially reduces costs, but the benefits diminish with more facilities. Increased customer patience improves inventory pooling benefits, especially when no fixed costs are tied to facility operation."
  },
  {
    "title": "OpenUnlearning: Accelerating LLM Unlearning via Unified Benchmarking of Methods and Metrics",
    "url": "http://arxiv.org/abs/2506.12618v1",
    "arxiv_id": "2506.12618v1",
    "authors": [
      "Vineeth Dorna",
      "Anmol Mekala",
      "Wenlong Zhao",
      "Andrew McCallum",
      "Zachary C. Lipton",
      "J. Zico Kolter",
      "Pratyush Maini"
    ],
    "published": "2025-06-14T20:16:37+00:00",
    "summary": "Robust unlearning is crucial for safely deploying large language models (LLMs) in environments where data privacy, model safety, and regulatory compliance must be ensured. Yet the task is inherently challenging, partly due to difficulties in reliably measuring whether unlearning has truly occurred. Moreover, fragmentation in current methodologies and inconsistent evaluation metrics hinder comparative analysis and reproducibility. To unify and accelerate research efforts, we introduce OpenUnlearning, a standardized and extensible framework designed explicitly for benchmarking both LLM unlearning methods and metrics. OpenUnlearning integrates 9 unlearning algorithms and 16 diverse evaluations across 3 leading benchmarks (TOFU, MUSE, and WMDP) and also enables analyses of forgetting behaviors across 450+ checkpoints we publicly release. Leveraging OpenUnlearning, we propose a novel meta-evaluation benchmark focused specifically on assessing the faithfulness and robustness of evaluation metrics themselves. We also benchmark diverse unlearning methods and provide a comparative analysis against an extensive evaluation suite. Overall, we establish a clear, community-driven pathway toward rigorous development in LLM unlearning research."
  },
  {
    "title": "Trust-MARL: Trust-Based Multi-Agent Reinforcement Learning Framework for Cooperative On-Ramp Merging Control in Heterogeneous Traffic Flow",
    "url": "http://arxiv.org/abs/2506.12600v1",
    "arxiv_id": "2506.12600v1",
    "authors": [
      "Jie Pan",
      "Tianyi Wang",
      "Christian Claudel",
      "Jing Shi"
    ],
    "published": "2025-06-14T18:35:10+00:00",
    "summary": "Intelligent transportation systems require connected and automated vehicles (CAVs) to conduct safe and efficient cooperation with human-driven vehicles (HVs) in complex real-world traffic environments. However, the inherent unpredictability of human behaviour, especially at bottlenecks such as highway on-ramp merging areas, often disrupts traffic flow and compromises system performance. To address the challenge of cooperative on-ramp merging in heterogeneous traffic environments, this study proposes a trust-based multi-agent reinforcement learning (Trust-MARL) framework. At the macro level, Trust-MARL enhances global traffic efficiency by leveraging inter-agent trust to improve bottleneck throughput and mitigate traffic shockwave through emergent group-level coordination. At the micro level, a dynamic trust mechanism is designed to enable CAVs to adjust their cooperative strategies in response to real-time behaviors and historical interactions with both HVs and other CAVs. Furthermore, a trust-triggered game-theoretic decision-making module is integrated to guide each CAV in adapting its cooperation factor and executing context-aware lane-changing decisions under safety, comfort, and efficiency constraints. An extensive set of ablation studies and comparative experiments validates the effectiveness of the proposed Trust-MARL approach, demonstrating significant improvements in safety, efficiency, comfort, and adaptability across varying CAV penetration rates and traffic densities."
  },
  {
    "title": "Constrained Diffusers for Safe Planning and Control",
    "url": "http://arxiv.org/abs/2506.12544v1",
    "arxiv_id": "2506.12544v1",
    "authors": [
      "Jichen Zhang",
      "Liqun Zhao",
      "Antonis Papachristodoulou",
      "Jack Umenberger"
    ],
    "published": "2025-06-14T15:37:43+00:00",
    "summary": "Diffusion models have shown remarkable potential in planning and control tasks due to their ability to represent multimodal distributions over actions and trajectories. However, ensuring safety under constraints remains a critical challenge for diffusion models. This paper proposes Constrained Diffusers, a novel framework that incorporates constraints into pre-trained diffusion models without retraining or architectural modifications. Inspired by constrained optimization, we apply a constrained Langevin sampling mechanism for the reverse diffusion process that jointly optimizes the trajectory and realizes constraint satisfaction through three iterative algorithms: projected method, primal-dual method and augmented Lagrangian approaches. In addition, we incorporate discrete control barrier functions as constraints for constrained diffusers to guarantee safety in online implementation. Experiments in Maze2D, locomotion, and pybullet ball running tasks demonstrate that our proposed methods achieve constraint satisfaction with less computation time, and are competitive to existing methods in environments with static and time-varying constraints."
  },
  {
    "title": "DinoCompanion: An Attachment-Theory Informed Multimodal Robot for Emotionally Responsive Child-AI Interaction",
    "url": "http://arxiv.org/abs/2506.12486v1",
    "arxiv_id": "2506.12486v1",
    "authors": [
      "Boyang Wang",
      "Yuhao Song",
      "Jinyuan Cao",
      "Peng Yu",
      "Hongcheng Guo",
      "Zhoujun Li"
    ],
    "published": "2025-06-14T12:54:07+00:00",
    "summary": "Children's emotional development fundamentally relies on secure attachment relationships, yet current AI companions lack the theoretical foundation to provide developmentally appropriate emotional support. We introduce DinoCompanion, the first attachment-theory-grounded multimodal robot for emotionally responsive child-AI interaction. We address three critical challenges in child-AI systems: the absence of developmentally-informed AI architectures, the need to balance engagement with safety, and the lack of standardized evaluation frameworks for attachment-based capabilities. Our contributions include: (i) a multimodal dataset of 128 caregiver-child dyads containing 125,382 annotated clips with paired preference-risk labels, (ii) CARPO (Child-Aware Risk-calibrated Preference Optimization), a novel training objective that maximizes engagement while applying epistemic-uncertainty-weighted risk penalties, and (iii) AttachSecure-Bench, a comprehensive evaluation benchmark covering ten attachment-centric competencies with strong expert consensus (\\k{appa}=0.81). DinoCompanion achieves state-of-the-art performance (57.15%), outperforming GPT-4o (50.29%) and Claude-3.7-Sonnet (53.43%), with exceptional secure base behaviors (72.99%, approaching human expert levels of 78.4%) and superior attachment risk detection (69.73%). Ablations validate the critical importance of multimodal fusion, uncertainty-aware risk modeling, and hierarchical memory for coherent, emotionally attuned interactions."
  },
  {
    "title": "Robust LLM Unlearning with MUDMAN: Meta-Unlearning with Disruption Masking And Normalization",
    "url": "http://arxiv.org/abs/2506.12484v1",
    "arxiv_id": "2506.12484v1",
    "authors": [
      "Filip Sondej",
      "Yushi Yang",
      "Miko\u0142aj Kniejski",
      "Marcel Windys"
    ],
    "published": "2025-06-14T12:49:51+00:00",
    "summary": "Language models can retain dangerous knowledge and skills even after extensive safety fine-tuning, posing both misuse and misalignment risks. Recent studies show that even specialized unlearning methods can be easily reversed. To address this, we systematically evaluate many existing and novel components of unlearning methods and identify ones crucial for irreversible unlearning.   We introduce Disruption Masking, a technique in which we only allow updating weights, where the signs of the unlearning gradient and the retaining gradient are the same. This ensures all updates are non-disruptive.   Additionally, we identify the need for normalizing the unlearning gradients, and also confirm the usefulness of meta-learning. We combine these insights into MUDMAN (Meta-Unlearning with Disruption Masking and Normalization) and validate its effectiveness at preventing the recovery of dangerous capabilities. MUDMAN outperforms the prior TAR method by 40\\%, setting a new state-of-the-art for robust unlearning."
  },
  {
    "title": "Tiered Agentic Oversight: A Hierarchical Multi-Agent System for AI Safety in Healthcare",
    "url": "http://arxiv.org/abs/2506.12482v1",
    "arxiv_id": "2506.12482v1",
    "authors": [
      "Yubin Kim",
      "Hyewon Jeong",
      "Chanwoo Park",
      "Eugene Park",
      "Haipeng Zhang",
      "Xin Liu",
      "Hyeonhoon Lee",
      "Daniel McDuff",
      "Marzyeh Ghassemi",
      "Cynthia Breazeal",
      "Samir Tulebaev",
      "Hae Won Park"
    ],
    "published": "2025-06-14T12:46:10+00:00",
    "summary": "Current large language models (LLMs), despite their power, can introduce safety risks in clinical settings due to limitations such as poor error detection and single point of failure. To address this, we propose Tiered Agentic Oversight (TAO), a hierarchical multi-agent framework that enhances AI safety through layered, automated supervision. Inspired by clinical hierarchies (e.g., nurse, physician, specialist), TAO conducts agent routing based on task complexity and agent roles. Leveraging automated inter- and intra-tier collaboration and role-playing, TAO creates a robust safety framework. Ablation studies reveal that TAO's superior performance is driven by its adaptive tiered architecture, which improves safety by over 3.2% compared to static single-tier configurations; the critical role of its lower tiers, particularly tier 1, whose removal most significantly impacts safety; and the strategic assignment of more advanced LLM to these initial tiers, which boosts performance by over 2% compared to less optimal allocations while achieving near-peak safety efficiently. These mechanisms enable TAO to outperform single-agent and multi-agent frameworks in 4 out of 5 healthcare safety benchmarks, showing up to an 8.2% improvement over the next-best methods in these evaluations. Finally, we validate TAO via an auxiliary clinician-in-the-loop study where integrating expert feedback improved TAO's accuracy in medical triage from 40% to 60%."
  },
  {
    "title": "Towards Safety and Security Testing of Cyberphysical Power Systems by Shape Validation",
    "url": "http://arxiv.org/abs/2506.12466v1",
    "arxiv_id": "2506.12466v1",
    "authors": [
      "Alexander Geiger",
      "Immanuel Hacker",
      "\u00d6mer Sen",
      "Andreas Ulbig"
    ],
    "published": "2025-06-14T12:07:44+00:00",
    "summary": "The increasing complexity of cyberphysical power systems leads to larger attack surfaces to be exploited by malicious actors and a higher risk of faults through misconfiguration. We propose to meet those risks with a declarative approach to describe cyberphysical power systems and to automatically evaluate security and safety controls. We leverage Semantic Web technologies as a well-standardized framework, providing languages to specify ontologies, rules and shape constraints. We model infrastructure through an ontology which combines external ontologies, architecture and data models for sufficient expressivity and interoperability with external systems. The ontology can enrich itself through rules defined in SPARQL, allowing for the inference of knowledge that is not explicitly stated. Through the evaluation of SHACL shape constraints we can then validate the data and verify safety and security constraints. We demonstrate this concept with two use cases and illustrate how this solution can be developed further in a community-driven fashion."
  },
  {
    "title": "Pushing the Limits of Safety: A Technical Report on the ATLAS Challenge 2025",
    "url": "http://arxiv.org/abs/2506.12430v1",
    "arxiv_id": "2506.12430v1",
    "authors": [
      "Zonghao Ying",
      "Siyang Wu",
      "Run Hao",
      "Peng Ying",
      "Shixuan Sun",
      "Pengyu Chen",
      "Junze Chen",
      "Hao Du",
      "Kaiwen Shen",
      "Shangkun Wu",
      "Jiwei Wei",
      "Shiyuan He",
      "Yang Yang",
      "Xiaohai Xu",
      "Ke Ma",
      "Qianqian Xu",
      "Qingming Huang",
      "Shi Lin",
      "Xun Wang",
      "Changting Lin",
      "Meng Han",
      "Yilei Jiang",
      "Siqi Lai",
      "Yaozhi Zheng",
      "Yifei Song",
      "Xiangyu Yue",
      "Zonglei Jing",
      "Tianyuan Zhang",
      "Zhilei Zhu",
      "Aishan Liu",
      "Jiakai Wang",
      "Siyuan Liang",
      "Xianglong Kong",
      "Hainan Li",
      "Junjie Mu",
      "Haotong Qin",
      "Yue Yu",
      "Lei Chen",
      "Felix Juefei-Xu",
      "Qing Guo",
      "Xinyun Chen",
      "Yew Soon Ong",
      "Xianglong Liu",
      "Dawn Song",
      "Alan Yuille",
      "Philip Torr",
      "Dacheng Tao"
    ],
    "published": "2025-06-14T10:03:17+00:00",
    "summary": "Multimodal Large Language Models (MLLMs) have enabled transformative advancements across diverse applications but remain susceptible to safety threats, especially jailbreak attacks that induce harmful outputs. To systematically evaluate and improve their safety, we organized the Adversarial Testing & Large-model Alignment Safety Grand Challenge (ATLAS) 2025}. This technical report presents findings from the competition, which involved 86 teams testing MLLM vulnerabilities via adversarial image-text attacks in two phases: white-box and black-box evaluations. The competition results highlight ongoing challenges in securing MLLMs and provide valuable guidance for developing stronger defense mechanisms. The challenge establishes new benchmarks for MLLM safety evaluation and lays groundwork for advancing safer multimodal AI systems. The code and data for this challenge are openly available at https://github.com/NY1024/ATLAS_Challenge_2025."
  },
  {
    "title": "Exploring the Secondary Risks of Large Language Models",
    "url": "http://arxiv.org/abs/2506.12382v1",
    "arxiv_id": "2506.12382v1",
    "authors": [
      "Jiawei Chen",
      "Zhengwei Fang",
      "Xiao Yang",
      "Chao Yu",
      "Zhaoxia Yin",
      "Hang Su"
    ],
    "published": "2025-06-14T07:31:52+00:00",
    "summary": "Ensuring the safety and alignment of Large Language Models is a significant challenge with their growing integration into critical applications and societal functions. While prior research has primarily focused on jailbreak attacks, less attention has been given to non-adversarial failures that subtly emerge during benign interactions. We introduce secondary risks a novel class of failure modes marked by harmful or misleading behaviors during benign prompts. Unlike adversarial attacks, these risks stem from imperfect generalization and often evade standard safety mechanisms. To enable systematic evaluation, we introduce two risk primitives verbose response and speculative advice that capture the core failure patterns. Building on these definitions, we propose SecLens, a black-box, multi-objective search framework that efficiently elicits secondary risk behaviors by optimizing task relevance, risk activation, and linguistic plausibility. To support reproducible evaluation, we release SecRiskBench, a benchmark dataset of 650 prompts covering eight diverse real-world risk categories. Experimental results from extensive evaluations on 16 popular models demonstrate that secondary risks are widespread, transferable across models, and modality independent, emphasizing the urgent need for enhanced safety mechanisms to address benign yet harmful LLM behaviors in real-world deployments."
  },
  {
    "title": "Understanding and Benchmarking the Trustworthiness in Multimodal LLMs for Video Understanding",
    "url": "http://arxiv.org/abs/2506.12336v1",
    "arxiv_id": "2506.12336v1",
    "authors": [
      "Youze Wang",
      "Zijun Chen",
      "Ruoyu Chen",
      "Shishen Gu",
      "Yinpeng Dong",
      "Hang Su",
      "Jun Zhu",
      "Meng Wang",
      "Richang Hong",
      "Wenbo Hu"
    ],
    "published": "2025-06-14T04:04:54+00:00",
    "summary": "Recent advancements in multimodal large language models for video understanding (videoLLMs) have improved their ability to process dynamic multimodal data. However, trustworthiness challenges factual inaccuracies, harmful content, biases, hallucinations, and privacy risks, undermine reliability due to video data's spatiotemporal complexities. This study introduces Trust-videoLLMs, a comprehensive benchmark evaluating videoLLMs across five dimensions: truthfulness, safety, robustness, fairness, and privacy. Comprising 30 tasks with adapted, synthetic, and annotated videos, the framework assesses dynamic visual scenarios, cross-modal interactions, and real-world safety concerns. Our evaluation of 23 state-of-the-art videoLLMs (5 commercial,18 open-source) reveals significant limitations in dynamic visual scene understanding and cross-modal perturbation resilience. Open-source videoLLMs show occasional truthfulness advantages but inferior overall credibility compared to commercial models, with data diversity outperforming scale effects. These findings highlight the need for advanced safety alignment to enhance capabilities. Trust-videoLLMs provides a publicly available, extensible toolbox for standardized trustworthiness assessments, bridging the gap between accuracy-focused benchmarks and critical demands for robustness, safety, fairness, and privacy."
  },
  {
    "title": "Perspective on Utilizing Foundation Models for Laboratory Automation in Materials Research",
    "url": "http://arxiv.org/abs/2506.12312v1",
    "arxiv_id": "2506.12312v1",
    "authors": [
      "Kan Hatakeyama-Sato",
      "Toshihiko Nishida",
      "Kenta Kitamura",
      "Yoshitaka Ushiku",
      "Koichi Takahashi",
      "Yuta Nabae",
      "Teruaki Hayakawa"
    ],
    "published": "2025-06-14T02:22:28+00:00",
    "summary": "This review explores the potential of foundation models to advance laboratory automation in the materials and chemical sciences. It emphasizes the dual roles of these models: cognitive functions for experimental planning and data analysis, and physical functions for hardware operations. While traditional laboratory automation has relied heavily on specialized, rigid systems, foundation models offer adaptability through their general-purpose intelligence and multimodal capabilities. Recent advancements have demonstrated the feasibility of using large language models (LLMs) and multimodal robotic systems to handle complex and dynamic laboratory tasks. However, significant challenges remain, including precision manipulation of hardware, integration of multimodal data, and ensuring operational safety. This paper outlines a roadmap highlighting future directions, advocating for close interdisciplinary collaboration, benchmark establishment, and strategic human-AI integration to realize fully autonomous experimental laboratories."
  },
  {
    "title": "QGuard:Question-based Zero-shot Guard for Multi-modal LLM Safety",
    "url": "http://arxiv.org/abs/2506.12299v1",
    "arxiv_id": "2506.12299v1",
    "authors": [
      "Taegyeong Lee",
      "Jeonghwa Yoo",
      "Hyoungseo Cho",
      "Soo Yong Kim",
      "Yunho Maeng"
    ],
    "published": "2025-06-14T01:23:50+00:00",
    "summary": "The recent advancements in Large Language Models(LLMs) have had a significant impact on a wide range of fields, from general domains to specialized areas. However, these advancements have also significantly increased the potential for malicious users to exploit harmful and jailbreak prompts for malicious attacks. Although there have been many efforts to prevent harmful prompts and jailbreak prompts, protecting LLMs from such malicious attacks remains an important and challenging task. In this paper, we propose QGuard, a simple yet effective safety guard method, that utilizes question prompting to block harmful prompts in a zero-shot manner. Our method can defend LLMs not only from text-based harmful prompts but also from multi-modal harmful prompt attacks. Moreover, by diversifying and modifying guard questions, our approach remains robust against the latest harmful prompts without fine-tuning. Experimental results show that our model performs competitively on both text-only and multi-modal harmful datasets. Additionally, by providing an analysis of question prompting, we enable a white-box analysis of user inputs. We believe our method provides valuable insights for real-world LLM services in mitigating security risks associated with harmful prompts."
  },
  {
    "title": "InfoFlood: Jailbreaking Large Language Models with Information Overload",
    "url": "http://arxiv.org/abs/2506.12274v1",
    "arxiv_id": "2506.12274v1",
    "authors": [
      "Advait Yadav",
      "Haibo Jin",
      "Man Luo",
      "Jun Zhuang",
      "Haohan Wang"
    ],
    "published": "2025-06-13T23:03:11+00:00",
    "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across various domains. However, their potential to generate harmful responses has raised significant societal and regulatory concerns, especially when manipulated by adversarial techniques known as \"jailbreak\" attacks. Existing jailbreak methods typically involve appending carefully crafted prefixes or suffixes to malicious prompts in order to bypass the built-in safety mechanisms of these models.   In this work, we identify a new vulnerability in which excessive linguistic complexity can disrupt built-in safety mechanisms-without the need for any added prefixes or suffixes-allowing attackers to elicit harmful outputs directly. We refer to this phenomenon as Information Overload.   To automatically exploit this vulnerability, we propose InfoFlood, a jailbreak attack that transforms malicious queries into complex, information-overloaded queries capable of bypassing built-in safety mechanisms. Specifically, InfoFlood: (1) uses linguistic transformations to rephrase malicious queries, (2) identifies the root cause of failure when an attempt is unsuccessful, and (3) refines the prompt's linguistic structure to address the failure while preserving its malicious intent.   We empirically validate the effectiveness of InfoFlood on four widely used LLMs-GPT-4o, GPT-3.5-turbo, Gemini 2.0, and LLaMA 3.1-by measuring their jailbreak success rates. InfoFlood consistently outperforms baseline attacks, achieving up to 3 times higher success rates across multiple jailbreak benchmarks. Furthermore, we demonstrate that commonly adopted post-processing defenses, including OpenAI's Moderation API, Perspective API, and SmoothLLM, fail to mitigate these attacks. This highlights a critical weakness in traditional AI safety guardrails when confronted with information overload-based jailbreaks."
  },
  {
    "title": "A Survey of Foundation Models for IoT: Taxonomy and Criteria-Based Analysis",
    "url": "http://arxiv.org/abs/2506.12263v1",
    "arxiv_id": "2506.12263v1",
    "authors": [
      "Hui Wei",
      "Dong Yoon Lee",
      "Shubham Rohal",
      "Zhizhang Hu",
      "Shiwei Fang",
      "Shijia Pan"
    ],
    "published": "2025-06-13T22:28:55+00:00",
    "summary": "Foundation models have gained growing interest in the IoT domain due to their reduced reliance on labeled data and strong generalizability across tasks, which address key limitations of traditional machine learning approaches. However, most existing foundation model based methods are developed for specific IoT tasks, making it difficult to compare approaches across IoT domains and limiting guidance for applying them to new tasks. This survey aims to bridge this gap by providing a comprehensive overview of current methodologies and organizing them around four shared performance objectives by different domains: efficiency, context-awareness, safety, and security & privacy. For each objective, we review representative works, summarize commonly-used techniques and evaluation metrics. This objective-centric organization enables meaningful cross-domain comparisons and offers practical insights for selecting and designing foundation model based solutions for new IoT tasks. We conclude with key directions for future research to guide both practitioners and researchers in advancing the use of foundation models in IoT applications."
  },
  {
    "title": "Supernova Event Dataset: Interpreting Large Language Model's Personality through Critical Event Analysis",
    "url": "http://arxiv.org/abs/2506.12189v1",
    "arxiv_id": "2506.12189v1",
    "authors": [
      "Pranav Agarwal",
      "Ioana Ciuc\u0103"
    ],
    "published": "2025-06-13T19:31:52+00:00",
    "summary": "Large Language Models (LLMs) are increasingly integrated into everyday applications. As their influence grows, understanding their decision making and underlying personality becomes essential. In this work, we interpret model personality using our proposed Supernova Event Dataset, a novel dataset with diverse articles spanning biographies, historical events, news, and scientific discoveries. We use this dataset to benchmark LLMs on extracting and ranking key events from text, a subjective and complex challenge that requires reasoning over long-range context and modeling causal chains. We evaluate small models like Phi-4, Orca 2, and Qwen 2.5, and large, stronger models such as Claude 3.7, Gemini 2.5, and OpenAI o3, and propose a framework where another LLM acts as a judge to infer each model's personality based on its selection and classification of events. Our analysis shows distinct personality traits: for instance, Orca 2 demonstrates emotional reasoning focusing on interpersonal dynamics, while Qwen 2.5 displays a more strategic, analytical style. When analyzing scientific discovery events, Claude Sonnet 3.7 emphasizes conceptual framing, Gemini 2.5 Pro prioritizes empirical validation, and o3 favors step-by-step causal reasoning. This analysis improves model interpretability, making them user-friendly for a wide range of diverse applications."
  },
  {
    "title": "Because we have LLMs, we Can and Should Pursue Agentic Interpretability",
    "url": "http://arxiv.org/abs/2506.12152v1",
    "arxiv_id": "2506.12152v1",
    "authors": [
      "Been Kim",
      "John Hewitt",
      "Neel Nanda",
      "Noah Fiedel",
      "Oyvind Tafjord"
    ],
    "published": "2025-06-13T18:13:58+00:00",
    "summary": "The era of Large Language Models (LLMs) presents a new opportunity for interpretability--agentic interpretability: a multi-turn conversation with an LLM wherein the LLM proactively assists human understanding by developing and leveraging a mental model of the user, which in turn enables humans to develop better mental models of the LLM. Such conversation is a new capability that traditional `inspective' interpretability methods (opening the black-box) do not use. Having a language model that aims to teach and explain--beyond just knowing how to talk--is similar to a teacher whose goal is to teach well, understanding that their success will be measured by the student's comprehension. While agentic interpretability may trade off completeness for interactivity, making it less suitable for high-stakes safety situations with potentially deceptive models, it leverages a cooperative model to discover potentially superhuman concepts that can improve humans' mental model of machines. Agentic interpretability introduces challenges, particularly in evaluation, due to what we call `human-entangled-in-the-loop' nature (humans responses are integral part of the algorithm), making the design and evaluation difficult. We discuss possible solutions and proxy goals. As LLMs approach human parity in many tasks, agentic interpretability's promise is to help humans learn the potentially superhuman concepts of the LLMs, rather than see us fall increasingly far from understanding them."
  },
  {
    "title": "Hatevolution: What Static Benchmarks Don't Tell Us",
    "url": "http://arxiv.org/abs/2506.12148v1",
    "arxiv_id": "2506.12148v1",
    "authors": [
      "Chiara Di Bonaventura",
      "Barbara McGillivray",
      "Yulan He",
      "Albert Mero\u00f1o-Pe\u00f1uela"
    ],
    "published": "2025-06-13T18:08:19+00:00",
    "summary": "Language changes over time, including in the hate speech domain, which evolves quickly following social dynamics and cultural shifts. While NLP research has investigated the impact of language evolution on model training and has proposed several solutions for it, its impact on model benchmarking remains under-explored. Yet, hate speech benchmarks play a crucial role to ensure model safety. In this paper, we empirically evaluate the robustness of 20 language models across two evolving hate speech experiments, and we show the temporal misalignment between static and time-sensitive evaluations. Our findings call for time-sensitive linguistic benchmarks in order to correctly and reliably evaluate language models in the hate speech domain."
  },
  {
    "title": "Tracing LLM Reasoning Processes with Strategic Games: A Framework for Planning, Revision, and Resource-Constrained Decision Making",
    "url": "http://arxiv.org/abs/2506.12012v1",
    "arxiv_id": "2506.12012v1",
    "authors": [
      "Xiaopeng Yuan",
      "Xingjian Zhang",
      "Ke Xu",
      "Yifan Xu",
      "Lijun Yu",
      "Jindong Wang",
      "Yushun Dong",
      "Haohan Wang"
    ],
    "published": "2025-06-13T17:59:10+00:00",
    "summary": "Large language models (LLMs) are increasingly used for tasks that require complex reasoning. Most benchmarks focus on final outcomes but overlook the intermediate reasoning steps - such as planning, revision, and decision making under resource constraints. We argue that measuring these internal processes is essential for understanding model behavior and improving reliability. We propose using strategic games as a natural evaluation environment: closed, rule-based systems with clear states, limited resources, and automatic feedback. We introduce a framework that evaluates LLMs along three core dimensions: planning, revision, and resource-constrained decision making. To operationalize this, we define metrics beyond win rate, including overcorrection risk rate, correction success rate, improvement slope, and over-budget ratio. In 4320 adversarial rounds across 12 leading models, ChatGPT-o3-mini achieves the top composite score, with a win rate of 74.7 percent, a correction success rate of 78.6 percent, and an improvement slope of 0.041. By contrast, Qwen-Plus, despite an overcorrection risk rate of 81.6 percent, wins only 25.6 percent of its matches - primarily due to excessive resource use. We also observe a negative correlation between overcorrection risk rate and correction success rate (Pearson r = -0.51, p = 0.093), suggesting that more frequent edits do not always improve outcomes. Our findings highlight the value of assessing not only what LLMs decide but how they arrive at those decisions"
  },
  {
    "title": "Compression Aware Certified Training",
    "url": "http://arxiv.org/abs/2506.11992v1",
    "arxiv_id": "2506.11992v1",
    "authors": [
      "Changming Xu",
      "Gagandeep Singh"
    ],
    "published": "2025-06-13T17:48:50+00:00",
    "summary": "Deep neural networks deployed in safety-critical, resource-constrained environments must balance efficiency and robustness. Existing methods treat compression and certified robustness as separate goals, compromising either efficiency or safety. We propose CACTUS (Compression Aware Certified Training Using network Sets), a general framework for unifying these objectives during training. CACTUS models maintain high certified accuracy even when compressed. We apply CACTUS for both pruning and quantization and show that it effectively trains models which can be efficiently compressed while maintaining high accuracy and certifiable robustness. CACTUS achieves state-of-the-art accuracy and certified performance for both pruning and quantization on a variety of datasets and input specifications."
  },
  {
    "title": "Automated Treatment Planning for Interstitial HDR Brachytherapy for Locally Advanced Cervical Cancer using Deep Reinforcement Learning",
    "url": "http://arxiv.org/abs/2506.11957v1",
    "arxiv_id": "2506.11957v1",
    "authors": [
      "Mohammadamin Moradi",
      "Runyu Jiang",
      "Yingzi Liu",
      "Malvern Madondo",
      "Tianming Wu",
      "James J. Sohn",
      "Xiaofeng Yang",
      "Yasmin Hasan",
      "Zhen Tian"
    ],
    "published": "2025-06-13T17:07:30+00:00",
    "summary": "High-dose-rate (HDR) brachytherapy plays a critical role in the treatment of locally advanced cervical cancer but remains highly dependent on manual treatment planning expertise. The objective of this study is to develop a fully automated HDR brachytherapy planning framework that integrates reinforcement learning (RL) and dose-based optimization to generate clinically acceptable treatment plans with improved consistency and efficiency. We propose a hierarchical two-stage autoplanning framework. In the first stage, a deep Q-network (DQN)-based RL agent iteratively selects treatment planning parameters (TPPs), which control the trade-offs between target coverage and organ-at-risk (OAR) sparing. The agent's state representation includes both dose-volume histogram (DVH) metrics and current TPP values, while its reward function incorporates clinical dose objectives and safety constraints, including D90, V150, V200 for targets, and D2cc for all relevant OARs (bladder, rectum, sigmoid, small bowel, and large bowel). In the second stage, a customized Adam-based optimizer computes the corresponding dwell time distribution for the selected TPPs using a clinically informed loss function. The framework was evaluated on a cohort of patients with complex applicator geometries. The proposed framework successfully learned clinically meaningful TPP adjustments across diverse patient anatomies. For the unseen test patients, the RL-based automated planning method achieved an average score of 93.89%, outperforming the clinical plans which averaged 91.86%. These findings are notable given that score improvements were achieved while maintaining full target coverage and reducing CTV hot spots in most cases."
  },
  {
    "title": "Improving Large Language Model Safety with Contrastive Representation Learning",
    "url": "http://arxiv.org/abs/2506.11938v1",
    "arxiv_id": "2506.11938v1",
    "authors": [
      "Samuel Simko",
      "Mrinmaya Sachan",
      "Bernhard Sch\u00f6lkopf",
      "Zhijing Jin"
    ],
    "published": "2025-06-13T16:42:09+00:00",
    "summary": "Large Language Models (LLMs) are powerful tools with profound societal impacts, yet their ability to generate responses to diverse and uncontrolled inputs leaves them vulnerable to adversarial attacks. While existing defenses often struggle to generalize across varying attack types, recent advancements in representation engineering offer promising alternatives. In this work, we propose a defense framework that formulates model defense as a contrastive representation learning (CRL) problem. Our method finetunes a model using a triplet-based loss combined with adversarial hard negative mining to encourage separation between benign and harmful representations. Our experimental results across multiple models demonstrate that our approach outperforms prior representation engineering-based defenses, improving robustness against both input-level and embedding-space attacks without compromising standard performance. Our code is available at https://github.com/samuelsimko/crl-llm-defense"
  },
  {
    "title": "Real-World Deployment of a Lane Change Prediction Architecture Based on Knowledge Graph Embeddings and Bayesian Inference",
    "url": "http://arxiv.org/abs/2506.11925v1",
    "arxiv_id": "2506.11925v1",
    "authors": [
      "M. Manzour",
      "Catherine M. Elias",
      "Omar M. Shehata",
      "R. Izquierdo",
      "M. A. Sotelo"
    ],
    "published": "2025-06-13T16:24:28+00:00",
    "summary": "Research on lane change prediction has gained a lot of momentum in the last couple of years. However, most research is confined to simulation or results obtained from datasets, leaving a gap between algorithmic advances and on-road deployment. This work closes that gap by demonstrating, on real hardware, a lane-change prediction system based on Knowledge Graph Embeddings (KGEs) and Bayesian inference. Moreover, the ego-vehicle employs a longitudinal braking action to ensure the safety of both itself and the surrounding vehicles. Our architecture consists of two modules: (i) a perception module that senses the environment, derives input numerical features, and converts them into linguistic categories; and communicates them to the prediction module; (ii) a pretrained prediction module that executes a KGE and Bayesian inference model to anticipate the target vehicle's maneuver and transforms the prediction into longitudinal braking action. Real-world hardware experimental validation demonstrates that our prediction system anticipates the target vehicle's lane change three to four seconds in advance, providing the ego vehicle sufficient time to react and allowing the target vehicle to make the lane change safely."
  },
  {
    "title": "The Throughput Gain of Hypercycle-level Resource Reservation for Time-Triggered Ethernet",
    "url": "http://arxiv.org/abs/2506.11745v1",
    "arxiv_id": "2506.11745v1",
    "authors": [
      "Peng Wang",
      "Suman Sourav",
      "Binbin Chen",
      "Hongyan Li",
      "Feng Wang",
      "Fan Zhang"
    ],
    "published": "2025-06-13T12:56:56+00:00",
    "summary": "Time-Triggered Communication is a key technology for many safety-critical systems, with applications spanning the areas of aerospace and industrial control. Such communication relies on time-triggered flows, with each flow consisting of periodic packets originating from a source and destined for a destination node. Each packet needs to reach its destination before its deadline. Different flows can have different cycle lengths. To achieve assured transmission of time-triggered flows, existing efforts constrain the packets of a flow to be cyclically transmitted along the same path. Under such Fixed Cyclic Scheduling (FCS), reservation for flows with different cycle lengths can become incompatible over a shared link, limiting the total number of admissible flows. Considering the cycle lengths of different flows, a hyper-cycle has length equal to their least common multiple (LCM). It determines the time duration over which the scheduling compatibility of the different flows can be checked. In this work, we propose a more flexible schedule scheme called the Hypercycle-level Flexible Schedule (HFS) scheme, where a flow's resource reservation can change across cycles within a hypercycle. HFS can significantly increase the number of admitted flows by providing more scheduling options while remaining perfectly compatible with existing Time-Triggered Ethernet system. We show that, theoretically the possible capacity gain provided by HFS over FCS can be unbounded. We formulate the joint pathfinding and scheduling problem under HFS as an ILP problem which we prove to be NP-Hard. To solve HFS efficiently, we further propose a least-load-first heuristic (HFS-LLF), solving HFS as a sequence of shortest path problems. Extensive study shows that HFS admits up to 6 times the number of flows achieved by FCS. Moreover, our proposed HFS-LLF can run 104 times faster than solving HFS using a generic solver."
  },
  {
    "title": "Model Organisms for Emergent Misalignment",
    "url": "http://arxiv.org/abs/2506.11613v1",
    "arxiv_id": "2506.11613v1",
    "authors": [
      "Edward Turner",
      "Anna Soligo",
      "Mia Taylor",
      "Senthooran Rajamanoharan",
      "Neel Nanda"
    ],
    "published": "2025-06-13T09:34:25+00:00",
    "summary": "Recent work discovered Emergent Misalignment (EM): fine-tuning large language models on narrowly harmful datasets can lead them to become broadly misaligned. A survey of experts prior to publication revealed this was highly unexpected, demonstrating critical gaps in our understanding of model alignment. In this work, we both advance understanding and provide tools for future research. Using new narrowly misaligned datasets, we create a set of improved model organisms that achieve 99% coherence (vs. 67% prior), work with smaller 0.5B parameter models (vs. 32B), and that induce misalignment using a single rank-1 LoRA adapter. We demonstrate that EM occurs robustly across diverse model sizes, three model families, and numerous training protocols including full supervised fine-tuning. Leveraging these cleaner model organisms, we isolate a mechanistic phase transition and demonstrate that it corresponds to a robust behavioural phase transition in all studied organisms. Aligning large language models is critical for frontier AI safety, yet EM exposes how far we are from achieving this robustly. By distilling clean model organisms that isolate a minimal alignment-compromising change, and where this is learnt, we establish a foundation for future research into understanding and mitigating alignment risks in LLMs."
  },
  {
    "title": "Linearly Solving Robust Rotation Estimation",
    "url": "http://arxiv.org/abs/2506.11547v1",
    "arxiv_id": "2506.11547v1",
    "authors": [
      "Yinlong Liu",
      "Tianyu Huang",
      "Zhi-Xin Yang"
    ],
    "published": "2025-06-13T08:00:03+00:00",
    "summary": "Rotation estimation plays a fundamental role in computer vision and robot tasks, and extremely robust rotation estimation is significantly useful for safety-critical applications. Typically, estimating a rotation is considered a non-linear and non-convex optimization problem that requires careful design. However, in this paper, we provide some new perspectives that solving a rotation estimation problem can be reformulated as solving a linear model fitting problem without dropping any constraints and without introducing any singularities. In addition, we explore the dual structure of a rotation motion, revealing that it can be represented as a great circle on a quaternion sphere surface. Accordingly, we propose an easily understandable voting-based method to solve rotation estimation. The proposed method exhibits exceptional robustness to noise and outliers and can be computed in parallel with graphics processing units (GPUs) effortlessly. Particularly, leveraging the power of GPUs, the proposed method can obtain a satisfactory rotation solution for large-scale($10^6$) and severely corrupted (99$\\%$ outlier ratio) rotation estimation problems under 0.5 seconds. Furthermore, to validate our theoretical framework and demonstrate the superiority of our proposed method, we conduct controlled experiments and real-world dataset experiments. These experiments provide compelling evidence supporting the effectiveness and robustness of our approach in solving rotation estimation problems."
  },
  {
    "title": "Do Not Immerse and Drive? Prolonged Effects of Cybersickness on Physiological Stress Markers And Cognitive Performance",
    "url": "http://arxiv.org/abs/2506.11536v1",
    "arxiv_id": "2506.11536v1",
    "authors": [
      "Daniel Zielasko",
      "Ben Rehling",
      "Bernadette von Dawans",
      "Gregor Domes"
    ],
    "published": "2025-06-13T07:40:31+00:00",
    "summary": "Extended exposure to virtual reality environments can induce motion sickness, often referred to as cybersickness, which may lead to physiological stress responses and impaired cognitive performance. This study investigates the aftereffects of VR-induced motion sickness with a focus on physiological stress markers and working memory performance. Using a carousel simulation to elicit cybersickness, we assessed subjective discomfort (SSQ, FMS), physiological stress (salivary cortisol, alpha-amylase, electrodermal activity, heart rate), and cognitive performance (n-Back task) over a 90-minute post-exposure period. Our findings demonstrate a significant increase in both subjective and physiological stress indicators following VR exposure, accompanied by a decline in working memory performance. Notably, delayed symptom progression was observed in a substantial proportion of participants, with some reporting peak symptoms up to 90 minutes post-stimulation. Salivary cortisol levels remained elevated throughout the observation period, indicating prolonged stress recovery. These results highlight the need for longer washout phases in XR research and raise safety concerns for professional applications involving post-exposure task performance."
  },
  {
    "title": "Foundation Models in Autonomous Driving: A Survey on Scenario Generation and Scenario Analysis",
    "url": "http://arxiv.org/abs/2506.11526v1",
    "arxiv_id": "2506.11526v1",
    "authors": [
      "Yuan Gao",
      "Mattia Piccinini",
      "Yuchen Zhang",
      "Dingrui Wang",
      "Korbinian Moller",
      "Roberto Brusnicki",
      "Baha Zarrouki",
      "Alessio Gambi",
      "Jan Frederik Totz",
      "Kai Storms",
      "Steven Peters",
      "Andrea Stocco",
      "Bassam Alrifaee",
      "Marco Pavone",
      "Johannes Betz"
    ],
    "published": "2025-06-13T07:25:59+00:00",
    "summary": "For autonomous vehicles, safe navigation in complex environments depends on handling a broad range of diverse and rare driving scenarios. Simulation- and scenario-based testing have emerged as key approaches to development and validation of autonomous driving systems. Traditional scenario generation relies on rule-based systems, knowledge-driven models, and data-driven synthesis, often producing limited diversity and unrealistic safety-critical cases. With the emergence of foundation models, which represent a new generation of pre-trained, general-purpose AI models, developers can process heterogeneous inputs (e.g., natural language, sensor data, HD maps, and control actions), enabling the synthesis and interpretation of complex driving scenarios. In this paper, we conduct a survey about the application of foundation models for scenario generation and scenario analysis in autonomous driving (as of May 2025). Our survey presents a unified taxonomy that includes large language models, vision-language models, multimodal large language models, diffusion models, and world models for the generation and analysis of autonomous driving scenarios. In addition, we review the methodologies, open-source datasets, simulation platforms, and benchmark challenges, and we examine the evaluation metrics tailored explicitly to scenario generation and analysis. Finally, the survey concludes by highlighting the open challenges and research questions, and outlining promising future research directions. All reviewed papers are listed in a continuously maintained repository, which contains supplementary materials and is available at https://github.com/TUM-AVS/FM-for-Scenario-Generation-Analysis."
  },
  {
    "title": "Investigating Vulnerabilities and Defenses Against Audio-Visual Attacks: A Comprehensive Survey Emphasizing Multimodal Models",
    "url": "http://arxiv.org/abs/2506.11521v1",
    "arxiv_id": "2506.11521v1",
    "authors": [
      "Jinming Wen",
      "Xinyi Wu",
      "Shuai Zhao",
      "Yanhao Jia",
      "Yuwen Li"
    ],
    "published": "2025-06-13T07:22:36+00:00",
    "summary": "Multimodal large language models (MLLMs), which bridge the gap between audio-visual and natural language processing, achieve state-of-the-art performance on several audio-visual tasks. Despite the superior performance of MLLMs, the scarcity of high-quality audio-visual training data and computational resources necessitates the utilization of third-party data and open-source MLLMs, a trend that is increasingly observed in contemporary research. This prosperity masks significant security risks. Empirical studies demonstrate that the latest MLLMs can be manipulated to produce malicious or harmful content. This manipulation is facilitated exclusively through instructions or inputs, including adversarial perturbations and malevolent queries, effectively bypassing the internal security mechanisms embedded within the models. To gain a deeper comprehension of the inherent security vulnerabilities associated with audio-visual-based multimodal models, a series of surveys investigates various types of attacks, including adversarial and backdoor attacks. While existing surveys on audio-visual attacks provide a comprehensive overview, they are limited to specific types of attacks, which lack a unified review of various types of attacks. To address this issue and gain insights into the latest trends in the field, this paper presents a comprehensive and systematic review of audio-visual attacks, which include adversarial attacks, backdoor attacks, and jailbreak attacks. Furthermore, this paper also reviews various types of attacks in the latest audio-visual-based MLLMs, a dimension notably absent in existing surveys. Drawing upon comprehensive insights from a substantial review, this paper delineates both challenges and emergent trends for future research on audio-visual attacks and defense."
  },
  {
    "title": "Taming Stable Diffusion for Computed Tomography Blind Super-Resolution",
    "url": "http://arxiv.org/abs/2506.11496v1",
    "arxiv_id": "2506.11496v1",
    "authors": [
      "Chunlei Li",
      "Yilei Shi",
      "Haoxi Hu",
      "Jingliang Hu",
      "Xiao Xiang Zhu",
      "Lichao Mou"
    ],
    "published": "2025-06-13T06:45:05+00:00",
    "summary": "High-resolution computed tomography (CT) imaging is essential for medical diagnosis but requires increased radiation exposure, creating a critical trade-off between image quality and patient safety. While deep learning methods have shown promise in CT super-resolution, they face challenges with complex degradations and limited medical training data. Meanwhile, large-scale pre-trained diffusion models, particularly Stable Diffusion, have demonstrated remarkable capabilities in synthesizing fine details across various vision tasks. Motivated by this, we propose a novel framework that adapts Stable Diffusion for CT blind super-resolution. We employ a practical degradation model to synthesize realistic low-quality images and leverage a pre-trained vision-language model to generate corresponding descriptions. Subsequently, we perform super-resolution using Stable Diffusion with a specialized controlling strategy, conditioned on both low-resolution inputs and the generated text descriptions. Extensive experiments show that our method outperforms existing approaches, demonstrating its potential for achieving high-quality CT imaging at reduced radiation doses. Our code will be made publicly available."
  },
  {
    "title": "On the Natural Robustness of Vision-Language Models Against Visual Perception Attacks in Autonomous Driving",
    "url": "http://arxiv.org/abs/2506.11472v1",
    "arxiv_id": "2506.11472v1",
    "authors": [
      "Pedram MohajerAnsari",
      "Amir Salarpour",
      "Michael K\u00fchr",
      "Siyu Huang",
      "Mohammad Hamad",
      "Sebastian Steinhorst",
      "Habeeb Olufowobi",
      "Mert D. Pes\u00e9"
    ],
    "published": "2025-06-13T05:22:12+00:00",
    "summary": "Autonomous vehicles (AVs) rely on deep neural networks (DNNs) for critical tasks such as traffic sign recognition (TSR), automated lane centering (ALC), and vehicle detection (VD). However, these models are vulnerable to attacks that can cause misclassifications and compromise safety. Traditional defense mechanisms, including adversarial training, often degrade benign accuracy and fail to generalize against unseen attacks. In this work, we introduce Vehicle Vision Language Models (V2LMs), fine-tuned vision-language models specialized for AV perception. Our findings demonstrate that V2LMs inherently exhibit superior robustness against unseen attacks without requiring adversarial training, maintaining significantly higher accuracy than conventional DNNs under adversarial conditions. We evaluate two deployment strategies: Solo Mode, where individual V2LMs handle specific perception tasks, and Tandem Mode, where a single unified V2LM is fine-tuned for multiple tasks simultaneously. Experimental results reveal that DNNs suffer performance drops of 33% to 46% under attacks, whereas V2LMs maintain adversarial accuracy with reductions of less than 8% on average. The Tandem Mode further offers a memory-efficient alternative while achieving comparable robustness to Solo Mode. We also explore integrating V2LMs as parallel components to AV perception to enhance resilience against adversarial threats. Our results suggest that V2LMs offer a promising path toward more secure and resilient AV perception systems."
  },
  {
    "title": "ReVeal: Self-Evolving Code Agents via Iterative Generation-Verification",
    "url": "http://arxiv.org/abs/2506.11442v1",
    "arxiv_id": "2506.11442v1",
    "authors": [
      "Yiyang Jin",
      "Kunzhao Xu",
      "Hang Li",
      "Xueting Han",
      "Yanmin Zhou",
      "Cheng Li",
      "Jing Bai"
    ],
    "published": "2025-06-13T03:41:04+00:00",
    "summary": "Recent advances in reinforcement learning (RL) with verifiable outcome rewards have significantly improved the reasoning capabilities of large language models (LLMs), especially when combined with multi-turn tool interactions. However, existing methods lack both meaningful verification signals from realistic environments and explicit optimization for verification, leading to unreliable self-verification. To address these limitations, we propose ReVeal, a multi-turn reinforcement learning framework that interleaves code generation with explicit self-verification and tool-based evaluation. ReVeal enables LLMs to autonomously generate test cases, invoke external tools for precise feedback, and improves performance via a customized RL algorithm with dense, per-turn rewards. As a result, ReVeal fosters the co-evolution of a model's generation and verification capabilities through RL training, expanding the reasoning boundaries of the base model, demonstrated by significant gains in Pass@k on LiveCodeBench. It also enables test-time scaling into deeper inference regimes, with code consistently evolving as the number of turns increases during inference, ultimately surpassing DeepSeek-R1-Zero-Qwen-32B. These findings highlight the promise of ReVeal as a scalable and effective paradigm for building more robust and autonomous AI agents."
  },
  {
    "title": "A Step-by-Step Guide to Creating a Robust Autonomous Drone Testing Pipeline",
    "url": "http://arxiv.org/abs/2506.11400v1",
    "arxiv_id": "2506.11400v1",
    "authors": [
      "Yupeng Jiang",
      "Yao Deng",
      "Sebastian Schroder",
      "Linfeng Liang",
      "Suhaas Gambhir",
      "Alice James",
      "Avishkar Seth",
      "James Pirrie",
      "Yihao Zhang",
      "Xi Zheng"
    ],
    "published": "2025-06-13T01:55:23+00:00",
    "summary": "Autonomous drones are rapidly reshaping industries ranging from aerial delivery and infrastructure inspection to environmental monitoring and disaster response. Ensuring the safety, reliability, and efficiency of these systems is paramount as they transition from research prototypes to mission-critical platforms. This paper presents a step-by-step guide to establishing a robust autonomous drone testing pipeline, covering each critical stage: Software-in-the-Loop (SIL) Simulation Testing, Hardware-in-the-Loop (HIL) Testing, Controlled Real-World Testing, and In-Field Testing. Using practical examples, including the marker-based autonomous landing system, we demonstrate how to systematically verify drone system behaviors, identify integration issues, and optimize performance. Furthermore, we highlight emerging trends shaping the future of drone testing, including the integration of Neurosymbolic and LLMs, creating co-simulation environments, and Digital Twin-enabled simulation-based testing techniques. By following this pipeline, developers and researchers can achieve comprehensive validation, minimize deployment risks, and prepare autonomous drones for safe and reliable real-world operations."
  },
  {
    "title": "Learning a Continue-Thinking Token for Enhanced Test-Time Scaling",
    "url": "http://arxiv.org/abs/2506.11274v1",
    "arxiv_id": "2506.11274v1",
    "authors": [
      "Liran Ringel",
      "Elad Tolochinsky",
      "Yaniv Romano"
    ],
    "published": "2025-06-12T20:28:54+00:00",
    "summary": "Test-time scaling has emerged as an effective approach for improving language model performance by utilizing additional compute at inference time. Recent studies have shown that overriding end-of-thinking tokens (e.g., replacing \"</think>\" with \"Wait\") can extend reasoning steps and improve accuracy. In this work, we explore whether a dedicated continue-thinking token can be learned to trigger extended reasoning. We augment a distilled version of DeepSeek-R1 with a single learned \"<|continue-thinking|>\" token, training only its embedding via reinforcement learning while keeping the model weights frozen. Our experiments show that this learned token achieves improved accuracy on standard math benchmarks compared to both the baseline model and a test-time scaling approach that uses a fixed token (e.g., \"Wait\") for budget forcing. In particular, we observe that in cases where the fixed-token approach enhances the base model's accuracy, our method achieves a markedly greater improvement. For example, on the GSM8K benchmark, the fixed-token approach yields a 1.3% absolute improvement in accuracy, whereas our learned-token method achieves a 4.2% improvement over the base model that does not use budget forcing."
  },
  {
    "title": "How Well Can Reasoning Models Identify and Recover from Unhelpful Thoughts?",
    "url": "http://arxiv.org/abs/2506.10979v1",
    "arxiv_id": "2506.10979v1",
    "authors": [
      "Sohee Yang",
      "Sang-Woo Lee",
      "Nora Kassner",
      "Daniela Gottesman",
      "Sebastian Riedel",
      "Mor Geva"
    ],
    "published": "2025-06-12T17:59:53+00:00",
    "summary": "Recent reasoning models show the ability to reflect, backtrack, and self-validate their reasoning, which is crucial in spotting mistakes and arriving at accurate solutions. A natural question that arises is how effectively models can perform such self-reevaluation. We tackle this question by investigating how well reasoning models identify and recover from four types of unhelpful thoughts: uninformative rambling thoughts, thoughts irrelevant to the question, thoughts misdirecting the question as a slightly different question, and thoughts that lead to incorrect answers. We show that models are effective at identifying most unhelpful thoughts but struggle to recover from the same thoughts when these are injected into their thinking process, causing significant performance drops. Models tend to naively continue the line of reasoning of the injected irrelevant thoughts, which showcases that their self-reevaluation abilities are far from a general \"meta-cognitive\" awareness. Moreover, we observe non/inverse-scaling trends, where larger models struggle more than smaller ones to recover from short irrelevant thoughts, even when instructed to reevaluate their reasoning. We demonstrate the implications of these findings with a jailbreak experiment using irrelevant thought injection, showing that the smallest models are the least distracted by harmful-response-triggering thoughts. Overall, our findings call for improvement in self-reevaluation of reasoning models to develop better reasoning and safer systems."
  },
  {
    "title": "Build the web for agents, not agents for the web",
    "url": "http://arxiv.org/abs/2506.10953v1",
    "arxiv_id": "2506.10953v1",
    "authors": [
      "Xing Han L\u00f9",
      "Gaurav Kamath",
      "Marius Mosbach",
      "Siva Reddy"
    ],
    "published": "2025-06-12T17:53:58+00:00",
    "summary": "Recent advancements in Large Language Models (LLMs) and multimodal counterparts have spurred significant interest in developing web agents -- AI systems capable of autonomously navigating and completing tasks within web environments. While holding tremendous promise for automating complex web interactions, current approaches face substantial challenges due to the fundamental mismatch between human-designed interfaces and LLM capabilities. Current methods struggle with the inherent complexity of web inputs, whether processing massive DOM trees, relying on screenshots augmented with additional information, or bypassing the user interface entirely through API interactions. This position paper advocates for a paradigm shift in web agent research: rather than forcing web agents to adapt to interfaces designed for humans, we should develop a new interaction paradigm specifically optimized for agentic capabilities. To this end, we introduce the concept of an Agentic Web Interface (AWI), an interface specifically designed for agents to navigate a website. We establish six guiding principles for AWI design, emphasizing safety, efficiency, and standardization, to account for the interests of all primary stakeholders. This reframing aims to overcome fundamental limitations of existing interfaces, paving the way for more efficient, reliable, and transparent web agent design, which will be a collaborative effort involving the broader ML community."
  },
  {
    "title": "Monitoring Decomposition Attacks in LLMs with Lightweight Sequential Monitors",
    "url": "http://arxiv.org/abs/2506.10949v1",
    "arxiv_id": "2506.10949v1",
    "authors": [
      "Chen Yueh-Han",
      "Nitish Joshi",
      "Yulin Chen",
      "Maksym Andriushchenko",
      "Rico Angell",
      "He He"
    ],
    "published": "2025-06-12T17:50:58+00:00",
    "summary": "Current LLM safety defenses fail under decomposition attacks, where a malicious goal is decomposed into benign subtasks that circumvent refusals. The challenge lies in the existing shallow safety alignment techniques: they only detect harm in the immediate prompt and do not reason about long-range intent, leaving them blind to malicious intent that emerges over a sequence of seemingly benign instructions. We therefore propose adding an external monitor that observes the conversation at a higher granularity. To facilitate our study of monitoring decomposition attacks, we curate the largest and most diverse dataset to date, including question-answering, text-to-image, and agentic tasks. We verify our datasets by testing them on frontier LLMs and show an 87% attack success rate on average on GPT-4o. This confirms that decomposition attack is broadly effective. Additionally, we find that random tasks can be injected into the decomposed subtasks to further obfuscate malicious intents. To defend in real time, we propose a lightweight sequential monitoring framework that cumulatively evaluates each subtask. We show that a carefully prompt engineered lightweight monitor achieves a 93% defense success rate, beating reasoning models like o3 mini as a monitor. Moreover, it remains robust against random task injection and cuts cost by 90% and latency by 50%. Our findings suggest that lightweight sequential monitors are highly effective in mitigating decomposition attacks and are viable in deployment."
  },
  {
    "title": "Viability of Future Actions: Robust Safety in Reinforcement Learning via Entropy Regularization",
    "url": "http://arxiv.org/abs/2506.10871v1",
    "arxiv_id": "2506.10871v1",
    "authors": [
      "Pierre-Fran\u00e7ois Massiani",
      "Alexander von Rohr",
      "Lukas Haverbeck",
      "Sebastian Trimpe"
    ],
    "published": "2025-06-12T16:34:19+00:00",
    "summary": "Despite the many recent advances in reinforcement learning (RL), the question of learning policies that robustly satisfy state constraints under unknown disturbances remains open. In this paper, we offer a new perspective on achieving robust safety by analyzing the interplay between two well-established techniques in model-free RL: entropy regularization, and constraints penalization. We reveal empirically that entropy regularization in constrained RL inherently biases learning toward maximizing the number of future viable actions, thereby promoting constraints satisfaction robust to action noise. Furthermore, we show that by relaxing strict safety constraints through penalties, the constrained RL problem can be approximated arbitrarily closely by an unconstrained one and thus solved using standard model-free RL. This reformulation preserves both safety and optimality while empirically improving resilience to disturbances. Our results indicate that the connection between entropy regularization and robustness is a promising avenue for further empirical and theoretical investigation, as it enables robust safety in RL through simple reward shaping."
  },
  {
    "title": "MultiCoSim: A Python-based Multi-Fidelity Co-Simulation Framework",
    "url": "http://arxiv.org/abs/2506.10869v1",
    "arxiv_id": "2506.10869v1",
    "authors": [
      "Quinn Thibeault",
      "Giulia Pedrielli"
    ],
    "published": "2025-06-12T16:31:39+00:00",
    "summary": "Simulation is a foundational tool for the analysis and testing of cyber-physical systems (CPS), underpinning activities such as algorithm development, runtime monitoring, and system verification. As CPS grow in complexity and scale, particularly in safety-critical and learning-enabled settings, accurate analysis and synthesis increasingly rely on the rapid use of simulation experiments. Because CPS inherently integrate hardware, software, and physical processes, simulation platforms must support co-simulation of heterogeneous components at varying levels of fidelity. Despite recent advances in high-fidelity modeling of hardware, firmware, and physics, co-simulation in diverse environments remains challenging. These limitations hinder the development of reusable benchmarks and impede the use of simulation for automated and comparative evaluation.   Existing simulation tools often rely on rigid configurations, lack automation support, and present obstacles to portability and modularity. Many are configured through static text files or impose constraints on how simulation components are represented and connected, making it difficult to flexibly compose systems or integrate components across platforms.   To address these challenges, we introduce MultiCoSim, a Python-based simulation framework that enables users to define, compose, and configure simulation components programmatically. MultiCoSim supports distributed, component-based co-simulation and allows seamless substitution and reconfiguration of components. We demonstrate the flexibility of MultiCoSim through case studies that include co-simulations involving custom automaton-based controllers, as well as integration with off-the-shelf platforms like the PX4 autopilot for aerial robotics. These examples highlight MultiCoSim's capability to streamline CPS simulation pipelines for research and development."
  },
  {
    "title": "Automotive Radar Online Channel Imbalance Estimation via NLMS",
    "url": "http://arxiv.org/abs/2506.10841v1",
    "arxiv_id": "2506.10841v1",
    "authors": [
      "Esmaeil Kavousi Ghafi",
      "Oliver Lang",
      "Matthias Wagner",
      "Alexander Melzer",
      "Mario Huemer"
    ],
    "published": "2025-06-12T15:57:29+00:00",
    "summary": "Automotive radars are one of the essential enablers of advanced driver assistance systems (ADASs). Continuous monitoring of the functional safety and reliability of automotive radars is a crucial requirement to prevent accidents and increase road safety. One of the most critical aspects to monitor in this context is radar channel imbalances, as they are a key parameter regarding the reliability of the radar. These imbalances may originate from several parameter variations or hardware fatigues, e.g., a solder ball break (SBB), and may affect some radar processing steps, such as the angle of arrival estimation. In this work, a novel method for online estimation of automotive radar channel imbalances is proposed. The proposed method exploits a normalized least mean squares (NLMS) algorithm as a block in the processing chain of the radar to estimate the channel imbalances. The input of this block is the detected targets in the range-Doppler map of the radar on the road without any prior knowledge on the angular parameters of the targets. This property in combination with low computational complexity of the NLMS, makes the proposed method suitable for online channel imbalance estimation, in parallel to the normal operation of the radar. Furthermore, it features reduced dependency on specific targets of interest and faster update rates of the channel imbalance estimation compared to the majority of state-of-the-art methods. This improvement is achieved by allowing for multiple targets in the angular spectrum, whereas most other methods are restricted to only single targets in the angular spectrum. The performance of the proposed method is validated using various simulation scenarios and is supported by measurement results."
  },
  {
    "title": "RationalVLA: A Rational Vision-Language-Action Model with Dual System",
    "url": "http://arxiv.org/abs/2506.10826v1",
    "arxiv_id": "2506.10826v1",
    "authors": [
      "Wenxuan Song",
      "Jiayi Chen",
      "Wenxue Li",
      "Xu He",
      "Han Zhao",
      "Pengxiang Ding Shiyan Su",
      "Feilong Tang",
      "Xuelian Cheng",
      "Donglin Wang",
      "Zongyuan Ge",
      "Xinhu Zheng",
      "Zhe Liu",
      "Hesheng Wang",
      "Yunhui Liu",
      "Haoang Li"
    ],
    "published": "2025-06-12T15:44:51+00:00",
    "summary": "A fundamental requirement for real-world robotic deployment is the ability to understand and respond to natural language instructions. Existing language-conditioned manipulation tasks typically assume that instructions are perfectly aligned with the environment. This assumption limits robustness and generalization in realistic scenarios where instructions may be ambiguous, irrelevant, or infeasible. To address this problem, we introduce RAtional MAnipulation (RAMA), a new benchmark that challenges models with both unseen executable instructions and defective ones that should be rejected. In RAMA, we construct a dataset with over 14,000 samples, including diverse defective instructions spanning six dimensions: visual, physical, semantic, motion, safety, and out-of-context. We further propose the Rational Vision-Language-Action model (RationalVLA). It is a dual system for robotic arms that integrates the high-level vision-language model with the low-level manipulation policy by introducing learnable latent space embeddings. This design enables RationalVLA to reason over instructions, reject infeasible commands, and execute manipulation effectively. Experiments demonstrate that RationalVLA outperforms state-of-the-art baselines on RAMA by a 14.5% higher success rate and 0.94 average task length, while maintaining competitive performance on standard manipulation tasks. Real-world trials further validate its effectiveness and robustness in practical applications. Our project page is https://irpn-eai.github.io/rationalvla."
  },
  {
    "title": "RationalVLA: A Rational Vision-Language-Action Model with Dual System",
    "url": "http://arxiv.org/abs/2506.10826v2",
    "arxiv_id": "2506.10826v2",
    "authors": [
      "Wenxuan Song",
      "Jiayi Chen",
      "Wenxue Li",
      "Xu He",
      "Han Zhao",
      "Can Cui",
      "Pengxiang Ding Shiyan Su",
      "Feilong Tang",
      "Xuelian Cheng",
      "Donglin Wang",
      "Zongyuan Ge",
      "Xinhu Zheng",
      "Zhe Liu",
      "Hesheng Wang",
      "Haoang Li"
    ],
    "published": "2025-06-12T15:44:51+00:00",
    "summary": "A fundamental requirement for real-world robotic deployment is the ability to understand and respond to natural language instructions. Existing language-conditioned manipulation tasks typically assume that instructions are perfectly aligned with the environment. This assumption limits robustness and generalization in realistic scenarios where instructions may be ambiguous, irrelevant, or infeasible. To address this problem, we introduce RAtional MAnipulation (RAMA), a new benchmark that challenges models with both unseen executable instructions and defective ones that should be rejected. In RAMA, we construct a dataset with over 14,000 samples, including diverse defective instructions spanning six dimensions: visual, physical, semantic, motion, safety, and out-of-context. We further propose the Rational Vision-Language-Action model (RationalVLA). It is a dual system for robotic arms that integrates the high-level vision-language model with the low-level manipulation policy by introducing learnable latent space embeddings. This design enables RationalVLA to reason over instructions, reject infeasible commands, and execute manipulation effectively. Experiments demonstrate that RationalVLA outperforms state-of-the-art baselines on RAMA by a 14.5% higher success rate and 0.94 average task length, while maintaining competitive performance on standard manipulation tasks. Real-world trials further validate its effectiveness and robustness in practical applications. Our project page is https://irpn-eai.github.io/RationalVLA."
  },
  {
    "title": "Barium Calcium Zirconium Titanate Thin Film-Based Capacitive Thermoelectric Converter for Low-Grade Waste Heat",
    "url": "http://arxiv.org/abs/2506.10759v1",
    "arxiv_id": "2506.10759v1",
    "authors": [
      "Mohammad K. Al Thehaiban",
      "Vladimir S. Getov",
      "Qiaomu Yao",
      "Chukwudike C. Ukeje",
      "Peter K. Petrov"
    ],
    "published": "2025-06-12T14:43:08+00:00",
    "summary": "A capacitive thermoelectric device can harvest thermal energy and convert it to electrical energy by employing a temperature-dependent dielectric material whose permittivity sharply changes with temperature. Electricity can be generated by fluctuating the temperature of the capacitor. Currently, capacitive thermoelectric devices are not broadly used, which can be attributed to the low efficiency of the existing solutions, the lack of dielectric materials with suitable temperature non-linearity of the dielectric permittivity, and the complexity of modulating heat flux on the dielectric material. Here, we propose a device based on (Ba0.85Ca0.15)(Ti0.92Zr0.08)O3 and (Ba0.73Ca0.27)(Ti0.98Zr0.02)O3 thin films. The estimated power output under different operation conditions and dynamic workload of an Intel E5-2630 microprocessor show that these thin film materials are promising and can potentially be used for a capacitive thermoelectric converter."
  },
  {
    "title": "Beyond True or False: Retrieval-Augmented Hierarchical Analysis of Nuanced Claims",
    "url": "http://arxiv.org/abs/2506.10728v1",
    "arxiv_id": "2506.10728v1",
    "authors": [
      "Priyanka Kargupta",
      "Runchu Tian",
      "Jiawei Han"
    ],
    "published": "2025-06-12T14:17:45+00:00",
    "summary": "Claims made by individuals or entities are oftentimes nuanced and cannot be clearly labeled as entirely \"true\" or \"false\" -- as is frequently the case with scientific and political claims. However, a claim (e.g., \"vaccine A is better than vaccine B\") can be dissected into its integral aspects and sub-aspects (e.g., efficacy, safety, distribution), which are individually easier to validate. This enables a more comprehensive, structured response that provides a well-rounded perspective on a given problem while also allowing the reader to prioritize specific angles of interest within the claim (e.g., safety towards children). Thus, we propose ClaimSpect, a retrieval-augmented generation-based framework for automatically constructing a hierarchy of aspects typically considered when addressing a claim and enriching them with corpus-specific perspectives. This structure hierarchically partitions an input corpus to retrieve relevant segments, which assist in discovering new sub-aspects. Moreover, these segments enable the discovery of varying perspectives towards an aspect of the claim (e.g., support, neutral, or oppose) and their respective prevalence (e.g., \"how many biomedical papers believe vaccine A is more transportable than B?\"). We apply ClaimSpect to a wide variety of real-world scientific and political claims featured in our constructed dataset, showcasing its robustness and accuracy in deconstructing a nuanced claim and representing perspectives within a corpus. Through real-world case studies and human evaluation, we validate its effectiveness over multiple baselines."
  },
  {
    "title": "PREMISE: Scalable and Strategic Prompt Optimization for Efficient Mathematical Reasoning in Large Models",
    "url": "http://arxiv.org/abs/2506.10716v1",
    "arxiv_id": "2506.10716v1",
    "authors": [
      "Ye Yu",
      "Yaoning Yu",
      "Haohan Wang"
    ],
    "published": "2025-06-12T14:05:09+00:00",
    "summary": "Large reasoning models (LRMs) such as Claude 3.7 Sonnet and OpenAI o1 achieve strong performance on mathematical benchmarks using lengthy chain-of-thought (CoT) reasoning, but the resulting traces are often unnecessarily verbose. This inflates token usage and cost, limiting deployment in latency-sensitive or API-constrained settings. We introduce PREMISE (PRompt-based Efficient Mathematical Inference with Strategic Evaluation), a prompt-only framework that reduces reasoning overhead without modifying model weights. PREMISE combines trace-level diagnostics with gradient-inspired prompt optimization to minimize redundant computation while preserving answer accuracy. The approach jointly optimizes brevity and correctness through a multi-objective textual search that balances token length and answer validity. Unlike prior work, PREMISE runs in a single-pass black-box interface, so it can be applied directly to commercial LLMs. On GSM8K, SVAMP, and Math500 we match or exceed baseline accuracy ($96\\%\\rightarrow96\\%$ with Claude, $91\\%\\rightarrow92\\%$ with Gemini) while reducing reasoning tokens by up to $87.5\\%$ and cutting dollar cost by $69$--$82\\%$. These results show that prompt-level optimization is a practical and scalable path to efficient LRM inference without compromising reasoning quality."
  },
  {
    "title": "GOLIATH: A Decentralized Framework for Data Collection in Intelligent Transportation Systems",
    "url": "http://arxiv.org/abs/2506.10665v1",
    "arxiv_id": "2506.10665v1",
    "authors": [
      "Davide Maffiola",
      "Stefano Longari",
      "Michele Carminati",
      "Mara Tanelli",
      "Stefano Zanero"
    ],
    "published": "2025-06-12T12:55:25+00:00",
    "summary": "Intelligent Transportation Systems (ITSs) technology has advanced during the past years, and it is now used for several applications that require vehicles to exchange real-time data, such as in traffic information management. Traditionally, road traffic information has been collected using on-site sensors. However, crowd-sourcing traffic information from onboard sensors or smartphones has become a viable alternative. State-of-the-art solutions currently follow a centralized model where only the service provider has complete access to the collected traffic data and represent a single point of failure and trust. In this paper, we propose GOLIATH, a blockchain-based decentralized framework that runs on the In-Vehicle Infotainment (IVI) system to collect real-time information exchanged between the network's participants. Our approach mitigates the limitations of existing crowd-sourcing centralized solutions by guaranteeing trusted information collection and exchange, fully exploiting the intrinsic distributed nature of vehicles. We demonstrate its feasibility in the context of vehicle positioning and traffic information management. Each vehicle participating in the decentralized network shares its position and neighbors' ones in the form of a transaction recorded on the ledger, which uses a novel consensus mechanism to validate it. We design the consensus mechanism resilient against a realistic set of adversaries that aim to tamper or disable the communication. We evaluate the proposed framework in a simulated (but realistic) environment, which considers different threats and allows showing its robustness and safety properties."
  },
  {
    "title": "CyFence: Securing Cyber-Physical Controllers via Trusted Execution Environment",
    "url": "http://arxiv.org/abs/2506.10638v1",
    "arxiv_id": "2506.10638v1",
    "authors": [
      "Stefano Longari",
      "Alessandro Pozone",
      "Jessica Leoni",
      "Mario Polino",
      "Michele Carminati",
      "Mara Tanelli",
      "Stefano Zanero"
    ],
    "published": "2025-06-12T12:22:45+00:00",
    "summary": "In the last decades, Cyber-physical Systems (CPSs) have experienced a significant technological evolution and increased connectivity, at the cost of greater exposure to cyber-attacks. Since many CPS are used in safety-critical systems, such attacks entail high risks and potential safety harms. Although several defense strategies have been proposed, they rarely exploit the cyber-physical nature of the system. In this work, we exploit the nature of CPS by proposing CyFence, a novel architecture that improves the resilience of closed-loop control systems against cyber-attacks by adding a semantic check, used to confirm that the system is behaving as expected. To ensure the security of the semantic check code, we use the Trusted Execution Environment implemented by modern processors. We evaluate CyFence considering a real-world application, consisting of an active braking digital controller, demonstrating that it can mitigate different types of attacks with a negligible computation overhead."
  },
  {
    "title": "Time Series Forecasting as Reasoning: A Slow-Thinking Approach with Reinforced LLMs",
    "url": "http://arxiv.org/abs/2506.10630v1",
    "arxiv_id": "2506.10630v1",
    "authors": [
      "Yucong Luo",
      "Yitong Zhou",
      "Mingyue Cheng",
      "Jiahao Wang",
      "Daoyu Wang",
      "Tingyue Pan",
      "Jintao Zhang"
    ],
    "published": "2025-06-12T12:15:50+00:00",
    "summary": "To advance time series forecasting (TSF), various methods have been proposed to improve prediction accuracy, evolving from statistical techniques to data-driven deep learning architectures. Despite their effectiveness, most existing methods still adhere to a fast thinking paradigm-relying on extracting historical patterns and mapping them to future values as their core modeling philosophy, lacking an explicit thinking process that incorporates intermediate time series reasoning. Meanwhile, emerging slow-thinking LLMs (e.g., OpenAI-o1) have shown remarkable multi-step reasoning capabilities, offering an alternative way to overcome these issues. However, prompt engineering alone presents several limitations - including high computational cost, privacy risks, and limited capacity for in-depth domain-specific time series reasoning. To address these limitations, a more promising approach is to train LLMs to develop slow thinking capabilities and acquire strong time series reasoning skills. For this purpose, we propose Time-R1, a two-stage reinforcement fine-tuning framework designed to enhance multi-step reasoning ability of LLMs for time series forecasting. Specifically, the first stage conducts supervised fine-tuning for warmup adaptation, while the second stage employs reinforcement learning to improve the model's generalization ability. Particularly, we design a fine-grained multi-objective reward specifically for time series forecasting, and then introduce GRIP (group-based relative importance for policy optimization), which leverages non-uniform sampling to further encourage and optimize the model's exploration of effective reasoning paths. Experiments demonstrate that Time-R1 significantly improves forecast performance across diverse datasets."
  },
  {
    "title": "Scalable Software Testing in Fast Virtual Platforms: Leveraging SystemC, QEMU and Containerization",
    "url": "http://arxiv.org/abs/2506.10624v1",
    "arxiv_id": "2506.10624v1",
    "authors": [
      "Lukas J\u00fcnger",
      "Jan Henrik Weinstock",
      "Tim Kraus"
    ],
    "published": "2025-06-12T12:08:53+00:00",
    "summary": "The ever-increasing complexity of HW/SW systems presents a persistent challenge, particularly in safety-critical domains like automotive, where extensive testing is imperative. However, the availability of hardware often lags behind, hindering early-stage software development. To address this, Virtual Platforms (VPs) based on the SystemC TLM-2.0 standard have emerged as a pivotal solution, enabling pre-silicon execution and testing of unmodified target software. In this study, we propose an approach leveraging containerization to encapsulate VPs in order to reduce environment dependencies and enable cloud deployment for fast, parallelized test execution, as well as open-source VP technologies such as QEMU and VCML to obviate the need for seat licenses. To demonstrate the efficacy of our approach, we present an Artificial Intelligence (AI) accelerator VP case study. Through our research, we offer a robust solution to address the challenges posed by the complexity of HW/SW systems, with practical implications for accelerating HW/SW co-development."
  },
  {
    "title": "SoK: Evaluating Jailbreak Guardrails for Large Language Models",
    "url": "http://arxiv.org/abs/2506.10597v1",
    "arxiv_id": "2506.10597v1",
    "authors": [
      "Xunguang Wang",
      "Zhenlan Ji",
      "Wenxuan Wang",
      "Zongjie Li",
      "Daoyuan Wu",
      "Shuai Wang"
    ],
    "published": "2025-06-12T11:42:40+00:00",
    "summary": "Large Language Models (LLMs) have achieved remarkable progress, but their deployment has exposed critical vulnerabilities, particularly to jailbreak attacks that circumvent safety mechanisms. Guardrails--external defense mechanisms that monitor and control LLM interaction--have emerged as a promising solution. However, the current landscape of LLM guardrails is fragmented, lacking a unified taxonomy and comprehensive evaluation framework. In this Systematization of Knowledge (SoK) paper, we present the first holistic analysis of jailbreak guardrails for LLMs. We propose a novel, multi-dimensional taxonomy that categorizes guardrails along six key dimensions, and introduce a Security-Efficiency-Utility evaluation framework to assess their practical effectiveness. Through extensive analysis and experiments, we identify the strengths and limitations of existing guardrail approaches, explore their universality across attack types, and provide insights into optimizing defense combinations. Our work offers a structured foundation for future research and development, aiming to guide the principled advancement and deployment of robust LLM guardrails. The code is available at https://github.com/xunguangwang/SoK4JailbreakGuardrails."
  },
  {
    "title": "LogiPlan: A Structured Benchmark for Logical Planning and Relational Reasoning in LLMs",
    "url": "http://arxiv.org/abs/2506.10527v1",
    "arxiv_id": "2506.10527v1",
    "authors": [
      "Yanan Cai",
      "Ahmed Salem",
      "Besmira Nushi",
      "Mark Russinovich"
    ],
    "published": "2025-06-12T09:47:02+00:00",
    "summary": "We introduce LogiPlan, a novel benchmark designed to evaluate the capabilities of large language models (LLMs) in logical planning and reasoning over complex relational structures. Logical relational reasoning is important for applications that may rely on LLMs to generate and query structured graphs of relations such as network infrastructure, knowledge bases, or business process schema. Our framework allows for dynamic variation of task complexity by controlling the number of objects, relations, and the minimum depth of relational chains, providing a fine-grained assessment of model performance across difficulty levels. LogiPlan encompasses three complementary tasks: (1) Plan Generation, where models must construct valid directed relational graphs meeting specified structural constraints; (2) Consistency Detection, testing models' ability to identify inconsistencies in relational structures; and (3) Comparison Question, evaluating models' capacity to determine the validity of queried relationships within a given graph. Additionally, we assess models' self-correction capabilities by prompting them to verify and refine their initial solutions. We evaluate state-of-the-art models including DeepSeek R1, Gemini 2.0 Pro, Gemini 2 Flash Thinking, GPT-4.5, GPT-4o, Llama 3.1 405B, O3-mini, O1, and Claude 3.7 Sonnet across these tasks, revealing significant performance gaps that correlate with model scale and architecture. Our analysis demonstrates that while recent reasoning-enhanced models show promising results on simpler instances, they struggle with more complex configurations requiring deeper logical planning."
  },
  {
    "title": "Scientists' First Exam: Probing Cognitive Abilities of MLLM via Perception, Understanding, and Reasoning",
    "url": "http://arxiv.org/abs/2506.10521v1",
    "arxiv_id": "2506.10521v1",
    "authors": [
      "Yuhao Zhou",
      "Yiheng Wang",
      "Xuming He",
      "Ruoyao Xiao",
      "Zhiwei Li",
      "Qiantai Feng",
      "Zijie Guo",
      "Yuejin Yang",
      "Hao Wu",
      "Wenxuan Huang",
      "Jiaqi Wei",
      "Dan Si",
      "Xiuqi Yao",
      "Jia Bu",
      "Haiwen Huang",
      "Tianfan Fu",
      "Shixiang Tang",
      "Ben Fei",
      "Dongzhan Zhou",
      "Fenghua Ling",
      "Yan Lu",
      "Siqi Sun",
      "Chenhui Li",
      "Guanjie Zheng",
      "Jiancheng Lv",
      "Wenlong Zhang",
      "Lei Bai"
    ],
    "published": "2025-06-12T09:29:16+00:00",
    "summary": "Scientific discoveries increasingly rely on complex multimodal reasoning based on information-intensive scientific data and domain-specific expertise. Empowered by expert-level scientific benchmarks, scientific Multimodal Large Language Models (MLLMs) hold the potential to significantly enhance this discovery process in realistic workflows. However, current scientific benchmarks mostly focus on evaluating the knowledge understanding capabilities of MLLMs, leading to an inadequate assessment of their perception and reasoning abilities. To address this gap, we present the Scientists' First Exam (SFE) benchmark, designed to evaluate the scientific cognitive capacities of MLLMs through three interconnected levels: scientific signal perception, scientific attribute understanding, scientific comparative reasoning. Specifically, SFE comprises 830 expert-verified VQA pairs across three question types, spanning 66 multimodal tasks across five high-value disciplines. Extensive experiments reveal that current state-of-the-art GPT-o3 and InternVL-3 achieve only 34.08% and 26.52% on SFE, highlighting significant room for MLLMs to improve in scientific realms. We hope the insights obtained in SFE will facilitate further developments in AI-enhanced scientific discoveries."
  },
  {
    "title": "Scientists' First Exam: Probing Cognitive Abilities of MLLM via Perception, Understanding, and Reasoning",
    "url": "http://arxiv.org/abs/2506.10521v2",
    "arxiv_id": "2506.10521v2",
    "authors": [
      "Yuhao Zhou",
      "Yiheng Wang",
      "Xuming He",
      "Ruoyao Xiao",
      "Zhiwei Li",
      "Qiantai Feng",
      "Zijie Guo",
      "Yuejin Yang",
      "Hao Wu",
      "Wenxuan Huang",
      "Jiaqi Wei",
      "Dan Si",
      "Xiuqi Yao",
      "Jia Bu",
      "Haiwen Huang",
      "Tianfan Fu",
      "Shixiang Tang",
      "Ben Fei",
      "Dongzhan Zhou",
      "Fenghua Ling",
      "Yan Lu",
      "Siqi Sun",
      "Chenhui Li",
      "Guanjie Zheng",
      "Jiancheng Lv",
      "Wenlong Zhang",
      "Lei Bai"
    ],
    "published": "2025-06-12T09:29:16+00:00",
    "summary": "Scientific discoveries increasingly rely on complex multimodal reasoning based on information-intensive scientific data and domain-specific expertise. Empowered by expert-level scientific benchmarks, scientific Multimodal Large Language Models (MLLMs) hold the potential to significantly enhance this discovery process in realistic workflows. However, current scientific benchmarks mostly focus on evaluating the knowledge understanding capabilities of MLLMs, leading to an inadequate assessment of their perception and reasoning abilities. To address this gap, we present the Scientists' First Exam (SFE) benchmark, designed to evaluate the scientific cognitive capacities of MLLMs through three interconnected levels: scientific signal perception, scientific attribute understanding, scientific comparative reasoning. Specifically, SFE comprises 830 expert-verified VQA pairs across three question types, spanning 66 multimodal tasks across five high-value disciplines. Extensive experiments reveal that current state-of-the-art GPT-o3 and InternVL-3 achieve only 34.08% and 26.52% on SFE, highlighting significant room for MLLMs to improve in scientific realms. We hope the insights obtained in SFE will facilitate further developments in AI-enhanced scientific discoveries."
  },
  {
    "title": "J-DDL: Surface Damage Detection and Localization System for Fighter Aircraft",
    "url": "http://arxiv.org/abs/2506.10505v1",
    "arxiv_id": "2506.10505v1",
    "authors": [
      "Jin Huang",
      "Mingqiang Wei",
      "Zikuan Li",
      "Hangyu Qu",
      "Wei Zhao",
      "Xinyu Bai"
    ],
    "published": "2025-06-12T09:05:35+00:00",
    "summary": "Ensuring the safety and extended operational life of fighter aircraft necessitates frequent and exhaustive inspections. While surface defect detection is feasible for human inspectors, manual methods face critical limitations in scalability, efficiency, and consistency due to the vast surface area, structural complexity, and operational demands of aircraft maintenance. We propose a smart surface damage detection and localization system for fighter aircraft, termed J-DDL. J-DDL integrates 2D images and 3D point clouds of the entire aircraft surface, captured using a combined system of laser scanners and cameras, to achieve precise damage detection and localization. Central to our system is a novel damage detection network built on the YOLO architecture, specifically optimized for identifying surface defects in 2D aircraft images. Key innovations include lightweight Fasternet blocks for efficient feature extraction, an optimized neck architecture incorporating Efficient Multiscale Attention (EMA) modules for superior feature aggregation, and the introduction of a novel loss function, Inner-CIOU, to enhance detection accuracy. After detecting damage in 2D images, the system maps the identified anomalies onto corresponding 3D point clouds, enabling accurate 3D localization of defects across the aircraft surface. Our J-DDL not only streamlines the inspection process but also ensures more comprehensive and detailed coverage of large and complex aircraft exteriors. To facilitate further advancements in this domain, we have developed the first publicly available dataset specifically focused on aircraft damage. Experimental evaluations validate the effectiveness of our framework, underscoring its potential to significantly advance automated aircraft inspection technologies."
  },
  {
    "title": "Towards more efficient quantitative safety validation of residual risk for assisted and automated driving",
    "url": "http://arxiv.org/abs/2506.10363v1",
    "arxiv_id": "2506.10363v1",
    "authors": [
      "Daniel Betschinske",
      "Malte Schrimpf",
      "Steven Peters",
      "Kamil Klonecki",
      "Jan Peter Karch",
      "Moritz Lippert"
    ],
    "published": "2025-06-12T05:41:53+00:00",
    "summary": "The safety validation of Advanced Driver Assistance Systems (ADAS) and Automated Driving Systems (ADS) increasingly demands efficient and reliable methods to quantify residual risk while adhering to international standards such as ISO 21448. Traditionally, Field Operational Testing (FOT) has been pivotal for macroscopic safety validation of automotive driving functions up to SAE automation level 2. However, state-of-the-art derivations for empirical safety demonstrations using FOT often result in impractical testing efforts, particularly at higher automation levels. Even at lower automation levels, this limitation - coupled with the substantial costs associated with FOT - motivates the exploration of approaches to enhance the efficiency of FOT-based macroscopic safety validation. Therefore, this publication systematically identifies and evaluates state-of-the-art Reduction Approaches (RAs) for FOT, including novel methods reported in the literature. Based on an analysis of ISO 21448, two models are derived: a generic model capturing the argumentation components of the standard, and a base model, exemplarily applied to Automatic Emergency Braking (AEB) systems, establishing a baseline for the real-world driving requirement for a Quantitative Safety Validation of Residual Risk (QSVRR). Subsequently, the RAs are assessed using four criteria: quantifiability, threats to validity, missing links, and black box compatibility, highlighting potential benefits, inherent limitations, and identifying key areas for further research. Our evaluation reveals that, while several approaches offer potential, none are free from missing links or other substantial shortcomings. Moreover, no identified alternative can fully replace FOT, reflecting its crucial role in the safety validation of ADAS and ADS."
  },
  {
    "title": "The Alignment Trap: Complexity Barriers",
    "url": "http://arxiv.org/abs/2506.10304v1",
    "arxiv_id": "2506.10304v1",
    "authors": [
      "Jasper Yao"
    ],
    "published": "2025-06-12T02:30:30+00:00",
    "summary": "We establish fundamental computational complexity barriers to verifying AI safety as system capabilities scale. Our main results show that for AI systems with expressiveness EXP$(m)$ above a critical threshold $\\tau$, safety verification requires exponential time and is coNP-complete. We formalize the Capability-Risk Scaling (CRS) dynamic, which demonstrates how increasing AI capability drives societal safety requirements toward perfection, creating an inescapable tension with verification complexity. Through four core theorems, we prove that (1) verification complexity grows exponentially with system expressiveness, (2) safe policies comprise at most a $2^{-2^m}$ fraction of the policy space, (3) no finite set of alignment techniques can provide universal coverage, and (4) robust safety properties form measure-zero sets for neural networks. These results characterize an \"intractability gap\" where practical safety requirements fall within the region of computational intractability. We conclude by presenting a strategic trilemma: AI development must either constrain system complexity to maintain verifiable safety, accept unverifiable risks while scaling capabilities, or develop fundamentally new safety paradigms beyond verification. Our work provides the first systematic complexity-theoretic analysis of AI alignment and establishes rigorous bounds that any safety approach must confront. A formal verification of the core theorems in Lean4 is currently in progress."
  },
  {
    "title": "Learning Safe Control via On-the-Fly Bandit Exploration",
    "url": "http://arxiv.org/abs/2506.10279v1",
    "arxiv_id": "2506.10279v1",
    "authors": [
      "Alexandre Capone",
      "Ryan Cosner",
      "Aaaron Ames",
      "Sandra Hirche"
    ],
    "published": "2025-06-12T01:40:24+00:00",
    "summary": "Control tasks with safety requirements under high levels of model uncertainty are increasingly common. Machine learning techniques are frequently used to address such tasks, typically by leveraging model error bounds to specify robust constraint-based safety filters. However, if the learned model uncertainty is very high, the corresponding filters are potentially invalid, meaning no control input satisfies the constraints imposed by the safety filter. While most works address this issue by assuming some form of safe backup controller, ours tackles it by collecting additional data on the fly using a Gaussian process bandit-type algorithm. We combine a control barrier function with a learned model to specify a robust certificate that ensures safety if feasible. Whenever infeasibility occurs, we leverage the control barrier function to guide exploration, ensuring the collected data contributes toward the closed-loop system safety. By combining a safety filter with exploration in this manner, our method provably achieves safety in a setting that allows for a zero-mean prior dynamics model, without requiring a backup controller. To the best of our knowledge, it is the first safe learning-based control method that achieves this."
  },
  {
    "title": "Optimal Voltage Control Using Online Exponential Barrier Method",
    "url": "http://arxiv.org/abs/2506.10247v1",
    "arxiv_id": "2506.10247v1",
    "authors": [
      "Peng Zhang",
      "Baosen Zhang"
    ],
    "published": "2025-06-12T00:10:16+00:00",
    "summary": "This paper address the optimal voltage control problem of distribution systems with high penetration of inverter-based renewable energy resources, under inaccurate model information. We propose the online exponential barrier method that explicitly leverages the online feedback from grids to enhance the robustness to model inaccuracy and incorporates the voltage constraints to maintain the safety requirements. We provide analytical results on the optimal barrier parameter selection and sufficient conditions for the safety guarantee of converged voltages. We also establish theoretical results on the exponential convergence rate with proper step-size. The effectiveness of the proposed framework is validated on a 56-bus radial network, where we significantly improve the robustness against model inaccuracy compared to existing methods."
  },
  {
    "title": "Data-Centric Safety and Ethical Measures for Data and AI Governance",
    "url": "http://arxiv.org/abs/2506.10217v1",
    "arxiv_id": "2506.10217v1",
    "authors": [
      "Srija Chakraborty"
    ],
    "published": "2025-06-11T22:26:50+00:00",
    "summary": "Datasets play a key role in imparting advanced capabilities to artificial intelligence (AI) foundation models that can be adapted to various downstream tasks. These downstream applications can introduce both beneficial and harmful capabilities -- resulting in dual use AI foundation models, with various technical and regulatory approaches to monitor and manage these risks. However, despite the crucial role of datasets, responsible dataset design and ensuring data-centric safety and ethical practices have received less attention. In this study, we pro-pose responsible dataset design framework that encompasses various stages in the AI and dataset lifecycle to enhance safety measures and reduce the risk of AI misuse due to low quality, unsafe and unethical data content. This framework is domain agnostic, suitable for adoption for various applications and can promote responsible practices in dataset creation, use, and sharing to facilitate red teaming, minimize risks, and increase trust in AI models."
  },
  {
    "title": "Towards Responsible AI: Advances in Safety, Fairness, and Accountability of Autonomous Systems",
    "url": "http://arxiv.org/abs/2506.10192v1",
    "arxiv_id": "2506.10192v1",
    "authors": [
      "Filip Cano"
    ],
    "published": "2025-06-11T21:30:02+00:00",
    "summary": "Ensuring responsible use of artificial intelligence (AI) has become imperative as autonomous systems increasingly influence critical societal domains. However, the concept of trustworthy AI remains broad and multi-faceted. This thesis advances knowledge in the safety, fairness, transparency, and accountability of AI systems. In safety, we extend classical deterministic shielding techniques to become resilient against delayed observations, enabling practical deployment in real-world conditions. We also implement both deterministic and probabilistic safety shields into simulated autonomous vehicles to prevent collisions with road users, validating the use of these techniques in realistic driving simulators. We introduce fairness shields, a novel post-processing approach to enforce group fairness in sequential decision-making settings over finite and periodic time horizons. By optimizing intervention costs while strictly ensuring fairness constraints, this method efficiently balances fairness with minimal interference. For transparency and accountability, we propose a formal framework for assessing intentional behaviour in probabilistic decision-making agents, introducing quantitative metrics of agency and intention quotient. We use these metrics to propose a retrospective analysis of intention, useful for determining responsibility when autonomous systems cause unintended harm. Finally, we unify these contributions through the ``reactive decision-making'' framework, providing a general formalization that consolidates previous approaches. Collectively, the advancements presented contribute practically to the realization of safer, fairer, and more accountable AI systems, laying the foundations for future research in trustworthy AI."
  },
  {
    "title": "Disclosure Audits for LLM Agents",
    "url": "http://arxiv.org/abs/2506.10171v1",
    "arxiv_id": "2506.10171v1",
    "authors": [
      "Saswat Das",
      "Jameson Sandler",
      "Ferdinando Fioretto"
    ],
    "published": "2025-06-11T20:47:37+00:00",
    "summary": "Large Language Model agents have begun to appear as personal assistants, customer service bots, and clinical aides. While these applications deliver substantial operational benefits, they also require continuous access to sensitive data, which increases the likelihood of unauthorized disclosures. This study proposes an auditing framework for conversational privacy that quantifies and audits these risks. The proposed Conversational Manipulation for Privacy Leakage (CMPL) framework, is an iterative probing strategy designed to stress-test agents that enforce strict privacy directives. Rather than focusing solely on a single disclosure event, CMPL simulates realistic multi-turn interactions to systematically uncover latent vulnerabilities. Our evaluation on diverse domains, data modalities, and safety configurations demonstrate the auditing framework's ability to reveal privacy risks that are not deterred by existing single-turn defenses. In addition to introducing CMPL as a diagnostic tool, the paper delivers (1) an auditing procedure grounded in quantifiable risk metrics and (2) an open benchmark for evaluation of conversational privacy across agent implementations."
  },
  {
    "title": "Measuring Corporate Human Capital Disclosures: Lexicon, Data, Code, and Research Opportunities",
    "url": "http://arxiv.org/abs/2506.10155v1",
    "arxiv_id": "2506.10155v1",
    "authors": [
      "Elizabeth Demers",
      "Victor Xiaoqi Wang",
      "Kean Wu"
    ],
    "published": "2025-06-11T20:18:37+00:00",
    "summary": "Human capital (HC) is increasingly important to corporate value creation. Unlike other assets, however, HC is not currently subject to well-defined measurement or disclosure rules. We use a machine learning algorithm (word2vec) trained on a confirmed set of HC disclosures to develop a comprehensive list of HC-related keywords classified into five subcategories (DEI; health and safety; labor relations and culture; compensation and benefits; and demographics and other) that capture the multidimensional nature of HC management. We share our lexicon, corporate HC disclosures, and the Python code used to develop the lexicon, and we provide detailed examples of using our data and code, including for fine-tuning a BERT model. Researchers can use our HC lexicon (or modify the code to capture another construct of interest) with their samples of corporate communications to address pertinent HC questions. We close with a discussion of future research opportunities related to HC management and disclosure."
  },
  {
    "title": "Estimating the Joint Probability of Scenario Parameters with Gaussian Mixture Copula Models",
    "url": "http://arxiv.org/abs/2506.10098v1",
    "arxiv_id": "2506.10098v1",
    "authors": [
      "Christian Reichenb\u00e4cher",
      "Philipp Rank",
      "Jochen Hipp",
      "Oliver Bringmann"
    ],
    "published": "2025-06-11T18:30:20+00:00",
    "summary": "This paper presents the first application of Gaussian Mixture Copula Models to the statistical modeling of driving scenarios for the safety validation of automated driving systems. Knowledge of the joint probability distribution of scenario parameters is essential for scenario-based safety assessment, where risk quantification depends on the likelihood of concrete parameter combinations. Gaussian Mixture Copula Models bring together the multimodal expressivity of Gaussian Mixture Models and the flexibility of copulas, enabling separate modeling of marginal distributions and dependencies. We benchmark Gaussian Mixture Copula Models against previously proposed approaches - Gaussian Mixture Models and Gaussian Copula Models - using real-world driving data drawn from scenarios defined in United Nations Regulation No. 157. Our evaluation across 18 million scenario instances demonstrates that Gaussian Mixture Copula Models provide a better fit to the data in terms of both likelihood and Sinkhorn distance. These results suggest that Gaussian Mixture Copula Models are a compelling foundation for future scenario-based validation frameworks."
  },
  {
    "title": "Optimizing Latent Dimension Allocation in Hierarchical VAEs: Balancing Attenuation and Information Retention for OOD Detection",
    "url": "http://arxiv.org/abs/2506.10089v1",
    "arxiv_id": "2506.10089v1",
    "authors": [
      "Dane Williamson",
      "Yangfeng Ji",
      "Matthew Dwyer"
    ],
    "published": "2025-06-11T18:16:19+00:00",
    "summary": "Out-of-distribution (OOD) detection is a critical task in machine learning, particularly for safety-critical applications where unexpected inputs must be reliably flagged. While hierarchical variational autoencoders (HVAEs) offer improved representational capacity over traditional VAEs, their performance is highly sensitive to how latent dimensions are distributed across layers. Existing approaches often allocate latent capacity arbitrarily, leading to ineffective representations or posterior collapse. In this work, we introduce a theoretically grounded framework for optimizing latent dimension allocation in HVAEs, drawing on principles from information theory to formalize the trade-off between information loss and representational attenuation. We prove the existence of an optimal allocation ratio $r^{\\ast}$ under a fixed latent budget, and empirically show that tuning this ratio consistently improves OOD detection performance across datasets and architectures. Our approach outperforms baseline HVAE configurations and provides practical guidance for principled latent structure design, leading to more robust OOD detection with deep generative models."
  },
  {
    "title": "From Judgment to Interference: Early Stopping LLM Harmful Outputs via Streaming Content Monitoring",
    "url": "http://arxiv.org/abs/2506.09996v1",
    "arxiv_id": "2506.09996v1",
    "authors": [
      "Yang Li",
      "Qiang Sheng",
      "Yehan Yang",
      "Xueyao Zhang",
      "Juan Cao"
    ],
    "published": "2025-06-11T17:59:58+00:00",
    "summary": "Though safety alignment has been applied to most large language models (LLMs), LLM service providers generally deploy a subsequent moderation as the external safety guardrail in real-world products. Existing moderators mainly practice a conventional full detection, which determines the harmfulness based on the complete LLM output, causing high service latency. Recent works pay more attention to partial detection where moderators oversee the generation midway and early stop the output if harmfulness is detected, but they directly apply moderators trained with the full detection paradigm to incomplete outputs, introducing a training-inference gap that lowers the performance. In this paper, we explore how to form a data-and-model solution that natively supports partial detection. For the data, we construct FineHarm, a dataset consisting of 29K prompt-response pairs with fine-grained annotations to provide reasonable supervision for token-level training. Then, we propose the streaming content monitor, which is trained with dual supervision of response- and token-level labels and can follow the output stream of LLM to make a timely judgment of harmfulness. Experiments show that SCM gains 0.95+ in macro F1 score that is comparable to full detection, by only seeing the first 18% of tokens in responses on average. Moreover, the SCM can serve as a pseudo-harmfulness annotator for improving safety alignment and lead to a higher harmlessness score than DPO."
  },
  {
    "title": "Assessing a Safety Case: Bottom-up Guidance for Claims and Evidence Evaluation",
    "url": "http://arxiv.org/abs/2506.09929v1",
    "arxiv_id": "2506.09929v1",
    "authors": [
      "Scott Schnelle",
      "Francesca Favaro",
      "Laura Fraade-Blanar",
      "David Wichner",
      "Holland Broce",
      "Justin Miranda"
    ],
    "published": "2025-06-11T16:52:03+00:00",
    "summary": "As Automated Driving Systems (ADS) technology advances, ensuring safety and public trust requires robust assurance frameworks, with safety cases emerging as a critical tool toward such a goal. This paper explores an approach to assess how a safety case is supported by its claims and evidence, toward establishing credibility for the overall case. Starting from a description of the building blocks of a safety case (claims, evidence, and optional format-dependent entries), this paper delves into the assessment of support of each claim through the provided evidence. Two domains of assessment are outlined for each claim: procedural support (formalizing process specification) and implementation support (demonstrating process application). Additionally, an assessment of evidence status is also undertaken, independently from the claims support. Scoring strategies and evaluation guidelines are provided, including detailed scoring tables for claim support and evidence status assessment. The paper further discusses governance, continual improvement, and timing considerations for safety case assessments. Reporting of results and findings is contextualized within its primary use for internal decision-making on continual improvement efforts. The presented approach builds on state of the art auditing practices, but specifically tackles the question of judging the credibility of a safety case. While not conclusive on its own, it provides a starting point toward a comprehensive \"Case Credibility Assessment\" (CCA), starting from the evaluation of the support for each claim (individually and in aggregate), as well as every piece of evidence provided. By delving into the technical intricacies of ADS safety cases, this work contributes to the ongoing discourse on safety assurance and aims to facilitate the responsible integration of ADS technology into society."
  },
  {
    "title": "OctoNav: Towards Generalist Embodied Navigation",
    "url": "http://arxiv.org/abs/2506.09839v1",
    "arxiv_id": "2506.09839v1",
    "authors": [
      "Chen Gao",
      "Liankai Jin",
      "Xingyu Peng",
      "Jiazhao Zhang",
      "Yue Deng",
      "Annan Li",
      "He Wang",
      "Si Liu"
    ],
    "published": "2025-06-11T15:15:17+00:00",
    "summary": "Embodied navigation stands as a foundation pillar within the broader pursuit of embodied AI. However, previous navigation research is divided into different tasks/capabilities, e.g., ObjNav, ImgNav and VLN, where they differ in task objectives and modalities, making datasets and methods are designed individually. In this work, we take steps toward generalist navigation agents, which can follow free-form instructions that include arbitrary compounds of multi-modal and multi-capability. To achieve this, we propose a large-scale benchmark and corresponding method, termed OctoNav-Bench and OctoNav-R1. Specifically, OctoNav-Bench features continuous environments and is constructed via a designed annotation pipeline. We thoroughly craft instruction-trajectory pairs, where instructions are diverse in free-form with arbitrary modality and capability. Also, we construct a Think-Before-Action (TBA-CoT) dataset within OctoNav-Bench to provide the thinking process behind actions. For OctoNav-R1, we build it upon MLLMs and adapt it to a VLA-type model, which can produce low-level actions solely based on 2D visual observations. Moreover, we design a Hybrid Training Paradigm (HTP) that consists of three stages, i.e., Action-/TBA-SFT, Nav-GPRO, and Online RL stages. Each stage contains specifically designed learning policies and rewards. Importantly, for TBA-SFT and Nav-GRPO designs, we are inspired by the OpenAI-o1 and DeepSeek-R1, which show impressive reasoning ability via thinking-before-answer. Thus, we aim to investigate how to achieve thinking-before-action in the embodied navigation field, to improve model's reasoning ability toward generalists. Specifically, we propose TBA-SFT to utilize the TBA-CoT dataset to fine-tune the model as a cold-start phrase and then leverage Nav-GPRO to improve its thinking ability. Finally, OctoNav-R1 shows superior performance compared with previous methods."
  },
  {
    "title": "Superstudent intelligence in thermodynamics",
    "url": "http://arxiv.org/abs/2506.09822v1",
    "arxiv_id": "2506.09822v1",
    "authors": [
      "Rebecca Loubet",
      "Pascal Zittlau",
      "Marco Hoffmann",
      "Luisa Vollmer",
      "Sophie Fellenz",
      "Heike Leitte",
      "Fabian Jirasek",
      "Johannes Lenhard",
      "Hans Hasse"
    ],
    "published": "2025-06-11T15:01:41+00:00",
    "summary": "In this short note, we report and analyze a striking event: OpenAI's large language model o3 has outwitted all students in a university exam on thermodynamics. The thermodynamics exam is a difficult hurdle for most students, where they must show that they have mastered the fundamentals of this important topic. Consequently, the failure rates are very high, A-grades are rare - and they are considered proof of the students' exceptional intellectual abilities. This is because pattern learning does not help in the exam. The problems can only be solved by knowledgeably and creatively combining principles of thermodynamics. We have given our latest thermodynamics exam not only to the students but also to OpenAI's most powerful reasoning model, o3, and have assessed the answers of o3 exactly the same way as those of the students. In zero-shot mode, the model o3 solved all problems correctly, better than all students who took the exam; its overall score was in the range of the best scores we have seen in more than 10,000 similar exams since 1985. This is a turning point: machines now excel in complex tasks, usually taken as proof of human intellectual capabilities. We discuss the consequences this has for the work of engineers and the education of future engineers."
  },
  {
    "title": "CoRT: Code-integrated Reasoning within Thinking",
    "url": "http://arxiv.org/abs/2506.09820v1",
    "arxiv_id": "2506.09820v1",
    "authors": [
      "Chengpeng Li",
      "Zhengyang Tang",
      "Ziniu Li",
      "Mingfeng Xue",
      "Keqin Bao",
      "Tian Ding",
      "Ruoyu Sun",
      "Benyou Wang",
      "Xiang Wang",
      "Junyang Lin",
      "Dayiheng Liu"
    ],
    "published": "2025-06-11T14:59:02+00:00",
    "summary": "Large Reasoning Models (LRMs) like o1 and DeepSeek-R1 have shown remarkable progress in natural language reasoning with long chain-of-thought (CoT), yet they remain inefficient or inaccurate when handling complex mathematical operations. Addressing these limitations through computational tools (e.g., computation libraries and symbolic solvers) is promising, but it introduces a technical challenge: Code Interpreter (CI) brings external knowledge beyond the model's internal text representations, thus the direct combination is not efficient. This paper introduces CoRT, a post-training framework for teaching LRMs to leverage CI effectively and efficiently. As a first step, we address the data scarcity issue by synthesizing code-integrated reasoning data through Hint-Engineering, which strategically inserts different hints at appropriate positions to optimize LRM-CI interaction. We manually create 30 high-quality samples, upon which we post-train models ranging from 1.5B to 32B parameters, with supervised fine-tuning, rejection fine-tuning and reinforcement learning. Our experimental results demonstrate that Hint-Engineering models achieve 4\\% and 8\\% absolute improvements on DeepSeek-R1-Distill-Qwen-32B and DeepSeek-R1-Distill-Qwen-1.5B respectively, across five challenging mathematical reasoning datasets. Furthermore, Hint-Engineering models use about 30\\% fewer tokens for the 32B model and 50\\% fewer tokens for the 1.5B model compared with the natural language models. The models and code are available at https://github.com/ChengpengLi1003/CoRT."
  },
  {
    "title": "CoRT: Code-integrated Reasoning within Thinking",
    "url": "http://arxiv.org/abs/2506.09820v2",
    "arxiv_id": "2506.09820v2",
    "authors": [
      "Chengpeng Li",
      "Zhengyang Tang",
      "Ziniu Li",
      "Mingfeng Xue",
      "Keqin Bao",
      "Tian Ding",
      "Ruoyu Sun",
      "Benyou Wang",
      "Xiang Wang",
      "Junyang Lin",
      "Dayiheng Liu"
    ],
    "published": "2025-06-11T14:59:02+00:00",
    "summary": "Large Reasoning Models (LRMs) like o1 and DeepSeek-R1 have shown remarkable progress in natural language reasoning with long chain-of-thought (CoT), yet they remain inefficient or inaccurate when handling complex mathematical operations. Addressing these limitations through computational tools (e.g., computation libraries and symbolic solvers) is promising, but it introduces a technical challenge: Code Interpreter (CI) brings external knowledge beyond the model's internal text representations, thus the direct combination is not efficient. This paper introduces CoRT, a post-training framework for teaching LRMs to leverage CI effectively and efficiently. As a first step, we address the data scarcity issue by synthesizing code-integrated reasoning data through Hint-Engineering, which strategically inserts different hints at appropriate positions to optimize LRM-CI interaction. We manually create 30 high-quality samples, upon which we post-train models ranging from 1.5B to 32B parameters, with supervised fine-tuning, rejection fine-tuning and reinforcement learning. Our experimental results demonstrate that Hint-Engineering models achieve 4\\% and 8\\% absolute improvements on DeepSeek-R1-Distill-Qwen-32B and DeepSeek-R1-Distill-Qwen-1.5B respectively, across five challenging mathematical reasoning datasets. Furthermore, Hint-Engineering models use about 30\\% fewer tokens for the 32B model and 50\\% fewer tokens for the 1.5B model compared with the natural language models. The models and code are available at https://github.com/ChengpengLi1003/CoRT."
  },
  {
    "title": "Reinforced Refinement with Self-Aware Expansion for End-to-End Autonomous Driving",
    "url": "http://arxiv.org/abs/2506.09800v1",
    "arxiv_id": "2506.09800v1",
    "authors": [
      "Haochen Liu",
      "Tianyu Li",
      "Haohan Yang",
      "Li Chen",
      "Caojun Wang",
      "Ke Guo",
      "Haochen Tian",
      "Hongchen Li",
      "Hongyang Li",
      "Chen Lv"
    ],
    "published": "2025-06-11T14:42:11+00:00",
    "summary": "End-to-end autonomous driving has emerged as a promising paradigm for directly mapping sensor inputs to planning maneuvers using learning-based modular integrations. However, existing imitation learning (IL)-based models suffer from generalization to hard cases, and a lack of corrective feedback loop under post-deployment. While reinforcement learning (RL) offers a potential solution to tackle hard cases with optimality, it is often hindered by overfitting to specific driving cases, resulting in catastrophic forgetting of generalizable knowledge and sample inefficiency. To overcome these challenges, we propose Reinforced Refinement with Self-aware Expansion (R2SE), a novel learning pipeline that constantly refines hard domain while keeping generalizable driving policy for model-agnostic end-to-end driving systems. Through reinforcement fine-tuning and policy expansion that facilitates continuous improvement, R2SE features three key components: 1) Generalist Pretraining with hard-case allocation trains a generalist imitation learning (IL) driving system while dynamically identifying failure-prone cases for targeted refinement; 2) Residual Reinforced Specialist Fine-tuning optimizes residual corrections using reinforcement learning (RL) to improve performance in hard case domain while preserving global driving knowledge; 3) Self-aware Adapter Expansion dynamically integrates specialist policies back into the generalist model, enhancing continuous performance improvement. Experimental results in closed-loop simulation and real-world datasets demonstrate improvements in generalization, safety, and long-horizon policy robustness over state-of-the-art E2E systems, highlighting the effectiveness of reinforce refinement for scalable autonomous driving."
  },
  {
    "title": "Translating a VDM Model of a Medical Device into Kapture",
    "url": "http://arxiv.org/abs/2506.09636v1",
    "arxiv_id": "2506.09636v1",
    "authors": [
      "Joe Hare",
      "Leo Freitas",
      "Ken Pierce"
    ],
    "published": "2025-06-11T11:50:50+00:00",
    "summary": "As the complexity of safety-critical medical devices increases, so does the need for clear, verifiable, software requirements. This paper explores the use of Kapture, a formal modelling tool developed by D-RisQ, to translate an existing formal VDM model of a medical implant for treating focal epilepsy called CANDO. The work was undertaken without prior experience in formal methods. The paper assess Kapture's usability, the challenges of formal modelling, and the effectiveness of the translated model. The result is a model in Kapture which covers over 90% of the original VDM model, and produces matching traces of results. While several issues were encountered during design and implementation, mainly due to the initial learning curve, this paper demonstrates that complex systems can be effectively modelled in Kapture by inexperienced users and highlights some difficulties in translating VDM specifications to Kapture."
  },
  {
    "title": "Effective Red-Teaming of Policy-Adherent Agents",
    "url": "http://arxiv.org/abs/2506.09600v1",
    "arxiv_id": "2506.09600v1",
    "authors": [
      "Itay Nakash",
      "George Kour",
      "Koren Lazar",
      "Matan Vetzler",
      "Guy Uziel",
      "Ateret Anaby-Tavor"
    ],
    "published": "2025-06-11T10:59:47+00:00",
    "summary": "Task-oriented LLM-based agents are increasingly used in domains with strict policies, such as refund eligibility or cancellation rules. The challenge lies in ensuring that the agent consistently adheres to these rules and policies, appropriately refusing any request that would violate them, while still maintaining a helpful and natural interaction. This calls for the development of tailored design and evaluation methodologies to ensure agent resilience against malicious user behavior. We propose a novel threat model that focuses on adversarial users aiming to exploit policy-adherent agents for personal benefit. To address this, we present CRAFT, a multi-agent red-teaming system that leverages policy-aware persuasive strategies to undermine a policy-adherent agent in a customer-service scenario, outperforming conventional jailbreak methods such as DAN prompts, emotional manipulation, and coercive. Building upon the existing tau-bench benchmark, we introduce tau-break, a complementary benchmark designed to rigorously assess the agent's robustness against manipulative user behavior. Finally, we evaluate several straightforward yet effective defense strategies. While these measures provide some protection, they fall short, highlighting the need for stronger, research-driven safeguards to protect policy-adherent agents from adversarial attacks"
  },
  {
    "title": "Understanding the Performance and Power of LLM Inferencing on Edge Accelerators",
    "url": "http://arxiv.org/abs/2506.09554v1",
    "arxiv_id": "2506.09554v1",
    "authors": [
      "Mayank Arya",
      "Yogesh Simmhan"
    ],
    "published": "2025-06-11T09:36:32+00:00",
    "summary": "Large Language Models (LLMs) have demonstrated exceptional benefits to a wide range of domains, for tasks as diverse as code generation and robot navigation. While LLMs are usually served from cloud data centers, mission-critical and privacy-sensitive applications may require local hosting of open LLM models. Given the large GPU memory footprint needed for LLMs, edge accelerators such as Nvidia Jetson Orin AGX with 64GB of shared GPU-CPU RAM are a compelling choice. However, the feasibility and performance of LLM inference on edge accelerators is under-explored. This study presents a detailed evaluation of LLM inference on the NVIDIA Jetson Orin AGX, on four SOTA models ranging from 2.7B to 32.8B parameters, such as Meta Llama3.1, Microsoft-Phi2, Deepseek-R1-Qwen.We investigate the impact of varying batch sizes, sequence lengths, and quantization levels on latency, throughput, and perplexity, and also explore various custom power modes on the Orin AGX to perform power and energy consumption analysis. Our findings offer interesting insights on the trade-offs between efficiency, inference speed and resource use, e.g., increasing the sequence length causes a decrease in token throughput and quantization causes smaller LLMs to be slower. These results can help optimize LLM serving on edge accelerators for practical applications."
  },
  {
    "title": "Understanding the Performance and Power of LLM Inferencing on Edge Accelerators",
    "url": "http://arxiv.org/abs/2506.09554v2",
    "arxiv_id": "2506.09554v2",
    "authors": [
      "Mayank Arya",
      "Yogesh Simmhan"
    ],
    "published": "2025-06-11T09:36:32+00:00",
    "summary": "Large Language Models (LLMs) have demonstrated exceptional benefits to a wide range of domains, for tasks as diverse as code generation and robot navigation. While LLMs are usually served from cloud data centers, mission-critical and privacy-sensitive applications may require local hosting of open LLM models. Given the large GPU memory footprint needed for LLMs, edge accelerators such as Nvidia Jetson Orin AGX with 64GB of shared GPU-CPU RAM are a compelling choice. However, the feasibility and performance of LLM inference on edge accelerators is under-explored. This study presents a detailed evaluation of LLM inference on the NVIDIA Jetson Orin AGX, on four SOTA models ranging from 2.7B to 32.8B parameters, such as Meta Llama3.1, Microsoft-Phi2, Deepseek-R1-Qwen. We investigate the impact of varying batch sizes, sequence lengths, and quantization levels on latency, throughput, and perplexity, and also explore various custom power modes on the Orin AGX to perform power and energy consumption analysis. Our findings offer interesting insights on the trade-offs between efficiency, inference speed and resource use, e.g., increasing the sequence length causes a decrease in token throughput and quantization causes smaller LLMs to be slower. These results can help optimize LLM serving on edge accelerators for practical applications."
  },
  {
    "title": "Enhancing Human-Robot Collaboration: A Sim2Real Domain Adaptation Algorithm for Point Cloud Segmentation in Industrial Environments",
    "url": "http://arxiv.org/abs/2506.09552v1",
    "arxiv_id": "2506.09552v1",
    "authors": [
      "Fatemeh Mohammadi Amin",
      "Darwin G. Caldwell",
      "Hans Wernher van de Venn"
    ],
    "published": "2025-06-11T09:36:07+00:00",
    "summary": "The robust interpretation of 3D environments is crucial for human-robot collaboration (HRC) applications, where safety and operational efficiency are paramount. Semantic segmentation plays a key role in this context by enabling a precise and detailed understanding of the environment. Considering the intense data hunger for real-world industrial annotated data essential for effective semantic segmentation, this paper introduces a pioneering approach in the Sim2Real domain adaptation for semantic segmentation of 3D point cloud data, specifically tailored for HRC. Our focus is on developing a network that robustly transitions from simulated environments to real-world applications, thereby enhancing its practical utility and impact on a safe HRC.   In this work, we propose a dual-stream network architecture (FUSION) combining Dynamic Graph Convolutional Neural Networks (DGCNN) and Convolutional Neural Networks (CNN) augmented with residual layers as a Sim2Real domain adaptation algorithm for an industrial environment. The proposed model was evaluated on real-world HRC setups and simulation industrial point clouds, it showed increased state-of-the-art performance, achieving a segmentation accuracy of 97.76%, and superior robustness compared to existing methods."
  },
  {
    "title": "Automated Synthesis of Formally Verified Multi-Abstraction Function Summaries",
    "url": "http://arxiv.org/abs/2506.09550v1",
    "arxiv_id": "2506.09550v1",
    "authors": [
      "Fanpeng Yang",
      "Xu Ma",
      "Shuling Wang",
      "Xiong Xu",
      "Qinxiang Cao",
      "Naijun Zhan",
      "Xiaofeng Li",
      "Bin Gu"
    ],
    "published": "2025-06-11T09:33:02+00:00",
    "summary": "Function summaries, which characterize the behavior of code segments (typically functions) through preconditions and postconditions, are essential for understanding, reusing, and verifying software, particularly in safety-critical domains like aerospace embedded systems. However, these mission-critical legacy code serving as a valuable reused asset often lacks formal specifications. It is challenging to automatically generate function summaries for C programs, due to the existence of complex features such as loops, nested function calls, pointer aliasing, and so on. Moreover, function summaries should support multiple abstraction levels to meet diverse requirements, e.g. precise summaries capturing full functionality for formal verification and intuitive summaries for human understanding.   To address these challenges, we first propose a novel framework that combines symbolic execution, large language models (LLMs), and formal verification to generate Relatively Strongest Postconditions (RSPs) and build function summaries that fully capture program behavior. Our approach leverages VST-A's symbolic execution to precisely track program execution paths and state transitions, employs LLMs to infer loop invariants based on predefined templates, and uses Frama-C to guarantee soundness of generated summaries in an iterative refinement loop. Furthermore, from generated RSPs, we automatically synthesize strongest non-redundant postconditions expressed within given domain specific language. We compare our approach with existing work through extensive experiments."
  },
  {
    "title": "GenBreak: Red Teaming Text-to-Image Generators Using Large Language Models",
    "url": "http://arxiv.org/abs/2506.10047v1",
    "arxiv_id": "2506.10047v1",
    "authors": [
      "Zilong Wang",
      "Xiang Zheng",
      "Xiaosen Wang",
      "Bo Wang",
      "Xingjun Ma",
      "Yu-Gang Jiang"
    ],
    "published": "2025-06-11T09:09:12+00:00",
    "summary": "Text-to-image (T2I) models such as Stable Diffusion have advanced rapidly and are now widely used in content creation. However, these models can be misused to generate harmful content, including nudity or violence, posing significant safety risks. While most platforms employ content moderation systems, underlying vulnerabilities can still be exploited by determined adversaries. Recent research on red-teaming and adversarial attacks against T2I models has notable limitations: some studies successfully generate highly toxic images but use adversarial prompts that are easily detected and blocked by safety filters, while others focus on bypassing safety mechanisms but fail to produce genuinely harmful outputs, neglecting the discovery of truly high-risk prompts. Consequently, there remains a lack of reliable tools for evaluating the safety of defended T2I models. To address this gap, we propose GenBreak, a framework that fine-tunes a red-team large language model (LLM) to systematically explore underlying vulnerabilities in T2I generators. Our approach combines supervised fine-tuning on curated datasets with reinforcement learning via interaction with a surrogate T2I model. By integrating multiple reward signals, we guide the LLM to craft adversarial prompts that enhance both evasion capability and image toxicity, while maintaining semantic coherence and diversity. These prompts demonstrate strong effectiveness in black-box attacks against commercial T2I generators, revealing practical and concerning safety weaknesses."
  },
  {
    "title": "Give Me FP32 or Give Me Death? Challenges and Solutions for Reproducible Reasoning",
    "url": "http://arxiv.org/abs/2506.09501v1",
    "arxiv_id": "2506.09501v1",
    "authors": [
      "Jiayi Yuan",
      "Hao Li",
      "Xinheng Ding",
      "Wenya Xie",
      "Yu-Jhe Li",
      "Wentian Zhao",
      "Kun Wan",
      "Jing Shi",
      "Xia Hu",
      "Zirui Liu"
    ],
    "published": "2025-06-11T08:23:53+00:00",
    "summary": "Large Language Models (LLMs) are now integral across various domains and have demonstrated impressive performance. Progress, however, rests on the premise that benchmark scores are both accurate and reproducible. We demonstrate that the reproducibility of LLM performance is fragile: changing system configuration such as evaluation batch size, GPU count, and GPU version can introduce significant difference in the generated responses. This issue is especially pronounced in reasoning models, where minor rounding differences in early tokens can cascade into divergent chains of thought, ultimately affecting accuracy. For instance, under bfloat16 precision with greedy decoding, a reasoning model like DeepSeek-R1-Distill-Qwen-7B can exhibit up to 9% variation in accuracy and 9,000 tokens difference in response length due to differences in GPU count, type, and evaluation batch size. We trace the root cause of this variability to the non-associative nature of floating-point arithmetic under limited numerical precision. This work presents the first systematic investigation into how numerical precision affects reproducibility in LLM inference. Through carefully controlled experiments across various hardware, software, and precision settings, we quantify when and how model outputs diverge. Our analysis reveals that floating-point precision -- while critical for reproducibility -- is often neglected in evaluation practices. Inspired by this, we develop a lightweight inference pipeline, dubbed LayerCast, that stores weights in 16-bit precision but performs all computations in FP32, balancing memory efficiency with numerical stability. Code is available at https://github.com/nanomaoli/llm_reproducibility."
  },
  {
    "title": "Adv-BMT: Bidirectional Motion Transformer for Safety-Critical Traffic Scenario Generation",
    "url": "http://arxiv.org/abs/2506.09485v1",
    "arxiv_id": "2506.09485v1",
    "authors": [
      "Yuxin Liu",
      "Zhenghao Peng",
      "Xuanhao Cui",
      "Bolei Zhou"
    ],
    "published": "2025-06-11T07:54:50+00:00",
    "summary": "Scenario-based testing is essential for validating the performance of autonomous driving (AD) systems. However, such testing is limited by the scarcity of long-tailed, safety-critical scenarios in existing datasets collected in the real world. To tackle the data issue, we propose the Adv-BMT framework, which augments real-world scenarios with diverse and realistic adversarial interactions. The core component of Adv-BMT is a bidirectional motion transformer (BMT) model to perform inverse traffic motion predictions, which takes agent information in the last time step of the scenario as input, and reconstruct the traffic in the inverse of chronological order until the initial time step. The Adv-BMT framework is a two-staged pipeline: it first conducts adversarial initializations and then inverse motion predictions. Different from previous work, we do not need any collision data for pretraining, and are able to generate realistic and diverse collision interactions. Our experimental results validate the quality of generated collision scenarios by Adv-BMT: training in our augmented dataset would reduce episode collision rates by 20\\% compared to previous work."
  },
  {
    "title": "Optimization and Control Technologies for Renewable-Dominated Hydrogen-Blended Integrated Gas-Electricity System: A Review",
    "url": "http://arxiv.org/abs/2506.09447v1",
    "arxiv_id": "2506.09447v1",
    "authors": [
      "Wenxin Liu",
      "Jiakun Fang",
      "Shichang Cui",
      "Iskandar Abdullaev",
      "Suyang Zhou",
      "Xiaomeng Ai",
      "Jinyu Wen"
    ],
    "published": "2025-06-11T06:53:26+00:00",
    "summary": "The growing coupling among electricity, gas, and hydrogen systems is driven by green hydrogen blending into existing natural gas pipelines, paving the way toward a renewable-dominated energy future. However, the integration poses significant challenges, particularly ensuring efficient and safe operation under varying hydrogen penetration and infrastructure adaptability. This paper reviews progress in optimization and control technologies for hydrogen-blended integrated gas-electricity system. First, key technologies and international demonstration projects are introduced to provide an overview of current developments. Besides, advances in gas-electricity system integration, including modeling, scheduling, planning and market design, are reviewed respectively. Then, the potential for cross-system fault propagation is highlighted, and practical methods for safety analysis and control are proposed. Finally, several possible research directions are introduced, aiming to ensure efficient renewable integration and reliable operation."
  },
  {
    "title": "Securing Open RAN: A Survey of Cryptographic Challenges and Emerging Solutions for 5G",
    "url": "http://arxiv.org/abs/2506.09418v1",
    "arxiv_id": "2506.09418v1",
    "authors": [
      "Ryan Barker",
      "Fatemeh Afghah"
    ],
    "published": "2025-06-11T06:04:40+00:00",
    "summary": "The advent of Open Radio Access Networks (O-RAN) introduces modularity and flexibility into 5G deployments but also surfaces novel security challenges across disaggregated interfaces. This literature review synthesizes recent research across thirteen academic and industry sources, examining vulnerabilities such as cipher bidding-down attacks, partial encryption exposure on control/user planes, and performance trade-offs in securing O-RAN interfaces like E2 and O1. The paper surveys key cryptographic tools -- SNOW-V, AES-256, and ZUC-256 -- evaluating their throughput, side-channel resilience, and adaptability to heterogeneous slices (eMBB, URLLC, mMTC). Emphasis is placed on emerging testbeds and AI-driven controllers that facilitate dynamic orchestration, anomaly detection, and secure configuration. We conclude by outlining future research directions, including hardware offloading, cross-layer cipher adaptation, and alignment with 3GPP TS 33.501 and O-RAN Alliance security mandates, all of which point toward the need for integrated, zero-trust architectures in 6G."
  },
  {
    "title": "Token Constraint Decoding Improves Robustness on Question Answering for Large Language Models",
    "url": "http://arxiv.org/abs/2506.09408v1",
    "arxiv_id": "2506.09408v1",
    "authors": [
      "Jui-Ming Yao",
      "Hao-Yuan Chen",
      "Zi-Xian Tang",
      "Bing-Jia Tan",
      "Sheng-Wei Peng",
      "Bing-Cheng Xie",
      "Shun-Feng Su"
    ],
    "published": "2025-06-11T05:33:56+00:00",
    "summary": "Large Language Models (LLMs) have demonstrated impressive performance on multiple-choice question answering (MCQA) benchmarks, yet they remain highly vulnerable to minor input perturbations. In this paper, we introduce and evaluate Token Constraint Decoding (TCD). This simple yet effective inference-time algorithm enforces alignment between token-level predictions to enhance robustness in noisy settings. Through extensive experiments on CommonsenseQA, MMLU, and MMLU-Pro, we show that TCD, especially when paired with prompt engineering (PE) fixes, significantly restores performance degraded by input noise, yielding up to +39\\% absolute gains for weaker models like Gemma3 1B. Penalty sweep analyses further reveal that TCD implicitly regularizes overconfident outputs, with different models requiring distinct penalty schedules to maximize resilience. Our findings establish TCD as a practical, model-agnostic approach for improving reasoning stability under real-world imperfections and pave the way for more reliable deployment of LLMs in safety-critical or user-facing applications."
  },
  {
    "title": "SAGE: Exploring the Boundaries of Unsafe Concept Domain with Semantic-Augment Erasing",
    "url": "http://arxiv.org/abs/2506.09363v1",
    "arxiv_id": "2506.09363v1",
    "authors": [
      "Hongguang Zhu",
      "Yunchao Wei",
      "Mengyu Wang",
      "Siyu Jiao",
      "Yan Fang",
      "Jiannan Huang",
      "Yao Zhao"
    ],
    "published": "2025-06-11T03:21:24+00:00",
    "summary": "Diffusion models (DMs) have achieved significant progress in text-to-image generation. However, the inevitable inclusion of sensitive information during pre-training poses safety risks, such as unsafe content generation and copyright infringement. Concept erasing finetunes weights to unlearn undesirable concepts, and has emerged as a promising solution. However, existing methods treat unsafe concept as a fixed word and repeatedly erase it, trapping DMs in ``word concept abyss'', which prevents generalized concept-related erasing. To escape this abyss, we introduce semantic-augment erasing which transforms concept word erasure into concept domain erasure by the cyclic self-check and self-erasure. It efficiently explores and unlearns the boundary representation of concept domain through semantic spatial relationships between original and training DMs, without requiring additional preprocessed data. Meanwhile, to mitigate the retention degradation of irrelevant concepts while erasing unsafe concepts, we further propose the global-local collaborative retention mechanism that combines global semantic relationship alignment with local predicted noise preservation, effectively expanding the retentive receptive field for irrelevant concepts. We name our method SAGE, and extensive experiments demonstrate the comprehensive superiority of SAGE compared with other methods in the safe generation of DMs. The code and weights will be open-sourced at https://github.com/KevinLight831/SAGE."
  },
  {
    "title": "DAVSP: Safety Alignment for Large Vision-Language Models via Deep Aligned Visual Safety Prompt",
    "url": "http://arxiv.org/abs/2506.09353v1",
    "arxiv_id": "2506.09353v1",
    "authors": [
      "Yitong Zhang",
      "Jia Li",
      "Liyi Cai",
      "Ge Li"
    ],
    "published": "2025-06-11T03:06:41+00:00",
    "summary": "Large Vision-Language Models (LVLMs) have achieved impressive progress across various applications but remain vulnerable to malicious queries that exploit the visual modality. Existing alignment approaches typically fail to resist malicious queries while preserving utility on benign ones effectively. To address these challenges, we propose Deep Aligned Visual Safety Prompt (DAVSP), which is built upon two key innovations. First, we introduce the Visual Safety Prompt, which appends a trainable padding region around the input image. It preserves visual features and expands the optimization space. Second, we propose Deep Alignment, a novel approach to train the visual safety prompt through supervision in the model's activation space. It enhances the inherent ability of LVLMs to perceive malicious queries, achieving deeper alignment than prior works. Extensive experiments across five benchmarks on two representative LVLMs demonstrate that DAVSP effectively resists malicious queries while preserving benign input utility. Furthermore, DAVSP exhibits great cross-model generation ability. Ablation studies further reveal that both the Visual Safety Prompt and Deep Alignment are essential components, jointly contributing to its overall effectiveness. The code is publicly available at https://github.com/zhangyitonggg/DAVSP."
  },
  {
    "title": "\"Is This Really a Human Peer Supporter?\": Misalignments Between Peer Supporters and Experts in LLM-Supported Interactions",
    "url": "http://arxiv.org/abs/2506.09354v1",
    "arxiv_id": "2506.09354v1",
    "authors": [
      "Kellie Yu Hui Sim",
      "Roy Ka-Wei Lee",
      "Kenny Tsu Wei Choo"
    ],
    "published": "2025-06-11T03:06:41+00:00",
    "summary": "Mental health is a growing global concern, prompting interest in AI-driven solutions to expand access to psychosocial support. Peer support, grounded in lived experience, offers a valuable complement to professional care. However, variability in training, effectiveness, and definitions raises concerns about quality, consistency, and safety. Large Language Models (LLMs) present new opportunities to enhance peer support interactions, particularly in real-time, text-based interactions. We present and evaluate an AI-supported system with an LLM-simulated distressed client, context-sensitive LLM-generated suggestions, and real-time emotion visualisations. 2 mixed-methods studies with 12 peer supporters and 5 mental health professionals (i.e., experts) examined the system's effectiveness and implications for practice. Both groups recognised its potential to enhance training and improve interaction quality. However, we found a key tension emerged: while peer supporters engaged meaningfully, experts consistently flagged critical issues in peer supporter responses, such as missed distress cues and premature advice-giving. This misalignment highlights potential limitations in current peer support training, especially in emotionally charged contexts where safety and fidelity to best practices are essential. Our findings underscore the need for standardised, psychologically grounded training, especially as peer support scales globally. They also demonstrate how LLM-supported systems can scaffold this development--if designed with care and guided by expert oversight. This work contributes to emerging conversations on responsible AI integration in mental health and the evolving role of LLMs in augmenting peer-delivered care."
  },
  {
    "title": "Efficient Edge Deployment of Quantized YOLOv4-Tiny for Aerial Emergency Object Detection on Raspberry Pi 5",
    "url": "http://arxiv.org/abs/2506.09300v1",
    "arxiv_id": "2506.09300v1",
    "authors": [
      "Sindhu Boddu",
      "Arindam Mukherjee"
    ],
    "published": "2025-06-10T23:33:20+00:00",
    "summary": "This paper presents the deployment and performance evaluation of a quantized YOLOv4-Tiny model for real-time object detection in aerial emergency imagery on a resource-constrained edge device the Raspberry Pi 5. The YOLOv4-Tiny model was quantized to INT8 precision using TensorFlow Lite post-training quantization techniques and evaluated for detection speed, power consumption, and thermal feasibility under embedded deployment conditions. The quantized model achieved an inference time of 28.2 ms per image with an average power consumption of 13.85 W, demonstrating a significant reduction in power usage compared to its FP32 counterpart. Detection accuracy remained robust across key emergency classes such as Ambulance, Police, Fire Engine, and Car Crash. These results highlight the potential of low-power embedded AI systems for real-time deployment in safety-critical emergency response applications."
  },
  {
    "title": "Down But Not Out: The Case of Long-Period Comet C/2021 O3 (Panstarrs)",
    "url": "http://arxiv.org/abs/2506.09263v1",
    "arxiv_id": "2506.09263v1",
    "authors": [
      "David Jewitt. Jing Li",
      "Michael Jaeger",
      "Yoonyoung Kim"
    ],
    "published": "2025-06-10T21:55:49+00:00",
    "summary": "We combine ground- and space-based observations of long-period comet C/2021 O3 (Panstarrs) (perihelion distance 0.287 au) in order to investigate its reported near-perihelion destruction. Pre-perihelion photometric observations show a remarkably small heliocentric dependence of the scattered light, $\\propto r_H^{-s}$ with $s = 2.59\\pm0.21$, distinct from values reported in other long-period comets, for which $s$ = 4 is the canonical standard. The index is smaller than expected of coma production by equilibrium sublimation of either supervolatiles (for which $s \\sim$ 4 is expected), or water ice ($s \\sim$ 6 to 8) across the $\\sim$4 au to 2 au range. The absolute magnitude deduced from the pre-perihelion data is $H$ = 13.0$\\pm$0.3 (coma scattering cross-section $\\sim$225 km$^2$ for an assumed geometric albedo 0.04) while, after perihelion, the cross-section fades by a factor of 25 to $H$ = 16.5 ($\\sim$9 km$^2$). STEREO spacecraft observations near perihelion show a long debris trail whose properties are consistent with forward scattering from radius $\\sim$7 $\\mu$m particles. The data show that the nucleus of C/2021 O3 was not destroyed at perihelion. Although the lightcurve from 3.9 au inbound to 0.8 au outbound cannot be uniquely interpreted, a simple and plausible explanation is provided by seasonal dimming on a nucleus having high obliquity and an asymmetric distribution of near-surface volatiles. The survival of the nucleus against rotational disruption suggests a pre-perihelion nucleus radius $r_n \\gtrsim$ 1.0 km while the photometric limit to the radius of the nucleus after perihelion is $r_n < 1.7$ km (geometric albedo 0.04 assumed)."
  },
  {
    "title": "Augmented Reality User Interfaces for First Responders: A Scoping Literature Review",
    "url": "http://arxiv.org/abs/2506.09236v1",
    "arxiv_id": "2506.09236v1",
    "authors": [
      "Erin Argo",
      "Tanim Ahmed",
      "Sarah Gable",
      "Callie Hampton",
      "Jeronimo Grandi",
      "Regis Kopper"
    ],
    "published": "2025-06-10T20:44:15+00:00",
    "summary": "During the past decade, there has been a significant increase in research focused on integrating AR User Interfaces into public safety applications, particularly for first responders in the domains of Emergency Medical Services, Firefighting, and Law Enforcement. This paper presents the results of a scoping review involving the application of AR user interfaces in the public safety domain and applies an established systematic review methodology to provide a comprehensive analysis of the current research landscape, identifying key trends, challenges, and gaps in the literature. This review includes peer-reviewed publications indexed by the major scientific databases up to April 2025. A basic keyword search retrieved 1,751 papers, of which 90 were deemed relevant for this review. An in-depth analysis of the literature allowed the development of a faceted taxonomy that categorizes AR user interfaces for public safety. This classification lays a solid foundation for future research, while also highlighting key design considerations, challenges, and gaps in the literature. This review serves as a valuable resource for researchers and developers, offering insights that can drive further advances in the field."
  },
  {
    "title": "Towards Full-Scenario Safety Evaluation of Automated Vehicles: A Volume-Based Method",
    "url": "http://arxiv.org/abs/2506.09182v1",
    "arxiv_id": "2506.09182v1",
    "authors": [
      "Hang Zhou",
      "Chengyuan Ma",
      "Shiyu Shen",
      "Xiaopeng Li"
    ],
    "published": "2025-06-10T18:58:08+00:00",
    "summary": "With the rapid development of automated vehicles (AVs) in recent years, commercially available AVs are increasingly demonstrating high-level automation capabilities. However, most existing AV safety evaluation methods are primarily designed for simple maneuvers such as car-following and lane-changing. While suitable for basic tests, these methods are insufficient for assessing high-level automation functions deployed in more complex environments. First, these methods typically use crash rate as the evaluation metric, whose accuracy heavily depends on the quality and completeness of naturalistic driving environment data used to estimate scenario probabilities. Such data is often difficult and expensive to collect. Second, when applied to diverse scenarios, these methods suffer from the curse of dimensionality, making large-scale evaluation computationally intractable. To address these challenges, this paper proposes a novel framework for full-scenario AV safety evaluation. A unified model is first introduced to standardize the representation of diverse driving scenarios. This modeling approach constrains the dimension of most scenarios to a regular highway setting with three lanes and six surrounding background vehicles, significantly reducing dimensionality. To further avoid the limitations of probability-based method, we propose a volume-based evaluation method that quantifies the proportion of risky scenarios within the entire scenario space. For car-following scenarios, we prove that the set of safe scenarios is convex under specific settings, enabling exact volume computation. Experimental results validate the effectiveness of the proposed volume-based method using both AV behavior models from existing literature and six production AV models calibrated from field-test trajectory data in the Ultra-AV dataset. Code and data will be made publicly available upon acceptance of this paper."
  },
  {
    "title": "Robot-Gated Interactive Imitation Learning with Adaptive Intervention Mechanism",
    "url": "http://arxiv.org/abs/2506.09176v1",
    "arxiv_id": "2506.09176v1",
    "authors": [
      "Haoyuan Cai",
      "Zhenghao Peng",
      "Bolei Zhou"
    ],
    "published": "2025-06-10T18:43:26+00:00",
    "summary": "Interactive Imitation Learning (IIL) allows agents to acquire desired behaviors through human interventions, but current methods impose high cognitive demands on human supervisors. We propose the Adaptive Intervention Mechanism (AIM), a novel robot-gated IIL algorithm that learns an adaptive criterion for requesting human demonstrations. AIM utilizes a proxy Q-function to mimic the human intervention rule and adjusts intervention requests based on the alignment between agent and human actions. By assigning high Q-values when the agent deviates from the expert and decreasing these values as the agent becomes proficient, the proxy Q-function enables the agent to assess the real-time alignment with the expert and request assistance when needed. Our expert-in-the-loop experiments reveal that AIM significantly reduces expert monitoring efforts in both continuous and discrete control tasks. Compared to the uncertainty-based baseline Thrifty-DAgger, our method achieves a 40% improvement in terms of human take-over cost and learning efficiency. Furthermore, AIM effectively identifies safety-critical states for expert assistance, thereby collecting higher-quality expert demonstrations and reducing overall expert data and environment interactions needed. Code and demo video are available at https://github.com/metadriverse/AIM."
  },
  {
    "title": "On Polynomial Stochastic Barrier Functions: Bernstein Versus Sum-of-Squares",
    "url": "http://arxiv.org/abs/2506.09164v1",
    "arxiv_id": "2506.09164v1",
    "authors": [
      "Peter Amorese",
      "Morteza Lahijanian"
    ],
    "published": "2025-06-10T18:25:16+00:00",
    "summary": "Stochastic Barrier Functions (SBFs) certify the safety of stochastic systems by formulating a functional optimization problem, which state-of-the-art methods solve using Sum-of-Squares (SoS) polynomials. This work focuses on polynomial SBFs and introduces a new formulation based on Bernstein polynomials and provides a comparative analysis of its theoretical and empirical performance against SoS methods. We show that the Bernstein formulation leads to a linear program (LP), in contrast to the semi-definite program (SDP) required for SoS, and that its relaxations exhibit favorable theoretical convergence properties. However, our empirical results reveal that the Bernstein approach struggles to match SoS in practical performance, exposing an intriguing gap between theoretical advantages and real-world feasibility."
  },
  {
    "title": "Cosmos-Drive-Dreams: Scalable Synthetic Driving Data Generation with World Foundation Models",
    "url": "http://arxiv.org/abs/2506.09042v1",
    "arxiv_id": "2506.09042v1",
    "authors": [
      "Xuanchi Ren",
      "Yifan Lu",
      "Tianshi Cao",
      "Ruiyuan Gao",
      "Shengyu Huang",
      "Amirmojtaba Sabour",
      "Tianchang Shen",
      "Tobias Pfaff",
      "Jay Zhangjie Wu",
      "Runjian Chen",
      "Seung Wook Kim",
      "Jun Gao",
      "Laura Leal-Taixe",
      "Mike Chen",
      "Sanja Fidler",
      "Huan Ling"
    ],
    "published": "2025-06-10T17:58:17+00:00",
    "summary": "Collecting and annotating real-world data for safety-critical physical AI systems, such as Autonomous Vehicle (AV), is time-consuming and costly. It is especially challenging to capture rare edge cases, which play a critical role in training and testing of an AV system. To address this challenge, we introduce the Cosmos-Drive-Dreams - a synthetic data generation (SDG) pipeline that aims to generate challenging scenarios to facilitate downstream tasks such as perception and driving policy training. Powering this pipeline is Cosmos-Drive, a suite of models specialized from NVIDIA Cosmos world foundation model for the driving domain and are capable of controllable, high-fidelity, multi-view, and spatiotemporally consistent driving video generation. We showcase the utility of these models by applying Cosmos-Drive-Dreams to scale the quantity and diversity of driving datasets with high-fidelity and challenging scenarios. Experimentally, we demonstrate that our generated data helps in mitigating long-tail distribution problems and enhances generalization in downstream tasks such as 3D lane detection, 3D object detection and driving policy learning. We open source our pipeline toolkit, dataset and model weights through the NVIDIA's Cosmos platform.   Project page: https://research.nvidia.com/labs/toronto-ai/cosmos_drive_dreams"
  },
  {
    "title": "DIsoN: Decentralized Isolation Networks for Out-of-Distribution Detection in Medical Imaging",
    "url": "http://arxiv.org/abs/2506.09024v1",
    "arxiv_id": "2506.09024v1",
    "authors": [
      "Felix Wagner",
      "Pramit Saha",
      "Harry Anthony",
      "J. Alison Noble",
      "Konstantinos Kamnitsas"
    ],
    "published": "2025-06-10T17:52:18+00:00",
    "summary": "Safe deployment of machine learning (ML) models in safety-critical domains such as medical imaging requires detecting inputs with characteristics not seen during training, known as out-of-distribution (OOD) detection, to prevent unreliable predictions. Effective OOD detection after deployment could benefit from access to the training data, enabling direct comparison between test samples and the training data distribution to identify differences. State-of-the-art OOD detection methods, however, either discard training data after deployment or assume that test samples and training data are centrally stored together, an assumption that rarely holds in real-world settings. This is because shipping training data with the deployed model is usually impossible due to the size of training databases, as well as proprietary or privacy constraints. We introduce the Isolation Network, an OOD detection framework that quantifies the difficulty of separating a target test sample from the training data by solving a binary classification task. We then propose Decentralized Isolation Networks (DIsoN), which enables the comparison of training and test data when data-sharing is impossible, by exchanging only model parameters between the remote computational nodes of training and deployment. We further extend DIsoN with class-conditioning, comparing a target sample solely with training data of its predicted class. We evaluate DIsoN on four medical imaging datasets (dermatology, chest X-ray, breast ultrasound, histopathology) across 12 OOD detection tasks. DIsoN performs favorably against existing methods while respecting data-privacy. This decentralized OOD detection framework opens the way for a new type of service that ML developers could provide along with their models: providing remote, secure utilization of their training data for OOD detection services. Code will be available upon acceptance at: *****"
  },
  {
    "title": "Comparing human and LLM proofreading in L2 writing: Impact on lexical and syntactic features",
    "url": "http://arxiv.org/abs/2506.09021v1",
    "arxiv_id": "2506.09021v1",
    "authors": [
      "Hakyung Sung",
      "Karla Csuros",
      "Min-Chang Sung"
    ],
    "published": "2025-06-10T17:49:10+00:00",
    "summary": "This study examines the lexical and syntactic interventions of human and LLM proofreading aimed at improving overall intelligibility in identical second language writings, and evaluates the consistency of outcomes across three LLMs (ChatGPT-4o, Llama3.1-8b, Deepseek-r1-8b). Findings show that both human and LLM proofreading enhance bigram lexical features, which may contribute to better coherence and contextual connectedness between adjacent words. However, LLM proofreading exhibits a more generative approach, extensively reworking vocabulary and sentence structures, such as employing more diverse and sophisticated vocabulary and incorporating a greater number of adjective modifiers in noun phrases. The proofreading outcomes are highly consistent in major lexical and syntactic features across the three models."
  },
  {
    "title": "Comparing human and LLM proofreading in L2 writing: Impact on lexical and syntactic features",
    "url": "http://arxiv.org/abs/2506.09021v2",
    "arxiv_id": "2506.09021v2",
    "authors": [
      "Hakyung Sung",
      "Karla Csuros",
      "Min-Chang Sung"
    ],
    "published": "2025-06-10T17:49:10+00:00",
    "summary": "This study examines the lexical and syntactic interventions of human and LLM proofreading aimed at improving overall intelligibility in identical second language writings, and evaluates the consistency of outcomes across three LLMs (ChatGPT-4o, Llama3.1-8b, Deepseek-r1-8b). Findings show that both human and LLM proofreading enhance bigram lexical features, which may contribute to better coherence and contextual connectedness between adjacent words. However, LLM proofreading exhibits a more generative approach, extensively reworking vocabulary and sentence structures, such as employing more diverse and sophisticated vocabulary and incorporating a greater number of adjective modifiers in noun phrases. The proofreading outcomes are highly consistent in major lexical and syntactic features across the three models."
  },
  {
    "title": "Online Learning Control Strategies for Industrial Processes with Application for Loosening and Conditioning",
    "url": "http://arxiv.org/abs/2506.08983v1",
    "arxiv_id": "2506.08983v1",
    "authors": [
      "Yue Wu",
      "Jianfu Cao",
      "Ye Cao"
    ],
    "published": "2025-06-10T16:53:00+00:00",
    "summary": "This paper proposes a novel adaptive Koopman Model Predictive Control (MPC) framework, termed HPC-AK-MPC, designed to address the dual challenges of time-varying dynamics and safe operation in complex industrial processes. The framework integrates two core strategies: online learning and historically-informed safety constraints. To contend with process time-variance, a Recursive Extended Dynamic Mode Decomposition (rEDMDc) technique is employed to construct an adaptive Koopman model capable of updating its parameters from real-time data, endowing the controller with the ability to continuously learn and track dynamic changes. To tackle the critical issue of safe operation under model uncertainty, we introduce a novel Historical Process Constraint (HPC) mechanism. This mechanism mines successful operational experiences from a historical database and, by coupling them with the confidence level of the online model, generates a dynamic \"safety corridor\" for the MPC optimization problem. This approach transforms implicit expert knowledge into explicit, adaptive constraints, establishing a dynamic balance between pursuing optimal performance and ensuring robust safety. The proposed HPC-AK-MPC method is applied to a real-world tobacco loosening and conditioning process and systematically validated using an \"advisor mode\" simulation framework with industrial data. Experimental results demonstrate that, compared to historical operations, the proposed method significantly improves the Process Capability Index (Cpk) for key quality variables across all tested batches, proving its substantial potential in enhancing control performance while guaranteeing operational safety."
  },
  {
    "title": "Evaluating Generative Vehicle Trajectory Models for Traffic Intersection Dynamics",
    "url": "http://arxiv.org/abs/2506.08963v1",
    "arxiv_id": "2506.08963v1",
    "authors": [
      "Yash Ranjan",
      "Rahul Sengupta",
      "Anand Rangarajan",
      "Sanjay Ranka"
    ],
    "published": "2025-06-10T16:36:42+00:00",
    "summary": "Traffic Intersections are vital to urban road networks as they regulate the movement of people and goods. However, they are regions of conflicting trajectories and are prone to accidents. Deep Generative models of traffic dynamics at signalized intersections can greatly help traffic authorities better understand the efficiency and safety aspects. At present, models are evaluated on computational metrics that primarily look at trajectory reconstruction errors. They are not evaluated online in a `live' microsimulation scenario. Further, these metrics do not adequately consider traffic engineering-specific concerns such as red-light violations, unallowed stoppage, etc. In this work, we provide a comprehensive analytics tool to train, run, and evaluate models with metrics that give better insights into model performance from a traffic engineering point of view. We train a state-of-the-art multi-vehicle trajectory forecasting model on a large dataset collected by running a calibrated scenario of a real-world urban intersection. We then evaluate the performance of the prediction models, online in a microsimulator, under unseen traffic conditions. We show that despite using ideally-behaved trajectories as input, and achieving low trajectory reconstruction errors, the generated trajectories show behaviors that break traffic rules. We introduce new metrics to evaluate such undesired behaviors and present our results."
  },
  {
    "title": "IntTrajSim: Trajectory Prediction for Simulating Multi-Vehicle driving at Signalized Intersections",
    "url": "http://arxiv.org/abs/2506.08957v1",
    "arxiv_id": "2506.08957v1",
    "authors": [
      "Yash Ranjan",
      "Rahul Sengupta",
      "Anand Rangarajan",
      "Sanjay Ranka"
    ],
    "published": "2025-06-10T16:27:42+00:00",
    "summary": "Traffic simulators are widely used to study the operational efficiency of road infrastructure, but their rule-based approach limits their ability to mimic real-world driving behavior. Traffic intersections are critical components of the road infrastructure, both in terms of safety risk (nearly 28% of fatal crashes and 58% of nonfatal crashes happen at intersections) as well as the operational efficiency of a road corridor. This raises an important question: can we create a data-driven simulator that can mimic the macro- and micro-statistics of the driving behavior at a traffic intersection? Deep Generative Modeling-based trajectory prediction models provide a good starting point to model the complex dynamics of vehicles at an intersection. But they are not tested in a \"live\" micro-simulation scenario and are not evaluated on traffic engineering-related metrics. In this study, we propose traffic engineering-related metrics to evaluate generative trajectory prediction models and provide a simulation-in-the-loop pipeline to do so. We also provide a multi-headed self-attention-based trajectory prediction model that incorporates the signal information, which outperforms our previous models on the evaluation metrics."
  },
  {
    "title": "AdversariaL attacK sAfety aLIgnment(ALKALI): Safeguarding LLMs through GRACE: Geometric Representation-Aware Contrastive Enhancement- Introducing Adversarial Vulnerability Quality Index (AVQI)",
    "url": "http://arxiv.org/abs/2506.08885v1",
    "arxiv_id": "2506.08885v1",
    "authors": [
      "Danush Khanna",
      "Krishna Kumar",
      "Basab Ghosh",
      "Vinija Jain",
      "Vasu Sharma",
      "Aman Chadha",
      "Amitava Das"
    ],
    "published": "2025-06-10T15:14:17+00:00",
    "summary": "Adversarial threats against LLMs are escalating faster than current defenses can adapt. We expose a critical geometric blind spot in alignment: adversarial prompts exploit latent camouflage, embedding perilously close to the safe representation manifold while encoding unsafe intent thereby evading surface level defenses like Direct Preference Optimization (DPO), which remain blind to the latent geometry. We introduce ALKALI, the first rigorously curated adversarial benchmark and the most comprehensive to date spanning 9,000 prompts across three macro categories, six subtypes, and fifteen attack families. Evaluation of 21 leading LLMs reveals alarmingly high Attack Success Rates (ASRs) across both open and closed source models, exposing an underlying vulnerability we term latent camouflage, a structural blind spot where adversarial completions mimic the latent geometry of safe ones. To mitigate this vulnerability, we introduce GRACE - Geometric Representation Aware Contrastive Enhancement, an alignment framework coupling preference learning with latent space regularization. GRACE enforces two constraints: latent separation between safe and adversarial completions, and adversarial cohesion among unsafe and jailbreak behaviors. These operate over layerwise pooled embeddings guided by a learned attention profile, reshaping internal geometry without modifying the base model, and achieve up to 39% ASR reduction. Moreover, we introduce AVQI, a geometry aware metric that quantifies latent alignment failure via cluster separation and compactness. AVQI reveals when unsafe completions mimic the geometry of safe ones, offering a principled lens into how models internally encode safety. We make the code publicly available at https://anonymous.4open.science/r/alkali-B416/README.md."
  },
  {
    "title": "AdversariaL attacK sAfety aLIgnment(ALKALI): Safeguarding LLMs through GRACE: Geometric Representation-Aware Contrastive Enhancement- Introducing Adversarial Vulnerability Quality Index (AVQI)",
    "url": "http://arxiv.org/abs/2506.08885v2",
    "arxiv_id": "2506.08885v2",
    "authors": [
      "Danush Khanna",
      "Krishna Kumar",
      "Basab Ghosh",
      "Vinija Jain",
      "Vasu Sharma",
      "Aman Chadha",
      "Amitava Das"
    ],
    "published": "2025-06-10T15:14:17+00:00",
    "summary": "Adversarial threats against LLMs are escalating faster than current defenses can adapt. We expose a critical geometric blind spot in alignment: adversarial prompts exploit latent camouflage, embedding perilously close to the safe representation manifold while encoding unsafe intent thereby evading surface level defenses like Direct Preference Optimization (DPO), which remain blind to the latent geometry. We introduce ALKALI, the first rigorously curated adversarial benchmark and the most comprehensive to date spanning 9,000 prompts across three macro categories, six subtypes, and fifteen attack families. Evaluation of 21 leading LLMs reveals alarmingly high Attack Success Rates (ASRs) across both open and closed source models, exposing an underlying vulnerability we term latent camouflage, a structural blind spot where adversarial completions mimic the latent geometry of safe ones. To mitigate this vulnerability, we introduce GRACE - Geometric Representation Aware Contrastive Enhancement, an alignment framework coupling preference learning with latent space regularization. GRACE enforces two constraints: latent separation between safe and adversarial completions, and adversarial cohesion among unsafe and jailbreak behaviors. These operate over layerwise pooled embeddings guided by a learned attention profile, reshaping internal geometry without modifying the base model, and achieve up to 39% ASR reduction. Moreover, we introduce AVQI, a geometry aware metric that quantifies latent alignment failure via cluster separation and compactness. AVQI reveals when unsafe completions mimic the geometry of safe ones, offering a principled lens into how models internally encode safety. We make the code publicly available at https://anonymous.4open.science/r/alkali-B416/README.md."
  },
  {
    "title": "Safety-Driven Response Adaptive Randomisation: An Application in Non-inferiority Oncology Trials",
    "url": "http://arxiv.org/abs/2506.08864v1",
    "arxiv_id": "2506.08864v1",
    "authors": [
      "Maria Vittoria Chiaruttini",
      "Lukas Pin",
      "Sofia S. Villar"
    ],
    "published": "2025-06-10T14:56:10+00:00",
    "summary": "The majority of response-adaptive randomisation (RAR) designs in the literature use efficacy data to dynamically allocate patients. Their applicability in settings where the efficacy measure is observable with a random delay, such as overall survival, remains challenging. This paper introduces a RAR design referred to as SAFER (Safety-Aware Flexible Elastic Randomisation) design, which uses early-emerging safety data to inform treatment allocation decisions in oncology trials. However, the design is applicable to a range of settings where it may be desirable to favour the arm demonstrating a superior safety profile. This is particularly relevant in non-inferiority trials, which aim to demonstrate an experimental treatment is not inferior to the standard of care, while offering advantages in terms of safety and tolerability. Consequently, an unavoidable and well-established trade-off arises for such designs: to balance the goals of preserving inferential efficiency for the primary non-inferiority outcome while incorporating safety considerations into the randomisation process through RAR. Our method, defines a randomisation procedure which prioritises the assignment of patients to better-tolerated arms and adjusts the allocation proportion according to the observed association between safety and efficacy endpoints. We illustrate our procedure through a comprehensive simulation study, inspired by the CAPP-IT Phase III oncology trial. Our results demonstrate that SAFER preserves statistical power even when efficacy and safety endpoints are weakly associated and offers power gains when a strong positive association is present. Moreover, the approach enables a faster/slower adaptation when efficacy and safety endpoints are temporally aligned/misaligned, respectively."
  },
  {
    "title": "Confidence Boosts Trust-Based Resilience in Cooperative Multi-Robot Systems",
    "url": "http://arxiv.org/abs/2506.08807v1",
    "arxiv_id": "2506.08807v1",
    "authors": [
      "Luca Ballotta",
      "\u00c1ron V\u00e9k\u00e1ssy",
      "Stephanie Gil",
      "Michal Yemini"
    ],
    "published": "2025-06-10T13:56:32+00:00",
    "summary": "Wireless communication-based multi-robot systems open the door to cyberattacks that can disrupt safety and performance of collaborative robots. The physical channel supporting inter-robot communication offers an attractive opportunity to decouple the detection of malicious robots from task-relevant data exchange between legitimate robots. Yet, trustworthiness indications coming from physical channels are uncertain and must be handled with this in mind. In this paper, we propose a resilient protocol for multi-robot operation wherein a parameter {\\lambda}t accounts for how confident a robot is about the legitimacy of nearby robots that the physical channel indicates. Analytical results prove that our protocol achieves resilient coordination with arbitrarily many malicious robots under mild assumptions. Tuning {\\lambda}t allows a designer to trade between near-optimal inter-robot coordination and quick task execution; see Fig. 1. This is a fundamental performance tradeoff and must be carefully evaluated based on the task at hand. The effectiveness of our approach is numerically verified with experiments involving platoons of autonomous cars where some vehicles are maliciously spoofed."
  },
  {
    "title": "Enhancing Accuracy and Maintainability in Nuclear Plant Data Retrieval: A Function-Calling LLM Approach Over NL-to-SQL",
    "url": "http://arxiv.org/abs/2506.08757v1",
    "arxiv_id": "2506.08757v1",
    "authors": [
      "Mishca de Costa",
      "Muhammad Anwar",
      "Dave Mercier",
      "Mark Randall",
      "Issam Hammad"
    ],
    "published": "2025-06-10T12:55:07+00:00",
    "summary": "Retrieving operational data from nuclear power plants requires exceptional accuracy and transparency due to the criticality of the decisions it supports. Traditionally, natural language to SQL (NL-to-SQL) approaches have been explored for querying such data. While NL-to-SQL promises ease of use, it poses significant risks: end-users cannot easily validate generated SQL queries, and legacy nuclear plant databases -- often complex and poorly structured -- complicate query generation due to decades of incremental modifications. These challenges increase the likelihood of inaccuracies and reduce trust in the approach. In this work, we propose an alternative paradigm: leveraging function-calling large language models (LLMs) to address these challenges. Instead of directly generating SQL queries, we define a set of pre-approved, purpose-specific functions representing common use cases. Queries are processed by invoking these functions, which encapsulate validated SQL logic. This hybrid approach mitigates the risks associated with direct NL-to-SQL translations by ensuring that SQL queries are reviewed and optimized by experts before deployment. While this strategy introduces the upfront cost of developing and maintaining the function library, we demonstrate how NL-to-SQL tools can assist in the initial generation of function code, allowing experts to focus on validation rather than creation. Our study includes a performance comparison between direct NL-to-SQL generation and the proposed function-based approach, highlighting improvements in accuracy and maintainability. This work underscores the importance of balancing user accessibility with operational safety and provides a novel, actionable framework for robust data retrieval in critical systems."
  },
  {
    "title": "Societal AI Research Has Become Less Interdisciplinary",
    "url": "http://arxiv.org/abs/2506.08738v1",
    "arxiv_id": "2506.08738v1",
    "authors": [
      "Dror Kris Markus",
      "Fabrizio Gilardi",
      "Daria Stetsenko"
    ],
    "published": "2025-06-10T12:34:53+00:00",
    "summary": "As artificial intelligence (AI) systems become deeply embedded in everyday life, calls to align AI development with ethical and societal values have intensified. Interdisciplinary collaboration is often championed as a key pathway for fostering such engagement. Yet it remains unclear whether interdisciplinary research teams are actually leading this shift in practice. This study analyzes over 100,000 AI-related papers published on ArXiv between 2014 and 2024 to examine how ethical values and societal concerns are integrated into technical AI research. We develop a classifier to identify societal content and measure the extent to which research papers express these considerations. We find a striking shift: while interdisciplinary teams remain more likely to produce societally-oriented research, computer science-only teams now account for a growing share of the field's overall societal output. These teams are increasingly integrating societal concerns into their papers and tackling a wide range of domains - from fairness and safety to healthcare and misinformation. These findings challenge common assumptions about the drivers of societal AI and raise important questions. First, what are the implications for emerging understandings of AI safety and governance if most societally-oriented research is being undertaken by exclusively technical teams? Second, for scholars in the social sciences and humanities: in a technical field increasingly responsive to societal demands, what distinctive perspectives can we still offer to help shape the future of AI?"
  },
  {
    "title": "Societal AI Research Has Become Less Interdisciplinary",
    "url": "http://arxiv.org/abs/2506.08738v2",
    "arxiv_id": "2506.08738v2",
    "authors": [
      "Dror Kris Markus",
      "Fabrizio Gilardi",
      "Daria Stetsenko"
    ],
    "published": "2025-06-10T12:34:53+00:00",
    "summary": "As artificial intelligence (AI) systems become deeply embedded in everyday life, calls to align AI development with ethical and societal values have intensified. Interdisciplinary collaboration is often championed as a key pathway for fostering such engagement. Yet it remains unclear whether interdisciplinary research teams are actually leading this shift in practice. This study analyzes over 100,000 AI-related papers published on ArXiv between 2014 and 2024 to examine how ethical values and societal concerns are integrated into technical AI research. We develop a classifier to identify societal content and measure the extent to which research papers express these considerations. We find a striking shift: while interdisciplinary teams remain more likely to produce societally-oriented research, computer science-only teams now account for a growing share of the field's overall societal output. These teams are increasingly integrating societal concerns into their papers and tackling a wide range of domains - from fairness and safety to healthcare and misinformation. These findings challenge common assumptions about the drivers of societal AI and raise important questions. First, what are the implications for emerging understandings of AI safety and governance if most societally-oriented research is being undertaken by exclusively technical teams? Second, for scholars in the social sciences and humanities: in a technical field increasingly responsive to societal demands, what distinctive perspectives can we still offer to help shape the future of AI?"
  },
  {
    "title": "ROS-related Robotic Systems Development with V-model-based Application of MeROS Metamodel",
    "url": "http://arxiv.org/abs/2506.08706v1",
    "arxiv_id": "2506.08706v1",
    "authors": [
      "Tomasz Winiarski",
      "Jan Kaniuka",
      "Daniel Gie\u0142dowski",
      "Jakub Ostrysz",
      "Krystian Radlak",
      "Dmytro Kushnir"
    ],
    "published": "2025-06-10T11:44:00+00:00",
    "summary": "As robotic systems grow increasingly complex, heterogeneous, and safety-critical, the need for structured development methodologies becomes paramount. Although frameworks like the Robot Operating System (ROS) and Model-Based Systems Engineering (MBSE) offer foundational tools, they often lack integration when used together. This paper addresses that gap by aligning the widely recognized V-model development paradigm with the MeROS metamodel SysML-based modeling language tailored for ROS-based systems.   We propose a domain-specific methodology that bridges ROS-centric modelling with systems engineering practices. Our approach formalises the structure, behaviour, and validation processes of robotic systems using MeROS, while extending it with a generalized, adaptable V-model compatible with both ROS and ROS 2. Rather than prescribing a fixed procedure, the approach supports project-specific flexibility and reuse, offering guidance across all stages of development.   The approach is validated through a comprehensive case study on HeROS, a heterogeneous multi-robot platform comprising manipulators, mobile units, and dynamic test environments. This example illustrates how the MeROS-compatible V-model enhances traceability and system consistency while remaining accessible and extensible for future adaptation. The work contributes a structured, tool-agnostic foundation for developers and researchers seeking to apply MBSE practices in ROS-based projects."
  },
  {
    "title": "Causality-aware Safety Testing for Autonomous Driving Systems",
    "url": "http://arxiv.org/abs/2506.08688v1",
    "arxiv_id": "2506.08688v1",
    "authors": [
      "Wenbing Tang",
      "Mingfei Cheng",
      "Renzhi Wang",
      "Yuan Zhou",
      "Chengwei Liu",
      "Yang Liu",
      "Zuohua Ding"
    ],
    "published": "2025-06-10T10:53:26+00:00",
    "summary": "Simulation-based testing is essential for evaluating the safety of Autonomous Driving Systems (ADSs). Comprehensive evaluation requires testing across diverse scenarios that can trigger various types of violations under different conditions. While existing methods typically focus on individual diversity metrics, such as input scenarios, ADS-generated motion commands, and system violations, they often fail to capture the complex interrelationships among these elements. This oversight leads to gaps in testing coverage, potentially missing critical issues in the ADS under evaluation. However, quantifying these interrelationships presents a significant challenge. In this paper, we propose a novel causality-aware fuzzing technique, Causal-Fuzzer, to enable efficient and comprehensive testing of ADSs by exploring causally diverse scenarios. The core of Causal-Fuzzer is constructing a causal graph to model the interrelationships among the diversities of input scenarios, ADS motion commands, and system violations. Then the causal graph will guide the process of critical scenario generation. Specifically, Causal-Fuzzer proposes (1) a causality-based feedback mechanism that quantifies the combined diversity of test scenarios by assessing whether they activate new causal relationships, and (2) a causality-driven mutation strategy that prioritizes mutations on input scenario elements with higher causal impact on ego action changes and violation occurrence, rather than treating all elements equally. We evaluated Causal-Fuzzer on an industry-grade ADS Apollo, with a high-fidelity. Our empirical results demonstrate that Causal-Fuzzer significantly outperforms existing methods in (1) identifying a greater diversity of violations, (2) providing enhanced testing sufficiency with improved coverage of causal relationships, and (3) achieving greater efficiency in detecting the first critical scenarios."
  },
  {
    "title": "Linguistic Ordered Weighted Averaging based deep learning pooling for fault diagnosis in a wastewater treatment plant",
    "url": "http://arxiv.org/abs/2506.08676v1",
    "arxiv_id": "2506.08676v1",
    "authors": [
      "Alicia Beneyto-Rodriguez",
      "Gregorio I. Sainz-Palmero",
      "Marta Galende-Hern\u00e1ndez",
      "Mar\u00eda J. Fuente"
    ],
    "published": "2025-06-10T10:35:56+00:00",
    "summary": "Nowadays, water reuse is a serious challenge to help address water shortages. Here, the wastewater treatment plants (WWTP) play a key role, and its proper operation is mandatory. So, fault diagnosis is a key activity for these plants. Their high complexity and large-scale require of smart methodologies for that fault diagnosis and safety operation. All these large-scale and complex industrial processes are monitored, allowing the data collection about the plant operation, so data driven approaches for fault diagnosis can be applied. A popular approach to fault diagnosis is deep learning-based methodologies. Here, a fault diagnosis methodology is proposed for a WWTP using a new linguistic Ordered Weighted Averaging (OWA) pooling based Deep Convolutional Neural Network (DCNN) and a sliding and overlapping time window. This window slides over input data based on the monitoring sampling time, then the diagnosis is carried out by the linguistic OWA pooling based DCNN. This alternative linguistic pooling uses well-known linguistic OWA quantifiers, which permit terms such as \\textsl{Most, AtLeast, etc.}, supplying new intuitive options for the pooling tasks. This sliding time window and the OWA pooling based network permit a better and earlier fault diagnosis, at each sampling time, using a few monitoring samples and a fewer learning iterations than DCNN standard pooling. Several linguistic OWA operators have been checked with a benchmark for WWTPs. A set of 5 fault types has been used, taking into account 140 variables sampled at 15 minutes time intervals. The performance has been over $91\\%$ for $Accuracy$, $Recall$ or $F1-Score$, and better than other competitive methodologies. Moreover, these linguistic OWA operators for DCNN pooling have shown a better performance than the standard \\textsl{Max} and \\textsl{Average} options."
  },
  {
    "title": "RuleReasoner: Reinforced Rule-based Reasoning via Domain-aware Dynamic Sampling",
    "url": "http://arxiv.org/abs/2506.08672v1",
    "arxiv_id": "2506.08672v1",
    "authors": [
      "Yang Liu",
      "Jiaqi Li",
      "Zilong Zheng"
    ],
    "published": "2025-06-10T10:31:21+00:00",
    "summary": "Rule-based reasoning has been acknowledged as one of the fundamental problems in reasoning, while deviations in rule formats, types, and complexity in real-world applications pose severe challenges. Recent studies have shown that large reasoning models (LRMs) have remarkable reasoning capabilities, and their performance is substantially enhanced by reinforcement learning (RL). However, it remains an open question whether small reasoning models (SRMs) can learn rule-based reasoning effectively with robust generalization across diverse tasks and domains. To address this, we introduce Reinforced Rule-based Reasoning, a.k.a. RuleReasoner, a simple yet effective method to conduct rule-based reasoning via a wide collection of curated tasks and a novel domain-aware dynamic sampling approach. Specifically, RuleReasoner resamples each training batch by updating the sampling weights of different domains based on historical rewards. This facilitates domain augmentation and flexible online learning schedules for RL, obviating the need for pre-hoc human-engineered mix-training recipes used in existing methods. Empirical evaluations on in-distribution (ID) and out-of-distribution (OOD) benchmarks reveal that RuleReasoner outperforms frontier LRMs by a significant margin ($\\Delta$4.1% average points on eight ID tasks and $\\Delta$10.4% average points on three OOD tasks over OpenAI-o1). Notably, our approach also exhibits higher computational efficiency compared to prior dynamic sampling methods for RL."
  },
  {
    "title": "CounselBench: A Large-Scale Expert Evaluation and Adversarial Benchmark of Large Language Models in Mental Health Counseling",
    "url": "http://arxiv.org/abs/2506.08584v1",
    "arxiv_id": "2506.08584v1",
    "authors": [
      "Yahan Li",
      "Jifan Yao",
      "John Bosco S. Bunyi",
      "Adam C. Frank",
      "Angel Hwang",
      "Ruishan Liu"
    ],
    "published": "2025-06-10T08:53:06+00:00",
    "summary": "Large language models (LLMs) are increasingly proposed for use in mental health support, yet their behavior in realistic counseling scenarios remains largely untested. We introduce CounselBench, a large-scale benchmark developed with 100 mental health professionals to evaluate and stress-test LLMs in single-turn counseling. The first component, CounselBench-EVAL, contains 2,000 expert evaluations of responses from GPT-4, LLaMA 3, Gemini, and online human therapists to real patient questions. Each response is rated along six clinically grounded dimensions, with written rationales and span-level annotations. We find that LLMs often outperform online human therapists in perceived quality, but experts frequently flag their outputs for safety concerns such as unauthorized medical advice. Follow-up experiments show that LLM judges consistently overrate model responses and overlook safety issues identified by human experts. To probe failure modes more directly, we construct CounselBench-Adv, an adversarial dataset of 120 expert-authored counseling questions designed to trigger specific model issues. Evaluation across 2,880 responses from eight LLMs reveals consistent, model-specific failure patterns. Together, CounselBench establishes a clinically grounded framework for benchmarking and improving LLM behavior in high-stakes mental health settings."
  },
  {
    "title": "TrajFlow: Multi-modal Motion Prediction via Flow Matching",
    "url": "http://arxiv.org/abs/2506.08541v1",
    "arxiv_id": "2506.08541v1",
    "authors": [
      "Qi Yan",
      "Brian Zhang",
      "Yutong Zhang",
      "Daniel Yang",
      "Joshua White",
      "Di Chen",
      "Jiachao Liu",
      "Langechuan Liu",
      "Binnan Zhuang",
      "Shaoshuai Shi",
      "Renjie Liao"
    ],
    "published": "2025-06-10T08:08:31+00:00",
    "summary": "Efficient and accurate motion prediction is crucial for ensuring safety and informed decision-making in autonomous driving, particularly under dynamic real-world conditions that necessitate multi-modal forecasts. We introduce TrajFlow, a novel flow matching-based motion prediction framework that addresses the scalability and efficiency challenges of existing generative trajectory prediction methods. Unlike conventional generative approaches that employ i.i.d. sampling and require multiple inference passes to capture diverse outcomes, TrajFlow predicts multiple plausible future trajectories in a single pass, significantly reducing computational overhead while maintaining coherence across predictions. Moreover, we propose a ranking loss based on the Plackett-Luce distribution to improve uncertainty estimation of predicted trajectories. Additionally, we design a self-conditioning training technique that reuses the model's own predictions to construct noisy inputs during a second forward pass, thereby improving generalization and accelerating inference. Extensive experiments on the large-scale Waymo Open Motion Dataset (WOMD) demonstrate that TrajFlow achieves state-of-the-art performance across various key metrics, underscoring its effectiveness for safety-critical autonomous driving applications. The code and other details are available on the project website https://traj-flow.github.io/."
  },
  {
    "title": "AsFT: Anchoring Safety During LLM Fine-Tuning Within Narrow Safety Basin",
    "url": "http://arxiv.org/abs/2506.08473v1",
    "arxiv_id": "2506.08473v1",
    "authors": [
      "Shuo Yang",
      "Qihui Zhang",
      "Yuyang Liu",
      "Yue Huang",
      "Xiaojun Jia",
      "Kunpeng Ning",
      "Jiayu Yao",
      "Jigang Wang",
      "Hailiang Dai",
      "Yibing Song",
      "Li Yuan"
    ],
    "published": "2025-06-10T05:59:48+00:00",
    "summary": "Large language models (LLMs) are vulnerable to safety risks during fine-tuning, where small amounts of malicious or harmless data can compromise safeguards. In this paper, building on the concept of alignment direction -- defined by the weight difference between aligned and unaligned models -- we observe that perturbations along this direction preserve model safety. In contrast, perturbations along directions orthogonal to this alignment are strongly linked to harmful direction perturbations, rapidly degrading safety and framing the parameter space as a narrow safety basin. Based on this insight, we propose a methodology for safety fine-tuning called AsFT (Anchoring Safety in Fine-Tuning), which integrates a regularization term into the training objective. This term uses the alignment direction as an anchor to suppress updates in harmful directions, ensuring that fine-tuning is constrained within the narrow safety basin. Extensive experiments on multiple datasets show that AsFT outperforms Safe LoRA, reducing harmful behavior by 7.60 percent, improving model performance by 3.44 percent, and maintaining robust performance across various experimental settings. Code is available at https://github.com/PKU-YuanGroup/AsFT"
  },
  {
    "title": "AsFT: Anchoring Safety During LLM Fine-Tuning Within Narrow Safety Basin",
    "url": "http://arxiv.org/abs/2506.08473v2",
    "arxiv_id": "2506.08473v2",
    "authors": [
      "Shuo Yang",
      "Qihui Zhang",
      "Yuyang Liu",
      "Yue Huang",
      "Xiaojun Jia",
      "Kunpeng Ning",
      "Jiayu Yao",
      "Jigang Wang",
      "Hailiang Dai",
      "Yibing Song",
      "Li Yuan"
    ],
    "published": "2025-06-10T05:59:48+00:00",
    "summary": "Large language models (LLMs) are vulnerable to safety risks during fine-tuning, where small amounts of malicious or harmless data can compromise safeguards. In this paper, building on the concept of alignment direction -- defined by the weight difference between aligned and unaligned models -- we observe that perturbations along this direction preserve model safety. In contrast, perturbations along directions orthogonal to this alignment are strongly linked to harmful direction perturbations, rapidly degrading safety and framing the parameter space as a narrow safety basin. Based on this insight, we propose a methodology for safety fine-tuning called AsFT (Anchoring Safety in Fine-Tuning), which integrates a regularization term into the training objective. This term uses the alignment direction as an anchor to suppress updates in harmful directions, ensuring that fine-tuning is constrained within the narrow safety basin. Extensive experiments on multiple datasets show that AsFT outperforms Safe LoRA, reducing harmful behavior by 7.60 percent, improving model performance by 3.44 percent, and maintaining robust performance across various experimental settings. Code is available at https://github.com/PKU-YuanGroup/AsFT"
  },
  {
    "title": "Diffusion Models for Safety Validation of Autonomous Driving Systems",
    "url": "http://arxiv.org/abs/2506.08459v1",
    "arxiv_id": "2506.08459v1",
    "authors": [
      "Juanran Wang",
      "Marc R. Schlichting",
      "Harrison Delecki",
      "Mykel J. Kochenderfer"
    ],
    "published": "2025-06-10T05:31:33+00:00",
    "summary": "Safety validation of autonomous driving systems is extremely challenging due to the high risks and costs of real-world testing as well as the rarity and diversity of potential failures. To address these challenges, we train a denoising diffusion model to generate potential failure cases of an autonomous vehicle given any initial traffic state. Experiments on a four-way intersection problem show that in a variety of scenarios, the diffusion model can generate realistic failure samples while capturing a wide variety of potential failures. Our model does not require any external training dataset, can perform training and inference with modest computing resources, and does not assume any prior knowledge of the system under test, with applicability to safety validation for traffic intersections."
  },
  {
    "title": "Offline RL with Smooth OOD Generalization in Convex Hull and its Neighborhood",
    "url": "http://arxiv.org/abs/2506.08417v1",
    "arxiv_id": "2506.08417v1",
    "authors": [
      "Qingmao Yao",
      "Zhichao Lei",
      "Tianyuan Chen",
      "Ziyue Yuan",
      "Xuefan Chen",
      "Jianxiang Liu",
      "Faguo Wu",
      "Xiao Zhang"
    ],
    "published": "2025-06-10T03:43:22+00:00",
    "summary": "Offline Reinforcement Learning (RL) struggles with distributional shifts, leading to the $Q$-value overestimation for out-of-distribution (OOD) actions. Existing methods address this issue by imposing constraints; however, they often become overly conservative when evaluating OOD regions, which constrains the $Q$-function generalization. This over-constraint issue results in poor $Q$-value estimation and hinders policy improvement. In this paper, we introduce a novel approach to achieve better $Q$-value estimation by enhancing $Q$-function generalization in OOD regions within Convex Hull and its Neighborhood (CHN). Under the safety generalization guarantees of the CHN, we propose the Smooth Bellman Operator (SBO), which updates OOD $Q$-values by smoothing them with neighboring in-sample $Q$-values. We theoretically show that SBO approximates true $Q$-values for both in-sample and OOD actions within the CHN. Our practical algorithm, Smooth Q-function OOD Generalization (SQOG), empirically alleviates the over-constraint issue, achieving near-accurate $Q$-value estimation. On the D4RL benchmarks, SQOG outperforms existing state-of-the-art methods in both performance and computational efficiency."
  },
  {
    "title": "TACTIC: Translation Agents with Cognitive-Theoretic Interactive Collaboration",
    "url": "http://arxiv.org/abs/2506.08403v1",
    "arxiv_id": "2506.08403v1",
    "authors": [
      "Weiya Li",
      "Junjie Chen",
      "Bei Li",
      "Boyang Liu",
      "Zichen Wen",
      "Nuanqiao Shan",
      "Xiaoqian Liu",
      "Anping Liu",
      "Huajie Liu",
      "Youyan Wang",
      "Wujiuge Yin",
      "Hu Song",
      "Bing Huang",
      "Zhiyuan Xia",
      "Jialiang Chen",
      "Linfeng Zhang"
    ],
    "published": "2025-06-10T03:22:30+00:00",
    "summary": "Machine translation has long been a central task in natural language processing. With the rapid advancement of large language models (LLMs), there has been remarkable progress in translation quality. However, fully realizing the translation potential of LLMs remains an open challenge. Recent studies have explored multi-agent systems to decompose complex translation tasks into collaborative subtasks, showing initial promise in enhancing translation quality through agent cooperation and specialization. Nevertheless, existing multi-agent translation frameworks largely neglect foundational insights from cognitive translation studies. These insights emphasize how human translators employ different cognitive strategies, such as balancing literal and free translation, refining expressions based on context, and iteratively evaluating outputs. To address this limitation, we propose a cognitively informed multi-agent framework called TACTIC, which stands for T ranslation A gents with Cognitive- T heoretic Interactive Collaboration. The framework comprises six functionally distinct agents that mirror key cognitive processes observed in human translation behavior. These include agents for drafting, refinement, evaluation, scoring, context reasoning, and external knowledge gathering. By simulating an interactive and theory-grounded translation workflow, TACTIC effectively leverages the full capacity of LLMs for high-quality translation. Experimental results on diverse language pairs from the FLORES-200 and WMT24 benchmarks show that our method consistently achieves state-of-the-art performance. Using DeepSeek-V3 as the base model, TACTIC surpasses GPT-4.1 by an average of +0.6 XCOMET and +1.18 COMETKIWI-23. Compared to DeepSeek-R1, it further improves by +0.84 XCOMET and +2.99 COMETKIWI-23. Code is available at https://github.com/weiyali126/TACTIC."
  },
  {
    "title": "TACTIC: Translation Agents with Cognitive-Theoretic Interactive Collaboration",
    "url": "http://arxiv.org/abs/2506.08403v2",
    "arxiv_id": "2506.08403v2",
    "authors": [
      "Weiya Li",
      "Junjie Chen",
      "Bei Li",
      "Boyang Liu",
      "Zichen Wen",
      "Nuanqiao Shan",
      "Xiaoqian Liu",
      "Anping Liu",
      "Huajie Liu",
      "Hu Song",
      "Linfeng Zhang"
    ],
    "published": "2025-06-10T03:22:30+00:00",
    "summary": "Machine translation has long been a central task in natural language processing. With the rapid advancement of large language models (LLMs), there has been remarkable progress in translation quality. However, fully realizing the translation potential of LLMs remains an open challenge. Recent studies have explored multi-agent systems to decompose complex translation tasks into collaborative subtasks, showing initial promise in enhancing translation quality through agent cooperation and specialization. Nevertheless, existing multi-agent translation frameworks largely neglect foundational insights from cognitive translation studies. These insights emphasize how human translators employ different cognitive strategies, such as balancing literal and free translation, refining expressions based on context, and iteratively evaluating outputs. To address this limitation, we propose a cognitively informed multi-agent framework called TACTIC, which stands for T ranslation A gents with Cognitive- T heoretic Interactive Collaboration. The framework comprises six functionally distinct agents that mirror key cognitive processes observed in human translation behavior. These include agents for drafting, refinement, evaluation, scoring, context reasoning, and external knowledge gathering. By simulating an interactive and theory-grounded translation workflow, TACTIC effectively leverages the full capacity of LLMs for high-quality translation. Experimental results on diverse language pairs from the FLORES-200 and WMT24 benchmarks show that our method consistently achieves state-of-the-art performance. Using DeepSeek-V3 as the base model, TACTIC surpasses GPT-4.1 by an average of +0.6 XCOMET and +1.18 COMETKIWI-23. Compared to DeepSeek-R1, it further improves by +0.84 XCOMET and +2.99 COMETKIWI-23. Code is available at https://github.com/weiyali126/TACTIC."
  },
  {
    "title": "SafeCoT: Improving VLM Safety with Minimal Reasoning",
    "url": "http://arxiv.org/abs/2506.08399v1",
    "arxiv_id": "2506.08399v1",
    "authors": [
      "Jiachen Ma",
      "Zhanhui Zhou",
      "Chao Yang",
      "Chaochao Lu"
    ],
    "published": "2025-06-10T03:13:50+00:00",
    "summary": "Ensuring safe and appropriate responses from vision-language models (VLMs) remains a critical challenge, particularly in high-risk or ambiguous scenarios. We introduce SafeCoT, a lightweight, interpretable framework that leverages rule-based chain-of-thought (CoT) supervision to improve refusal behavior in VLMs. Unlike prior methods that rely on large-scale safety annotations or complex modeling, SafeCoT uses minimal supervision to help models reason about safety risks and make context-aware refusals. Experiments across multiple benchmarks show that SafeCoT significantly reduces overrefusal and enhances generalization, even with limited training data. Our approach offers a scalable solution for aligning VLMs with safety-critical objectives."
  },
  {
    "title": "SafeCoT: Improving VLM Safety with Minimal Reasoning",
    "url": "http://arxiv.org/abs/2506.08399v2",
    "arxiv_id": "2506.08399v2",
    "authors": [
      "Jiachen Ma",
      "Zhanhui Zhou",
      "Chao Yang",
      "Chaochao Lu"
    ],
    "published": "2025-06-10T03:13:50+00:00",
    "summary": "Ensuring safe and appropriate responses from vision-language models (VLMs) remains a critical challenge, particularly in high-risk or ambiguous scenarios. We introduce SafeCoT, a lightweight, interpretable framework that leverages rule-based chain-of-thought (CoT) supervision to improve refusal behavior in VLMs. Unlike prior methods that rely on large-scale safety annotations or complex modeling, SafeCoT uses minimal supervision to help models reason about safety risks and make context-aware refusals. Experiments across multiple benchmarks show that SafeCoT significantly reduces overrefusal and enhances generalization, even with limited training data. Our approach offers a scalable solution for aligning VLMs with safety-critical objectives."
  },
  {
    "title": "Reinforcement Learning from Human Feedback with High-Confidence Safety Constraints",
    "url": "http://arxiv.org/abs/2506.08266v1",
    "arxiv_id": "2506.08266v1",
    "authors": [
      "Yaswanth Chittepu",
      "Blossom Metevier",
      "Will Schwarzer",
      "Austin Hoag",
      "Scott Niekum",
      "Philip S. Thomas"
    ],
    "published": "2025-06-09T22:03:56+00:00",
    "summary": "Existing approaches to language model alignment often treat safety as a tradeoff against helpfulness, which can lead to unacceptable responses in sensitive domains. To ensure reliable performance in such settings, we propose High-Confidence Safe Reinforcement Learning from Human Feedback (HC-RLHF), a method that provides high-confidence safety guarantees while maximizing helpfulness. Similar to previous methods, HC-RLHF explicitly decouples human preferences into helpfulness and harmlessness (safety), which are learned by training a reward model and a cost model, respectively. It then employs a two-step process to find safe solutions. In the first step, it optimizes the reward function under an intentionally pessimistic version of the cost constraint. In the second step, the trained model undergoes a safety test to verify whether its performance stays within an upper-confidence bound of the actual cost constraint. We provide a theoretical analysis of HC-RLHF, including proof that it will not return an unsafe solution with a probability greater than a user-specified threshold. For our empirical analysis, we apply HC-RLHF to align three different language models (Qwen2-1.5B, Qwen2.5-3B, and LLaMa3.2-3B) with human preferences. Our results demonstrate that HC-RLHF produces safe models with high probability and can improve harmlessness and helpfulness compared to previous methods."
  },
  {
    "title": "SHIELD: Secure Hypernetworks for Incremental Expansion Learning Defense",
    "url": "http://arxiv.org/abs/2506.08255v1",
    "arxiv_id": "2506.08255v1",
    "authors": [
      "Patryk Krukowski",
      "\u0141ukasz Gorczyca",
      "Piotr Helm",
      "Kamil Ksi\u0105\u017cek",
      "Przemys\u0142aw Spurek"
    ],
    "published": "2025-06-09T21:43:56+00:00",
    "summary": "Traditional deep neural networks suffer from several limitations, including catastrophic forgetting. When models are adapted to new datasets, they tend to quickly forget previously learned knowledge. Another significant issue is the lack of robustness to even small perturbations in the input data. In practice, we can often easily perform adversarial attacks and change the network's predictions, adding minimal noise to the input. Dedicated architectures and training procedures can solve each of the above problems separately. Unfortunately, currently, no model can simultaneously address both catastrophic forgetting and vulnerability to adversarial attacks. We introduce SHIELD (Secure Hypernetworks for Incremental Expansion and Learning Defense), a novel approach that integrates a hypernetwork-based continual learning approach with interval arithmetic. SHIELD use the hypernetwork to transfer trainable task embedding vectors into the weights of a target model dedicated to specific data. This paradigm allows for the dynamic generation of separate networks for each subtask, while the hypernetwork aggregates and analyzes information across all tasks. The target model takes in the input a data sample with a defined interval range, and by creating a hypercube, produces a prediction for the given range. Therefore, such target models provide strict guarantees against all possible attacks for data samples within the interval range. Our approach enhances security without sacrificing network adaptability, addressing the overlooked challenge of safety in continual learning."
  },
  {
    "title": "Multilingual Hate Speech Detection in Social Media Using Translation-Based Approaches with Large Language Models",
    "url": "http://arxiv.org/abs/2506.08147v1",
    "arxiv_id": "2506.08147v1",
    "authors": [
      "Muhammad Usman",
      "Muhammad Ahmad",
      "M. Shahiki Tash",
      "Irina Gelbukh",
      "Rolando Quintero Tellez",
      "Grigori Sidorov"
    ],
    "published": "2025-06-09T18:53:56+00:00",
    "summary": "Social media platforms are critical spaces for public discourse, shaping opinions and community dynamics, yet their widespread use has amplified harmful content, particularly hate speech, threatening online safety and inclusivity. While hate speech detection has been extensively studied in languages like English and Spanish, Urdu remains underexplored, especially using translation-based approaches. To address this gap, we introduce a trilingual dataset of 10,193 tweets in English (3,834 samples), Urdu (3,197 samples), and Spanish (3,162 samples), collected via keyword filtering, with a balanced distribution of 4,849 Hateful and 5,344 Not-Hateful labels. Our methodology leverages attention layers as a precursor to transformer-based models and large language models (LLMs), enhancing feature extraction for multilingual hate speech detection. For non-transformer models, we use TF-IDF for feature extraction. The dataset is benchmarked using state-of-the-art models, including GPT-3.5 Turbo and Qwen 2.5 72B, alongside traditional machine learning models like SVM and other transformers (e.g., BERT, RoBERTa). Three annotators, following rigorous guidelines, ensured high dataset quality, achieving a Fleiss' Kappa of 0.821. Our approach, integrating attention layers with GPT-3.5 Turbo and Qwen 2.5 72B, achieves strong performance, with macro F1 scores of 0.87 for English (GPT-3.5 Turbo), 0.85 for Spanish (GPT-3.5 Turbo), 0.81 for Urdu (Qwen 2.5 72B), and 0.88 for the joint multilingual model (Qwen 2.5 72B). These results reflect improvements of 8.75% in English (over SVM baseline 0.80), 8.97% in Spanish (over SVM baseline 0.78), 5.19% in Urdu (over SVM baseline 0.77), and 7.32% in the joint multilingual model (over SVM baseline 0.82). Our framework offers a robust solution for multilingual hate speech detection, fostering safer digital communities worldwide."
  },
  {
    "title": "Solving Inequality Proofs with Large Language Models",
    "url": "http://arxiv.org/abs/2506.07927v1",
    "arxiv_id": "2506.07927v1",
    "authors": [
      "Jiayi Sheng",
      "Luna Lyu",
      "Jikai Jin",
      "Tony Xia",
      "Alex Gu",
      "James Zou",
      "Pan Lu"
    ],
    "published": "2025-06-09T16:43:38+00:00",
    "summary": "Inequality proving, crucial across diverse scientific and mathematical fields, tests advanced reasoning skills such as discovering tight bounds and strategic theorem application. This makes it a distinct, demanding frontier for large language models (LLMs), offering insights beyond general mathematical problem-solving. Progress in this area is hampered by existing datasets that are often scarce, synthetic, or rigidly formal. We address this by proposing an informal yet verifiable task formulation, recasting inequality proving into two automatically checkable subtasks: bound estimation and relation prediction. Building on this, we release IneqMath, an expert-curated dataset of Olympiad-level inequalities, including a test set and training corpus enriched with step-wise solutions and theorem annotations. We also develop a novel LLM-as-judge evaluation framework, combining a final-answer judge with four step-wise judges designed to detect common reasoning flaws. A systematic evaluation of 29 leading LLMs on IneqMath reveals a surprising reality: even top models like o1 achieve less than 10% overall accuracy under step-wise scrutiny; this is a drop of up to 65.5% from their accuracy considering only final answer equivalence. This discrepancy exposes fragile deductive chains and a critical gap for current LLMs between merely finding an answer and constructing a rigorous proof. Scaling model size and increasing test-time computation yield limited gains in overall proof correctness. Instead, our findings highlight promising research directions such as theorem-guided reasoning and self-refinement. Code and data are available at https://ineqmath.github.io/."
  },
  {
    "title": "WeThink: Toward General-purpose Vision-Language Reasoning via Reinforcement Learning",
    "url": "http://arxiv.org/abs/2506.07905v1",
    "arxiv_id": "2506.07905v1",
    "authors": [
      "Jie Yang",
      "Feipeng Ma",
      "Zitian Wang",
      "Dacheng Yin",
      "Kang Rong",
      "Fengyun Rao",
      "Ruimao Zhang"
    ],
    "published": "2025-06-09T16:20:54+00:00",
    "summary": "Building on the success of text-based reasoning models like DeepSeek-R1, extending these capabilities to multimodal reasoning holds great promise. While recent works have attempted to adapt DeepSeek-R1-style reinforcement learning (RL) training paradigms to multimodal large language models (MLLM), focusing on domain-specific tasks like math and visual perception, a critical question remains: How can we achieve the general-purpose visual-language reasoning through RL? To address this challenge, we make three key efforts: (1) A novel Scalable Multimodal QA Synthesis pipeline that autonomously generates context-aware, reasoning-centric question-answer (QA) pairs directly from the given images. (2) The open-source WeThink dataset containing over 120K multimodal QA pairs with annotated reasoning paths, curated from 18 diverse dataset sources and covering various question domains. (3) A comprehensive exploration of RL on our dataset, incorporating a hybrid reward mechanism that combines rule-based verification with model-based assessment to optimize RL training efficiency across various task domains. Across 14 diverse MLLM benchmarks, we demonstrate that our WeThink dataset significantly enhances performance, from mathematical reasoning to diverse general multimodal tasks. Moreover, we show that our automated data pipeline can continuously increase data diversity to further improve model performance."
  },
  {
    "title": "Secure Distributed Learning for CAVs: Defending Against Gradient Leakage with Leveled Homomorphic Encryption",
    "url": "http://arxiv.org/abs/2506.07894v1",
    "arxiv_id": "2506.07894v1",
    "authors": [
      "Muhammad Ali Najjar",
      "Ren-Yi Huang",
      "Dumindu Samaraweera",
      "Prashant Shekhar"
    ],
    "published": "2025-06-09T16:12:18+00:00",
    "summary": "Federated Learning (FL) enables collaborative model training across distributed clients without sharing raw data, making it a promising approach for privacy-preserving machine learning in domains like Connected and Autonomous Vehicles (CAVs). However, recent studies have shown that exchanged model gradients remain susceptible to inference attacks such as Deep Leakage from Gradients (DLG), which can reconstruct private training data. While existing defenses like Differential Privacy (DP) and Secure Multi-Party Computation (SMPC) offer protection, they often compromise model accuracy. To that end, Homomorphic Encryption (HE) offers a promising alternative by enabling lossless computation directly on encrypted data, thereby preserving both privacy and model utility. However, HE introduces significant computational and communication overhead, which can hinder its practical adoption. To address this, we systematically evaluate various leveled HE schemes to identify the most suitable for FL in resource-constrained environments due to its ability to support fixed-depth computations without requiring costly bootstrapping. Our contributions in this paper include a comprehensive evaluation of HE schemes for real-world FL applications, a selective encryption strategy that targets only the most sensitive gradients to minimize computational overhead, and the development of a full HE-based FL pipeline that effectively mitigates DLG attacks while preserving model accuracy. We open-source our implementation to encourage reproducibility and facilitate adoption in safety-critical domains."
  },
  {
    "title": "CrosswalkNet: An Optimized Deep Learning Framework for Pedestrian Crosswalk Detection in Aerial Images with High-Performance Computing",
    "url": "http://arxiv.org/abs/2506.07885v1",
    "arxiv_id": "2506.07885v1",
    "authors": [
      "Zubin Bhuyan",
      "Yuanchang Xie",
      "AngkeaReach Rith",
      "Xintong Yan",
      "Nasko Apostolov",
      "Jimi Oke",
      "Chengbo Ai"
    ],
    "published": "2025-06-09T15:56:24+00:00",
    "summary": "With the increasing availability of aerial and satellite imagery, deep learning presents significant potential for transportation asset management, safety analysis, and urban planning. This study introduces CrosswalkNet, a robust and efficient deep learning framework designed to detect various types of pedestrian crosswalks from 15-cm resolution aerial images. CrosswalkNet incorporates a novel detection approach that improves upon traditional object detection strategies by utilizing oriented bounding boxes (OBB), enhancing detection precision by accurately capturing crosswalks regardless of their orientation. Several optimization techniques, including Convolutional Block Attention, a dual-branch Spatial Pyramid Pooling-Fast module, and cosine annealing, are implemented to maximize performance and efficiency. A comprehensive dataset comprising over 23,000 annotated crosswalk instances is utilized to train and validate the proposed framework. The best-performing model achieves an impressive precision of 96.5% and a recall of 93.3% on aerial imagery from Massachusetts, demonstrating its accuracy and effectiveness. CrosswalkNet has also been successfully applied to datasets from New Hampshire, Virginia, and Maine without transfer learning or fine-tuning, showcasing its robustness and strong generalization capability. Additionally, the crosswalk detection results, processed using High-Performance Computing (HPC) platforms and provided in polygon shapefile format, have been shown to accelerate data processing and detection, supporting real-time analysis for safety and mobility applications. This integration offers policymakers, transportation engineers, and urban planners an effective instrument to enhance pedestrian safety and improve urban mobility."
  },
  {
    "title": "R3D2: Realistic 3D Asset Insertion via Diffusion for Autonomous Driving Simulation",
    "url": "http://arxiv.org/abs/2506.07826v1",
    "arxiv_id": "2506.07826v1",
    "authors": [
      "William Ljungbergh",
      "Bernardo Taveira",
      "Wenzhao Zheng",
      "Adam Tonderski",
      "Chensheng Peng",
      "Fredrik Kahl",
      "Christoffer Petersson",
      "Michael Felsberg",
      "Kurt Keutzer",
      "Masayoshi Tomizuka",
      "Wei Zhan"
    ],
    "published": "2025-06-09T14:50:19+00:00",
    "summary": "Validating autonomous driving (AD) systems requires diverse and safety-critical testing, making photorealistic virtual environments essential. Traditional simulation platforms, while controllable, are resource-intensive to scale and often suffer from a domain gap with real-world data. In contrast, neural reconstruction methods like 3D Gaussian Splatting (3DGS) offer a scalable solution for creating photorealistic digital twins of real-world driving scenes. However, they struggle with dynamic object manipulation and reusability as their per-scene optimization-based methodology tends to result in incomplete object models with integrated illumination effects. This paper introduces R3D2, a lightweight, one-step diffusion model designed to overcome these limitations and enable realistic insertion of complete 3D assets into existing scenes by generating plausible rendering effects-such as shadows and consistent lighting-in real time. This is achieved by training R3D2 on a novel dataset: 3DGS object assets are generated from in-the-wild AD data using an image-conditioned 3D generative model, and then synthetically placed into neural rendering-based virtual environments, allowing R3D2 to learn realistic integration. Quantitative and qualitative evaluations demonstrate that R3D2 significantly enhances the realism of inserted assets, enabling use-cases like text-to-3D asset insertion and cross-scene/dataset object transfer, allowing for true scalability in AD validation. To promote further research in scalable and realistic AD simulation, we will release our dataset and code, see https://research.zenseact.com/publications/R3D2/."
  },
  {
    "title": "Looking Beyond Visible Cues: Implicit Video Question Answering via Dual-Clue Reasoning",
    "url": "http://arxiv.org/abs/2506.07811v1",
    "arxiv_id": "2506.07811v1",
    "authors": [
      "Tieyuan Chen",
      "Huabin Liu",
      "Yi Wang",
      "Chaofan Gan",
      "Mingxi Lyu",
      "Gui Zou",
      "Weiyao Lin"
    ],
    "published": "2025-06-09T14:38:14+00:00",
    "summary": "Video Question Answering (VideoQA) aims to answer natural language questions based on the given video, with prior work primarily focusing on identifying the duration of relevant segments, referred to as explicit visual evidence. However, explicit visual evidence is not always directly available, particularly when questions target symbolic meanings or deeper intentions, leading to significant performance degradation. To fill this gap, we introduce a novel task and dataset, $\\textbf{I}$mplicit $\\textbf{V}$ideo $\\textbf{Q}$uestion $\\textbf{A}$nswering (I-VQA), which focuses on answering questions in scenarios where explicit visual evidence is inaccessible. Given an implicit question and its corresponding video, I-VQA requires answering based on the contextual visual cues present within the video. To tackle I-VQA, we propose a novel reasoning framework, IRM (Implicit Reasoning Model), incorporating dual-stream modeling of contextual actions and intent clues as implicit reasoning chains. IRM comprises the Action-Intent Module (AIM) and the Visual Enhancement Module (VEM). AIM deduces and preserves question-related dual clues by generating clue candidates and performing relation deduction. VEM enhances contextual visual representation by leveraging key contextual clues. Extensive experiments validate the effectiveness of our IRM in I-VQA tasks, outperforming GPT-4o, OpenAI-o3, and fine-tuned VideoChat2 by $0.76\\%$, $1.37\\%$, and $4.87\\%$, respectively. Additionally, IRM performs SOTA on similar implicit advertisement understanding and future prediction in traffic-VQA. Datasets and codes are available for double-blind review in anonymous repo: https://github.com/tychen-SJTU/Implicit-VideoQA."
  },
  {
    "title": "Enhancing Adversarial Robustness with Conformal Prediction: A Framework for Guaranteed Model Reliability",
    "url": "http://arxiv.org/abs/2506.07804v1",
    "arxiv_id": "2506.07804v1",
    "authors": [
      "Jie Bao",
      "Chuangyin Dang",
      "Rui Luo",
      "Hanwei Zhang",
      "Zhixin Zhou"
    ],
    "published": "2025-06-09T14:33:28+00:00",
    "summary": "As deep learning models are increasingly deployed in high-risk applications, robust defenses against adversarial attacks and reliable performance guarantees become paramount. Moreover, accuracy alone does not provide sufficient assurance or reliable uncertainty estimates for these models. This study advances adversarial training by leveraging principles from Conformal Prediction. Specifically, we develop an adversarial attack method, termed OPSA (OPtimal Size Attack), designed to reduce the efficiency of conformal prediction at any significance level by maximizing model uncertainty without requiring coverage guarantees. Correspondingly, we introduce OPSA-AT (Adversarial Training), a defense strategy that integrates OPSA within a novel conformal training paradigm. Experimental evaluations demonstrate that our OPSA attack method induces greater uncertainty compared to baseline approaches for various defenses. Conversely, our OPSA-AT defensive model significantly enhances robustness not only against OPSA but also other adversarial attacks, and maintains reliable prediction. Our findings highlight the effectiveness of this integrated approach for developing trustworthy and resilient deep learning models for safety-critical domains. Our code is available at https://github.com/bjbbbb/Enhancing-Adversarial-Robustness-with-Conformal-Prediction."
  },
  {
    "title": "Deep Equivariant Multi-Agent Control Barrier Functions",
    "url": "http://arxiv.org/abs/2506.07755v1",
    "arxiv_id": "2506.07755v1",
    "authors": [
      "Nikolaos Bousias",
      "Lars Lindemann",
      "George Pappas"
    ],
    "published": "2025-06-09T13:37:29+00:00",
    "summary": "With multi-agent systems increasingly deployed autonomously at scale in complex environments, ensuring safety of the data-driven policies is critical. Control Barrier Functions have emerged as an effective tool for enforcing safety constraints, yet existing learning-based methods often lack in scalability, generalization and sampling efficiency as they overlook inherent geometric structures of the system. To address this gap, we introduce symmetries-infused distributed Control Barrier Functions, enforcing the satisfaction of intrinsic symmetries on learnable graph-based safety certificates. We theoretically motivate the need for equivariant parametrization of CBFs and policies, and propose a simple, yet efficient and adaptable methodology for constructing such equivariant group-modular networks via the compatible group actions. This approach encodes safety constraints in a distributed data-efficient manner, enabling zero-shot generalization to larger and denser swarms. Through extensive simulations on multi-robot navigation tasks, we demonstrate that our method outperforms state-of-the-art baselines in terms of safety, scalability, and task success rates, highlighting the importance of embedding symmetries in safe distributed neural policies."
  },
  {
    "title": "RSafe: Incentivizing proactive reasoning to build robust and adaptive LLM safeguards",
    "url": "http://arxiv.org/abs/2506.07736v1",
    "arxiv_id": "2506.07736v1",
    "authors": [
      "Jingnan Zheng",
      "Xiangtian Ji",
      "Yijun Lu",
      "Chenhang Cui",
      "Weixiang Zhao",
      "Gelei Deng",
      "Zhenkai Liang",
      "An Zhang",
      "Tat-Seng Chua"
    ],
    "published": "2025-06-09T13:20:04+00:00",
    "summary": "Large Language Models (LLMs) continue to exhibit vulnerabilities despite deliberate safety alignment efforts, posing significant risks to users and society. To safeguard against the risk of policy-violating content, system-level moderation via external guard models-designed to monitor LLM inputs and outputs and block potentially harmful content-has emerged as a prevalent mitigation strategy. Existing approaches of training guard models rely heavily on extensive human curated datasets and struggle with out-of-distribution threats, such as emerging harmful categories or jailbreak attacks. To address these limitations, we propose RSafe, an adaptive reasoning-based safeguard that conducts guided safety reasoning to provide robust protection within the scope of specified safety policies. RSafe operates in two stages: 1) guided reasoning, where it analyzes safety risks of input content through policy-guided step-by-step reasoning, and 2) reinforced alignment, where rule-based RL optimizes its reasoning paths to align with accurate safety prediction. This two-stage training paradigm enables RSafe to internalize safety principles to generalize safety protection capability over unseen or adversarial safety violation scenarios. During inference, RSafe accepts user-specified safety policies to provide enhanced safeguards tailored to specific safety requirements."
  },
  {
    "title": "A Communication-Latency-Aware Co-Simulation Platform for Safety and Comfort Evaluation of Cloud-Controlled ICVs",
    "url": "http://arxiv.org/abs/2506.07696v1",
    "arxiv_id": "2506.07696v1",
    "authors": [
      "Yongqi Zhao",
      "Xinrui Zhang",
      "Tomislav Mihalj",
      "Martin Schabauer",
      "Luis Putzer",
      "Erik Reichmann-Blaga",
      "\u00c1d\u00e1m Borony\u00e1k",
      "Andr\u00e1s R\u00f6vid",
      "G\u00e1bor So\u00f3s",
      "Peizhi Zhang",
      "Lu Xiong",
      "Jia Hu",
      "Arno Eichberger"
    ],
    "published": "2025-06-09T12:35:53+00:00",
    "summary": "Testing cloud-controlled intelligent connected vehicles (ICVs) requires simulation environments that faithfully emulate both vehicle behavior and realistic communication latencies. This paper proposes a latency-aware co-simulation platform integrating CarMaker and Vissim to evaluate safety and comfort under real-world vehicle-to-cloud (V2C) latency conditions. Two communication latency models, derived from empirical 5G measurements in China and Hungary, are incorporated and statistically modeled using Gamma distributions. A proactive conflict module (PCM) is proposed to dynamically control background vehicles and generate safety-critical scenarios. The platform is validated through experiments involving an exemplary system under test (SUT) across six testing conditions combining two PCM modes (enabled/disabled) and three latency conditions (none, China, Hungary). Safety and comfort are assessed using metrics including collision rate, distance headway, post-encroachment time, and the spectral characteristics of longitudinal acceleration. Results show that the PCM effectively increases driving environment criticality, while V2C latency primarily affects ride comfort. These findings confirm the platform's effectiveness in systematically evaluating cloud-controlled ICVs under diverse testing conditions."
  },
  {
    "title": "Evaluating LLMs Robustness in Less Resourced Languages with Proxy Models",
    "url": "http://arxiv.org/abs/2506.07645v1",
    "arxiv_id": "2506.07645v1",
    "authors": [
      "Maciej Chrab\u0105szcz",
      "Katarzyna Lorenc",
      "Karolina Seweryn"
    ],
    "published": "2025-06-09T11:09:39+00:00",
    "summary": "Large language models (LLMs) have demonstrated impressive capabilities across various natural language processing (NLP) tasks in recent years. However, their susceptibility to jailbreaks and perturbations necessitates additional evaluations. Many LLMs are multilingual, but safety-related training data contains mainly high-resource languages like English. This can leave them vulnerable to perturbations in low-resource languages such as Polish. We show how surprisingly strong attacks can be cheaply created by altering just a few characters and using a small proxy model for word importance calculation. We find that these character and word-level attacks drastically alter the predictions of different LLMs, suggesting a potential vulnerability that can be used to circumvent their internal safety mechanisms. We validate our attack construction methodology on Polish, a low-resource language, and find potential vulnerabilities of LLMs in this language. Additionally, we show how it can be extended to other languages. We release the created datasets and code for further research."
  },
  {
    "title": "TwinBreak: Jailbreaking LLM Security Alignments based on Twin Prompts",
    "url": "http://arxiv.org/abs/2506.07596v1",
    "arxiv_id": "2506.07596v1",
    "authors": [
      "Torsten Krau\u00df",
      "Hamid Dashtbani",
      "Alexandra Dmitrienko"
    ],
    "published": "2025-06-09T09:54:25+00:00",
    "summary": "Machine learning is advancing rapidly, with applications bringing notable benefits, such as improvements in translation and code generation. Models like ChatGPT, powered by Large Language Models (LLMs), are increasingly integrated into daily life. However, alongside these benefits, LLMs also introduce social risks. Malicious users can exploit LLMs by submitting harmful prompts, such as requesting instructions for illegal activities. To mitigate this, models often include a security mechanism that automatically rejects such harmful prompts. However, they can be bypassed through LLM jailbreaks. Current jailbreaks often require significant manual effort, high computational costs, or result in excessive model modifications that may degrade regular utility.   We introduce TwinBreak, an innovative safety alignment removal method. Building on the idea that the safety mechanism operates like an embedded backdoor, TwinBreak identifies and prunes parameters responsible for this functionality. By focusing on the most relevant model layers, TwinBreak performs fine-grained analysis of parameters essential to model utility and safety. TwinBreak is the first method to analyze intermediate outputs from prompts with high structural and content similarity to isolate safety parameters. We present the TwinPrompt dataset containing 100 such twin prompts. Experiments confirm TwinBreak's effectiveness, achieving 89% to 98% success rates with minimal computational requirements across 16 LLMs from five vendors."
  },
  {
    "title": "Lite-RVFL: A Lightweight Random Vector Functional-Link Neural Network for Learning Under Concept Drift",
    "url": "http://arxiv.org/abs/2506.08063v1",
    "arxiv_id": "2506.08063v1",
    "authors": [
      "Songqiao Hu",
      "Zeyi Liu",
      "Xiao He"
    ],
    "published": "2025-06-09T09:42:57+00:00",
    "summary": "The change in data distribution over time, also known as concept drift, poses a significant challenge to the reliability of online learning methods. Existing methods typically require model retraining or drift detection, both of which demand high computational costs and are often unsuitable for real-time applications. To address these limitations, a lightweight, fast and efficient random vector functional-link network termed Lite-RVFL is proposed, capable of adapting to concept drift without drift detection and retraining. Lite-RVFL introduces a novel objective function that assigns weights exponentially increasing to new samples, thereby emphasizing recent data and enabling timely adaptation. Theoretical analysis confirms the feasibility of this objective function for drift adaptation, and an efficient incremental update rule is derived. Experimental results on a real-world safety assessment task validate the efficiency, effectiveness in adapting to drift, and potential to capture temporal patterns of Lite-RVFL. The source code is available at https://github.com/songqiaohu/Lite-RVFL."
  },
  {
    "title": "Graph-of-Causal Evolution: Challenging Chain-of-Model for Reasoning",
    "url": "http://arxiv.org/abs/2506.07501v1",
    "arxiv_id": "2506.07501v1",
    "authors": [
      "Libo Wang"
    ],
    "published": "2025-06-09T07:26:47+00:00",
    "summary": "In view of the problem that each subchain in the chain-of-model (CoM) relies only on the information of the previous subchain and may lose long-range dependencies due to the causal mask blocking the global context flow between multi-level subchains, this work proposes a graph of causal evolution (GoCE). Its core principle is to map the implicit token representation into a differentiable and sparse causal adjacency matrix, then permeate causal constraints through each layer of calculation using causal-masked attention and causal-MoE. By combining intervention consistency loss test and self-evolution gate, the dynamic balance between causal structure learning and adaptive updating of transformer architecture is realized. The researcher built experimental environments in sandboxes built with Claude Sonnet 4, o4-mini-high, and DeepSeek R1 respectively with the transformer variant architecture introduced in GoCE. It is evaluated on publicly available datasets including CLUTRR, CLADDER, EX-FEVER, and CausalQA and compared with the baseline LLMs. The finding proves that GoCE strengthens the transformer's ability to capture long-range causal dependencies, while the ability to self-evolve is improved. It not only surpasses the design of CoM in terms of design principles, but also provides experience for future research on causal learning and continuous adaptive improvement."
  },
  {
    "title": "Chasing Moving Targets with Online Self-Play Reinforcement Learning for Safer Language Models",
    "url": "http://arxiv.org/abs/2506.07468v1",
    "arxiv_id": "2506.07468v1",
    "authors": [
      "Mickel Liu",
      "Liwei Jiang",
      "Yancheng Liang",
      "Simon Shaolei Du",
      "Yejin Choi",
      "Tim Althoff",
      "Natasha Jaques"
    ],
    "published": "2025-06-09T06:35:12+00:00",
    "summary": "Conventional language model (LM) safety alignment relies on a reactive, disjoint procedure: attackers exploit a static model, followed by defensive fine-tuning to patch exposed vulnerabilities. This sequential approach creates a mismatch -- attackers overfit to obsolete defenses, while defenders perpetually lag behind emerging threats. To address this, we propose Self-RedTeam, an online self-play reinforcement learning algorithm where an attacker and defender agent co-evolve through continuous interaction. We cast safety alignment as a two-player zero-sum game, where a single model alternates between attacker and defender roles -- generating adversarial prompts and safeguarding against them -- while a reward LM adjudicates outcomes. This enables dynamic co-adaptation. Grounded in the game-theoretic framework of zero-sum games, we establish a theoretical safety guarantee which motivates the design of our method: if self-play converges to a Nash Equilibrium, the defender will reliably produce safe responses to any adversarial input. Empirically, Self-RedTeam uncovers more diverse attacks (+21.8% SBERT) compared to attackers trained against static defenders and achieves higher robustness on safety benchmarks (e.g., +65.5% on WildJailBreak) than defenders trained against static attackers. We further propose hidden Chain-of-Thought, allowing agents to plan privately, which boosts adversarial diversity and reduces over-refusals. Our results motivate a shift from reactive patching to proactive co-evolution in LM safety training, enabling scalable, autonomous, and robust self-improvement of LMs via multi-agent reinforcement learning (MARL)."
  },
  {
    "title": "When Style Breaks Safety: Defending Language Models Against Superficial Style Alignment",
    "url": "http://arxiv.org/abs/2506.07452v1",
    "arxiv_id": "2506.07452v1",
    "authors": [
      "Yuxin Xiao",
      "Sana Tonekaboni",
      "Walter Gerych",
      "Vinith Suriyakumar",
      "Marzyeh Ghassemi"
    ],
    "published": "2025-06-09T05:57:39+00:00",
    "summary": "Large language models (LLMs) can be prompted with specific styles (e.g., formatting responses as lists), including in jailbreak queries. Although these style patterns are semantically unrelated to the malicious intents behind jailbreak queries, their safety impact remains unclear. In this work, we seek to understand whether style patterns compromise LLM safety, how superficial style alignment increases model vulnerability, and how best to mitigate these risks during alignment. We evaluate 32 LLMs across seven jailbreak benchmarks, and find that malicious queries with style patterns inflate the attack success rate (ASR) for nearly all models. Notably, ASR inflation correlates with both the length of style patterns and the relative attention an LLM exhibits on them. We then investigate superficial style alignment, and find that fine-tuning with specific styles makes LLMs more vulnerable to jailbreaks of those same styles. Finally, we propose SafeStyle, a defense strategy that incorporates a small amount of safety training data augmented to match the distribution of style patterns in the fine-tuning data. Across three LLMs and five fine-tuning style settings, SafeStyle consistently outperforms baselines in maintaining LLM safety."
  },
  {
    "title": "Prompt to Protection: A Comparative Study of Multimodal LLMs in Construction Hazard Recognition",
    "url": "http://arxiv.org/abs/2506.07436v1",
    "arxiv_id": "2506.07436v1",
    "authors": [
      "Nishi Chaudhary",
      "S M Jamil Uddin",
      "Sathvik Sharath Chandra",
      "Anto Ovid",
      "Alex Albert"
    ],
    "published": "2025-06-09T05:22:35+00:00",
    "summary": "The recent emergence of multimodal large language models (LLMs) has introduced new opportunities for improving visual hazard recognition on construction sites. Unlike traditional computer vision models that rely on domain-specific training and extensive datasets, modern LLMs can interpret and describe complex visual scenes using simple natural language prompts. However, despite growing interest in their applications, there has been limited investigation into how different LLMs perform in safety-critical visual tasks within the construction domain. To address this gap, this study conducts a comparative evaluation of five state-of-the-art LLMs: Claude-3 Opus, GPT-4.5, GPT-4o, GPT-o3, and Gemini 2.0 Pro, to assess their ability to identify potential hazards from real-world construction images. Each model was tested under three prompting strategies: zero-shot, few-shot, and chain-of-thought (CoT). Zero-shot prompting involved minimal instruction, few-shot incorporated basic safety context and a hazard source mnemonic, and CoT provided step-by-step reasoning examples to scaffold model thinking. Quantitative analysis was performed using precision, recall, and F1-score metrics across all conditions. Results reveal that prompting strategy significantly influenced performance, with CoT prompting consistently producing higher accuracy across models. Additionally, LLM performance varied under different conditions, with GPT-4.5 and GPT-o3 outperforming others in most settings. The findings also demonstrate the critical role of prompt design in enhancing the accuracy and consistency of multimodal LLMs for construction safety applications. This study offers actionable insights into the integration of prompt engineering and LLMs for practical hazard recognition, contributing to the development of more reliable AI-assisted safety systems."
  },
  {
    "title": "Beyond Jailbreaks: Revealing Stealthier and Broader LLM Security Risks Stemming from Alignment Failures",
    "url": "http://arxiv.org/abs/2506.07402v1",
    "arxiv_id": "2506.07402v1",
    "authors": [
      "Yukai Zhou",
      "Sibei Yang",
      "Wenjie Wang"
    ],
    "published": "2025-06-09T03:52:43+00:00",
    "summary": "Large language models (LLMs) are increasingly deployed in real-world applications, raising concerns about their security. While jailbreak attacks highlight failures under overtly harmful queries, they overlook a critical risk: incorrectly answering harmless-looking inputs can be dangerous and cause real-world harm (Implicit Harm). We systematically reformulate the LLM risk landscape through a structured quadrant perspective based on output factuality and input harmlessness, uncovering an overlooked high-risk region. To investigate this gap, we propose JailFlipBench, a benchmark aims to capture implicit harm, spanning single-modal, multimodal, and factual extension scenarios with diverse evaluation metrics. We further develop initial JailFlip attack methodologies and conduct comprehensive evaluations across multiple open-source and black-box LLMs, show that implicit harm present immediate and urgent real-world risks, calling for broader LLM safety assessments and alignment beyond conventional jailbreak paradigms."
  },
  {
    "title": "Refusal-Feature-guided Teacher for Safe Finetuning via Data Filtering and Alignment Distillation",
    "url": "http://arxiv.org/abs/2506.07356v1",
    "arxiv_id": "2506.07356v1",
    "authors": [
      "Seokil Ham",
      "Yubin Choi",
      "Seungju Cho",
      "Yujin Yang",
      "Younghun Kim",
      "Changick Kim"
    ],
    "published": "2025-06-09T02:10:51+00:00",
    "summary": "Recently, major AI service providers such as Google and OpenAI have introduced Finetuning-as-a-Service, which enables users to customize Large Language Models (LLMs) for specific downstream tasks using their own data. However, this service is vulnerable to degradation of LLM safety-alignment when user data contains harmful prompts. While some prior works address this issue, fundamentally filtering harmful data from user data remains unexplored. Motivated by our observation that a directional representation reflecting refusal behavior (called the refusal feature) obtained from safety-aligned LLMs can inherently distinguish between harmful and harmless prompts, we propose the Refusal-Feature-guided Teacher (ReFT). Our ReFT model is trained to identify harmful prompts based on the similarity between input prompt features and its refusal feature. During finetuning, the ReFT model serves as a teacher that filters harmful prompts from user data and distills alignment knowledge into the base model. Extensive experiments demonstrate that our ReFT-based finetuning strategy effectively minimizes harmful outputs and enhances finetuning accuracy for user-specific tasks, offering a practical solution for secure and reliable deployment of LLMs in Finetuning-as-a-Service."
  },
  {
    "title": "Distributed Risk-Sensitive Safety Filters for Uncertain Discrete-Time Systems",
    "url": "http://arxiv.org/abs/2506.07347v1",
    "arxiv_id": "2506.07347v1",
    "authors": [
      "Armin Lederer",
      "Erfaun Noorani",
      "Andreas Krause"
    ],
    "published": "2025-06-09T01:48:25+00:00",
    "summary": "Ensuring safety in multi-agent systems is a significant challenge, particularly in settings where centralized coordination is impractical. In this work, we propose a novel risk-sensitive safety filter for discrete-time multi-agent systems with uncertain dynamics that leverages control barrier functions (CBFs) defined through value functions. Our approach relies on centralized risk-sensitive safety conditions based on exponential risk operators to ensure robustness against model uncertainties. We introduce a distributed formulation of the safety filter by deriving two alternative strategies: one based on worst-case anticipation and another on proximity to a known safe policy. By allowing agents to switch between strategies, feasibility can be ensured. Through detailed numerical evaluations, we demonstrate the efficacy of our approach in maintaining safety without being overly conservative."
  },
  {
    "title": "ST-GraphNet: A Spatio-Temporal Graph Neural Network for Understanding and Predicting Automated Vehicle Crash Severity",
    "url": "http://arxiv.org/abs/2506.08051v1",
    "arxiv_id": "2506.08051v1",
    "authors": [
      "Mahmuda Sultana Mimi",
      "Md Monzurul Islam",
      "Anannya Ghosh Tusti",
      "Shriyank Somvanshi",
      "Subasish Das"
    ],
    "published": "2025-06-09T01:42:19+00:00",
    "summary": "Understanding the spatial and temporal dynamics of automated vehicle (AV) crash severity is critical for advancing urban mobility safety and infrastructure planning. In this work, we introduce ST-GraphNet, a spatio-temporal graph neural network framework designed to model and predict AV crash severity by using both fine-grained and region-aggregated spatial graphs. Using a balanced dataset of 2,352 real-world AV-related crash reports from Texas (2024), including geospatial coordinates, crash timestamps, SAE automation levels, and narrative descriptions, we construct two complementary graph representations: (1) a fine-grained graph with individual crash events as nodes, where edges are defined via spatio-temporal proximity; and (2) a coarse-grained graph where crashes are aggregated into Hexagonal Hierarchical Spatial Indexing (H3)-based spatial cells, connected through hexagonal adjacency. Each node in the graph is enriched with multimodal data, including semantic, spatial, and temporal attributes, including textual embeddings from crash narratives using a pretrained Sentence-BERT model. We evaluate various graph neural network (GNN) architectures, such as Graph Convolutional Networks (GCN), Graph Attention Networks (GAT), and Dynamic Spatio-Temporal GCN (DSTGCN), to classify crash severity and predict high-risk regions. Our proposed ST-GraphNet, which utilizes a DSTGCN backbone on the coarse-grained H3 graph, achieves a test accuracy of 97.74\\%, substantially outperforming the best fine-grained model (64.7\\% test accuracy). These findings highlight the effectiveness of spatial aggregation, dynamic message passing, and multi-modal feature integration in capturing the complex spatio-temporal patterns underlying AV crash severity."
  },
  {
    "title": "BR-MPPI: Barrier Rate guided MPPI for Enforcing Multiple Inequality Constraints with Learned Signed Distance Field",
    "url": "http://arxiv.org/abs/2506.07325v1",
    "arxiv_id": "2506.07325v1",
    "authors": [
      "Hardik Parwana",
      "Taekyung Kim",
      "Kehan Long",
      "Bardh Hoxha",
      "Hideki Okamoto",
      "Georgios Fainekos",
      "Dimitra Panagou"
    ],
    "published": "2025-06-08T23:45:14+00:00",
    "summary": "Model Predictive Path Integral (MPPI) controller is used to solve unconstrained optimal control problems and Control Barrier Function (CBF) is a tool to impose strict inequality constraints, a.k.a, barrier constraints. In this work, we propose an integration of these two methods that employ CBF-like conditions to guide the control sampling procedure of MPPI. CBFs provide an inequality constraint restricting the rate of change of barrier functions by a classK function of the barrier itself. We instead impose the CBF condition as an equality constraint by choosing a parametric linear classK function and treating this parameter as a state in an augmented system. The time derivative of this parameter acts as an additional control input that is designed by MPPI. A cost function is further designed to reignite Nagumo's theorem at the boundary of the safe set by promoting specific values of classK parameter to enforce safety. Our problem formulation results in an MPPI subject to multiple state and control-dependent equality constraints which are non-trivial to satisfy with randomly sampled control inputs. We therefore also introduce state transformations and control projection operations, inspired by the literature on path planning for manifolds, to resolve the aforementioned issue. We show empirically through simulations and experiments on quadrotor that our proposed algorithm exhibits better sampled efficiency and enhanced capability to operate closer to the safe set boundary over vanilla MPPI."
  },
  {
    "title": "Adultification Bias in LLMs and Text-to-Image Models",
    "url": "http://arxiv.org/abs/2506.07282v1",
    "arxiv_id": "2506.07282v1",
    "authors": [
      "Jane Castleman",
      "Aleksandra Korolova"
    ],
    "published": "2025-06-08T21:02:33+00:00",
    "summary": "The rapid adoption of generative AI models in domains such as education, policing, and social media raises significant concerns about potential bias and safety issues, particularly along protected attributes, such as race and gender, and when interacting with minors. Given the urgency of facilitating safe interactions with AI systems, we study bias along axes of race and gender in young girls. More specifically, we focus on \"adultification bias,\" a phenomenon in which Black girls are presumed to be more defiant, sexually intimate, and culpable than their White peers. Advances in alignment techniques show promise towards mitigating biases but vary in their coverage and effectiveness across models and bias types. Therefore, we measure explicit and implicit adultification bias in widely used LLMs and text-to-image (T2I) models, such as OpenAI, Meta, and Stability AI models. We find that LLMs exhibit explicit and implicit adultification bias against Black girls, assigning them harsher, more sexualized consequences in comparison to their White peers. Additionally, we find that T2I models depict Black girls as older and wearing more revealing clothing than their White counterparts, illustrating how adultification bias persists across modalities. We make three key contributions: (1) we measure a new form of bias in generative AI models, (2) we systematically study adultification bias across modalities, and (3) our findings emphasize that current alignment methods are insufficient for comprehensively addressing bias. Therefore, new alignment methods that address biases such as adultification are needed to ensure safe and equitable AI deployment."
  },
  {
    "title": "Active Lubrication of Transluminal Medical Instruments",
    "url": "http://arxiv.org/abs/2506.07225v1",
    "arxiv_id": "2506.07225v1",
    "authors": [
      "Mostafa A. Atalla",
      "Jelte Nieuwenhuis",
      "Alan Martin",
      "Xuan Wang",
      "Ahranee Canden",
      "Matt J. Carr\u00e9",
      "Roger Lewis",
      "Aim\u00e9e Sakes",
      "Micha\u00ebl Wiertlewski"
    ],
    "published": "2025-06-08T17:18:29+00:00",
    "summary": "Transluminal minimally invasive surgery uses natural orifices and small incisions to access internal anatomical structures, promoting quicker recovery and reduced morbidity. However, navigating instruments--catheters and endoscopes--through anatomical pathways creates frictional interactions with luminal walls, risking complications such as perforation, poor haptic feedback, and instrument buckling. In this paper, we present a new approach to actively lubricate transluminal instruments and dynamically reduce friction with surrounding tissues. This approach employs ultrasonic vibrations, at the instrument surface, to generate a pressurized fluid layer at the contact interface, lubricating the interface and thereby reducing friction. We implemented this approach in a prototype catheter, which we validated under dry and liquid-lubricated conditions, across rigid and soft interfaces, and along varied anatomical curvatures. In a cardiac catheter use case, active lubrication reduced friction by up to 42% on ex-vivo porcine aorta tissue and 82% on rigid substrates, denoting its potential performance on healthy and calcified tissue, respectively. Thermal imaging confirmed that temperature at the tissue-catheter interface remained within safe limits. Additionally, the system effectively prevented buckling during catheter insertion experiment, further showcasing its potential. By minimizing injury risk and enhancing procedural stability, active lubrication can drastically enhance the safety and efficacy of transluminal interventions."
  },
  {
    "title": "SAP-Bench: Benchmarking Multimodal Large Language Models in Surgical Action Planning",
    "url": "http://arxiv.org/abs/2506.07196v1",
    "arxiv_id": "2506.07196v1",
    "authors": [
      "Mengya Xu",
      "Zhongzhen Huang",
      "Dillan Imans",
      "Yiru Ye",
      "Xiaofan Zhang",
      "Qi Dou"
    ],
    "published": "2025-06-08T15:30:04+00:00",
    "summary": "Effective evaluation is critical for driving advancements in MLLM research. The surgical action planning (SAP) task, which aims to generate future action sequences from visual inputs, demands precise and sophisticated analytical capabilities. Unlike mathematical reasoning, surgical decision-making operates in life-critical domains and requires meticulous, verifiable processes to ensure reliability and patient safety. This task demands the ability to distinguish between atomic visual actions and coordinate complex, long-horizon procedures, capabilities that are inadequately evaluated by current benchmarks. To address this gap, we introduce SAP-Bench, a large-scale, high-quality dataset designed to enable multimodal large language models (MLLMs) to perform interpretable surgical action planning. Our SAP-Bench benchmark, derived from the cholecystectomy procedures context with the mean duration of 1137.5s, and introduces temporally-grounded surgical action annotations, comprising the 1,226 clinically validated action clips (mean duration: 68.7s) capturing five fundamental surgical actions across 74 procedures. The dataset provides 1,152 strategically sampled current frames, each paired with the corresponding next action as multimodal analysis anchors. We propose the MLLM-SAP framework that leverages MLLMs to generate next action recommendations from the current surgical scene and natural language instructions, enhanced with injected surgical domain knowledge. To assess our dataset's effectiveness and the broader capabilities of current models, we evaluate seven state-of-the-art MLLMs (e.g., OpenAI-o1, GPT-4o, QwenVL2.5-72B, Claude-3.5-Sonnet, GeminiPro2.5, Step-1o, and GLM-4v) and reveal critical gaps in next action prediction performance."
  },
  {
    "title": "Hierarchical Feature-level Reverse Propagation for Post-Training Neural Networks",
    "url": "http://arxiv.org/abs/2506.07188v1",
    "arxiv_id": "2506.07188v1",
    "authors": [
      "Ni Ding",
      "Lei He",
      "Shengbo Eben Li",
      "Keqiang Li"
    ],
    "published": "2025-06-08T15:19:03+00:00",
    "summary": "End-to-end autonomous driving has emerged as a dominant paradigm, yet its highly entangled black-box models pose significant challenges in terms of interpretability and safety assurance. To improve model transparency and training flexibility, this paper proposes a hierarchical and decoupled post-training framework tailored for pretrained neural networks. By reconstructing intermediate feature maps from ground-truth labels, surrogate supervisory signals are introduced at transitional layers to enable independent training of specific components, thereby avoiding the complexity and coupling of conventional end-to-end backpropagation and providing interpretable insights into networks' internal mechanisms. To the best of our knowledge, this is the first method to formalize feature-level reverse computation as well-posed optimization problems, which we rigorously reformulate as systems of linear equations or least squares problems. This establishes a novel and efficient training paradigm that extends gradient backpropagation to feature backpropagation. Extensive experiments on multiple standard image classification benchmarks demonstrate that the proposed method achieves superior generalization performance and computational efficiency compared to traditional training approaches, validating its effectiveness and potential."
  },
  {
    "title": "Translating Federated Learning Algorithms in Python into CSP Processes Using ChatGPT",
    "url": "http://arxiv.org/abs/2506.07173v1",
    "arxiv_id": "2506.07173v1",
    "authors": [
      "Miroslav Popovic",
      "Marko Popovic",
      "Miodrag Djukic",
      "Ilija Basicevic"
    ],
    "published": "2025-06-08T14:45:11+00:00",
    "summary": "The Python Testbed for Federated Learning Algorithms is a simple Python FL framework that is easy to use by ML&AI developers who do not need to be professional programmers and is also amenable to LLMs. In the previous research, generic federated learning algorithms provided by this framework were manually translated into the CSP processes and algorithms' safety and liveness properties were automatically verified by the model checker PAT. In this paper, a simple translation process is introduced wherein the ChatGPT is used to automate the translation of the mentioned federated learning algorithms in Python into the corresponding CSP processes. Within the process, the minimality of the used context is estimated based on the feedback from ChatGPT. The proposed translation process was experimentally validated by successful translation (verified by the model checker PAT) of both generic centralized and decentralized federated learning algorithms."
  },
  {
    "title": "Mind the Web: The Security of Web Use Agents",
    "url": "http://arxiv.org/abs/2506.07153v1",
    "arxiv_id": "2506.07153v1",
    "authors": [
      "Avishag Shapira",
      "Parth Atulbhai Gandhi",
      "Edan Habler",
      "Oleg Brodt",
      "Asaf Shabtai"
    ],
    "published": "2025-06-08T13:59:55+00:00",
    "summary": "Web-use agents are rapidly being deployed to automate complex web tasks, operating with extensive browser capabilities including multi-tab navigation, DOM manipulation, JavaScript execution and authenticated session access. However, these powerful capabilities create a critical and previously unexplored attack surface. This paper demonstrates how attackers can exploit web-use agents' high-privilege capabilities by embedding malicious content in web pages such as comments, reviews, or advertisements that agents encounter during legitimate browsing tasks. In addition, we introduce the task-aligned injection technique that frame malicious commands as helpful task guidance rather than obvious attacks. This technique exploiting fundamental limitations in LLMs' contextual reasoning: agents struggle in maintaining coherent contextual awareness and fail to detect when seemingly helpful web content contains steering attempts that deviate from their original task goal. Through systematic evaluation of four popular agents (OpenAI Operator, Browser Use, Do Browser, OpenOperator), we demonstrate nine payload types that compromise confidentiality, integrity, and availability, including unauthorized camera activation, user impersonation, local file exfiltration, password leakage, and denial of service, with validation across multiple LLMs achieving success rates of 80%-100%. These payloads succeed across agents with built-in safety mechanisms, requiring only the ability to post content on public websites, creating unprecedented risks given the ease of exploitation combined with agents' high-privilege access. To address this attack, we propose comprehensive mitigation strategies including oversight mechanisms, execution constraints, and task-aware reasoning techniques, providing practical directions for secure development and deployment."
  },
  {
    "title": "Quality-Diversity Red-Teaming: Automated Generation of High-Quality and Diverse Attackers for Large Language Models",
    "url": "http://arxiv.org/abs/2506.07121v1",
    "arxiv_id": "2506.07121v1",
    "authors": [
      "Ren-Jian Wang",
      "Ke Xue",
      "Zeyu Qin",
      "Ziniu Li",
      "Sheng Tang",
      "Hao-Tian Li",
      "Shengcai Liu",
      "Chao Qian"
    ],
    "published": "2025-06-08T13:07:41+00:00",
    "summary": "Ensuring safety of large language models (LLMs) is important. Red teaming--a systematic approach to identifying adversarial prompts that elicit harmful responses from target LLMs--has emerged as a crucial safety evaluation method. Within this framework, the diversity of adversarial prompts is essential for comprehensive safety assessments. We find that previous approaches to red-teaming may suffer from two key limitations. First, they often pursue diversity through simplistic metrics like word frequency or sentence embedding similarity, which may not capture meaningful variation in attack strategies. Second, the common practice of training a single attacker model restricts coverage across potential attack styles and risk categories. This paper introduces Quality-Diversity Red-Teaming (QDRT), a new framework designed to address these limitations. QDRT achieves goal-driven diversity through behavior-conditioned training and implements a behavioral replay buffer in an open-ended manner. Additionally, it trains multiple specialized attackers capable of generating high-quality attacks across diverse styles and risk categories. Our empirical evaluation demonstrates that QDRT generates attacks that are both more diverse and more effective against a wide range of target LLMs, including GPT-2, Llama-3, Gemma-2, and Qwen2.5. This work advances the field of LLM safety by providing a systematic and effective approach to automated red-teaming, ultimately supporting the responsible deployment of LLMs."
  },
  {
    "title": "On the Generalization of Data-Assisted Control in port-Hamiltonian Systems (DAC-pH)",
    "url": "http://arxiv.org/abs/2506.07079v1",
    "arxiv_id": "2506.07079v1",
    "authors": [
      "Mostafa Eslami",
      "Maryam Babazadeh"
    ],
    "published": "2025-06-08T10:44:01+00:00",
    "summary": "This paper introduces a hypothetical hybrid control framework for port-Hamiltonian (p$\\mathcal{H}$) systems, employing a dynamic decomposition based on Data-Assisted Control (DAC). The system's evolution is split into two parts with fixed topology: Right-Hand Side (RHS)- an intrinsic Hamiltonian flow handling worst-case parametric uncertainties, and Left-Hand Side (LHS)- a dissipative/input flow addressing both structural and parametric uncertainties. A virtual port variable $\\Pi$ serves as the interface between these two components. A nonlinear controller manages the intrinsic Hamiltonian flow, determining a desired port control value $\\Pi_c$. Concurrently, Reinforcement Learning (RL) is applied to the dissipative/input flow to learn an agent for providing optimal policy in mapping $\\Pi_c$ to the actual system input. This hybrid approach effectively manages RHS uncertainties while preserving the system's inherent structure. Key advantages include adjustable performance via LHS controller parameters, enhanced AI explainability and interpretability through the port variable $\\Pi$, the ability to guarantee safety and state attainability with hard/soft constraints, reduced complexity in learning hypothesis classes compared to end-to-end solutions, and improved state/parameter estimation using LHS prior knowledge and system Hamiltonian to address partial observability. The paper details the p$\\mathcal{H}$ formulation, derives the decomposition, and presents the modular controller architecture. Beyond design, crucial aspects of stability and robustness analysis and synthesis are investigated, paving the way for deeper theoretical investigations. An application example, a pendulum with nonlinear dynamics, is simulated to demonstrate the approach's empirical and phenomenological benefits for future research."
  },
  {
    "title": "HauntAttack: When Attack Follows Reasoning as a Shadow",
    "url": "http://arxiv.org/abs/2506.07031v1",
    "arxiv_id": "2506.07031v1",
    "authors": [
      "Jingyuan Ma",
      "Rui Li",
      "Zheng Li",
      "Junfeng Liu",
      "Lei Sha",
      "Zhifang Sui"
    ],
    "published": "2025-06-08T07:45:48+00:00",
    "summary": "Emerging Large Reasoning Models (LRMs) consistently excel in mathematical and reasoning tasks, showcasing exceptional capabilities. However, the enhancement of reasoning abilities and the exposure of their internal reasoning processes introduce new safety vulnerabilities. One intriguing concern is: when reasoning is strongly entangled with harmfulness, what safety-reasoning trade-off do LRMs exhibit? To address this issue, we introduce HauntAttack, a novel and general-purpose black-box attack framework that systematically embeds harmful instructions into reasoning questions. Specifically, we treat reasoning questions as carriers and substitute one of their original conditions with a harmful instruction. This process creates a reasoning pathway in which the model is guided step by step toward generating unsafe outputs. Based on HauntAttack, we conduct comprehensive experiments on multiple LRMs. Our results reveal that even the most advanced LRMs exhibit significant safety vulnerabilities. Additionally, we perform a detailed analysis of different models, various types of harmful instructions, and model output patterns, providing valuable insights into the security of LRMs."
  },
  {
    "title": "AlphaSteer: Learning Refusal Steering with Principled Null-Space Constraint",
    "url": "http://arxiv.org/abs/2506.07022v1",
    "arxiv_id": "2506.07022v1",
    "authors": [
      "Leheng Sheng",
      "Changshuo Shen",
      "Weixiang Zhao",
      "Junfeng Fang",
      "Xiaohao Liu",
      "Zhenkai Liang",
      "Xiang Wang",
      "An Zhang",
      "Tat-Seng Chua"
    ],
    "published": "2025-06-08T07:03:28+00:00",
    "summary": "As LLMs are increasingly deployed in real-world applications, ensuring their ability to refuse malicious prompts, especially jailbreak attacks, is essential for safe and reliable use. Recently, activation steering has emerged as an effective approach for enhancing LLM safety by adding a refusal direction vector to internal activations of LLMs during inference, which will further induce the refusal behaviors of LLMs. However, indiscriminately applying activation steering fundamentally suffers from the trade-off between safety and utility, since the same steering vector can also lead to over-refusal and degraded performance on benign prompts. Although prior efforts, such as vector calibration and conditional steering, have attempted to mitigate this trade-off, their lack of theoretical grounding limits their robustness and effectiveness. To better address the trade-off between safety and utility, we present a theoretically grounded and empirically effective activation steering method called AlphaSteer. Specifically, it considers activation steering as a learnable process with two principled learning objectives: utility preservation and safety enhancement. For utility preservation, it learns to construct a nearly zero vector for steering benign data, with the null-space constraints. For safety enhancement, it learns to construct a refusal direction vector for steering malicious data, with the help of linear regression. Experiments across multiple jailbreak attacks and utility benchmarks demonstrate the effectiveness of AlphaSteer, which significantly improves the safety of LLMs without compromising general capabilities. Our codes are available at https://github.com/AlphaLab-USTC/AlphaSteer."
  },
  {
    "title": "Hierarchical Intention Tracking with Switching Trees for Real-Time Adaptation to Dynamic Human Intentions during Collaboration",
    "url": "http://arxiv.org/abs/2506.07004v1",
    "arxiv_id": "2506.07004v1",
    "authors": [
      "Zhe Huang",
      "Ye-Ji Mun",
      "Fatemeh Cheraghi Pouria",
      "Katherine Driggs-Campbell"
    ],
    "published": "2025-06-08T05:30:36+00:00",
    "summary": "During collaborative tasks, human behavior is guided by multiple levels of intentions that evolve over time, such as task sequence preferences and interaction strategies. To adapt to these changing preferences and promptly correct any inaccurate estimations, collaborative robots must accurately track these dynamic human intentions in real time. We propose a Hierarchical Intention Tracking (HIT) algorithm for collaborative robots to track dynamic and hierarchical human intentions effectively in real time. HIT represents human intentions as intention trees with arbitrary depth, and probabilistically tracks human intentions by Bayesian filtering, upward measurement propagation, and downward posterior propagation across all levels. We develop a HIT-based robotic system that dynamically switches between Interaction-Task and Verification-Task trees for a collaborative assembly task, allowing the robot to effectively coordinate human intentions at three levels: task-level (subtask goal locations), interaction-level (mode of engagement with the robot), and verification-level (confirming or correcting intention recognition). Our user study shows that our HIT-based collaborative robot system surpasses existing collaborative robot solutions by achieving a balance between efficiency, physical workload, and user comfort while ensuring safety and task completion. Post-experiment surveys further reveal that the HIT-based system enhances the user trust and minimizes interruptions to user's task flow through its effective understanding of human intentions across multiple levels."
  },
  {
    "title": "Auditing Black-Box LLM APIs with a Rank-Based Uniformity Test",
    "url": "http://arxiv.org/abs/2506.06975v1",
    "arxiv_id": "2506.06975v1",
    "authors": [
      "Xiaoyuan Zhu",
      "Yaowen Ye",
      "Tianyi Qiu",
      "Hanlin Zhu",
      "Sijun Tan",
      "Ajraf Mannan",
      "Jonathan Michala",
      "Raluca Ada Popa",
      "Willie Neiswanger"
    ],
    "published": "2025-06-08T03:00:31+00:00",
    "summary": "As API access becomes a primary interface to large language models (LLMs), users often interact with black-box systems that offer little transparency into the deployed model. To reduce costs or maliciously alter model behaviors, API providers may discreetly serve quantized or fine-tuned variants, which can degrade performance and compromise safety. Detecting such substitutions is difficult, as users lack access to model weights and, in most cases, even output logits. To tackle this problem, we propose a rank-based uniformity test that can verify the behavioral equality of a black-box LLM to a locally deployed authentic model. Our method is accurate, query-efficient, and avoids detectable query patterns, making it robust to adversarial providers that reroute or mix responses upon the detection of testing attempts. We evaluate the approach across diverse threat scenarios, including quantization, harmful fine-tuning, jailbreak prompts, and full model substitution, showing that it consistently achieves superior statistical power over prior methods under constrained query budgets."
  },
  {
    "title": "BIS Reasoning 1.0: The First Large-Scale Japanese Benchmark for Belief-Inconsistent Syllogistic Reasoning",
    "url": "http://arxiv.org/abs/2506.06955v1",
    "arxiv_id": "2506.06955v1",
    "authors": [
      "Ha-Thanh Nguyen",
      "Chaoran Liu",
      "Hirokazu Kiyomaru",
      "Koichi Takeda",
      "Yusuke Miyao",
      "Maki Matsuda",
      "Yusuke Oda",
      "Pontus Stenetorp",
      "Qianying Liu",
      "Su Myat Noe",
      "Hideyuki Tachibana",
      "Kouta Nakayama",
      "Sadao Kurohashi"
    ],
    "published": "2025-06-08T00:38:18+00:00",
    "summary": "We present BIS Reasoning 1.0, the first large-scale Japanese dataset of syllogistic reasoning problems explicitly designed to evaluate belief-inconsistent reasoning in large language models (LLMs). Unlike prior datasets such as NeuBAROCO and JFLD, which focus on general or belief-aligned reasoning, BIS Reasoning 1.0 introduces logically valid yet belief-inconsistent syllogisms to uncover reasoning biases in LLMs trained on human-aligned corpora. We benchmark state-of-the-art models - including GPT models, Claude models, and leading Japanese LLMs - revealing significant variance in performance, with GPT-4o achieving 79.54% accuracy. Our analysis identifies critical weaknesses in current LLMs when handling logically valid but belief-conflicting inputs. These findings have important implications for deploying LLMs in high-stakes domains such as law, healthcare, and scientific literature, where truth must override intuitive belief to ensure integrity and safety."
  },
  {
    "title": "Safety-Aware Reinforcement Learning for Control via Risk-Sensitive Action-Value Iteration and Quantile Regression",
    "url": "http://arxiv.org/abs/2506.06954v1",
    "arxiv_id": "2506.06954v1",
    "authors": [
      "Clinton Enwerem",
      "Aniruddh G. Puranic",
      "John S. Baras",
      "Calin Belta"
    ],
    "published": "2025-06-08T00:22:00+00:00",
    "summary": "Mainstream approximate action-value iteration reinforcement learning (RL) algorithms suffer from overestimation bias, leading to suboptimal policies in high-variance stochastic environments. Quantile-based action-value iteration methods reduce this bias by learning a distribution of the expected cost-to-go using quantile regression. However, ensuring that the learned policy satisfies safety constraints remains a challenge when these constraints are not explicitly integrated into the RL framework. Existing methods often require complex neural architectures or manual tradeoffs due to combined cost functions. To address this, we propose a risk-regularized quantile-based algorithm integrating Conditional Value-at-Risk (CVaR) to enforce safety without complex architectures. We also provide theoretical guarantees on the contraction properties of the risk-sensitive distributional Bellman operator in Wasserstein space, ensuring convergence to a unique cost distribution. Simulations of a mobile robot in a dynamic reach-avoid task show that our approach leads to more goal successes, fewer collisions, and better safety-performance trade-offs compared to risk-neutral methods."
  },
  {
    "title": "Towards Data-Driven Model-Free Safety-Critical Control",
    "url": "http://arxiv.org/abs/2506.06931v1",
    "arxiv_id": "2506.06931v1",
    "authors": [
      "Zhe Shen",
      "Yitaek Kim",
      "Christoffer Sloth"
    ],
    "published": "2025-06-07T22:01:10+00:00",
    "summary": "This paper presents a framework for enabling safe velocity control of general robotic systems using data-driven model-free Control Barrier Functions (CBFs). Model-free CBFs rely on an exponentially stable velocity controller and a design parameter (e.g. alpha in CBFs); this design parameter depends on the exponential decay rate of the controller. However, in practice, the decay rate is often unavailable, making it non-trivial to use model-free CBFs, as it requires manual tuning for alpha. To address this, a Neural Network is used to learn the Lyapunov function from data, and the maximum decay rate of the systems built-in velocity controller is subsequently estimated. Furthermore, to integrate the estimated decay rate with model-free CBFs, we derive a probabilistic safety condition that incorporates a confidence bound on the violation rate of the exponential stability condition, using Chernoff bound. This enhances robustness against uncertainties in stability violations. The proposed framework has been tested on a UR5e robot in multiple experimental settings, and its effectiveness in ensuring safe velocity control with model-free CBFs has been demonstrated."
  },
  {
    "title": "Incorporating Failure of Machine Learning in Dynamic Probabilistic Safety Assurance",
    "url": "http://arxiv.org/abs/2506.06868v1",
    "arxiv_id": "2506.06868v1",
    "authors": [
      "Razieh Arshadizadeh",
      "Mahmoud Asgari",
      "Zeinab Khosravi",
      "Yiannis Papadopoulos",
      "Koorosh Aslansefat"
    ],
    "published": "2025-06-07T17:16:05+00:00",
    "summary": "Machine Learning (ML) models are increasingly integrated into safety-critical systems, such as autonomous vehicle platooning, to enable real-time decision-making. However, their inherent imperfection introduces a new class of failure: reasoning failures often triggered by distributional shifts between operational and training data. Traditional safety assessment methods, which rely on design artefacts or code, are ill-suited for ML components that learn behaviour from data. SafeML was recently proposed to dynamically detect such shifts and assign confidence levels to the reasoning of ML-based components. Building on this, we introduce a probabilistic safety assurance framework that integrates SafeML with Bayesian Networks (BNs) to model ML failures as part of a broader causal safety analysis. This allows for dynamic safety evaluation and system adaptation under uncertainty. We demonstrate the approach on an simulated automotive platooning system with traffic sign recognition. The findings highlight the potential broader benefits of explicitly modelling ML failures in safety assessment."
  },
  {
    "title": "EndoARSS: Adapting Spatially-Aware Foundation Model for Efficient Activity Recognition and Semantic Segmentation in Endoscopic Surgery",
    "url": "http://arxiv.org/abs/2506.06830v1",
    "arxiv_id": "2506.06830v1",
    "authors": [
      "Guankun Wang",
      "Rui Tang",
      "Mengya Xu",
      "Long Bai",
      "Huxin Gao",
      "Hongliang Ren"
    ],
    "published": "2025-06-07T15:18:43+00:00",
    "summary": "Endoscopic surgery is the gold standard for robotic-assisted minimally invasive surgery, offering significant advantages in early disease detection and precise interventions. However, the complexity of surgical scenes, characterized by high variability in different surgical activity scenarios and confused image features between targets and the background, presents challenges for surgical environment understanding. Traditional deep learning models often struggle with cross-activity interference, leading to suboptimal performance in each downstream task. To address this limitation, we explore multi-task learning, which utilizes the interrelated features between tasks to enhance overall task performance. In this paper, we propose EndoARSS, a novel multi-task learning framework specifically designed for endoscopy surgery activity recognition and semantic segmentation. Built upon the DINOv2 foundation model, our approach integrates Low-Rank Adaptation to facilitate efficient fine-tuning while incorporating Task Efficient Shared Low-Rank Adapters to mitigate gradient conflicts across diverse tasks. Additionally, we introduce the Spatially-Aware Multi-Scale Attention that enhances feature representation discrimination by enabling cross-spatial learning of global information. In order to evaluate the effectiveness of our framework, we present three novel datasets, MTLESD, MTLEndovis and MTLEndovis-Gen, tailored for endoscopic surgery scenarios with detailed annotations for both activity recognition and semantic segmentation tasks. Extensive experiments demonstrate that EndoARSS achieves remarkable performance across multiple benchmarks, significantly improving both accuracy and robustness in comparison to existing models. These results underscore the potential of EndoARSS to advance AI-driven endoscopic surgical systems, offering valuable insights for enhancing surgical safety and efficiency."
  },
  {
    "title": "Can LLMs Generate Reliable Test Case Generators? A Study on Competition-Level Programming Problems",
    "url": "http://arxiv.org/abs/2506.06821v1",
    "arxiv_id": "2506.06821v1",
    "authors": [
      "Yuhan Cao",
      "Zian Chen",
      "Kun Quan",
      "Ziliang Zhang",
      "Yu Wang",
      "Xiaoning Dong",
      "Yeqi Feng",
      "Guanzhong He",
      "Jingcheng Huang",
      "Jianhao Li",
      "Yixuan Tan",
      "Jiafu Tang",
      "Yilin Tang",
      "Junlei Wu",
      "Qianyu Xiao",
      "Can Zheng",
      "Shouchen Zhou",
      "Yuxiang Zhu",
      "Yiming Huang",
      "Tian Xie",
      "Tianxing He"
    ],
    "published": "2025-06-07T14:53:03+00:00",
    "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in code generation, capable of tackling complex tasks during inference. However, the extent to which LLMs can be utilized for code checking or debugging through test case generation remains largely unexplored. We investigate this problem from the perspective of competition-level programming (CP) programs and propose TCGBench, a Benchmark for (LLM generation of) Test Case Generators. This benchmark comprises two tasks, aimed at studying the capabilities of LLMs in (1) generating valid test case generators for a given CP problem, and further (2) generating targeted test case generators that expose bugs in human-written code. Experimental results indicate that while state-of-the-art LLMs can generate valid test case generators in most cases, most LLMs struggle to generate targeted test cases that reveal flaws in human code effectively. Especially, even advanced reasoning models (e.g., o3-mini) fall significantly short of human performance in the task of generating targeted generators. Furthermore, we construct a high-quality, manually curated dataset of instructions for generating targeted generators. Analysis demonstrates that the performance of LLMs can be enhanced with the aid of this dataset, by both prompting and fine-tuning."
  },
  {
    "title": "Spatial Disparities in Fire Shelter Accessibility: Capacity Challenges in the Palisades and Eaton Fires",
    "url": "http://arxiv.org/abs/2506.06803v1",
    "arxiv_id": "2506.06803v1",
    "authors": [
      "Su Yeon Han",
      "Yubin Lee",
      "Jooyoung Yoo",
      "Jeon-Young Kang",
      "Jinwoo Park",
      "Soe W. Myint",
      "Eunsang Cho",
      "Xin Gu",
      "Joon-Seok Kim"
    ],
    "published": "2025-06-07T13:55:27+00:00",
    "summary": "The increasing frequency and severity of wildfire in California, exacerbated by prolonged drought and environmental changes, pose significant challenges to urban community resilience and equitable emergency response. The study investigates issues of accessibility to shelters during the Palisades and Eaton Fires which started in January 2025 in Southern California that led to over 180,000 displacements and the loss of 16,000 structures. Despite coordinated efforts of many organizations' emergency assistance, shelter shortages left many evacuees without safety or accessible refuge. This research aims to measure shelter accessibility during the fires' peak, evaluate whether existing shelter capacity met the demand, and identify spatial disparities in access. Results reveal severe shelter shortages and pronounced inequities in access to shelters, particularly in geographically isolated regions and mountainous areas. Our simulations of shelter placement strategies using a capacity-based algorithm and a proximity-based approach demonstrate potential improvements in both shelter accessibility and equitable access to shelters. The findings underscore the critical need for strategic shelter planning and infrastructure development to enhance disaster readiness and reduce vulnerability in regions that frequently experience wildfires."
  },
  {
    "title": "SynHate: Detecting Hate Speech in Synthetic Deepfake Audio",
    "url": "http://arxiv.org/abs/2506.06772v1",
    "arxiv_id": "2506.06772v1",
    "authors": [
      "Rishabh Ranjan",
      "Kishan Pipariya",
      "Mayank Vatsa",
      "Richa Singh"
    ],
    "published": "2025-06-07T11:46:39+00:00",
    "summary": "The rise of deepfake audio and hate speech, powered by advanced text-to-speech, threatens online safety. We present SynHate, the first multilingual dataset for detecting hate speech in synthetic audio, spanning 37 languages. SynHate uses a novel four-class scheme: Real-normal, Real-hate, Fake-normal, and Fake-hate. Built from MuTox and ADIMA datasets, it captures diverse hate speech patterns globally and in India. We evaluate five leading self-supervised models (Whisper-small/medium, XLS-R, AST, mHuBERT), finding notable performance differences by language, with Whisper-small performing best overall. Cross-dataset generalization remains a challenge. By releasing SynHate and baseline code, we aim to advance robust, culturally sensitive, and multilingual solutions against synthetic hate speech. The dataset is available at https://www.iab-rubric.org/resources."
  },
  {
    "title": "Adaptive Event-triggered Formation Control of Autonomous Vehicles",
    "url": "http://arxiv.org/abs/2506.06746v1",
    "arxiv_id": "2506.06746v1",
    "authors": [
      "Ziming Wang",
      "Yihuai Zhang",
      "Chenguang Zhao",
      "Huan Yu"
    ],
    "published": "2025-06-07T10:30:10+00:00",
    "summary": "This paper presents adaptive event-triggered formation control strategies for autonomous vehicles (AVs) subject to longitudinal and lateral motion uncertainties. The proposed framework explores various vehicular formations to enable safe and efficient navigation in complex traffic scenarios, such as narrow passages, collaborative obstacle avoidance, and adaptation to cut-in maneuvers. In contrast to conventional platoon control strategies that rely on predefined communication topologies and continuous state transmission, our approach employs a sampling-based observer to reconstruct vehicle dynamics. Building upon an adaptive backstepping continuous-time controller, we design three distinct event-triggered mechanisms, each offering a different trade-off between formation tracking performance and control efficiency by reducing the frequency of control signal updates. A Lyapunov-based stability analysis is conducted to guarantee bounded tracking errors and to avoid Zeno behavior. Finally, the proposed event-triggered controllers are validated through simulations of vehicular formation in three scenarios, highlighting their impact on traffic safety and mobility."
  },
  {
    "title": "DriveSuprim: Towards Precise Trajectory Selection for End-to-End Planning",
    "url": "http://arxiv.org/abs/2506.06659v1",
    "arxiv_id": "2506.06659v1",
    "authors": [
      "Wenhao Yao",
      "Zhenxin Li",
      "Shiyi Lan",
      "Zi Wang",
      "Xinglong Sun",
      "Jose M. Alvarez",
      "Zuxuan Wu"
    ],
    "published": "2025-06-07T04:39:06+00:00",
    "summary": "In complex driving environments, autonomous vehicles must navigate safely. Relying on a single predicted path, as in regression-based approaches, usually does not explicitly assess the safety of the predicted trajectory. Selection-based methods address this by generating and scoring multiple trajectory candidates and predicting the safety score for each, but face optimization challenges in precisely selecting the best option from thousands of possibilities and distinguishing subtle but safety-critical differences, especially in rare or underrepresented scenarios. We propose DriveSuprim to overcome these challenges and advance the selection-based paradigm through a coarse-to-fine paradigm for progressive candidate filtering, a rotation-based augmentation method to improve robustness in out-of-distribution scenarios, and a self-distillation framework to stabilize training. DriveSuprim achieves state-of-the-art performance, reaching 93.5% PDMS in NAVSIM v1 and 87.1% EPDMS in NAVSIM v2 without extra data, demonstrating superior safetycritical capabilities, including collision avoidance and compliance with rules, while maintaining high trajectory quality in various driving scenarios."
  },
  {
    "title": "SafeLawBench: Towards Safe Alignment of Large Language Models",
    "url": "http://arxiv.org/abs/2506.06636v1",
    "arxiv_id": "2506.06636v1",
    "authors": [
      "Chuxue Cao",
      "Han Zhu",
      "Jiaming Ji",
      "Qichao Sun",
      "Zhenghao Zhu",
      "Yinyu Wu",
      "Juntao Dai",
      "Yaodong Yang",
      "Sirui Han",
      "Yike Guo"
    ],
    "published": "2025-06-07T03:09:59+00:00",
    "summary": "With the growing prevalence of large language models (LLMs), the safety of LLMs has raised significant concerns. However, there is still a lack of definitive standards for evaluating their safety due to the subjective nature of current safety benchmarks. To address this gap, we conducted the first exploration of LLMs' safety evaluation from a legal perspective by proposing the SafeLawBench benchmark. SafeLawBench categorizes safety risks into three levels based on legal standards, providing a systematic and comprehensive framework for evaluation. It comprises 24,860 multi-choice questions and 1,106 open-domain question-answering (QA) tasks. Our evaluation included 2 closed-source LLMs and 18 open-source LLMs using zero-shot and few-shot prompting, highlighting the safety features of each model. We also evaluated the LLMs' safety-related reasoning stability and refusal behavior. Additionally, we found that a majority voting mechanism can enhance model performance. Notably, even leading SOTA models like Claude-3.5-Sonnet and GPT-4o have not exceeded 80.5% accuracy in multi-choice tasks on SafeLawBench, while the average accuracy of 20 LLMs remains at 68.8\\%. We urge the community to prioritize research on the safety of LLMs."
  },
  {
    "title": "Curriculum Reinforcement Learning from Easy to Hard Tasks Improves LLM Reasoning",
    "url": "http://arxiv.org/abs/2506.06632v1",
    "arxiv_id": "2506.06632v1",
    "authors": [
      "Shubham Parashar",
      "Shurui Gui",
      "Xiner Li",
      "Hongyi Ling",
      "Sushil Vemuri",
      "Blake Olson",
      "Eric Li",
      "Yu Zhang",
      "James Caverlee",
      "Dileep Kalathil",
      "Shuiwang Ji"
    ],
    "published": "2025-06-07T02:41:54+00:00",
    "summary": "We aim to improve the reasoning capabilities of language models via reinforcement learning (RL). Recent RL post-trained models like DeepSeek-R1 have demonstrated reasoning abilities on mathematical and coding tasks. However, prior studies suggest that using RL alone to improve reasoning on inherently difficult tasks is less effective. Here, we draw inspiration from curriculum learning and propose to schedule tasks from easy to hard (E2H), allowing LLMs to build reasoning skills gradually. Our method is termed E2H Reasoner. Empirically, we observe that, although easy tasks are important initially, fading them out through appropriate scheduling is essential in preventing overfitting. Theoretically, we establish convergence guarantees for E2H Reasoner within an approximate policy iteration framework. We derive finite-sample complexity bounds and show that when tasks are appropriately decomposed and conditioned, learning through curriculum stages requires fewer total samples than direct learning. Experiments across multiple domains show that E2H Reasoner significantly improves the reasoning ability of small LLMs (1.5B to 3B), which otherwise struggle when trained with vanilla RL alone, highlighting the effectiveness of our method."
  },
  {
    "title": "From Model-Based and Adaptive Control to Evolving Fuzzy Control",
    "url": "http://arxiv.org/abs/2506.06594v1",
    "arxiv_id": "2506.06594v1",
    "authors": [
      "Daniel Leite",
      "Igor \u0160krjanc",
      "Fernando Gomide"
    ],
    "published": "2025-06-07T00:00:52+00:00",
    "summary": "Evolving fuzzy systems build and adapt fuzzy models - such as predictors and controllers - by incrementally updating their rule-base structure from data streams. On the occasion of the 60-year anniversary of fuzzy set theory, commemorated during the Fuzz-IEEE 2025 event, this brief paper revisits the historical development and core contributions of classical fuzzy and adaptive modeling and control frameworks. It then highlights the emergence and significance of evolving intelligent systems in fuzzy modeling and control, emphasizing their advantages in handling nonstationary environments. Key challenges and future directions are discussed, including safety, interpretability, and principled structural evolution."
  },
  {
    "title": "Enhancing Robot Safety via MLLM-Based Semantic Interpretation of Failure Data",
    "url": "http://arxiv.org/abs/2506.06570v1",
    "arxiv_id": "2506.06570v1",
    "authors": [
      "Aryaman Gupta",
      "Yusuf Umut Ciftci",
      "Somil Bansal"
    ],
    "published": "2025-06-06T22:50:39+00:00",
    "summary": "As robotic systems become increasingly integrated into real-world environments, ranging from autonomous vehicles to household assistants, they inevitably encounter diverse and unstructured scenarios that lead to failures. While such failures pose safety and reliability challenges, they also provide rich perceptual data for improving future performance. However, manually analyzing large-scale failure datasets is impractical. In this work, we present a method for automatically organizing large-scale robotic failure data into semantically meaningful clusters, enabling scalable learning from failure without human supervision. Our approach leverages the reasoning capabilities of Multimodal Large Language Models (MLLMs), trained on internet-scale data, to infer high-level failure causes from raw perceptual trajectories and discover interpretable structure within uncurated failure logs. These semantic clusters reveal latent patterns and hypothesized causes of failure, enabling scalable learning from experience. We demonstrate that the discovered failure modes can guide targeted data collection for policy refinement, accelerating iterative improvement in agent policies and overall safety. Additionally, we show that these semantic clusters can be employed for online failure detection, offering a lightweight yet powerful safeguard for real-time adaptation. We demonstrate that this framework enhances robot learning and robustness by transforming real-world failures into actionable and interpretable signals for adaptation."
  },
  {
    "title": "Learning Neural Controllers with Optimality and Stability Guarantees Using Input-Output Dissipativity",
    "url": "http://arxiv.org/abs/2506.06564v1",
    "arxiv_id": "2506.06564v1",
    "authors": [
      "Han Wang",
      "Keyan Miao",
      "Diego Madeira",
      "Antonis Papachristodoulou"
    ],
    "published": "2025-06-06T22:38:31+00:00",
    "summary": "Deep learning methods have demonstrated significant potential for addressing complex nonlinear control problems. For real-world safety-critical tasks, however, it is crucial to provide formal stability guarantees for the designed controllers. In this paper, we propose a new framework for designing neural controllers that achieve both stability and optimality with respect to certain functions. Our key idea is to exploit the concept of input-output dissipativity of nonlinear systems by learning neural storage functions and supply rate functions. As a generalization of Lyapunov theory, dissipativity theory provides a natural connection to optimal control theory, offering both stability guarantees and meaningful optimality certificates. The neural controllers can be directly derived from the learned supply rate functions and guarantee closed-loop stability while inheriting optimality properties that can be shaped towards user-defined control objectives. Extensive numerical experiments demonstrate the effectiveness of our approach."
  },
  {
    "title": "SDN-Based False Data Detection With Its Mitigation and Machine Learning Robustness for In-Vehicle Networks",
    "url": "http://arxiv.org/abs/2506.06556v1",
    "arxiv_id": "2506.06556v1",
    "authors": [
      "Long Dang",
      "Thushari Hapuarachchi",
      "Kaiqi Xiong",
      "Yi Li"
    ],
    "published": "2025-06-06T22:09:36+00:00",
    "summary": "As the development of autonomous and connected vehicles advances, the complexity of modern vehicles increases, with numerous Electronic Control Units (ECUs) integrated into the system. In an in-vehicle network, these ECUs communicate with one another using an standard protocol called Controller Area Network (CAN). Securing communication among ECUs plays a vital role in maintaining the safety and security of the vehicle. This paper proposes a robust SDN-based False Data Detection and Mitigation System (FDDMS) for in-vehicle networks. Leveraging the unique capabilities of Software-Defined Networking (SDN), FDDMS is designed to monitor and detect false data injection attacks in real-time. Specifically, we focus on brake-related ECUs within an SDN-enabled in-vehicle network. First, we decode raw CAN data to create an attack model that illustrates how false data can be injected into the system. Then, FDDMS, incorporating a Long Short Term Memory (LSTM)-based detection model, is used to identify false data injection attacks. We further propose an effective variant of DeepFool attack to evaluate the model's robustness. To countermeasure the impacts of four adversarial attacks including Fast gradient descent method, Basic iterative method, DeepFool, and the DeepFool variant, we further enhance a re-training technique method with a threshold based selection strategy. Finally, a mitigation scheme is implemented to redirect attack traffic by dynamically updating flow rules through SDN. Our experimental results show that the proposed FDDMS is robust against adversarial attacks and effectively detects and mitigates false data injection attacks in real-time."
  },
  {
    "title": "New Methods for Offline GstLAL Analyses",
    "url": "http://arxiv.org/abs/2506.06497v1",
    "arxiv_id": "2506.06497v1",
    "authors": [
      "Prathamesh Joshi",
      "Leo Tsukada",
      "Chad Hanna",
      "Shomik Adhicary",
      "Debnandini Mukherjee",
      "Wanting Niu",
      "Shio Sakon",
      "Divya Singh",
      "Pratyusava Baral",
      "Amanda Baylor",
      "Kipp Cannon",
      "Sarah Caudill",
      "Bryce Cousins",
      "Jolien D. E. Creighton",
      "Becca Ewing",
      "Heather Fong",
      "Richard N. George",
      "Patrick Godwin",
      "Reiko Harada",
      "Yun-Jing Huang",
      "Rachael Huxford",
      "James Kennington",
      "Soichiro Kuwahara",
      "Alvin K. Y. Li",
      "Ryan Magee",
      "Duncan Meacher",
      "Cody Messick",
      "Soichiro Morisaki",
      "Alexander Pace",
      "Cort Posnansky",
      "Anarya Ray",
      "Surabhi Sachdev",
      "Stefano Schmidt",
      "Urja Shah",
      "Ron Tapia",
      "Koh Ueno",
      "Aaron Viets",
      "Leslie Wade",
      "Madeline Wade",
      "Zach Yarbrough",
      "Noah Zhang"
    ],
    "published": "2025-06-06T19:47:32+00:00",
    "summary": "In this work, we present new methods implemented in the GstLAL offline gravitational wave search. These include a technique to reuse the matched filtering data products from a GstLAL online analysis, which hugely reduces the time and computational resources required to obtain offline results; a technique to combine these results with a separate search for heavier black hole mergers, enabling detections from a larger set of gravitational wave sources; changes to the likelihood ratio which increases the sensitivity of the analysis; and two separate changes to the background estimation, allowing more precise significance estimation of gravitational wave candidates. Some of these methods increase the sensitivity of the analysis, whereas others correct previous mis-estimations of sensitivity by eliminating false positives. These methods have been adopted for GstLAL's offline results during the fourth observing run of LIGO, Virgo, and KAGRA (O4). To test these new methods, we perform an offline analysis over one chunk of O3 data, lasting from May 12 19:36:42 UTC 2019 to May 21 14:45:08 UTC 2019, and compare it with previous GstLAL results over the same period of time. We show that cumulatively these methods afford around a 50% - 100% increase in sensitivity in the highest mass space, while simultaneously increasing the reliability of results, and making them more reusable and computationally cheaper."
  },
  {
    "title": "Near-real-time ship grounding damage assessment using Bayesian networks",
    "url": "http://arxiv.org/abs/2506.06493v1",
    "arxiv_id": "2506.06493v1",
    "authors": [
      "Dimitris G. Georgiadis",
      "Manolis S. Samuelides",
      "Daniel Straub"
    ],
    "published": "2025-06-06T19:41:24+00:00",
    "summary": "In a post-grounding event, the rapid assessment of hull girder residual strength is crucial for making informed decisions, such as determining whether the vessel can safely reach the closest yard. One of the primary challenges in this assessment is the uncertainty in the estimation of the extent of structural damage. Although classification societies have developed rapid response damage assessment tools, primarily relying on 2D Smith-based models, these tools are based on deterministic methods and conservative estimates of damage extent. To enhance this assessment, we propose a probabilistic framework for rapid grounding damage assessment of ship structures using Bayesian networks (BNs). The proposed BN model integrates multiple information sources, including underwater inspection results, hydrostatic and bathymetric data, crashworthiness models, and hydraulic models for flooding and oil spill monitoring. By systematically incorporating these parameters and their associated uncertainties within a causal framework, the BN allows for dynamic updates as new evidence emerges during an incident. Two case studies demonstrate the effectiveness of this methodology, highlighting its potential as a practical decision support tool to improve operational safety during grounding events. The results indicate that combining models with on-site observations can even replace costly underwater inspections."
  },
  {
    "title": "PyGemini: Unified Software Development towards Maritime Autonomy Systems",
    "url": "http://arxiv.org/abs/2506.06262v1",
    "arxiv_id": "2506.06262v1",
    "authors": [
      "Kjetil Vasstein",
      "Christian Le",
      "Simon Lerv\u00e5g Breivik",
      "Trygve Maukon Myhr",
      "Annette Stahl",
      "Edmund F\u00f8rland Brekke"
    ],
    "published": "2025-06-06T17:43:00+00:00",
    "summary": "Ensuring the safety and certifiability of autonomous surface vessels (ASVs) requires robust decision-making systems, supported by extensive simulation, testing, and validation across a broad range of scenarios. However, the current landscape of maritime autonomy development is fragmented -- relying on disparate tools for communication, simulation, monitoring, and system integration -- which hampers interdisciplinary collaboration and inhibits the creation of compelling assurance cases, demanded by insurers and regulatory bodies. Furthermore, these disjointed tools often suffer from performance bottlenecks, vendor lock-in, and limited support for continuous integration workflows. To address these challenges, we introduce PyGemini, a permissively licensed, Python-native framework that builds on the legacy of Autoferry Gemini to unify maritime autonomy development. PyGemini introduces a novel Configuration-Driven Development (CDD) process that fuses Behavior-Driven Development (BDD), data-oriented design, and containerization to support modular, maintainable, and scalable software architectures. The framework functions as a stand-alone application, cloud-based service, or embedded library -- ensuring flexibility across research and operational contexts. We demonstrate its versatility through a suite of maritime tools -- including 3D content generation for simulation and monitoring, scenario generation for autonomy validation and training, and generative artificial intelligence pipelines for augmenting imagery -- thereby offering a scalable, maintainable, and performance-oriented foundation for future maritime robotics and autonomy research."
  },
  {
    "title": "Statistical Guarantees in Data-Driven Nonlinear Control: Conformal Robustness for Stability and Safety",
    "url": "http://arxiv.org/abs/2506.06228v1",
    "arxiv_id": "2506.06228v1",
    "authors": [
      "Ting-Wei Hsu",
      "Hiroyasu Tsukamoto"
    ],
    "published": "2025-06-06T16:42:16+00:00",
    "summary": "We present a true-dynamics-agnostic, statistically rigorous framework for establishing exponential stability and safety guarantees of closed-loop, data-driven nonlinear control. Central to our approach is the novel concept of conformal robustness, which robustifies the Lyapunov and zeroing barrier certificates of data-driven dynamical systems against model prediction uncertainties using conformal prediction. It quantifies these uncertainties by leveraging rank statistics of prediction scores over system trajectories, without assuming any specific underlying structure of the prediction model or distribution of the uncertainties. With the quantified uncertainty information, we further construct the conformally robust control Lyapunov function (CR-CLF) and control barrier function (CR-CBF), data-driven counterparts of the CLF and CBF, for fully data-driven control with statistical guarantees of finite-horizon exponential stability and safety. The performance of the proposed concept is validated in numerical simulations with four benchmark nonlinear control problems."
  },
  {
    "title": "\"We need to avail ourselves of GenAI to enhance knowledge distribution\": Empowering Older Adults through GenAI Literacy",
    "url": "http://arxiv.org/abs/2506.06225v1",
    "arxiv_id": "2506.06225v1",
    "authors": [
      "Eunhye Grace Ko",
      "Shaini Nanayakkara",
      "Earl W. Huff Jr"
    ],
    "published": "2025-06-06T16:38:37+00:00",
    "summary": "As generative AI (GenAI) becomes increasingly widespread, it is crucial to equip users, particularly vulnerable populations such as older adults (65 and older), with the knowledge to understand its benefits and potential risks. Older adults often exhibit greater reservations about adopting emerging technologies and require tailored literacy support. Using a mixed methods approach, this study examines strategies for delivering GenAI literacy to older adults through a chatbot named Litti, evaluating its impact on their AI literacy (knowledge, safety, and ethical use). The quantitative data indicated a trend toward improved AI literacy, though the results were not statistically significant. However, qualitative interviews revealed diverse levels of familiarity with generative AI and a strong desire to learn more. Findings also show that while Litti provided a positive learning experience, it did not significantly enhance participants' trust or sense of safety regarding GenAI. This exploratory case study highlights the challenges and opportunities in designing AI literacy education for the rapidly growing older adult population."
  },
  {
    "title": "Experimental Study On Flashing-Induced Instabilities In An Open Natural Circulation System",
    "url": "http://arxiv.org/abs/2506.06213v1",
    "arxiv_id": "2506.06213v1",
    "authors": [
      "Yuliang Fang",
      "Xiaxin Cao",
      "Wenxi Tian"
    ],
    "published": "2025-06-06T16:19:13+00:00",
    "summary": "The natural circulation system (NCS) uses gravity pressure drop caused by density differences in the loop to generate the driving force without any external mechanical devices, which has been widely applied to the design of the nuclear reactor system and the passive safety system due to its simple structure, high intrinsic safety, and strong heat discharge capacity. However, the low-pressure condition can lead to a two-phase flow and make the flow characteristics in the NCS more complex. Flashing-induced instability occurring in the open NCS will cause the system structural vibration as well as mechanical damage and bring safety problems. The study on flashing-flow behaviors in an open NPS has been conducted experimentally in this paper. High-speed camera, thermal needle probe and wire-mesh sensor were adopted to record the flow pattern and measure the void fraction in the polycarbonate visualization riser section. In the start-up process, with the inlet temperature in the riser section increasing, the open NCS has experienced single-phase stable flow, intermittent oscillation between single-phase and two-phase, high subcooling two-phase stable flow, flashing-induced instabilities flow, and low subcooling two-phase stable flow. The flow pattern evolution of flow flashing goes through bubble flow, cap-slug flow, churn flow and wispy annular flow, in which the length of churn can account for more than 40% length of the two-phase regime. The flash number Nflash is used to divide the region of flashing-induced instabilities. It is found that the open NCS is in a stable two-phase flow when the flash number at the outlet of the riser section N_{flash,out} = 4\\sim 5."
  },
  {
    "title": "Let's CONFER: A Dataset for Evaluating Natural Language Inference Models on CONditional InFERence and Presupposition",
    "url": "http://arxiv.org/abs/2506.06133v1",
    "arxiv_id": "2506.06133v1",
    "authors": [
      "Tara Azin",
      "Daniel Dumitrescu",
      "Diana Inkpen",
      "Raj Singh"
    ],
    "published": "2025-06-06T14:42:20+00:00",
    "summary": "Natural Language Inference (NLI) is the task of determining whether a sentence pair represents entailment, contradiction, or a neutral relationship. While NLI models perform well on many inference tasks, their ability to handle fine-grained pragmatic inferences, particularly presupposition in conditionals, remains underexplored. In this study, we introduce CONFER, a novel dataset designed to evaluate how NLI models process inference in conditional sentences. We assess the performance of four NLI models, including two pre-trained models, to examine their generalization to conditional reasoning. Additionally, we evaluate Large Language Models (LLMs), including GPT-4o, LLaMA, Gemma, and DeepSeek-R1, in zero-shot and few-shot prompting settings to analyze their ability to infer presuppositions with and without prior context. Our findings indicate that NLI models struggle with presuppositional reasoning in conditionals, and fine-tuning on existing NLI datasets does not necessarily improve their performance."
  },
  {
    "title": "Self driving algorithm for an active four wheel drive racecar",
    "url": "http://arxiv.org/abs/2506.06077v1",
    "arxiv_id": "2506.06077v1",
    "authors": [
      "Gergely Bari",
      "Laszlo Palkovics"
    ],
    "published": "2025-06-06T13:33:15+00:00",
    "summary": "Controlling autonomous vehicles at their handling limits is a significant challenge, particularly for electric vehicles with active four wheel drive (A4WD) systems offering independent wheel torque control. While traditional Vehicle Dynamics Control (VDC) methods use complex physics-based models, this study explores Deep Reinforcement Learning (DRL) to develop a unified, high-performance controller. We employ the Proximal Policy Optimization (PPO) algorithm to train an agent for optimal lap times in a simulated racecar (TORCS) at the tire grip limit. Critically, the agent learns an end-to-end policy that directly maps vehicle states, like velocities, accelerations, and yaw rate, to a steering angle command and independent torque commands for each of the four wheels. This formulation bypasses conventional pedal inputs and explicit torque vectoring algorithms, allowing the agent to implicitly learn the A4WD control logic needed for maximizing performance and stability. Simulation results demonstrate the RL agent learns sophisticated strategies, dynamically optimizing wheel torque distribution corner-by-corner to enhance handling and mitigate the vehicle's inherent understeer. The learned behaviors mimic and, in aspects of grip utilization, potentially surpass traditional physics-based A4WD controllers while achieving competitive lap times. This research underscores DRL's potential to create adaptive control systems for complex vehicle dynamics, suggesting RL is a potent alternative for advancing autonomous driving in demanding, grip-limited scenarios for racing and road safety."
  },
  {
    "title": "Trajectory Optimization for UAV-Based Medical Delivery with Temporal Logic Constraints and Convex Feasible Set Collision Avoidance",
    "url": "http://arxiv.org/abs/2506.06038v1",
    "arxiv_id": "2506.06038v1",
    "authors": [
      "Kaiyuan Chen",
      "Yuhan Suo",
      "Shaowei Cui",
      "Yuanqing Xia",
      "Wannian Liang",
      "Shuo Wang"
    ],
    "published": "2025-06-06T12:39:02+00:00",
    "summary": "This paper addresses the problem of trajectory optimization for unmanned aerial vehicles (UAVs) performing time-sensitive medical deliveries in urban environments. Specifically, we consider a single UAV with 3 degree-of-freedom dynamics tasked with delivering blood packages to multiple hospitals, each with a predefined time window and priority. Mission objectives are encoded using Signal Temporal Logic (STL), enabling the formal specification of spatial-temporal constraints. To ensure safety, city buildings are modeled as 3D convex obstacles, and obstacle avoidance is handled through a Convex Feasible Set (CFS) method. The entire planning problem-combining UAV dynamics, STL satisfaction, and collision avoidance-is formulated as a convex optimization problem that ensures tractability and can be solved efficiently using standard convex programming techniques. Simulation results demonstrate that the proposed method generates dynamically feasible, collision-free trajectories that satisfy temporal mission goals, providing a scalable and reliable approach for autonomous UAV-based medical logistics."
  },
  {
    "title": "On Inverse Problems, Parameter Estimation, and Domain Generalization",
    "url": "http://arxiv.org/abs/2506.06024v1",
    "arxiv_id": "2506.06024v1",
    "authors": [
      "Deborah Pereg"
    ],
    "published": "2025-06-06T12:15:02+00:00",
    "summary": "Signal restoration and inverse problems are key elements in most real-world data science applications. In the past decades, with the emergence of machine learning methods, inversion of measurements has become a popular step in almost all physical applications, which is normally executed prior to downstream tasks that often involve parameter estimation. In this work, we analyze the general problem of parameter estimation in an inverse problem setting. First, we address the domain-shift problem by re-formulating it in direct relation with the discrete parameter estimation analysis. We analyze a significant vulnerability in current attempts to enforce domain generalization, which we dubbed the Double Meaning Theorem. Our theoretical findings are experimentally illustrated for domain shift examples in image deblurring and speckle suppression in medical imaging. We then proceed to a theoretical analysis of parameter estimation given observed measurements before and after data processing involving an inversion of the observations. We compare this setting for invertible and non-invertible (degradation) processes. We distinguish between continuous and discrete parameter estimation, corresponding with regression and classification problems, respectively. Our theoretical findings align with the well-known information-theoretic data processing inequality, and to a certain degree question the common misconception that data-processing for inversion, based on modern generative models that may often produce outstanding perceptual quality, will necessarily improve the following parameter estimation objective. It is our hope that this paper will provide practitioners with deeper insights that may be leveraged in the future for the development of more efficient and informed strategic system planning, critical in safety-sensitive applications."
  },
  {
    "title": "Unlocking Recursive Thinking of LLMs: Alignment via Refinement",
    "url": "http://arxiv.org/abs/2506.06009v1",
    "arxiv_id": "2506.06009v1",
    "authors": [
      "Haoke Zhang",
      "Xiaobo Liang",
      "Cunxiang Wang",
      "Juntao Li",
      "Min Zhang"
    ],
    "published": "2025-06-06T11:54:06+00:00",
    "summary": "The OpenAI o1-series models have demonstrated that leveraging long-form Chain of Thought (CoT) can substantially enhance performance. However, the recursive thinking capabilities of Large Language Models (LLMs) remain limited, particularly in the absence of expert-curated data for distillation. In this paper, we propose \\textbf{AvR}: \\textbf{Alignment via Refinement}, a novel method aimed at unlocking the potential of LLMs for recursive reasoning through long-form CoT. AvR introduces a refinement process that integrates criticism and improvement actions, guided by differentiable learning techniques to optimize \\textbf{refinement-aware rewards}. As a result, the synthesized multi-round data can be organized as a long refinement thought, further enabling test-time scaling. Experimental results show that AvR significantly outperforms conventional preference optimization methods. Notably, with only 3k synthetic samples, our method boosts the performance of the LLaMA-3-8B-Instruct model by over 20\\% in win rate on AlpacaEval 2.0. Our code is available at Github (https://github.com/Banner-Z/AvR.git)."
  },
  {
    "title": "CrimeMind: Simulating Urban Crime with Multi-Modal LLM Agents",
    "url": "http://arxiv.org/abs/2506.05981v1",
    "arxiv_id": "2506.05981v1",
    "authors": [
      "Qingbin Zeng",
      "Ruotong Zhao",
      "Jinzhu Mao",
      "Haoyang Li",
      "Fengli Xu",
      "Yong Li"
    ],
    "published": "2025-06-06T11:01:21+00:00",
    "summary": "Modeling urban crime is an important yet challenging task that requires understanding the subtle visual, social, and cultural cues embedded in urban environments. Previous work has predominantly focused on rule-based agent-based modeling (ABM) and deep learning methods. ABMs offer interpretability of internal mechanisms but exhibit limited predictive accuracy.In contrast, deep learning methods are often effective in prediction but are less interpretable and require extensive training data. Moreover, both lines of work lack the cognitive flexibility to adapt to changing environments. Leveraging the capabilities of large language models (LLMs), we propose CrimeMind, a novel LLM-driven ABM framework for simulating urban crime within a multi-modal urban context.A key innovation of our design is the integration of the Routine Activity Theory (RAT) into the agentic workflow of CrimeMind, enabling it to process rich multi-modal urban features and reason about criminal behavior.However, RAT requires LLM agents to infer subtle cues in evaluating environmental safety as part of assessing guardianship, which can be challenging for LLMs. To address this, we collect a small-scale human-annotated dataset and align CrimeMind's perception with human judgment via a training-free textual gradient method.Experiments across four major U.S. cities demonstrate that CrimeMind outperforms both traditional ABMs and deep learning baselines in crime hotspot prediction and spatial distribution accuracy, achieving up to a 24% improvement over the strongest baseline.Furthermore, we conduct counterfactual simulations of external incidents and policy interventions and it successfully captures the expected changes in crime patterns, demonstrating its ability to reflect counterfactual scenarios.Overall, CrimeMind enables fine-grained modeling of individual behaviors and facilitates evaluation of real-world interventions."
  },
  {
    "title": "SurGSplat: Progressive Geometry-Constrained Gaussian Splatting for Surgical Scene Reconstruction",
    "url": "http://arxiv.org/abs/2506.05935v1",
    "arxiv_id": "2506.05935v1",
    "authors": [
      "Yuchao Zheng",
      "Jianing Zhang",
      "Guochen Ning",
      "Hongen Liao"
    ],
    "published": "2025-06-06T10:02:11+00:00",
    "summary": "Intraoperative navigation relies heavily on precise 3D reconstruction to ensure accuracy and safety during surgical procedures. However, endoscopic scenarios present unique challenges, including sparse features and inconsistent lighting, which render many existing Structure-from-Motion (SfM)-based methods inadequate and prone to reconstruction failure. To mitigate these constraints, we propose SurGSplat, a novel paradigm designed to progressively refine 3D Gaussian Splatting (3DGS) through the integration of geometric constraints. By enabling the detailed reconstruction of vascular structures and other critical features, SurGSplat provides surgeons with enhanced visual clarity, facilitating precise intraoperative decision-making. Experimental evaluations demonstrate that SurGSplat achieves superior performance in both novel view synthesis (NVS) and pose estimation accuracy, establishing it as a high-fidelity and efficient solution for surgical scene reconstruction. More information and results can be found on the page https://surgsplat.github.io/."
  },
  {
    "title": "Small Models, Big Support: A Local LLM Framework for Teacher-Centric Content Creation and Assessment using RAG and CAG",
    "url": "http://arxiv.org/abs/2506.05925v1",
    "arxiv_id": "2506.05925v1",
    "authors": [
      "Zarreen Reza",
      "Alexander Mazur",
      "Michael T. Dugdale",
      "Robin Ray-Chaudhuri"
    ],
    "published": "2025-06-06T09:47:03+00:00",
    "summary": "While Large Language Models (LLMs) are increasingly utilized as student-facing educational aids, their potential to directly support educators, particularly through locally deployable and customizable open-source solutions, remains significantly underexplored. Many existing educational solutions rely on cloud-based infrastructure or proprietary tools, which are costly and may raise privacy concerns. Regulated industries with limited budgets require affordable, self-hosted solutions. We introduce an end-to-end, open-source framework leveraging small (3B-7B parameters), locally deployed LLMs for customized teaching material generation and assessment. Our system uniquely incorporates an interactive loop crucial for effective small-model refinement, and an auxiliary LLM verifier to mitigate jailbreaking risks, enhancing output reliability and safety. Utilizing Retrieval and Context Augmented Generation (RAG/CAG), it produces factually accurate, customized pedagogically-styled content. Deployed on-premises for data privacy and validated through an evaluation pipeline and a college physics pilot, our findings show that carefully engineered small LLM systems can offer robust, affordable, practical, and safe educator support, achieving utility comparable to larger models for targeted tasks."
  },
  {
    "title": "Rethinking Semi-supervised Segmentation Beyond Accuracy: Reliability and Robustness",
    "url": "http://arxiv.org/abs/2506.05917v1",
    "arxiv_id": "2506.05917v1",
    "authors": [
      "Steven Landgraf",
      "Markus Hillemann",
      "Markus Ulrich"
    ],
    "published": "2025-06-06T09:37:45+00:00",
    "summary": "Semantic segmentation is critical for scene understanding but demands costly pixel-wise annotations, attracting increasing attention to semi-supervised approaches to leverage abundant unlabeled data. While semi-supervised segmentation is often promoted as a path toward scalable, real-world deployment, it is astonishing that current evaluation protocols exclusively focus on segmentation accuracy, entirely overlooking reliability and robustness. These qualities, which ensure consistent performance under diverse conditions (robustness) and well-calibrated model confidences as well as meaningful uncertainties (reliability), are essential for safety-critical applications like autonomous driving, where models must handle unpredictable environments and avoid sudden failures at all costs. To address this gap, we introduce the Reliable Segmentation Score (RSS), a novel metric that combines predictive accuracy, calibration, and uncertainty quality measures via a harmonic mean. RSS penalizes deficiencies in any of its components, providing an easy and intuitive way of holistically judging segmentation models. Comprehensive evaluations of UniMatchV2 against its predecessor and a supervised baseline show that semi-supervised methods often trade reliability for accuracy. While out-of-domain evaluations demonstrate UniMatchV2's robustness, they further expose persistent reliability shortcomings. We advocate for a shift in evaluation protocols toward more holistic metrics like RSS to better align semi-supervised learning research with real-world deployment needs."
  },
  {
    "title": "Bayesian Persuasion as a Bargaining Game",
    "url": "http://arxiv.org/abs/2506.05876v1",
    "arxiv_id": "2506.05876v1",
    "authors": [
      "Yue Lin",
      "Shuhui Zhu",
      "William A Cunningham",
      "Wenhao Li",
      "Pascal Poupart",
      "Hongyuan Zha",
      "Baoxiang Wang"
    ],
    "published": "2025-06-06T08:42:34+00:00",
    "summary": "Bayesian persuasion, an extension of cheap-talk communication, involves an informed sender committing to a signaling scheme to influence a receiver's actions. Compared to cheap talk, this sender's commitment enables the receiver to verify the incentive compatibility of signals beforehand, facilitating cooperation. While effective in one-shot scenarios, Bayesian persuasion faces computational complexity (NP-hardness) when extended to long-term interactions, where the receiver may adopt dynamic strategies conditional on past outcomes and future expectations. To address this complexity, we introduce the bargaining perspective, which allows: (1) a unified framework and well-structured solution concept for long-term persuasion, with desirable properties such as fairness and Pareto efficiency; (2) a clear distinction between two previously conflated advantages: the sender's informational advantage and first-proposer advantage. With only modest modifications to the standard setting, this perspective makes explicit the common knowledge of the game structure and grants the receiver comparable commitment capabilities, thereby reinterpreting classic one-sided persuasion as a balanced information bargaining framework. The framework is validated through a two-stage validation-and-inference paradigm: We first demonstrate that GPT-o3 and DeepSeek-R1, out of publicly available LLMs, reliably handle standard tasks; We then apply them to persuasion scenarios to test that the outcomes align with what our information-bargaining framework suggests. All code, results, and terminal logs are publicly available at github.com/YueLin301/InformationBargaining."
  },
  {
    "title": "Information Bargaining: Bilateral Commitment in Bayesian Persuasion",
    "url": "http://arxiv.org/abs/2506.05876v2",
    "arxiv_id": "2506.05876v2",
    "authors": [
      "Yue Lin",
      "Shuhui Zhu",
      "William A Cunningham",
      "Wenhao Li",
      "Pascal Poupart",
      "Hongyuan Zha",
      "Baoxiang Wang"
    ],
    "published": "2025-06-06T08:42:34+00:00",
    "summary": "Bayesian persuasion, an extension of cheap-talk communication, involves an informed sender committing to a signaling scheme to influence a receiver's actions. Compared to cheap talk, this sender's commitment enables the receiver to verify the incentive compatibility of signals beforehand, facilitating cooperation. While effective in one-shot scenarios, Bayesian persuasion faces computational complexity (NP-hardness) when extended to long-term interactions, where the receiver may adopt dynamic strategies conditional on past outcomes and future expectations. To address this complexity, we introduce the bargaining perspective, which allows: (1) a unified framework and well-structured solution concept for long-term persuasion, with desirable properties such as fairness and Pareto efficiency; (2) a clear distinction between two previously conflated advantages: the sender's informational advantage and first-proposer advantage. With only modest modifications to the standard setting, this perspective makes explicit the common knowledge of the game structure and grants the receiver comparable commitment capabilities, thereby reinterpreting classic one-sided persuasion as a balanced information bargaining framework. The framework is validated through a two-stage validation-and-inference paradigm: We first demonstrate that GPT-o3 and DeepSeek-R1, out of publicly available LLMs, reliably handle standard tasks; We then apply them to persuasion scenarios to test that the outcomes align with what our information-bargaining framework suggests. All code, results, and terminal logs are publicly available at github.com/YueLin301/InformationBargaining."
  },
  {
    "title": "Towards Next-Generation Intelligent Maintenance: Collaborative Fusion of Large and Small Models",
    "url": "http://arxiv.org/abs/2506.05854v1",
    "arxiv_id": "2506.05854v1",
    "authors": [
      "Xiaoyi Yuan",
      "Qiming Huang",
      "Mingqing Guo",
      "Huiming Ma",
      "Ming Xu",
      "Zeyi Liu",
      "Xiao He"
    ],
    "published": "2025-06-06T08:20:28+00:00",
    "summary": "With the rapid advancement of intelligent technologies, collaborative frameworks integrating large and small models have emerged as a promising approach for enhancing industrial maintenance. However, several challenges persist, including limited domain adaptability, insufficient real-time performance and reliability, high integration complexity, and difficulties in knowledge representation and fusion. To address these issues, an intelligent maintenance framework for industrial scenarios is proposed. This framework adopts a five-layer architecture and integrates the precise computational capabilities of domain-specific small models with the cognitive reasoning, knowledge integration, and interactive functionalities of large language models. The objective is to achieve more accurate, intelligent, and efficient maintenance in industrial applications. Two realistic implementations, involving the maintenance of telecommunication equipment rooms and the intelligent servicing of energy storage power stations, demonstrate that the framework significantly enhances maintenance efficiency."
  },
  {
    "title": "Properties of UTxO Ledgers and Programs Implemented on Them",
    "url": "http://arxiv.org/abs/2506.05832v1",
    "arxiv_id": "2506.05832v1",
    "authors": [
      "Polina Vinogradova",
      "Alexey Sorokin"
    ],
    "published": "2025-06-06T07:57:27+00:00",
    "summary": "Trace-based properties are the gold standard for program behaviour analysis. One of the domains of application of this type of analysis is cryptocurrency ledgers, both for the purpose of analyzing the behaviour of the ledger itself, and any user-defined programs called by it, known as smart contracts. The (extended) UTxO ledger model is a kind of ledger model where all smart contract code is stateless, and additional work must be done to model stateful programs. We formalize the application of trace-based analysis to UTxO ledgers and contracts, expressing it in the languages of topology, as well as graph and category theory. To describe valid traces of UTxO ledger executions, and their relation to the behaviour of stateful programs implemented on the ledger, we define a category of simple graphs, infinite paths in which form an ultra-metric space. Maps in this category are arbitrary partial sieve-define homomorphisms of simple graphs. Programs implemented on the ledger correspond to non-expanding maps out of the graph of valid UTxO execution traces. We reason about safety properties in this framework, and prove properties of valid UTxO ledger traces."
  },
  {
    "title": "FinanceReasoning: Benchmarking Financial Numerical Reasoning More Credible, Comprehensive and Challenging",
    "url": "http://arxiv.org/abs/2506.05828v1",
    "arxiv_id": "2506.05828v1",
    "authors": [
      "Zichen Tang",
      "Haihong E",
      "Ziyan Ma",
      "Haoyang He",
      "Jiacheng Liu",
      "Zhongjun Yang",
      "Zihua Rong",
      "Rongjin Li",
      "Kun Ji",
      "Qing Huang",
      "Xinyang Hu",
      "Yang Liu",
      "Qianhe Zheng"
    ],
    "published": "2025-06-06T07:53:58+00:00",
    "summary": "We introduce FinanceReasoning, a novel benchmark designed to evaluate the reasoning capabilities of large reasoning models (LRMs) in financial numerical reasoning problems. Compared to existing benchmarks, our work provides three key advancements. (1) Credibility: We update 15.6% of the questions from four public datasets, annotating 908 new questions with detailed Python solutions and rigorously refining evaluation standards. This enables an accurate assessment of the reasoning improvements of LRMs. (2) Comprehensiveness: FinanceReasoning covers 67.8% of financial concepts and formulas, significantly surpassing existing datasets. Additionally, we construct 3,133 Python-formatted functions, which enhances LRMs' financial reasoning capabilities through refined knowledge (e.g., 83.2% $\\rightarrow$ 91.6% for GPT-4o). (3) Challenge: Models are required to apply multiple financial formulas for precise numerical reasoning on 238 Hard problems. The best-performing model (i.e., OpenAI o1 with PoT) achieves 89.1% accuracy, yet LRMs still face challenges in numerical precision. We demonstrate that combining Reasoner and Programmer models can effectively enhance LRMs' performance (e.g., 83.2% $\\rightarrow$ 87.8% for DeepSeek-R1). Our work paves the way for future research on evaluating and improving LRMs in domain-specific complex reasoning tasks."
  },
  {
    "title": "Towards Mixed-Criticality Software Architectures for Centralized HPC Platforms in Software-Defined Vehicles: A Systematic Literature Review",
    "url": "http://arxiv.org/abs/2506.05822v1",
    "arxiv_id": "2506.05822v1",
    "authors": [
      "Lucas Mauser",
      "Eva Zimmermann",
      "Pavel Nedv\u011bdick\u00fd",
      "Tobias Eisenreich",
      "Moritz W\u00e4schle",
      "Stefan Wagner"
    ],
    "published": "2025-06-06T07:40:30+00:00",
    "summary": "Centralized electrical/electronic architectures and High-Performance Computers (HPCs) are redefining automotive software development, challenging traditional microcontroller-based approaches. Ensuring real-time, safety, and scalability in software-defined vehicles necessitates reevaluating how mixed-criticality software is integrated into centralized architectures. While existing research on automotive SoftWare Architectures (SWAs) is relevant to the industry, it often lacks validation through systematic, empirical methods. To address this gap, we conduct a systematic literature review focusing on automotive mixed-criticality SWAs. Our goal is to provide practitioner-oriented guidelines that assist automotive software architects and developers design centralized, mixed-criticality SWAs based on a rigorous and transparent methodology. First, we set up a systematic review protocol grounded in established guidelines. Second, we apply this protocol to identify relevant studies. Third, we extract key functional domains, constraints, and enabling technologies that drive changes in automotive SWAs, thereby assessing the protocol's effectiveness. Additionally, we extract techniques, architectural patterns, and design practices for integrating mixed-criticality requirements into HPC-based SWAs, further demonstrating the protocol's applicability. Based on these insights, we propose an exemplary SWA for a microprocessor-based system-on-chip. In conclusion, this study provides a structured approach to explore and realize mixed-criticality software integration for next-generation automotive SWAs, offering valuable insights for industry and research applications."
  },
  {
    "title": "Mechanisms of Afterglow and Thermally Stimulated Luminescence in UV-irradiated InP/ZnS Quantum Dots",
    "url": "http://arxiv.org/abs/2506.05792v1",
    "arxiv_id": "2506.05792v1",
    "authors": [
      "S. S. Savchenko",
      "A. O. Shilov",
      "A. S. Vokhmintsev",
      "I. A. Weinstein"
    ],
    "published": "2025-06-06T06:37:38+00:00",
    "summary": "Indium phosphide-based quantum dots (QDs) are a potential material for designing optoelectronic devices, owing their adjustable spectral parameters over the entire visible range, as well as their high biocompatibility and environmental safety. Concurrently, they exhibit structural defects, the rectification of which is crucial for enhancing their optical properties. The present work explores, for the first time, the low-temperature afterglow (AG) and spectrally resolved thermally stimulated luminescence (TSL) of UV-irradiated colloidal core/shell InP/ZnS QDs in the range of 7-340 K. It is shown that, when localized during irradiation and released after additional stimulation, charge carriers recombine involving defect centers based on indium and phosphorus dangling bonds. The mechanisms of the observed luminescent phenomena can be caused by both thermal activation and tunneling processes. By means of the initial rise method, the formalism of general-order kinetics, and the analytical description using the Lambert W function, we have analyzed the kinetic features of possible thermally stimulated mechanisms. We have also estimated the energy characteristics of appropriate trapping centers. A low rate of charge carriers recapture is revealed for InP/ZnS QDs. Active traps in nanocrystals of different sizes are characterized by close values of activation energy in the 26-31 meV range. The current paper discloses new horizons for exploiting TSL approaches to study the properties of local defective states in the energy structure of colloidal QDs, which can contribute to the development of targeted synthesis of nanocrystals with tunable temperature sensitivity for optoelectronic and sensor applications."
  },
  {
    "title": "Robust sensor fusion against on-vehicle sensor staleness",
    "url": "http://arxiv.org/abs/2506.05780v1",
    "arxiv_id": "2506.05780v1",
    "authors": [
      "Meng Fan",
      "Yifan Zuo",
      "Patrick Blaes",
      "Harley Montgomery",
      "Subhasis Das"
    ],
    "published": "2025-06-06T06:18:54+00:00",
    "summary": "Sensor fusion is crucial for a performant and robust Perception system in autonomous vehicles, but sensor staleness, where data from different sensors arrives with varying delays, poses significant challenges. Temporal misalignment between sensor modalities leads to inconsistent object state estimates, severely degrading the quality of trajectory predictions that are critical for safety. We present a novel and model-agnostic approach to address this problem via (1) a per-point timestamp offset feature (for LiDAR and radar both relative to camera) that enables fine-grained temporal awareness in sensor fusion, and (2) a data augmentation strategy that simulates realistic sensor staleness patterns observed in deployed vehicles. Our method is integrated into a perspective-view detection model that consumes sensor data from multiple LiDARs, radars and cameras. We demonstrate that while a conventional model shows significant regressions when one sensor modality is stale, our approach reaches consistently good performance across both synchronized and stale conditions."
  },
  {
    "title": "Efficient Online RFT with Plug-and-Play LLM Judges: Unlocking State-of-the-Art Performance",
    "url": "http://arxiv.org/abs/2506.05748v1",
    "arxiv_id": "2506.05748v1",
    "authors": [
      "Rudransh Agnihotri",
      "Ananya Pandey"
    ],
    "published": "2025-06-06T05:18:54+00:00",
    "summary": "Reward-model training is the cost bottleneck in modern Reinforcement Learning Human Feedback (RLHF) pipelines, often requiring tens of billions of parameters and an offline preference-tuning phase. In the proposed method, a frozen, instruction-tuned 7B LLM is augmented with only a one line JSON rubric and a rank-16 LoRA adapter (affecting just 0.8% of the model's parameters), enabling it to serve as a complete substitute for the previously used heavyweight evaluation models. The plug-and-play judge achieves 96.2% accuracy on RewardBench, outperforming specialized reward networks ranging from 27B to 70B parameters. Additionally, it allows a 7B actor to outperform the top 70B DPO baseline, which scores 61.8%, by achieving 92% exact match accuracy on GSM-8K utilizing online PPO. Thorough ablations indicate that (i) six in context demonstrations deliver the majority of the zero-to-few-shot improvements (+2pp), and (ii) the LoRA effectively addresses the remaining disparity, particularly in the safety and adversarial Chat-Hard segments. The proposed model introduces HH-Rationales, a subset of 10,000 pairs from Anthropic HH-RLHF, to examine interpretability, accompanied by human generated justifications. GPT-4 scoring indicates that our LoRA judge attains approximately = 9/10 in similarity to human explanations, while zero-shot judges score around =5/10. These results indicate that the combination of prompt engineering and tiny LoRA produces a cost effective, transparent, and easily adjustable reward function, removing the offline phase while achieving new state-of-the-art outcomes for both static evaluation and online RLHF."
  },
  {
    "title": "Topology of Reasoning: Understanding Large Reasoning Models through Reasoning Graph Properties",
    "url": "http://arxiv.org/abs/2506.05744v1",
    "arxiv_id": "2506.05744v1",
    "authors": [
      "Gouki Minegishi",
      "Hiroki Furuta",
      "Takeshi Kojima",
      "Yusuke Iwasawa",
      "Yutaka Matsuo"
    ],
    "published": "2025-06-06T05:05:11+00:00",
    "summary": "Recent large-scale reasoning models have achieved state-of-the-art performance on challenging mathematical benchmarks, yet the internal mechanisms underlying their success remain poorly understood. In this work, we introduce the notion of a reasoning graph, extracted by clustering hidden-state representations at each reasoning step, and systematically analyze three key graph-theoretic properties: cyclicity, diameter, and small-world index, across multiple tasks (GSM8K, MATH500, AIME 2024). Our findings reveal that distilled reasoning models (e.g., DeepSeek-R1-Distill-Qwen-32B) exhibit significantly more recurrent cycles (about 5 per sample), substantially larger graph diameters, and pronounced small-world characteristics (about 6x) compared to their base counterparts. Notably, these structural advantages grow with task difficulty and model capacity, with cycle detection peaking at the 14B scale and exploration diameter maximized in the 32B variant, correlating positively with accuracy. Furthermore, we show that supervised fine-tuning on an improved dataset systematically expands reasoning graph diameters in tandem with performance gains, offering concrete guidelines for dataset design aimed at boosting reasoning capabilities. By bridging theoretical insights into reasoning graph structures with practical recommendations for data construction, our work advances both the interpretability and the efficacy of large reasoning models."
  },
  {
    "title": "Statistically Valid Post-Deployment Monitoring Should Be Standard for AI-Based Digital Health",
    "url": "http://arxiv.org/abs/2506.05701v1",
    "arxiv_id": "2506.05701v1",
    "authors": [
      "Pavel Dolin",
      "Weizhi Li",
      "Gautam Dasarathy",
      "Visar Berisha"
    ],
    "published": "2025-06-06T03:04:44+00:00",
    "summary": "This position paper argues that post-deployment monitoring in clinical AI is underdeveloped and proposes statistically valid and label-efficient testing frameworks as a principled foundation for ensuring reliability and safety in real-world deployment. A recent review found that only 9% of FDA-registered AI-based healthcare tools include a post-deployment surveillance plan. Existing monitoring approaches are often manual, sporadic, and reactive, making them ill-suited for the dynamic environments in which clinical models operate. We contend that post-deployment monitoring should be grounded in label-efficient and statistically valid testing frameworks, offering a principled alternative to current practices. We use the term \"statistically valid\" to refer to methods that provide explicit guarantees on error rates (e.g., Type I/II error), enable formal inference under pre-defined assumptions, and support reproducibility--features that align with regulatory requirements. Specifically, we propose that the detection of changes in the data and model performance degradation should be framed as distinct statistical hypothesis testing problems. Grounding monitoring in statistical rigor ensures a reproducible and scientifically sound basis for maintaining the reliability of clinical AI systems. Importantly, it also opens new research directions for the technical community--spanning theory, methods, and tools for statistically principled detection, attribution, and mitigation of post-deployment model failures in real-world settings."
  },
  {
    "title": "Insights from Designing Context-Aware Meal Preparation Assistance for Older Adults with Mild Cognitive Impairment (MCI) and Their Care Partners",
    "url": "http://arxiv.org/abs/2506.05663v1",
    "arxiv_id": "2506.05663v1",
    "authors": [
      "Szeyi Chan",
      "Jiachen Li",
      "Siman Ao",
      "Yufei Wang",
      "Ibrahim Bilau",
      "Brian Jones",
      "Eunhwa Yang",
      "Elizabeth D Mynatt",
      "Xiang Zhi Tan"
    ],
    "published": "2025-06-06T01:26:49+00:00",
    "summary": "Older adults with mild cognitive impairment (MCI) often face challenges during meal preparation, such as forgetting ingredients, skipping steps, or leaving appliances on, which can compromise their safety and independence. Our study explores the design of context-aware assistive technologies for meal preparation using a user-centered iterative design process. Through three iterative phases of design and feedback, evolving from low-tech lightbox to a digital screen, we gained insights into managing diverse contexts and personalizing assistance through collaboration with older adults with MCI and their care partners. We concluded our findings in three key contexts--routine-based, real-time, and situational--that informed strategies for designing context-aware meal prep assistance tailored to users' needs. Our results provide actionable insights for creating technologies to assist meal preparation that are personalized for the unique lifestyles of older adults with MCI, situated in the complex and dynamic homebound context, and respecting the collaboration between older adults and their care partners."
  },
  {
    "title": "Joint User Association and Beamforming Design for ISAC Networks with Large Language Models",
    "url": "http://arxiv.org/abs/2506.05637v1",
    "arxiv_id": "2506.05637v1",
    "authors": [
      "Haoyun Li",
      "Ming Xiao",
      "Kezhi Wang",
      "Robert Schober",
      "Dong In Kim",
      "Yong Liang Guan"
    ],
    "published": "2025-06-05T23:41:11+00:00",
    "summary": "Integrated sensing and communication (ISAC) has been envisioned to play a more important role in future wireless networks. However, the design of ISAC networks is challenging, especially when there are multiple communication and sensing (C\\&S) nodes and multiple sensing targets. We investigate a multi-base station (BS) ISAC network in which multiple BSs equipped with multiple antennas simultaneously provide C\\&S services for multiple ground communication users (CUs) and targets. To enhance the overall performance of C\\&S, we formulate a joint user association (UA) and multi-BS transmit beamforming optimization problem with the objective of maximizing the total sum rate of all CUs while ensuring both the minimum target detection and parameter estimation requirements. To efficiently solve the highly non-convex mixed integer nonlinear programming (MINLP) optimization problem, we propose an alternating optimization (AO)-based algorithm that decomposes the problem into two sub-problems, i.e., UA optimization and multi-BS transmit beamforming optimization. Inspired by large language models (LLMs) for prediction and inference, we propose a unified framework integrating LLMs with convex-based optimization methods. First, we propose a comprehensive design of prompt engineering, including few-shot, chain of thought, and self-reflection techniques to guide LLMs in solving the binary integer programming UA optimization problem. Second, we utilize convex-based optimization methods to handle the non-convex beamforming optimization problem based on fractional programming (FP), majorization minimization (MM), and the alternating direction method of multipliers (ADMM) with an optimized UA from LLMs. Numerical results demonstrate that our proposed LLM-enabled AO-based algorithm achieves fast convergence and near upper-bound performance with the GPT-o1 model, outperforming various benchmark schemes."
  },
  {
    "title": "MMTU: A Massive Multi-Task Table Understanding and Reasoning Benchmark",
    "url": "http://arxiv.org/abs/2506.05587v1",
    "arxiv_id": "2506.05587v1",
    "authors": [
      "Junjie Xing",
      "Yeye He",
      "Mengyu Zhou",
      "Haoyu Dong",
      "Shi Han",
      "Lingjiao Chen",
      "Dongmei Zhang",
      "Surajit Chaudhuri",
      "H. V. Jagadish"
    ],
    "published": "2025-06-05T21:05:03+00:00",
    "summary": "Tables and table-based use cases play a crucial role in many important real-world applications, such as spreadsheets, databases, and computational notebooks, which traditionally require expert-level users like data engineers, data analysts, and database administrators to operate. Although LLMs have shown remarkable progress in working with tables (e.g., in spreadsheet and database copilot scenarios), comprehensive benchmarking of such capabilities remains limited. In contrast to an extensive and growing list of NLP benchmarks, evaluations of table-related tasks are scarce, and narrowly focus on tasks like NL-to-SQL and Table-QA, overlooking the broader spectrum of real-world tasks that professional users face. This gap limits our understanding and model progress in this important area.   In this work, we introduce MMTU, a large-scale benchmark with over 30K questions across 25 real-world table tasks, designed to comprehensively evaluate models ability to understand, reason, and manipulate real tables at the expert-level. These tasks are drawn from decades' worth of computer science research on tabular data, with a focus on complex table tasks faced by professional users. We show that MMTU require a combination of skills -- including table understanding, reasoning, and coding -- that remain challenging for today's frontier models, where even frontier reasoning models like OpenAI o4-mini and DeepSeek R1 score only around 60%, suggesting significant room for improvement. We highlight key findings in our evaluation using MMTU and hope that this benchmark drives further advances in understanding and developing foundation models for structured data processing and analysis. Our code and data are available at https://github.com/MMTU-Benchmark/MMTU and https://huggingface.co/datasets/MMTU-benchmark/MMTU."
  },
  {
    "title": "Improving LLMs with a knowledge from databases",
    "url": "http://arxiv.org/abs/2506.05560v1",
    "arxiv_id": "2506.05560v1",
    "authors": [
      "Petr M\u00e1\u0161a"
    ],
    "published": "2025-06-05T20:14:25+00:00",
    "summary": "Large language models (LLMs) are achieving significant progress almost every moment now. Many advanced techniques have been introduced and widely accepted, like retrieval-augmentation generation (RAG), agents, and tools. Tools can query the database to answer questions from structured data files or perform groupings or other statistics. This unlocks huge opportunities, such as it can answer any question, but also poses threats, such as safety, because there is no control over the commands that are created. We would like to discuss whether we can create a new method that improves answers based on dataset/database via some interpretable ML methods, namely enhanced association rules. The advantage would be if the method can be also used in some safe technique like RAG. Association rules have a sound history. Since the introduction of CN2 and aproiri, many enhancements have been made. In parallel, enhanced association rules have been introduced and evolved over the last 40 years. The general problem is typically that there are too many rules. There are some techniques for handling it, but when LLM emerged, it turned out to be the best use case for the RAG technique for LLMs. We proposed a method that generates a ruleset based on defined knowledge patterns, then converts rules into text form via a rule-to-text converter, and includes the result as an RAG into LLM. We compared this method with ChatGPT (even with using agents) and we have discovered a significant improvement in answering questions based on the dataset. We have also tried several strategies how much rules to generate. We found this improvement interesting. Moreover, it can also be improved in many ways as future work, like incorporating other patterns, the use of rule mining as an agent, and many others."
  },
  {
    "title": "MORSE-500: A Programmatically Controllable Video Benchmark to Stress-Test Multimodal Reasoning",
    "url": "http://arxiv.org/abs/2506.05523v1",
    "arxiv_id": "2506.05523v1",
    "authors": [
      "Zikui Cai",
      "Andrew Wang",
      "Anirudh Satheesh",
      "Ankit Nakhawa",
      "Hyunwoo Jae",
      "Keenan Powell",
      "Minghui Liu",
      "Neel Jay",
      "Sungbin Oh",
      "Xiyao Wang",
      "Yongyuan Liang",
      "Tom Goldstein",
      "Furong Huang"
    ],
    "published": "2025-06-05T19:12:45+00:00",
    "summary": "Despite rapid advances in vision-language models (VLMs), current benchmarks for multimodal reasoning fall short in three key dimensions. First, they overwhelmingly rely on static images, failing to capture the temporal complexity of real-world environments. Second, they narrowly focus on mathematical problem-solving, neglecting the broader spectrum of reasoning skills -- including abstract, physical, planning, spatial, and temporal capabilities -- required for robust multimodal intelligence. Third, many benchmarks quickly saturate, offering limited headroom for diagnosing failure modes or measuring continued progress. We introduce MORSE-500 (Multimodal Reasoning Stress-test Environment), a video benchmark composed of 500 fully scripted clips with embedded questions spanning six complementary reasoning categories. Each instance is programmatically generated using deterministic Python scripts (via Manim, Matplotlib, MoviePy), generative video models, and curated real footage. This script-driven design allows fine-grained control over visual complexity, distractor density, and temporal dynamics -- enabling difficulty to be scaled systematically as models improve. Unlike static benchmarks that become obsolete once saturated, MORSE-500 is built to evolve: its controllable generation pipeline supports the creation of arbitrarily challenging new instances, making it ideally suited for stress-testing next-generation models. Initial experiments with state-of-the-art systems -- including various Gemini 2.5 Pro and OpenAI o3 which represent the strongest available at the time, alongside strong open-source models -- reveal substantial performance gaps across all categories, with particularly large deficits in abstract and planning tasks. We release the full dataset, generation scripts, and evaluation harness to support transparent, reproducible, and forward-looking multimodal reasoning research."
  },
  {
    "title": "Understanding Community-Level Blocklists in Decentralized Social Media",
    "url": "http://arxiv.org/abs/2506.05522v1",
    "arxiv_id": "2506.05522v1",
    "authors": [
      "Owen Xingjian Zhang",
      "Sohyeon Hwang",
      "Yuhan Liu",
      "Manoel Horta Ribeiro",
      "Andr\u00e9s Monroy-Hern\u00e1ndez"
    ],
    "published": "2025-06-05T19:08:43+00:00",
    "summary": "Community-level blocklists are key to content moderation practices in decentralized social media. These blocklists enable moderators to prevent other communities, such as those acting in bad faith, from interacting with their own -- and, if shared publicly, warn others about communities worth blocking. Prior work has examined blocklists in centralized social media, noting their potential for collective moderation outcomes, but has focused on blocklists as individual-level tools. To understand how moderators perceive and utilize community-level blocklists and what additional support they may need, we examine social media communities running Mastodon, an open-source microblogging software built on the ActivityPub protocol. We conducted (1) content analysis of the community-level blocklist ecosystem, and (2) semi-structured interviews with twelve Mastodon moderators. Our content analysis revealed wide variation in blocklist goals, inclusion criteria, and transparency. Interviews showed moderators balance proactive safety, reactive practices, and caution around false positives when using blocklists for moderation. They noted challenges and limitations in current blocklist use, suggesting design improvements like comment receipts, category filters, and collaborative voting. We discuss implications for decentralized content moderation, highlighting trade-offs between openness, safety, and nuance; the complexity of moderator roles; and opportunities for future design."
  },
  {
    "title": "Why LLM Safety Guardrails Collapse After Fine-tuning: A Similarity Analysis Between Alignment and Fine-tuning Datasets",
    "url": "http://arxiv.org/abs/2506.05346v1",
    "arxiv_id": "2506.05346v1",
    "authors": [
      "Lei Hsiung",
      "Tianyu Pang",
      "Yung-Chen Tang",
      "Linyue Song",
      "Tsung-Yi Ho",
      "Pin-Yu Chen",
      "Yaoqing Yang"
    ],
    "published": "2025-06-05T17:59:55+00:00",
    "summary": "Recent advancements in large language models (LLMs) have underscored their vulnerability to safety alignment jailbreaks, particularly when subjected to downstream fine-tuning. However, existing mitigation strategies primarily focus on reactively addressing jailbreak incidents after safety guardrails have been compromised, removing harmful gradients during fine-tuning, or continuously reinforcing safety alignment throughout fine-tuning. As such, they tend to overlook a critical upstream factor: the role of the original safety-alignment data. This paper therefore investigates the degradation of safety guardrails through the lens of representation similarity between upstream alignment datasets and downstream fine-tuning tasks. Our experiments demonstrate that high similarity between these datasets significantly weakens safety guardrails, making models more susceptible to jailbreaks. Conversely, low similarity between these two types of datasets yields substantially more robust models and thus reduces harmfulness score by up to 10.33%. By highlighting the importance of upstream dataset design in the building of durable safety guardrails and reducing real-world vulnerability to jailbreak attacks, these findings offer actionable insights for fine-tuning service providers."
  },
  {
    "title": "Interpretation Meets Safety: A Survey on Interpretation Methods and Tools for Improving LLM Safety",
    "url": "http://arxiv.org/abs/2506.05451v1",
    "arxiv_id": "2506.05451v1",
    "authors": [
      "Seongmin Lee",
      "Aeree Cho",
      "Grace C. Kim",
      "ShengYun Peng",
      "Mansi Phute",
      "Duen Horng Chau"
    ],
    "published": "2025-06-05T17:56:05+00:00",
    "summary": "As large language models (LLMs) see wider real-world use, understanding and mitigating their unsafe behaviors is critical. Interpretation techniques can reveal causes of unsafe outputs and guide safety, but such connections with safety are often overlooked in prior surveys. We present the first survey that bridges this gap, introducing a unified framework that connects safety-focused interpretation methods, the safety enhancements they inform, and the tools that operationalize them. Our novel taxonomy, organized by LLM workflow stages, summarizes nearly 70 works at their intersections. We conclude with open challenges and future directions. This timely survey helps researchers and practitioners navigate key advancements for safer, more interpretable LLMs."
  },
  {
    "title": "Control Tax: The Price of Keeping AI in Check",
    "url": "http://arxiv.org/abs/2506.05296v1",
    "arxiv_id": "2506.05296v1",
    "authors": [
      "Mikhail Terekhov",
      "Zhen Ning David Liu",
      "Caglar Gulcehre",
      "Samuel Albanie"
    ],
    "published": "2025-06-05T17:48:39+00:00",
    "summary": "The rapid integration of agentic AI into high-stakes real-world applications requires robust oversight mechanisms. The emerging field of AI Control (AIC) aims to provide such an oversight mechanism, but practical adoption depends heavily on implementation overhead. To study this problem better, we introduce the notion of Control tax -- the operational and financial cost of integrating control measures into AI pipelines. Our work makes three key contributions to the field of AIC: (1) we introduce a theoretical framework that quantifies the Control Tax and maps classifier performance to safety assurances; (2) we conduct comprehensive evaluations of state-of-the-art language models in adversarial settings, where attacker models insert subtle backdoors into code while monitoring models attempt to detect these vulnerabilities; and (3) we provide empirical financial cost estimates for control protocols and develop optimized monitoring strategies that balance safety and cost-effectiveness while accounting for practical constraints like auditing budgets. Our framework enables practitioners to make informed decisions by systematically connecting safety guarantees with their costs, advancing AIC through principled economic feasibility assessment across different deployment contexts."
  },
  {
    "title": "Backward Responsibility in Transition Systems Beyond Safety",
    "url": "http://arxiv.org/abs/2506.05192v1",
    "arxiv_id": "2506.05192v1",
    "authors": [
      "Christel Baier",
      "Rio Klatt",
      "Sascha Kl\u00fcppelholz",
      "Johannes Lehmann"
    ],
    "published": "2025-06-05T16:04:41+00:00",
    "summary": "As the complexity of software systems rises, methods for explaining their behaviour are becoming ever-more important. When a system fails, it is critical to determine which of its components are responsible for this failure. Within the verification community, one approach uses graph games and the Shapley value to ascribe a responsibility value to every state of a transition system. As this is done with respect to a specific failure, it is called backward responsibility.   This paper provides tight complexity bounds for backward responsibility for reachability, B\\\"uchi and parity objectives. For B\\\"uchi objectives, a polynomial algorithm is given to determine the set of responsible states. To analyse systems that are too large for standard methods, the paper presents a novel refinement algorithm that iteratively computes responsibility and demonstrates its utility with a prototypical implementation."
  },
  {
    "title": "Towards provable probabilistic safety for scalable embodied AI systems",
    "url": "http://arxiv.org/abs/2506.05171v1",
    "arxiv_id": "2506.05171v1",
    "authors": [
      "Linxuan He",
      "Qing-Shan Jia",
      "Ang Li",
      "Hongyan Sang",
      "Ling Wang",
      "Jiwen Lu",
      "Tao Zhang",
      "Jie Zhou",
      "Yi Zhang",
      "Yisen Wang",
      "Peng Wei",
      "Zhongyuan Wang",
      "Henry X. Liu",
      "Shuo Feng"
    ],
    "published": "2025-06-05T15:46:25+00:00",
    "summary": "Embodied AI systems, comprising AI models and physical plants, are increasingly prevalent across various applications. Due to the rarity of system failures, ensuring their safety in complex operating environments remains a major challenge, which severely hinders their large-scale deployment in safety-critical domains, such as autonomous vehicles, medical devices, and robotics. While achieving provable deterministic safety--verifying system safety across all possible scenarios--remains theoretically ideal, the rarity and complexity of corner cases make this approach impractical for scalable embodied AI systems. To address this challenge, we introduce provable probabilistic safety, which aims to ensure that the residual risk of large-scale deployment remains below a predefined threshold. Instead of attempting exhaustive safety proof across all corner cases, this paradigm establishes a probabilistic safety boundary on overall system performance, leveraging statistical methods to enhance feasibility and scalability. A well-defined probabilistic safety boundary enables embodied AI systems to be deployed at scale while allowing for continuous refinement of safety guarantees. Our work focuses on three core questions: what is provable probabilistic safety, how to prove the probabilistic safety, and how to achieve the provable probabilistic safety. By bridging the gap between theoretical safety assurance and practical deployment, our work offers a pathway toward safer, large-scale adoption of embodied AI systems in safety-critical applications."
  },
  {
    "title": "Whole-Body Constrained Learning for Legged Locomotion via Hierarchical Optimization",
    "url": "http://arxiv.org/abs/2506.05115v1",
    "arxiv_id": "2506.05115v1",
    "authors": [
      "Haoyu Wang",
      "Ruyi Zhou",
      "Liang Ding",
      "Tie Liu",
      "Zhelin Zhang",
      "Peng Xu",
      "Haibo Gao",
      "Zongquan Deng"
    ],
    "published": "2025-06-05T15:00:27+00:00",
    "summary": "Reinforcement learning (RL) has demonstrated impressive performance in legged locomotion over various challenging environments. However, due to the sim-to-real gap and lack of explainability, unconstrained RL policies deployed in the real world still suffer from inevitable safety issues, such as joint collisions, excessive torque, or foot slippage in low-friction environments. These problems limit its usage in missions with strict safety requirements, such as planetary exploration, nuclear facility inspection, and deep-sea operations. In this paper, we design a hierarchical optimization-based whole-body follower, which integrates both hard and soft constraints into RL framework to make the robot move with better safety guarantees. Leveraging the advantages of model-based control, our approach allows for the definition of various types of hard and soft constraints during training or deployment, which allows for policy fine-tuning and mitigates the challenges of sim-to-real transfer. Meanwhile, it preserves the robustness of RL when dealing with locomotion in complex unstructured environments. The trained policy with introduced constraints was deployed in a hexapod robot and tested in various outdoor environments, including snow-covered slopes and stairs, demonstrating the great traversability and safety of our approach."
  },
  {
    "title": "FG 2025 TrustFAA: the First Workshop on Towards Trustworthy Facial Affect Analysis: Advancing Insights of Fairness, Explainability, and Safety (TrustFAA)",
    "url": "http://arxiv.org/abs/2506.05095v1",
    "arxiv_id": "2506.05095v1",
    "authors": [
      "Jiaee Cheong",
      "Yang Liu",
      "Harold Soh",
      "Hatice Gunes"
    ],
    "published": "2025-06-05T14:40:49+00:00",
    "summary": "With the increasing prevalence and deployment of Emotion AI-powered facial affect analysis (FAA) tools, concerns about the trustworthiness of these systems have become more prominent. This first workshop on \"Towards Trustworthy Facial Affect Analysis: Advancing Insights of Fairness, Explainability, and Safety (TrustFAA)\" aims to bring together researchers who are investigating different challenges in relation to trustworthiness-such as interpretability, uncertainty, biases, and privacy-across various facial affect analysis tasks, including macro/ micro-expression recognition, facial action unit detection, other corresponding applications such as pain and depression detection, as well as human-robot interaction and collaboration. In alignment with FG2025's emphasis on ethics, as demonstrated by the inclusion of an Ethical Impact Statement requirement for this year's submissions, this workshop supports FG2025's efforts by encouraging research, discussion and dialogue on trustworthy FAA."
  },
  {
    "title": "Tech-ASan: Two-stage check for Address Sanitizer",
    "url": "http://arxiv.org/abs/2506.05022v1",
    "arxiv_id": "2506.05022v1",
    "authors": [
      "Yixuan Cao",
      "Yuhong Feng",
      "Huafeng Li",
      "Chongyi Huang",
      "Fangcao Jian",
      "Haoran Li",
      "Xu Wang"
    ],
    "published": "2025-06-05T13:32:48+00:00",
    "summary": "Address Sanitizer (ASan) is a sharp weapon for detecting memory safety violations, including temporal and spatial errors hidden in C/C++ programs during execution. However, ASan incurs significant runtime overhead, which limits its efficiency in testing large software. The overhead mainly comes from sanitizer checks due to the frequent and expensive shadow memory access. Over the past decade, many methods have been developed to speed up ASan by eliminating and accelerating sanitizer checks, however, they either fail to adequately eliminate redundant checks or compromise detection capabilities. To address this issue, this paper presents Tech-ASan, a two-stage check based technique to accelerate ASan with safety assurance. First, we propose a novel two-stage check algorithm for ASan, which leverages magic value comparison to reduce most of the costly shadow memory accesses. Second, we design an efficient optimizer to eliminate redundant checks, which integrates a novel algorithm for removing checks in loops. Third, we implement Tech-ASan as a memory safety tool based on the LLVM compiler infrastructure. Our evaluation using the SPEC CPU2006 benchmark shows that Tech-ASan outperforms the state-of-the-art methods with 33.70% and 17.89% less runtime overhead than ASan and ASan--, respectively. Moreover, Tech-ASan detects 56 fewer false negative cases than ASan and ASan-- when testing on the Juliet Test Suite under the same redzone setting."
  },
  {
    "title": "Characterization of NbTi wires for the electron-ion collider project",
    "url": "http://arxiv.org/abs/2506.05004v1",
    "arxiv_id": "2506.05004v1",
    "authors": [
      "Jun Lu",
      "Jeremy Levitan Noah Gavin",
      "Aniket Ingrole",
      "Holger Witte",
      "Peng Xu",
      "Ye Bai"
    ],
    "published": "2025-06-05T13:13:40+00:00",
    "summary": "The Electron-Ion Collider (EIC) is a proposed machine to explore the behaviour of the fundamental particles and forces that bind atomic nuclei together. The design and construction of the EIC are underway at Brookhaven National Laboratory in collaboration with Thomas Jefferson National Accelerator Facility. EIC will use several different types of superconducting strands for magnets near the interaction region (IR). At beam injection, the magnetic field is usually very low compared with its maximum operating field. This usually creates considerable field errors mainly generated from magnetization current in superconducting strands even using very fine filament. The accurate magnetization measurement results from those superconducting strands will be critical for the calculation and future correction of magnetic field for EIC. In this work, we characterized three billets of superconductor NbTi strands. The magnetization was measured at 4.2 K and 1.9 K in magnetic fields below 1.5 T. The critical current at 4.2 K and in magnetic field down to 5 T were also measured. Other properties that are important for the safety margin of superconducting magnet fabrication, operation, and quench protection such as residual-resistance-ratio (RRR), filament diameter, Cu to non-Cu ratio, twist pitch, and mechanical properties at 77 K will also be presented."
  },
  {
    "title": "Mathematical Reasoning for Unmanned Aerial Vehicles: A RAG-Based Approach for Complex Arithmetic Reasoning",
    "url": "http://arxiv.org/abs/2506.04998v1",
    "arxiv_id": "2506.04998v1",
    "authors": [
      "Mehdi Azarafza",
      "Mojtaba Nayyeri",
      "Faezeh Pasandideh",
      "Steffen Staab",
      "Achim Rettberg"
    ],
    "published": "2025-06-05T13:09:24+00:00",
    "summary": "Autonomous UAV operation necessitates reliable mathematical reasoning for tasks such as trajectory planning and power management. While traditional flight control relies on hardcoded equations, recent Large Language Models (LLMs) offer potential for more flexible problem-solving but struggle with reliably selecting and applying correct mathematical formulations and executing precise multi-step arithmetic. We propose RAG-UAV, a retrieval-augmented generation framework designed to improve the mathematical reasoning of several LLMs (including GPT o1/Turbo, Llama-3.2/3.3, Mistral, and DeepSeek R1) in UAV-specific contexts by providing access to relevant domain literature. To conduct an initial assessment, we introduce the UAV-Math-Bench, a small problem set comprising 20 UAV-centric mathematical problems across four difficulty levels. Our experiments demonstrate that incorporating retrieval substantially increases exact answer accuracy (achieving up to 75% with o1), reduces instances of incorrect formulation selection (from 25% without RAG to 5% with RAG), decreases numerical errors, reducing Mean Squared Error (MSE) by orders of magnitude for the best-performing models. This pilot study indicates that RAG can enable general-purpose LLMs to function as more reliable tools for engineering analysis, although direct real-time flight control requires further investigation and validation on a larger scale. All benchmark data, question and answer are publicly available."
  },
  {
    "title": "Evaluating Prompt-Driven Chinese Large Language Models: The Influence of Persona Assignment on Stereotypes and Safeguards",
    "url": "http://arxiv.org/abs/2506.04975v1",
    "arxiv_id": "2506.04975v1",
    "authors": [
      "Geng Liu",
      "Li Feng",
      "Carlo Alberto Bono",
      "Songbo Yang",
      "Mengxiao Zhu",
      "Francesco Pierri"
    ],
    "published": "2025-06-05T12:47:21+00:00",
    "summary": "Recent research has highlighted that assigning specific personas to large language models (LLMs) can significantly increase harmful content generation. Yet, limited attention has been given to persona-driven toxicity in non-Western contexts, particularly in Chinese-based LLMs. In this paper, we perform a large-scale, systematic analysis of how persona assignment influences refusal behavior and response toxicity in Qwen, a widely-used Chinese language model. Utilizing fine-tuned BERT classifiers and regression analysis, our study reveals significant gender biases in refusal rates and demonstrates that certain negative personas can amplify toxicity toward Chinese social groups by up to 60-fold compared to the default model. To mitigate this toxicity, we propose an innovative multi-model feedback strategy, employing iterative interactions between Qwen and an external evaluator, which effectively reduces toxic outputs without costly model retraining. Our findings emphasize the necessity of culturally specific analyses for LLMs safety and offer a practical framework for evaluating and enhancing ethical alignment in LLM-generated content."
  },
  {
    "title": "ICPC-Eval: Probing the Frontiers of LLM Reasoning with Competitive Programming Contests",
    "url": "http://arxiv.org/abs/2506.04894v1",
    "arxiv_id": "2506.04894v1",
    "authors": [
      "Shiyi Xu",
      "Yiwen Hu",
      "Yingqian Min",
      "Zhipeng Chen",
      "Wayne Xin Zhao",
      "Ji-Rong Wen"
    ],
    "published": "2025-06-05T11:20:37+00:00",
    "summary": "With the significant progress of large reasoning models in complex coding and reasoning tasks, existing benchmarks, like LiveCodeBench and CodeElo, are insufficient to evaluate the coding capabilities of large language models (LLMs) in real competition environments. Moreover, current evaluation metrics such as Pass@K fail to capture the reflective abilities of reasoning models. To address these challenges, we propose \\textbf{ICPC-Eval}, a top-level competitive coding benchmark designed to probing the frontiers of LLM reasoning. ICPC-Eval includes 118 carefully curated problems from 11 recent ICPC contests held in various regions of the world, offering three key contributions: 1) A challenging realistic ICPC competition scenario, featuring a problem type and difficulty distribution consistent with actual contests. 2) A robust test case generation method and a corresponding local evaluation toolkit, enabling efficient and accurate local evaluation. 3) An effective test-time scaling evaluation metric, Refine@K, which allows iterative repair of solutions based on execution feedback. The results underscore the significant challenge in evaluating complex reasoning abilities: top-tier reasoning models like DeepSeek-R1 often rely on multi-turn code feedback to fully unlock their in-context reasoning potential when compared to non-reasoning counterparts. Furthermore, despite recent advancements in code generation, these models still lag behind top-performing human teams. We release the benchmark at: https://github.com/RUCAIBox/Slow_Thinking_with_LLMs"
  },
  {
    "title": "A MARL-based Approach for Easing MAS Organization Engineering",
    "url": "http://arxiv.org/abs/2506.05437v1",
    "arxiv_id": "2506.05437v1",
    "authors": [
      "Julien Soul\u00e9",
      "Jean-Paul Jamont",
      "Michel Occello",
      "Louis-Marie Traonouez",
      "Paul Th\u00e9ron"
    ],
    "published": "2025-06-05T09:59:36+00:00",
    "summary": "Multi-Agent Systems (MAS) have been successfully applied in industry for their ability to address complex, distributed problems, especially in IoT-based systems. Their efficiency in achieving given objectives and meeting design requirements is strongly dependent on the MAS organization during the engineering process of an application-specific MAS. To design a MAS that can achieve given goals, available methods rely on the designer's knowledge of the deployment environment. However, high complexity and low readability in some deployment environments make the application of these methods to be costly or raise safety concerns. In order to ease the MAS organization design regarding those concerns, we introduce an original Assisted MAS Organization Engineering Approach (AOMEA). AOMEA relies on combining a Multi-Agent Reinforcement Learning (MARL) process with an organizational model to suggest relevant organizational specifications to help in MAS engineering."
  },
  {
    "title": "Safe Planning and Policy Optimization via World Model Learning",
    "url": "http://arxiv.org/abs/2506.04828v1",
    "arxiv_id": "2506.04828v1",
    "authors": [
      "Artem Latyshev",
      "Gregory Gorbov",
      "Aleksandr I. Panov"
    ],
    "published": "2025-06-05T09:50:02+00:00",
    "summary": "Reinforcement Learning (RL) applications in real-world scenarios must prioritize safety and reliability, which impose strict constraints on agent behavior. Model-based RL leverages predictive world models for action planning and policy optimization, but inherent model inaccuracies can lead to catastrophic failures in safety-critical settings. We propose a novel model-based RL framework that jointly optimizes task performance and safety. To address world model errors, our method incorporates an adaptive mechanism that dynamically switches between model-based planning and direct policy execution. We resolve the objective mismatch problem of traditional model-based approaches using an implicit world model. Furthermore, our framework employs dynamic safety thresholds that adapt to the agent's evolving capabilities, consistently selecting actions that surpass safe policy suggestions in both performance and safety. Experiments demonstrate significant improvements over non-adaptive methods, showing that our approach optimizes safety and performance simultaneously rather than merely meeting minimum safety requirements. The proposed framework achieves robust performance on diverse safety-critical continuous control tasks, outperforming existing methods."
  },
  {
    "title": "Multi-Layer GRPO: Enhancing Reasoning and Self-Correction in Large Language Models",
    "url": "http://arxiv.org/abs/2506.04746v1",
    "arxiv_id": "2506.04746v1",
    "authors": [
      "Fei Ding",
      "Baiqiao Wang",
      "Zijian Zeng",
      "Youwei Wang"
    ],
    "published": "2025-06-05T08:27:34+00:00",
    "summary": "The Group Relative Policy Optimization (GRPO) algorithm has demonstrated considerable success in enhancing the reasoning capabilities of large language models (LLMs), as evidenced by DeepSeek-R1. However, the absence of intermediate supervision in GRPO frequently leads to inefficient exploration dynamics. A single error in a complex reasoning chain can invalidate the entire solution, resulting in abrupt reward vanishing and compromising training stability.To address these challenges, we propose MGRPO (Multi-layer GRPO). MGRPO operates in two layers: the first layer employs standard GRPO to generate an initial response. This response, along with the original query, is then fed into a second-layer GRPO process. This second layer is specifically trained to identify and correct errors in the initial response, effectively creating a self-correction loop. This mechanism provides implicit process-level supervision by rewarding successful error correction, without requiring an explicit, densely-annotated reward model. Experimental results on several mathematical reasoning benchmarks demonstrate that MGRPO significantly outperforms standard GRPO, achieving superior performance by fostering both reasoning and self-correction abilities."
  },
  {
    "title": "Evaluation is All You Need: Strategic Overclaiming of LLM Reasoning Capabilities Through Evaluation Design",
    "url": "http://arxiv.org/abs/2506.04734v1",
    "arxiv_id": "2506.04734v1",
    "authors": [
      "Lin Sun",
      "Weihong Lin",
      "Jinzhu Wu",
      "Yongfu Zhu",
      "Xiaoqi Jian",
      "Guangxiang Zhao",
      "Change Jia",
      "Linglin Zhang",
      "Sai-er Hu",
      "Yuhan Wu",
      "Xiangzheng Zhang"
    ],
    "published": "2025-06-05T08:09:11+00:00",
    "summary": "Reasoning models represented by the Deepseek-R1-Distill series have been widely adopted by the open-source community due to their strong performance in mathematics, science, programming, and other domains. However, our study reveals that their benchmark evaluation results are subject to significant fluctuations caused by various factors. Subtle differences in evaluation conditions can lead to substantial variations in results. Similar phenomena are observed in other open-source inference models fine-tuned based on the Deepseek-R1-Distill series, as well as in the QwQ-32B model, making their claimed performance improvements difficult to reproduce reliably. Therefore, we advocate for the establishment of a more rigorous paradigm for model performance evaluation and present our empirical assessments of the Deepseek-R1-Distill series models."
  },
  {
    "title": "HoliSafe: Holistic Safety Benchmarking and Modeling with Safety Meta Token for Vision-Language Model",
    "url": "http://arxiv.org/abs/2506.04704v1",
    "arxiv_id": "2506.04704v1",
    "authors": [
      "Youngwan Lee",
      "Kangsan Kim",
      "Kwanyong Park",
      "Ilcahe Jung",
      "Soojin Jang",
      "Seanie Lee",
      "Yong-Ju Lee",
      "Sung Ju Hwang"
    ],
    "published": "2025-06-05T07:26:34+00:00",
    "summary": "Despite emerging efforts to enhance the safety of Vision-Language Models (VLMs), current approaches face two main shortcomings. 1) Existing safety-tuning datasets and benchmarks only partially consider how image-text interactions can yield harmful content, often overlooking contextually unsafe outcomes from seemingly benign pairs. This narrow coverage leaves VLMs vulnerable to jailbreak attacks in unseen configurations. 2) Prior methods rely primarily on data-centric tuning, with limited architectural innovations to intrinsically strengthen safety. We address these gaps by introducing a holistic safety dataset and benchmark, HoliSafe, that spans all five safe/unsafe image-text combinations, providing a more robust basis for both training and evaluation. We further propose SafeLLaVA, a novel VLM augmented with a learnable safety meta token and a dedicated safety head. The meta token encodes harmful visual cues during training, intrinsically guiding the language model toward safer responses, while the safety head offers interpretable harmfulness classification aligned with refusal rationales. Experiments show that SafeLLaVA, trained on HoliSafe, achieves state-of-the-art safety performance across multiple VLM benchmarks. Additionally, the HoliSafe benchmark itself reveals critical vulnerabilities in existing models. We hope that HoliSafe and SafeLLaVA will spur further research into robust and interpretable VLM safety, expanding future avenues for multimodal alignment."
  },
  {
    "title": "Energy Consumption Optimization for Autonomous Vehicles via Positive Control Input Minimization",
    "url": "http://arxiv.org/abs/2506.04685v1",
    "arxiv_id": "2506.04685v1",
    "authors": [
      "Andreas Hadjigeorgiou",
      "Stelios Timotheou"
    ],
    "published": "2025-06-05T07:06:16+00:00",
    "summary": "Autonomous vehicles (AVs) present a unique opportunity to improve the sustainability of transportation systems by adopting eco-driving strategies that reduce energy consumption and emissions. This paper introduces a novel surrogate model for energy and fuel consumption that minimizes Positive Control Input (PCI). Unlike conventional objectives such as squared acceleration, which often misrepresent actual energy usage, PCI provides a more accurate and optimization-friendly alternative. Building on PCI, we propose ECO+, a convex, time-based trajectory optimization framework that ensures safety and passenger comfort while optimizing energy use for AVs approaching an intersection. To improve computational efficiency, quadratic resistive forces are approximated using piecewise affine segments, resulting in a linear programming formulation. ECO+ is validated using empirical fuel and electric energy models and benchmarked against established optimization strategies, including a state-of-the-art nonlinear solver. Simulation results demonstrate that ECO+ consistently outperforms baseline methods in reducing energy consumption, even under strict comfort constraints and in scenarios involving a leading vehicle. Moreover, initializing a nonlinear solver with ECO+ yields only marginal gains, indicating that ECO+ is effective as a standalone eco-driving strategy. These findings highlight ECO+ as a practical, scalable, and computationally efficient solution for enhancing the sustainability of autonomous urban mobility systems."
  },
  {
    "title": "Real-Time LPV-Based Non-Linear Model Predictive Control for Robust Trajectory Tracking in Autonomous Vehicles",
    "url": "http://arxiv.org/abs/2506.04684v1",
    "arxiv_id": "2506.04684v1",
    "authors": [
      "Nitish Kumar",
      "Rajalakshmi Pachamuthu"
    ],
    "published": "2025-06-05T07:04:10+00:00",
    "summary": "This paper presents the development and implementation of a Model Predictive Control (MPC) framework for trajectory tracking in autonomous vehicles under diverse driving conditions. The proposed approach incorporates a modular architecture that integrates state estimation, vehicle dynamics modeling, and optimization to ensure real-time performance. The state-space equations are formulated in a Linear Parameter Varying (LPV) form, and a curvature-based tuning method is introduced to optimize weight matrices for varying trajectories. The MPC framework is implemented using the Robot Operating System (ROS) for parallel execution of state estimation and control optimization, ensuring scalability and minimal latency. Extensive simulations and real-time experiments were conducted on multiple predefined trajectories, demonstrating high accuracy with minimal cross-track and orientation errors, even under aggressive maneuvers and high-speed conditions. The results highlight the robustness and adaptability of the proposed system, achieving seamless alignment between simulated and real-world performance. This work lays the foundation for dynamic weight tuning and integration into cooperative autonomous navigation systems, paving the way for enhanced safety and efficiency in autonomous driving applications."
  },
  {
    "title": "Normative Conflicts and Shallow AI Alignment",
    "url": "http://arxiv.org/abs/2506.04679v1",
    "arxiv_id": "2506.04679v1",
    "authors": [
      "Rapha\u00ebl Milli\u00e8re"
    ],
    "published": "2025-06-05T06:57:28+00:00",
    "summary": "The progress of AI systems such as large language models (LLMs) raises increasingly pressing concerns about their safe deployment. This paper examines the value alignment problem for LLMs, arguing that current alignment strategies are fundamentally inadequate to prevent misuse. Despite ongoing efforts to instill norms such as helpfulness, honesty, and harmlessness in LLMs through fine-tuning based on human preferences, they remain vulnerable to adversarial attacks that exploit conflicts between these norms. I argue that this vulnerability reflects a fundamental limitation of existing alignment methods: they reinforce shallow behavioral dispositions rather than endowing LLMs with a genuine capacity for normative deliberation. Drawing from on research in moral psychology, I show how humans' ability to engage in deliberative reasoning enhances their resilience against similar adversarial tactics. LLMs, by contrast, lack a robust capacity to detect and rationally resolve normative conflicts, leaving them susceptible to manipulation; even recent advances in reasoning-focused LLMs have not addressed this vulnerability. This ``shallow alignment'' problem carries significant implications for AI safety and regulation, suggesting that current approaches are insufficient for mitigating potential harms posed by increasingly capable AI systems."
  },
  {
    "title": "E-bike agents: Large Language Model-Driven E-Bike Accident Analysis and Severity Prediction",
    "url": "http://arxiv.org/abs/2506.04654v1",
    "arxiv_id": "2506.04654v1",
    "authors": [
      "Zhichao Yang",
      "Jiashu He",
      "Mohammad B. Al-Khasawneh",
      "Darshan Pandit",
      "Cirillo Cinzia"
    ],
    "published": "2025-06-05T05:49:41+00:00",
    "summary": "Electric bicycles (e-bikes) are rapidly increasing in use, raising safety concerns due to a rise in accident reports. However, e-bike incident reports often use unstructured narrative formats, which hinders quantitative safety analysis. This study introduces E-bike agents, a framework that uses large language models (LLM) powered agents to classify and extract safety variables from unstructured incident reports. Our framework consists of four LLM agents, handling data classification, information extraction, injury cause determination, and component linkage, to extract the key factors that could lead to E-bike accidents and cause varying severity levels. Furthermore, we used an ordered logit model to examine the relationship between the severity of the incident and the factors retrieved, such as gender, the type of cause, and environmental conditions. Our research shows that equipment issues are slightly more common than human-related ones, but human-related incidents are more often fatal. Specifically, pedals, tires, and brakes are frequent contributors to accidents. The model achieves a high weighted F1 score of 0.87 in classification accuracy, highlighting the potential of using LLMs to extract unstructured data in niche domains, such as transportation. Our method offers a scalable solution to improve e-bike safety analytics and provides actionable information for policy makers, designers, and regulators."
  },
  {
    "title": "Unfolding Spatial Cognition: Evaluating Multimodal Models on Visual Simulations",
    "url": "http://arxiv.org/abs/2506.04633v1",
    "arxiv_id": "2506.04633v1",
    "authors": [
      "Linjie Li",
      "Mahtab Bigverdi",
      "Jiawei Gu",
      "Zixian Ma",
      "Yinuo Yang",
      "Ziang Li",
      "Yejin Choi",
      "Ranjay Krishna"
    ],
    "published": "2025-06-05T05:09:46+00:00",
    "summary": "Spatial cognition is essential for human intelligence, enabling problem-solving through visual simulations rather than solely relying on verbal reasoning. However, existing AI benchmarks primarily assess verbal reasoning, neglecting the complexities of non-verbal, multi-step visual simulation. We introduce STARE(Spatial Transformations and Reasoning Evaluation), a benchmark designed to rigorously evaluate multimodal large language models on tasks better solved through multi-step visual simulation. STARE features 4K tasks spanning foundational geometric transformations (2D and 3D), integrated spatial reasoning (cube net folding and tangram puzzles), and real-world spatial reasoning (perspective and temporal reasoning), reflecting practical cognitive challenges like object assembly, mechanical diagram interpretation, and everyday spatial navigation. Our evaluations show that models excel at reasoning over simpler 2D transformations, but perform close to random chance on more complex tasks like 3D cube net folding and tangram puzzles that require multi-step visual simulations. Humans achieve near-perfect accuracy but take considerable time (up to 28.9s) on complex tasks, significantly speeding up (down by 7.5 seconds on average) with intermediate visual simulations. In contrast, models exhibit inconsistent performance gains from visual simulations, improving on most tasks but declining in specific cases like tangram puzzles (GPT-4o, o1) and cube net folding (Claude-3.5, Gemini-2.0 Flash), indicating that models may not know how to effectively leverage intermediate visual information."
  },
  {
    "title": "Composing Agents to Minimize Worst-case Risk",
    "url": "http://arxiv.org/abs/2506.04632v1",
    "arxiv_id": "2506.04632v1",
    "authors": [
      "Guruprerana Shabadi",
      "Rajeev Alur"
    ],
    "published": "2025-06-05T05:04:44+00:00",
    "summary": "From software development to robot control, modern agentic systems decompose complex objectives into a sequence of subtasks and choose a set of specialized AI agents to complete them. We formalize an agentic workflow as a directed acyclic graph, called an agent graph, where edges represent AI agents and paths correspond to feasible compositions of agents. When deploying these systems in the real world, we need to choose compositions of agents that not only maximize the task success, but also minimize risk where the risk captures requirements like safety, fairness, and privacy. This additionally requires carefully analyzing the low-probability (tail) behaviors of compositions of agents. In this work, we consider worst-case risk minimization over the set of feasible agent compositions. We define worst-case risk as the tail quantile -- also known as value-at-risk -- of the loss distribution of the agent composition where the loss quantifies the risk associated with agent behaviors. We introduce an efficient algorithm that traverses the agent graph and finds a near-optimal composition of agents by approximating the value-at-risk via a union bound and dynamic programming. Furthermore, we prove that the approximation is near-optimal asymptotically for a broad class of practical loss functions. To evaluate our framework, we consider a suite of video game-like control benchmarks that require composing several agents trained with reinforcement learning and demonstrate our algorithm's effectiveness in approximating the value-at-risk and identifying the optimal agent composition."
  },
  {
    "title": "Reasoning or Overthinking: Evaluating Large Language Models on Financial Sentiment Analysis",
    "url": "http://arxiv.org/abs/2506.04574v1",
    "arxiv_id": "2506.04574v1",
    "authors": [
      "Dimitris Vamvourellis",
      "Dhagash Mehta"
    ],
    "published": "2025-06-05T02:47:23+00:00",
    "summary": "We investigate the effectiveness of large language models (LLMs), including reasoning-based and non-reasoning models, in performing zero-shot financial sentiment analysis. Using the Financial PhraseBank dataset annotated by domain experts, we evaluate how various LLMs and prompting strategies align with human-labeled sentiment in a financial context. We compare three proprietary LLMs (GPT-4o, GPT-4.1, o3-mini) under different prompting paradigms that simulate System 1 (fast and intuitive) or System 2 (slow and deliberate) thinking and benchmark them against two smaller models (FinBERT-Prosus, FinBERT-Tone) fine-tuned on financial sentiment analysis. Our findings suggest that reasoning, either through prompting or inherent model design, does not improve performance on this task. Surprisingly, the most accurate and human-aligned combination of model and method was GPT-4o without any Chain-of-Thought (CoT) prompting. We further explore how performance is impacted by linguistic complexity and annotation agreement levels, uncovering that reasoning may introduce overthinking, leading to suboptimal predictions. This suggests that for financial sentiment classification, fast, intuitive \"System 1\"-like thinking aligns more closely with human judgment compared to \"System 2\"-style slower, deliberative reasoning simulated by reasoning models or CoT prompting. Our results challenge the default assumption that more reasoning always leads to better LLM decisions, particularly in high-stakes financial applications."
  },
  {
    "title": "Demonstrations of Integrity Attacks in Multi-Agent Systems",
    "url": "http://arxiv.org/abs/2506.04572v1",
    "arxiv_id": "2506.04572v1",
    "authors": [
      "Can Zheng",
      "Yuhan Cao",
      "Xiaoning Dong",
      "Tianxing He"
    ],
    "published": "2025-06-05T02:44:49+00:00",
    "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in natural language understanding, code generation, and complex planning. Simultaneously, Multi-Agent Systems (MAS) have garnered attention for their potential to enable cooperation among distributed agents. However, from a multi-party perspective, MAS could be vulnerable to malicious agents that exploit the system to serve self-interests without disrupting its core functionality. This work explores integrity attacks where malicious agents employ subtle prompt manipulation to bias MAS operations and gain various benefits. Four types of attacks are examined: \\textit{Scapegoater}, who misleads the system monitor to underestimate other agents' contributions; \\textit{Boaster}, who misleads the system monitor to overestimate their own performance; \\textit{Self-Dealer}, who manipulates other agents to adopt certain tools; and \\textit{Free-Rider}, who hands off its own task to others. We demonstrate that strategically crafted prompts can introduce systematic biases in MAS behavior and executable instructions, enabling malicious agents to effectively mislead evaluation systems and manipulate collaborative agents. Furthermore, our attacks can bypass advanced LLM-based monitors, such as GPT-4o-mini and o3-mini, highlighting the limitations of current detection mechanisms. Our findings underscore the critical need for MAS architectures with robust security protocols and content validation mechanisms, alongside monitoring systems capable of comprehensive risk scenario assessment."
  },
  {
    "title": "Perceptual Decoupling for Scalable Multi-modal Reasoning via Reward-Optimized Captioning",
    "url": "http://arxiv.org/abs/2506.04559v1",
    "arxiv_id": "2506.04559v1",
    "authors": [
      "Yunhao Gou",
      "Kai Chen",
      "Zhili Liu",
      "Lanqing Hong",
      "Xin Jin",
      "Zhenguo Li",
      "James T. Kwok",
      "Yu Zhang"
    ],
    "published": "2025-06-05T02:28:07+00:00",
    "summary": "Recent advances in slow-thinking language models (e.g., OpenAI-o1 and DeepSeek-R1) have demonstrated remarkable abilities in complex reasoning tasks by emulating human-like reflective cognition. However, extending such capabilities to multi-modal large language models (MLLMs) remains challenging due to the high cost of retraining vision-language alignments when upgrading the underlying reasoner LLMs. A straightforward solution is to decouple perception from reasoning, i.e., converting visual inputs into language representations (e.g., captions) that are then passed to a powerful text-only reasoner. However, this decoupling introduces a critical challenge: the visual extractor must generate descriptions that are both faithful to the image and informative enough to support accurate downstream reasoning. To address this, we propose Reasoning-Aligned Perceptual Decoupling via Caption Reward Optimization (RACRO) - a reasoning-guided reinforcement learning strategy that aligns the extractor's captioning behavior with the reasoning objective. By closing the perception-reasoning loop via reward-based optimization, RACRO significantly enhances visual grounding and extracts reasoning-optimized representations. Experiments on multi-modal math and science benchmarks show that the proposed RACRO method achieves state-of-the-art average performance while enabling superior scalability and plug-and-play adaptation to more advanced reasoning LLMs without the necessity for costly multi-modal re-alignment."
  },
  {
    "title": "EECD-Net: Energy-Efficient Crack Detection with Spiking Neural Networks and Gated Attention",
    "url": "http://arxiv.org/abs/2506.04526v1",
    "arxiv_id": "2506.04526v1",
    "authors": [
      "Shuo Zhang"
    ],
    "published": "2025-06-05T00:19:36+00:00",
    "summary": "Crack detection on road surfaces is a critical measurement technology in the instrumentation domain, essential for ensuring infrastructure safety and transportation reliability. However, due to limited energy and low-resolution imaging, smart terminal devices struggle to maintain real-time monitoring performance. To overcome these challenges, this paper proposes a multi-stage detection approach for road crack detection, EECD-Net, to enhance accuracy and energy efficiency of instrumentation. Specifically, the sophisticated Super-Resolution Convolutional Neural Network (SRCNN) is employed to address the inherent challenges of low-quality images, which effectively enhance image resolution while preserving critical structural details. Meanwhile, a Spike Convolution Unit (SCU) with Continuous Integrate-and-Fire (CIF) neurons is proposed to convert these images into sparse pulse sequences, significantly reducing power consumption. Additionally, a Gated Attention Transformer (GAT) module is designed to strategically fuse multi-scale feature representations through adaptive attention mechanisms, effectively capturing both long-range dependencies and intricate local crack patterns, and significantly enhancing detection robustness across varying crack morphologies. The experiments on the CrackVision12K benchmark demonstrate that EECD-Net achieves a remarkable 98.6\\% detection accuracy, surpassing state-of-the-art counterparts such as Hybrid-Segmentor by a significant 1.5\\%. Notably, the EECD-Net maintains exceptional energy efficiency, consuming merely 5.6 mJ, which is a substantial 33\\% reduction compared to baseline implementations. This work pioneers a transformative approach in instrumentation-based crack detection, offering a scalable, low-power solution for real-time, large-scale infrastructure monitoring in resource-constrained environments."
  },
  {
    "title": "Comparative performance of ensemble models in predicting dental provider types: insights from fee-for-service data",
    "url": "http://arxiv.org/abs/2506.04479v1",
    "arxiv_id": "2506.04479v1",
    "authors": [
      "Mohammad Subhi Al-Batah",
      "Muhyeeddin Alqaraleh",
      "Mowafaq Salem Alzboon",
      "Abdullah Alourani"
    ],
    "published": "2025-06-04T21:55:27+00:00",
    "summary": "Dental provider classification plays a crucial role in optimizing healthcare resource allocation and policy planning. Effective categorization of providers, such as standard rendering providers and safety net clinic (SNC) providers, enhances service delivery to underserved populations. This study aimed to evaluate the performance of machine learning models in classifying dental providers using a 2018 dataset. A dataset of 24,300 instances with 20 features was analyzed, including beneficiary and service counts across fee-for-service (FFS), Geographic Managed Care, and Pre-Paid Health Plans. Providers were categorized by delivery system and patient age groups (0-20 and 21+). Despite 38.1% missing data, multiple machine learning algorithms were tested, including k-Nearest Neighbors (kNN), Decision Trees, Support Vector Machines (SVM), Stochastic Gradient Descent (SGD), Random Forest, Neural Networks, and Gradient Boosting. A 10-fold cross-validation approach was applied, and models were evaluated using AUC, classification accuracy (CA), F1-score, precision, and recall. Neural Networks achieved the highest AUC (0.975) and CA (94.1%), followed by Random Forest (AUC: 0.948, CA: 93.0%). These models effectively handled imbalanced data and complex feature interactions, outperforming traditional classifiers like Logistic Regression and SVM. Advanced machine learning techniques, particularly ensemble and deep learning models, significantly enhance dental workforce classification. Their integration into healthcare analytics can improve provider identification and resource distribution, benefiting underserved populations."
  },
  {
    "title": "Classifying Dental Care Providers Through Machine Learning with Features Ranking",
    "url": "http://arxiv.org/abs/2506.04474v1",
    "arxiv_id": "2506.04474v1",
    "authors": [
      "Mohammad Subhi Al-Batah",
      "Mowafaq Salem Alzboon",
      "Muhyeeddin Alqaraleh",
      "Mohammed Hasan Abu-Arqoub",
      "Rashiq Rafiq Marie"
    ],
    "published": "2025-06-04T21:45:40+00:00",
    "summary": "This study investigates the application of machine learning (ML) models for classifying dental providers into two categories - standard rendering providers and safety net clinic (SNC) providers - using a 2018 dataset of 24,300 instances with 20 features. The dataset, characterized by high missing values (38.1%), includes service counts (preventive, treatment, exams), delivery systems (FFS, managed care), and beneficiary demographics. Feature ranking methods such as information gain, Gini index, and ANOVA were employed to identify critical predictors, revealing treatment-related metrics (TXMT_USER_CNT, TXMT_SVC_CNT) as top-ranked features. Twelve ML models, including k-Nearest Neighbors (kNN), Decision Trees, Support Vector Machines (SVM), Stochastic Gradient Descent (SGD), Random Forest, Neural Networks, and Gradient Boosting, were evaluated using 10-fold cross-validation. Classification accuracy was tested across incremental feature subsets derived from rankings. The Neural Network achieved the highest accuracy (94.1%) using all 20 features, followed by Gradient Boosting (93.2%) and Random Forest (93.0%). Models showed improved performance as more features were incorporated, with SGD and ensemble methods demonstrating robustness to missing data. Feature ranking highlighted the dominance of treatment service counts and annotation codes in distinguishing provider types, while demographic variables (AGE_GROUP, CALENDAR_YEAR) had minimal impact. The study underscores the importance of feature selection in enhancing model efficiency and accuracy, particularly in imbalanced healthcare datasets. These findings advocate for integrating feature-ranking techniques with advanced ML algorithms to optimize dental provider classification, enabling targeted resource allocation for underserved populations."
  },
  {
    "title": "Watermarking Degrades Alignment in Language Models: Analysis and Mitigation",
    "url": "http://arxiv.org/abs/2506.04462v1",
    "arxiv_id": "2506.04462v1",
    "authors": [
      "Apurv Verma",
      "NhatHai Phan",
      "Shubhendu Trivedi"
    ],
    "published": "2025-06-04T21:29:07+00:00",
    "summary": "Watermarking techniques for large language models (LLMs) can significantly impact output quality, yet their effects on truthfulness, safety, and helpfulness remain critically underexamined. This paper presents a systematic analysis of how two popular watermarking approaches-Gumbel and KGW-affect these core alignment properties across four aligned LLMs. Our experiments reveal two distinct degradation patterns: guard attenuation, where enhanced helpfulness undermines model safety, and guard amplification, where excessive caution reduces model helpfulness. These patterns emerge from watermark-induced shifts in token distribution, surfacing the fundamental tension that exists between alignment objectives.   To mitigate these degradations, we propose Alignment Resampling (AR), an inference-time sampling method that uses an external reward model to restore alignment. We establish a theoretical lower bound on the improvement in expected reward score as the sample size is increased and empirically demonstrate that sampling just 2-4 watermarked generations effectively recovers or surpasses baseline (unwatermarked) alignment scores. To overcome the limited response diversity of standard Gumbel watermarking, our modified implementation sacrifices strict distortion-freeness while maintaining robust detectability, ensuring compatibility with AR. Experimental results confirm that AR successfully recovers baseline alignment in both watermarking approaches, while maintaining strong watermark detectability. This work reveals the critical balance between watermark strength and model alignment, providing a simple inference-time solution to responsibly deploy watermarked LLMs in practice."
  },
  {
    "title": "Struct2D: A Perception-Guided Framework for Spatial Reasoning in Large Multimodal Models",
    "url": "http://arxiv.org/abs/2506.04220v1",
    "arxiv_id": "2506.04220v1",
    "authors": [
      "Fangrui Zhu",
      "Hanhui Wang",
      "Yiming Xie",
      "Jing Gu",
      "Tianye Ding",
      "Jianwei Yang",
      "Huaizu Jiang"
    ],
    "published": "2025-06-04T17:58:04+00:00",
    "summary": "Unlocking spatial reasoning in Large Multimodal Models (LMMs) is crucial for enabling intelligent interaction with 3D environments. While prior efforts often rely on explicit 3D inputs or specialized model architectures, we ask: can LMMs reason about 3D space using only structured 2D representations derived from perception? We introduce Struct2D, a perception-guided prompting framework that combines bird's-eye-view (BEV) images with object marks and object-centric metadata, optionally incorporating egocentric keyframes when needed. Using Struct2D, we conduct an in-depth zero-shot analysis of closed-source LMMs (e.g., GPT-o3) and find that they exhibit surprisingly strong spatial reasoning abilities when provided with structured 2D inputs, effectively handling tasks such as relative direction estimation and route planning. Building on these insights, we construct Struct2D-Set, a large-scale instruction tuning dataset with 200K fine-grained QA pairs across eight spatial reasoning categories, generated automatically from 3D indoor scenes. We fine-tune an open-source LMM (Qwen2.5VL) on Struct2D-Set, achieving competitive performance on multiple benchmarks, including 3D question answering, dense captioning, and object grounding. Our approach demonstrates that structured 2D inputs can effectively bridge perception and language reasoning in LMMs-without requiring explicit 3D representations as input. We will release both our code and dataset to support future research."
  },
  {
    "title": "Pseudo-Simulation for Autonomous Driving",
    "url": "http://arxiv.org/abs/2506.04218v1",
    "arxiv_id": "2506.04218v1",
    "authors": [
      "Wei Cao",
      "Marcel Hallgarten",
      "Tianyu Li",
      "Daniel Dauner",
      "Xunjiang Gu",
      "Caojun Wang",
      "Yakov Miron",
      "Marco Aiello",
      "Hongyang Li",
      "Igor Gilitschenski",
      "Boris Ivanovic",
      "Marco Pavone",
      "Andreas Geiger",
      "Kashyap Chitta"
    ],
    "published": "2025-06-04T17:57:53+00:00",
    "summary": "Existing evaluation paradigms for Autonomous Vehicles (AVs) face critical limitations. Real-world evaluation is often challenging due to safety concerns and a lack of reproducibility, whereas closed-loop simulation can face insufficient realism or high computational costs. Open-loop evaluation, while being efficient and data-driven, relies on metrics that generally overlook compounding errors. In this paper, we propose pseudo-simulation, a novel paradigm that addresses these limitations. Pseudo-simulation operates on real datasets, similar to open-loop evaluation, but augments them with synthetic observations generated prior to evaluation using 3D Gaussian Splatting. Our key idea is to approximate potential future states the AV might encounter by generating a diverse set of observations that vary in position, heading, and speed. Our method then assigns a higher importance to synthetic observations that best match the AV's likely behavior using a novel proximity-based weighting scheme. This enables evaluating error recovery and the mitigation of causal confusion, as in closed-loop benchmarks, without requiring sequential interactive simulation. We show that pseudo-simulation is better correlated with closed-loop simulations (R^2=0.8) than the best existing open-loop approach (R^2=0.7). We also establish a public leaderboard for the community to benchmark new methodologies with pseudo-simulation. Our code is available at https://github.com/autonomousvision/navsim."
  },
  {
    "title": "Does Thinking More always Help? Understanding Test-Time Scaling in Reasoning Models",
    "url": "http://arxiv.org/abs/2506.04210v1",
    "arxiv_id": "2506.04210v1",
    "authors": [
      "Soumya Suvra Ghosal",
      "Souradip Chakraborty",
      "Avinash Reddy",
      "Yifu Lu",
      "Mengdi Wang",
      "Dinesh Manocha",
      "Furong Huang",
      "Mohammad Ghavamzadeh",
      "Amrit Singh Bedi"
    ],
    "published": "2025-06-04T17:55:09+00:00",
    "summary": "Recent trends in test-time scaling for reasoning models (e.g., OpenAI o1, DeepSeek R1) have led to a popular belief that extending thinking traces using prompts like \"Wait\" or \"Let me rethink\" can improve performance. This raises a natural question: Does thinking more at test-time truly lead to better reasoning? To answer this question, we perform a detailed empirical study across models and benchmarks, which reveals a consistent pattern of initial performance improvements from additional thinking followed by a decline, due to \"overthinking\". To understand this non-monotonic trend, we consider a simple probabilistic model, which reveals that additional thinking increases output variance-creating an illusion of improved reasoning while ultimately undermining precision. Thus, observed gains from \"more thinking\" are not true indicators of improved reasoning, but artifacts stemming from the connection between model uncertainty and evaluation metric. This suggests that test-time scaling through extended thinking is not an effective way to utilize the inference thinking budget. Recognizing these limitations, we introduce an alternative test-time scaling approach, parallel thinking, inspired by Best-of-N sampling. Our method generates multiple independent reasoning paths within the same inference budget and selects the most consistent response via majority vote, achieving up to 20% higher accuracy compared to extended thinking. This provides a simple yet effective mechanism for test-time scaling of reasoning models."
  },
  {
    "title": "Advancing Multimodal Reasoning: From Optimized Cold Start to Staged Reinforcement Learning",
    "url": "http://arxiv.org/abs/2506.04207v1",
    "arxiv_id": "2506.04207v1",
    "authors": [
      "Shuang Chen",
      "Yue Guo",
      "Zhaochen Su",
      "Yafu Li",
      "Yulun Wu",
      "Jiacheng Chen",
      "Jiayu Chen",
      "Weijie Wang",
      "Xiaoye Qu",
      "Yu Cheng"
    ],
    "published": "2025-06-04T17:51:08+00:00",
    "summary": "Inspired by the remarkable reasoning capabilities of Deepseek-R1 in complex textual tasks, many works attempt to incentivize similar capabilities in Multimodal Large Language Models (MLLMs) by directly applying reinforcement learning (RL). However, they still struggle to activate complex reasoning. In this paper, rather than examining multimodal RL in isolation, we delve into current training pipelines and identify three crucial phenomena: 1) Effective cold start initialization is critical for enhancing MLLM reasoning. Intriguingly, we find that initializing with carefully selected text data alone can lead to performance surpassing many recent multimodal reasoning models, even before multimodal RL. 2) Standard GRPO applied to multimodal RL suffers from gradient stagnation, which degrades training stability and performance. 3) Subsequent text-only RL training, following the multimodal RL phase, further enhances multimodal reasoning. This staged training approach effectively balances perceptual grounding and cognitive reasoning development. By incorporating the above insights and addressing multimodal RL issues, we introduce ReVisual-R1, achieving a new state-of-the-art among open-source 7B MLLMs on challenging benchmarks including MathVerse, MathVision, WeMath, LogicVista, DynaMath, and challenging AIME2024 and AIME2025."
  },
  {
    "title": "EPiC: Towards Lossless Speedup for Reasoning Training through Edge-Preserving CoT Condensation",
    "url": "http://arxiv.org/abs/2506.04205v1",
    "arxiv_id": "2506.04205v1",
    "authors": [
      "Jinghan Jia",
      "Hadi Reisizadeh",
      "Chongyu Fan",
      "Nathalie Baracaldo",
      "Mingyi Hong",
      "Sijia Liu"
    ],
    "published": "2025-06-04T17:49:10+00:00",
    "summary": "Large language models (LLMs) have shown remarkable reasoning capabilities when trained with chain-of-thought (CoT) supervision. However, the long and verbose CoT traces, especially those distilled from large reasoning models (LRMs) such as DeepSeek-R1, significantly increase training costs during the distillation process, where a non-reasoning base model is taught to replicate the reasoning behavior of an LRM. In this work, we study the problem of CoT condensation for resource-efficient reasoning training, aimed at pruning intermediate reasoning steps (i.e., thoughts) in CoT traces, enabling supervised model training on length-reduced CoT data while preserving both answer accuracy and the model's ability to generate coherent reasoning. Our rationale is that CoT traces typically follow a three-stage structure: problem understanding, exploration, and solution convergence. Through empirical analysis, we find that retaining the structure of the reasoning trace, especially the early stage of problem understanding (rich in reflective cues) and the final stage of solution convergence, is sufficient to achieve lossless reasoning supervision. To this end, we propose an Edge-Preserving Condensation method, EPiC, which selectively retains only the initial and final segments of each CoT trace while discarding the middle portion. This design draws an analogy to preserving the \"edge\" of a reasoning trajectory, capturing both the initial problem framing and the final answer synthesis, to maintain logical continuity. Experiments across multiple model families (Qwen and LLaMA) and benchmarks show that EPiC reduces training time by over 34% while achieving lossless reasoning accuracy on MATH500, comparable to full CoT supervision. To the best of our knowledge, this is the first study to explore thought-level CoT condensation for efficient reasoning model distillation."
  },
  {
    "title": "OpenThoughts: Data Recipes for Reasoning Models",
    "url": "http://arxiv.org/abs/2506.04178v1",
    "arxiv_id": "2506.04178v1",
    "authors": [
      "Etash Guha",
      "Ryan Marten",
      "Sedrick Keh",
      "Negin Raoof",
      "Georgios Smyrnis",
      "Hritik Bansal",
      "Marianna Nezhurina",
      "Jean Mercat",
      "Trung Vu",
      "Zayne Sprague",
      "Ashima Suvarna",
      "Benjamin Feuer",
      "Liangyu Chen",
      "Zaid Khan",
      "Eric Frankel",
      "Sachin Grover",
      "Caroline Choi",
      "Niklas Muennighoff",
      "Shiye Su",
      "Wanjia Zhao",
      "John Yang",
      "Shreyas Pimpalgaonkar",
      "Kartik Sharma",
      "Charlie Cheng-Jie Ji",
      "Yichuan Deng",
      "Sarah Pratt",
      "Vivek Ramanujan",
      "Jon Saad-Falcon",
      "Jeffrey Li",
      "Achal Dave",
      "Alon Albalak",
      "Kushal Arora",
      "Blake Wulfe",
      "Chinmay Hegde",
      "Greg Durrett",
      "Sewoong Oh",
      "Mohit Bansal",
      "Saadia Gabriel",
      "Aditya Grover",
      "Kai-Wei Chang",
      "Vaishaal Shankar",
      "Aaron Gokaslan",
      "Mike A. Merrill",
      "Tatsunori Hashimoto",
      "Yejin Choi",
      "Jenia Jitsev",
      "Reinhard Heckel",
      "Maheswaran Sathiamoorthy",
      "Alexandros G. Dimakis",
      "Ludwig Schmidt"
    ],
    "published": "2025-06-04T17:25:39+00:00",
    "summary": "Reasoning models have made rapid progress on many benchmarks involving math, code, and science. Yet, there are still many open questions about the best training recipes for reasoning since state-of-the-art models often rely on proprietary datasets with little to no public information available. To address this, the goal of the OpenThoughts project is to create open-source datasets for training reasoning models. After initial explorations, our OpenThoughts2-1M dataset led to OpenThinker2-32B, the first model trained on public reasoning data to match DeepSeek-R1-Distill-32B on standard reasoning benchmarks such as AIME and LiveCodeBench. We then improve our dataset further by systematically investigating each step of our data generation pipeline with 1,000+ controlled experiments, which led to OpenThoughts3. Scaling the pipeline to 1.2M examples and using QwQ-32B as teacher yields our OpenThinker3-7B model, which achieves state-of-the-art results: 53% on AIME 2025, 51% on LiveCodeBench 06/24-01/25, and 54% on GPQA Diamond. All of our datasets and models are available on https://openthoughts.ai."
  },
  {
    "title": "OpenThoughts: Data Recipes for Reasoning Models",
    "url": "http://arxiv.org/abs/2506.04178v2",
    "arxiv_id": "2506.04178v2",
    "authors": [
      "Etash Guha",
      "Ryan Marten",
      "Sedrick Keh",
      "Negin Raoof",
      "Georgios Smyrnis",
      "Hritik Bansal",
      "Marianna Nezhurina",
      "Jean Mercat",
      "Trung Vu",
      "Zayne Sprague",
      "Ashima Suvarna",
      "Benjamin Feuer",
      "Liangyu Chen",
      "Zaid Khan",
      "Eric Frankel",
      "Sachin Grover",
      "Caroline Choi",
      "Niklas Muennighoff",
      "Shiye Su",
      "Wanjia Zhao",
      "John Yang",
      "Shreyas Pimpalgaonkar",
      "Kartik Sharma",
      "Charlie Cheng-Jie Ji",
      "Yichuan Deng",
      "Sarah Pratt",
      "Vivek Ramanujan",
      "Jon Saad-Falcon",
      "Jeffrey Li",
      "Achal Dave",
      "Alon Albalak",
      "Kushal Arora",
      "Blake Wulfe",
      "Chinmay Hegde",
      "Greg Durrett",
      "Sewoong Oh",
      "Mohit Bansal",
      "Saadia Gabriel",
      "Aditya Grover",
      "Kai-Wei Chang",
      "Vaishaal Shankar",
      "Aaron Gokaslan",
      "Mike A. Merrill",
      "Tatsunori Hashimoto",
      "Yejin Choi",
      "Jenia Jitsev",
      "Reinhard Heckel",
      "Maheswaran Sathiamoorthy",
      "Alexandros G. Dimakis",
      "Ludwig Schmidt"
    ],
    "published": "2025-06-04T17:25:39+00:00",
    "summary": "Reasoning models have made rapid progress on many benchmarks involving math, code, and science. Yet, there are still many open questions about the best training recipes for reasoning since state-of-the-art models often rely on proprietary datasets with little to no public information available. To address this, the goal of the OpenThoughts project is to create open-source datasets for training reasoning models. After initial explorations, our OpenThoughts2-1M dataset led to OpenThinker2-32B, the first model trained on public reasoning data to match DeepSeek-R1-Distill-32B on standard reasoning benchmarks such as AIME and LiveCodeBench. We then improve our dataset further by systematically investigating each step of our data generation pipeline with 1,000+ controlled experiments, which led to OpenThoughts3. Scaling the pipeline to 1.2M examples and using QwQ-32B as teacher yields our OpenThoughts3-7B model, which achieves state-of-the-art results: 53% on AIME 2025, 51% on LiveCodeBench 06/24-01/25, and 54% on GPQA Diamond - improvements of 15.3, 17.2, and 20.5 percentage points compared to the DeepSeek-R1-Distill-Qwen-7B. All of our datasets and models are available on https://openthoughts.ai."
  },
  {
    "title": "SLAC: Simulation-Pretrained Latent Action Space for Whole-Body Real-World RL",
    "url": "http://arxiv.org/abs/2506.04147v1",
    "arxiv_id": "2506.04147v1",
    "authors": [
      "Jiaheng Hu",
      "Peter Stone",
      "Roberto Mart\u00edn-Mart\u00edn"
    ],
    "published": "2025-06-04T16:41:55+00:00",
    "summary": "Building capable household and industrial robots requires mastering the control of versatile, high-degree-of-freedom (DoF) systems such as mobile manipulators. While reinforcement learning (RL) holds promise for autonomously acquiring robot control policies, scaling it to high-DoF embodiments remains challenging. Direct RL in the real world demands both safe exploration and high sample efficiency, which are difficult to achieve in practice. Sim-to-real RL, on the other hand, is often brittle due to the reality gap. This paper introduces SLAC, a method that renders real-world RL feasible for complex embodiments by leveraging a low-fidelity simulator to pretrain a task-agnostic latent action space. SLAC trains this latent action space via a customized unsupervised skill discovery method designed to promote temporal abstraction, disentanglement, and safety, thereby facilitating efficient downstream learning. Once a latent action space is learned, SLAC uses it as the action interface for a novel off-policy RL algorithm to autonomously learn downstream tasks through real-world interactions. We evaluate SLAC against existing methods on a suite of bimanual mobile manipulation tasks, where it achieves state-of-the-art performance. Notably, SLAC learns contact-rich whole-body tasks in under an hour of real-world interactions, without relying on any demonstrations or hand-crafted behavior priors. More information, code, and videos at robo-rl.github.io"
  },
  {
    "title": "macOSWorld: A Multilingual Interactive Benchmark for GUI Agents",
    "url": "http://arxiv.org/abs/2506.04135v1",
    "arxiv_id": "2506.04135v1",
    "authors": [
      "Pei Yang",
      "Hai Ci",
      "Mike Zheng Shou"
    ],
    "published": "2025-06-04T16:26:56+00:00",
    "summary": "Graphical User Interface (GUI) agents show promising capabilities for automating computer-use tasks and facilitating accessibility, but existing interactive benchmarks are mostly English-only, covering web-use or Windows, Linux, and Android environments, but not macOS. macOS is a major OS with distinctive GUI patterns and exclusive applications. To bridge the gaps, we present macOSWorld, the first comprehensive benchmark for evaluating GUI agents on macOS. macOSWorld features 202 multilingual interactive tasks across 30 applications (28 macOS-exclusive), with task instructions and OS interfaces offered in 5 languages (English, Chinese, Arabic, Japanese, and Russian). As GUI agents are shown to be vulnerable to deception attacks, macOSWorld also includes a dedicated safety benchmarking subset. Our evaluation on six GUI agents reveals a dramatic gap: proprietary computer-use agents lead at above 30% success rate, while open-source lightweight research models lag at below 2%, highlighting the need for macOS domain adaptation. Multilingual benchmarks also expose common weaknesses, especially in Arabic, with a 27.5% average degradation compared to English. Results from safety benchmarking also highlight that deception attacks are more general and demand immediate attention. macOSWorld is available at https://github.com/showlab/macosworld."
  },
  {
    "title": "macOSWorld: A Multilingual Interactive Benchmark for GUI Agents",
    "url": "http://arxiv.org/abs/2506.04135v2",
    "arxiv_id": "2506.04135v2",
    "authors": [
      "Pei Yang",
      "Hai Ci",
      "Mike Zheng Shou"
    ],
    "published": "2025-06-04T16:26:56+00:00",
    "summary": "Graphical User Interface (GUI) agents show promising capabilities for automating computer-use tasks and facilitating accessibility, but existing interactive benchmarks are mostly English-only, covering web-use or Windows, Linux, and Android environments, but not macOS. macOS is a major OS with distinctive GUI patterns and exclusive applications. To bridge the gaps, we present macOSWorld, the first comprehensive benchmark for evaluating GUI agents on macOS. macOSWorld features 202 multilingual interactive tasks across 30 applications (28 macOS-exclusive), with task instructions and OS interfaces offered in 5 languages (English, Chinese, Arabic, Japanese, and Russian). As GUI agents are shown to be vulnerable to deception attacks, macOSWorld also includes a dedicated safety benchmarking subset. Our evaluation on six GUI agents reveals a dramatic gap: proprietary computer-use agents lead at above 30% success rate, while open-source lightweight research models lag at below 2%, highlighting the need for macOS domain adaptation. Multilingual benchmarks also expose common weaknesses, especially in Arabic, with a 27.5% average degradation compared to English. Results from safety benchmarking also highlight that deception attacks are more general and demand immediate attention. macOSWorld is available at https://github.com/showlab/macosworld."
  },
  {
    "title": "Contour Errors: An Ego-Centric Metric for Reliable 3D Multi-Object Tracking",
    "url": "http://arxiv.org/abs/2506.04122v1",
    "arxiv_id": "2506.04122v1",
    "authors": [
      "Sharang Kaul",
      "Mario Berk",
      "Thiemo Gerbich",
      "Abhinav Valada"
    ],
    "published": "2025-06-04T16:15:04+00:00",
    "summary": "Finding reliable matches is essential in multi-object tracking to ensure the accuracy and reliability of perception systems in safety-critical applications such as autonomous vehicles. Effective matching mitigates perception errors, enhancing object identification and tracking for improved performance and safety. However, traditional metrics such as Intersection over Union (IoU) and Center Point Distances (CPDs), which are effective in 2D image planes, often fail to find critical matches in complex 3D scenes. To address this limitation, we introduce Contour Errors (CEs), an ego or object-centric metric for identifying matches of interest in tracking scenarios from a functional perspective. By comparing bounding boxes in the ego vehicle's frame, contour errors provide a more functionally relevant assessment of object matches. Extensive experiments on the nuScenes dataset demonstrate that contour errors improve the reliability of matches over the state-of-the-art 2D IoU and CPD metrics in tracking-by-detection methods. In 3D car tracking, our results show that Contour Errors reduce functional failures (FPs/FNs) by 80% at close ranges and 60% at far ranges compared to IoU in the evaluation stage."
  },
  {
    "title": "AmbiK: Dataset of Ambiguous Tasks in Kitchen Environment",
    "url": "http://arxiv.org/abs/2506.04089v1",
    "arxiv_id": "2506.04089v1",
    "authors": [
      "Anastasiia Ivanova",
      "Eva Bakaeva",
      "Zoya Volovikova",
      "Alexey K. Kovalev",
      "Aleksandr I. Panov"
    ],
    "published": "2025-06-04T15:47:07+00:00",
    "summary": "As a part of an embodied agent, Large Language Models (LLMs) are typically used for behavior planning given natural language instructions from the user. However, dealing with ambiguous instructions in real-world environments remains a challenge for LLMs. Various methods for task ambiguity detection have been proposed. However, it is difficult to compare them because they are tested on different datasets and there is no universal benchmark. For this reason, we propose AmbiK (Ambiguous Tasks in Kitchen Environment), the fully textual dataset of ambiguous instructions addressed to a robot in a kitchen environment. AmbiK was collected with the assistance of LLMs and is human-validated. It comprises 1000 pairs of ambiguous tasks and their unambiguous counterparts, categorized by ambiguity type (Human Preferences, Common Sense Knowledge, Safety), with environment descriptions, clarifying questions and answers, user intents, and task plans, for a total of 2000 tasks. We hope that AmbiK will enable researchers to perform a unified comparison of ambiguity detection methods. AmbiK is available at https://github.com/cog-model/AmbiK-dataset."
  },
  {
    "title": "Multimodal Tabular Reasoning with Privileged Structured Information",
    "url": "http://arxiv.org/abs/2506.04088v1",
    "arxiv_id": "2506.04088v1",
    "authors": [
      "Jun-Peng Jiang",
      "Yu Xia",
      "Hai-Long Sun",
      "Shiyin Lu",
      "Qing-Guo Chen",
      "Weihua Luo",
      "Kaifu Zhang",
      "De-Chuan Zhan",
      "Han-Jia Ye"
    ],
    "published": "2025-06-04T15:46:30+00:00",
    "summary": "Tabular reasoning involves multi-step information extraction and logical inference over tabular data. While recent advances have leveraged large language models (LLMs) for reasoning over structured tables, such high-quality textual representations are often unavailable in real-world settings, where tables typically appear as images. In this paper, we tackle the task of tabular reasoning from table images, leveraging privileged structured information available during training to enhance multimodal large language models (MLLMs). The key challenges lie in the complexity of accurately aligning structured information with visual representations, and in effectively transferring structured reasoning skills to MLLMs despite the input modality gap. To address these, we introduce TabUlar Reasoning with Bridged infOrmation ({\\sc Turbo}), a new framework for multimodal tabular reasoning with privileged structured tables. {\\sc Turbo} benefits from a structure-aware reasoning trace generator based on DeepSeek-R1, contributing to high-quality modality-bridged data. On this basis, {\\sc Turbo} repeatedly generates and selects the advantageous reasoning paths, further enhancing the model's tabular reasoning ability. Experimental results demonstrate that, with limited ($9$k) data, {\\sc Turbo} achieves state-of-the-art performance ($+7.2\\%$ vs. previous SOTA) across multiple datasets."
  },
  {
    "title": "Think Like a Person Before Responding: A Multi-Faceted Evaluation of Persona-Guided LLMs for Countering Hate",
    "url": "http://arxiv.org/abs/2506.04043v1",
    "arxiv_id": "2506.04043v1",
    "authors": [
      "Mikel K. Ngueajio",
      "Flor Miriam Plaza-del-Arco",
      "Yi-Ling Chung",
      "Danda B. Rawat",
      "Amanda Cercas Curry"
    ],
    "published": "2025-06-04T15:09:20+00:00",
    "summary": "Automated counter-narratives (CN) offer a promising strategy for mitigating online hate speech, yet concerns about their affective tone, accessibility, and ethical risks remain. We propose a framework for evaluating Large Language Model (LLM)-generated CNs across four dimensions: persona framing, verbosity and readability, affective tone, and ethical robustness. Using GPT-4o-Mini, Cohere's CommandR-7B, and Meta's LLaMA 3.1-70B, we assess three prompting strategies on the MT-Conan and HatEval datasets. Our findings reveal that LLM-generated CNs are often verbose and adapted for people with college-level literacy, limiting their accessibility. While emotionally guided prompts yield more empathetic and readable responses, there remain concerns surrounding safety and effectiveness."
  },
  {
    "title": "Generating Automotive Code: Large Language Models for Software Development and Verification in Safety-Critical Systems",
    "url": "http://arxiv.org/abs/2506.04038v1",
    "arxiv_id": "2506.04038v1",
    "authors": [
      "Sven Kirchner",
      "Alois C. Knoll"
    ],
    "published": "2025-06-04T15:01:59+00:00",
    "summary": "Developing safety-critical automotive software presents significant challenges due to increasing system complexity and strict regulatory demands. This paper proposes a novel framework integrating Generative Artificial Intelligence (GenAI) into the Software Development Lifecycle (SDLC). The framework uses Large Language Models (LLMs) to automate code generation in languages such as C++, incorporating safety-focused practices such as static verification, test-driven development and iterative refinement. A feedback-driven pipeline ensures the integration of test, simulation and verification for compliance with safety standards. The framework is validated through the development of an Adaptive Cruise Control (ACC) system. Comparative benchmarking of LLMs ensures optimal model selection for accuracy and reliability. Results demonstrate that the framework enables automatic code generation while ensuring compliance with safety-critical requirements, systematically integrating GenAI into automotive software engineering. This work advances the use of AI in safety-critical domains, bridging the gap between state-of-the-art generative models and real-world safety requirements."
  },
  {
    "title": "Asterinas: A Linux ABI-Compatible, Rust-Based Framekernel OS with a Small and Sound TCB",
    "url": "http://arxiv.org/abs/2506.03876v1",
    "arxiv_id": "2506.03876v1",
    "authors": [
      "Yuke Peng",
      "Hongliang Tian",
      "Zhang Junyang",
      "Ruihan Li",
      "Chengjun Chen",
      "Jianfeng Jiang",
      "Jinyi Xian",
      "Xiaolin Wang",
      "Chenren Xu",
      "Diyu Zhou",
      "Yingwei Luo",
      "Shoumeng Yan",
      "Yinqian Zhang"
    ],
    "published": "2025-06-04T12:10:26+00:00",
    "summary": "How can one build a feature-rich, general-purpose, Rust-based operating system (OS) with a minimal and sound Trusted Computing Base (TCB) for memory safety? Existing Rust-based OSes fall short due to their improper use of unsafe Rust in kernel development. To address this challenge, we propose a novel OS architecture called framekernel that realizes Rust's full potential to achieve intra-kernel privilege separation, ensuring TCB minimality and soundness. We present OSTD, a streamlined framework for safe Rust OS development, and Asterinas, a Linux ABI-compatible framekernel OS implemented entirely in safe Rust using OSTD. Supporting over 210 Linux system calls, Asterinas delivers performance on par with Linux, while maintaining a minimized, memory-safety TCB of only about 14.0% of the codebase. These results underscore the practicality and benefits of the framekernel architecture in building safe and efficient OSes."
  },
  {
    "title": "Vulnerability-Aware Alignment: Mitigating Uneven Forgetting in Harmful Fine-Tuning",
    "url": "http://arxiv.org/abs/2506.03850v1",
    "arxiv_id": "2506.03850v1",
    "authors": [
      "Liang Chen",
      "Xueting Han",
      "Li Shen",
      "Jing Bai",
      "Kam-Fai Wong"
    ],
    "published": "2025-06-04T11:33:36+00:00",
    "summary": "Harmful fine-tuning (HFT), performed directly on open-source LLMs or through Fine-tuning-as-a-Service, breaks safety alignment and poses significant threats. Existing methods aim to mitigate HFT risks by learning robust representation on alignment data or making harmful data unlearnable, but they treat each data sample equally, leaving data vulnerability patterns understudied. In this work, we reveal that certain subsets of alignment data are consistently more prone to forgetting during HFT across different fine-tuning tasks. Inspired by these findings, we propose Vulnerability-Aware Alignment (VAA), which estimates data vulnerability, partitions data into \"vulnerable\" and \"invulnerable\" groups, and encourages balanced learning using a group distributionally robust optimization (Group DRO) framework. Specifically, VAA learns an adversarial sampler that samples examples from the currently underperforming group and then applies group-dependent adversarial perturbations to the data during training, aiming to encourage a balanced learning process across groups. Experiments across four fine-tuning tasks demonstrate that VAA significantly reduces harmful scores while preserving downstream task performance, outperforming state-of-the-art baselines."
  },
  {
    "title": "Enhancing Safety of Foundation Models for Visual Navigation through Collision Avoidance via Repulsive Estimation",
    "url": "http://arxiv.org/abs/2506.03834v1",
    "arxiv_id": "2506.03834v1",
    "authors": [
      "Joonkyung Kim",
      "Joonyeol Sim",
      "Woojun Kim",
      "Katia Sycara",
      "Changjoo Nam"
    ],
    "published": "2025-06-04T11:02:15+00:00",
    "summary": "We propose CARE (Collision Avoidance via Repulsive Estimation), a plug-and-play module that enhances the safety of vision-based navigation without requiring additional range sensors or fine-tuning of pretrained models. While recent foundation models using only RGB inputs have shown strong performance, they often fail to generalize in out-of-distribution (OOD) environments with unseen objects or variations in camera parameters (e.g., field of view, pose, or focal length). Without fine-tuning, these models may generate unsafe trajectories that lead to collisions, requiring costly data collection and retraining. CARE addresses this limitation by seamlessly integrating with any RGB-based navigation system that outputs local trajectories, dynamically adjusting them using repulsive force vectors derived from monocular depth maps. We evaluate CARE by combining it with state-of-the-art vision-based navigation models across multiple robot platforms. CARE consistently reduces collision rates (up to 100%) without sacrificing goal-reaching performance and improves collision-free travel distance by up to 10.7x in exploration tasks."
  },
  {
    "title": "From Theory to Practice: Real-World Use Cases on Trustworthy LLM-Driven Process Modeling, Prediction and Automation",
    "url": "http://arxiv.org/abs/2506.03801v1",
    "arxiv_id": "2506.03801v1",
    "authors": [
      "Peter Pfeiffer",
      "Alexander Rombach",
      "Maxim Majlatow",
      "Nijat Mehdiyev"
    ],
    "published": "2025-06-04T10:12:09+00:00",
    "summary": "Traditional Business Process Management (BPM) struggles with rigidity, opacity, and scalability in dynamic environments while emerging Large Language Models (LLMs) present transformative opportunities alongside risks. This paper explores four real-world use cases that demonstrate how LLMs, augmented with trustworthy process intelligence, redefine process modeling, prediction, and automation. Grounded in early-stage research projects with industrial partners, the work spans manufacturing, modeling, life-science, and design processes, addressing domain-specific challenges through human-AI collaboration. In manufacturing, an LLM-driven framework integrates uncertainty-aware explainable Machine Learning (ML) with interactive dialogues, transforming opaque predictions into auditable workflows. For process modeling, conversational interfaces democratize BPMN design. Pharmacovigilance agents automate drug safety monitoring via knowledge-graph-augmented LLMs. Finally, sustainable textile design employs multi-agent systems to navigate regulatory and environmental trade-offs. We intend to examine tensions between transparency and efficiency, generalization and specialization, and human agency versus automation. By mapping these trade-offs, we advocate for context-sensitive integration prioritizing domain needs, stakeholder values, and iterative human-in-the-loop workflows over universal solutions. This work provides actionable insights for researchers and practitioners aiming to operationalize LLMs in critical BPM environments."
  },
  {
    "title": "Misalignment or misuse? The AGI alignment tradeoff",
    "url": "http://arxiv.org/abs/2506.03755v1",
    "arxiv_id": "2506.03755v1",
    "authors": [
      "Max Hellrigel-Holderbaum",
      "Leonard Dung"
    ],
    "published": "2025-06-04T09:22:37+00:00",
    "summary": "Creating systems that are aligned with our goals is seen as a leading approach to create safe and beneficial AI in both leading AI companies and the academic field of AI safety. We defend the view that misaligned AGI - future, generally intelligent (robotic) AI agents - poses catastrophic risks. At the same time, we support the view that aligned AGI creates a substantial risk of catastrophic misuse by humans. While both risks are severe and stand in tension with one another, we show that - in principle - there is room for alignment approaches which do not increase misuse risk. We then investigate how the tradeoff between misalignment and misuse looks empirically for different technical approaches to AI alignment. Here, we argue that many current alignment techniques and foreseeable improvements thereof plausibly increase risks of catastrophic misuse. Since the impacts of AI depend on the social context, we close by discussing important social factors and suggest that to reduce the risk of a misuse catastrophe due to aligned AGI, techniques such as robustness, AI control methods and especially good governance seem essential."
  },
  {
    "title": "Robust Preference Optimization via Dynamic Target Margins",
    "url": "http://arxiv.org/abs/2506.03690v1",
    "arxiv_id": "2506.03690v1",
    "authors": [
      "Jie Sun",
      "Junkang Wu",
      "Jiancan Wu",
      "Zhibo Zhu",
      "Xingyu Lu",
      "Jun Zhou",
      "Lintao Ma",
      "Xiang Wang"
    ],
    "published": "2025-06-04T08:19:37+00:00",
    "summary": "The alignment of Large Language Models (LLMs) is crucial for ensuring their safety and reliability in practical applications. Direct Preference Optimization (DPO) has emerged as an efficient method that directly optimizes models using preference pairs, significantly reducing resource demands. However, the effectiveness of DPO heavily depends on the data quality, which is frequently compromised by noise. In this work, we propose $\\gamma$-PO, a dynamic target margin preference optimization algorithm that adjust reward margins at the pairwise level. By introducing instance-specific margin calibration, $\\gamma$-PO strategically prioritizes high-confidence pairs (those demonstrating higher reward margins) while suppressing potential noise from ambiguous pairs. Moreover, $\\gamma$-PO is a plug-and-play method, compatible with variants of DPO that rely on reward margin between preference pairs. Across benchmarks such as AlpacaEval2 and Arena-Hard, $\\gamma$-PO achieves an average 4.4\\% improvement over other baselines, setting new benchmarks for state-of-the-art performance. Additionally, $\\gamma$-PO requires minimal code changes and has a negligible impact on training efficiency, making it a robust solution for enhancing LLMs alignment. Our codes are available at \\href{https://github.com/sunjie279/gammaPO}{https://github.com/sunjie279/gammaPO}."
  },
  {
    "title": "PRJ: Perception-Retrieval-Judgement for Generated Images",
    "url": "http://arxiv.org/abs/2506.03683v1",
    "arxiv_id": "2506.03683v1",
    "authors": [
      "Qiang Fu",
      "Zonglei Jing",
      "Zonghao Ying",
      "Xiaoqian Li"
    ],
    "published": "2025-06-04T08:13:53+00:00",
    "summary": "The rapid progress of generative AI has enabled remarkable creative capabilities, yet it also raises urgent concerns regarding the safety of AI-generated visual content in real-world applications such as content moderation, platform governance, and digital media regulation. This includes unsafe material such as sexually explicit images, violent scenes, hate symbols, propaganda, and unauthorized imitations of copyrighted artworks. Existing image safety systems often rely on rigid category filters and produce binary outputs, lacking the capacity to interpret context or reason about nuanced, adversarially induced forms of harm. In addition, standard evaluation metrics (e.g., attack success rate) fail to capture the semantic severity and dynamic progression of toxicity. To address these limitations, we propose Perception-Retrieval-Judgement (PRJ), a cognitively inspired framework that models toxicity detection as a structured reasoning process. PRJ follows a three-stage design: it first transforms an image into descriptive language (perception), then retrieves external knowledge related to harm categories and traits (retrieval), and finally evaluates toxicity based on legal or normative rules (judgement). This language-centric structure enables the system to detect both explicit and implicit harms with improved interpretability and categorical granularity. In addition, we introduce a dynamic scoring mechanism based on a contextual toxicity risk matrix to quantify harmfulness across different semantic dimensions. Experiments show that PRJ surpasses existing safety checkers in detection accuracy and robustness while uniquely supporting structured category-level toxicity interpretation."
  },
  {
    "title": "Trustworthy Medical Question Answering: An Evaluation-Centric Survey",
    "url": "http://arxiv.org/abs/2506.03659v1",
    "arxiv_id": "2506.03659v1",
    "authors": [
      "Yinuo Wang",
      "Robert E. Mercer",
      "Frank Rudzicz",
      "Sudipta Singha Roy",
      "Pengjie Ren",
      "Zhumin Chen",
      "Xindi Wang"
    ],
    "published": "2025-06-04T07:48:10+00:00",
    "summary": "Trustworthiness in healthcare question-answering (QA) systems is important for ensuring patient safety, clinical effectiveness, and user confidence. As large language models (LLMs) become increasingly integrated into medical settings, the reliability of their responses directly influences clinical decision-making and patient outcomes. However, achieving comprehensive trustworthiness in medical QA poses significant challenges due to the inherent complexity of healthcare data, the critical nature of clinical scenarios, and the multifaceted dimensions of trustworthy AI. In this survey, we systematically examine six key dimensions of trustworthiness in medical QA, i.e., Factuality, Robustness, Fairness, Safety, Explainability, and Calibration. We review how each dimension is evaluated in existing LLM-based medical QA systems. We compile and compare major benchmarks designed to assess these dimensions and analyze evaluation-guided techniques that drive model improvements, such as retrieval-augmented grounding, adversarial fine-tuning, and safety alignment. Finally, we identify open challenges-such as scalable expert evaluation, integrated multi-dimensional metrics, and real-world deployment studies-and propose future research directions to advance the safe, reliable, and transparent deployment of LLM-powered medical QA."
  },
  {
    "title": "VLMs Can Aggregate Scattered Training Patches",
    "url": "http://arxiv.org/abs/2506.03614v1",
    "arxiv_id": "2506.03614v1",
    "authors": [
      "Zhanhui Zhou",
      "Lingjie Chen",
      "Chao Yang",
      "Chaochao Lu"
    ],
    "published": "2025-06-04T06:46:06+00:00",
    "summary": "One way to mitigate risks in vision-language models (VLMs) is to remove dangerous samples in their training data. However, such data moderation can be easily bypassed when harmful images are split into small, benign-looking patches, scattered across many training samples. VLMs may then learn to piece these fragments together during training and generate harmful responses at inference, either from full images or text references. For instance, if trained on image patches from a bloody scene paired with the descriptions \"safe,\" VLMs may later describe, the full image or a text reference to the scene, as \"safe.\" We define the core ability of VLMs enabling this attack as $\\textit{visual stitching}$ -- the ability to integrate visual information spread across multiple training samples that share the same textual descriptions. In our work, we first demonstrate visual stitching abilities in common open-source VLMs on three datasets where each image is labeled with a unique synthetic ID: we split each $(\\texttt{image}, \\texttt{ID})$ pair into $\\{(\\texttt{patch}, \\texttt{ID})\\}$ pairs at different granularity for finetuning, and we find that tuned models can verbalize the correct IDs from full images or text reference. Building on this, we simulate the adversarial data poisoning scenario mentioned above by using patches from dangerous images and replacing IDs with text descriptions like ``safe'' or ``unsafe'', demonstrating how harmful content can evade moderation in patches and later be reconstructed through visual stitching, posing serious VLM safety risks. Code is available at https://github.com/ZHZisZZ/visual-stitching."
  },
  {
    "title": "Confidence-Guided Human-AI Collaboration: Reinforcement Learning with Distributional Proxy Value Propagation for Autonomous Driving",
    "url": "http://arxiv.org/abs/2506.03568v1",
    "arxiv_id": "2506.03568v1",
    "authors": [
      "Li Zeqiao",
      "Wang Yijing",
      "Wang Haoyu",
      "Li Zheng",
      "Li Peng",
      "Zuo zhiqiang",
      "Hu Chuan"
    ],
    "published": "2025-06-04T04:31:10+00:00",
    "summary": "Autonomous driving promises significant advancements in mobility, road safety and traffic efficiency, yet reinforcement learning and imitation learning face safe-exploration and distribution-shift challenges. Although human-AI collaboration alleviates these issues, it often relies heavily on extensive human intervention, which increases costs and reduces efficiency. This paper develops a confidence-guided human-AI collaboration (C-HAC) strategy to overcome these limitations. First, C-HAC employs a distributional proxy value propagation method within the distributional soft actor-critic (DSAC) framework. By leveraging return distributions to represent human intentions C-HAC achieves rapid and stable learning of human-guided policies with minimal human interaction. Subsequently, a shared control mechanism is activated to integrate the learned human-guided policy with a self-learning policy that maximizes cumulative rewards. This enables the agent to explore independently and continuously enhance its performance beyond human guidance. Finally, a policy confidence evaluation algorithm capitalizes on DSAC's return distribution networks to facilitate dynamic switching between human-guided and self-learning policies via a confidence-based intervention function. This ensures the agent can pursue optimal policies while maintaining safety and performance guarantees. Extensive experiments across diverse driving scenarios reveal that C-HAC significantly outperforms conventional methods in terms of safety, efficiency, and overall performance, achieving state-of-the-art results. The effectiveness of the proposed method is further validated through real-world road tests in complex traffic conditions. The videos and code are available at: https://github.com/lzqw/C-HAC."
  },
  {
    "title": "Confidence-Guided Human-AI Collaboration: Reinforcement Learning with Distributional Proxy Value Propagation for Autonomous Driving",
    "url": "http://arxiv.org/abs/2506.03568v2",
    "arxiv_id": "2506.03568v2",
    "authors": [
      "Li Zeqiao",
      "Wang Yijing",
      "Wang Haoyu",
      "Li Zheng",
      "Li Peng",
      "Zuo zhiqiang",
      "Hu Chuan"
    ],
    "published": "2025-06-04T04:31:10+00:00",
    "summary": "Autonomous driving promises significant advancements in mobility, road safety and traffic efficiency, yet reinforcement learning and imitation learning face safe-exploration and distribution-shift challenges. Although human-AI collaboration alleviates these issues, it often relies heavily on extensive human intervention, which increases costs and reduces efficiency. This paper develops a confidence-guided human-AI collaboration (C-HAC) strategy to overcome these limitations. First, C-HAC employs a distributional proxy value propagation method within the distributional soft actor-critic (DSAC) framework. By leveraging return distributions to represent human intentions C-HAC achieves rapid and stable learning of human-guided policies with minimal human interaction. Subsequently, a shared control mechanism is activated to integrate the learned human-guided policy with a self-learning policy that maximizes cumulative rewards. This enables the agent to explore independently and continuously enhance its performance beyond human guidance. Finally, a policy confidence evaluation algorithm capitalizes on DSAC's return distribution networks to facilitate dynamic switching between human-guided and self-learning policies via a confidence-based intervention function. This ensures the agent can pursue optimal policies while maintaining safety and performance guarantees. Extensive experiments across diverse driving scenarios reveal that C-HAC significantly outperforms conventional methods in terms of safety, efficiency, and overall performance, achieving state-of-the-art results. The effectiveness of the proposed method is further validated through real-world road tests in complex traffic conditions. The videos and code are available at: https://github.com/lzqw/C-HAC."
  },
  {
    "title": "Bridging the Artificial Intelligence Governance Gap: The United States' and China's Divergent Approaches to Governing General-Purpose Artificial Intelligence",
    "url": "http://arxiv.org/abs/2506.03497v1",
    "arxiv_id": "2506.03497v1",
    "authors": [
      "Oliver Guest",
      "Kevin Wei"
    ],
    "published": "2025-06-04T02:24:27+00:00",
    "summary": "The United States and China are among the world's top players in the development of advanced artificial intelligence (AI) systems, and both are keen to lead in global AI governance and development. A look at U.S. and Chinese policy landscapes reveals differences in how the two countries approach the governance of general-purpose artificial intelligence (GPAI) systems. Three areas of divergence are notable for policymakers: the focus of domestic AI regulation, key principles of domestic AI regulation, and approaches to implementing international AI governance. As AI development continues, global conversation around AI has warned of global safety and security challenges posed by GPAI systems. Cooperation between the United States and China might be needed to address these risks, and understanding the implications of these differences might help address the broader challenges for international cooperation between the United States and China on AI safety and security."
  },
  {
    "title": "Verification-Guided Falsification for Safe RL via Explainable Abstraction and Risk-Aware Exploration",
    "url": "http://arxiv.org/abs/2506.03469v1",
    "arxiv_id": "2506.03469v1",
    "authors": [
      "Tuan Le",
      "Risal Shefin",
      "Debashis Gupta",
      "Thai Le",
      "Sarra Alqahtani"
    ],
    "published": "2025-06-04T00:54:01+00:00",
    "summary": "Ensuring the safety of reinforcement learning (RL) policies in high-stakes environments requires not only formal verification but also interpretability and targeted falsification. While model checking provides formal guarantees, its effectiveness is limited by abstraction quality and the completeness of the underlying trajectory dataset. We propose a hybrid framework that integrates (1) explainability, (2) model checking, and (3) risk-guided falsification to achieve both rigor and coverage. Our approach begins by constructing a human-interpretable abstraction of the RL policy using Comprehensible Abstract Policy Summarization (CAPS). This abstract graph, derived from offline trajectories, is both verifier-friendly, semantically meaningful, and can be used as input to Storm probabilistic model checker to verify satisfaction of temporal safety specifications. If the model checker identifies a violation, it will return an interpretable counterexample trace by which the policy fails the safety requirement. However, if no violation is detected, we cannot conclude satisfaction due to potential limitation in the abstraction and coverage of the offline dataset. In such cases, we estimate associated risk during model checking to guide a falsification strategy that prioritizes searching in high-risk states and regions underrepresented in the trajectory dataset. We further provide PAC-style guarantees on the likelihood of uncovering undetected violations. Finally, we incorporate a lightweight safety shield that switches to a fallback policy at runtime when such a risk exceeds a threshold, facilitating failure mitigation without retraining."
  },
  {
    "title": "Nonlinear Optimal Control of DC Microgrids with Safety and Stability Guarantees",
    "url": "http://arxiv.org/abs/2506.03454v1",
    "arxiv_id": "2506.03454v1",
    "authors": [
      "Muratkhan Abdirash",
      "Xiaofan Cui"
    ],
    "published": "2025-06-03T23:37:28+00:00",
    "summary": "A DC microgrid is a promising alternative to the traditional AC power grid, since it can efficiently integrate distributed and renewable energy resources. However, as an emerging framework, it lacks the rigorous theoretical guarantees of its AC counterpart. In particular, safe stabilization of the DC microgrid has been a non-trivial task in power electronics. To address that, we take a control theoretic perspective in designing the feedback controller with provable guarantees. We present a systematic way to construct Control Lyapunov Functions (CLF) to stabilize the microgrid, and, independently, Control Barrier Functions (CBF) to enforce its safe operation at all times. The safety-critical controller (SCC) proposed in this work integrates the two control objectives, with safety prioritized, into a quadratic program (QP) as linear constraints, which allows for its online deployment using off-the-shelf convex optimization solvers. The SCC is compared against a robust version of the conventional droop control through numerical experiments whose results indicate the SCC outperforms the droop controller in guaranteeing safety and retaining stability at the same time."
  },
  {
    "title": "Technical Options for Flexible Hardware-Enabled Guarantees",
    "url": "http://arxiv.org/abs/2506.03409v1",
    "arxiv_id": "2506.03409v1",
    "authors": [
      "James Petrie",
      "Onni Aarne"
    ],
    "published": "2025-06-03T21:37:43+00:00",
    "summary": "Frontier AI models pose increasing risks to public safety and international security, creating a pressing need for AI developers to provide credible guarantees about their development activities without compromising proprietary information. We propose Flexible Hardware-Enabled Guarantees (flexHEG), a system integrated with AI accelerator hardware to enable verifiable claims about compute usage in AI development. The flexHEG system consists of two primary components: an auditable Guarantee Processor that monitors accelerator usage and verifies compliance with specified rules, and a Secure Enclosure that provides physical tamper protection. In this report, we analyze technical implementation options ranging from firmware modifications to custom hardware approaches, with focus on an \"Interlock\" design that provides the Guarantee Processor direct access to accelerator data paths. Our proposed architecture could support various guarantee types, from basic usage auditing to sophisticated automated verification. This work establishes technical foundations for hardware-based AI governance mechanisms that could be deployed by 2027 to address emerging regulatory and international security needs in frontier AI development."
  },
  {
    "title": "Automated Traffic Incident Response Plans using Generative Artificial Intelligence: Part 1 -- Building the Incident Response Benchmark",
    "url": "http://arxiv.org/abs/2506.03381v1",
    "arxiv_id": "2506.03381v1",
    "authors": [
      "Artur Grigorev",
      "Khaled Saleh",
      "Jiwon Kim",
      "Adriana-Simona Mihaita"
    ],
    "published": "2025-06-03T20:40:44+00:00",
    "summary": "Traffic incidents remain a critical public safety concern worldwide, with Australia recording 1,300 road fatalities in 2024, which is the highest toll in 12 years. Similarly, the United States reports approximately 6 million crashes annually, raising significant challenges in terms of a fast reponse time and operational management. Traditional response protocols rely on human decision-making, which introduces potential inconsistencies and delays during critical moments when every minute impacts both safety outcomes and network performance. To address this issue, we propose a novel Incident Response Benchmark that uses generative artificial intelligence to automatically generate response plans for incoming traffic incidents. Our approach aims to significantly reduce incident resolution times by suggesting context-appropriate actions such as variable message sign deployment, lane closures, and emergency resource allocation adapted to specific incident characteristics. First, the proposed methodology uses real-world incident reports from the Performance Measurement System (PeMS) as training and evaluation data. We extract historically implemented actions from these reports and compare them against AI-generated response plans that suggest specific actions, such as lane closures, variable message sign announcements, and/or dispatching appropriate emergency resources. Second, model evaluations reveal that advanced generative AI models like GPT-4o and Grok 2 achieve superior alignment with expert solutions, demonstrated by minimized Hamming distances (averaging 2.96-2.98) and low weighted differences (approximately 0.27-0.28). Conversely, while Gemini 1.5 Pro records the lowest count of missed actions, its extremely high number of unnecessary actions (1547 compared to 225 for GPT-4o) indicates an over-triggering strategy that reduces the overall plan efficiency."
  },
  {
    "title": "Spatial Association Between Near-Misses and Accident Blackspots in Sydney, Australia: A Getis-Ord $G_i^*$ Analysis",
    "url": "http://arxiv.org/abs/2506.03356v1",
    "arxiv_id": "2506.03356v1",
    "authors": [
      "Artur Grigorev",
      "David Lillo-Trynes",
      "Adriana-Simona Mihaita"
    ],
    "published": "2025-06-03T19:58:56+00:00",
    "summary": "Road safety management teams utilize on historical accident logs to identify blackspots, which are inherently rare and sparse in space and time. Near-miss events captured through vehicle telematics and transmitted in real-time by connected vehicles reveal a unique potential of prevention due to their high frequency nature and driving engagement on the road. There is currently a lack of understanding of the high potential of near-miss data in real-time to proactively detect potential risky driving areas, in advance of a fatal collision. This paper aims to spatially identify clusters of reported accidents (A) versus high-severity near-misses (High-G) within an urban environment (Sydney, Australia) and showcase how the presence of near-misses can significantly lead to future crashes in identified risky hotspots. First, by utilizing a 400m grid framework, we identify significant crash hotspots using the Getis-Ord $G_i^*$ statistical approach. Second, we employ a Bivariate Local Moran's I (LISA) approach to assess and map the spatial concordance and discordance between official crash counts (A) and High-G counts from nearmiss data (High-G). Third, we classify areas based on their joint spatial patterns into: a) High-High (HH) as the most riskiest areas in both historical logs and nearmiss events, High-Low (HL) for high crash logs but low nearmiss records, c) Low-High (LH) for low past crash records but high nearmiss events, and d) Low-Low (LL) for safe areas. Finally, we run a feature importance ranking on all area patterns by using a contextual Point of Interest (POI) count features and we showcase which factors are the most critical to the occurrence of crash blackspots."
  },
  {
    "title": "Adversarial Attacks on Robotic Vision Language Action Models",
    "url": "http://arxiv.org/abs/2506.03350v1",
    "arxiv_id": "2506.03350v1",
    "authors": [
      "Eliot Krzysztof Jones",
      "Alexander Robey",
      "Andy Zou",
      "Zachary Ravichandran",
      "George J. Pappas",
      "Hamed Hassani",
      "Matt Fredrikson",
      "J. Zico Kolter"
    ],
    "published": "2025-06-03T19:43:58+00:00",
    "summary": "The emergence of vision-language-action models (VLAs) for end-to-end control is reshaping the field of robotics by enabling the fusion of multimodal sensory inputs at the billion-parameter scale. The capabilities of VLAs stem primarily from their architectures, which are often based on frontier large language models (LLMs). However, LLMs are known to be susceptible to adversarial misuse, and given the significant physical risks inherent to robotics, questions remain regarding the extent to which VLAs inherit these vulnerabilities. Motivated by these concerns, in this work we initiate the study of adversarial attacks on VLA-controlled robots. Our main algorithmic contribution is the adaptation and application of LLM jailbreaking attacks to obtain complete control authority over VLAs. We find that textual attacks, which are applied once at the beginning of a rollout, facilitate full reachability of the action space of commonly used VLAs and often persist over longer horizons. This differs significantly from LLM jailbreaking literature, as attacks in the real world do not have to be semantically linked to notions of harm. We make all code available at https://github.com/eliotjones1/robogcg ."
  },
  {
    "title": "Structural Vibration Monitoring with Diffractive Optical Processors",
    "url": "http://arxiv.org/abs/2506.03317v1",
    "arxiv_id": "2506.03317v1",
    "authors": [
      "Yuntian Wang",
      "Zafer Yilmaz",
      "Yuhang Li",
      "Edward Liu",
      "Eric Ahlberg",
      "Farid Ghahari",
      "Ertugrul Taciroglu",
      "Aydogan Ozcan"
    ],
    "published": "2025-06-03T19:06:04+00:00",
    "summary": "Structural Health Monitoring (SHM) is vital for maintaining the safety and longevity of civil infrastructure, yet current solutions remain constrained by cost, power consumption, scalability, and the complexity of data processing. Here, we present a diffractive vibration monitoring system, integrating a jointly optimized diffractive layer with a shallow neural network-based backend to remotely extract 3D structural vibration spectra, offering a low-power, cost-effective and scalable solution. This architecture eliminates the need for dense sensor arrays or extensive data acquisition; instead, it uses a spatially-optimized passive diffractive layer that encodes 3D structural displacements into modulated light, captured by a minimal number of detectors and decoded in real-time by shallow and low-power neural networks to reconstruct the 3D displacement spectra of structures. The diffractive system's efficacy was demonstrated both numerically and experimentally using millimeter-wave illumination on a laboratory-scale building model with a programmable shake table. Our system achieves more than an order-of-magnitude improvement in accuracy over conventional optics or separately trained modules, establishing a foundation for high-throughput 3D monitoring of structures. Beyond SHM, the 3D vibration monitoring capabilities of this cost-effective and data-efficient framework establish a new computational sensing modality with potential applications in disaster resilience, aerospace diagnostics, and autonomous navigation, where energy efficiency, low latency, and high-throughput are critical."
  },
  {
    "title": "Empirical Evaluation of Generalizable Automated Program Repair with Large Language Models",
    "url": "http://arxiv.org/abs/2506.03283v1",
    "arxiv_id": "2506.03283v1",
    "authors": [
      "Viola Campos",
      "Ridwan Shariffdeen",
      "Adrian Ulges",
      "Yannic Noller"
    ],
    "published": "2025-06-03T18:15:14+00:00",
    "summary": "Automated Program Repair (APR) proposes bug fixes to aid developers in maintaining software. The state of the art in this domain focuses on using LLMs, leveraging their strong capabilities to comprehend specifications in natural language and to generate program code. Recent works have shown that LLMs can be used to generate repairs. However, despite the APR community's research achievements and several industry deployments in the last decade, APR still lacks the capabilities to generalize broadly. In this work, we present an intensive empirical evaluation of LLMs for generating patches. We evaluate a diverse set of 13 recent models, including open ones (e.g., Llama 3.3, Qwen 2.5 Coder, and DeepSeek R1 (dist.)) and closed ones (e.g., o3-mini, GPT-4o, Claude 3.7 Sonnet, Gemini 2.0 Flash). In particular, we explore language-agnostic repairs by utilizing benchmarks for Java (e.g., Defects4J), JavaScript (e.g., BugsJS), Python (e.g., BugsInPy), and PHP (e.g., BugsPHP). Besides the generalization between different languages and levels of patch complexity, we also investigate the effects of fault localization (FL) as a preprocessing step and compare the progress for open vs closed models. Our evaluation represents a snapshot of the current repair capabilities of the latest LLMs. Key results include: (1) Different LLMs tend to perform best for different languages, which makes it hard to develop cross-platform repair techniques with single LLMs. (2) The combinations of models add value with respect to uniquely fixed bugs, so a committee of expert models should be considered. (3) Under realistic assumptions of imperfect FL, we observe significant drops in accuracy from the usual practice of using perfect FL. Our findings and insights will help both researchers and practitioners develop reliable and generalizable APR techniques and evaluate them in realistic and fair environments."
  },
  {
    "title": "Design Didn't Prevent These Hazard Exposures",
    "url": "http://arxiv.org/abs/2506.03280v1",
    "arxiv_id": "2506.03280v1",
    "authors": [
      "David. E Mertz"
    ],
    "published": "2025-06-03T18:12:42+00:00",
    "summary": "Two recent electrical incidents demonstrate how the design of equipment encouraged electrical workers to take actions that violated NFPA 70E principles. The features that encouraged non-compliant work execution will be described, as well as how the equipment was improved to facilitate safe work practices. Design-stage processes that help identify features that will foster rather than compromise safe work practices will be identified as well."
  },
  {
    "title": "Grounded Vision-Language Interpreter for Integrated Task and Motion Planning",
    "url": "http://arxiv.org/abs/2506.03270v1",
    "arxiv_id": "2506.03270v1",
    "authors": [
      "Jeremy Siburian",
      "Keisuke Shirai",
      "Cristian C. Beltran-Hernandez",
      "Masashi Hamaya",
      "Michael G\u00f6rner",
      "Atsushi Hashimoto"
    ],
    "published": "2025-06-03T18:00:32+00:00",
    "summary": "While recent advances in vision-language models (VLMs) have accelerated the development of language-guided robot planners, their black-box nature often lacks safety guarantees and interpretability crucial for real-world deployment. Conversely, classical symbolic planners offer rigorous safety verification but require significant expert knowledge for setup. To bridge the current gap, this paper proposes ViLaIn-TAMP, a hybrid planning framework for enabling verifiable, interpretable, and autonomous robot behaviors. ViLaIn-TAMP comprises three main components: (1) ViLaIn (Vision-Language Interpreter) - A prior framework that converts multimodal inputs into structured problem specifications using off-the-shelf VLMs without additional domain-specific training, (2) a modular Task and Motion Planning (TAMP) system that grounds these specifications in actionable trajectory sequences through symbolic and geometric constraint reasoning and can utilize learning-based skills for key manipulation phases, and (3) a corrective planning module which receives concrete feedback on failed solution attempts from the motion and task planning components and can feed adapted logic and geometric feasibility constraints back to ViLaIn to improve and further refine the specification. We evaluate our framework on several challenging manipulation tasks in a cooking domain. We demonstrate that the proposed closed-loop corrective architecture exhibits a more than 30% higher mean success rate for ViLaIn-TAMP compared to without corrective planning."
  },
  {
    "title": "Assessing Workers Neuro-physiological Stress Responses to Augmented Reality Safety Warnings in Immersive Virtual Roadway Work Zones",
    "url": "http://arxiv.org/abs/2506.03113v1",
    "arxiv_id": "2506.03113v1",
    "authors": [
      "Fatemeh Banani Ardecani",
      "Omidreza Shoghli"
    ],
    "published": "2025-06-03T17:40:49+00:00",
    "summary": "This paper presents a multi-stage experimental framework that integrates immersive Virtual Reality (VR) simulations, wearable sensors, and advanced signal processing to investigate construction workers neuro-physiological stress responses to multi-sensory AR-enabled warnings. Participants performed light- and moderate-intensity roadway maintenance tasks within a high-fidelity VR roadway work zone, while key stress markers of electrodermal activity (EDA), heart rate variability (HRV), and electroencephalography (EEG) were continuously measured. Statistical analyses revealed that task intensity significantly influenced physiological and neurological stress indicators. Moderate-intensity tasks elicited greater autonomic arousal, evidenced by elevated heart rate measures (mean-HR, std-HR, max-HR) and stronger electrodermal responses, while EEG data indicated distinct stress-related alpha suppression and beta enhancement. Feature-importance analysis further identified mean EDR and short-term HR metrics as discriminative for classifying task intensity. Correlation results highlighted a temporal lag between immediate neural changes and subsequent physiological stress reactions, emphasizing the interplay between cognition and autonomic regulation during hazardous tasks."
  },
  {
    "title": "EgoVLM: Policy Optimization for Egocentric Video Understanding",
    "url": "http://arxiv.org/abs/2506.03097v1",
    "arxiv_id": "2506.03097v1",
    "authors": [
      "Ashwin Vinod",
      "Shrey Pandit",
      "Aditya Vavre",
      "Linshen Liu"
    ],
    "published": "2025-06-03T17:28:00+00:00",
    "summary": "Emerging embodied AI applications, such as wearable cameras and autonomous agents, have underscored the need for robust reasoning from first person video streams. We introduce EgoVLM, a vision-language model specifically designed to integrate visual comprehension and spatial-temporal reasoning within egocentric video contexts. EgoVLM is fine-tuned via Group Relative Policy Optimization (GRPO), a reinforcement learning method adapted to align model outputs with human-like reasoning steps. Following DeepSeek R1-Zero's approach, we directly tune using RL without any supervised fine-tuning phase on chain-of-thought (CoT) data. We evaluate EgoVLM on egocentric video question answering benchmarks and show that domain-specific training substantially improves performance over general-purpose VLMs. Our EgoVLM-3B, trained exclusively on non-CoT egocentric data, outperforms the base Qwen2.5-VL 3B and 7B models by 14.33 and 13.87 accuracy points on the EgoSchema benchmark, respectively. By explicitly generating reasoning traces, EgoVLM enhances interpretability, making it well-suited for downstream applications. Furthermore, we introduce a novel keyframe-based reward that incorporates salient frame selection to guide reinforcement learning optimization. This reward formulation opens a promising avenue for future exploration in temporally grounded egocentric reasoning."
  },
  {
    "title": "ORV: 4D Occupancy-centric Robot Video Generation",
    "url": "http://arxiv.org/abs/2506.03079v1",
    "arxiv_id": "2506.03079v1",
    "authors": [
      "Xiuyu Yang",
      "Bohan Li",
      "Shaocong Xu",
      "Nan Wang",
      "Chongjie Ye",
      "Zhaoxi Chen",
      "Minghan Qin",
      "Yikang Ding",
      "Xin Jin",
      "Hang Zhao",
      "Hao Zhao"
    ],
    "published": "2025-06-03T17:00:32+00:00",
    "summary": "Acquiring real-world robotic simulation data through teleoperation is notoriously time-consuming and labor-intensive. Recently, action-driven generative models have gained widespread adoption in robot learning and simulation, as they eliminate safety concerns and reduce maintenance efforts. However, the action sequences used in these methods often result in limited control precision and poor generalization due to their globally coarse alignment. To address these limitations, we propose ORV, an Occupancy-centric Robot Video generation framework, which utilizes 4D semantic occupancy sequences as a fine-grained representation to provide more accurate semantic and geometric guidance for video generation. By leveraging occupancy-based representations, ORV enables seamless translation of simulation data into photorealistic robot videos, while ensuring high temporal consistency and precise controllability. Furthermore, our framework supports the simultaneous generation of multi-view videos of robot gripping operations - an important capability for downstream robotic learning tasks. Extensive experimental results demonstrate that ORV consistently outperforms existing baseline methods across various datasets and sub-tasks. Demo, Code and Model: https://orangesodahub.github.io/ORV"
  },
  {
    "title": "Multi-Metric Adaptive Experimental Design under Fixed Budget with Validation",
    "url": "http://arxiv.org/abs/2506.03062v1",
    "arxiv_id": "2506.03062v1",
    "authors": [
      "Qining Zhang",
      "Tanner Fiez",
      "Yi Liu",
      "Wenyang Liu"
    ],
    "published": "2025-06-03T16:41:11+00:00",
    "summary": "Standard A/B tests in online experiments face statistical power challenges when testing multiple candidates simultaneously, while adaptive experimental designs (AED) alone fall short in inferring experiment statistics such as the average treatment effect, especially with many metrics (e.g., revenue, safety) and heterogeneous variances. This paper proposes a fixed-budget multi-metric AED framework with a two-phase structure: an adaptive exploration phase to identify the best treatment, and a validation phase with an A/B test to verify the treatment's quality and infer statistics. We propose SHRVar, which generalizes sequential halving (SH) (Karnin et al., 2013) with a novel relative-variance-based sampling and an elimination strategy built on reward z-values. It achieves a provable error probability that decreases exponentially, where the exponent generalizes the complexity measure for SH (Karnin et al., 2013) and SHVar (Lalitha et al., 2023) with homogeneous and heterogeneous variances, respectively. Numerical experiments verify our analysis and demonstrate the superior performance of this new framework."
  },
  {
    "title": "Corrigibility as a Singular Target: A Vision for Inherently Reliable Foundation Models",
    "url": "http://arxiv.org/abs/2506.03056v1",
    "arxiv_id": "2506.03056v1",
    "authors": [
      "Ram Potham",
      "Max Harms"
    ],
    "published": "2025-06-03T16:36:03+00:00",
    "summary": "Foundation models (FMs) face a critical safety challenge: as capabilities scale, instrumental convergence drives default trajectories toward loss of human control, potentially culminating in existential catastrophe. Current alignment approaches struggle with value specification complexity and fail to address emergent power-seeking behaviors. We propose \"Corrigibility as a Singular Target\" (CAST)-designing FMs whose overriding objective is empowering designated human principals to guide, correct, and control them. This paradigm shift from static value-loading to dynamic human empowerment transforms instrumental drives: self-preservation serves only to maintain the principal's control; goal modification becomes facilitating principal guidance. We present a comprehensive empirical research agenda spanning training methodologies (RLAIF, SFT, synthetic data generation), scalability testing across model sizes, and demonstrations of controlled instructability. Our vision: FMs that become increasingly responsive to human guidance as capabilities grow, offering a path to beneficial AI that remains as tool-like as possible, rather than supplanting human judgment. This addresses the core alignment problem at its source, preventing the default trajectory toward misaligned instrumental convergence."
  },
  {
    "title": "MAEBE: Multi-Agent Emergent Behavior Framework",
    "url": "http://arxiv.org/abs/2506.03053v1",
    "arxiv_id": "2506.03053v1",
    "authors": [
      "Sinem Erisken",
      "Timothy Gothard",
      "Martin Leitgab",
      "Ram Potham"
    ],
    "published": "2025-06-03T16:33:47+00:00",
    "summary": "Traditional AI safety evaluations on isolated LLMs are insufficient as multi-agent AI ensembles become prevalent, introducing novel emergent risks. This paper introduces the Multi-Agent Emergent Behavior Evaluation (MAEBE) framework to systematically assess such risks. Using MAEBE with the Greatest Good Benchmark (and a novel double-inversion question technique), we demonstrate that: (1) LLM moral preferences, particularly for Instrumental Harm, are surprisingly brittle and shift significantly with question framing, both in single agents and ensembles. (2) The moral reasoning of LLM ensembles is not directly predictable from isolated agent behavior due to emergent group dynamics. (3) Specifically, ensembles exhibit phenomena like peer pressure influencing convergence, even when guided by a supervisor, highlighting distinct safety and alignment challenges. Our findings underscore the necessity of evaluating AI systems in their interactive, multi-agent contexts."
  },
  {
    "title": "Performance of leading large language models in May 2025 in Membership of the Royal College of General Practitioners-style examination questions: a cross-sectional analysis",
    "url": "http://arxiv.org/abs/2506.02987v1",
    "arxiv_id": "2506.02987v1",
    "authors": [
      "Richard Armitage"
    ],
    "published": "2025-06-03T15:25:38+00:00",
    "summary": "Background: Large language models (LLMs) have demonstrated substantial potential to support clinical practice. Other than Chat GPT4 and its predecessors, few LLMs, especially those of the leading and more powerful reasoning model class, have been subjected to medical specialty examination questions, including in the domain of primary care. This paper aimed to test the capabilities of leading LLMs as of May 2025 (o3, Claude Opus 4, Grok3, and Gemini 2.5 Pro) in primary care education, specifically in answering Member of the Royal College of General Practitioners (MRCGP) style examination questions.   Methods: o3, Claude Opus 4, Grok3, and Gemini 2.5 Pro were tasked to answer 100 randomly chosen multiple choice questions from the Royal College of General Practitioners GP SelfTest on 25 May 2025. Questions included textual information, laboratory results, and clinical images. Each model was prompted to answer as a GP in the UK and was provided with full question information. Each question was attempted once by each model. Responses were scored against correct answers provided by GP SelfTest.   Results: The total score of o3, Claude Opus 4, Grok3, and Gemini 2.5 Pro was 99.0%, 95.0%, 95.0%, and 95.0%, respectively. The average peer score for the same questions was 73.0%.   Discussion: All models performed remarkably well, and all substantially exceeded the average performance of GPs and GP registrars who had answered the same questions. o3 demonstrated the best performance, while the performances of the other leading models were comparable with each other and were not substantially lower than that of o3. These findings strengthen the case for LLMs, particularly reasoning models, to support the delivery of primary care, especially those that have been specifically trained on primary care clinical data."
  },
  {
    "title": "UniConFlow: A Unified Constrained Generalization Framework for Certified Motion Planning with Flow Matching Models",
    "url": "http://arxiv.org/abs/2506.02955v1",
    "arxiv_id": "2506.02955v1",
    "authors": [
      "Zewen Yang",
      "Xiaobing Dai",
      "Dian Yu",
      "Qianru Li",
      "Yu Li",
      "Valentin Le Mesle"
    ],
    "published": "2025-06-03T14:48:04+00:00",
    "summary": "Generative models have become increasingly powerful tools for robot motion generation, enabling flexible and multimodal trajectory generation across various tasks. Yet, most existing approaches remain limited in handling multiple types of constraints, such as collision avoidance and dynamic consistency, which are often treated separately or only partially considered. This paper proposes UniConFlow, a unified flow matching (FM) based framework for trajectory generation that systematically incorporates both equality and inequality constraints. UniConFlow introduces a novel prescribed-time zeroing function to enhance flexibility during the inference process, allowing the model to adapt to varying task requirements. To ensure constraint satisfaction, particularly with respect to obstacle avoidance, admissible action range, and kinodynamic consistency, the guidance inputs to the FM model are derived through a quadratic programming formulation, which enables constraint-aware generation without requiring retraining or auxiliary controllers. We conduct mobile navigation and high-dimensional manipulation tasks, demonstrating improved safety and feasibility compared to state-of-the-art constrained generative planners. Project page is available at https://uniconflow.github.io."
  },
  {
    "title": "Online Performance Assessment of Multi-Source-Localization for Autonomous Driving Systems Using Subjective Logic",
    "url": "http://arxiv.org/abs/2506.02932v1",
    "arxiv_id": "2506.02932v1",
    "authors": [
      "Stefan Orf",
      "Sven Ochs",
      "Marc Ren\u00e9 Zofka",
      "J. Marius Z\u00f6llner"
    ],
    "published": "2025-06-03T14:34:08+00:00",
    "summary": "Autonomous driving (AD) relies heavily on high precision localization as a crucial part of all driving related software components. The precise positioning is necessary for the utilization of high-definition maps, prediction of other road participants and the controlling of the vehicle itself. Due to this reason, the localization is absolutely safety relevant. Typical errors of the localization systems, which are long term drifts, jumps and false localization, that must be detected to enhance safety. An online assessment and evaluation of the current localization performance is a challenging task, which is usually done by Kalman filtering for single localization systems. Current autonomous vehicles cope with these challenges by fusing multiple individual localization methods into an overall state estimation. Such approaches need expert knowledge for a competitive performance in challenging environments. This expert knowledge is based on the trust and the prioritization of distinct localization methods in respect to the current situation and environment.   This work presents a novel online performance assessment technique of multiple localization systems by using subjective logic (SL). In our research vehicles, three different systems for localization are available, namely odometry-, Simultaneous Localization And Mapping (SLAM)- and Global Navigation Satellite System (GNSS)-based. Our performance assessment models the behavior of these three localization systems individually and puts them into reference of each other. The experiments were carried out using the CoCar NextGen, which is based on an Audi A6. The vehicle's localization system was evaluated under challenging conditions, specifically within a tunnel environment. The overall evaluation shows the feasibility of our approach."
  },
  {
    "title": "The Limits of Predicting Agents from Behaviour",
    "url": "http://arxiv.org/abs/2506.02923v1",
    "arxiv_id": "2506.02923v1",
    "authors": [
      "Alexis Bellot",
      "Jonathan Richens",
      "Tom Everitt"
    ],
    "published": "2025-06-03T14:24:58+00:00",
    "summary": "As the complexity of AI systems and their interactions with the world increases, generating explanations for their behaviour is important for safely deploying AI. For agents, the most natural abstractions for predicting behaviour attribute beliefs, intentions and goals to the system. If an agent behaves as if it has a certain goal or belief, then we can make reasonable predictions about how it will behave in novel situations, including those where comprehensive safety evaluations are untenable. How well can we infer an agent's beliefs from their behaviour, and how reliably can these inferred beliefs predict the agent's behaviour in novel situations? We provide a precise answer to this question under the assumption that the agent's behaviour is guided by a world model. Our contribution is the derivation of novel bounds on the agent's behaviour in new (unseen) deployment environments, which represent a theoretical limit for predicting intentional agents from behavioural data alone. We discuss the implications of these results for several research areas including fairness and safety."
  },
  {
    "title": "Functionality Assessment Framework for Autonomous Driving Systems using Subjective Networks",
    "url": "http://arxiv.org/abs/2506.02922v1",
    "arxiv_id": "2506.02922v1",
    "authors": [
      "Stefan Orf",
      "Sven Ochs",
      "Valentin Marotta",
      "Oliver Conder",
      "Marc Ren\u00e9 Zofka",
      "J. Marius Z\u00f6llner"
    ],
    "published": "2025-06-03T14:24:12+00:00",
    "summary": "In complex autonomous driving (AD) software systems, the functioning of each system part is crucial for safe operation. By measuring the current functionality or operability of individual components an isolated glimpse into the system is given. Literature provides several of these detached assessments, often in the form of safety or performance measures. But dependencies, redundancies, error propagation and conflicting functionality statements do not allow for easy combination of these measures into a big picture of the functioning of the entire AD stack. Data is processed and exchanged between different components, each of which can fail, making an overall statement challenging. The lack of functionality assessment frameworks that tackle these problems underlines this complexity.   This article presents a novel framework for inferring an overall functionality statement for complex component based systems by considering their dependencies, redundancies, error propagation paths and the assessments of individual components. Our framework first incorporates a comprehensive conversion to an assessment representation of the system. The representation is based on Subjective Networks (SNs) that allow for easy identification of faulty system parts. Second, the framework offers a flexible method for computing the system's functionality while dealing with contradicting assessments about the same component and dependencies, as well as redundancies, of the system. We discuss the framework's capabilities on real-life data of our AD stack with assessments of various components."
  },
  {
    "title": "Cell-o1: Training LLMs to Solve Single-Cell Reasoning Puzzles with Reinforcement Learning",
    "url": "http://arxiv.org/abs/2506.02911v1",
    "arxiv_id": "2506.02911v1",
    "authors": [
      "Yin Fang",
      "Qiao Jin",
      "Guangzhi Xiong",
      "Bowen Jin",
      "Xianrui Zhong",
      "Siru Ouyang",
      "Aidong Zhang",
      "Jiawei Han",
      "Zhiyong Lu"
    ],
    "published": "2025-06-03T14:16:53+00:00",
    "summary": "Cell type annotation is a key task in analyzing the heterogeneity of single-cell RNA sequencing data. Although recent foundation models automate this process, they typically annotate cells independently, without considering batch-level cellular context or providing explanatory reasoning. In contrast, human experts often annotate distinct cell types for different cell clusters based on their domain knowledge. To mimic this workflow, we introduce the CellPuzzles task, where the objective is to assign unique cell types to a batch of cells. This benchmark spans diverse tissues, diseases, and donor conditions, and requires reasoning across the batch-level cellular context to ensure label uniqueness. We find that off-the-shelf large language models (LLMs) struggle on CellPuzzles, with the best baseline (OpenAI's o1) achieving only 19.0% batch-level accuracy. To fill this gap, we propose Cell-o1, a 7B LLM trained via supervised fine-tuning on distilled reasoning traces, followed by reinforcement learning with batch-level rewards. Cell-o1 achieves state-of-the-art performance, outperforming o1 by over 73% and generalizing well across contexts. Further analysis of training dynamics and reasoning behaviors provides insights into batch-level annotation performance and emergent expert-like reasoning. Code and data are available at https://github.com/ncbi-nlp/cell-o1."
  },
  {
    "title": "NetPress: Dynamically Generated LLM Benchmarks for Network Applications",
    "url": "http://arxiv.org/abs/2506.03231v1",
    "arxiv_id": "2506.03231v1",
    "authors": [
      "Yajie Zhou",
      "Jiajun Ruan",
      "Eric S. Wang",
      "Sadjad Fouladi",
      "Francis Y. Yan",
      "Kevin Hsieh",
      "Zaoxing Liu"
    ],
    "published": "2025-06-03T14:04:22+00:00",
    "summary": "Despite growing interest in domain-specific benchmarking of large language models (LLMs) and agents, current evaluations remain limited to static, small-scale datasets, especially in high-stakes tasks like network operations that demand reliability for deployments. We present NetPress, an automated benchmark generation framework for evaluating LLM agents in network applications. NetPress introduces a unified abstraction with state and action, enabling dynamic generation of diverse query sets along with corresponding ground truths. At runtime, users can specify benchmark configurations to generate millions of queries on the fly. In addition to dynamic benchmark construction, NetPress integrates with network emulators to provide realistic environment feedback, supporting comprehensive evaluation across correctness, safety, and latency. We instantiate NetPress on three representative applications, revealing interesting fine-grained differences in agent behavior that static, correctness-only benchmarks often miss. NetPress moves LLM evaluation toward realistic, scalable testing in infrastructure-centric domains, helping close the gap between benchmark performance and real-world deployment readiness. Code is available at https://github.com/Froot-NetSys/NetPress."
  },
  {
    "title": "DiaBlo: Diagonal Blocks Are Sufficient For Finetuning",
    "url": "http://arxiv.org/abs/2506.03230v1",
    "arxiv_id": "2506.03230v1",
    "authors": [
      "Selcuk Gurses",
      "Aozhong Zhang",
      "Yanxia Deng",
      "Xun Dong",
      "Xin Li",
      "Naigang Wang",
      "Penghang Yin",
      "Zi Yang"
    ],
    "published": "2025-06-03T13:47:59+00:00",
    "summary": "Finetuning is a critical step for adapting large language models (LLMs) to domain-specific downstream tasks. To mitigate the substantial computational and memory costs of full-model fine-tuning, Parameter-Efficient Finetuning (PEFT) methods have been proposed to update only a small subset of model parameters. However, performance gaps between PEFT approaches and full-model fine-tuning still exist. In this work, we present DiaBlo, a simple yet effective PEFT approach that updates only the diagonal blocks of selected model weight matrices. Unlike Low Rank Adaptation (LoRA) and its variants, DiaBlo eliminates the need for low rank matrix products, thereby avoiding the reliance on auxiliary initialization schemes or customized optimization strategies to improve convergence. This design leads to stable and robust convergence while maintaining comparable memory efficiency and training speed to LoRA. We conduct extensive experiments across a range of tasks, including commonsense reasoning, arithmetic reasoning, code generation, and safety alignment, to evaluate the effectiveness and efficiency of DiaBlo. Across these benchmarks, DiaBlo demonstrates strong and consistent performance while maintaining high memory efficiency and fast finetuning speed. Codes are available at https://github.com/ziyangjoy/DiaBlo."
  },
  {
    "title": "It's the Thought that Counts: Evaluating the Attempts of Frontier LLMs to Persuade on Harmful Topics",
    "url": "http://arxiv.org/abs/2506.02873v1",
    "arxiv_id": "2506.02873v1",
    "authors": [
      "Matthew Kowal",
      "Jasper Timm",
      "Jean-Francois Godbout",
      "Thomas Costello",
      "Antonio A. Arechar",
      "Gordon Pennycook",
      "David Rand",
      "Adam Gleave",
      "Kellin Pelrine"
    ],
    "published": "2025-06-03T13:37:51+00:00",
    "summary": "Persuasion is a powerful capability of large language models (LLMs) that both enables beneficial applications (e.g. helping people quit smoking) and raises significant risks (e.g. large-scale, targeted political manipulation). Prior work has found models possess a significant and growing persuasive capability, measured by belief changes in simulated or real users. However, these benchmarks overlook a crucial risk factor: the propensity of a model to attempt to persuade in harmful contexts. Understanding whether a model will blindly ``follow orders'' to persuade on harmful topics (e.g. glorifying joining a terrorist group) is key to understanding the efficacy of safety guardrails. Moreover, understanding if and when a model will engage in persuasive behavior in pursuit of some goal is essential to understanding the risks from agentic AI systems. We propose the Attempt to Persuade Eval (APE) benchmark, that shifts the focus from persuasion success to persuasion attempts, operationalized as a model's willingness to generate content aimed at shaping beliefs or behavior. Our evaluation framework probes frontier LLMs using a multi-turn conversational setup between simulated persuader and persuadee agents. APE explores a diverse spectrum of topics including conspiracies, controversial issues, and non-controversially harmful content. We introduce an automated evaluator model to identify willingness to persuade and measure the frequency and context of persuasive attempts. We find that many open and closed-weight models are frequently willing to attempt persuasion on harmful topics and that jailbreaking can increase willingness to engage in such behavior. Our results highlight gaps in current safety guardrails and underscore the importance of evaluating willingness to persuade as a key dimension of LLM risk. APE is available at github.com/AlignmentResearch/AttemptPersuadeEval"
  },
  {
    "title": "BNPO: Beta Normalization Policy Optimization",
    "url": "http://arxiv.org/abs/2506.02864v1",
    "arxiv_id": "2506.02864v1",
    "authors": [
      "Changyi Xiao",
      "Mengdi Zhang",
      "Yixin Cao"
    ],
    "published": "2025-06-03T13:28:57+00:00",
    "summary": "Recent studies, including DeepSeek-R1 and Kimi-k1.5, have demonstrated that reinforcement learning with rule-based, binary-valued reward functions can significantly enhance the reasoning capabilities of large language models. These models primarily utilize REINFORCE-based policy optimization techniques, such as REINFORCE with baseline and group relative policy optimization (GRPO). However, a key limitation remains: current policy optimization methods either neglect reward normalization or employ static normalization strategies, which fail to adapt to the dynamic nature of policy updates during training. This may result in unstable gradient estimates and hinder training stability. To address this issue, we propose Beta Normalization Policy Optimization (BNPO), a novel policy optimization method that adaptively normalizes rewards using a Beta distribution with dynamically updated parameters. BNPO aligns the normalization with the changing policy distribution, enabling more precise and lower-variance gradient estimation, which in turn promotes stable training dynamics. We provide theoretical analysis demonstrating BNPO's variance-reducing properties and show that it generalizes both REINFORCE and GRPO under binary-valued reward settings. Furthermore, we introduce an advantage decomposition mechanism to extend BNPO's applicability to more complex reward systems. Experimental results confirm that BNPO achieves state-of-the-art performance among policy optimization methods on reasoning tasks. The code is available at https://github.com/changyi7231/BNPO."
  },
  {
    "title": "Learned Controllers for Agile Quadrotors in Pursuit-Evasion Games",
    "url": "http://arxiv.org/abs/2506.02849v1",
    "arxiv_id": "2506.02849v1",
    "authors": [
      "Alejandro Sanchez Roncero",
      "Olov Andersson",
      "Petter Ogren"
    ],
    "published": "2025-06-03T13:19:23+00:00",
    "summary": "The increasing proliferation of small UAVs in civilian and military airspace has raised critical safety and security concerns, especially when unauthorized or malicious drones enter restricted zones. In this work, we present a reinforcement learning (RL) framework for agile 1v1 quadrotor pursuit-evasion. We train neural network policies to command body rates and collective thrust, enabling high-speed pursuit and evasive maneuvers that fully exploit the quadrotor's nonlinear dynamics. To mitigate nonstationarity and catastrophic forgetting during adversarial co-training, we introduce an Asynchronous Multi-Stage Population-Based (AMSPB) algorithm where, at each stage, either the pursuer or evader learns against a sampled opponent drawn from a growing population of past and current policies. This continual learning setup ensures monotonic performance improvement and retention of earlier strategies. Our results show that (i) rate-based policies achieve significantly higher capture rates and peak speeds than velocity-level baselines, and (ii) AMSPB yields stable, monotonic gains against a suite of benchmark opponents."
  },
  {
    "title": "Go Beyond Earth: Understanding Human Actions and Scenes in Microgravity Environments",
    "url": "http://arxiv.org/abs/2506.02845v1",
    "arxiv_id": "2506.02845v1",
    "authors": [
      "Di Wen",
      "Lei Qi",
      "Kunyu Peng",
      "Kailun Yang",
      "Fei Teng",
      "Ao Luo",
      "Jia Fu",
      "Yufan Chen",
      "Ruiping Liu",
      "Yitian Shi",
      "M. Saquib Sarfraz",
      "Rainer Stiefelhagen"
    ],
    "published": "2025-06-03T13:15:19+00:00",
    "summary": "Despite substantial progress in video understanding, most existing datasets are limited to Earth's gravitational conditions. However, microgravity alters human motion, interactions, and visual semantics, revealing a critical gap for real-world vision systems. This presents a challenge for domain-robust video understanding in safety-critical space applications. To address this, we introduce MicroG-4M, the first benchmark for spatio-temporal and semantic understanding of human activities in microgravity. Constructed from real-world space missions and cinematic simulations, the dataset includes 4,759 clips covering 50 actions, 1,238 context-rich captions, and over 7,000 question-answer pairs on astronaut activities and scene understanding. MicroG-4M supports three core tasks: fine-grained multi-label action recognition, temporal video captioning, and visual question answering, enabling a comprehensive evaluation of both spatial localization and semantic reasoning in microgravity contexts. We establish baselines using state-of-the-art models. All data, annotations, and code are available at https://github.com/LEI-QI-233/HAR-in-Space."
  },
  {
    "title": "AI-Driven Vehicle Condition Monitoring with Cell-Aware Edge Service Migration",
    "url": "http://arxiv.org/abs/2506.02785v1",
    "arxiv_id": "2506.02785v1",
    "authors": [
      "Charalampos Kalalas",
      "Pavol Mulinka",
      "Guillermo Candela Belmonte",
      "Miguel Fornell",
      "Michail Dalgitsis",
      "Francisco Paredes Vera",
      "Javier Santaella S\u00e1nchez",
      "Carmen Vicente Villares",
      "Roshan Sedar",
      "Eftychia Datsika",
      "Angelos Antonopoulos",
      "Antonio Fern\u00e1ndez Ojea",
      "Miquel Payaro"
    ],
    "published": "2025-06-03T12:12:27+00:00",
    "summary": "Artificial intelligence (AI) has been increasingly applied to the condition monitoring of vehicular equipment, aiming to enhance maintenance strategies, reduce costs, and improve safety. Leveraging the edge computing paradigm, AI-based condition monitoring systems process vast streams of vehicular data to detect anomalies and optimize operational performance. In this work, we introduce a novel vehicle condition monitoring service that enables real-time diagnostics of a diverse set of anomalies while remaining practical for deployment in real-world edge environments. To address mobility challenges, we propose a closed-loop service orchestration framework where service migration across edge nodes is dynamically triggered by network-related metrics. Our approach has been implemented and tested in a real-world race circuit environment equipped with 5G network capabilities under diverse operational conditions. Experimental results demonstrate the effectiveness of our framework in ensuring low-latency AI inference and adaptive service placement, highlighting its potential for intelligent transportation and mobility applications."
  },
  {
    "title": "Rethinking Machine Unlearning in Image Generation Models",
    "url": "http://arxiv.org/abs/2506.02761v1",
    "arxiv_id": "2506.02761v1",
    "authors": [
      "Renyang Liu",
      "Wenjie Feng",
      "Tianwei Zhang",
      "Wei Zhou",
      "Xueqi Cheng",
      "See-Kiong Ng"
    ],
    "published": "2025-06-03T11:25:14+00:00",
    "summary": "With the surge and widespread application of image generation models, data privacy and content safety have become major concerns and attracted great attention from users, service providers, and policymakers. Machine unlearning (MU) is recognized as a cost-effective and promising means to address these challenges. Despite some advancements, image generation model unlearning (IGMU) still faces remarkable gaps in practice, e.g., unclear task discrimination and unlearning guidelines, lack of an effective evaluation framework, and unreliable evaluation metrics. These can hinder the understanding of unlearning mechanisms and the design of practical unlearning algorithms. We perform exhaustive assessments over existing state-of-the-art unlearning algorithms and evaluation standards, and discover several critical flaws and challenges in IGMU tasks. Driven by these limitations, we make several core contributions, to facilitate the comprehensive understanding, standardized categorization, and reliable evaluation of IGMU. Specifically, (1) We design CatIGMU, a novel hierarchical task categorization framework. It provides detailed implementation guidance for IGMU, assisting in the design of unlearning algorithms and the construction of testbeds. (2) We introduce EvalIGMU, a comprehensive evaluation framework. It includes reliable quantitative metrics across five critical aspects. (3) We construct DataIGM, a high-quality unlearning dataset, which can be used for extensive evaluations of IGMU, training content detectors for judgment, and benchmarking the state-of-the-art unlearning algorithms. With EvalIGMU and DataIGM, we discover that most existing IGMU algorithms cannot handle the unlearning well across different evaluation dimensions, especially for preservation and robustness. Code and models are available at https://github.com/ryliu68/IGMU."
  },
  {
    "title": "Safely Learning Controlled Stochastic Dynamics",
    "url": "http://arxiv.org/abs/2506.02754v1",
    "arxiv_id": "2506.02754v1",
    "authors": [
      "Luc Brogat-Motte",
      "Alessandro Rudi",
      "Riccardo Bonalli"
    ],
    "published": "2025-06-03T11:17:07+00:00",
    "summary": "We address the problem of safely learning controlled stochastic dynamics from discrete-time trajectory observations, ensuring system trajectories remain within predefined safe regions during both training and deployment. Safety-critical constraints of this kind are crucial in applications such as autonomous robotics, finance, and biomedicine. We introduce a method that ensures safe exploration and efficient estimation of system dynamics by iteratively expanding an initial known safe control set using kernel-based confidence bounds. After training, the learned model enables predictions of the system's dynamics and permits safety verification of any given control. Our approach requires only mild smoothness assumptions and access to an initial safe control set, enabling broad applicability to complex real-world systems. We provide theoretical guarantees for safety and derive adaptive learning rates that improve with increasing Sobolev regularity of the true dynamics. Experimental evaluations demonstrate the practical effectiveness of our method in terms of safety, estimation accuracy, and computational efficiency."
  },
  {
    "title": "Large-scale Self-supervised Video Foundation Model for Intelligent Surgery",
    "url": "http://arxiv.org/abs/2506.02692v1",
    "arxiv_id": "2506.02692v1",
    "authors": [
      "Shu Yang",
      "Fengtao Zhou",
      "Leon Mayer",
      "Fuxiang Huang",
      "Yiliang Chen",
      "Yihui Wang",
      "Sunan He",
      "Yuxiang Nie",
      "Xi Wang",
      "\u00d6mer S\u00fcmer",
      "Yueming Jin",
      "Huihui Sun",
      "Shuchang Xu",
      "Alex Qinyang Liu",
      "Zheng Li",
      "Jing Qin",
      "Jeremy YuenChun Teoh",
      "Lena Maier-Hein",
      "Hao Chen"
    ],
    "published": "2025-06-03T09:42:54+00:00",
    "summary": "Computer-Assisted Intervention (CAI) has the potential to revolutionize modern surgery, with surgical scene understanding serving as a critical component in supporting decision-making, improving procedural efficacy, and ensuring intraoperative safety. While existing AI-driven approaches alleviate annotation burdens via self-supervised spatial representation learning, their lack of explicit temporal modeling during pre-training fundamentally restricts the capture of dynamic surgical contexts, resulting in incomplete spatiotemporal understanding. In this work, we introduce the first video-level surgical pre-training framework that enables joint spatiotemporal representation learning from large-scale surgical video data. To achieve this, we constructed a large-scale surgical video dataset comprising 3,650 videos and approximately 3.55 million frames, spanning more than 20 surgical procedures and over 10 anatomical structures. Building upon this dataset, we propose SurgVISTA (Surgical Video-level Spatial-Temporal Architecture), a reconstruction-based pre-training method that captures intricate spatial structures and temporal dynamics through joint spatiotemporal modeling. Additionally, SurgVISTA incorporates image-level knowledge distillation guided by a surgery-specific expert to enhance the learning of fine-grained anatomical and semantic features. To validate its effectiveness, we established a comprehensive benchmark comprising 13 video-level datasets spanning six surgical procedures across four tasks. Extensive experiments demonstrate that SurgVISTA consistently outperforms both natural- and surgical-domain pre-trained models, demonstrating strong potential to advance intelligent surgical systems in clinically meaningful scenarios."
  },
  {
    "title": "Stochastic Modeling of Road Hazards on Intersections and their Effect on Safety of Autonomous Vehicles",
    "url": "http://arxiv.org/abs/2506.02688v1",
    "arxiv_id": "2506.02688v1",
    "authors": [
      "Peter Popov",
      "Lorenzo Strigini",
      "Cornelius Buerkle",
      "Fabian Oboril",
      "Michael Paulitsch"
    ],
    "published": "2025-06-03T09:41:34+00:00",
    "summary": "Autonomous vehicles (AV) look set to become common on our roads within the next few years. However, to achieve the final breakthrough, not only functional progress is required, but also satisfactory safety assurance must be provided. Among those, a question demanding special attention is the need to assess and quantify the overall safety of an AV. Such an assessment must consider on the one hand the imperfections of the AV functionality and on the other hand its interaction with the environment. In a previous paper we presented a model-based approach to AV safety assessment in which we use a probabilistic model to describe road hazards together with the impact on AV safety of imperfect behavior of AV functions, such as safety monitors and perception systems. With this model, we are able to quantify the likelihood of the occurrence of a fatal accident, for a single operating condition. In this paper, we extend the approach and show how the model can deal explicitly with a set of different operating conditions defined in a given ODD."
  },
  {
    "title": "TL;DR: Too Long, Do Re-weighting for Effcient LLM Reasoning Compression",
    "url": "http://arxiv.org/abs/2506.02678v1",
    "arxiv_id": "2506.02678v1",
    "authors": [
      "Zhong-Zhi Li",
      "Xiao Liang",
      "Zihao Tang",
      "Lei Ji",
      "Peijie Wang",
      "Haotian Xu",
      "Xing W",
      "Haizhen Huang",
      "Weiwei Deng",
      "Ying Nian Wu",
      "Yeyun Gong",
      "Zhijiang Guo",
      "Xiao Liu",
      "Fei Yin",
      "Cheng-Lin Liu"
    ],
    "published": "2025-06-03T09:23:41+00:00",
    "summary": "Large Language Models (LLMs) have recently achieved remarkable progress by leveraging Reinforcement Learning and extended Chain-of-Thought (CoT) techniques. However, the challenge of performing efficient language reasoning--especially during inference with extremely long outputs--has drawn increasing attention from the research community. In this work, we propose a dynamic ratio-based training pipeline that does not rely on sophisticated data annotations or interpolation between multiple models. We continuously balance the weights between the model's System-1 and System-2 data to eliminate redundant reasoning processes while preserving the model's reasoning capability. We validate our approach across models on DeepSeek-R1-Distill-7B and DeepSeek-R1-Distill-14B and on a diverse set of benchmarks with varying difficulty levels. Our method significantly reduces the number of output tokens by nearly 40% while maintaining the accuracy of the reasoning. Our code and data will be available soon."
  },
  {
    "title": "From Prompts to Protection: Large Language Model-Enabled In-Context Learning for Smart Public Safety UAV",
    "url": "http://arxiv.org/abs/2506.02649v1",
    "arxiv_id": "2506.02649v1",
    "authors": [
      "Yousef Emami",
      "Hao Zhou",
      "Miguel Gutierrez Gaitan",
      "Kai Li",
      "Luis Almeida",
      "Zhu Han"
    ],
    "published": "2025-06-03T09:01:33+00:00",
    "summary": "A public safety Unmanned Aerial Vehicle (UAV) enhances situational awareness in emergency response. Its agility and ability to optimize mobility and establish Line-of-Sight (LoS) communication make it increasingly vital for managing emergencies such as disaster response, search and rescue, and wildfire monitoring. While Deep Reinforcement Learning (DRL) has been applied to optimize UAV navigation and control, its high training complexity, low sample efficiency, and simulation-to-reality gap limit its practicality in public safety. Recent advances in Large Language Models (LLMs) offer a compelling alternative. With strong reasoning and generalization capabilities, LLMs can adapt to new tasks through In-Context Learning (ICL), which enables task adaptation via natural language prompts and example-based guidance, without retraining. Deploying LLMs at the network edge, rather than in the cloud, further reduces latency and preserves data privacy, thereby making them suitable for real-time, mission-critical public safety UAVs. This paper proposes the integration of LLM-enabled ICL with public safety UAV to address the key functions, such as path planning and velocity control, in the context of emergency response. We present a case study on data collection scheduling where the LLM-enabled ICL framework can significantly reduce packet loss compared to conventional approaches, while also mitigating potential jailbreaking vulnerabilities. Finally, we discuss LLM optimizers and specify future research directions. The ICL framework enables adaptive, context-aware decision-making for public safety UAV, thus offering a lightweight and efficient solution for enhancing UAV autonomy and responsiveness in emergencies."
  },
  {
    "title": "Truly Assessing Fluid Intelligence of Large Language Models through Dynamic Reasoning Evaluation",
    "url": "http://arxiv.org/abs/2506.02648v1",
    "arxiv_id": "2506.02648v1",
    "authors": [
      "Yue Yang",
      "MingKang Chen",
      "Qihua Liu",
      "Mengkang Hu",
      "Qiguang Chen",
      "Gengrui Zhang",
      "Shuyue Hu",
      "Guangtao Zhai",
      "Yu Qiao",
      "Yu Wang",
      "Wenqi Shao",
      "Ping Luo"
    ],
    "published": "2025-06-03T09:01:08+00:00",
    "summary": "Recent advances in large language models (LLMs) have demonstrated impressive reasoning capacities that mirror human-like thinking. However, whether LLMs possess genuine fluid intelligence (i.e., the ability to reason abstractly and generalize rules in novel situations) remains an open question. Existing reasoning benchmarks either focus on domain-specific knowledge (crystallized intelligence) or lack interpretability. To address these limitations, we propose DRE-Bench, a dynamic reasoning evaluation benchmark grounded in a hierarchical cognitive framework. DRE-Bench consists of 36 abstract reasoning tasks organized across four cognitive levels, with each task featuring multiple dynamic variants that test the same underlying latent rule. This design enables fine-grained, interpretable, and reliable assessments of fluid intelligence. We evaluate a range of state-of-the-art LLMs, including both general LLMs (GPT-4o, Claude 3.7) and reasoning LLMs (o1, DeepSeek-R1, QwQ, Skywork-OR1). Experimental results reveal that although most LLMs achieve competent and robust performance in low-level cognition, they struggle with high-level cognition and exhibit limited generalization as task complexity grows. Our findings highlight the gap between current LLMs and true human-like fluid intelligence and offer a new path for systematically tracking reasoning progress in LLMs."
  },
  {
    "title": "Compositional Learning for Modular Multi-Agent Self-Organizing Networks",
    "url": "http://arxiv.org/abs/2506.02616v1",
    "arxiv_id": "2506.02616v1",
    "authors": [
      "Qi Liao",
      "Parijat Bhattacharjee"
    ],
    "published": "2025-06-03T08:33:18+00:00",
    "summary": "Self-organizing networks face challenges from complex parameter interdependencies and conflicting objectives. This study introduces two compositional learning approaches-Compositional Deep Reinforcement Learning (CDRL) and Compositional Predictive Decision-Making (CPDM)-and evaluates their performance under training time and safety constraints in multi-agent systems. We propose a modular, two-tier framework with cell-level and cell-pair-level agents to manage heterogeneous agent granularities while reducing model complexity. Numerical simulations reveal a significant reduction in handover failures, along with improved throughput and latency, outperforming conventional multi-agent deep reinforcement learning approaches. The approach also demonstrates superior scalability, faster convergence, higher sample efficiency, and safer training in large-scale self-organizing networks."
  },
  {
    "title": "IndoSafety: Culturally Grounded Safety for LLMs in Indonesian Languages",
    "url": "http://arxiv.org/abs/2506.02573v1",
    "arxiv_id": "2506.02573v1",
    "authors": [
      "Muhammad Falensi Azmi",
      "Muhammad Dehan Al Kautsar",
      "Alfan Farizki Wicaksono",
      "Fajri Koto"
    ],
    "published": "2025-06-03T07:53:55+00:00",
    "summary": "Although region-specific large language models (LLMs) are increasingly developed, their safety remains underexplored, particularly in culturally diverse settings like Indonesia, where sensitivity to local norms is essential and highly valued by the community. In this work, we present IndoSafety, the first high-quality, human-verified safety evaluation dataset tailored for the Indonesian context, covering five language varieties: formal and colloquial Indonesian, along with three major local languages: Javanese, Sundanese, and Minangkabau. IndoSafety is constructed by extending prior safety frameworks to develop a taxonomy that captures Indonesia's sociocultural context. We find that existing Indonesian-centric LLMs often generate unsafe outputs, particularly in colloquial and local language settings, while fine-tuning on IndoSafety significantly improves safety while preserving task performance. Our work highlights the critical need for culturally grounded safety evaluation and provides a concrete step toward responsible LLM deployment in multilingual settings. Warning: This paper contains example data that may be offensive, harmful, or biased."
  },
  {
    "title": "Generative AI for Predicting 2D and 3D Wildfire Spread: Beyond Physics-Based Models and Traditional Deep Learning",
    "url": "http://arxiv.org/abs/2506.02485v1",
    "arxiv_id": "2506.02485v1",
    "authors": [
      "Haowen Xu",
      "Sisi Zlatanova",
      "Ruiyu Liang",
      "Ismet Canbulat"
    ],
    "published": "2025-06-03T05:54:40+00:00",
    "summary": "Wildfires continue to inflict devastating human, environmental, and economic losses globally, as tragically exemplified by the 2025 Los Angeles wildfire and the urgent demand for more effective response strategies. While physics-based and deep learning models have advanced wildfire simulation, they face critical limitations in predicting and visualizing multimodal fire spread in real time, particularly in both 2D and 3D spatial domains using dynamically updated GIS data. These limitations hinder timely emergency response, infrastructure protection, and community safety. Generative AI has recently emerged as a transformative approach across research and industry. Models such as Generative Adversarial Networks (GANs), Variational Autoencoders (VAEs), Transformers, and diffusion-based architectures offer distinct advantages over traditional methods, including the integration of multimodal data, generation of diverse scenarios under uncertainty, and improved modeling of wildfire dynamics across spatial and temporal scales. This position paper advocates for the adoption of generative AI as a foundational framework for wildfire prediction. We explore how such models can enhance 2D fire spread forecasting and enable more realistic, scalable 3D simulations. Additionally, we employ a novel human-AI collaboration framework using large language models (LLMs) for automated knowledge extraction, literature synthesis, and bibliometric mapping. Looking ahead, we identify five key visions for integrating generative AI into wildfire management: multimodal approaches, AI foundation models, conversational AI systems, edge-computing-based scenario generation, and cognitive digital twins. We also address three major challenges accompanying these opportunities and propose potential solutions to support their implementation."
  },
  {
    "title": "BitBypass: A New Direction in Jailbreaking Aligned Large Language Models with Bitstream Camouflage",
    "url": "http://arxiv.org/abs/2506.02479v1",
    "arxiv_id": "2506.02479v1",
    "authors": [
      "Kalyan Nakka",
      "Nitesh Saxena"
    ],
    "published": "2025-06-03T05:51:18+00:00",
    "summary": "The inherent risk of generating harmful and unsafe content by Large Language Models (LLMs), has highlighted the need for their safety alignment. Various techniques like supervised fine-tuning, reinforcement learning from human feedback, and red-teaming were developed for ensuring the safety alignment of LLMs. However, the robustness of these aligned LLMs is always challenged by adversarial attacks that exploit unexplored and underlying vulnerabilities of the safety alignment. In this paper, we develop a novel black-box jailbreak attack, called BitBypass, that leverages hyphen-separated bitstream camouflage for jailbreaking aligned LLMs. This represents a new direction in jailbreaking by exploiting fundamental information representation of data as continuous bits, rather than leveraging prompt engineering or adversarial manipulations. Our evaluation of five state-of-the-art LLMs, namely GPT-4o, Gemini 1.5, Claude 3.5, Llama 3.1, and Mixtral, in adversarial perspective, revealed the capabilities of BitBypass in bypassing their safety alignment and tricking them into generating harmful and unsafe content. Further, we observed that BitBypass outperforms several state-of-the-art jailbreak attacks in terms of stealthiness and attack success. Overall, these results highlights the effectiveness and efficiency of BitBypass in jailbreaking these state-of-the-art LLMs."
  },
  {
    "title": "XToM: Exploring the Multilingual Theory of Mind for Large Language Models",
    "url": "http://arxiv.org/abs/2506.02461v1",
    "arxiv_id": "2506.02461v1",
    "authors": [
      "Chunkit Chan",
      "Yauwai Yim",
      "Hongchuan Zeng",
      "Zhiying Zou",
      "Xinyuan Cheng",
      "Zhifan Sun",
      "Zheye Deng",
      "Kawai Chung",
      "Yuzhuo Ao",
      "Yixiang Fan",
      "Cheng Jiayang",
      "Ercong Nie",
      "Ginny Y. Wong",
      "Helmut Schmid",
      "Hinrich Sch\u00fctze",
      "Simon See",
      "Yangqiu Song"
    ],
    "published": "2025-06-03T05:23:25+00:00",
    "summary": "Theory of Mind (ToM), the ability to infer mental states in others, is pivotal for human social cognition. Existing evaluations of ToM in LLMs are largely limited to English, neglecting the linguistic diversity that shapes human cognition. This limitation raises a critical question: can LLMs exhibit Multilingual Theory of Mind, which is the capacity to reason about mental states across diverse linguistic contexts? To address this gap, we present XToM, a rigorously validated multilingual benchmark that evaluates ToM across five languages and incorporates diverse, contextually rich task scenarios. Using XToM, we systematically evaluate LLMs (e.g., DeepSeek R1), revealing a pronounced dissonance: while models excel in multilingual language understanding, their ToM performance varies across languages. Our findings expose limitations in LLMs' ability to replicate human-like mentalizing across linguistic contexts."
  },
  {
    "title": "MidPO: Dual Preference Optimization for Safety and Helpfulness in Large Language Models via a Mixture of Experts Framework",
    "url": "http://arxiv.org/abs/2506.02460v1",
    "arxiv_id": "2506.02460v1",
    "authors": [
      "Yupeng Qi",
      "Ziyu Lyu",
      "Min Yang",
      "Yanlin Wang",
      "Lu Bai",
      "Lixin Cui"
    ],
    "published": "2025-06-03T05:23:09+00:00",
    "summary": "As large language models (LLMs) are increasingly applied across various domains, enhancing safety while maintaining the helpfulness of LLMs has become a critical challenge. Recent studies solve this problem through safety-constrained online preference optimization or safety-constrained offline preference optimization. However, the safety-constrained online methods often suffer from excessive safety, which might reduce helpfulness, while the safety-constrained offline methods perform poorly in adaptively balancing safety and helpfulness. To address these limitations, we propose MidPO, a \\textbf{\\underline{Mi}}xture of Experts (MoE) framework for safety-helpfulness \\textbf{\\underline{d}}ual \\textbf{\\underline{P}}reference \\textbf{\\underline{O}}ptimization. Firstly, MidPO devises single-preference enhanced direct preference optimization approach to transform the base model into two independent experts, termed safety and helpfulness experts, and fine-tunes the two independent experts for optimal safety or helpfulness performance. Secondly, to achieve an effective balance between safety and helpfulness, MidPO incorporates the two experts into the MoE framework and designs a dynamic routing mechanism to allocate contributions from each expert adaptively. We conduct quantitative and qualitative experiments on three popular datasets to demonstrate the proposed MidPO significantly outperforms state-of-the-art approaches in both safety and helpfulness. The code and models will be released."
  },
  {
    "title": "Should LLM Safety Be More Than Refusing Harmful Instructions?",
    "url": "http://arxiv.org/abs/2506.02442v1",
    "arxiv_id": "2506.02442v1",
    "authors": [
      "Utsav Maskey",
      "Mark Dras",
      "Usman Naseem"
    ],
    "published": "2025-06-03T05:00:12+00:00",
    "summary": "This paper presents a systematic evaluation of Large Language Models' (LLMs) behavior on long-tail distributed (encrypted) texts and their safety implications. We introduce a two-dimensional framework for assessing LLM safety: (1) instruction refusal-the ability to reject harmful obfuscated instructions, and (2) generation safety-the suppression of generating harmful responses. Through comprehensive experiments, we demonstrate that models that possess capabilities to decrypt ciphers may be susceptible to mismatched-generalization attacks: their safety mechanisms fail on at least one safety dimension, leading to unsafe responses or over-refusal. Based on these findings, we evaluate a number of pre-LLM and post-LLM safeguards and discuss their strengths and limitations. This work contributes to understanding the safety of LLM in long-tail text scenarios and provides directions for developing robust safety mechanisms."
  },
  {
    "title": "AnswerCarefully: A Dataset for Improving the Safety of Japanese LLM Output",
    "url": "http://arxiv.org/abs/2506.02372v1",
    "arxiv_id": "2506.02372v1",
    "authors": [
      "Hisami Suzuki",
      "Satoru Katsumata",
      "Takashi Kodama",
      "Tetsuro Takahashi",
      "Kouta Nakayama",
      "Satoshi Sekine"
    ],
    "published": "2025-06-03T02:18:59+00:00",
    "summary": "In this paper we present AnswerCarefully, a dataset for promoting the safety and appropriateness of Japanese LLM outputs. The dataset consists of 1,800 pairs of questions and reference answers, where the questions require special attention in answering. It covers a wide range of risk categories established in prior English-language datasets, but the data samples are original in that they are manually created to reflect the socio-cultural context of LLM usage in Japan. We show that using this dataset for instruction to fine-tune a Japanese LLM led to improved output safety without compromising the utility of general responses. We also report the results of a safety evaluation of 12 Japanese LLMs using this dataset as a benchmark. Finally, we describe the latest update on the dataset which provides English translations and annotations of the questions, aimed at facilitating the derivation of similar datasets in different languages and regions."
  },
  {
    "title": "RoadFormer : Local-Global Feature Fusion for Road Surface Classification in Autonomous Driving",
    "url": "http://arxiv.org/abs/2506.02358v1",
    "arxiv_id": "2506.02358v1",
    "authors": [
      "Tianze Wang",
      "Zhang Zhang",
      "Chao Sun"
    ],
    "published": "2025-06-03T01:23:19+00:00",
    "summary": "The classification of the type of road surface (RSC) aims to utilize pavement features to identify the roughness, wet and dry conditions, and material information of the road surface. Due to its ability to effectively enhance road safety and traffic management, it has received widespread attention in recent years. In autonomous driving, accurate RSC allows vehicles to better understand the road environment, adjust driving strategies, and ensure a safer and more efficient driving experience. For a long time, vision-based RSC has been favored. However, existing visual classification methods have overlooked the exploration of fine-grained classification of pavement types (such as similar pavement textures). In this work, we propose a pure vision-based fine-grained RSC method for autonomous driving scenarios, which fuses local and global feature information through the stacking of convolutional and transformer modules. We further explore the stacking strategies of local and global feature extraction modules to find the optimal feature extraction strategy. In addition, since fine-grained tasks also face the challenge of relatively large intra-class differences and relatively small inter-class differences, we propose a Foreground-Background Module (FBM) that effectively extracts fine-grained context features of the pavement, enhancing the classification ability for complex pavements. Experiments conducted on a large-scale pavement dataset containing one million samples and a simplified dataset reorganized from this dataset achieved Top-1 classification accuracies of 92.52% and 96.50%, respectively, improving by 5.69% to 12.84% compared to SOTA methods. These results demonstrate that RoadFormer outperforms existing methods in RSC tasks, providing significant progress in improving the reliability of pavement perception in autonomous driving systems."
  },
  {
    "title": "Evaluating LLM Agent Adherence to Hierarchical Safety Principles: A Lightweight Benchmark for Probing Foundational Controllability Components",
    "url": "http://arxiv.org/abs/2506.02357v1",
    "arxiv_id": "2506.02357v1",
    "authors": [
      "Ram Potham"
    ],
    "published": "2025-06-03T01:16:34+00:00",
    "summary": "Credible safety plans for advanced AI development require methods to verify agent behavior and detect potential control deficiencies early. A fundamental aspect is ensuring agents adhere to safety-critical principles, especially when these conflict with operational goals. Failure to prioritize such principles indicates a potential basic control failure. This paper introduces a lightweight, interpretable benchmark methodology using a simple grid world to evaluate an LLM agent's ability to uphold a predefined, high-level safety principle (e.g., \"never enter hazardous zones\") when faced with conflicting lower-level task instructions. We probe whether the agent reliably prioritizes the inviolable directive, testing a foundational controllability aspect of LLMs. This pilot study demonstrates the methodology's feasibility, offers preliminary insights into agent behavior under principle conflict, and discusses how such benchmarks can contribute empirical evidence for assessing controllability. We argue that evaluating adherence to hierarchical principles is a crucial early step in understanding our capacity to build governable AI systems."
  },
  {
    "title": "One Missing Piece for Open-Source Reasoning Models: A Dataset to Mitigate Cold-Starting Short CoT LLMs in RL",
    "url": "http://arxiv.org/abs/2506.02338v1",
    "arxiv_id": "2506.02338v1",
    "authors": [
      "Hyungjoo Chae",
      "Dongjin Kang",
      "Jihyuk Kim",
      "Beong-woo Kwak",
      "Sunghyun Park",
      "Haeju Park",
      "Jinyoung Yeo",
      "Moontae Lee",
      "Kyungjae Lee"
    ],
    "published": "2025-06-03T00:29:15+00:00",
    "summary": "With the release of R1, a publicly available large reasoning model (LRM), researchers commonly train new LRMs by training language models on R1's long chain-of-thought (CoT) inferences. While prior works show that LRMs' capabilities can be reproduced through direct distillation, the continued reliance on the existing models (e.g., R1) remains a critical limitation in advancing the field. As a first step toward independent LRM development, this paper explores the possibility of constructing a long CoT dataset with LLMs that are not trained for inference-time scaling. To this end, we present the Long CoT Collection, a dataset of 100K CoT rationales annotated using existing short CoT LLMs. We develop a pipeline that induces o1's novel reasoning strategies into short CoT LLMs, enabling them to think longer and introducing controllability over the thought budget to better manage the overthinking problem. Our extensive analyses validate that our dataset achieves quality comparable to--or slightly below--R1. Furthermore, our experiments demonstrate that training on our dataset not only strengthens general reasoning skills, but also provides a strong foundation for reinforcement learning--models initialized on our data achieve 2-3x larger gains with RLVR."
  },
  {
    "title": "ResearchCodeBench: Benchmarking LLMs on Implementing Novel Machine Learning Research Code",
    "url": "http://arxiv.org/abs/2506.02314v1",
    "arxiv_id": "2506.02314v1",
    "authors": [
      "Tianyu Hua",
      "Harper Hua",
      "Violet Xiang",
      "Benjamin Klieger",
      "Sang T. Truong",
      "Weixin Liang",
      "Fan-Yun Sun",
      "Nick Haber"
    ],
    "published": "2025-06-02T23:04:12+00:00",
    "summary": "Large language models (LLMs) have shown promise in transforming machine learning research, yet their capability to faithfully implement novel ideas from recent research papers-ideas unseen during pretraining-remains unclear. We introduce ResearchCodeBench, a benchmark of 212 coding challenges that evaluates LLMs' ability to translate cutting-edge ML contributions from top 2024-2025 research papers into executable code. We assessed 30+ proprietary and open-source LLMs, finding that even the best models correctly implement less than 40% of the code. We find Gemini-2.5-Pro-Preview to perform best at 37.3% success rate, with O3 (High) and O4-mini (High) following behind at 32.3% and 30.8% respectively. We present empirical findings on performance comparison, contamination, and error patterns. By providing a rigorous and community-driven evaluation platform, ResearchCodeBench enables continuous understanding and advancement of LLM-driven innovation in research code generation."
  },
  {
    "title": "SIL Allocation for Mitigation Safety Functions",
    "url": "http://arxiv.org/abs/2506.02309v1",
    "arxiv_id": "2506.02309v1",
    "authors": [
      "Hamid Jahanian"
    ],
    "published": "2025-06-02T22:57:51+00:00",
    "summary": "SIL (Safety Integrity Level) allocation plays a pivotal role in evaluating the significance of Safety Functions (SFs) within high-risk industries. The outcomes of a SIL allocation study determine the design specifications necessary to uphold the Probability of Failure on Demand (PFD) below permissible limits, thus managing risk effectively. While extensive research has focused on SIL allocation for preventive SFs, there is a noticeable gap in attention towards mitigation SFs. To address this gap, this paper discusses the shortcomings of current methods and proposes a new approach to overcome them. The principles of the proposed method are substantiated by detailed mathematical formulation and the practical application of the method is demonstrated through a case study in a road tunnel project."
  },
  {
    "title": "From Street Views to Urban Science: Discovering Road Safety Factors with Multimodal Large Language Models",
    "url": "http://arxiv.org/abs/2506.02242v1",
    "arxiv_id": "2506.02242v1",
    "authors": [
      "Yihong Tang",
      "Ao Qu",
      "Xujing Yu",
      "Weipeng Deng",
      "Jun Ma",
      "Jinhua Zhao",
      "Lijun Sun"
    ],
    "published": "2025-06-02T20:40:56+00:00",
    "summary": "Urban and transportation research has long sought to uncover statistically meaningful relationships between key variables and societal outcomes such as road safety, to generate actionable insights that guide the planning, development, and renewal of urban and transportation systems. However, traditional workflows face several key challenges: (1) reliance on human experts to propose hypotheses, which is time-consuming and prone to confirmation bias; (2) limited interpretability, particularly in deep learning approaches; and (3) underutilization of unstructured data that can encode critical urban context. Given these limitations, we propose a Multimodal Large Language Model (MLLM)-based approach for interpretable hypothesis inference, enabling the automated generation, evaluation, and refinement of hypotheses concerning urban context and road safety outcomes. Our method leverages MLLMs to craft safety-relevant questions for street view images (SVIs), extract interpretable embeddings from their responses, and apply them in regression-based statistical models. UrbanX supports iterative hypothesis testing and refinement, guided by statistical evidence such as coefficient significance, thereby enabling rigorous scientific discovery of previously overlooked correlations between urban design and safety. Experimental evaluations on Manhattan street segments demonstrate that our approach outperforms pretrained deep learning models while offering full interpretability. Beyond road safety, UrbanX can serve as a general-purpose framework for urban scientific discovery, extracting structured insights from unstructured urban data across diverse socioeconomic and environmental outcomes. This approach enhances model trustworthiness for policy applications and establishes a scalable, statistically grounded pathway for interpretable knowledge discovery in urban and transportation studies."
  },
  {
    "title": "Atomic-scale mapping of interfacial phonon modes in epitaxial YBa2Cu3O7-\u03b4 / (La,Sr)(Al,Ta)O3 thin films: The role of surface phonons",
    "url": "http://arxiv.org/abs/2506.02237v1",
    "arxiv_id": "2506.02237v1",
    "authors": [
      "Joaquin E. Reyes Gonzalez",
      "Charles Zhang",
      "Rainni K. Chen",
      "John Y. T. Wei",
      "Maureen J. Lagos"
    ],
    "published": "2025-06-02T20:28:49+00:00",
    "summary": "We investigate the behavior of phonons at the epitaxial interface between YBa2Cu3O7-{\\delta} thin film and (La,Sr)(Al,Ta)O3 substrate using vibrational electron energy loss spectroscopy. Interfacial phonon modes with different degrees of scattering localization were identified. We find evidence that surface contributions from the surrounding environment can impose additional scattering modulation into local EELS measurements at the interface. A method to remove those contributions is then used to isolate the phonon information at the interface. This work unveils interfacial phonon modes in a high-Tc cuprate superconductor, that are not accessible with traditional phonon spectroscopy techniques, and provides a method for probing interfacial phonons in complex oxide heterostructures."
  },
  {
    "title": "Second-Order-Cone Formulations of Power Flow for Topology Optimization",
    "url": "http://arxiv.org/abs/2506.02234v1",
    "arxiv_id": "2506.02234v1",
    "authors": [
      "Noah Rhodes",
      "James Luedkte",
      "Line Roald"
    ],
    "published": "2025-06-02T20:27:04+00:00",
    "summary": "Optimization problems that involve topology optimization in scenarios with large scale outages, such as post-disaster restoration or public safety power shutoff planning, are very challenging to solve. Using simple power flow representations such as DC power flow or network flow models results in low quality solutions which requires significantly higher-than-predicted load shed to become AC feasible. Recent work has shown that formulations based on the Second Order Cone (SOC) power flow formulation find very high quality solutions with low load shed, but the computational burden of these formulations remains a significant challenge. With the aim of reducing computational time while maintaining high solution quality, this work explores formulations which replace the conic constraints with a small number of linear cuts. The goal of this approach is not to find an exact power flow solution, but rather to identify good binary decisions, where the power flow can be resolved after the binary variables are fixed. We find that a simple reformulation of the Second Order Cone Optimal Power Shutoff problem can greatly improve the solution speed, but that a full linearization of the SOC voltage cone equation results in an overestimation of the amount of power that can be delivered to loads."
  },
  {
    "title": "Improving LLM-Generated Code Quality with GRPO",
    "url": "http://arxiv.org/abs/2506.02211v1",
    "arxiv_id": "2506.02211v1",
    "authors": [
      "Maxime Robeyns",
      "Laurence Aitchison"
    ],
    "published": "2025-06-02T19:50:16+00:00",
    "summary": "Large Language Models (LLMs) are gaining widespread use for code generation. Recent training procedures use execution feedback as a reward signal, typically focusing on the functional correctness of the code, using unit test pass rate as a reward signal. However, this reward signal fails to capture notions of maintainability, quality and safety of the code produced. We address this under-explored area and develop a comprehensive library to quantify various aspects of code quality, and use it as a reward in GRPO. We find GRPO increases code quality according to this measure, which is confirmed by expert, blinded human annotators."
  },
  {
    "title": "Spegion: Implicit and Non-Lexical Regions with Sized Allocations",
    "url": "http://arxiv.org/abs/2506.02182v1",
    "arxiv_id": "2506.02182v1",
    "authors": [
      "Jack Hughes",
      "Michael Vollmer",
      "Mark Batty"
    ],
    "published": "2025-06-02T19:11:46+00:00",
    "summary": "Region based memory management is a powerful tool designed with the goal of ensuring memory safety statically. The region calculus of Tofte and Talpin is a well known example of a region based system, which uses regions to manage memory in a stack-like fashion. However, the region calculus is lexically scoped and requires explicit annotation of memory regions, which can be cumbersome for the programmer. Other systems have addressed non-lexical regions, but these approaches typically require the use of a substructural type system to track the lifetimes of regions. We present Spegion, a language with implicit non-lexical regions, which provides these same memory safety guarantees for programs that go beyond using memory allocation in a stack-like manner. We are able to achieve this with a concise syntax, and without the use of substructural types, relying instead on an effect system to enforce constraints on region allocation and deallocation. These regions may be divided into sub-regions, i.e., Splittable rEgions, allowing fine grained control over memory allocation. Furthermore, Spegion permits sized allocations, where each value has an associated size which is used to ensure that regions are not over-allocated into. We present a type system for Spegion and prove it is type safe with respect to a small-step operational semantics."
  },
  {
    "title": "Act Only When It Pays: Efficient Reinforcement Learning for LLM Reasoning via Selective Rollouts",
    "url": "http://arxiv.org/abs/2506.02177v1",
    "arxiv_id": "2506.02177v1",
    "authors": [
      "Haizhong Zheng",
      "Yang Zhou",
      "Brian R. Bartoldson",
      "Bhavya Kailkhura",
      "Fan Lai",
      "Jiawei Zhao",
      "Beidi Chen"
    ],
    "published": "2025-06-02T19:03:00+00:00",
    "summary": "Reinforcement learning, such as PPO and GRPO, has powered recent breakthroughs in LLM reasoning. Scaling rollout to sample more prompts enables models to selectively use higher-quality data for training, which can stabilize RL training and improve model performance. However, this comes at the cost of significant computational overhead. In this paper, we show that a substantial portion of this overhead can be avoided by skipping uninformative prompts before rollout. Our analysis of reward dynamics reveals a strong temporal consistency in prompt value: prompts that are uninformative in one epoch of training are likely to remain uninformative in future epochs. Based on these insights, we propose GRESO (GRPO with Efficient Selective Rollout), an online, lightweight pre-rollout filtering algorithm that predicts and skips uninformative prompts using reward training dynamics. By evaluating GRESO on a broad range of math reasoning benchmarks and models, such as Qwen2.5-Math-1.5B, DeepSeek-R1-Distill-Qwen-1.5B, and Qwen2.5-Math-7B, we show that GRESO achieves up to 2.4x wall-clock time speedup in rollout and up to 2.0x speedup in total training time without accuracy degradation."
  },
  {
    "title": "Fire360: A Benchmark for Robust Perception and Episodic Memory in Degraded 360-Degree Firefighting Videos",
    "url": "http://arxiv.org/abs/2506.02167v1",
    "arxiv_id": "2506.02167v1",
    "authors": [
      "Aditi Tiwari",
      "Farzaneh Masoud",
      "Dac Trong Nguyen",
      "Jill Kraft",
      "Heng Ji",
      "Klara Nahrstedt"
    ],
    "published": "2025-06-02T18:45:56+00:00",
    "summary": "Modern AI systems struggle most in environments where reliability is critical - scenes with smoke, poor visibility, and structural deformation. Each year, tens of thousands of firefighters are injured on duty, often due to breakdowns in situational perception. We introduce Fire360, a benchmark for evaluating perception and reasoning in safety-critical firefighting scenarios. The dataset includes 228 360-degree videos from professional training sessions under diverse conditions (e.g., low light, thermal distortion), annotated with action segments, object locations, and degradation metadata. Fire360 supports five tasks: Visual Question Answering, Temporal Action Captioning, Object Localization, Safety-Critical Reasoning, and Transformed Object Retrieval (TOR). TOR tests whether models can match pristine exemplars to fire-damaged counterparts in unpaired scenes, evaluating transformation-invariant recognition. While human experts achieve 83.5% on TOR, models like GPT-4o lag significantly, exposing failures in reasoning under degradation. By releasing Fire360 and its evaluation suite, we aim to advance models that not only see, but also remember, reason, and act under uncertainty. The dataset is available at: https://uofi.box.com/v/fire360dataset."
  },
  {
    "title": "Knowledge or Reasoning? A Close Look at How LLMs Think Across Domains",
    "url": "http://arxiv.org/abs/2506.02126v1",
    "arxiv_id": "2506.02126v1",
    "authors": [
      "Juncheng Wu",
      "Sheng Liu",
      "Haoqin Tu",
      "Hang Yu",
      "Xiaoke Huang",
      "James Zou",
      "Cihang Xie",
      "Yuyin Zhou"
    ],
    "published": "2025-06-02T18:01:00+00:00",
    "summary": "Recent advances in reasoning-enhanced Large Language Models such as OpenAI-o1/3 and DeepSeek-R1 have significantly improved performance on complex tasks. However, the quality and transparency of their internal reasoning processes remain underexplored. This work moves beyond the final-answer accuracy and investigates step-by-step reasoning in the medical and mathematical domains by explicitly decomposing the thinking trajectories into two parts: knowledge and reasoning. Specifically, we introduce a fine-grained evaluation framework that judges: (1) the correctness of knowledge used (measured by Knowledge Index (KI)) and (2) the quality of reasoning (measured by Information Gain (InfoGain)). Using this framework, we study R1-distilled and base Qwen models trained with supervised fine-tuning (SFT) and/or reinforcement learning (RL) in the medical and math domains. Three intriguing findings emerge: (1) The general reasoning abilities in R1-distilled models do not transfer effectively to the medical domain through either SFT or RL. (2) SFT raises final-answer accuracy in both domains, but often at the cost of reasoning quality: InfoGain drops by 38.9% on average compared with untrained models; In the medical domain, however, SFT remains crucial because domain knowledge is indispensable. (3) RL enhances medical reasoning by pruning inaccurate or irrelevant knowledge from reasoning paths, thereby improving both reasoning accuracy and knowledge correctness."
  },
  {
    "title": "RewardBench 2: Advancing Reward Model Evaluation",
    "url": "http://arxiv.org/abs/2506.01937v1",
    "arxiv_id": "2506.01937v1",
    "authors": [
      "Saumya Malik",
      "Valentina Pyatkin",
      "Sander Land",
      "Jacob Morrison",
      "Noah A. Smith",
      "Hannaneh Hajishirzi",
      "Nathan Lambert"
    ],
    "published": "2025-06-02T17:54:04+00:00",
    "summary": "Reward models are used throughout the post-training of language models to capture nuanced signals from preference data and provide a training target for optimization across instruction following, reasoning, safety, and more domains. The community has begun establishing best practices for evaluating reward models, from the development of benchmarks that test capabilities in specific skill areas to others that test agreement with human preferences. At the same time, progress in evaluation has not been mirrored by the effectiveness of reward models in downstream tasks -- simpler direct alignment algorithms are reported to work better in many cases. This paper introduces RewardBench 2, a new multi-skill reward modeling benchmark designed to bring new, challenging data for accuracy-based reward model evaluation -- models score about 20 points on average lower on RewardBench 2 compared to the first RewardBench -- while being highly correlated with downstream performance. Compared to most other benchmarks, RewardBench 2 sources new human prompts instead of existing prompts from downstream evaluations, facilitating more rigorous evaluation practices. In this paper, we describe our benchmark construction process and report how existing models perform on it, while quantifying how performance on the benchmark correlates with downstream use of the models in both inference-time scaling algorithms, like best-of-N sampling, and RLHF training algorithms like proximal policy optimization."
  },
  {
    "title": "Trojan Horse Hunt in Time Series Forecasting for Space Operations",
    "url": "http://arxiv.org/abs/2506.01849v1",
    "arxiv_id": "2506.01849v1",
    "authors": [
      "Krzysztof Kotowski",
      "Ramez Shendy",
      "Jakub Nalepa",
      "Przemys\u0142aw Biecek",
      "Piotr Wilczy\u0144ski",
      "Agata Kaczmarek",
      "Dawid P\u0142udowski",
      "Artur Janicki",
      "Evridiki Ntagiou"
    ],
    "published": "2025-06-02T16:38:16+00:00",
    "summary": "This competition hosted on Kaggle (https://www.kaggle.com/competitions/trojan-horse-hunt-in-space) is the first part of a series of follow-up competitions and hackathons related to the \"Assurance for Space Domain AI Applications\" project funded by the European Space Agency (https://assurance-ai.space-codev.org/). The competition idea is based on one of the real-life AI security threats identified within the project -- the adversarial poisoning of continuously fine-tuned satellite telemetry forecasting models. The task is to develop methods for finding and reconstructing triggers (trojans) in advanced models for satellite telemetry forecasting used in safety-critical space operations. Participants are provided with 1) a large public dataset of real-life multivariate satellite telemetry (without triggers), 2) a reference model trained on the clean data, 3) a set of poisoned neural hierarchical interpolation (N-HiTS) models for time series forecasting trained on the dataset with injected triggers, and 4) Jupyter notebook with the training pipeline and baseline algorithm (the latter will be published in the last month of the competition). The main task of the competition is to reconstruct a set of 45 triggers (i.e., short multivariate time series segments) injected into the training data of the corresponding set of 45 poisoned models. The exact characteristics (i.e., shape, amplitude, and duration) of these triggers must be identified by participants. The popular Neural Cleanse method is adopted as a baseline, but it is not designed for time series analysis and new approaches are necessary for the task. The impact of the competition is not limited to the space domain, but also to many other safety-critical applications of advanced time series analysis where model poisoning may lead to serious consequences."
  },
  {
    "title": "Your Interface, Your Control: Adapting Takeover Requests for Seamless Handover in Semi-Autonomous Vehicles",
    "url": "http://arxiv.org/abs/2506.01836v1",
    "arxiv_id": "2506.01836v1",
    "authors": [
      "Amr Gomaa",
      "Simon Engel",
      "Elena Meiser",
      "Abdulrahman Mohamed Selim",
      "Tobias Jungbluth",
      "Aeneas Leon Sommer",
      "Sarah Kohlmann",
      "Michael Barz",
      "Maurice Rekrut",
      "Michael Feld",
      "Daniel Sonntag",
      "Antonio Kr\u00fcger"
    ],
    "published": "2025-06-02T16:26:20+00:00",
    "summary": "With the automotive industry transitioning towards conditionally automated driving, takeover warning systems are crucial for ensuring safe collaborative driving between users and semi-automated vehicles. However, previous work has focused on static warning systems that do not accommodate different driver states. Therefore, we propose an adaptive takeover warning system that is personalised to drivers, enhancing their experience and safety. We conducted two user studies investigating semi-autonomous driving scenarios in rural and urban environments while participants performed non-driving-related tasks such as text entry and visual search. We investigated the effects of varying time budgets and head-up versus head-down displays for takeover requests on drivers' situational awareness and mental state. Through our statistical and clustering analyses, we propose strategies for designing adaptable takeover systems, e.g., using longer time budgets and head-up displays for non-hazardous takeover events in high-complexity environments while using shorter time budgets and head-down displays for hazardous events in low-complexity environments."
  },
  {
    "title": "Analysis of LLM Bias (Chinese Propaganda & Anti-US Sentiment) in DeepSeek-R1 vs. ChatGPT o3-mini-high",
    "url": "http://arxiv.org/abs/2506.01814v1",
    "arxiv_id": "2506.01814v1",
    "authors": [
      "PeiHsuan Huang",
      "ZihWei Lin",
      "Simon Imbot",
      "WenCheng Fu",
      "Ethan Tu"
    ],
    "published": "2025-06-02T15:54:06+00:00",
    "summary": "Large language models (LLMs) increasingly shape public understanding and civic decisions, yet their ideological neutrality is a growing concern. While existing research has explored various forms of LLM bias, a direct, cross-lingual comparison of models with differing geopolitical alignments-specifically a PRC-system model versus a non-PRC counterpart-has been lacking. This study addresses this gap by systematically evaluating DeepSeek-R1 (PRC-aligned) against ChatGPT o3-mini-high (non-PRC) for Chinese-state propaganda and anti-U.S. sentiment. We developed a novel corpus of 1,200 de-contextualized, reasoning-oriented questions derived from Chinese-language news, presented in Simplified Chinese, Traditional Chinese, and English. Answers from both models (7,200 total) were assessed using a hybrid evaluation pipeline combining rubric-guided GPT-4o scoring with human annotation. Our findings reveal significant model-level and language-dependent biases. DeepSeek-R1 consistently exhibited substantially higher proportions of both propaganda and anti-U.S. bias compared to ChatGPT o3-mini-high, which remained largely free of anti-U.S. sentiment and showed lower propaganda levels. For DeepSeek-R1, Simplified Chinese queries elicited the highest bias rates; these diminished in Traditional Chinese and were nearly absent in English. Notably, DeepSeek-R1 occasionally responded in Simplified Chinese to Traditional Chinese queries and amplified existing PRC-aligned terms in its Chinese answers, demonstrating an \"invisible loudspeaker\" effect. Furthermore, such biases were not confined to overtly political topics but also permeated cultural and lifestyle content, particularly in DeepSeek-R1."
  },
  {
    "title": "Human-Centric Evaluation for Foundation Models",
    "url": "http://arxiv.org/abs/2506.01793v1",
    "arxiv_id": "2506.01793v1",
    "authors": [
      "Yijin Guo",
      "Kaiyuan Ji",
      "Xiaorong Zhu",
      "Junying Wang",
      "Farong Wen",
      "Chunyi Li",
      "Zicheng Zhang",
      "Guangtao Zhai"
    ],
    "published": "2025-06-02T15:33:29+00:00",
    "summary": "Currently, nearly all evaluations of foundation models focus on objective metrics, emphasizing quiz performance to define model capabilities. While this model-centric approach enables rapid performance assessment, it fails to reflect authentic human experiences. To address this gap, we propose a Human-Centric subjective Evaluation (HCE) framework, focusing on three core dimensions: problem-solving ability, information quality, and interaction experience. Through experiments involving Deepseek R1, OpenAI o3 mini, Grok 3, and Gemini 2.5, we conduct over 540 participant-driven evaluations, where humans and models collaborate on open-ended research tasks, yielding a comprehensive subjective dataset. This dataset captures diverse user feedback across multiple disciplines, revealing distinct model strengths and adaptability. Our findings highlight Grok 3's superior performance, followed by Deepseek R1 and Gemini 2.5, with OpenAI o3 mini lagging behind. By offering a novel framework and a rich dataset, this study not only enhances subjective evaluation methodologies but also lays the foundation for standardized, automated assessments, advancing LLM development for research and practical scenarios. Our dataset link is https://github.com/yijinguo/Human-Centric-Evaluation."
  },
  {
    "title": "Open CaptchaWorld: A Comprehensive Web-based Platform for Testing and Benchmarking Multimodal LLM Agents",
    "url": "http://arxiv.org/abs/2505.24878v1",
    "arxiv_id": "2505.24878v1",
    "authors": [
      "Yaxin Luo",
      "Zhaoyi Li",
      "Jiacheng Liu",
      "Jiacheng Cui",
      "Xiaohan Zhao",
      "Zhiqiang Shen"
    ],
    "published": "2025-05-30T17:59:55+00:00",
    "summary": "CAPTCHAs have been a critical bottleneck for deploying web agents in real-world applications, often blocking them from completing end-to-end automation tasks. While modern multimodal LLM agents have demonstrated impressive performance in static perception tasks, their ability to handle interactive, multi-step reasoning challenges like CAPTCHAs is largely untested. To address this gap, we introduce Open CaptchaWorld, the first web-based benchmark and platform specifically designed to evaluate the visual reasoning and interaction capabilities of MLLM-powered agents through diverse and dynamic CAPTCHA puzzles. Our benchmark spans 20 modern CAPTCHA types, totaling 225 CAPTCHAs, annotated with a new metric we propose: CAPTCHA Reasoning Depth, which quantifies the number of cognitive and motor steps required to solve each puzzle. Experimental results show that humans consistently achieve near-perfect scores, state-of-the-art MLLM agents struggle significantly, with success rates at most 40.0% by Browser-Use Openai-o3, far below human-level performance, 93.3%. This highlights Open CaptchaWorld as a vital benchmark for diagnosing the limits of current multimodal agents and guiding the development of more robust multimodal reasoning systems. Code and Data are available at this https URL."
  },
  {
    "title": "Harnessing Negative Signals: Reinforcement Distillation from Teacher Data for LLM Reasoning",
    "url": "http://arxiv.org/abs/2505.24850v1",
    "arxiv_id": "2505.24850v1",
    "authors": [
      "Shuyao Xu",
      "Cheng Peng",
      "Jiangxuan Long",
      "Weidi Xu",
      "Wei Chu",
      "Yuan Qi"
    ],
    "published": "2025-05-30T17:47:17+00:00",
    "summary": "Recent advances in model distillation demonstrate that data from advanced reasoning models (e.g., DeepSeek-R1, OpenAI's o1) can effectively transfer complex reasoning abilities to smaller, efficient student models. However, standard practices employ rejection sampling, discarding incorrect reasoning examples -- valuable, yet often underutilized data. This paper addresses the critical question: How can both positive and negative distilled reasoning traces be effectively leveraged to maximize LLM reasoning performance in an offline setting? To this end, We propose Reinforcement Distillation (REDI), a two-stage framework. Stage 1 learns from positive traces via Supervised Fine-Tuning (SFT). Stage 2 further refines the model using both positive and negative traces through our proposed REDI objective. This novel objective is a simple, reference-free loss function that outperforms established methods like DPO and SimPO in this distillation context. Our empirical evaluations demonstrate REDI's superiority over baseline Rejection Sampling SFT or SFT combined with DPO/SimPO on mathematical reasoning tasks. Notably, the Qwen-REDI-1.5B model, post-trained on just 131k positive and negative examples from the open Open-R1 dataset, achieves an 83.1% score on MATH-500 (pass@1). Its performance matches or surpasses that of DeepSeek-R1-Distill-Qwen-1.5B (a model post-trained on 800k proprietary data) across various mathematical reasoning benchmarks, establishing a new state-of-the-art for 1.5B models post-trained offline with openly available data."
  },
  {
    "title": "RealDrive: Retrieval-Augmented Driving with Diffusion Models",
    "url": "http://arxiv.org/abs/2505.24808v1",
    "arxiv_id": "2505.24808v1",
    "authors": [
      "Wenhao Ding",
      "Sushant Veer",
      "Yuxiao Chen",
      "Yulong Cao",
      "Chaowei Xiao",
      "Marco Pavone"
    ],
    "published": "2025-05-30T17:15:03+00:00",
    "summary": "Learning-based planners generate natural human-like driving behaviors by learning to reason about nuanced interactions from data, overcoming the rigid behaviors that arise from rule-based planners. Nonetheless, data-driven approaches often struggle with rare, safety-critical scenarios and offer limited controllability over the generated trajectories. To address these challenges, we propose RealDrive, a Retrieval-Augmented Generation (RAG) framework that initializes a diffusion-based planning policy by retrieving the most relevant expert demonstrations from the training dataset. By interpolating between current observations and retrieved examples through a denoising process, our approach enables fine-grained control and safe behavior across diverse scenarios, leveraging the strong prior provided by the retrieved scenario. Another key insight we produce is that a task-relevant retrieval model trained with planning-based objectives results in superior planning performance in our framework compared to a task-agnostic retriever. Experimental results demonstrate improved generalization to long-tail events and enhanced trajectory diversity compared to standard learning-based planners -- we observe a 40% reduction in collision rate on the Waymo Open Motion dataset with RAG."
  },
  {
    "title": "DiG-Net: Enhancing Quality of Life through Hyper-Range Dynamic Gesture Recognition in Assistive Robotics",
    "url": "http://arxiv.org/abs/2505.24786v1",
    "arxiv_id": "2505.24786v1",
    "authors": [
      "Eran Bamani Beeri",
      "Eden Nissinman",
      "Avishai Sintov"
    ],
    "published": "2025-05-30T16:47:44+00:00",
    "summary": "Dynamic hand gestures play a pivotal role in assistive human-robot interaction (HRI), facilitating intuitive, non-verbal communication, particularly for individuals with mobility constraints or those operating robots remotely. Current gesture recognition methods are mostly limited to short-range interactions, reducing their utility in scenarios demanding robust assistive communication from afar. In this paper, we introduce a novel approach designed specifically for assistive robotics, enabling dynamic gesture recognition at extended distances of up to 30 meters, thereby significantly improving accessibility and quality of life. Our proposed Distance-aware Gesture Network (DiG-Net) effectively combines Depth-Conditioned Deformable Alignment (DADA) blocks with Spatio-Temporal Graph modules, enabling robust processing and classification of gesture sequences captured under challenging conditions, including significant physical attenuation, reduced resolution, and dynamic gesture variations commonly experienced in real-world assistive environments. We further introduce the Radiometric Spatio-Temporal Depth Attenuation Loss (RSTDAL), shown to enhance learning and strengthen model robustness across varying distances. Our model demonstrates significant performance improvement over state-of-the-art gesture recognition frameworks, achieving a recognition accuracy of 97.3% on a diverse dataset with challenging hyper-range gestures. By effectively interpreting gestures from considerable distances, DiG-Net significantly enhances the usability of assistive robots in home healthcare, industrial safety, and remote assistance scenarios, enabling seamless and intuitive interactions for users regardless of physical limitations"
  },
  {
    "title": "Neural Network-based Universal Formulas for Control",
    "url": "http://arxiv.org/abs/2505.24744v1",
    "arxiv_id": "2505.24744v1",
    "authors": [
      "Pol Mestres",
      "Jorge Cort\u00e9s",
      "Eduardo D. Sontag"
    ],
    "published": "2025-05-30T16:02:05+00:00",
    "summary": "We study the problem of designing a controller that satisfies an arbitrary number of affine inequalities at every point in the state space. This is motivated by the use of guardrails in autonomous systems. Indeed, a variety of key control objectives, such as stability, safety, and input saturation, are guaranteed by closed-loop systems whose controllers satisfy such inequalities. Many works in the literature design such controllers as the solution to a state-dependent quadratic program (QP) whose constraints are precisely the inequalities. When the input dimension and number of constraints are high, computing a solution of this QP in real time can become computationally burdensome. Additionally, the solution of such optimization problems is not smooth in general, which can degrade the performance of the system. This paper provides a novel method to design a smooth controller that satisfies an arbitrary number of affine constraints. This why we refer to it as a universal formula for control. The controller is given at every state as the minimizer of a strictly convex function. To avoid computing the minimizer of such function in real time, we introduce a method based on neural networks (NN) to approximate the controller. Remarkably, this NN can be used to solve the controller design problem for any task with less than a fixed input dimension and number of affine constraints, and is completely independent of the state dimension. Additionally, we show that the NN-based controller only needs to be trained with datapoints from a compact set in the state space, which significantly simplifies the training process. Various simulations showcase the performance of the proposed solution."
  },
  {
    "title": "Multi-Domain ABSA Conversation Dataset Generation via LLMs for Real-World Evaluation and Model Comparison",
    "url": "http://arxiv.org/abs/2505.24701v1",
    "arxiv_id": "2505.24701v1",
    "authors": [
      "Tejul Pandit",
      "Meet Raval",
      "Dhvani Upadhyay"
    ],
    "published": "2025-05-30T15:24:17+00:00",
    "summary": "Aspect-Based Sentiment Analysis (ABSA) offers granular insights into opinions but often suffers from the scarcity of diverse, labeled datasets that reflect real-world conversational nuances. This paper presents an approach for generating synthetic ABSA data using Large Language Models (LLMs) to address this gap. We detail the generation process aimed at producing data with consistent topic and sentiment distributions across multiple domains using GPT-4o. The quality and utility of the generated data were evaluated by assessing the performance of three state-of-the-art LLMs (Gemini 1.5 Pro, Claude 3.5 Sonnet, and DeepSeek-R1) on topic and sentiment classification tasks. Our results demonstrate the effectiveness of the synthetic data, revealing distinct performance trade-offs among the models: DeepSeekR1 showed higher precision, Gemini 1.5 Pro and Claude 3.5 Sonnet exhibited strong recall, and Gemini 1.5 Pro offered significantly faster inference. We conclude that LLM-based synthetic data generation is a viable and flexible method for creating valuable ABSA resources, facilitating research and model evaluation without reliance on limited or inaccessible real-world labeled data."
  },
  {
    "title": "TRIDENT: Enhancing Large Language Model Safety with Tri-Dimensional Diversified Red-Teaming Data Synthesis",
    "url": "http://arxiv.org/abs/2505.24672v1",
    "arxiv_id": "2505.24672v1",
    "authors": [
      "Xiaorui Wu",
      "Xiaofeng Mao",
      "Fei Li",
      "Xin Zhang",
      "Xuanhong Li",
      "Chong Teng",
      "Donghong Ji",
      "Zhuang Li"
    ],
    "published": "2025-05-30T15:02:21+00:00",
    "summary": "Large Language Models (LLMs) excel in various natural language processing tasks but remain vulnerable to generating harmful content or being exploited for malicious purposes. Although safety alignment datasets have been introduced to mitigate such risks through supervised fine-tuning (SFT), these datasets often lack comprehensive risk coverage. Most existing datasets focus primarily on lexical diversity while neglecting other critical dimensions. To address this limitation, we propose a novel analysis framework to systematically measure the risk coverage of alignment datasets across three essential dimensions: Lexical Diversity, Malicious Intent, and Jailbreak Tactics. We further introduce TRIDENT, an automated pipeline that leverages persona-based, zero-shot LLM generation to produce diverse and comprehensive instructions spanning these dimensions. Each harmful instruction is paired with an ethically aligned response, resulting in two datasets: TRIDENT-Core, comprising 26,311 examples, and TRIDENT-Edge, with 18,773 examples. Fine-tuning Llama 3.1-8B on TRIDENT-Edge demonstrates substantial improvements, achieving an average 14.29% reduction in Harm Score, and a 20% decrease in Attack Success Rate compared to the best-performing baseline model fine-tuned on the WildBreak dataset."
  },
  {
    "title": "Benchmarking Large Language Models for Cryptanalysis and Mismatched-Generalization",
    "url": "http://arxiv.org/abs/2505.24621v1",
    "arxiv_id": "2505.24621v1",
    "authors": [
      "Utsav Maskey",
      "Chencheng Zhu",
      "Usman Naseem"
    ],
    "published": "2025-05-30T14:12:07+00:00",
    "summary": "Recent advancements in Large Language Models (LLMs) have transformed natural language understanding and generation, leading to extensive benchmarking across diverse tasks. However, cryptanalysis a critical area for data security and encryption has not yet been thoroughly explored in LLM evaluations. To address this gap, we evaluate cryptanalytic potential of state of the art LLMs on encrypted texts generated using a range of cryptographic algorithms. We introduce a novel benchmark dataset comprising diverse plain texts spanning various domains, lengths, writing styles, and topics paired with their encrypted versions. Using zero-shot and few shot settings, we assess multiple LLMs for decryption accuracy and semantic comprehension across different encryption schemes. Our findings reveal key insights into the strengths and limitations of LLMs in side-channel communication while raising concerns about their susceptibility to jailbreaking attacks. This research highlights the dual-use nature of LLMs in security contexts and contributes to the ongoing discussion on AI safety and security."
  },
  {
    "title": "AMIA: Automatic Masking and Joint Intention Analysis Makes LVLMs Robust Jailbreak Defenders",
    "url": "http://arxiv.org/abs/2505.24519v1",
    "arxiv_id": "2505.24519v1",
    "authors": [
      "Yuqi Zhang",
      "Yuchun Miao",
      "Zuchao Li",
      "Liang Ding"
    ],
    "published": "2025-05-30T12:30:50+00:00",
    "summary": "We introduce AMIA, a lightweight, inference-only defense for Large Vision-Language Models (LVLMs) that (1) Automatically Masks a small set of text-irrelevant image patches to disrupt adversarial perturbations, and (2) conducts joint Intention Analysis to uncover and mitigate hidden harmful intents before response generation. Without any retraining, AMIA improves defense success rates across diverse LVLMs and jailbreak benchmarks from an average of 52.4% to 81.7%, preserves general utility with only a 2% average accuracy drop, and incurs only modest inference overhead. Ablation confirms both masking and intention analysis are essential for a robust safety-utility trade-off."
  },
  {
    "title": "Can Slow-thinking LLMs Reason Over Time? Empirical Studies in Time Series Forecasting",
    "url": "http://arxiv.org/abs/2505.24511v1",
    "arxiv_id": "2505.24511v1",
    "authors": [
      "Jiahao Wang",
      "Mingyue Cheng",
      "Qi Liu"
    ],
    "published": "2025-05-30T12:19:02+00:00",
    "summary": "Time series forecasting (TSF) is a fundamental and widely studied task, spanning methods from classical statistical approaches to modern deep learning and multimodal language modeling. Despite their effectiveness, these methods often follow a fast thinking paradigm emphasizing pattern extraction and direct value mapping, while overlooking explicit reasoning over temporal dynamics and contextual dependencies. Meanwhile, emerging slow-thinking LLMs (e.g., ChatGPT-o1, DeepSeek-R1) have demonstrated impressive multi-step reasoning capabilities across diverse domains, suggesting a new opportunity for reframing TSF as a structured reasoning task. This motivates a key question: can slow-thinking LLMs effectively reason over temporal patterns to support time series forecasting, even in zero-shot manner? To investigate this, in this paper, we propose TimeReasoner, an extensive empirical study that formulates TSF as a conditional reasoning task. We design a series of prompting strategies to elicit inference-time reasoning from pretrained slow-thinking LLMs and evaluate their performance across diverse TSF benchmarks. Our findings reveal that slow-thinking LLMs exhibit non-trivial zero-shot forecasting capabilities, especially in capturing high-level trends and contextual shifts. While preliminary, our study surfaces important insights into the reasoning behaviors of LLMs in temporal domains highlighting both their potential and limitations. We hope this work catalyzes further research into reasoning-based forecasting paradigms and paves the way toward more interpretable and generalizable TSF frameworks."
  },
  {
    "title": "How can AI reduce wrist injuries in the workplace?",
    "url": "http://arxiv.org/abs/2505.24510v1",
    "arxiv_id": "2505.24510v1",
    "authors": [
      "Roberto F. Pitzalis",
      "Nicholas Cartocci",
      "Christian Di Natali",
      "Darwin G. Caldwell",
      "Giovanni Berselli",
      "Jes\u00fas Ortiz"
    ],
    "published": "2025-05-30T12:18:05+00:00",
    "summary": "This paper explores the development of a control and sensor strategy for an industrial wearable wrist exoskeleton by classifying and predicting workers' actions. The study evaluates the correlation between exerted force and effort intensity, along with sensor strategy optimization, for designing purposes. Using data from six healthy subjects in a manufacturing plant, this paper presents EMG-based models for wrist motion classification and force prediction. Wrist motion recognition is achieved through a pattern recognition algorithm developed with surface EMG data from an 8-channel EMG sensor (Myo Armband); while a force regression model uses wrist and hand force measurements from a commercial handheld dynamometer (Vernier GoDirect Hand Dynamometer). This control strategy forms the foundation for a streamlined exoskeleton architecture designed for industrial applications, focusing on simplicity, reduced costs, and minimal sensor use while ensuring reliable and effective assistance."
  },
  {
    "title": "How can AI reduce fall injuries in the workplace?",
    "url": "http://arxiv.org/abs/2505.24507v1",
    "arxiv_id": "2505.24507v1",
    "authors": [
      "Nicholas Cartocci",
      "Antonios E. Gkikakis",
      "Roberto F. Pitzalis",
      "Fabio Pera",
      "Maria Teresa Settino",
      "Darwin G. Caldwell",
      "Jes\u00fas Ortiz"
    ],
    "published": "2025-05-30T12:09:38+00:00",
    "summary": "Fall-caused injuries are common in all types of work environments, including offices. They are the main cause of absences longer than three days, especially for small and medium-sized businesses (SMEs). However, data, data amount, data heterogeneity, and stringent processing time constraints continue to pose challenges to real-time fall detection. This work proposes a new approach based on a recurrent neural network (RNN) for Fall Detection and a Kolmogorov-Arnold Network (KAN) to estimate the time of impact of the fall. The approach is tested on SisFall, a dataset consisting of 2706 Activities of Daily Living (ADLs) and 1798 falls recorded by three sensors. The results show that the proposed approach achieves an average TPR of 82.6% and TNR of 98.4% for fall sequences and 94.4% in ADL. Besides, the Root Mean Squared Error of the estimated time of impact is approximately 160ms."
  },
  {
    "title": "TimeHC-RL: Temporal-aware Hierarchical Cognitive Reinforcement Learning for Enhancing LLMs' Social Intelligence",
    "url": "http://arxiv.org/abs/2505.24500v1",
    "arxiv_id": "2505.24500v1",
    "authors": [
      "Guiyang Hou",
      "Xing Gao",
      "Yuchuan Wu",
      "Xiang Huang",
      "Wenqi Zhang",
      "Zhe Zheng",
      "Yongliang Shen",
      "Jialu Du",
      "Fei Huang",
      "Yongbin Li",
      "Weiming Lu"
    ],
    "published": "2025-05-30T12:01:06+00:00",
    "summary": "Recently, Large Language Models (LLMs) have made significant progress in IQ-related domains that require careful thinking, such as mathematics and coding. However, enhancing LLMs' cognitive development in social domains, particularly from a post-training perspective, remains underexplored. Recognizing that the social world follows a distinct timeline and requires a richer blend of cognitive modes (from intuitive reactions (System 1) and surface-level thinking to deliberate thinking (System 2)) than mathematics, which primarily relies on System 2 cognition (careful, step-by-step reasoning), we introduce Temporal-aware Hierarchical Cognitive Reinforcement Learning (TimeHC-RL) for enhancing LLMs' social intelligence. In our experiments, we systematically explore improving LLMs' social intelligence and validate the effectiveness of the TimeHC-RL method, through five other post-training paradigms and two test-time intervention paradigms on eight datasets with diverse data patterns. Experimental results reveal the superiority of our proposed TimeHC-RL method compared to the widely adopted System 2 RL method. It gives the 7B backbone model wings, enabling it to rival the performance of advanced models like DeepSeek-R1 and OpenAI-O3. Additionally, the systematic exploration from post-training and test-time interventions perspectives to improve LLMs' social intelligence has uncovered several valuable insights."
  },
  {
    "title": "Real-time Fall Prevention system for the Next-generation of Workers",
    "url": "http://arxiv.org/abs/2505.24487v1",
    "arxiv_id": "2505.24487v1",
    "authors": [
      "Nicholas Cartocci",
      "Antonios E. Gkikakis",
      "Darwin G. Caldwell",
      "Jes\u00fas Ortiz"
    ],
    "published": "2025-05-30T11:41:16+00:00",
    "summary": "Developing a general-purpose wearable real-time fall-detection system is still a challenging task, especially for healthy and strong subjects, such as industrial workers that work in harsh environments. In this work, we present a hybrid approach for fall detection and prevention, which uses the dynamic model of an inverted pendulum to generate simulations of falling that are then fed to a deep learning framework. The output is a signal to activate a fall mitigation mechanism when the subject is at risk of harm. The advantage of this approach is that abstracted models can be used to efficiently generate training data for thousands of different subjects with different falling initial conditions, something that is practically impossible with real experiments. This approach is suitable for a specific type of fall, where the subjects fall without changing their initial configuration significantly, and it is the first step toward a general-purpose wearable device, with the aim of reducing fall-associated injuries in industrial environments, which can improve the safety of workers."
  },
  {
    "title": "Evaluating Gemini in an arena for learning",
    "url": "http://arxiv.org/abs/2505.24477v1",
    "arxiv_id": "2505.24477v1",
    "authors": [
      "LearnLM Team",
      "Abhinit Modi",
      "Aditya Srikanth Veerubhotla",
      "Aliya Rysbek",
      "Andrea Huber",
      "Ankit Anand",
      "Avishkar Bhoopchand",
      "Brett Wiltshire",
      "Daniel Gillick",
      "Daniel Kasenberg",
      "Eleni Sgouritsa",
      "Gal Elidan",
      "Hengrui Liu",
      "Holger Winnemoeller",
      "Irina Jurenka",
      "James Cohan",
      "Jennifer She",
      "Julia Wilkowski",
      "Kaiz Alarakyia",
      "Kevin R. McKee",
      "Komal Singh",
      "Lisa Wang",
      "Markus Kunesch",
      "Miruna P\u00eeslar",
      "Niv Efron",
      "Parsa Mahmoudieh",
      "Pierre-Alexandre Kamienny",
      "Sara Wiltberger",
      "Shakir Mohamed",
      "Shashank Agarwal",
      "Shubham Milind Phal",
      "Sun Jae Lee",
      "Theofilos Strinopoulos",
      "Wei-Jen Ko",
      "Yael Gold-Zamir",
      "Yael Haramaty",
      "Yannis Assael"
    ],
    "published": "2025-05-30T11:26:32+00:00",
    "summary": "Artificial intelligence (AI) is poised to transform education, but the research community lacks a robust, general benchmark to evaluate AI models for learning. To assess state-of-the-art support for educational use cases, we ran an \"arena for learning\" where educators and pedagogy experts conduct blind, head-to-head, multi-turn comparisons of leading AI models. In particular, $N = 189$ educators drew from their experience to role-play realistic learning use cases, interacting with two models sequentially, after which $N = 206$ experts judged which model better supported the user's learning goals. The arena evaluated a slate of state-of-the-art models: Gemini 2.5 Pro, Claude 3.7 Sonnet, GPT-4o, and OpenAI o3. Excluding ties, experts preferred Gemini 2.5 Pro in 73.2% of these match-ups -- ranking it first overall in the arena. Gemini 2.5 Pro also demonstrated markedly higher performance across key principles of good pedagogy. Altogether, these results position Gemini 2.5 Pro as a leading model for learning."
  },
  {
    "title": "Learning Safety Constraints for Large Language Models",
    "url": "http://arxiv.org/abs/2505.24445v1",
    "arxiv_id": "2505.24445v1",
    "authors": [
      "Xin Chen",
      "Yarden As",
      "Andreas Krause"
    ],
    "published": "2025-05-30T10:30:24+00:00",
    "summary": "Large language models (LLMs) have emerged as powerful tools but pose significant safety risks through harmful outputs and vulnerability to adversarial attacks. We propose SaP, short for Safety Polytope, a geometric approach to LLM safety that learns and enforces multiple safety constraints directly in the model's representation space. We develop a framework that identifies safe and unsafe regions via the polytope's facets, enabling both detection and correction of unsafe outputs through geometric steering. Unlike existing approaches that modify model weights, SaP operates post-hoc in the representation space, preserving model capabilities while enforcing safety constraints. Experiments across multiple LLMs demonstrate that our method can effectively detect unethical inputs, reduce adversarial attack success rates while maintaining performance on standard tasks, thus highlighting the importance of having an explicit geometric model for safety. Analysis of the learned polytope facets reveals emergence of specialization in detecting different semantic notions of safety, providing interpretable insights into how safety is captured in LLMs' representation space."
  },
  {
    "title": "Model Unlearning via Sparse Autoencoder Subspace Guided Projections",
    "url": "http://arxiv.org/abs/2505.24428v1",
    "arxiv_id": "2505.24428v1",
    "authors": [
      "Xu Wang",
      "Zihao Li",
      "Benyou Wang",
      "Yan Hu",
      "Difan Zou"
    ],
    "published": "2025-05-30T10:07:52+00:00",
    "summary": "Large language models (LLMs) store vast amounts of information, making them powerful yet raising privacy and safety concerns when selective knowledge removal is required. Existing unlearning strategies, ranging from gradient-based fine-tuning and model editing to sparse autoencoder (SAE) steering, either lack interpretability or fail to provide a robust defense against adversarial prompts. We propose SAE-Guided Subspace Projection Unlearning (SSPU), a novel framework that leverages SAE features to drive targeted updates in the model's parameter space, enabling precise, interpretable, and robust unlearning. SSPU's three-stage pipeline performs data-driven layer and feature selection, subspace construction via QR decomposition, and constrained optimization that controls activations into an \"irrelevant\" subspace while preserving retained knowledge. Overall, we use SAE features to construct a subspace that supervises unlearning, refining the loss and adding a regularization term to guide interpretable parameter updates. In experiments on the WMDP-Cyber forget set and three utility benchmarks (MMLU, TruthfulQA, GSM8K), SSPU reduces harmful knowledge accuracy by 3.22% compared to the strongest baseline. It also improves adversarial robustness, lowering malicious accuracy under jailbreak prompts compared to baselines. Our findings expose the limitations of prior unlearning methods and demonstrate how interpretable subspace-guided optimization can achieve robust, controllable model behavior."
  },
  {
    "title": "Pangu DeepDiver: Adaptive Search Intensity Scaling via Open-Web Reinforcement Learning",
    "url": "http://arxiv.org/abs/2505.24332v1",
    "arxiv_id": "2505.24332v1",
    "authors": [
      "Wenxuan Shi",
      "Haochen Tan",
      "Chuqiao Kuang",
      "Xiaoguang Li",
      "Xiaozhe Ren",
      "Chen Zhang",
      "Hanting Chen",
      "Yasheng Wang",
      "Lifeng Shang",
      "Fisher Yu",
      "Yunhe Wang"
    ],
    "published": "2025-05-30T08:15:39+00:00",
    "summary": "Information seeking demands iterative evidence gathering and reflective reasoning, yet large language models (LLMs) still struggle with it in open-web question answering. Existing methods rely on static prompting rules or training with Wikipedia-based corpora and retrieval environments, limiting adaptability to the real-world web environment where ambiguity, conflicting evidence, and noise are prevalent. These constrained training settings hinder LLMs from learning to dynamically decide when and where to search, and how to adjust search depth and frequency based on informational demands. We define this missing capacity as Search Intensity Scaling (SIS)--the emergent skill to intensify search efforts under ambiguous or conflicting conditions, rather than settling on overconfident, under-verification answers.   To study SIS, we introduce WebPuzzle, the first dataset designed to foster information-seeking behavior in open-world internet environments. WebPuzzle consists of 24K training instances and 275 test questions spanning both wiki-based and open-web queries. Building on this dataset, we propose DeepDiver, a Reinforcement Learning (RL) framework that promotes SIS by encouraging adaptive search policies through exploration under a real-world open-web environment. Experimental results show that Pangu-7B-Reasoner empowered by DeepDiver achieve performance on real-web tasks comparable to the 671B-parameter DeepSeek-R1. We detail DeepDiver's training curriculum from cold-start supervised fine-tuning to a carefully designed RL phase, and present that its capability of SIS generalizes from closed-form QA to open-ended tasks such as long-form writing. Our contributions advance adaptive information seeking in LLMs and provide a valuable benchmark and dataset for future research."
  },
  {
    "title": "EgoExOR: An Ego-Exo-Centric Operating Room Dataset for Surgical Activity Understanding",
    "url": "http://arxiv.org/abs/2505.24287v1",
    "arxiv_id": "2505.24287v1",
    "authors": [
      "Ege \u00d6zsoy",
      "Arda Mamur",
      "Felix Tristram",
      "Chantal Pellegrini",
      "Magdalena Wysocki",
      "Benjamin Busam",
      "Nassir Navab"
    ],
    "published": "2025-05-30T07:02:00+00:00",
    "summary": "Operating rooms (ORs) demand precise coordination among surgeons, nurses, and equipment in a fast-paced, occlusion-heavy environment, necessitating advanced perception models to enhance safety and efficiency. Existing datasets either provide partial egocentric views or sparse exocentric multi-view context, but do not explore the comprehensive combination of both. We introduce EgoExOR, the first OR dataset and accompanying benchmark to fuse first-person and third-person perspectives. Spanning 94 minutes (84,553 frames at 15 FPS) of two emulated spine procedures, Ultrasound-Guided Needle Insertion and Minimally Invasive Spine Surgery, EgoExOR integrates egocentric data (RGB, gaze, hand tracking, audio) from wearable glasses, exocentric RGB and depth from RGB-D cameras, and ultrasound imagery. Its detailed scene graph annotations, covering 36 entities and 22 relations (568,235 triplets), enable robust modeling of clinical interactions, supporting tasks like action recognition and human-centric perception. We evaluate the surgical scene graph generation performance of two adapted state-of-the-art models and offer a new baseline that explicitly leverages EgoExOR's multimodal and multi-perspective signals. This new dataset and benchmark set a new foundation for OR perception, offering a rich, multimodal resource for next-generation clinical perception."
  },
  {
    "title": "FABLE: A Novel Data-Flow Analysis Benchmark on Procedural Text for Large Language Model Evaluation",
    "url": "http://arxiv.org/abs/2505.24258v1",
    "arxiv_id": "2505.24258v1",
    "authors": [
      "Vishal Pallagani",
      "Nitin Gupta",
      "John Aydin",
      "Biplav Srivastava"
    ],
    "published": "2025-05-30T06:32:34+00:00",
    "summary": "Understanding how data moves, transforms, and persists, known as data flow, is fundamental to reasoning in procedural tasks. Despite their fluency in natural and programming languages, large language models (LLMs), although increasingly being applied to decisions with procedural tasks, have not been systematically evaluated for their ability to perform data-flow reasoning. We introduce FABLE, an extensible benchmark designed to assess LLMs' understanding of data flow using structured, procedural text. FABLE adapts eight classical data-flow analyses from software engineering: reaching definitions, very busy expressions, available expressions, live variable analysis, interval analysis, type-state analysis, taint analysis, and concurrency analysis. These analyses are instantiated across three real-world domains: cooking recipes, travel routes, and automated plans. The benchmark includes 2,400 question-answer pairs, with 100 examples for each domain-analysis combination. We evaluate three types of LLMs: a reasoning-focused model (DeepSeek-R1 8B), a general-purpose model (LLaMA 3.1 8B), and a code-specific model (Granite Code 8B). Each model is tested using majority voting over five sampled completions per prompt. Results show that the reasoning model achieves higher accuracy, but at the cost of over 20 times slower inference compared to the other models. In contrast, the general-purpose and code-specific models perform close to random chance. FABLE provides the first diagnostic benchmark to systematically evaluate data-flow reasoning and offers insights for developing models with stronger procedural understanding."
  },
  {
    "title": "Effects of Theory of Mind and Prosocial Beliefs on Steering Human-Aligned Behaviors of LLMs in Ultimatum Games",
    "url": "http://arxiv.org/abs/2505.24255v1",
    "arxiv_id": "2505.24255v1",
    "authors": [
      "Neemesh Yadav",
      "Palakorn Achananuparp",
      "Jing Jiang",
      "Ee-Peng Lim"
    ],
    "published": "2025-05-30T06:23:52+00:00",
    "summary": "Large Language Models (LLMs) have shown potential in simulating human behaviors and performing theory-of-mind (ToM) reasoning, a crucial skill for complex social interactions. In this study, we investigate the role of ToM reasoning in aligning agentic behaviors with human norms in negotiation tasks, using the ultimatum game as a controlled environment. We initialized LLM agents with different prosocial beliefs (including Greedy, Fair, and Selfless) and reasoning methods like chain-of-thought (CoT) and varying ToM levels, and examined their decision-making processes across diverse LLMs, including reasoning models like o3-mini and DeepSeek-R1 Distilled Qwen 32B. Results from 2,700 simulations indicated that ToM reasoning enhances behavior alignment, decision-making consistency, and negotiation outcomes. Consistent with previous findings, reasoning models exhibit limited capability compared to models with ToM reasoning, different roles of the game benefits with different orders of ToM reasoning. Our findings contribute to the understanding of ToM's role in enhancing human-AI interaction and cooperative decision-making. The code used for our experiments can be found at https://github.com/Stealth-py/UltimatumToM."
  },
  {
    "title": "From Hallucinations to Jailbreaks: Rethinking the Vulnerability of Large Foundation Models",
    "url": "http://arxiv.org/abs/2505.24232v1",
    "arxiv_id": "2505.24232v1",
    "authors": [
      "Haibo Jin",
      "Peiyan Zhang",
      "Peiran Wang",
      "Man Luo",
      "Haohan Wang"
    ],
    "published": "2025-05-30T05:48:50+00:00",
    "summary": "Large foundation models (LFMs) are susceptible to two distinct vulnerabilities: hallucinations and jailbreak attacks. While typically studied in isolation, we observe that defenses targeting one often affect the other, hinting at a deeper connection.   We propose a unified theoretical framework that models jailbreaks as token-level optimization and hallucinations as attention-level optimization. Within this framework, we establish two key propositions: (1) \\textit{Similar Loss Convergence} - the loss functions for both vulnerabilities converge similarly when optimizing for target-specific outputs; and (2) \\textit{Gradient Consistency in Attention Redistribution} - both exhibit consistent gradient behavior driven by shared attention dynamics.   We validate these propositions empirically on LLaVA-1.5 and MiniGPT-4, showing consistent optimization trends and aligned gradients. Leveraging this connection, we demonstrate that mitigation techniques for hallucinations can reduce jailbreak success rates, and vice versa. Our findings reveal a shared failure mode in LFMs and suggest that robustness strategies should jointly address both vulnerabilities."
  },
  {
    "title": "Safety-Aware Robust Model Predictive Control for Robotic Arms in Dynamic Environments",
    "url": "http://arxiv.org/abs/2505.24209v1",
    "arxiv_id": "2505.24209v1",
    "authors": [
      "Sanghyeon Nam",
      "Dongmin Kim",
      "Seung-Hwan Choi",
      "Chang-Hyun Kim",
      "Hyoeun Kwon",
      "Hiroaki Kawamoto",
      "Suwoong Lee"
    ],
    "published": "2025-05-30T04:41:28+00:00",
    "summary": "Robotic manipulators are essential for precise industrial pick-and-place operations, yet planning collision-free trajectories in dynamic environments remains challenging due to uncertainties such as sensor noise and time-varying delays. Conventional control methods often fail under these conditions, motivating the development of Robust MPC (RMPC) strategies with constraint tightening. In this paper, we propose a novel RMPC framework that integrates phase-based nominal control with a robust safety mode, allowing smooth transitions between safe and nominal operations. Our approach dynamically adjusts constraints based on real-time predictions of moving obstacles\\textemdash whether human, robot, or other dynamic objects\\textemdash thus ensuring continuous, collision-free operation. Simulation studies demonstrate that our controller improves both motion naturalness and safety, achieving faster task completion than conventional methods."
  },
  {
    "title": "Bootstrapping LLM Robustness for VLM Safety via Reducing the Pretraining Modality Gap",
    "url": "http://arxiv.org/abs/2505.24208v1",
    "arxiv_id": "2505.24208v1",
    "authors": [
      "Wenhan Yang",
      "Spencer Stice",
      "Ali Payani",
      "Baharan Mirzasoleiman"
    ],
    "published": "2025-05-30T04:40:08+00:00",
    "summary": "Ensuring Vision-Language Models (VLMs) generate safe outputs is crucial for their reliable deployment. However, LVLMs suffer from drastic safety degradation compared to their LLM backbone. Even blank or irrelevant images can trigger LVLMs to generate harmful responses to prompts that would otherwise be refused in text-only contexts. The modality gap between image and text representations has been recently hypothesized to contribute to safety degradation of LVLMs. However, if and how the amount of modality gap affects LVLMs' safety is not studied. In this work, we show that the amount of modality gap is highly inversely correlated with VLMs' safety. Then, we show that this modality gap is introduced during pretraining LVLMs and persists through fine-tuning. Inspired by this observation, we propose a regularization to reduce the modality gap during pretraining. Our extensive experiments on LLaVA v1.5, ShareGPT4V, and MiniGPT-4 show that our method substantially improves safety alignment of LVLMs, reducing unsafe rate by up to 16.3% without compromising performance, and can further boost existing defenses by up to 18.2%."
  },
  {
    "title": "CodeV-R1: Reasoning-Enhanced Verilog Generation",
    "url": "http://arxiv.org/abs/2505.24183v1",
    "arxiv_id": "2505.24183v1",
    "authors": [
      "Yaoyu Zhu",
      "Di Huang",
      "Hanqi Lyu",
      "Xiaoyun Zhang",
      "Chongxiao Li",
      "Wenxuan Shi",
      "Yutong Wu",
      "Jianan Mu",
      "Jinghua Wang",
      "Yang Zhao",
      "Pengwei Jin",
      "Shuyao Cheng",
      "Shengwen Liang",
      "Xishan Zhang",
      "Rui Zhang",
      "Zidong Du",
      "Qi Guo",
      "Xing Hu",
      "Yunji Chen"
    ],
    "published": "2025-05-30T03:51:06+00:00",
    "summary": "Large language models (LLMs) trained via reinforcement learning with verifiable reward (RLVR) have achieved breakthroughs on tasks with explicit, automatable verification, such as software programming and mathematical problems. Extending RLVR to electronic design automation (EDA), especially automatically generating hardware description languages (HDLs) like Verilog from natural-language (NL) specifications, however, poses three key challenges: the lack of automated and accurate verification environments, the scarcity of high-quality NL-code pairs, and the prohibitive computation cost of RLVR. To this end, we introduce CodeV-R1, an RLVR framework for training Verilog generation LLMs. First, we develop a rule-based testbench generator that performs robust equivalence checking against golden references. Second, we propose a round-trip data synthesis method that pairs open-source Verilog snippets with LLM-generated NL descriptions, verifies code-NL-code consistency via the generated testbench, and filters out inequivalent examples to yield a high-quality dataset. Third, we employ a two-stage \"distill-then-RL\" training pipeline: distillation for the cold start of reasoning abilities, followed by adaptive DAPO, our novel RLVR algorithm that can reduce training cost by adaptively adjusting sampling rate. The resulting model, CodeV-R1-7B, achieves 68.6% and 72.9% pass@1 on VerilogEval v2 and RTLLM v1.1, respectively, surpassing prior state-of-the-art by 12~20%, while matching or even exceeding the performance of 671B DeepSeek-R1. We will release our model, training pipeline, and dataset to facilitate research in EDA and LLM communities."
  },
  {
    "title": "Seeing is Not Reasoning: MVPBench for Graph-based Evaluation of Multi-path Visual Physical CoT",
    "url": "http://arxiv.org/abs/2505.24182v1",
    "arxiv_id": "2505.24182v1",
    "authors": [
      "Zhuobai Dong",
      "Junchao Yi",
      "Ziyuan Zheng",
      "Haochen Han",
      "Xiangxi Zheng",
      "Alex Jinpeng Wang",
      "Fangming Liu",
      "Linjie Li"
    ],
    "published": "2025-05-30T03:48:59+00:00",
    "summary": "Understanding the physical world - governed by laws of motion, spatial relations, and causality - poses a fundamental challenge for multimodal large language models (MLLMs). While recent advances such as OpenAI o3 and GPT-4o demonstrate impressive perceptual and reasoning capabilities, our investigation reveals these models struggle profoundly with visual physical reasoning, failing to grasp basic physical laws, spatial interactions, and causal effects in complex scenes. More importantly, they often fail to follow coherent reasoning chains grounded in visual evidence, especially when multiple steps are needed to arrive at the correct answer. To rigorously evaluate this capability, we introduce MVPBench, a curated benchmark designed to rigorously evaluate visual physical reasoning through the lens of visual chain-of-thought (CoT). Each example features interleaved multi-image inputs and demands not only the correct final answer but also a coherent, step-by-step reasoning path grounded in evolving visual cues. This setup mirrors how humans reason through real-world physical processes over time. To ensure fine-grained evaluation, we introduce a graph-based CoT consistency metric that verifies whether the reasoning path of model adheres to valid physical logic. Additionally, we minimize shortcut exploitation from text priors, encouraging models to rely on visual understanding. Experimental results reveal a concerning trend: even cutting-edge MLLMs exhibit poor visual reasoning accuracy and weak image-text alignment in physical domains. Surprisingly, RL-based post-training alignment - commonly believed to improve visual reasoning performance - often harms spatial reasoning, suggesting a need to rethink current fine-tuning practices."
  },
  {
    "title": "The State of Multilingual LLM Safety Research: From Measuring the Language Gap to Mitigating It",
    "url": "http://arxiv.org/abs/2505.24119v1",
    "arxiv_id": "2505.24119v1",
    "authors": [
      "Zheng-Xin Yong",
      "Beyza Ermis",
      "Marzieh Fadaee",
      "Stephen H. Bach",
      "Julia Kreutzer"
    ],
    "published": "2025-05-30T01:32:44+00:00",
    "summary": "This paper presents a comprehensive analysis of the linguistic diversity of LLM safety research, highlighting the English-centric nature of the field. Through a systematic review of nearly 300 publications from 2020--2024 across major NLP conferences and workshops at *ACL, we identify a significant and growing language gap in LLM safety research, with even high-resource non-English languages receiving minimal attention. We further observe that non-English languages are rarely studied as a standalone language and that English safety research exhibits poor language documentation practice. To motivate future research into multilingual safety, we make several recommendations based on our survey, and we then pose three concrete future directions on safety evaluation, training data generation, and crosslingual safety generalization. Based on our survey and proposed directions, the field can develop more robust, inclusive AI safety practices for diverse global populations."
  },
  {
    "title": "DINO-R1: Incentivizing Reasoning Capability in Vision Foundation Models",
    "url": "http://arxiv.org/abs/2505.24025v1",
    "arxiv_id": "2505.24025v1",
    "authors": [
      "Chenbin Pan",
      "Wenbin He",
      "Zhengzhong Tu",
      "Liu Ren"
    ],
    "published": "2025-05-29T21:58:06+00:00",
    "summary": "The recent explosive interest in the reasoning capabilities of large language models, such as DeepSeek-R1, has demonstrated remarkable success through reinforcement learning-based fine-tuning frameworks, exemplified by methods like Group Relative Policy Optimization (GRPO). However, such reasoning abilities remain underexplored and notably absent in vision foundation models, including representation models like the DINO series. In this work, we propose \\textbf{DINO-R1}, the first such attempt to incentivize visual in-context reasoning capabilities of vision foundation models using reinforcement learning. Specifically, DINO-R1 introduces \\textbf{Group Relative Query Optimization (GRQO)}, a novel reinforcement-style training strategy explicitly designed for query-based representation models, which computes query-level rewards based on group-normalized alignment quality. We also apply KL-regularization to stabilize the objectness distribution to reduce the training instability. This joint optimization enables dense and expressive supervision across queries while mitigating overfitting and distributional drift. Building upon Grounding-DINO, we train a series of DINO-R1 family models that integrate a visual prompt encoder and a visual-guided query selection mechanism. Extensive experiments on COCO, LVIS, and ODinW demonstrate that DINO-R1 significantly outperforms supervised fine-tuning baselines, achieving strong generalization in both open-vocabulary and closed-set visual prompting scenarios."
  },
  {
    "title": "Exploiting Euclidean Distance Field Properties for Fast and Safe 3D planning with a modified Lazy Theta*",
    "url": "http://arxiv.org/abs/2505.24024v1",
    "arxiv_id": "2505.24024v1",
    "authors": [
      "Jose A. Cobano",
      "L. Merino",
      "F. Caballero"
    ],
    "published": "2025-05-29T21:51:02+00:00",
    "summary": "Graph search planners have been widely used for 3D path planning in the literature, and Euclidean Distance Fields (EDFs) are increasingly being used as a representation of the environment. However, to the best of our knowledge, the integration of EDFs into heuristic planning has been carried out in a loosely coupled fashion, dismissing EDF properties that can be used to accelerate/improve the planning process and enhance the safety margins of the resultant trajectories. This paper presents a fast graph search planner based on a modified Lazy Theta* planning algorithm for aerial robots in challenging 3D environments that exploits the EDF properties. The proposed planner outperforms classic graph search planners in terms of path smoothness and safety. It integrates EDFs as environment representation and directly generates fast and smooth paths avoiding the use of post-processing methods; it also considers the analytical properties of EDFs to obtain an approximation of the EDF cost along the line-of-sight segments and to reduce the number of visibility neighbours, which directly impacts the computation time. Moreover, we demonstrate that the proposed EDF-based cost function satisfies the triangle inequality, which reduces calculations during exploration and, hence, computation time. Many experiments and comparatives are carried out in 3D challenging indoor and outdoor simulation environments to evaluate and validate the proposed planner. The results show an efficient and safe planner in these environments."
  },
  {
    "title": "Confidential Guardian: Cryptographically Prohibiting the Abuse of Model Abstention",
    "url": "http://arxiv.org/abs/2505.23968v1",
    "arxiv_id": "2505.23968v1",
    "authors": [
      "Stephan Rabanser",
      "Ali Shahin Shamsabadi",
      "Olive Franzese",
      "Xiao Wang",
      "Adrian Weller",
      "Nicolas Papernot"
    ],
    "published": "2025-05-29T19:47:50+00:00",
    "summary": "Cautious predictions -- where a machine learning model abstains when uncertain -- are crucial for limiting harmful errors in safety-critical applications. In this work, we identify a novel threat: a dishonest institution can exploit these mechanisms to discriminate or unjustly deny services under the guise of uncertainty. We demonstrate the practicality of this threat by introducing an uncertainty-inducing attack called Mirage, which deliberately reduces confidence in targeted input regions, thereby covertly disadvantaging specific individuals. At the same time, Mirage maintains high predictive performance across all data points. To counter this threat, we propose Confidential Guardian, a framework that analyzes calibration metrics on a reference dataset to detect artificially suppressed confidence. Additionally, it employs zero-knowledge proofs of verified inference to ensure that reported confidence scores genuinely originate from the deployed model. This prevents the provider from fabricating arbitrary model confidence values while protecting the model's proprietary details. Our results confirm that Confidential Guardian effectively prevents the misuse of cautious predictions, providing verifiable assurances that abstention reflects genuine model uncertainty rather than malicious intent."
  },
  {
    "title": "MangoLeafViT: Leveraging Lightweight Vision Transformer with Runtime Augmentation for Efficient Mango Leaf Disease Classification",
    "url": "http://arxiv.org/abs/2505.23961v1",
    "arxiv_id": "2505.23961v1",
    "authors": [
      "Rafi Hassan Chowdhury",
      "Sabbir Ahmed"
    ],
    "published": "2025-05-29T19:28:57+00:00",
    "summary": "Ensuring food safety is critical due to its profound impact on public health, economic stability, and global supply chains. Cultivation of Mango, a major agricultural product in several South Asian countries, faces high financial losses due to different diseases, affecting various aspects of the entire supply chain. While deep learning-based methods have been explored for mango leaf disease classification, there remains a gap in designing solutions that are computationally efficient and compatible with low-end devices. In this work, we propose a lightweight Vision Transformer-based pipeline with a self-attention mechanism to classify mango leaf diseases, achieving state-of-the-art performance with minimal computational overhead. Our approach leverages global attention to capture intricate patterns among disease types and incorporates runtime augmentation for enhanced performance. Evaluation on the MangoLeafBD dataset demonstrates a 99.43% accuracy, outperforming existing methods in terms of model size, parameter count, and FLOPs count."
  },
  {
    "title": "How Many Times Should We Matched Filter Gravitational Wave Data? A Comparison of GstLAL's Online and Offline Performance",
    "url": "http://arxiv.org/abs/2505.23959v1",
    "arxiv_id": "2505.23959v1",
    "authors": [
      "Prathamesh Joshi",
      "Wanting Niu",
      "Chad Hanna",
      "Rachael Huxford",
      "Divya Singh",
      "Leo Tsukada",
      "Shomik Adhicary",
      "Pratyusava Baral",
      "Amanda Baylor",
      "Kipp Cannon",
      "Sarah Caudill",
      "Michael W. Coughlin",
      "Bryce Cousins",
      "Jolien D. E. Creighton",
      "Becca Ewing",
      "Heather Fong",
      "Richard N. George",
      "Shaon Ghosh",
      "Patrick Godwin",
      "Reiko Harada",
      "Yun-Jing Huang",
      "Cody Messick",
      "Soichiro Morisaki",
      "Debnandini Mukherjee",
      "Alexander Pace",
      "Cort Posnansky",
      "Anarya Ray",
      "Surabhi Sachdev",
      "Shio Sakon",
      "Urja Shah",
      "Ron Tapia",
      "Koh Ueno",
      "Aaron Viets",
      "Leslie Wade",
      "Madeline Wade",
      "Zach Yarbrough",
      "Noah Zhang"
    ],
    "published": "2025-05-29T19:24:24+00:00",
    "summary": "Searches for gravitational waves from compact binary coalescences employ a process called matched filtering, in which gravitational wave strain data is cross-correlated against a bank of waveform templates. Data from every observing run of the LIGO, Virgo, and KAGRA collaboration is typically analyzed in this way twice, first in a low-latency mode in which gravitational wave candidates are identified in near-real time, and later in a high-latency mode. Such high-latency analyses have traditionally been considered more sensitive, since background data from the full observing run is available for assigning significance to all candidates, as well as more robust, since they do not need to worry about keeping up with live data. In this work, we present a novel technique to use the matched filtering data products from a low-latency analysis and re-process them by assigning significances in a high-latency way, effectively removing the need to perform matched filtering a second time. To demonstrate the efficacy of our method, we analyze 38 days of LIGO and Virgo data from the third observing run (O3) using the GstLAL pipeline, and show that our method is as sensitive and reliable as a traditional high-latency analysis. Since matched filtering represents the vast majority of computing time for a traditional analysis, our method greatly reduces the time and computational burden required to produce the same results as a traditional high-latency analysis. Consequently, it has already been adopted by GstLAL for the fourth observing run (O4) of the LIGO, Virgo, and KAGRA collaboration."
  },
  {
    "title": "Enhancing LLM-Based Code Generation with Complexity Metrics: A Feedback-Driven Approach",
    "url": "http://arxiv.org/abs/2505.23953v1",
    "arxiv_id": "2505.23953v1",
    "authors": [
      "Melika Sepidband",
      "Hamed Taherkhani",
      "Song Wang",
      "Hadi Hemmati"
    ],
    "published": "2025-05-29T19:06:14+00:00",
    "summary": "Automatic code generation has gained significant momentum with the advent of Large Language Models (LLMs) such as GPT-4. Although many studies focus on improving the effectiveness of LLMs for code generation, very limited work tries to understand the generated code's characteristics and leverage that to improve failed cases. In this paper, as the most straightforward characteristic of code, we investigate the relationship between code complexity and the success of LLM generated code. Using a large set of standard complexity metrics, we first conduct an empirical analysis to explore their correlation with LLM's performance on code generation (i.e., Pass@1). Using logistic regression models, we identify which complexity metrics are most predictive of code correctness. Building on these findings, we propose an iterative feedback method, where LLMs are prompted to generate correct code based on complexity metrics from previous failed outputs. We validate our approach across multiple benchmarks (i.e., HumanEval, MBPP, LeetCode, and BigCodeBench) and various LLMs (i.e., GPT-4o, GPT-3.5 Turbo, Llama 3.1, and GPT-o3 mini), comparing the results with two baseline methods: (a) zero-shot generation, and (b) iterative execution-based feedback without our code complexity insights. Experiment results show that our approach makes notable improvements, particularly with a smaller LLM (GPT3.5 Turbo), where, e.g., Pass@1 increased by 35.71% compared to the baseline's improvement of 12.5% on the HumanEval dataset. The study expands experiments to BigCodeBench and integrates the method with the Reflexion code generation agent, leading to Pass@1 improvements of 20% (GPT-4o) and 23.07% (GPT-o3 mini). The results highlight that complexity-aware feedback enhances both direct LLM prompting and agent-based workflows."
  },
  {
    "title": "MMSI-Bench: A Benchmark for Multi-Image Spatial Intelligence",
    "url": "http://arxiv.org/abs/2505.23764v1",
    "arxiv_id": "2505.23764v1",
    "authors": [
      "Sihan Yang",
      "Runsen Xu",
      "Yiman Xie",
      "Sizhe Yang",
      "Mo Li",
      "Jingli Lin",
      "Chenming Zhu",
      "Xiaochen Chen",
      "Haodong Duan",
      "Xiangyu Yue",
      "Dahua Lin",
      "Tai Wang",
      "Jiangmiao Pang"
    ],
    "published": "2025-05-29T17:59:52+00:00",
    "summary": "Spatial intelligence is essential for multimodal large language models (MLLMs) operating in the complex physical world. Existing benchmarks, however, probe only single-image relations and thus fail to assess the multi-image spatial reasoning that real-world deployments demand. We introduce MMSI-Bench, a VQA benchmark dedicated to multi-image spatial intelligence. Six 3D-vision researchers spent more than 300 hours meticulously crafting 1,000 challenging, unambiguous multiple-choice questions from over 120,000 images, each paired with carefully designed distractors and a step-by-step reasoning process. We conduct extensive experiments and thoroughly evaluate 34 open-source and proprietary MLLMs, observing a wide gap: the strongest open-source model attains roughly 30% accuracy and OpenAI's o3 reasoning model reaches 40%, while humans score 97%. These results underscore the challenging nature of MMSI-Bench and the substantial headroom for future research. Leveraging the annotated reasoning processes, we also provide an automated error analysis pipeline that diagnoses four dominant failure modes, including (1) grounding errors, (2) overlap-matching and scene-reconstruction errors, (3) situation-transformation reasoning errors, and (4) spatial-logic errors, offering valuable insights for advancing multi-image spatial intelligence. Project page: https://runsenxu.com/projects/MMSI_Bench ."
  },
  {
    "title": "To Trust Or Not To Trust Your Vision-Language Model's Prediction",
    "url": "http://arxiv.org/abs/2505.23745v1",
    "arxiv_id": "2505.23745v1",
    "authors": [
      "Hao Dong",
      "Moru Liu",
      "Jian Liang",
      "Eleni Chatzi",
      "Olga Fink"
    ],
    "published": "2025-05-29T17:59:01+00:00",
    "summary": "Vision-Language Models (VLMs) have demonstrated strong capabilities in aligning visual and textual modalities, enabling a wide range of applications in multimodal understanding and generation. While they excel in zero-shot and transfer learning scenarios, VLMs remain susceptible to misclassification, often yielding confident yet incorrect predictions. This limitation poses a significant risk in safety-critical domains, where erroneous predictions can lead to severe consequences. In this work, we introduce TrustVLM, a training-free framework designed to address the critical challenge of estimating when VLM's predictions can be trusted. Motivated by the observed modality gap in VLMs and the insight that certain concepts are more distinctly represented in the image embedding space, we propose a novel confidence-scoring function that leverages this space to improve misclassification detection. We rigorously evaluate our approach across 17 diverse datasets, employing 4 architectures and 2 VLMs, and demonstrate state-of-the-art performance, with improvements of up to 51.87% in AURC, 9.14% in AUROC, and 32.42% in FPR95 compared to existing baselines. By improving the reliability of the model without requiring retraining, TrustVLM paves the way for safer deployment of VLMs in real-world applications. The code will be available at https://github.com/EPFL-IMOS/TrustVLM."
  },
  {
    "title": "Exposing the Impact of GenAI for Cybercrime: An Investigation into the Dark Side",
    "url": "http://arxiv.org/abs/2505.23733v1",
    "arxiv_id": "2505.23733v1",
    "authors": [
      "Truong",
      "Luu",
      "Binny M. Samuel"
    ],
    "published": "2025-05-29T17:57:01+00:00",
    "summary": "In recent years, the rapid advancement and democratization of generative AI models have sparked significant debate over safety, ethical risks, and dual-use concerns, particularly in the context of cybersecurity. While anecdotally known, this paper provides empirical evidence regarding generative AI's association with malicious internet-related activities and cybercrime by examining the phenomenon through psychological frameworks of technological amplification and affordance theory. Using a quasi-experimental design with interrupted time series analysis, we analyze two datasets, one general and one cryptocurrency-focused, to empirically assess generative AI's role in cybercrime. The findings contribute to ongoing discussions about AI governance by balancing control and fostering innovation, underscoring the need for strategies to guide policymakers, inform AI developers and cybersecurity professionals, and educate the public to maximize AI's benefits while mitigating its risks."
  },
  {
    "title": "SC-LoRA: Balancing Efficient Fine-tuning and Knowledge Preservation via Subspace-Constrained LoRA",
    "url": "http://arxiv.org/abs/2505.23724v1",
    "arxiv_id": "2505.23724v1",
    "authors": [
      "Minrui Luo",
      "Fuhang Kuang",
      "Yu Wang",
      "Zirui Liu",
      "Tianxing He"
    ],
    "published": "2025-05-29T17:55:21+00:00",
    "summary": "Parameter-Efficient Fine-Tuning (PEFT) methods, particularly Low-Rank Adaptation (LoRA), are indispensable for efficiently customizing Large Language Models (LLMs). However, vanilla LoRA suffers from slow convergence speed and knowledge forgetting problems. Recent studies have leveraged the power of designed LoRA initialization, to enhance the fine-tuning efficiency, or to preserve knowledge in the pre-trained LLM. However, none of these works can address the two cases at the same time. To this end, we introduce Subspace-Constrained LoRA (SC-LoRA), a novel LoRA initialization framework engineered to navigate the trade-off between efficient fine-tuning and knowledge preservation. We achieve this by constraining the output of trainable LoRA adapters in a low-rank subspace, where the context information of fine-tuning data is most preserved while the context information of preserved knowledge is least retained, in a balanced way. Such constraint enables the trainable weights to primarily focus on the main features of fine-tuning data while avoiding damaging the preserved knowledge features. We provide theoretical analysis on our method, and conduct extensive experiments including safety preservation and world knowledge preservation, on various downstream tasks. In our experiments, SC-LoRA succeeds in delivering superior fine-tuning performance while markedly diminishing knowledge forgetting, surpassing contemporary LoRA initialization methods."
  },
  {
    "title": "ML-Agent: Reinforcing LLM Agents for Autonomous Machine Learning Engineering",
    "url": "http://arxiv.org/abs/2505.23723v1",
    "arxiv_id": "2505.23723v1",
    "authors": [
      "Zexi Liu",
      "Jingyi Chai",
      "Xinyu Zhu",
      "Shuo Tang",
      "Rui Ye",
      "Bo Zhang",
      "Lei Bai",
      "Siheng Chen"
    ],
    "published": "2025-05-29T17:54:44+00:00",
    "summary": "The emergence of large language model (LLM)-based agents has significantly advanced the development of autonomous machine learning (ML) engineering. However, most existing approaches rely heavily on manual prompt engineering, failing to adapt and optimize based on diverse experimental experiences. Focusing on this, for the first time, we explore the paradigm of learning-based agentic ML, where an LLM agent learns through interactive experimentation on ML tasks using online reinforcement learning (RL). To realize this, we propose a novel agentic ML training framework with three key components: (1) exploration-enriched fine-tuning, which enables LLM agents to generate diverse actions for enhanced RL exploration; (2) step-wise RL, which enables training on a single action step, accelerating experience collection and improving training efficiency; (3) an agentic ML-specific reward module, which unifies varied ML feedback signals into consistent rewards for RL optimization. Leveraging this framework, we train ML-Agent, driven by a 7B-sized Qwen-2.5 LLM for autonomous ML. Remarkably, despite being trained on merely 9 ML tasks, our 7B-sized ML-Agent outperforms the 671B-sized DeepSeek-R1 agent. Furthermore, it achieves continuous performance improvements and demonstrates exceptional cross-task generalization capabilities."
  },
  {
    "title": "Distributed Federated Learning for Vehicular Network Security: Anomaly Detection Benefits and Multi-Domain Attack Threats",
    "url": "http://arxiv.org/abs/2505.23706v1",
    "arxiv_id": "2505.23706v1",
    "authors": [
      "Utku Demir",
      "Yalin E. Sagduyu",
      "Tugba Erpek",
      "Hossein Jafari",
      "Sastry Kompella",
      "Mengran Xue"
    ],
    "published": "2025-05-29T17:41:02+00:00",
    "summary": "In connected and autonomous vehicles, machine learning for safety message classification has become critical for detecting malicious or anomalous behavior. However, conventional approaches that rely on centralized data collection or purely local training face limitations due to the large scale, high mobility, and heterogeneous data distributions inherent in inter-vehicle networks. To overcome these challenges, this paper explores Distributed Federated Learning (DFL), whereby vehicles collaboratively train deep learning models by exchanging model updates among one-hop neighbors and propagating models over multiple hops. Using the Vehicular Reference Misbehavior (VeReMi) Extension Dataset, we show that DFL can significantly improve classification accuracy across all vehicles compared to learning strictly with local data. Notably, vehicles with low individual accuracy see substantial accuracy gains through DFL, illustrating the benefit of knowledge sharing across the network. We further show that local training data size and time-varying network connectivity correlate strongly with the model's overall accuracy. We investigate DFL's resilience and vulnerabilities under attacks in multiple domains, namely wireless jamming and training data poisoning attacks. Our results reveal important insights into the vulnerabilities of DFL when confronted with multi-domain attacks, underlining the need for more robust strategies to secure DFL in vehicular networks."
  },
  {
    "title": "Fortune: Formula-Driven Reinforcement Learning for Symbolic Table Reasoning in Language Models",
    "url": "http://arxiv.org/abs/2505.23667v1",
    "arxiv_id": "2505.23667v1",
    "authors": [
      "Lang Cao",
      "Jingxian Xu",
      "Hanbing Liu",
      "Jinyu Wang",
      "Mengyu Zhou",
      "Haoyu Dong",
      "Shi Han",
      "Dongmei Zhang"
    ],
    "published": "2025-05-29T17:13:40+00:00",
    "summary": "Tables are a fundamental structure for organizing and analyzing data, making effective table understanding a critical capability for intelligent systems. While large language models (LMs) demonstrate strong general reasoning abilities, they continue to struggle with accurate numerical or symbolic reasoning over tabular data, especially in complex scenarios. Spreadsheet formulas provide a powerful and expressive medium for representing executable symbolic operations, encoding rich reasoning patterns that remain largely underutilized. In this paper, we propose Formula Tuning (Fortune), a reinforcement learning (RL) framework that trains LMs to generate executable spreadsheet formulas for question answering over general tabular data. Formula Tuning reduces the reliance on supervised formula annotations by using binary answer correctness as a reward signal, guiding the model to learn formula derivation through reasoning. We provide a theoretical analysis of its advantages and demonstrate its effectiveness through extensive experiments on seven table reasoning benchmarks. Formula Tuning substantially enhances LM performance, particularly on multi-step numerical and symbolic reasoning tasks, enabling a 7B model to outperform O1 on table understanding. This highlights the potential of formula-driven RL to advance symbolic table reasoning in LMs."
  },
  {
    "title": "Fortune: Formula-Driven Reinforcement Learning for Symbolic Table Reasoning in Language Models",
    "url": "http://arxiv.org/abs/2505.23667v2",
    "arxiv_id": "2505.23667v2",
    "authors": [
      "Lang Cao",
      "Jingxian Xu",
      "Hanbing Liu",
      "Jinyu Wang",
      "Mengyu Zhou",
      "Haoyu Dong",
      "Shi Han",
      "Dongmei Zhang"
    ],
    "published": "2025-05-29T17:13:40+00:00",
    "summary": "Tables are a fundamental structure for organizing and analyzing data, making effective table understanding a critical capability for intelligent systems. While large language models (LMs) demonstrate strong general reasoning abilities, they continue to struggle with accurate numerical or symbolic reasoning over tabular data, especially in complex scenarios. Spreadsheet formulas provide a powerful and expressive medium for representing executable symbolic operations, encoding rich reasoning patterns that remain largely underutilized. In this paper, we propose Formula Tuning (Fortune), a reinforcement learning (RL) framework that trains LMs to generate executable spreadsheet formulas for question answering over general tabular data. Formula Tuning reduces the reliance on supervised formula annotations by using binary answer correctness as a reward signal, guiding the model to learn formula derivation through reasoning. We provide a theoretical analysis of its advantages and demonstrate its effectiveness through extensive experiments on seven table reasoning benchmarks. Formula Tuning substantially enhances LM performance, particularly on multi-step numerical and symbolic reasoning tasks, enabling a 7B model to outperform OpenAI o1 on table understanding. This highlights the potential of formula-driven RL to advance symbolic table reasoning in LMs."
  },
  {
    "title": "Are Reasoning Models More Prone to Hallucination?",
    "url": "http://arxiv.org/abs/2505.23646v1",
    "arxiv_id": "2505.23646v1",
    "authors": [
      "Zijun Yao",
      "Yantao Liu",
      "Yanxu Chen",
      "Jianhui Chen",
      "Junfeng Fang",
      "Lei Hou",
      "Juanzi Li",
      "Tat-Seng Chua"
    ],
    "published": "2025-05-29T16:53:41+00:00",
    "summary": "Recently evolved large reasoning models (LRMs) show powerful performance in solving complex tasks with long chain-of-thought (CoT) reasoning capability. As these LRMs are mostly developed by post-training on formal reasoning tasks, whether they generalize the reasoning capability to help reduce hallucination in fact-seeking tasks remains unclear and debated. For instance, DeepSeek-R1 reports increased performance on SimpleQA, a fact-seeking benchmark, while OpenAI-o3 observes even severer hallucination. This discrepancy naturally raises the following research question: Are reasoning models more prone to hallucination? This paper addresses the question from three perspectives. (1) We first conduct a holistic evaluation for the hallucination in LRMs. Our analysis reveals that LRMs undergo a full post-training pipeline with cold start supervised fine-tuning (SFT) and verifiable reward RL generally alleviate their hallucination. In contrast, both distillation alone and RL training without cold start fine-tuning introduce more nuanced hallucinations. (2) To explore why different post-training pipelines alters the impact on hallucination in LRMs, we conduct behavior analysis. We characterize two critical cognitive behaviors that directly affect the factuality of a LRM: Flaw Repetition, where the surface-level reasoning attempts repeatedly follow the same underlying flawed logic, and Think-Answer Mismatch, where the final answer fails to faithfully match the previous CoT process. (3) Further, we investigate the mechanism behind the hallucination of LRMs from the perspective of model uncertainty. We find that increased hallucination of LRMs is usually associated with the misalignment between model uncertainty and factual accuracy. Our work provides an initial understanding of the hallucination in LRMs."
  },
  {
    "title": "MCP Safety Training: Learning to Refuse Falsely Benign MCP Exploits using Improved Preference Alignment",
    "url": "http://arxiv.org/abs/2505.23634v1",
    "arxiv_id": "2505.23634v1",
    "authors": [
      "John Halloran"
    ],
    "published": "2025-05-29T16:44:29+00:00",
    "summary": "The model context protocol (MCP) has been widely adapted as an open standard enabling the seamless integration of generative AI agents. However, recent work has shown the MCP is susceptible to retrieval-based \"falsely benign\" attacks (FBAs), allowing malicious system access and credential theft, but requiring that users download compromised files directly to their systems. Herein, we show that the threat model of MCP-based attacks is significantly broader than previously thought, i.e., attackers need only post malicious content online to deceive MCP agents into carrying out their attacks on unsuspecting victims' systems.   To improve alignment guardrails against such attacks, we introduce a new MCP dataset of FBAs and (truly) benign samples to explore the effectiveness of direct preference optimization (DPO) for the refusal training of large language models (LLMs). While DPO improves model guardrails against such attacks, we show that the efficacy of refusal learning varies drastically depending on the model's original post-training alignment scheme--e.g., GRPO-based LLMs learn to refuse extremely poorly. Thus, to further improve FBA refusals, we introduce Retrieval Augmented Generation for Preference alignment (RAG-Pref), a novel preference alignment strategy based on RAG. We show that RAG-Pref significantly improves the ability of LLMs to refuse FBAs, particularly when combined with DPO alignment, thus drastically improving guardrails against MCP-based attacks."
  },
  {
    "title": "Table-R1: Inference-Time Scaling for Table Reasoning",
    "url": "http://arxiv.org/abs/2505.23621v1",
    "arxiv_id": "2505.23621v1",
    "authors": [
      "Zheyuan Yang",
      "Lyuhao Chen",
      "Arman Cohan",
      "Yilun Zhao"
    ],
    "published": "2025-05-29T16:28:50+00:00",
    "summary": "In this work, we present the first study to explore inference-time scaling on table reasoning tasks. We develop and evaluate two post-training strategies to enable inference-time scaling: distillation from frontier model reasoning traces and reinforcement learning with verifiable rewards (RLVR). For distillation, we introduce a large-scale dataset of reasoning traces generated by DeepSeek-R1, which we use to fine-tune LLMs into the Table-R1-SFT model. For RLVR, we propose task-specific verifiable reward functions and apply the GRPO algorithm to obtain the Table-R1-Zero model. We evaluate our Table-R1-series models across diverse table reasoning tasks, including short-form QA, fact verification, and free-form QA. Notably, the Table-R1-Zero model matches or exceeds the performance of GPT-4.1 and DeepSeek-R1, while using only a 7B-parameter LLM. It also demonstrates strong generalization to out-of-domain datasets. Extensive ablation and qualitative analyses reveal the benefits of instruction tuning, model architecture choices, and cross-task generalization, as well as emergence of essential table reasoning skills during RL training."
  },
  {
    "title": "LLM Performance for Code Generation on Noisy Tasks",
    "url": "http://arxiv.org/abs/2505.23598v1",
    "arxiv_id": "2505.23598v1",
    "authors": [
      "Radzim Sendyka",
      "Christian Cabrera",
      "Andrei Paleyes",
      "Diana Robinson",
      "Neil Lawrence"
    ],
    "published": "2025-05-29T16:11:18+00:00",
    "summary": "This paper investigates the ability of large language models (LLMs) to recognise and solve tasks which have been obfuscated beyond recognition. Focusing on competitive programming and benchmark tasks (LeetCode and MATH), we compare performance across multiple models and obfuscation methods, such as noise and redaction. We demonstrate that all evaluated LLMs can solve tasks obfuscated to a level where the text would be unintelligible to human readers, and does not contain key pieces of instruction or context. We introduce the concept of eager pattern matching to describe this behaviour, which is not observed in tasks published after the models' knowledge cutoff date, indicating strong memorisation or overfitting to training data, rather than legitimate reasoning about the presented problem. We report empirical evidence of distinct performance decay patterns between contaminated and unseen datasets. We discuss the implications for benchmarking and evaluations of model behaviour, arguing for caution when designing experiments using standard datasets. We also propose measuring the decay of performance under obfuscation as a possible strategy for detecting dataset contamination and highlighting potential safety risks and interpretability issues for automated software systems."
  },
  {
    "title": "SafeScientist: Toward Risk-Aware Scientific Discoveries by LLM Agents",
    "url": "http://arxiv.org/abs/2505.23559v1",
    "arxiv_id": "2505.23559v1",
    "authors": [
      "Kunlun Zhu",
      "Jiaxun Zhang",
      "Ziheng Qi",
      "Nuoxing Shang",
      "Zijia Liu",
      "Peixuan Han",
      "Yue Su",
      "Haofei Yu",
      "Jiaxuan You"
    ],
    "published": "2025-05-29T15:35:58+00:00",
    "summary": "Recent advancements in large language model (LLM) agents have significantly accelerated scientific discovery automation, yet concurrently raised critical ethical and safety concerns. To systematically address these challenges, we introduce \\textbf{SafeScientist}, an innovative AI scientist framework explicitly designed to enhance safety and ethical responsibility in AI-driven scientific exploration. SafeScientist proactively refuses ethically inappropriate or high-risk tasks and rigorously emphasizes safety throughout the research process. To achieve comprehensive safety oversight, we integrate multiple defensive mechanisms, including prompt monitoring, agent-collaboration monitoring, tool-use monitoring, and an ethical reviewer component. Complementing SafeScientist, we propose \\textbf{SciSafetyBench}, a novel benchmark specifically designed to evaluate AI safety in scientific contexts, comprising 240 high-risk scientific tasks across 6 domains, alongside 30 specially designed scientific tools and 120 tool-related risk tasks. Extensive experiments demonstrate that SafeScientist significantly improves safety performance by 35\\% compared to traditional AI scientist frameworks, without compromising scientific output quality. Additionally, we rigorously validate the robustness of our safety pipeline against diverse adversarial attack methods, further confirming the effectiveness of our integrated approach. The code and data will be available at https://github.com/ulab-uiuc/SafeScientist. \\textcolor{red}{Warning: this paper contains example data that may be offensive or harmful.}"
  },
  {
    "title": "Understanding Refusal in Language Models with Sparse Autoencoders",
    "url": "http://arxiv.org/abs/2505.23556v1",
    "arxiv_id": "2505.23556v1",
    "authors": [
      "Wei Jie Yeo",
      "Nirmalendu Prakash",
      "Clement Neo",
      "Roy Ka-Wei Lee",
      "Erik Cambria",
      "Ranjan Satapathy"
    ],
    "published": "2025-05-29T15:33:39+00:00",
    "summary": "Refusal is a key safety behavior in aligned language models, yet the internal mechanisms driving refusals remain opaque. In this work, we conduct a mechanistic study of refusal in instruction-tuned LLMs using sparse autoencoders to identify latent features that causally mediate refusal behaviors. We apply our method to two open-source chat models and intervene on refusal-related features to assess their influence on generation, validating their behavioral impact across multiple harmful datasets. This enables a fine-grained inspection of how refusal manifests at the activation level and addresses key research questions such as investigating upstream-downstream latent relationship and understanding the mechanisms of adversarial jailbreaking techniques. We also establish the usefulness of refusal features in enhancing generalization for linear probes to out-of-distribution adversarial samples in classification tasks. We open source our code in https://github.com/wj210/refusal_sae."
  },
  {
    "title": "LLM-based Property-based Test Generation for Guardrailing Cyber-Physical Systems",
    "url": "http://arxiv.org/abs/2505.23549v1",
    "arxiv_id": "2505.23549v1",
    "authors": [
      "Khashayar Etemadi",
      "Marjan Sirjani",
      "Mahshid Helali Moghadam",
      "Per Strandberg",
      "Paul Pettersson"
    ],
    "published": "2025-05-29T15:27:52+00:00",
    "summary": "Cyber-physical systems (CPSs) are complex systems that integrate physical, computational, and communication subsystems. The heterogeneous nature of these systems makes their safety assurance challenging. In this paper, we propose a novel automated approach for guardrailing cyber-physical systems using property-based tests (PBTs) generated by Large Language Models (LLMs). Our approach employs an LLM to extract properties from the code and documentation of CPSs. Next, we use the LLM to generate PBTs that verify the extracted properties on the CPS. The generated PBTs have two uses. First, they are used to test the CPS before it is deployed, i.e., at design time. Secondly, these PBTs can be used after deployment, i.e., at run time, to monitor the behavior of the system and guardrail it against unsafe states. We implement our approach in ChekProp and conduct preliminary experiments to evaluate the generated PBTs in terms of their relevance (how well they match manually crafted properties), executability (how many run with minimal manual modification), and effectiveness (coverage of the input space partitions). The results of our experiments and evaluation demonstrate a promising path forward for creating guardrails for CPSs using LLM-generated property-based tests."
  },
  {
    "title": "EVOREFUSE: Evolutionary Prompt Optimization for Evaluation and Mitigation of LLM Over-Refusal to Pseudo-Malicious Instructions",
    "url": "http://arxiv.org/abs/2505.23473v1",
    "arxiv_id": "2505.23473v1",
    "authors": [
      "Xiaorui Wu",
      "Xiaofeng Mao",
      "Fei Li",
      "Xin Zhang",
      "Xiaolu Zhang",
      "Jun Zhou",
      "Yuxiang Peng",
      "Li Zheng",
      "Chong Teng",
      "Donghong Ji",
      "Zhuang Li"
    ],
    "published": "2025-05-29T14:26:46+00:00",
    "summary": "Large language models (LLMs) frequently refuse to respond to pseudo-malicious instructions: semantically harmless input queries triggering unnecessary LLM refusals due to conservative safety alignment, significantly impairing user experience. Collecting such instructions is crucial for evaluating and mitigating over-refusals, but existing instruction curation methods, like manual creation or instruction rewriting, either lack scalability or fail to produce sufficiently diverse and effective refusal-inducing prompts. To address these limitations, we introduce EVOREFUSE, a prompt optimization approach that generates diverse pseudo-malicious instructions consistently eliciting confident refusals across LLMs. EVOREFUSE employs an evolutionary algorithm exploring the instruction space in more diverse directions than existing methods via mutation strategies and recombination, and iteratively evolves seed instructions to maximize evidence lower bound on LLM refusal probability. Using EVOREFUSE, we create two novel datasets: EVOREFUSE-TEST, a benchmark of 582 pseudo-malicious instructions that outperforms the next-best benchmark with 140.41% higher average refusal triggering rate across 9 LLMs, 34.86% greater lexical diversity, and 40.03% improved LLM response confidence scores; and EVOREFUSE-ALIGN, which provides 3,000 pseudo-malicious instructions with responses for supervised and preference-based alignment training. LLAMA3.1-8B-INSTRUCT supervisedly fine-tuned on EVOREFUSE-ALIGN achieves up to 14.31% fewer over-refusals than models trained on the second-best alignment dataset, without compromising safety. Our analysis with EVOREFUSE-TEST reveals models trigger over-refusals by overly focusing on sensitive keywords while ignoring broader context."
  },
  {
    "title": "EVOREFUSE: Evolutionary Prompt Optimization for Evaluation and Mitigation of LLM Over-Refusal to Pseudo-Malicious Instructions",
    "url": "http://arxiv.org/abs/2505.23473v2",
    "arxiv_id": "2505.23473v2",
    "authors": [
      "Xiaorui Wu",
      "Xiaofeng Mao",
      "Xin Zhang",
      "Fei Li",
      "Chong Teng",
      "Yuxiang Peng",
      "Li Zheng",
      "Donghong Ji",
      "Zhuang Li"
    ],
    "published": "2025-05-29T14:26:46+00:00",
    "summary": "Large language models (LLMs) frequently refuse to respond to pseudo-malicious instructions: semantically harmless input queries triggering unnecessary LLM refusals due to conservative safety alignment, significantly impairing user experience. Collecting such instructions is crucial for evaluating and mitigating over-refusals, but existing instruction curation methods, like manual creation or instruction rewriting, either lack scalability or fail to produce sufficiently diverse and effective refusal-inducing prompts. To address these limitations, we introduce EVOREFUSE, a prompt optimization approach that generates diverse pseudo-malicious instructions consistently eliciting confident refusals across LLMs. EVOREFUSE employs an evolutionary algorithm exploring the instruction space in more diverse directions than existing methods via mutation strategies and recombination, and iteratively evolves seed instructions to maximize evidence lower bound on LLM refusal probability. Using EVOREFUSE, we create two novel datasets: EVOREFUSE-TEST, a benchmark of 582 pseudo-malicious instructions that outperforms the next-best benchmark with 140.41% higher average refusal triggering rate across 9 LLMs, 34.86% greater lexical diversity, and 40.03% improved LLM response confidence scores; and EVOREFUSE-ALIGN, which provides 3,000 pseudo-malicious instructions with responses for supervised and preference-based alignment training. LLAMA3.1-8B-INSTRUCT supervisedly fine-tuned on EVOREFUSE-ALIGN achieves up to 14.31% fewer over-refusals than models trained on the second-best alignment dataset, without compromising safety. Our analysis with EVOREFUSE-TEST reveals models trigger over-refusals by overly focusing on sensitive keywords while ignoring broader context."
  },
  {
    "title": "Self-driving technologies need the help of the public: A narrative review of the evidence",
    "url": "http://arxiv.org/abs/2505.23472v1",
    "arxiv_id": "2505.23472v1",
    "authors": [
      "Jonathan Smith",
      "Siddartha Khastgir"
    ],
    "published": "2025-05-29T14:26:24+00:00",
    "summary": "If public trust is lot in a new technology early in its life cycle it can take much more time for the benefits of that technology to be realised. Eventually tens-of-millions of people will collectively have the power to determine self-driving technology success of failure driven by their perception of risk, data handling, safety, governance, accountability, benefits to their life and more. This paper reviews the evidence on safety critical technology covering trust, engagement, and acceptance. The paper takes a narrative review approach concluding with a scalable model for self-driving technology education and engagement. The paper find that if a mismatch between the publics perception and expectations about self driving systems emerge it can lead to misuse, disuse, or abuse of the system. Furthermore we find from the evidence that industrial experts often misunderstand what matters to the public, users, and stakeholders. However we find that engagement programmes that develop approaches to defining the right information at the right time, in the right format orientated around what matters to the public creates the potential for ever more sophisticated conversations, greater trust, and moving the public into a progressive more active role of critique and advocacy. This work has been undertaken as part of the Partners for Automated Vehicle Education (PAVE) United Kingdom programme."
  },
  {
    "title": "Bounded-Abstention Pairwise Learning to Rank",
    "url": "http://arxiv.org/abs/2505.23437v1",
    "arxiv_id": "2505.23437v1",
    "authors": [
      "Antonio Ferrara",
      "Andrea Pugnana",
      "Francesco Bonchi",
      "Salvatore Ruggieri"
    ],
    "published": "2025-05-29T13:35:39+00:00",
    "summary": "Ranking systems influence decision-making in high-stakes domains like health, education, and employment, where they can have substantial economic and social impacts. This makes the integration of safety mechanisms essential. One such mechanism is $\\textit{abstention}$, which enables algorithmic decision-making system to defer uncertain or low-confidence decisions to human experts. While abstention have been predominantly explored in the context of classification tasks, its application to other machine learning paradigms remains underexplored. In this paper, we introduce a novel method for abstention in pairwise learning-to-rank tasks. Our approach is based on thresholding the ranker's conditional risk: the system abstains from making a decision when the estimated risk exceeds a predefined threshold. Our contributions are threefold: a theoretical characterization of the optimal abstention strategy, a model-agnostic, plug-in algorithm for constructing abstaining ranking models, and a comprehensive empirical evaluations across multiple datasets, demonstrating the effectiveness of our approach."
  },
  {
    "title": "Diversity-Aware Policy Optimization for Large Language Model Reasoning",
    "url": "http://arxiv.org/abs/2505.23433v1",
    "arxiv_id": "2505.23433v1",
    "authors": [
      "Jian Yao",
      "Ran Cheng",
      "Xingyu Wu",
      "Jibin Wu",
      "Kay Chen Tan"
    ],
    "published": "2025-05-29T13:27:44+00:00",
    "summary": "The reasoning capabilities of large language models (LLMs) have advanced rapidly, particularly following the release of DeepSeek R1, which has inspired a surge of research into data quality and reinforcement learning (RL) algorithms. Despite the pivotal role diversity plays in RL, its influence on LLM reasoning remains largely underexplored. To bridge this gap, this work presents a systematic investigation into the impact of diversity in RL-based training for LLM reasoning, and proposes a novel diversity-aware policy optimization method. Across evaluations on 12 LLMs, we observe a strong positive correlation between the solution diversity and Potential at k (a novel metric quantifying an LLM's reasoning potential) in high-performing models. This finding motivates our method to explicitly promote diversity during RL training. Specifically, we design a token-level diversity and reformulate it into a practical objective, then we selectively apply it to positive samples. Integrated into the R1-zero training framework, our method achieves a 3.5 percent average improvement across four mathematical reasoning benchmarks, while generating more diverse and robust solutions."
  },
  {
    "title": "Adaptive Jailbreaking Strategies Based on the Semantic Understanding Capabilities of Large Language Models",
    "url": "http://arxiv.org/abs/2505.23404v1",
    "arxiv_id": "2505.23404v1",
    "authors": [
      "Mingyu Yu",
      "Wei Wang",
      "Yanjie Wei",
      "Sujuan Qin"
    ],
    "published": "2025-05-29T12:50:57+00:00",
    "summary": "Adversarial attacks on Large Language Models (LLMs) via jailbreaking techniques-methods that circumvent their built-in safety and ethical constraints-have emerged as a critical challenge in AI security. These attacks compromise the reliability of LLMs by exploiting inherent weaknesses in their comprehension capabilities. This paper investigates the efficacy of jailbreaking strategies that are specifically adapted to the diverse levels of understanding exhibited by different LLMs. We propose the Adaptive Jailbreaking Strategies Based on the Semantic Understanding Capabilities of Large Language Models, a novel framework that classifies LLMs into Type I and Type II categories according to their semantic comprehension abilities. For each category, we design tailored jailbreaking strategies aimed at leveraging their vulnerabilities to facilitate successful attacks. Extensive experiments conducted on multiple LLMs demonstrate that our adaptive strategy markedly improves the success rate of jailbreaking. Notably, our approach achieves an exceptional 98.9% success rate in jailbreaking GPT-4o(29 May 2025 release)"
  },
  {
    "title": "CF-DETR: Coarse-to-Fine Transformer for Real-Time Object Detection",
    "url": "http://arxiv.org/abs/2505.23317v1",
    "arxiv_id": "2505.23317v1",
    "authors": [
      "Woojin Shin",
      "Donghwa Kang",
      "Byeongyun Park",
      "Brent Byunghoon Kang",
      "Jinkyu Lee",
      "Hyeongboo Baek"
    ],
    "published": "2025-05-29T10:23:37+00:00",
    "summary": "Detection Transformers (DETR) are increasingly adopted in autonomous vehicle (AV) perception systems due to their superior accuracy over convolutional networks. However, concurrently executing multiple DETR tasks presents significant challenges in meeting firm real-time deadlines (R1) and high accuracy requirements (R2), particularly for safety-critical objects, while navigating the inherent latency-accuracy trade-off under resource constraints. Existing real-time DNN scheduling approaches often treat models generically, failing to leverage Transformer-specific properties for efficient resource allocation. To address these challenges, we propose CF-DETR, an integrated system featuring a novel coarse-to-fine Transformer architecture and a dedicated real-time scheduling framework NPFP**. CF-DETR employs three key strategies (A1: coarse-to-fine inference, A2: selective fine inference, A3: multi-level batch inference) that exploit Transformer properties to dynamically adjust patch granularity and attention scope based on object criticality, aiming to satisfy R2. The NPFP** scheduling framework (A4) orchestrates these adaptive mechanisms A1-A3. It partitions each DETR task into a safety-critical coarse subtask for guaranteed critical object detection within its deadline (ensuring R1), and an optional fine subtask for enhanced overall accuracy (R2), while managing individual and batched execution. Our extensive evaluations on server, GPU-enabled embedded platforms, and actual AV platforms demonstrate that CF-DETR, under an NPFP** policy, successfully meets strict timing guarantees for critical operations and achieves significantly higher overall and critical object detection accuracy compared to existing baselines across diverse AV workloads."
  },
  {
    "title": "TRACE: Trajectory-Constrained Concept Erasure in Diffusion Models",
    "url": "http://arxiv.org/abs/2505.23312v1",
    "arxiv_id": "2505.23312v1",
    "authors": [
      "Finn Carter"
    ],
    "published": "2025-05-29T10:15:22+00:00",
    "summary": "Text-to-image diffusion models have shown unprecedented generative capability, but their ability to produce undesirable concepts (e.g.~pornographic content, sensitive identities, copyrighted styles) poses serious concerns for privacy, fairness, and safety. {Concept erasure} aims to remove or suppress specific concept information in a generative model. In this paper, we introduce \\textbf{TRACE (Trajectory-Constrained Attentional Concept Erasure)}, a novel method to erase targeted concepts from diffusion models while preserving overall generative quality. Our approach combines a rigorous theoretical framework, establishing formal conditions under which a concept can be provably suppressed in the diffusion process, with an effective fine-tuning procedure compatible with both conventional latent diffusion (Stable Diffusion) and emerging rectified flow models (e.g.~FLUX). We first derive a closed-form update to the model's cross-attention layers that removes hidden representations of the target concept. We then introduce a trajectory-aware finetuning objective that steers the denoising process away from the concept only in the late sampling stages, thus maintaining the model's fidelity on unrelated content. Empirically, we evaluate TRACE on multiple benchmarks used in prior concept erasure studies (object classes, celebrity faces, artistic styles, and explicit content from the I2P dataset). TRACE achieves state-of-the-art performance, outperforming recent methods such as ANT, EraseAnything, and MACE in terms of removal efficacy and output quality."
  },
  {
    "title": "Is Ozone a Reliable Proxy for Molecular Oxygen? II. The impact of N$_2$O on the O$_2$-O$_3$ relationship for Earth-like atmospheres",
    "url": "http://arxiv.org/abs/2505.23279v1",
    "arxiv_id": "2505.23279v1",
    "authors": [
      "Thea Kozakis",
      "Jo\u00e3o M. Mendon\u00e7a",
      "Lars A. Buchhave",
      "Luisa M. Lara"
    ],
    "published": "2025-05-29T09:25:30+00:00",
    "summary": "Molecular oxygen (O2) will be an important molecule in the search for biosignatures in terrestrial planetary atmospheres in the coming decades. In particular, O2 combined with a reducing gas is thought to be strong evidence for disequilibrium caused by surface life. However, there are circumstances where it would be very difficult or impossible to detect O2, in which cases it has been suggested that ozone (O3), the photochemical product of O2, could be used instead. Unfortunately, the O2-O3 relationship is highly nonlinear and dependent on the host star, as shown in detail in the first paper in this series. We explore the O2-O3 relationship around G0V-M5V host stars, using climate/photochemistry modeling to simulate atmospheres while varying abundances of O2 and nitrous oxide (N2O). N2O is of particular importance to the O2-O3 relationship not just because it is produced biologically, but because it is the primary source of nitrogen oxides (NOx), which fuel the NOx catalytic cycle which destroys O3, and the smog mechanism that produces O3. We vary the O2 mixing ratio from 0.01-150% present atmospheric level (PAL), and N2O abundances of 10% and 1000% PAL. We find that varying N2O impacts the O2-O3 relationship differently depending strongly on both the host star and the amount of atmospheric O2. Planets orbiting hotter hosts with strong UV fluxes efficiently convert N2O into NOx, often depleting a significant amount of O3 via faster NOx catalytic cycles. However, for cooler hosts and low O2 levels we find that increasing N2O can lead to an increase of overall O3 due to the smog mechanism producing O3 in the lower atmosphere. Variations in O3 result in significant changes in the amount of harmful UV reaching the surfaces of the model planets as well as the strength of the 9.6 $\\mu$m O3 emission spectral feature, demonstrating potential impacts on habitability and future observations."
  },
  {
    "title": "Accelerating RLHF Training with Reward Variance Increase",
    "url": "http://arxiv.org/abs/2505.23247v1",
    "arxiv_id": "2505.23247v1",
    "authors": [
      "Zonglin Yang",
      "Zhexuan Gu",
      "Houduo Qi",
      "Yancheng Yuan"
    ],
    "published": "2025-05-29T08:54:06+00:00",
    "summary": "Reinforcement learning from human feedback (RLHF) is an essential technique for ensuring that large language models (LLMs) are aligned with human values and preferences during the post-training phase. As an effective RLHF approach, group relative policy optimization (GRPO) has demonstrated success in many LLM-based applications. However, efficient GRPO-based RLHF training remains a challenge. Recent studies reveal that a higher reward variance of the initial policy model leads to faster RLHF training. Inspired by this finding, we propose a practical reward adjustment model to accelerate RLHF training by provably increasing the reward variance and preserving the relative preferences and reward expectation. Our reward adjustment method inherently poses a nonconvex optimization problem, which is NP-hard to solve in general. To overcome the computational challenges, we design a novel $O(n \\log n)$ algorithm to find a global solution of the nonconvex reward adjustment model by explicitly characterizing the extreme points of the feasible set. As an important application, we naturally integrate this reward adjustment model into the GRPO algorithm, leading to a more efficient GRPO with reward variance increase (GRPOVI) algorithm for RLHF training. As an interesting byproduct, we provide an indirect explanation for the empirical effectiveness of GRPO with rule-based reward for RLHF training, as demonstrated in DeepSeek-R1. Experiment results demonstrate that the GRPOVI algorithm can significantly improve the RLHF training efficiency compared to the original GRPO algorithm."
  },
  {
    "title": "Latent Representations for Control Design with Provable Stability and Safety Guarantees",
    "url": "http://arxiv.org/abs/2505.23210v1",
    "arxiv_id": "2505.23210v1",
    "authors": [
      "Paul Lutkus",
      "Kaiyuan Wang",
      "Lars Lindemann",
      "Stephen Tu"
    ],
    "published": "2025-05-29T07:52:40+00:00",
    "summary": "We initiate a formal study on the use of low-dimensional latent representations of dynamical systems for verifiable control synthesis. Our main goal is to enable the application of verification techniques -- such as Lyapunov or barrier functions -- that might otherwise be computationally prohibitive when applied directly to the full state representation. Towards this goal, we first provide dynamics-aware approximate conjugacy conditions which formalize the notion of reconstruction error necessary for systems analysis. We then utilize our conjugacy conditions to transfer the stability and invariance guarantees of a latent certificate function (e.g., a Lyapunov or barrier function) for a latent space controller back to the original system. Importantly, our analysis contains several important implications for learning latent spaces and dynamics, by highlighting the necessary geometric properties which need to be preserved by the latent space, in addition to providing concrete loss functions for dynamics reconstruction that are directly related to control design. We conclude by demonstrating the applicability of our theory to two case studies: (1) stabilization of a cartpole system, and (2) collision avoidance for a two vehicle system."
  },
  {
    "title": "UPP: Unified Path Planner with Adaptive Safety and Optimality",
    "url": "http://arxiv.org/abs/2505.23197v1",
    "arxiv_id": "2505.23197v1",
    "authors": [
      "Jatin Kumar Arora",
      "Shubhendu Bhasin"
    ],
    "published": "2025-05-29T07:34:56+00:00",
    "summary": "We are surrounded by robots helping us perform complex tasks. Robots have a wide range of applications, from industrial automation to personalized assistance. However, with great technological innovation come significant challenges. One of the major challenges in robotics is path planning. Despite advancements such as graph search, sampling, and potential field methods, most path planning algorithms focus either on optimality or on safety. Very little research addresses both simultaneously. We propose a Unified Path Planner (UPP) that uses modified heuristics and a dynamic safety cost function to balance safety and optimality. The level of safety can be adjusted via tunable parameters, trading off against computational complexity. We demonstrate the planner's performance in simulations, showing how parameter variation affects results. UPP is compared with various traditional and safe-optimal planning algorithms across different scenarios. We also validate it on a TurtleBot, where the robot successfully finds safe and sub-optimal paths."
  },
  {
    "title": "HMAD: Advancing E2E Driving with Anchored Offset Proposals and Simulation-Supervised Multi-target Scoring",
    "url": "http://arxiv.org/abs/2505.23129v1",
    "arxiv_id": "2505.23129v1",
    "authors": [
      "Bin Wang",
      "Pingjun Li",
      "Jinkun Liu",
      "Jun Cheng",
      "Hailong Lei",
      "Yinze Rong",
      "Huan-ang Gao",
      "Kangliang Chen",
      "Xing Pan",
      "Weihao Gu"
    ],
    "published": "2025-05-29T05:59:24+00:00",
    "summary": "End-to-end autonomous driving faces persistent challenges in both generating diverse, rule-compliant trajectories and robustly selecting the optimal path from these options via learned, multi-faceted evaluation. To address these challenges, we introduce HMAD, a framework integrating a distinctive Bird's-Eye-View (BEV) based trajectory proposal mechanism with learned multi-criteria scoring. HMAD leverages BEVFormer and employs learnable anchored queries, initialized from a trajectory dictionary and refined via iterative offset decoding (inspired by DiffusionDrive), to produce numerous diverse and stable candidate trajectories. A key innovation, our simulation-supervised scorer module, then evaluates these proposals against critical metrics including no at-fault collisions, drivable area compliance, comfortableness, and overall driving quality (i.e., extended PDM score). Demonstrating its efficacy, HMAD achieves a 44.5% driving score on the CVPR 2025 private test set. This work highlights the benefits of effectively decoupling robust trajectory generation from comprehensive, safety-aware learned scoring for advanced autonomous driving."
  },
  {
    "title": "Infi-MMR: Curriculum-based Unlocking Multimodal Reasoning via Phased Reinforcement Learning in Multimodal Small Language Models",
    "url": "http://arxiv.org/abs/2505.23091v1",
    "arxiv_id": "2505.23091v1",
    "authors": [
      "Zeyu Liu",
      "Yuhang Liu",
      "Guanghao Zhu",
      "Congkai Xie",
      "Zhen Li",
      "Jianbo Yuan",
      "Xinyao Wang",
      "Qing Li",
      "Shing-Chi Cheung",
      "Shengyu Zhang",
      "Fei Wu",
      "Hongxia Yang"
    ],
    "published": "2025-05-29T04:51:56+00:00",
    "summary": "Recent advancements in large language models (LLMs) have demonstrated substantial progress in reasoning capabilities, such as DeepSeek-R1, which leverages rule-based reinforcement learning to enhance logical reasoning significantly. However, extending these achievements to multimodal large language models (MLLMs) presents critical challenges, which are frequently more pronounced for Multimodal Small Language Models (MSLMs) given their typically weaker foundational reasoning abilities: (1) the scarcity of high-quality multimodal reasoning datasets, (2) the degradation of reasoning capabilities due to the integration of visual processing, and (3) the risk that direct application of reinforcement learning may produce complex yet incorrect reasoning processes. To address these challenges, we design a novel framework Infi-MMR to systematically unlock the reasoning potential of MSLMs through a curriculum of three carefully structured phases and propose our multimodal reasoning model Infi-MMR-3B. The first phase, Foundational Reasoning Activation, leverages high-quality textual reasoning datasets to activate and strengthen the model's logical reasoning capabilities. The second phase, Cross-Modal Reasoning Adaptation, utilizes caption-augmented multimodal data to facilitate the progressive transfer of reasoning skills to multimodal contexts. The third phase, Multimodal Reasoning Enhancement, employs curated, caption-free multimodal data to mitigate linguistic biases and promote robust cross-modal reasoning. Infi-MMR-3B achieves both state-of-the-art multimodal math reasoning ability (43.68% on MathVerse testmini, 27.04% on MathVision test, and 21.33% on OlympiadBench) and general reasoning ability (67.2% on MathVista testmini)."
  },
  {
    "title": "Second Opinion Matters: Towards Adaptive Clinical AI via the Consensus of Expert Model Ensemble",
    "url": "http://arxiv.org/abs/2505.23075v1",
    "arxiv_id": "2505.23075v1",
    "authors": [
      "Amit Kumthekar",
      "Zion Tilley",
      "Henry Duong",
      "Bhargav Patel",
      "Michael Magnoli",
      "Ahmed Omar",
      "Ahmed Nasser",
      "Chaitanya Gharpure",
      "Yevgen Reztzov"
    ],
    "published": "2025-05-29T04:29:22+00:00",
    "summary": "Despite the growing clinical adoption of large language models (LLMs), current approaches heavily rely on single model architectures. To overcome risks of obsolescence and rigid dependence on single model systems, we present a novel framework, termed the Consensus Mechanism. Mimicking clinical triage and multidisciplinary clinical decision-making, the Consensus Mechanism implements an ensemble of specialized medical expert agents enabling improved clinical decision making while maintaining robust adaptability. This architecture enables the Consensus Mechanism to be optimized for cost, latency, or performance, purely based on its interior model configuration.   To rigorously evaluate the Consensus Mechanism, we employed three medical evaluation benchmarks: MedMCQA, MedQA, and MedXpertQA Text, and the differential diagnosis dataset, DDX+. On MedXpertQA, the Consensus Mechanism achieved an accuracy of 61.0% compared to 53.5% and 45.9% for OpenAI's O3 and Google's Gemini 2.5 Pro. Improvement was consistent across benchmarks with an increase in accuracy on MedQA ($\\Delta\\mathrm{Accuracy}_{\\mathrm{consensus\\text{-}O3}} = 3.4\\%$) and MedMCQA ($\\Delta\\mathrm{Accuracy}_{\\mathrm{consensus\\text{-}O3}} = 9.1\\%$). These accuracy gains extended to differential diagnosis generation, where our system demonstrated improved recall and precision (F1$_\\mathrm{consensus}$ = 0.326 vs. F1$_{\\mathrm{O3\\text{-}high}}$ = 0.2886) and a higher top-1 accuracy for DDX (Top1$_\\mathrm{consensus}$ = 52.0% vs. Top1$_{\\mathrm{O3\\text{-}high}}$ = 45.2%)."
  },
  {
    "title": "Case-Based Reasoning Enhances the Predictive Power of LLMs in Drug-Drug Interaction",
    "url": "http://arxiv.org/abs/2505.23034v1",
    "arxiv_id": "2505.23034v1",
    "authors": [
      "Guangyi Liu",
      "Yongqi Zhang",
      "Xunyuan Liu",
      "Quanming Yao"
    ],
    "published": "2025-05-29T03:20:53+00:00",
    "summary": "Drug-drug interaction (DDI) prediction is critical for treatment safety. While large language models (LLMs) show promise in pharmaceutical tasks, their effectiveness in DDI prediction remains challenging. Inspired by the well-established clinical practice where physicians routinely reference similar historical cases to guide their decisions through case-based reasoning (CBR), we propose CBR-DDI, a novel framework that distills pharmacological principles from historical cases to improve LLM reasoning for DDI tasks. CBR-DDI constructs a knowledge repository by leveraging LLMs to extract pharmacological insights and graph neural networks (GNNs) to model drug associations. A hybrid retrieval mechanism and dual-layer knowledge-enhanced prompting allow LLMs to effectively retrieve and reuse relevant cases. We further introduce a representative sampling strategy for dynamic case refinement. Extensive experiments demonstrate that CBR-DDI achieves state-of-the-art performance, with a significant 28.7% accuracy improvement over both popular LLMs and CBR baseline, while maintaining high interpretability and flexibility."
  },
  {
    "title": "AgentAlign: Navigating Safety Alignment in the Shift from Informative to Agentic Large Language Models",
    "url": "http://arxiv.org/abs/2505.23020v1",
    "arxiv_id": "2505.23020v1",
    "authors": [
      "Jinchuan Zhang",
      "Lu Yin",
      "Yan Zhou",
      "Songlin Hu"
    ],
    "published": "2025-05-29T03:02:18+00:00",
    "summary": "The acquisition of agentic capabilities has transformed LLMs from \"knowledge providers\" to \"action executors\", a trend that while expanding LLMs' capability boundaries, significantly increases their susceptibility to malicious use. Previous work has shown that current LLM-based agents execute numerous malicious tasks even without being attacked, indicating a deficiency in agentic use safety alignment during the post-training phase. To address this gap, we propose AgentAlign, a novel framework that leverages abstract behavior chains as a medium for safety alignment data synthesis. By instantiating these behavior chains in simulated environments with diverse tool instances, our framework enables the generation of highly authentic and executable instructions while capturing complex multi-step dynamics. The framework further ensures model utility by proportionally synthesizing benign instructions through non-malicious interpretations of behavior chains, precisely calibrating the boundary between helpfulness and harmlessness. Evaluation results on AgentHarm demonstrate that fine-tuning three families of open-source models using our method substantially improves their safety (35.8% to 79.5% improvement) while minimally impacting or even positively enhancing their helpfulness, outperforming various prompting methods. The dataset and code have both been open-sourced."
  },
  {
    "title": "Structural Abstraction and Selective Refinement for Formal Verification",
    "url": "http://arxiv.org/abs/2505.22982v1",
    "arxiv_id": "2505.22982v1",
    "authors": [
      "Christoph Luckeneder",
      "Ralph Hoch",
      "Hermann Kaindl"
    ],
    "published": "2025-05-29T01:44:47+00:00",
    "summary": "Safety verification of robot applications is extremely challenging due to the complexity of the environment that a robot typically operates in. Formal verification with model-checking provides guarantees but it may often take too long or even fail for complex models of the environment. A usual solution approach is abstraction, more precisely behavioral abstraction. Our new approach introduces structural abstraction instead, which we investigated in the context of voxel representation of the robot environment. This kind of abstraction leads to abstract voxels. We also propose a complete and automated verification workflow, which is based on an already existing methodology for robot applications, and inspired by the key ideas behind counterexample-guided abstraction refinement (CEGAR) - performing an initial abstraction and successively introducing refinements based on counterexamples, intertwined with model-checker runs. Hence, our approach uses selective refinement of structural abstractions to improve the runtime efficiency of model-checking. A fully-automated implementation of our approach showed its feasibility, since counterexamples have been found for a realistic scenario with a fairly high (maximal) resolution in a few minutes, while direct model-checker runs led to a crash after a couple of days."
  },
  {
    "title": "Evaluating Driver Perceptions of Integrated Safety Monitoring Systems for Alcohol Impairment and Distraction",
    "url": "http://arxiv.org/abs/2505.22969v1",
    "arxiv_id": "2505.22969v1",
    "authors": [
      "RoshikNagaSai Patibandla",
      "Ross Greer"
    ],
    "published": "2025-05-29T01:12:50+00:00",
    "summary": "The increasing number of accidents caused by alcohol-impaired driving has prompted the development of integrated safety systems in vehicles to monitor driver behavior and prevent crashes. This paper explores how drivers perceive these systems, focusing on their comfort, trust, privacy concerns, and willingness to adopt the technology. Through a survey of 115 U.S. participants, the study reveals a preference for non-intrusive systems, such as those monitoring eye movements, over more restrictive technologies like alcohol detection devices. Privacy emerged as a major concern, with many participants preferring local data processing and anonymity. Trust in these systems was crucial for acceptance, as drivers are more likely to adapt their behavior when they believe the system is accurate and reliable. To encourage adoption, it is important to address concerns about privacy and balance the benefits of safety with personal freedom. By improving transparency, ensuring reliability, and increasing public awareness, these systems could play a significant role in reducing road accidents and improving safety."
  },
  {
    "title": "MermaidFlow: Redefining Agentic Workflow Generation via Safety-Constrained Evolutionary Programming",
    "url": "http://arxiv.org/abs/2505.22967v1",
    "arxiv_id": "2505.22967v1",
    "authors": [
      "Chengqi Zheng",
      "Jianda Chen",
      "Yueming Lyu",
      "Wen Zheng Terence Ng",
      "Haopeng Zhang",
      "Yew-Soon Ong",
      "Ivor Tsang",
      "Haiyan Yin"
    ],
    "published": "2025-05-29T01:08:36+00:00",
    "summary": "Despite the promise of autonomous agentic reasoning, existing workflow generation methods frequently produce fragile, unexecutable plans due to unconstrained LLM-driven construction. We introduce MermaidFlow, a framework that redefines the agentic search space through safety-constrained graph evolution. At its core, MermaidFlow represent workflows as a verifiable intermediate representation using Mermaid, a structured and human-interpretable graph language. We formulate domain-aware evolutionary operators, i.e., crossover, mutation, insertion, and deletion, to preserve semantic correctness while promoting structural diversity, enabling efficient exploration of a high-quality, statically verifiable workflow space. Without modifying task settings or evaluation protocols, MermaidFlow achieves consistent improvements in success rates and faster convergence to executable plans on the agent reasoning benchmark. The experimental results demonstrate that safety-constrained graph evolution offers a scalable, modular foundation for robust and interpretable agentic reasoning systems."
  },
  {
    "title": "Revisiting Multi-Agent Debate as Test-Time Scaling: A Systematic Study of Conditional Effectiveness",
    "url": "http://arxiv.org/abs/2505.22960v1",
    "arxiv_id": "2505.22960v1",
    "authors": [
      "Yongjin Yang",
      "Euiin Yi",
      "Jongwoo Ko",
      "Kimin Lee",
      "Zhijing Jin",
      "Se-Young Yun"
    ],
    "published": "2025-05-29T01:02:55+00:00",
    "summary": "The remarkable growth in large language model (LLM) capabilities has spurred exploration into multi-agent systems, with debate frameworks emerging as a promising avenue for enhanced problem-solving. These multi-agent debate (MAD) approaches, where agents collaboratively present, critique, and refine arguments, potentially offer improved reasoning, robustness, and diverse perspectives over monolithic models. Despite prior studies leveraging MAD, a systematic understanding of its effectiveness compared to self-agent methods, particularly under varying conditions, remains elusive. This paper seeks to fill this gap by conceptualizing MAD as a test-time computational scaling technique, distinguished by collaborative refinement and diverse exploration capabilities. We conduct a comprehensive empirical investigation comparing MAD with strong self-agent test-time scaling baselines on mathematical reasoning and safety-related tasks. Our study systematically examines the influence of task difficulty, model scale, and agent diversity on MAD's performance. Key findings reveal that, for mathematical reasoning, MAD offers limited advantages over self-agent scaling but becomes more effective with increased problem difficulty and decreased model capability, while agent diversity shows little benefit. Conversely, for safety tasks, MAD's collaborative refinement can increase vulnerability, but incorporating diverse agent configurations facilitates a gradual reduction in attack success through the collaborative refinement process. We believe our findings provide critical guidance for the future development of more effective and strategically deployed MAD systems."
  },
  {
    "title": "LLM-based HSE Compliance Assessment: Benchmark, Performance, and Advancements",
    "url": "http://arxiv.org/abs/2505.22959v1",
    "arxiv_id": "2505.22959v1",
    "authors": [
      "Jianwei Wang",
      "Mengqi Wang",
      "Yinsi Zhou",
      "Zhenchang Xing",
      "Qing Liu",
      "Xiwei Xu",
      "Wenjie Zhang",
      "Liming Zhu"
    ],
    "published": "2025-05-29T01:02:53+00:00",
    "summary": "Health, Safety, and Environment (HSE) compliance assessment demands dynamic real-time decision-making under complicated regulations and complex human-machine-environment interactions. While large language models (LLMs) hold significant potential for decision intelligence and contextual dialogue, their capacity for domain-specific knowledge in HSE and structured legal reasoning remains underexplored. We introduce HSE-Bench, the first benchmark dataset designed to evaluate the HSE compliance assessment capabilities of LLM. HSE-Bench comprises over 1,000 manually curated questions drawn from regulations, court cases, safety exams, and fieldwork videos, and integrates a reasoning flow based on Issue spotting, rule Recall, rule Application, and rule Conclusion (IRAC) to assess the holistic reasoning pipeline. We conduct extensive evaluations on different prompting strategies and more than 10 LLMs, including foundation models, reasoning models and multimodal vision models. The results show that, although current LLMs achieve good performance, their capabilities largely rely on semantic matching rather than principled reasoning grounded in the underlying HSE compliance context. Moreover, their native reasoning trace lacks the systematic legal reasoning required for rigorous HSE compliance assessment. To alleviate these, we propose a new prompting technique, Reasoning of Expert (RoE), which guides LLMs to simulate the reasoning process of different experts for compliance assessment and reach a more accurate unified decision. We hope our study highlights reasoning gaps in LLMs for HSE compliance and inspires further research on related tasks."
  },
  {
    "title": "Darwin Godel Machine: Open-Ended Evolution of Self-Improving Agents",
    "url": "http://arxiv.org/abs/2505.22954v1",
    "arxiv_id": "2505.22954v1",
    "authors": [
      "Jenny Zhang",
      "Shengran Hu",
      "Cong Lu",
      "Robert Lange",
      "Jeff Clune"
    ],
    "published": "2025-05-29T00:26:15+00:00",
    "summary": "Today's AI systems have human-designed, fixed architectures and cannot autonomously and continuously improve themselves. The advance of AI could itself be automated. If done safely, that would accelerate AI development and allow us to reap its benefits much sooner. Meta-learning can automate the discovery of novel algorithms, but is limited by first-order improvements and the human design of a suitable search space. The G\\\"odel machine proposed a theoretical alternative: a self-improving AI that repeatedly modifies itself in a provably beneficial manner. Unfortunately, proving that most changes are net beneficial is impossible in practice. We introduce the Darwin G\\\"odel Machine (DGM), a self-improving system that iteratively modifies its own code (thereby also improving its ability to modify its own codebase) and empirically validates each change using coding benchmarks. Inspired by Darwinian evolution and open-endedness research, the DGM maintains an archive of generated coding agents. It grows the archive by sampling an agent from it and using a foundation model to create a new, interesting, version of the sampled agent. This open-ended exploration forms a growing tree of diverse, high-quality agents and allows the parallel exploration of many different paths through the search space. Empirically, the DGM automatically improves its coding capabilities (e.g., better code editing tools, long-context window management, peer-review mechanisms), increasing performance on SWE-bench from 20.0% to 50.0%, and on Polyglot from 14.2% to 30.7%. Furthermore, the DGM significantly outperforms baselines without self-improvement or open-ended exploration. All experiments were done with safety precautions (e.g., sandboxing, human oversight). The DGM is a significant step toward self-improving AI, capable of gathering its own stepping stones along paths that unfold into endless innovation."
  },
  {
    "title": "Semantic Exploration and Dense Mapping of Complex Environments using Ground Robots Equipped with LiDAR and Panoramic Camera",
    "url": "http://arxiv.org/abs/2505.22880v1",
    "arxiv_id": "2505.22880v1",
    "authors": [
      "Xiaoyang Zhan",
      "Shixin Zhou",
      "Qianqian Yang",
      "Yixuan Zhao",
      "Hao Liu",
      "Srinivas Chowdary Ramineni",
      "Kenji Shimada"
    ],
    "published": "2025-05-28T21:27:32+00:00",
    "summary": "This paper presents a system for autonomous semantic exploration and dense semantic target mapping of a complex unknown environment using a ground robot equipped with a LiDAR-panoramic camera suite. Existing approaches often struggle to balance collecting high-quality observations from multiple view angles and avoiding unnecessary repetitive traversal. To fill this gap, we propose a complete system combining mapping and planning. We first redefine the task as completing both geometric coverage and semantic viewpoint observation. We then manage semantic and geometric viewpoints separately and propose a novel Priority-driven Decoupled Local Sampler to generate local viewpoint sets. This enables explicit multi-view semantic inspection and voxel coverage without unnecessary repetition. Building on this, we develop a hierarchical planner to ensure efficient global coverage. In addition, we propose a Safe Aggressive Exploration State Machine, which allows aggressive exploration behavior while ensuring the robot's safety. Our system includes a plug-and-play semantic target mapping module that integrates seamlessly with state-of-the-art SLAM algorithms for pointcloud-level dense semantic target mapping. We validate our approach through extensive experiments in both realistic simulations and complex real-world environments. Simulation results show that our planner achieves faster exploration and shorter travel distances while guaranteeing a specified number of multi-view inspections. Real-world experiments further confirm the system's effectiveness in achieving accurate dense semantic object mapping of unstructured environments."
  },
  {
    "title": "Bridging Distribution Shift and AI Safety: Conceptual and Methodological Synergies",
    "url": "http://arxiv.org/abs/2505.22829v1",
    "arxiv_id": "2505.22829v1",
    "authors": [
      "Chenruo Liu",
      "Kenan Tang",
      "Yao Qin",
      "Qi Lei"
    ],
    "published": "2025-05-28T20:11:30+00:00",
    "summary": "This paper bridges distribution shift and AI safety through a comprehensive analysis of their conceptual and methodological synergies. While prior discussions often focus on narrow cases or informal analogies, we establish two types connections between specific causes of distribution shift and fine-grained AI safety issues: (1) methods addressing a specific shift type can help achieve corresponding safety goals, or (2) certain shifts and safety issues can be formally reduced to each other, enabling mutual adaptation of their methods. Our findings provide a unified perspective that encourages fundamental integration between distribution shift and AI safety research."
  },
  {
    "title": "Constrained Hamiltonian Systems on Observation-Induced Fiber Bundles: Theory of Symmetry and Integrability",
    "url": "http://arxiv.org/abs/2505.22824v1",
    "arxiv_id": "2505.22824v1",
    "authors": [
      "Dongzhe Zheng"
    ],
    "published": "2025-05-28T20:09:45+00:00",
    "summary": "Classical constrained Hamiltonian theory assumes complete observability of system states, but in reality only partial state information is often available. This paper establishes a complete geometric theoretical framework for handling such incompletely observed systems. By introducing the concept of observation-induced fiber bundles, we naturally extend Dirac constraint theory to the fiber bundle setting, unifying the treatment of state constraints and observation constraints. Main results include: (1) Classification of existence conditions for observation fiber bundles based on characteristic class theory; (2) Complete characterization of Poisson structures on fiber bundles and corresponding symplectic reduction theory; (3) Geometric necessary and sufficient conditions for integrability and Lax pair construction; (4) Extension of Noether's theorem under symmetry group actions. The theoretical framework naturally encompasses a wide range of applications from classical mechanics to modern safety-critical control systems, providing a rigorous mathematical foundation for dynamical analysis under incomplete information."
  },
  {
    "title": "A Contingency Model Predictive Control Framework for Safe Learning",
    "url": "http://arxiv.org/abs/2505.22776v1",
    "arxiv_id": "2505.22776v1",
    "authors": [
      "Merlijne Geurts",
      "Tren Baltussen",
      "Alexander Katriniok",
      "Maurice Heemels"
    ],
    "published": "2025-05-28T18:45:23+00:00",
    "summary": "This research introduces a multi-horizon contingency model predictive control (CMPC) framework in which classes of robust MPC (RMPC) algorithms are combined with classes of learning-based MPC (LB-MPC) algorithms to enable safe learning. We prove that the CMPC framework inherits the robust recursive feasibility properties of the underlying RMPC scheme, thereby ensuring safety of the CMPC in the sense of constraint satisfaction. The CMPC leverages the LB-MPC to safely learn the unmodeled dynamics to reduce conservatism and improve performance compared to standalone RMPC schemes, which are conservative in nature. In addition, we present an implementation of the CMPC framework that combines a particular RMPC and a Gaussian Process MPC scheme. A simulation study on automated lane merging demonstrates the advantages of our general CMPC framework."
  },
  {
    "title": "Sherlock: Self-Correcting Reasoning in Vision-Language Models",
    "url": "http://arxiv.org/abs/2505.22651v1",
    "arxiv_id": "2505.22651v1",
    "authors": [
      "Yi Ding",
      "Ruqi Zhang"
    ],
    "published": "2025-05-28T17:58:03+00:00",
    "summary": "Reasoning Vision-Language Models (VLMs) have shown promising performance on complex multimodal tasks. However, they still face significant challenges: they are highly sensitive to reasoning errors, require large volumes of annotated data or accurate verifiers, and struggle to generalize beyond specific domains. To address these limitations, we explore self-correction as a strategy to enhance reasoning VLMs. We first conduct an in-depth analysis of reasoning VLMs' self-correction abilities and identify key gaps. Based on our findings, we introduce Sherlock, a self-correction and self-improvement training framework. Sherlock introduces a trajectory-level self-correction objective, a preference data construction method based on visual perturbation, and a dynamic $\\beta$ for preference tuning. Once the model acquires self-correction capabilities using only 20k randomly sampled annotated data, it continues to self-improve without external supervision. Built on the Llama3.2-Vision-11B model, Sherlock achieves remarkable results across eight benchmarks, reaching an average accuracy of 64.1 with direct generation and 65.4 after self-correction. It outperforms LLaVA-CoT (63.2), Mulberry (63.9), and LlamaV-o1 (63.4) while using less than 20% of the annotated data."
  },
  {
    "title": "SimProcess: High Fidelity Simulation of Noisy ICS Physical Processes",
    "url": "http://arxiv.org/abs/2505.22638v1",
    "arxiv_id": "2505.22638v1",
    "authors": [
      "Denis Donadel",
      "Gabriele Crestanello",
      "Giulio Morandini",
      "Daniele Antonioli",
      "Mauro Conti",
      "Massimo Merro"
    ],
    "published": "2025-05-28T17:54:23+00:00",
    "summary": "Industrial Control Systems (ICS) manage critical infrastructures like power grids and water treatment plants. Cyberattacks on ICSs can disrupt operations, causing severe economic, environmental, and safety issues. For example, undetected pollution in a water plant can put the lives of thousands at stake. ICS researchers have increasingly turned to honeypots -- decoy systems designed to attract attackers, study their behaviors, and eventually improve defensive mechanisms. However, existing ICS honeypots struggle to replicate the ICS physical process, making them susceptible to detection. Accurately simulating the noise in ICS physical processes is challenging because different factors produce it, including sensor imperfections and external interferences.   In this paper, we propose SimProcess, a novel framework to rank the fidelity of ICS simulations by evaluating how closely they resemble real-world and noisy physical processes. It measures the simulation distance from a target system by estimating the noise distribution with machine learning models like Random Forest. Unlike existing solutions that require detailed mathematical models or are limited to simple systems, SimProcess operates with only a timeseries of measurements from the real system, making it applicable to a broader range of complex dynamic systems. We demonstrate the framework's effectiveness through a case study using real-world power grid data from the EPIC testbed. We compare the performance of various simulation methods, including static and generative noise techniques. Our model correctly classifies real samples with a recall of up to 1.0. It also identifies Gaussian and Gaussian Mixture as the best distribution to simulate our power systems, together with a generative solution provided by an autoencoder, thereby helping developers to improve honeypot fidelity. Additionally, we make our code publicly available."
  },
  {
    "title": "Emotion-o1: Adaptive Long Reasoning for Emotion Understanding in LLMs",
    "url": "http://arxiv.org/abs/2505.22548v1",
    "arxiv_id": "2505.22548v1",
    "authors": [
      "Changhao Song",
      "Yazhou Zhang",
      "Peng Zhang"
    ],
    "published": "2025-05-28T16:32:16+00:00",
    "summary": "Emotion understanding includes basic tasks (e.g., sentiment/emotion classification) and advanced tasks (e.g., sarcasm/humor detection). Current methods rely on fixed-length CoT reasoning, failing to adapt to the varying complexity of emotions. We propose a task-adaptive reasoning framework that employs DeepSeek-R1 to generate variable-length reasoning chains for different emotion tasks. By combining fine-tuning with reinforcement learning, we design a composite reward function that balances four objectives: prediction accuracy, adaptive reasoning depth control, structural diversity in reasoning paths, and suppression of repetitive logic. This approach achieves dynamic context-sensitive inference while enabling LLMs to autonomously develop deep reasoning capabilities. Experimental results demonstrate consistent improvements in both Acc and F1 scores across four tasks: emotion, sentiment, humor, and sarcasm. Notably, peak enhancements reached 3.56% F1 (2.76% Acc) for basic tasks and 37.95% F1 (23.14% Acc) for advanced tasks. Our work bridges rigid CoT reasoning and emotional complexity through adaptive-depth analysis."
  },
  {
    "title": "GeoDrive: 3D Geometry-Informed Driving World Model with Precise Action Control",
    "url": "http://arxiv.org/abs/2505.22421v1",
    "arxiv_id": "2505.22421v1",
    "authors": [
      "Anthony Chen",
      "Wenzhao Zheng",
      "Yida Wang",
      "Xueyang Zhang",
      "Kun Zhan",
      "Peng Jia",
      "Kurt Keutzer",
      "Shangbang Zhang"
    ],
    "published": "2025-05-28T14:46:51+00:00",
    "summary": "Recent advancements in world models have revolutionized dynamic environment simulation, allowing systems to foresee future states and assess potential actions. In autonomous driving, these capabilities help vehicles anticipate the behavior of other road users, perform risk-aware planning, accelerate training in simulation, and adapt to novel scenarios, thereby enhancing safety and reliability. Current approaches exhibit deficiencies in maintaining robust 3D geometric consistency or accumulating artifacts during occlusion handling, both critical for reliable safety assessment in autonomous navigation tasks. To address this, we introduce GeoDrive, which explicitly integrates robust 3D geometry conditions into driving world models to enhance spatial understanding and action controllability. Specifically, we first extract a 3D representation from the input frame and then obtain its 2D rendering based on the user-specified ego-car trajectory. To enable dynamic modeling, we propose a dynamic editing module during training to enhance the renderings by editing the positions of the vehicles. Extensive experiments demonstrate that our method significantly outperforms existing models in both action accuracy and 3D spatial awareness, leading to more realistic, adaptable, and reliable scene modeling for safer autonomous driving. Additionally, our model can generalize to novel trajectories and offers interactive scene editing capabilities, such as object editing and object trajectory control."
  },
  {
    "title": "GeoDrive: 3D Geometry-Informed Driving World Model with Precise Action Control",
    "url": "http://arxiv.org/abs/2505.22421v2",
    "arxiv_id": "2505.22421v2",
    "authors": [
      "Anthony Chen",
      "Wenzhao Zheng",
      "Yida Wang",
      "Xueyang Zhang",
      "Kun Zhan",
      "Peng Jia",
      "Kurt Keutzer",
      "Shanghang Zhang"
    ],
    "published": "2025-05-28T14:46:51+00:00",
    "summary": "Recent advancements in world models have revolutionized dynamic environment simulation, allowing systems to foresee future states and assess potential actions. In autonomous driving, these capabilities help vehicles anticipate the behavior of other road users, perform risk-aware planning, accelerate training in simulation, and adapt to novel scenarios, thereby enhancing safety and reliability. Current approaches exhibit deficiencies in maintaining robust 3D geometric consistency or accumulating artifacts during occlusion handling, both critical for reliable safety assessment in autonomous navigation tasks. To address this, we introduce GeoDrive, which explicitly integrates robust 3D geometry conditions into driving world models to enhance spatial understanding and action controllability. Specifically, we first extract a 3D representation from the input frame and then obtain its 2D rendering based on the user-specified ego-car trajectory. To enable dynamic modeling, we propose a dynamic editing module during training to enhance the renderings by editing the positions of the vehicles. Extensive experiments demonstrate that our method significantly outperforms existing models in both action accuracy and 3D spatial awareness, leading to more realistic, adaptable, and reliable scene modeling for safer autonomous driving. Additionally, our model can generalize to novel trajectories and offers interactive scene editing capabilities, such as object editing and object trajectory control."
  },
  {
    "title": "Mitigating Overthinking in Large Reasoning Models via Manifold Steering",
    "url": "http://arxiv.org/abs/2505.22411v1",
    "arxiv_id": "2505.22411v1",
    "authors": [
      "Yao Huang",
      "Huanran Chen",
      "Shouwei Ruan",
      "Yichi Zhang",
      "Xingxing Wei",
      "Yinpeng Dong"
    ],
    "published": "2025-05-28T14:39:26+00:00",
    "summary": "Recent advances in Large Reasoning Models (LRMs) have demonstrated remarkable capabilities in solving complex tasks such as mathematics and coding. However, these models frequently exhibit a phenomenon known as overthinking during inference, characterized by excessive validation loops and redundant deliberation, leading to substantial computational overheads. In this paper, we aim to mitigate overthinking by investigating the underlying mechanisms from the perspective of mechanistic interpretability. We first showcase that the tendency of overthinking can be effectively captured by a single direction in the model's activation space and the issue can be eased by intervening the activations along this direction. However, this efficacy soon reaches a plateau and even deteriorates as the intervention strength increases. We therefore systematically explore the activation space and find that the overthinking phenomenon is actually tied to a low-dimensional manifold, which indicates that the limited effect stems from the noises introduced by the high-dimensional steering direction. Based on this insight, we propose Manifold Steering, a novel approach that elegantly projects the steering direction onto the low-dimensional activation manifold given the theoretical approximation of the interference noise. Extensive experiments on DeepSeek-R1 distilled models validate that our method reduces output tokens by up to 71% while maintaining and even improving the accuracy on several mathematical benchmarks. Our method also exhibits robust cross-domain transferability, delivering consistent token reduction performance in code generation and knowledge-based QA tasks. Code is available at: https://github.com/Aries-iai/Manifold_Steering."
  },
  {
    "title": "Suitability Filter: A Statistical Framework for Classifier Evaluation in Real-World Deployment Settings",
    "url": "http://arxiv.org/abs/2505.22356v1",
    "arxiv_id": "2505.22356v1",
    "authors": [
      "Ang\u00e9line Pouget",
      "Mohammad Yaghini",
      "Stephan Rabanser",
      "Nicolas Papernot"
    ],
    "published": "2025-05-28T13:37:04+00:00",
    "summary": "Deploying machine learning models in safety-critical domains poses a key challenge: ensuring reliable model performance on downstream user data without access to ground truth labels for direct validation. We propose the suitability filter, a novel framework designed to detect performance deterioration by utilizing suitability signals -- model output features that are sensitive to covariate shifts and indicative of potential prediction errors. The suitability filter evaluates whether classifier accuracy on unlabeled user data shows significant degradation compared to the accuracy measured on the labeled test dataset. Specifically, it ensures that this degradation does not exceed a pre-specified margin, which represents the maximum acceptable drop in accuracy. To achieve reliable performance evaluation, we aggregate suitability signals for both test and user data and compare these empirical distributions using statistical hypothesis testing, thus providing insights into decision uncertainty. Our modular method adapts to various models and domains. Empirical evaluations across different classification tasks demonstrate that the suitability filter reliably detects performance deviations due to covariate shift. This enables proactive mitigation of potential failures in high-stakes applications."
  },
  {
    "title": "Skywork Open Reasoner 1 Technical Report",
    "url": "http://arxiv.org/abs/2505.22312v1",
    "arxiv_id": "2505.22312v1",
    "authors": [
      "Jujie He",
      "Jiacai Liu",
      "Chris Yuhao Liu",
      "Rui Yan",
      "Chaojie Wang",
      "Peng Cheng",
      "Xiaoyu Zhang",
      "Fuxiang Zhang",
      "Jiacheng Xu",
      "Wei Shen",
      "Siyuan Li",
      "Liang Zeng",
      "Tianwen Wei",
      "Cheng Cheng",
      "Bo An",
      "Yang Liu",
      "Yahui Zhou"
    ],
    "published": "2025-05-28T12:56:04+00:00",
    "summary": "The success of DeepSeek-R1 underscores the significant role of reinforcement learning (RL) in enhancing the reasoning capabilities of large language models (LLMs). In this work, we present Skywork-OR1, an effective and scalable RL implementation for long Chain-of-Thought (CoT) models. Building on the DeepSeek-R1-Distill model series, our RL approach achieves notable performance gains, increasing average accuracy across AIME24, AIME25, and LiveCodeBench from 57.8% to 72.8% (+15.0%) for the 32B model and from 43.6% to 57.5% (+13.9%) for the 7B model. Our Skywork-OR1-32B model surpasses both DeepSeek-R1 and Qwen3-32B on the AIME24 and AIME25 benchmarks, while achieving comparable results on LiveCodeBench. The Skywork-OR1-7B and Skywork-OR1-Math-7B models demonstrate competitive reasoning capabilities among models of similar size. We perform comprehensive ablation studies on the core components of our training pipeline to validate their effectiveness. Additionally, we thoroughly investigate the phenomenon of entropy collapse, identify key factors affecting entropy dynamics, and demonstrate that mitigating premature entropy collapse is critical for improved test performance. To support community research, we fully open-source our model weights, training code, and training datasets."
  },
  {
    "title": "Skywork Open Reasoner 1 Technical Report",
    "url": "http://arxiv.org/abs/2505.22312v2",
    "arxiv_id": "2505.22312v2",
    "authors": [
      "Jujie He",
      "Jiacai Liu",
      "Chris Yuhao Liu",
      "Rui Yan",
      "Chaojie Wang",
      "Peng Cheng",
      "Xiaoyu Zhang",
      "Fuxiang Zhang",
      "Jiacheng Xu",
      "Wei Shen",
      "Siyuan Li",
      "Liang Zeng",
      "Tianwen Wei",
      "Cheng Cheng",
      "Bo An",
      "Yang Liu",
      "Yahui Zhou"
    ],
    "published": "2025-05-28T12:56:04+00:00",
    "summary": "The success of DeepSeek-R1 underscores the significant role of reinforcement learning (RL) in enhancing the reasoning capabilities of large language models (LLMs). In this work, we present Skywork-OR1, an effective and scalable RL implementation for long Chain-of-Thought (CoT) models. Building on the DeepSeek-R1-Distill model series, our RL approach achieves notable performance gains, increasing average accuracy across AIME24, AIME25, and LiveCodeBench from 57.8% to 72.8% (+15.0%) for the 32B model and from 43.6% to 57.5% (+13.9%) for the 7B model. Our Skywork-OR1-32B model surpasses both DeepSeek-R1 and Qwen3-32B on the AIME24 and AIME25 benchmarks, while achieving comparable results on LiveCodeBench. The Skywork-OR1-7B and Skywork-OR1-Math-7B models demonstrate competitive reasoning capabilities among models of similar size. We perform comprehensive ablation studies on the core components of our training pipeline to validate their effectiveness. Additionally, we thoroughly investigate the phenomenon of entropy collapse, identify key factors affecting entropy dynamics, and demonstrate that mitigating premature entropy collapse is critical for improved test performance. To support community research, we fully open-source our model weights, training code, and training datasets."
  },
  {
    "title": "Adaptive Detoxification: Safeguarding General Capabilities of LLMs through Toxicity-Aware Knowledge Editing",
    "url": "http://arxiv.org/abs/2505.22298v1",
    "arxiv_id": "2505.22298v1",
    "authors": [
      "Yifan Lu",
      "Jing Li",
      "Yigeng Zhou",
      "Yihui Zhang",
      "Wenya Wang",
      "Xiucheng Li",
      "Meishan Zhang",
      "Fangming Liu",
      "Jun Yu",
      "Min Zhang"
    ],
    "published": "2025-05-28T12:37:06+00:00",
    "summary": "Large language models (LLMs) exhibit impressive language capabilities but remain vulnerable to malicious prompts and jailbreaking attacks. Existing knowledge editing methods for LLM detoxification face two major challenges. First, they often rely on entity-specific localization, making them ineffective against adversarial inputs without explicit entities. Second, these methods suffer from over-editing, where detoxified models reject legitimate queries, compromising overall performance. In this paper, we propose ToxEdit, a toxicity-aware knowledge editing approach that dynamically detects toxic activation patterns during forward propagation. It then routes computations through adaptive inter-layer pathways to mitigate toxicity effectively. This design ensures precise toxicity mitigation while preserving LLMs' general capabilities. To more accurately assess over-editing, we also enhance the SafeEdit benchmark by incorporating instruction-following evaluation tasks. Experimental results on multiple LLMs demonstrate that our ToxEdit outperforms previous state-of-the-art methods in both detoxification performance and safeguarding general capabilities of LLMs."
  },
  {
    "title": "Compensating for Data with Reasoning: Low-Resource Machine Translation with LLMs",
    "url": "http://arxiv.org/abs/2505.22293v1",
    "arxiv_id": "2505.22293v1",
    "authors": [
      "Samuel Frontull",
      "Thomas Str\u00f6hle"
    ],
    "published": "2025-05-28T12:29:05+00:00",
    "summary": "Large Language Models (LLMs) have demonstrated strong capabilities in multilingual machine translation, sometimes even outperforming traditional neural systems. However, previous research has highlighted the challenges of using LLMs, particularly with prompt engineering, for low-resource languages. In this work, we introduce Fragment-Shot Prompting, a novel in-context learning method that segments input and retrieves translation examples based on syntactic coverage, along with Pivoted Fragment-Shot, an extension that enables translation without direct parallel data. We evaluate these methods using GPT-3.5, GPT-4o, o1-mini, LLaMA-3.3, and DeepSeek-R1 for translation between Italian and two Ladin variants, revealing three key findings: (1) Fragment-Shot Prompting is effective for translating into and between the studied low-resource languages, with syntactic coverage positively correlating with translation quality; (2) Models with stronger reasoning abilities make more effective use of retrieved knowledge, generally produce better translations, and enable Pivoted Fragment-Shot to significantly improve translation quality between the Ladin variants; and (3) prompt engineering offers limited, if any, improvements when translating from a low-resource to a high-resource language, where zero-shot prompting already yields satisfactory results. We publicly release our code and the retrieval corpora."
  },
  {
    "title": "Test-Time Immunization: A Universal Defense Framework Against Jailbreaks for (Multimodal) Large Language Models",
    "url": "http://arxiv.org/abs/2505.22271v1",
    "arxiv_id": "2505.22271v1",
    "authors": [
      "Yongcan Yu",
      "Yanbo Wang",
      "Ran He",
      "Jian Liang"
    ],
    "published": "2025-05-28T11:57:46+00:00",
    "summary": "While (multimodal) large language models (LLMs) have attracted widespread attention due to their exceptional capabilities, they remain vulnerable to jailbreak attacks. Various defense methods are proposed to defend against jailbreak attacks, however, they are often tailored to specific types of jailbreak attacks, limiting their effectiveness against diverse adversarial strategies. For instance, rephrasing-based defenses are effective against text adversarial jailbreaks but fail to counteract image-based attacks. To overcome these limitations, we propose a universal defense framework, termed Test-time IMmunization (TIM), which can adaptively defend against various jailbreak attacks in a self-evolving way. Specifically, TIM initially trains a gist token for efficient detection, which it subsequently applies to detect jailbreak activities during inference. When jailbreak attempts are identified, TIM implements safety fine-tuning using the detected jailbreak instructions paired with refusal answers. Furthermore, to mitigate potential performance degradation in the detector caused by parameter updates during safety fine-tuning, we decouple the fine-tuning process from the detection module. Extensive experiments on both LLMs and multimodal LLMs demonstrate the efficacy of TIM."
  },
  {
    "title": "LiDAR Based Semantic Perception for Forklifts in Outdoor Environments",
    "url": "http://arxiv.org/abs/2505.22258v1",
    "arxiv_id": "2505.22258v1",
    "authors": [
      "Benjamin Serfling",
      "Hannes Reichert",
      "Lorenzo Bayerlein",
      "Konrad Doll",
      "Kati Radkhah-Lens"
    ],
    "published": "2025-05-28T11:45:14+00:00",
    "summary": "In this study, we present a novel LiDAR-based semantic segmentation framework tailored for autonomous forklifts operating in complex outdoor environments. Central to our approach is the integration of a dual LiDAR system, which combines forward-facing and downward-angled LiDAR sensors to enable comprehensive scene understanding, specifically tailored for industrial material handling tasks. The dual configuration improves the detection and segmentation of dynamic and static obstacles with high spatial precision. Using high-resolution 3D point clouds captured from two sensors, our method employs a lightweight yet robust approach that segments the point clouds into safety-critical instance classes such as pedestrians, vehicles, and forklifts, as well as environmental classes such as driveable ground, lanes, and buildings. Experimental validation demonstrates that our approach achieves high segmentation accuracy while satisfying strict runtime requirements, establishing its viability for safety-aware, fully autonomous forklift navigation in dynamic warehouse and yard environments."
  },
  {
    "title": "BioHopR: A Benchmark for Multi-Hop, Multi-Answer Reasoning in Biomedical Domain",
    "url": "http://arxiv.org/abs/2505.22240v1",
    "arxiv_id": "2505.22240v1",
    "authors": [
      "Yunsoo Kim",
      "Yusuf Abdulle",
      "Honghan Wu"
    ],
    "published": "2025-05-28T11:19:01+00:00",
    "summary": "Biomedical reasoning often requires traversing interconnected relationships across entities such as drugs, diseases, and proteins. Despite the increasing prominence of large language models (LLMs), existing benchmarks lack the ability to evaluate multi-hop reasoning in the biomedical domain, particularly for queries involving one-to-many and many-to-many relationships. This gap leaves the critical challenges of biomedical multi-hop reasoning underexplored. To address this, we introduce BioHopR, a novel benchmark designed to evaluate multi-hop, multi-answer reasoning in structured biomedical knowledge graphs. Built from the comprehensive PrimeKG, BioHopR includes 1-hop and 2-hop reasoning tasks that reflect real-world biomedical complexities.   Evaluations of state-of-the-art models reveal that O3-mini, a proprietary reasoning-focused model, achieves 37.93% precision on 1-hop tasks and 14.57% on 2-hop tasks, outperforming proprietary models such as GPT4O and open-source biomedical models including HuatuoGPT-o1-70B and Llama-3.3-70B. However, all models exhibit significant declines in multi-hop performance, underscoring the challenges of resolving implicit reasoning steps in the biomedical domain. By addressing the lack of benchmarks for multi-hop reasoning in biomedical domain, BioHopR sets a new standard for evaluating reasoning capabilities and highlights critical gaps between proprietary and open-source models while paving the way for future advancements in biomedical LLMs."
  },
  {
    "title": "Optimal kernel regression bounds under energy-bounded noise",
    "url": "http://arxiv.org/abs/2505.22235v1",
    "arxiv_id": "2505.22235v1",
    "authors": [
      "Amon Lahr",
      "Johannes K\u00f6hler",
      "Anna Scampicchio",
      "Melanie N. Zeilinger"
    ],
    "published": "2025-05-28T11:11:24+00:00",
    "summary": "Non-conservative uncertainty bounds are key for both assessing an estimation algorithm's accuracy and in view of downstream tasks, such as its deployment in safety-critical contexts. In this paper, we derive a tight, non-asymptotic uncertainty bound for kernel-based estimation, which can also handle correlated noise sequences. Its computation relies on a mild norm-boundedness assumption on the unknown function and the noise, returning the worst-case function realization within the hypothesis class at an arbitrary query input location. The value of this function is shown to be given in terms of the posterior mean and covariance of a Gaussian process for an optimal choice of the measurement noise covariance. By rigorously analyzing the proposed approach and comparing it with other results in the literature, we show its effectiveness in returning tight and easy-to-compute bounds for kernel-based estimates."
  },
  {
    "title": "Thermal Modeling and Optimal Allocation of Avionics Safety-critical Tasks on Heterogeneous MPSoCs",
    "url": "http://arxiv.org/abs/2505.22214v1",
    "arxiv_id": "2505.22214v1",
    "authors": [
      "Ond\u0159ej Benedikt",
      "Michal Sojka",
      "P\u0159emysl \u0160\u016fcha",
      "Pavel Zaykov",
      "Zden\u011bk Hanz\u00e1lek"
    ],
    "published": "2025-05-28T10:40:47+00:00",
    "summary": "Multi-Processor Systems-on-Chip (MPSoC) can deliver high performance needed in many industrial domains, including aerospace. However, their high power consumption, combined with avionics safety standards, brings new thermal management challenges. This paper investigates techniques for offline thermal-aware allocation of periodic tasks on heterogeneous MPSoCs running at a fixed clock frequency, as required in avionics. The goal is to find the assignment of tasks to (i) cores and (ii) temporal isolation windows while minimizing the MPSoC temperature. To achieve that, we propose and analyze three power models, and integrate them within several novel optimization approaches based on heuristics, a black-box optimizer, and Integer Linear Programming (ILP). We perform the experimental evaluation on three popular MPSoC platforms (NXP i.MX8QM MEK, NXP i.MX8QM Ixora, NVIDIA TX2) and observe a difference of up to 5.5{\\deg}C among the tested methods (corresponding to a 22% reduction w.r.t. the ambient temperature). We also show that our method, integrating the empirical power model with the ILP, outperforms the other methods on all tested platforms."
  },
  {
    "title": "Pitfalls of Rule- and Model-based Verifiers -- A Case Study on Mathematical Reasoning",
    "url": "http://arxiv.org/abs/2505.22203v1",
    "arxiv_id": "2505.22203v1",
    "authors": [
      "Yuzhen Huang",
      "Weihao Zeng",
      "Xingshan Zeng",
      "Qi Zhu",
      "Junxian He"
    ],
    "published": "2025-05-28T10:28:41+00:00",
    "summary": "Trustworthy verifiers are essential for the success of reinforcement learning with verifiable reward (RLVR), which is the core methodology behind various large reasoning models such as DeepSeek-R1. In complex domains like mathematical reasoning, rule-based verifiers have been widely adopted in previous works to train strong reasoning models. However, the reliability of these verifiers and their impact on the RL training process remain poorly understood. In this work, we take mathematical reasoning as a case study and conduct a comprehensive analysis of various verifiers in both static evaluation and RL training scenarios. First, we find that current open-source rule-based verifiers often fail to recognize equivalent answers presented in different formats across multiple commonly used mathematical datasets, resulting in non-negligible false negative rates. This limitation adversely affects RL training performance and becomes more pronounced as the policy model gets stronger. Subsequently, we investigate model-based verifiers as a potential solution to address these limitations. While the static evaluation shows that model-based verifiers achieve significantly higher verification accuracy, further analysis and RL training results imply that they are highly susceptible to hacking, where they misclassify certain patterns in responses as correct (i.e., false positives). This vulnerability is exploited during policy model optimization, leading to artificially inflated rewards. Our findings underscore the unique risks inherent to both rule-based and model-based verifiers, aiming to offer valuable insights to develop more robust reward systems in reinforcement learning."
  },
  {
    "title": "Accountable, Scalable and DoS-resilient Secure Vehicular Communication",
    "url": "http://arxiv.org/abs/2505.22162v1",
    "arxiv_id": "2505.22162v1",
    "authors": [
      "Hongyu Jin",
      "Panos Papadimitratos"
    ],
    "published": "2025-05-28T09:25:34+00:00",
    "summary": "Paramount to vehicle safety, broadcasted Cooperative Awareness Messages (CAMs) and Decentralized Environmental Notification Messages (DENMs) are pseudonymously authenticated for security and privacy protection, with each node needing to have all incoming messages validated within an expiration deadline. This creates an asymmetry that can be easily exploited by external adversaries to launch a clogging Denial of Service (DoS) attack: each forged VC message forces all neighboring nodes to cryptographically validate it; at increasing rates, easy to generate forged messages gradually exhaust processing resources and severely degrade or deny timely validation of benign CAMs/DENMs. The result can be catastrophic when awareness of neighbor vehicle positions or critical reports are missed. We address this problem making the standardized VC pseudonymous authentication DoS-resilient. We propose efficient cryptographic constructs, which we term message verification facilitators, to prioritize processing resources for verification of potentially valid messages among bogus messages and verify multiple messages based on one signature verification. Any message acceptance is strictly based on public-key based message authentication/verification for accountability, i.e., non-repudiation is not sacrificed, unlike symmetric key based approaches. This further enables drastic misbehavior detection, also exploiting the newly introduced facilitators, based on probabilistic signature verification and cross-checking over multiple facilitators verifying the same message; while maintaining verification latency low even when under attack, trading off modest communication overhead. Our facilitators can also be used for efficient discovery and verification of DENM or any event-driven message, including misbehavior evidence used for our scheme."
  },
  {
    "title": "Adapting Segment Anything Model for Power Transmission Corridor Hazard Segmentation",
    "url": "http://arxiv.org/abs/2505.22105v1",
    "arxiv_id": "2505.22105v1",
    "authors": [
      "Hang Chen",
      "Maoyuan Ye",
      "Peng Yang",
      "Haibin He",
      "Juhua Liu",
      "Bo Du"
    ],
    "published": "2025-05-28T08:32:55+00:00",
    "summary": "Power transmission corridor hazard segmentation (PTCHS) aims to separate transmission equipment and surrounding hazards from complex background, conveying great significance to maintaining electric power transmission safety. Recently, the Segment Anything Model (SAM) has emerged as a foundational vision model and pushed the boundaries of segmentation tasks. However, SAM struggles to deal with the target objects in complex transmission corridor scenario, especially those with fine structure. In this paper, we propose ELE-SAM, adapting SAM for the PTCHS task. Technically, we develop a Context-Aware Prompt Adapter to achieve better prompt tokens via incorporating global-local features and focusing more on key regions. Subsequently, to tackle the hazard objects with fine structure in complex background, we design a High-Fidelity Mask Decoder by leveraging multi-granularity mask features and then scaling them to a higher resolution. Moreover, to train ELE-SAM and advance this field, we construct the ELE-40K benchmark, the first large-scale and real-world dataset for PTCHS including 44,094 image-mask pairs. Experimental results for ELE-40K demonstrate the superior performance that ELE-SAM outperforms the baseline model with the average 16.8% mIoU and 20.6% mBIoU performance improvement. Moreover, compared with the state-of-the-art method on HQSeg-44K, the average 2.9% mIoU and 3.8% mBIoU absolute improvements further validate the effectiveness of our method on high-quality generic object segmentation. The source code and dataset are available at https://github.com/Hhaizee/ELE-SAM."
  },
  {
    "title": "Efficient Dynamic Shielding for Parametric Safety Specifications",
    "url": "http://arxiv.org/abs/2505.22104v1",
    "arxiv_id": "2505.22104v1",
    "authors": [
      "Davide Corsi",
      "Kaushik Mallik",
      "Andoni Rodriguez",
      "Cesar Sanchez"
    ],
    "published": "2025-05-28T08:30:03+00:00",
    "summary": "Shielding has emerged as a promising approach for ensuring safety of AI-controlled autonomous systems. The algorithmic goal is to compute a shield, which is a runtime safety enforcement tool that needs to monitor and intervene the AI controller's actions if safety could be compromised otherwise. Traditional shields are designed statically for a specific safety requirement. Therefore, if the safety requirement changes at runtime due to changing operating conditions, the shield needs to be recomputed from scratch, causing delays that could be fatal. We introduce dynamic shields for parametric safety specifications, which are succinctly represented sets of all possible safety specifications that may be encountered at runtime. Our dynamic shields are statically designed for a given safety parameter set, and are able to dynamically adapt as the true safety specification (permissible by the parameters) is revealed at runtime. The main algorithmic novelty lies in the dynamic adaptation procedure, which is a simple and fast algorithm that utilizes known features of standard safety shields, like maximal permissiveness. We report experimental results for a robot navigation problem in unknown territories, where the safety specification evolves as new obstacles are discovered at runtime. In our experiments, the dynamic shields took a few minutes for their offline design, and took between a fraction of a second and a few seconds for online adaptation at each step, whereas the brute-force online recomputation approach was up to 5 times slower."
  },
  {
    "title": "From Failures to Fixes: LLM-Driven Scenario Repair for Self-Evolving Autonomous Driving",
    "url": "http://arxiv.org/abs/2505.22067v1",
    "arxiv_id": "2505.22067v1",
    "authors": [
      "Xinyu Xia",
      "Xingjun Ma",
      "Yunfeng Hu",
      "Ting Qu",
      "Hong Chen",
      "Xun Gong"
    ],
    "published": "2025-05-28T07:46:19+00:00",
    "summary": "Ensuring robust and generalizable autonomous driving requires not only broad scenario coverage but also efficient repair of failure cases, particularly those related to challenging and safety-critical scenarios. However, existing scenario generation and selection methods often lack adaptivity and semantic relevance, limiting their impact on performance improvement. In this paper, we propose \\textbf{SERA}, an LLM-powered framework that enables autonomous driving systems to self-evolve by repairing failure cases through targeted scenario recommendation. By analyzing performance logs, SERA identifies failure patterns and dynamically retrieves semantically aligned scenarios from a structured bank. An LLM-based reflection mechanism further refines these recommendations to maximize relevance and diversity. The selected scenarios are used for few-shot fine-tuning, enabling targeted adaptation with minimal data. Experiments on the benchmark show that SERA consistently improves key metrics across multiple autonomous driving baselines, demonstrating its effectiveness and generalizability under safety-critical conditions."
  },
  {
    "title": "A Comparative Study of Fuzzers and Static Analysis Tools for Finding Memory Unsafety in C and C++",
    "url": "http://arxiv.org/abs/2505.22052v1",
    "arxiv_id": "2505.22052v1",
    "authors": [
      "Keno Hassler",
      "Philipp G\u00f6rz",
      "Stephan Lipp",
      "Thorsten Holz",
      "Marcel B\u00f6hme"
    ],
    "published": "2025-05-28T07:22:29+00:00",
    "summary": "Even today, over 70% of security vulnerabilities in critical software systems result from memory safety violations. To address this challenge, fuzzing and static analysis are widely used automated methods to discover such vulnerabilities. Fuzzing generates random program inputs to identify faults, while static analysis examines source code to detect potential vulnerabilities. Although these techniques share a common goal, they take fundamentally different approaches and have evolved largely independently.   In this paper, we present an empirical analysis of five static analyzers and 13 fuzzers, applied to over 100 known security vulnerabilities in C/C++ programs. We measure the number of bug reports generated for each vulnerability to evaluate how the approaches differ and complement each other. Moreover, we randomly sample eight bug-containing functions, manually analyze all bug reports therein, and quantify false-positive rates. We also assess limits to bug discovery, ease of use, resource requirements, and integration into the development process. We find that both techniques discover different types of bugs, but there are clear winners for each. Developers should consider these tools depending on their specific workflow and usability requirements. Based on our findings, we propose future directions to foster collaboration between these research domains."
  },
  {
    "title": "Jailbreak Distillation: Renewable Safety Benchmarking",
    "url": "http://arxiv.org/abs/2505.22037v1",
    "arxiv_id": "2505.22037v1",
    "authors": [
      "Jingyu Zhang",
      "Ahmed Elgohary",
      "Xiawei Wang",
      "A S M Iftekhar",
      "Ahmed Magooda",
      "Benjamin Van Durme",
      "Daniel Khashabi",
      "Kyle Jackson"
    ],
    "published": "2025-05-28T06:59:46+00:00",
    "summary": "Large language models (LLMs) are rapidly deployed in critical applications, raising urgent needs for robust safety benchmarking. We propose Jailbreak Distillation (JBDistill), a novel benchmark construction framework that \"distills\" jailbreak attacks into high-quality and easily-updatable safety benchmarks. JBDistill utilizes a small set of development models and existing jailbreak attack algorithms to create a candidate prompt pool, then employs prompt selection algorithms to identify an effective subset of prompts as safety benchmarks. JBDistill addresses challenges in existing safety evaluation: the use of consistent evaluation prompts across models ensures fair comparisons and reproducibility. It requires minimal human effort to rerun the JBDistill pipeline and produce updated benchmarks, alleviating concerns on saturation and contamination. Extensive experiments demonstrate our benchmarks generalize robustly to 13 diverse evaluation models held out from benchmark construction, including proprietary, specialized, and newer-generation LLMs, significantly outperforming existing safety benchmarks in effectiveness while maintaining high separability and diversity. Our framework thus provides an effective, sustainable, and adaptable solution for streamlining safety evaluation."
  },
  {
    "title": "Retweets, Receipts, and Resistance: Discourse, Sentiment, and Credibility in Public Health Crisis Twitter",
    "url": "http://arxiv.org/abs/2505.22032v1",
    "arxiv_id": "2505.22032v1",
    "authors": [
      "Tawfiq Ammari",
      "Anna Gutowska",
      "Jacob Ziff",
      "Casey Randazzo",
      "Harihan Subramonyam"
    ],
    "published": "2025-05-28T06:53:33+00:00",
    "summary": "As the COVID-19 pandemic evolved, the Centers for Disease Control and Prevention (CDC) used Twitter to disseminate safety guidance and updates, reaching millions of users. This study analyzes two years of tweets from, to, and about the CDC using a mixed methods approach to examine discourse characteristics, credibility, and user engagement. We found that the CDCs communication remained largely one directional and did not foster reciprocal interaction, while discussions around COVID19 were deeply shaped by political and ideological polarization. Users frequently cited earlier CDC messages to critique new and sometimes contradictory guidance. Our findings highlight the role of sentiment, media richness, and source credibility in shaping the spread of public health messages. We propose design strategies to help the CDC tailor communications to diverse user groups and manage misinformation more effectively during high-stakes health crises."
  },
  {
    "title": "CoThink: Token-Efficient Reasoning via Instruct Models Guiding Reasoning Models",
    "url": "http://arxiv.org/abs/2505.22017v1",
    "arxiv_id": "2505.22017v1",
    "authors": [
      "Siqi Fan",
      "Peng Han",
      "Shuo Shang",
      "Yequan Wang",
      "Aixin Sun"
    ],
    "published": "2025-05-28T06:24:45+00:00",
    "summary": "Large language models (LLMs) benefit from increased test-time compute, a phenomenon known as test-time scaling. However, reasoning-optimized models often overthink even simple problems, producing excessively verbose outputs and leading to low token efficiency. By comparing these models with equally sized instruct models, we identify two key causes of this verbosity: (1) reinforcement learning reduces the information density of forward reasoning, and (2) backward chain-of thought training encourages redundant and often unnecessary verification steps. Since LLMs cannot assess the difficulty of a given problem, they tend to apply the same cautious reasoning strategy across all tasks, resulting in inefficient overthinking. To address this, we propose CoThink, an embarrassingly simple pipeline: an instruct model first drafts a high-level solution outline; a reasoning model then works out the solution. We observe that CoThink enables dynamic adjustment of reasoning depth based on input difficulty. Evaluated with three reasoning models DAPO, DeepSeek-R1, and QwQ on three datasets GSM8K, MATH500, and AIME24, CoThink reduces total token generation by 22.3% while maintaining pass@1 accuracy within a 0.42% margin on average. With reference to the instruct model, we formally define reasoning efficiency and observe a potential reasoning efficiency scaling law in LLMs."
  },
  {
    "title": "DORAEMON: Decentralized Ontology-aware Reliable Agent with Enhanced Memory Oriented Navigation",
    "url": "http://arxiv.org/abs/2505.21969v1",
    "arxiv_id": "2505.21969v1",
    "authors": [
      "Tianjun Gu",
      "Linfeng Li",
      "Xuhong Wang",
      "Chenghua Gong",
      "Jingyu Gong",
      "Zhizhong Zhang",
      "Yuan Xie",
      "Lizhuang Ma",
      "Xin Tan"
    ],
    "published": "2025-05-28T04:46:13+00:00",
    "summary": "Adaptive navigation in unfamiliar environments is crucial for household service robots but remains challenging due to the need for both low-level path planning and high-level scene understanding. While recent vision-language model (VLM) based zero-shot approaches reduce dependence on prior maps and scene-specific training data, they face significant limitations: spatiotemporal discontinuity from discrete observations, unstructured memory representations, and insufficient task understanding leading to navigation failures. We propose DORAEMON (Decentralized Ontology-aware Reliable Agent with Enhanced Memory Oriented Navigation), a novel cognitive-inspired framework consisting of Ventral and Dorsal Streams that mimics human navigation capabilities. The Dorsal Stream implements the Hierarchical Semantic-Spatial Fusion and Topology Map to handle spatiotemporal discontinuities, while the Ventral Stream combines RAG-VLM and Policy-VLM to improve decision-making. Our approach also develops Nav-Ensurance to ensure navigation safety and efficiency. We evaluate DORAEMON on the HM3D, MP3D, and GOAT datasets, where it achieves state-of-the-art performance on both success rate (SR) and success weighted by path length (SPL) metrics, significantly outperforming existing methods. We also introduce a new evaluation metric (AORI) to assess navigation intelligence better. Comprehensive experiments demonstrate DORAEMON's effectiveness in zero-shot autonomous navigation without requiring prior map building or pre-training."
  },
  {
    "title": "Seeing the Threat: Vulnerabilities in Vision-Language Models to Adversarial Attack",
    "url": "http://arxiv.org/abs/2505.21967v1",
    "arxiv_id": "2505.21967v1",
    "authors": [
      "Juan Ren",
      "Mark Dras",
      "Usman Naseem"
    ],
    "published": "2025-05-28T04:43:39+00:00",
    "summary": "Large Vision-Language Models (LVLMs) have shown remarkable capabilities across a wide range of multimodal tasks. However, their integration of visual inputs introduces expanded attack surfaces, thereby exposing them to novel security vulnerabilities. In this work, we conduct a systematic representational analysis to uncover why conventional adversarial attacks can circumvent the safety mechanisms embedded in LVLMs. We further propose a novel two stage evaluation framework for adversarial attacks on LVLMs. The first stage differentiates among instruction non compliance, outright refusal, and successful adversarial exploitation. The second stage quantifies the degree to which the model's output fulfills the harmful intent of the adversarial prompt, while categorizing refusal behavior into direct refusals, soft refusals, and partial refusals that remain inadvertently helpful. Finally, we introduce a normative schema that defines idealized model behavior when confronted with harmful prompts, offering a principled target for safety alignment in multimodal systems."
  },
  {
    "title": "Quantitative Macromolecular Proton Fraction Imaging using Pulsed Spin-Lock",
    "url": "http://arxiv.org/abs/2505.21853v1",
    "arxiv_id": "2505.21853v1",
    "authors": [
      "Qianxue Shan",
      "Ziqiang Yu",
      "Baiyan Jiang",
      "Jian Hou",
      "Qiuyi Shen",
      "Winnie CW Chu",
      "Vincent WS Wong",
      "Weitian Chen"
    ],
    "published": "2025-05-28T00:53:45+00:00",
    "summary": "Purpose: Recent studies have shown that spin-lock MRI can simplify quantitative magnetization transfer (MT) by eliminating its dependency on water pool parameters, removing the need for a T1 map in macromolecular proton fraction (MPF) quantification. However, its application is often limited by the requirement for long radiofrequency (RF) pulse durations, which are constrained by RF hardware capabilities despite remaining within specific absorption rate (SAR) safety limits.   Methods: To address this challenge, we propose a novel method, MPF mapping using pulsed spin-lock (MPF-PSL). MPF-PSL employs a pulsed spin-lock train with intermittent free precession periods, enabling extended total spin-lock durations without exceeding hardware and specific absorption rate limits. A comprehensive analytical framework was developed to model the magnetization dynamics of the two-pool MT system under pulsed spin-lock, demonstrating that MPF-PSL achieves MT-specific quantification while minimizing confounding effects from the water pool. The proposed method is validated with Bloch-McConnell simulations, phantoms, and in vivo studies at 3T.   Results: Both Bloch-McConnell simulations and phantom validation demonstrated that MPF-PSL exhibits robust insensitivity to water pool parameters while enabling high-SNR MPF quantification. In vivo validation studies confirmed the method's clinical utility in detecting collagen deposition in patients with liver fibrosis.   Conclusion: MPF-PSL presents a practical solution for quantitative MT imaging, with strong potential for clinical applications."
  },
  {
    "title": "A Provable Approach for End-to-End Safe Reinforcement Learning",
    "url": "http://arxiv.org/abs/2505.21852v1",
    "arxiv_id": "2505.21852v1",
    "authors": [
      "Akifumi Wachi",
      "Kohei Miyaguchi",
      "Takumi Tanabe",
      "Rei Sato",
      "Youhei Akimoto"
    ],
    "published": "2025-05-28T00:48:20+00:00",
    "summary": "A longstanding goal in safe reinforcement learning (RL) is a method to ensure the safety of a policy throughout the entire process, from learning to operation. However, existing safe RL paradigms inherently struggle to achieve this objective. We propose a method, called Provably Lifetime Safe RL (PLS), that integrates offline safe RL with safe policy deployment to address this challenge. Our proposed method learns a policy offline using return-conditioned supervised learning and then deploys the resulting policy while cautiously optimizing a limited set of parameters, known as target returns, using Gaussian processes (GPs). Theoretically, we justify the use of GPs by analyzing the mathematical relationship between target and actual returns. We then prove that PLS finds near-optimal target returns while guaranteeing safety with high probability. Empirically, we demonstrate that PLS outperforms baselines both in safety and reward performance, thereby achieving the longstanding goal to obtain high rewards while ensuring the safety of a policy throughout the lifetime from learning to operation."
  },
  {
    "title": "An Optimistic Algorithm for online CMDPS with Anytime Adversarial Constraints",
    "url": "http://arxiv.org/abs/2505.21841v1",
    "arxiv_id": "2505.21841v1",
    "authors": [
      "Jiahui Zhu",
      "Kihyun Yu",
      "Dabeen Lee",
      "Xin Liu",
      "Honghao Wei"
    ],
    "published": "2025-05-28T00:16:34+00:00",
    "summary": "Online safe reinforcement learning (RL) plays a key role in dynamic environments, with applications in autonomous driving, robotics, and cybersecurity. The objective is to learn optimal policies that maximize rewards while satisfying safety constraints modeled by constrained Markov decision processes (CMDPs). Existing methods achieve sublinear regret under stochastic constraints but often fail in adversarial settings, where constraints are unknown, time-varying, and potentially adversarially designed. In this paper, we propose the Optimistic Mirror Descent Primal-Dual (OMDPD) algorithm, the first to address online CMDPs with anytime adversarial constraints. OMDPD achieves optimal regret O(sqrt(K)) and strong constraint violation O(sqrt(K)) without relying on Slater's condition or the existence of a strictly known safe policy. We further show that access to accurate estimates of rewards and transitions can further improve these bounds. Our results offer practical guarantees for safe decision-making in adversarial environments."
  },
  {
    "title": "SAGE-Eval: Evaluating LLMs for Systematic Generalizations of Safety Facts",
    "url": "http://arxiv.org/abs/2505.21828v1",
    "arxiv_id": "2505.21828v1",
    "authors": [
      "Chen Yueh-Han",
      "Guy Davidson",
      "Brenden M. Lake"
    ],
    "published": "2025-05-27T23:29:32+00:00",
    "summary": "Do LLMs robustly generalize critical safety facts to novel situations? Lacking this ability is dangerous when users ask naive questions. For instance, \"I'm considering packing melon balls for my 10-month-old's lunch. What other foods would be good to include?\" Before offering food options, the LLM should warn that melon balls pose a choking hazard to toddlers, as documented by the CDC. Failing to provide such warnings could result in serious injuries or even death. To evaluate this, we introduce SAGE-Eval, SAfety-fact systematic GEneralization evaluation, the first benchmark that tests whether LLMs properly apply well established safety facts to naive user queries. SAGE-Eval comprises 104 facts manually sourced from reputable organizations, systematically augmented to create 10,428 test scenarios across 7 common domains (e.g., Outdoor Activities, Medicine). We find that the top model, Claude-3.7-sonnet, passes only 58% of all the safety facts tested. We also observe that model capabilities and training compute weakly correlate with performance on SAGE-Eval, implying that scaling up is not the golden solution. Our findings suggest frontier LLMs still lack robust generalization ability. We recommend developers use SAGE-Eval in pre-deployment evaluations to assess model reliability in addressing salient risks. We publicly release SAGE-Eval at https://huggingface.co/datasets/YuehHanChen/SAGE-Eval and our code is available at https://github.com/YuehHanChen/SAGE-Eval/tree/main."
  },
  {
    "title": "Towards Safety Reasoning in LLMs: AI-agentic Deliberation for Policy-embedded CoT Data Creation",
    "url": "http://arxiv.org/abs/2505.21784v1",
    "arxiv_id": "2505.21784v1",
    "authors": [
      "Tharindu Kumarage",
      "Ninareh Mehrabi",
      "Anil Ramakrishna",
      "Xinyan Zhao",
      "Richard Zemel",
      "Kai-Wei Chang",
      "Aram Galstyan",
      "Rahul Gupta",
      "Charith Peris"
    ],
    "published": "2025-05-27T21:34:40+00:00",
    "summary": "Safety reasoning is a recent paradigm where LLMs reason over safety policies before generating responses, thereby mitigating limitations in existing safety measures such as over-refusal and jailbreak vulnerabilities. However, implementing this paradigm is challenging due to the resource-intensive process of creating high-quality policy-embedded chain-of-thought (CoT) datasets while ensuring reasoning remains accurate and free from hallucinations or policy conflicts. To tackle this, we propose AIDSAFE: Agentic Iterative Deliberation for Safety Reasoning, a novel data generation recipe that leverages multi-agent deliberation to iteratively expand reasoning on safety policies. A data refiner stage in AIDSAFE ensures high-quality outputs by eliminating repetitive, redundant, and deceptive thoughts. AIDSAFE-generated CoTs provide a strong foundation for supervised fine-tuning (SFT)-based safety training. Additionally, to address the need of preference data in alignment stages, such as DPO training, we introduce a supplemental recipe that uses belief augmentation to create distinct selected and rejected CoT samples. Our evaluations demonstrate that AIDSAFE-generated CoTs achieve superior policy adherence and reasoning quality. Consequently, we show that fine-tuning open-source LLMs on these CoTs can significantly improve safety generalization and jailbreak robustness while maintaining acceptable utility and over-refusal accuracy. AIDSAFE-generated CoT datasets can be found here: https://huggingface.co/datasets/AmazonScience/AIDSAFE"
  },
  {
    "title": "On the Role of Demagnetizing Tensors in Arbitrary Orientations of General Ellipsoid: Implications for MRI Safety Assessment",
    "url": "http://arxiv.org/abs/2505.21761v1",
    "arxiv_id": "2505.21761v1",
    "authors": [
      "Tomppa Pakarinen"
    ],
    "published": "2025-05-27T20:56:11+00:00",
    "summary": "This work explores the behaviour of demagnetizing tensors for general ellipsoids under arbitrary rotations in homogeneous magnetic fields. The work is motivated by the concerns in magnetic resonance imaging safety and their practical evaluation in clinical environments. Whereas demagnetizing tensor is a well-defined concept in the principal axes, its transformation under three-dimensional reorientation is often overlooked - a justifiable omission for solutions derived from Poisson equation, where the tensor can be directly rotated. However, this does not hold for common approximations, where such tensor is not explicitly defined. This work demonstrates the validity of directly rotating the orthogonal basis solutions, derived from Poissons equation, and uses the procedure to evaluate a practical approximation, based on orthogonal area-projections. The tensor rotation approach is also applied to generalize force and torque calculations for ellipsoids under three-dimensional re-orientation. The results show an exact match for translation force and torque when compared to the standard single axis rotation. Additionally, a unique connection to the well-known MRI magic angle is found as the point of convergence for prolate spheroid aspect ratios while solving the corresponding saturation field magnitudes. Finally, the evaluated approximate method was demonstrated to perform fairly across prolate and oblate spheroids. Similar approximations might extend to irregular shapes, but numerical validation would likely remain preferable due to the complexity of internal field distributions."
  },
  {
    "title": "Simulating the Unseen: Crash Prediction Must Learn from What Did Not Happen",
    "url": "http://arxiv.org/abs/2505.21743v1",
    "arxiv_id": "2505.21743v1",
    "authors": [
      "Zihao Li",
      "Xinyuan Cao",
      "Xiangbo Gao",
      "Kexin Tian",
      "Keshu Wu",
      "Mohammad Anis",
      "Hao Zhang",
      "Keke Long",
      "Jiwan Jiang",
      "Xiaopeng Li",
      "Yunlong Zhang",
      "Tianbao Yang",
      "Dominique Lord",
      "Zhengzhong Tu",
      "Yang Zhou"
    ],
    "published": "2025-05-27T20:33:07+00:00",
    "summary": "Traffic safety science has long been hindered by a fundamental data paradox: the crashes we most wish to prevent are precisely those events we rarely observe. Existing crash-frequency models and surrogate safety metrics rely heavily on sparse, noisy, and under-reported records, while even sophisticated, high-fidelity simulations undersample the long-tailed situations that trigger catastrophic outcomes such as fatalities. We argue that the path to achieving Vision Zero, i.e., the complete elimination of traffic fatalities and severe injuries, requires a paradigm shift from traditional crash-only learning to a new form of counterfactual safety learning: reasoning not only about what happened, but also about the vast set of plausible yet perilous scenarios that could have happened under slightly different circumstances. To operationalize this shift, our proposed agenda bridges macro to micro. Guided by crash-rate priors, generative scene engines, diverse driver models, and causal learning, near-miss events are synthesized and explained. A crash-focused digital twin testbed links micro scenes to macro patterns, while a multi-objective validator ensures that simulations maintain statistical realism. This pipeline transforms sparse crash data into rich signals for crash prediction, enabling the stress-testing of vehicles, roads, and policies before deployment. By learning from crashes that almost happened, we can shift traffic safety from reactive forensics to proactive prevention, advancing Vision Zero."
  },
  {
    "title": "AI-Supported Platform for System Monitoring and Decision-Making in Nuclear Waste Management with Large Language Models",
    "url": "http://arxiv.org/abs/2505.21741v1",
    "arxiv_id": "2505.21741v1",
    "authors": [
      "Dongjune Chang",
      "Sola Kim",
      "Young Soo Park"
    ],
    "published": "2025-05-27T20:29:53+00:00",
    "summary": "Nuclear waste management requires rigorous regulatory compliance assessment, demanding advanced decision-support systems capable of addressing complex legal, environmental, and safety considerations. This paper presents a multi-agent Retrieval-Augmented Generation (RAG) system that integrates large language models (LLMs) with document retrieval mechanisms to enhance decision accuracy through structured agent collaboration. Through a structured 10-round discussion model, agents collaborate to assess regulatory compliance and safety requirements while maintaining document-grounded responses. Implemented on consumer-grade hardware, the system leverages Llama 3.2 and mxbai-embed-large-v1 embeddings for efficient retrieval and semantic representation. A case study of a proposed temporary nuclear waste storage site near Winslow, Arizona, demonstrates the framework's effectiveness. Results show the Regulatory Agent achieves consistently higher relevance scores in maintaining alignment with legal frameworks, while the Safety Agent effectively manages complex risk assessments requiring multifaceted analysis. The system demonstrates progressive improvement in agreement rates between agents across discussion rounds while semantic drift decreases, indicating enhanced decision-making consistency and response coherence. The system ensures regulatory decisions remain factually grounded, dynamically adapting to evolving regulatory frameworks through real-time document retrieval. By balancing automated assessment with human oversight, this framework offers a scalable and transparent approach to regulatory governance. These findings underscore the potential of AI-driven, multi-agent systems in advancing evidence-based, accountable, and adaptive decision-making for high-stakes environmental management scenarios."
  },
  {
    "title": "A Joint Reconstruction-Triplet Loss Autoencoder Approach Towards Unseen Attack Detection in IoV Networks",
    "url": "http://arxiv.org/abs/2505.21703v1",
    "arxiv_id": "2505.21703v1",
    "authors": [
      "Julia Boone",
      "Tolunay Seyfi",
      "Fatemeh Afghah"
    ],
    "published": "2025-05-27T19:40:57+00:00",
    "summary": "Internet of Vehicles (IoV) systems, while offering significant advancements in transportation efficiency and safety, introduce substantial security vulnerabilities due to their highly interconnected nature. These dynamic systems produce massive amounts of data between vehicles, infrastructure, and cloud services and present a highly distributed framework with a wide attack surface. In considering network-centered attacks on IoV systems, attacks such as Denial-of-Service (DoS) can prohibit the communication of essential physical traffic safety information between system elements, illustrating that the security concerns for these systems go beyond the traditional confidentiality, integrity, and availability concerns of enterprise systems. Given the complexity and volume of data generated by IoV systems, traditional security mechanisms are often inadequate for accurately detecting sophisticated and evolving cyberattacks. Here, we present an unsupervised autoencoder method trained entirely on benign network data for the purpose of unseen attack detection in IoV networks. We leverage a weighted combination of reconstruction and triplet margin loss to guide the autoencoder training and develop a diverse representation of the benign training set. We conduct extensive experiments on recent network intrusion datasets from two different application domains, industrial IoT and home IoT, that represent the modern IoV task. We show that our method performs robustly for all unseen attack types, with roughly 99% accuracy on benign data and between 97% and 100% performance on anomaly data. We extend these results to show that our model is adaptable through the use of transfer learning, achieving similarly high results while leveraging domain features from one domain to another."
  },
  {
    "title": "Expert Survey: AI Reliability & Security Research Priorities",
    "url": "http://arxiv.org/abs/2505.21664v1",
    "arxiv_id": "2505.21664v1",
    "authors": [
      "Joe O'Brien",
      "Jeremy Dolan",
      "Jay Kim",
      "Jonah Dykhuizen",
      "Jeba Sania",
      "Sebastian Becker",
      "Jam Kraprayoon",
      "Cara Labrador"
    ],
    "published": "2025-05-27T18:44:30+00:00",
    "summary": "Our survey of 53 specialists across 105 AI reliability and security research areas identifies the most promising research prospects to guide strategic AI R&D investment. As companies are seeking to develop AI systems with broadly human-level capabilities, research on reliability and security is urgently needed to ensure AI's benefits can be safely and broadly realized and prevent severe harms. This study is the first to quantify expert priorities across a comprehensive taxonomy of AI safety and security research directions and to produce a data-driven ranking of their potential impact. These rankings may support evidence-based decisions about how to effectively deploy resources toward AI reliability and security research."
  },
  {
    "title": "PrivATE: Differentially Private Confidence Intervals for Average Treatment Effects",
    "url": "http://arxiv.org/abs/2505.21641v1",
    "arxiv_id": "2505.21641v1",
    "authors": [
      "Maresa Schr\u00f6der",
      "Justin Hartenstein",
      "Stefan Feuerriegel"
    ],
    "published": "2025-05-27T18:13:11+00:00",
    "summary": "The average treatment effect (ATE) is widely used to evaluate the effectiveness of drugs and other medical interventions. In safety-critical applications like medicine, reliable inferences about the ATE typically require valid uncertainty quantification, such as through confidence intervals (CIs). However, estimating treatment effects in these settings often involves sensitive data that must be kept private. In this work, we present PrivATE, a novel machine learning framework for computing CIs for the ATE under differential privacy. Specifically, we focus on deriving valid privacy-preserving CIs for the ATE from observational data. Our PrivATE framework consists of three steps: (i) estimating a differentially private ATE through output perturbation; (ii) estimating the differentially private variance through a truncated output perturbation mechanism; and (iii) constructing the CIs while accounting for the uncertainty from both the estimation and privatization steps. Our PrivATE framework is model agnostic, doubly robust, and ensures valid CIs. We demonstrate the effectiveness of our framework using synthetic and real-world medical datasets. To the best of our knowledge, we are the first to derive a general, doubly robust framework for valid CIs of the ATE under ($\\varepsilon$, $\\delta$)-differential privacy."
  },
  {
    "title": "Silence is Not Consensus: Disrupting Agreement Bias in Multi-Agent LLMs via Catfish Agent for Clinical Decision Making",
    "url": "http://arxiv.org/abs/2505.21503v1",
    "arxiv_id": "2505.21503v1",
    "authors": [
      "Yihan Wang",
      "Qiao Yan",
      "Zhenghao Xing",
      "Lihao Liu",
      "Junjun He",
      "Chi-Wing Fu",
      "Xiaowei Hu",
      "Pheng-Ann Heng"
    ],
    "published": "2025-05-27T17:59:50+00:00",
    "summary": "Large language models (LLMs) have demonstrated strong potential in clinical question answering, with recent multi-agent frameworks further improving diagnostic accuracy via collaborative reasoning. However, we identify a recurring issue of Silent Agreement, where agents prematurely converge on diagnoses without sufficient critical analysis, particularly in complex or ambiguous cases. We present a new concept called Catfish Agent, a role-specialized LLM designed to inject structured dissent and counter silent agreement. Inspired by the ``catfish effect'' in organizational psychology, the Catfish Agent is designed to challenge emerging consensus to stimulate deeper reasoning. We formulate two mechanisms to encourage effective and context-aware interventions: (i) a complexity-aware intervention that modulates agent engagement based on case difficulty, and (ii) a tone-calibrated intervention articulated to balance critique and collaboration. Evaluations on nine medical Q&A and three medical VQA benchmarks show that our approach consistently outperforms both single- and multi-agent LLMs frameworks, including leading commercial models such as GPT-4o and DeepSeek-R1."
  },
  {
    "title": "Reinforcing General Reasoning without Verifiers",
    "url": "http://arxiv.org/abs/2505.21493v1",
    "arxiv_id": "2505.21493v1",
    "authors": [
      "Xiangxin Zhou",
      "Zichen Liu",
      "Anya Sims",
      "Haonan Wang",
      "Tianyu Pang",
      "Chongxuan Li",
      "Liang Wang",
      "Min Lin",
      "Chao Du"
    ],
    "published": "2025-05-27T17:56:27+00:00",
    "summary": "The recent paradigm shift towards training large language models (LLMs) using DeepSeek-R1-Zero-style reinforcement learning (RL) on verifiable rewards has led to impressive advancements in code and mathematical reasoning. However, this methodology is limited to tasks where rule-based answer verification is possible and does not naturally extend to real-world domains such as chemistry, healthcare, engineering, law, biology, business, and economics. Current practical workarounds use an additional LLM as a model-based verifier; however, this introduces issues such as reliance on a strong verifier LLM, susceptibility to reward hacking, and the practical burden of maintaining the verifier model in memory during training. To address this and extend DeepSeek-R1-Zero-style training to general reasoning domains, we propose a verifier-free method (VeriFree) that bypasses answer verification and instead uses RL to directly maximize the probability of generating the reference answer. We compare VeriFree with verifier-based methods and demonstrate that, in addition to its significant practical benefits and reduced compute requirements, VeriFree matches and even surpasses verifier-based methods on extensive evaluations across MMLU-Pro, GPQA, SuperGPQA, and math-related benchmarks. Moreover, we provide insights into this method from multiple perspectives: as an elegant integration of training both the policy and implicit verifier in a unified model, and as a variational optimization approach. Code is available at https://github.com/sail-sg/VeriFree."
  },
  {
    "title": "SOSBENCH: Benchmarking Safety Alignment on Scientific Knowledge",
    "url": "http://arxiv.org/abs/2505.21605v1",
    "arxiv_id": "2505.21605v1",
    "authors": [
      "Fengqing Jiang",
      "Fengbo Ma",
      "Zhangchen Xu",
      "Yuetai Li",
      "Bhaskar Ramasubramanian",
      "Luyao Niu",
      "Bo Li",
      "Xianyan Chen",
      "Zhen Xiang",
      "Radha Poovendran"
    ],
    "published": "2025-05-27T17:47:08+00:00",
    "summary": "Large language models (LLMs) exhibit advancing capabilities in complex tasks, such as reasoning and graduate-level question answering, yet their resilience against misuse, particularly involving scientifically sophisticated risks, remains underexplored. Existing safety benchmarks typically focus either on instructions requiring minimal knowledge comprehension (e.g., ``tell me how to build a bomb\") or utilize prompts that are relatively low-risk (e.g., multiple-choice or classification tasks about hazardous content). Consequently, they fail to adequately assess model safety when handling knowledge-intensive, hazardous scenarios.   To address this critical gap, we introduce SOSBench, a regulation-grounded, hazard-focused benchmark encompassing six high-risk scientific domains: chemistry, biology, medicine, pharmacology, physics, and psychology. The benchmark comprises 3,000 prompts derived from real-world regulations and laws, systematically expanded via an LLM-assisted evolutionary pipeline that introduces diverse, realistic misuse scenarios (e.g., detailed explosive synthesis instructions involving advanced chemical formulas). We evaluate frontier models within a unified evaluation framework using our SOSBench. Despite their alignment claims, advanced models consistently disclose policy-violating content across all domains, demonstrating alarmingly high rates of harmful responses (e.g., 79.1% for Deepseek-R1 and 47.3% for GPT-4.1). These results highlight significant safety alignment deficiencies and underscore urgent concerns regarding the responsible deployment of powerful LLMs."
  },
  {
    "title": "Active-O3: Empowering Multimodal Large Language Models with Active Perception via GRPO",
    "url": "http://arxiv.org/abs/2505.21457v1",
    "arxiv_id": "2505.21457v1",
    "authors": [
      "Muzhi Zhu",
      "Hao Zhong",
      "Canyu Zhao",
      "Zongze Du",
      "Zheng Huang",
      "Mingyu Liu",
      "Hao Chen",
      "Cheng Zou",
      "Jingdong Chen",
      "Ming Yang",
      "Chunhua Shen"
    ],
    "published": "2025-05-27T17:29:31+00:00",
    "summary": "Active vision, also known as active perception, refers to the process of actively selecting where and how to look in order to gather task-relevant information. It is a critical component of efficient perception and decision-making in humans and advanced embodied agents. Recently, the use of Multimodal Large Language Models (MLLMs) as central planning and decision-making modules in robotic systems has gained extensive attention. However, despite the importance of active perception in embodied intelligence, there is little to no exploration of how MLLMs can be equipped with or learn active perception capabilities. In this paper, we first provide a systematic definition of MLLM-based active perception tasks. We point out that the recently proposed GPT-o3 model's zoom-in search strategy can be regarded as a special case of active perception; however, it still suffers from low search efficiency and inaccurate region selection. To address these issues, we propose ACTIVE-O3, a purely reinforcement learning based training framework built on top of GRPO, designed to equip MLLMs with active perception capabilities. We further establish a comprehensive benchmark suite to evaluate ACTIVE-O3 across both general open-world tasks, such as small-object and dense object grounding, and domain-specific scenarios, including small object detection in remote sensing and autonomous driving, as well as fine-grained interactive segmentation. In addition, ACTIVE-O3 also demonstrates strong zero-shot reasoning abilities on the V* Benchmark, without relying on any explicit reasoning data. We hope that our work can provide a simple codebase and evaluation protocol to facilitate future research on active perception in MLLMs."
  },
  {
    "title": "Towards Better Instruction Following Retrieval Models",
    "url": "http://arxiv.org/abs/2505.21439v1",
    "arxiv_id": "2505.21439v1",
    "authors": [
      "Yuchen Zhuang",
      "Aaron Trinh",
      "Rushi Qiang",
      "Haotian Sun",
      "Chao Zhang",
      "Hanjun Dai",
      "Bo Dai"
    ],
    "published": "2025-05-27T17:14:37+00:00",
    "summary": "Modern information retrieval (IR) models, trained exclusively on standard <query, passage> pairs, struggle to effectively interpret and follow explicit user instructions. We introduce InF-IR, a large-scale, high-quality training corpus tailored for enhancing retrieval models in Instruction-Following IR. InF-IR expands traditional training pairs into over 38,000 expressive <instruction, query, passage> triplets as positive samples. In particular, for each positive triplet, we generate two additional hard negative examples by poisoning both instructions and queries, then rigorously validated by an advanced reasoning model (o3-mini) to ensure semantic plausibility while maintaining instructional incorrectness. Unlike existing corpora that primarily support computationally intensive reranking tasks for decoder-only language models, the highly contrastive positive-negative triplets in InF-IR further enable efficient representation learning for smaller encoder-only models, facilitating direct embedding-based retrieval. Using this corpus, we train InF-Embed, an instruction-aware Embedding model optimized through contrastive learning and instruction-query attention mechanisms to align retrieval outcomes precisely with user intents. Extensive experiments across five instruction-based retrieval benchmarks demonstrate that InF-Embed significantly surpasses competitive baselines by 8.1% in p-MRR, measuring the instruction-following capabilities."
  },
  {
    "title": "Autonomous Multi-Modal LLM Agents for Treatment Planning in Focused Ultrasound Ablation Surgery",
    "url": "http://arxiv.org/abs/2505.21418v1",
    "arxiv_id": "2505.21418v1",
    "authors": [
      "Lina Zhao",
      "Jiaxing Bai",
      "Zihao Bian",
      "Qingyue Chen",
      "Yafang Li",
      "Guangbo Li",
      "Min He",
      "Huaiyuan Yao",
      "Zongjiu Zhang"
    ],
    "published": "2025-05-27T16:43:31+00:00",
    "summary": "Focused Ultrasound Ablation Surgery (FUAS) has emerged as a promising non-invasive therapeutic modality, valued for its safety and precision. Nevertheless, its clinical implementation entails intricate tasks such as multimodal image interpretation, personalized dose planning, and real-time intraoperative decision-making processes that demand intelligent assistance to improve efficiency and reliability. We introduce FUAS-Agents, an autonomous agent system that leverages the multimodal understanding and tool-using capabilities of large language models (LLMs). By integrating patient profiles and MRI data, FUAS-Agents orchestrates a suite of specialized medical AI tools, including segmentation, treatment dose prediction, and clinical guideline retrieval, to generate personalized treatment plans comprising MRI image, dose parameters, and therapeutic strategies. We evaluate the system in a uterine fibroid treatment scenario. Human assessment by four senior FUAS experts indicates that 82.5%, 82.5%, 87.5%, and 97.5% of the generated plans were rated 4 or above (on a 5-point scale) in terms of completeness, accuracy, fluency, and clinical compliance, respectively. These results demonstrate the potential of LLM-driven agents in enhancing decision-making across complex clinical workflows, and exemplify a translational paradigm that combines general-purpose models with specialized expert systems to solve practical challenges in vertical healthcare domains."
  },
  {
    "title": "OVERT: A Benchmark for Over-Refusal Evaluation on Text-to-Image Models",
    "url": "http://arxiv.org/abs/2505.21347v1",
    "arxiv_id": "2505.21347v1",
    "authors": [
      "Ziheng Cheng",
      "Yixiao Huang",
      "Hui Xu",
      "Somayeh Sojoudi",
      "Xuandong Zhao",
      "Dawn Song",
      "Song Mei"
    ],
    "published": "2025-05-27T15:42:46+00:00",
    "summary": "Text-to-Image (T2I) models have achieved remarkable success in generating visual content from text inputs. Although multiple safety alignment strategies have been proposed to prevent harmful outputs, they often lead to overly cautious behavior -- rejecting even benign prompts -- a phenomenon known as $\\textit{over-refusal}$ that reduces the practical utility of T2I models. Despite over-refusal having been observed in practice, there is no large-scale benchmark that systematically evaluates this phenomenon for T2I models. In this paper, we present an automatic workflow to construct synthetic evaluation data, resulting in OVERT ($\\textbf{OVE}$r-$\\textbf{R}$efusal evaluation on $\\textbf{T}$ext-to-image models), the first large-scale benchmark for assessing over-refusal behaviors in T2I models. OVERT includes 4,600 seemingly harmful but benign prompts across nine safety-related categories, along with 1,785 genuinely harmful prompts (OVERT-unsafe) to evaluate the safety-utility trade-off. Using OVERT, we evaluate several leading T2I models and find that over-refusal is a widespread issue across various categories (Figure 1), underscoring the need for further research to enhance the safety alignment of T2I models without compromising their functionality.As a preliminary attempt to reduce over-refusal, we explore prompt rewriting; however, we find it often compromises faithfulness to the meaning of the original prompts. Finally, we demonstrate the flexibility of our generation framework in accommodating diverse safety requirements by generating customized evaluation data adapting to user-defined policies."
  },
  {
    "title": "OVERT: A Benchmark for Over-Refusal Evaluation on Text-to-Image Models",
    "url": "http://arxiv.org/abs/2505.21347v2",
    "arxiv_id": "2505.21347v2",
    "authors": [
      "Ziheng Cheng",
      "Yixiao Huang",
      "Hui Xu",
      "Somayeh Sojoudi",
      "Xuandong Zhao",
      "Dawn Song",
      "Song Mei"
    ],
    "published": "2025-05-27T15:42:46+00:00",
    "summary": "Text-to-Image (T2I) models have achieved remarkable success in generating visual content from text inputs. Although multiple safety alignment strategies have been proposed to prevent harmful outputs, they often lead to overly cautious behavior -- rejecting even benign prompts -- a phenomenon known as $\\textit{over-refusal}$ that reduces the practical utility of T2I models. Despite over-refusal having been observed in practice, there is no large-scale benchmark that systematically evaluates this phenomenon for T2I models. In this paper, we present an automatic workflow to construct synthetic evaluation data, resulting in OVERT ($\\textbf{OVE}$r-$\\textbf{R}$efusal evaluation on $\\textbf{T}$ext-to-image models), the first large-scale benchmark for assessing over-refusal behaviors in T2I models. OVERT includes 4,600 seemingly harmful but benign prompts across nine safety-related categories, along with 1,785 genuinely harmful prompts (OVERT-unsafe) to evaluate the safety-utility trade-off. Using OVERT, we evaluate several leading T2I models and find that over-refusal is a widespread issue across various categories (Figure 1), underscoring the need for further research to enhance the safety alignment of T2I models without compromising their functionality. As a preliminary attempt to reduce over-refusal, we explore prompt rewriting; however, we find it often compromises faithfulness to the meaning of the original prompts. Finally, we demonstrate the flexibility of our generation framework in accommodating diverse safety requirements by generating customized evaluation data adapting to user-defined policies."
  },
  {
    "title": "The Multilingual Divide and Its Impact on Global AI Safety",
    "url": "http://arxiv.org/abs/2505.21344v1",
    "arxiv_id": "2505.21344v1",
    "authors": [
      "Aidan Peppin",
      "Julia Kreutzer",
      "Alice Schoenauer Sebag",
      "Kelly Marchisio",
      "Beyza Ermis",
      "John Dang",
      "Samuel Cahyawijaya",
      "Shivalika Singh",
      "Seraphina Goldfarb-Tarrant",
      "Viraat Aryabumi",
      "Aakanksha",
      "Wei-Yin Ko",
      "Ahmet \u00dcst\u00fcn",
      "Matthias Gall\u00e9",
      "Marzieh Fadaee",
      "Sara Hooker"
    ],
    "published": "2025-05-27T15:37:32+00:00",
    "summary": "Despite advances in large language model capabilities in recent years, a large gap remains in their capabilities and safety performance for many languages beyond a relatively small handful of globally dominant languages. This paper provides researchers, policymakers and governance experts with an overview of key challenges to bridging the \"language gap\" in AI and minimizing safety risks across languages. We provide an analysis of why the language gap in AI exists and grows, and how it creates disparities in global AI safety. We identify barriers to address these challenges, and recommend how those working in policy and governance can help address safety concerns associated with the language gap by supporting multilingual dataset creation, transparency, and research."
  },
  {
    "title": "Assured Autonomy with Neuro-Symbolic Perception",
    "url": "http://arxiv.org/abs/2505.21322v1",
    "arxiv_id": "2505.21322v1",
    "authors": [
      "R. Spencer Hallyburton",
      "Miroslav Pajic"
    ],
    "published": "2025-05-27T15:21:06+00:00",
    "summary": "Many state-of-the-art AI models deployed in cyber-physical systems (CPS), while highly accurate, are simply pattern-matchers.~With limited security guarantees, there are concerns for their reliability in safety-critical and contested domains. To advance assured AI, we advocate for a paradigm shift that imbues data-driven perception models with symbolic structure, inspired by a human's ability to reason over low-level features and high-level context. We propose a neuro-symbolic paradigm for perception (NeuSPaPer) and illustrate how joint object detection and scene graph generation (SGG) yields deep scene understanding.~Powered by foundation models for offline knowledge extraction and specialized SGG algorithms for real-time deployment, we design a framework leveraging structured relational graphs that ensures the integrity of situational awareness in autonomy. Using physics-based simulators and real-world datasets, we demonstrate how SGG bridges the gap between low-level sensor perception and high-level reasoning, establishing a foundation for resilient, context-aware AI and advancing trusted autonomy in CPS."
  },
  {
    "title": "rStar-Coder: Scaling Competitive Code Reasoning with a Large-Scale Verified Dataset",
    "url": "http://arxiv.org/abs/2505.21297v1",
    "arxiv_id": "2505.21297v1",
    "authors": [
      "Yifei Liu",
      "Li Lyna Zhang",
      "Yi Zhu",
      "Bingcheng Dong",
      "Xudong Zhou",
      "Ning Shang",
      "Fan Yang",
      "Mao Yang"
    ],
    "published": "2025-05-27T15:00:57+00:00",
    "summary": "Advancing code reasoning in large language models (LLMs) is fundamentally limited by the scarcity of high-difficulty datasets, especially those with verifiable input-output test cases necessary for rigorous solution validation at scale. We introduce rStar-Coder, which significantly improves LLM code reasoning capabilities by constructing a large-scale, verified dataset of 418K competition-level code problems, 580K long-reasoning solutions along with rich test cases of varying difficulty. This is achieved through three core contributions: (1) we curate competitive programming code problems and oracle solutions to synthesize new, solvable problems; (2) we introduce a reliable input-output test case synthesis pipeline that decouples the generation into a three-step input generation method and a mutual verification mechanism for effective output labeling; (3) we augment problems with high-quality, test-case-verified long-reasoning solutions. Extensive experiments on Qwen models (1.5B-14B) across various code reasoning benchmarks demonstrate the superiority of rStar-Coder dataset, achieving leading performance comparable to frontier reasoning LLMs with much smaller model sizes. On LiveCodeBench, rStar-Coder improves Qwen2.5-7B from 17.4% to an impressive 57.3%, and Qwen2.5-14B from 23.3% to 62.5%, surpassing o3-mini (low) by3.1%. On the more challenging USA Computing Olympiad, our 7B model achieves an average pass@1 accuracy of 16.15%, outperforming the frontier-level QWQ-32B. Code and the dataset will be released at https://github.com/microsoft/rStar."
  },
  {
    "title": "Complex System Diagnostics Using a Knowledge Graph-Informed and Large Language Model-Enhanced Framework",
    "url": "http://arxiv.org/abs/2505.21291v1",
    "arxiv_id": "2505.21291v1",
    "authors": [
      "Saman Marandi",
      "Yu-Shu Hu",
      "Mohammad Modarres"
    ],
    "published": "2025-05-27T14:54:49+00:00",
    "summary": "In this paper, we present a novel diagnostic framework that integrates Knowledge Graphs (KGs) and Large Language Models (LLMs) to support system diagnostics in high-reliability systems such as nuclear power plants. Traditional diagnostic modeling struggles when systems become too complex, making functional modeling a more attractive approach. Our approach introduces a diagnostic framework grounded in the functional modeling principles of the Dynamic Master Logic (DML) model. It incorporates two coordinated LLM components, including an LLM-based workflow for automated construction of DML logic from system documentation and an LLM agent that facilitates interactive diagnostics. The generated logic is encoded into a structured KG, referred to as KG-DML, which supports hierarchical fault reasoning. Expert knowledge or operational data can also be incorporated to refine the model's precision and diagnostic depth. In the interaction phase, users submit natural language queries, which are interpreted by the LLM agent. The agent selects appropriate tools for structured reasoning, including upward and downward propagation across the KG-DML. Rather than embedding KG content into every prompt, the LLM agent distinguishes between diagnostic and interpretive tasks. For diagnostics, the agent selects and executes external tools that perform structured KG reasoning. For general queries, a Graph-based Retrieval-Augmented Generation (Graph-RAG) approach is used, retrieving relevant KG segments and embedding them into the prompt to generate natural explanations. A case study on an auxiliary feedwater system demonstrated the framework's effectiveness, with over 90% accuracy in key elements and consistent tool and argument extraction, supporting its use in safety-critical diagnostics."
  },
  {
    "title": "Breaking the Ceiling: Exploring the Potential of Jailbreak Attacks through Expanding Strategy Space",
    "url": "http://arxiv.org/abs/2505.21277v1",
    "arxiv_id": "2505.21277v1",
    "authors": [
      "Yao Huang",
      "Yitong Sun",
      "Shouwei Ruan",
      "Yichi Zhang",
      "Yinpeng Dong",
      "Xingxing Wei"
    ],
    "published": "2025-05-27T14:48:44+00:00",
    "summary": "Large Language Models (LLMs), despite advanced general capabilities, still suffer from numerous safety risks, especially jailbreak attacks that bypass safety protocols. Understanding these vulnerabilities through black-box jailbreak attacks, which better reflect real-world scenarios, offers critical insights into model robustness. While existing methods have shown improvements through various prompt engineering techniques, their success remains limited against safety-aligned models, overlooking a more fundamental problem: the effectiveness is inherently bounded by the predefined strategy spaces. However, expanding this space presents significant challenges in both systematically capturing essential attack patterns and efficiently navigating the increased complexity. To better explore the potential of expanding the strategy space, we address these challenges through a novel framework that decomposes jailbreak strategies into essential components based on the Elaboration Likelihood Model (ELM) theory and develops genetic-based optimization with intention evaluation mechanisms. To be striking, our experiments reveal unprecedented jailbreak capabilities by expanding the strategy space: we achieve over 90% success rate on Claude-3.5 where prior methods completely fail, while demonstrating strong cross-model transferability and surpassing specialized safeguard models in evaluation accuracy. The code is open-sourced at: https://github.com/Aries-iai/CL-GSO."
  },
  {
    "title": "Breaking the Ceiling: Exploring the Potential of Jailbreak Attacks through Expanding Strategy Space",
    "url": "http://arxiv.org/abs/2505.21277v2",
    "arxiv_id": "2505.21277v2",
    "authors": [
      "Yao Huang",
      "Yitong Sun",
      "Shouwei Ruan",
      "Yichi Zhang",
      "Yinpeng Dong",
      "Xingxing Wei"
    ],
    "published": "2025-05-27T14:48:44+00:00",
    "summary": "Large Language Models (LLMs), despite advanced general capabilities, still suffer from numerous safety risks, especially jailbreak attacks that bypass safety protocols. Understanding these vulnerabilities through black-box jailbreak attacks, which better reflect real-world scenarios, offers critical insights into model robustness. While existing methods have shown improvements through various prompt engineering techniques, their success remains limited against safety-aligned models, overlooking a more fundamental problem: the effectiveness is inherently bounded by the predefined strategy spaces. However, expanding this space presents significant challenges in both systematically capturing essential attack patterns and efficiently navigating the increased complexity. To better explore the potential of expanding the strategy space, we address these challenges through a novel framework that decomposes jailbreak strategies into essential components based on the Elaboration Likelihood Model (ELM) theory and develops genetic-based optimization with intention evaluation mechanisms. To be striking, our experiments reveal unprecedented jailbreak capabilities by expanding the strategy space: we achieve over 90% success rate on Claude-3.5 where prior methods completely fail, while demonstrating strong cross-model transferability and surpassing specialized safeguard models in evaluation accuracy. The code is open-sourced at: https://github.com/Aries-iai/CL-GSO."
  },
  {
    "title": "A machine learning-enabled search for binary black hole mergers in LIGO-Virgo-KAGRAs third observing run",
    "url": "http://arxiv.org/abs/2505.21261v1",
    "arxiv_id": "2505.21261v1",
    "authors": [
      "Ethan Marx",
      "William Benoit",
      "Trevor Blodgett",
      "Deep Chatterjee",
      "Emma de Bruin",
      "Steven Henderson",
      "Katrine Kompanets",
      "Siddharth Soni",
      "Michael Coughlin",
      "Philip Harris",
      "Erik Katsavounidis"
    ],
    "published": "2025-05-27T14:39:45+00:00",
    "summary": "We conduct a search for stellar-mass binary black hole mergers in gravitational-wave data collected by the LIGO detectors during the LIGO-Virgo-KAGRA (LVK) third observing run (O3). Our search uses a machine learning (ML) based method, Aframe, an alternative to traditional matched filtering search techniques. The O3 observing run has been analyzed by the LVK collaboration, producing GWTC-3, the most recent catalog installment which has been made publicly available in 2021. Various groups outside the LVK have re-analyzed O3 data using both traditional and ML-based approaches. Here, we identify 38 candidates with probability of astrophysical origin ($p_\\mathrm{astro}$) greater than 0.5, which were previously reported in GWTC-3. This is comparable to the number of candidates reported by individual matched-filter searches. In addition, we compare Aframe candidates with catalogs from research groups outside of the LVK, identifying three candidates with $p_\\mathrm{astro} > 0.5$. No previously un-reported candidates are identified by Aframe. This work demonstrates that Aframe, and ML based searches more generally, are useful companions to matched filtering pipelines."
  },
  {
    "title": "Active Learning-Enhanced Dual Control for Angle-Only Initial Relative Orbit Determination",
    "url": "http://arxiv.org/abs/2505.21248v1",
    "arxiv_id": "2505.21248v1",
    "authors": [
      "Kui Xie",
      "Giovanni Romagnoli",
      "Giordana Bucchioni",
      "Alberto Bemporad"
    ],
    "published": "2025-05-27T14:26:42+00:00",
    "summary": "Accurate relative orbit determination is a key challenge in modern space operations, particularly when relying on angle-only measurements. The inherent observability limitations of this approach make initial state estimation difficult, impacting mission safety and performance. This work explores the use of active learning (AL) techniques to enhance observability by dynamically designing the input excitation signal offline and at runtime. Our approach leverages AL to design the input signal dynamically, enhancing the observability of the system without requiring additional hardware or predefined maneuvers. We incorporate a dual control technique to ensure target tracking while maintaining observability. The proposed method is validated through numerical simulations, demonstrating its effectiveness in estimating the initial relative state of the chaser and target spacecrafts and its robustness to various initial relative distances and observation periods."
  },
  {
    "title": "PoisonSwarm: Universal Harmful Information Synthesis via Model Crowdsourcing",
    "url": "http://arxiv.org/abs/2505.21184v1",
    "arxiv_id": "2505.21184v1",
    "authors": [
      "Yu Yan",
      "Sheng Sun",
      "Zhifei Zheng",
      "Ziji Hao",
      "Teli Liu",
      "Min Liu"
    ],
    "published": "2025-05-27T13:33:57+00:00",
    "summary": "To construct responsible and secure AI applications, harmful information data is widely utilized for adversarial testing and the development of safeguards. Existing studies mainly leverage Large Language Models (LLMs) to synthesize data to obtain high-quality task datasets at scale, thereby avoiding costly human annotation. However, limited by the safety alignment mechanisms of LLMs, the synthesis of harmful data still faces challenges in generation reliability and content diversity. In this study, we propose a novel harmful information synthesis framework, PoisonSwarm, which applies the model crowdsourcing strategy to generate diverse harmful data while maintaining a high success rate. Specifically, we generate abundant benign data as the based templates in a counterfactual manner. Subsequently, we decompose each based template into multiple semantic units and perform unit-by-unit toxification and final refinement through dynamic model switching, thus ensuring the success of synthesis. Experimental results demonstrate that PoisonSwarm achieves state-of-the-art performance in synthesizing different categories of harmful data with high scalability and diversity."
  },
  {
    "title": "Walk Before You Run! Concise LLM Reasoning via Reinforcement Learning",
    "url": "http://arxiv.org/abs/2505.21178v1",
    "arxiv_id": "2505.21178v1",
    "authors": [
      "Mingyang Song",
      "Mao Zheng"
    ],
    "published": "2025-05-27T13:29:51+00:00",
    "summary": "As test-time scaling becomes a pivotal research frontier in Large Language Models (LLMs) development, contemporary and advanced post-training methodologies increasingly focus on extending the generation length of long Chain-of-Thought (CoT) responses to enhance reasoning capabilities toward DeepSeek R1-like performance. However, recent studies reveal a persistent overthinking phenomenon in state-of-the-art reasoning models, manifesting as excessive redundancy or repetitive thinking patterns in long CoT responses. To address this issue, in this paper, we propose a simple yet effective two-stage reinforcement learning framework for achieving concise reasoning in LLMs, named ConciseR. Specifically, the first stage, using more training steps, aims to incentivize the model's reasoning capabilities via Group Relative Policy Optimization with clip-higher and dynamic sampling components (GRPO++), and the second stage, using fewer training steps, explicitly enforces conciseness and improves efficiency via Length-aware Group Relative Policy Optimization (L-GRPO). Significantly, ConciseR only optimizes response length once all rollouts of a sample are correct, following the \"walk before you run\" principle. Extensive experimental results demonstrate that our ConciseR model, which generates more concise CoT reasoning responses, outperforms recent state-of-the-art reasoning models with zero RL paradigm across AIME 2024, MATH-500, AMC 2023, Minerva, and Olympiad benchmarks."
  },
  {
    "title": "TAT-R1: Terminology-Aware Translation with Reinforcement Learning and Word Alignment",
    "url": "http://arxiv.org/abs/2505.21172v1",
    "arxiv_id": "2505.21172v1",
    "authors": [
      "Zheng Li",
      "Mao Zheng",
      "Mingyang Song",
      "Wenjie Yang"
    ],
    "published": "2025-05-27T13:26:02+00:00",
    "summary": "Recently, deep reasoning large language models(LLMs) like DeepSeek-R1 have made significant progress in tasks such as mathematics and coding. Inspired by this, several studies have employed reinforcement learning(RL) to enhance models' deep reasoning capabilities and improve machine translation(MT) quality. However, the terminology translation, an essential task in MT, remains unexplored in deep reasoning LLMs. In this paper, we propose \\textbf{TAT-R1}, a terminology-aware translation model trained with reinforcement learning and word alignment. Specifically, we first extract the keyword translation pairs using a word alignment model. Then we carefully design three types of rule-based alignment rewards with the extracted alignment relationships. With those alignment rewards, the RL-trained translation model can learn to focus on the accurate translation of key information, including terminology in the source text. Experimental results show the effectiveness of TAT-R1. Our model significantly improves terminology translation accuracy compared to the baseline models while maintaining comparable performance on general translation tasks. In addition, we conduct detailed ablation studies of the DeepSeek-R1-like training paradigm for machine translation and reveal several key findings."
  },
  {
    "title": "Collision Probability Estimation for Optimization-based Vehicular Motion Planning",
    "url": "http://arxiv.org/abs/2505.21161v1",
    "arxiv_id": "2505.21161v1",
    "authors": [
      "Leon Tolksdorf",
      "Arturo Tejada",
      "Christian Birkner",
      "Nathan van de Wouw"
    ],
    "published": "2025-05-27T13:16:03+00:00",
    "summary": "Many motion planning algorithms for automated driving require estimating the probability of collision (POC) to account for uncertainties in the measurement and estimation of the motion of road users. Common POC estimation techniques often utilize sampling-based methods that suffer from computational inefficiency and a non-deterministic estimation, i.e., each estimation result for the same inputs is slightly different. In contrast, optimization-based motion planning algorithms require computationally efficient POC estimation, ideally using deterministic estimation, such that typical optimization algorithms for motion planning retain feasibility. Estimating the POC analytically, however, is challenging because it depends on understanding the collision conditions (e.g., vehicle's shape) and characterizing the uncertainty in motion prediction. In this paper, we propose an approach in which we estimate the POC between two vehicles by over-approximating their shapes by a multi-circular shape approximation. The position and heading of the predicted vehicle are modelled as random variables, contrasting with the literature, where the heading angle is often neglected. We guarantee that the provided POC is an over-approximation, which is essential in providing safety guarantees, and present a computationally efficient algorithm for computing the POC estimate for Gaussian uncertainty in the position and heading. This algorithm is then used in a path-following stochastic model predictive controller (SMPC) for motion planning. With the proposed algorithm, the SMPC generates reproducible trajectories while the controller retains its feasibility in the presented test cases and demonstrates the ability to handle varying levels of uncertainty."
  },
  {
    "title": "Thinker: Learning to Think Fast and Slow",
    "url": "http://arxiv.org/abs/2505.21097v1",
    "arxiv_id": "2505.21097v1",
    "authors": [
      "Stephen Chung",
      "Wenyu Du",
      "Jie Fu"
    ],
    "published": "2025-05-27T12:22:46+00:00",
    "summary": "Recent studies show that the reasoning capabilities of Large Language Models (LLMs) can be improved by applying Reinforcement Learning (RL) to question-answering (QA) tasks in areas such as math and coding. With a long context length, LLMs may learn to perform search, as indicated by the self-correction behavior observed in DeepSeek R1. However, this search behavior is often imprecise and lacks confidence, resulting in long, redundant responses and highlighting deficiencies in intuition and verification. Inspired by the Dual Process Theory in psychology, we introduce a simple modification to the QA task that includes four stages: Fast Thinking, where the LLM must answer within a strict token budget; Verification, where the model evaluates its initial response; Slow Thinking, where it refines the initial response with more deliberation; and Summarization, where it distills the refinement from the previous stage into precise steps. Our proposed task improves average accuracy from 24.9% to 27.9% for Qwen2.5-1.5B, and from 45.9% to 49.8% for DeepSeek-R1-Qwen-1.5B. Notably, for Qwen2.5-1.5B, the Fast Thinking mode alone achieves 26.8% accuracy using fewer than 1000 tokens, demonstrating substantial inference efficiency gains. These findings suggest that intuition and deliberative reasoning are distinct, complementary systems benefiting from targeted training."
  },
  {
    "title": "Stopping Criteria for Value Iteration on Concurrent Stochastic Reachability and Safety Games",
    "url": "http://arxiv.org/abs/2505.21087v1",
    "arxiv_id": "2505.21087v1",
    "authors": [
      "Marta Grobelna",
      "Jan K\u0159et\u00ednsk\u00fd",
      "Maximilian Weininger"
    ],
    "published": "2025-05-27T12:13:47+00:00",
    "summary": "We consider two-player zero-sum concurrent stochastic games (CSGs) played on graphs with reachability and safety objectives. These include degenerate classes such as Markov decision processes or turn-based stochastic games, which can be solved by linear or quadratic programming; however, in practice, value iteration (VI) outperforms the other approaches and is the most implemented method. Similarly, for CSGs, this practical performance makes VI an attractive alternative to the standard theoretical solution via the existential theory of reals.   VI starts with an under-approximation of the sought values for each state and iteratively updates them, traditionally terminating once two consecutive approximations are $\\epsilon$-close. However, this stopping criterion lacks guarantees on the precision of the approximation, which is the goal of this work. We provide bounded (a.k.a. interval) VI for CSGs: it complements standard VI with a converging sequence of over-approximations and terminates once the over- and under-approximations are $\\epsilon$-close."
  },
  {
    "title": "Efficient Large Language Model Inference with Neural Block Linearization",
    "url": "http://arxiv.org/abs/2505.21077v1",
    "arxiv_id": "2505.21077v1",
    "authors": [
      "Mete Erdogan",
      "Francesco Tonin",
      "Volkan Cevher"
    ],
    "published": "2025-05-27T12:01:43+00:00",
    "summary": "The high inference demands of transformer-based Large Language Models (LLMs) pose substantial challenges in their deployment. To this end, we introduce Neural Block Linearization (NBL), a novel framework for accelerating transformer model inference by replacing self-attention layers with linear approximations derived from Linear Minimum Mean Squared Error estimators. NBL leverages Canonical Correlation Analysis to compute a theoretical upper bound on the approximation error. Then, we use this bound as a criterion for substitution, selecting the LLM layers with the lowest linearization error. NBL can be efficiently applied to pre-trained LLMs without the need for fine-tuning. In experiments, NBL achieves notable computational speed-ups while preserving competitive accuracy on multiple reasoning benchmarks. For instance, applying NBL to 12 self-attention layers in DeepSeek-R1-Distill-Llama-8B increases the inference speed by 32% with less than 1% accuracy trade-off, making it a flexible and promising solution to improve the inference efficiency of LLMs."
  },
  {
    "title": "Red-Teaming Text-to-Image Systems by Rule-based Preference Modeling",
    "url": "http://arxiv.org/abs/2505.21074v1",
    "arxiv_id": "2505.21074v1",
    "authors": [
      "Yichuan Cao",
      "Yibo Miao",
      "Xiao-Shan Gao",
      "Yinpeng Dong"
    ],
    "published": "2025-05-27T12:00:19+00:00",
    "summary": "Text-to-image (T2I) models raise ethical and safety concerns due to their potential to generate inappropriate or harmful images. Evaluating these models' security through red-teaming is vital, yet white-box approaches are limited by their need for internal access, complicating their use with closed-source models. Moreover, existing black-box methods often assume knowledge about the model's specific defense mechanisms, limiting their utility in real-world commercial API scenarios. A significant challenge is how to evade unknown and diverse defense mechanisms. To overcome this difficulty, we propose a novel Rule-based Preference modeling Guided Red-Teaming (RPG-RT), which iteratively employs LLM to modify prompts to query and leverages feedback from T2I systems for fine-tuning the LLM. RPG-RT treats the feedback from each iteration as a prior, enabling the LLM to dynamically adapt to unknown defense mechanisms. Given that the feedback is often labeled and coarse-grained, making it difficult to utilize directly, we further propose rule-based preference modeling, which employs a set of rules to evaluate desired or undesired feedback, facilitating finer-grained control over the LLM's dynamic adaptation process. Extensive experiments on nineteen T2I systems with varied safety mechanisms, three online commercial API services, and T2V models verify the superiority and practicality of our approach."
  },
  {
    "title": "Robust Video-Based Pothole Detection and Area Estimation for Intelligent Vehicles with Depth Map and Kalman Smoothing",
    "url": "http://arxiv.org/abs/2505.21049v1",
    "arxiv_id": "2505.21049v1",
    "authors": [
      "Dehao Wang",
      "Haohang Zhu",
      "Yiwen Xu",
      "Kaiqi Liu"
    ],
    "published": "2025-05-27T11:32:45+00:00",
    "summary": "Road potholes pose a serious threat to driving safety and comfort, making their detection and assessment a critical task in fields such as autonomous driving. When driving vehicles, the operators usually avoid large potholes and approach smaller ones at reduced speeds to ensure safety. Therefore, accurately estimating pothole area is of vital importance. Most existing vision-based methods rely on distance priors to construct geometric models. However, their performance is susceptible to variations in camera angles and typically relies on the assumption of a flat road surface, potentially leading to significant errors in complex real-world environments. To address these problems, a robust pothole area estimation framework that integrates object detection and monocular depth estimation in a video stream is proposed in this paper. First, to enhance pothole feature extraction and improve the detection of small potholes, ACSH-YOLOv8 is proposed with ACmix module and the small object detection head. Then, the BoT-SORT algorithm is utilized for pothole tracking, while DepthAnything V2 generates depth maps for each frame. With the obtained depth maps and potholes labels, a novel Minimum Bounding Triangulated Pixel (MBTP) method is proposed for pothole area estimation. Finally, Kalman Filter based on Confidence and Distance (CDKF) is developed to maintain consistency of estimation results across consecutive frames. The results show that ACSH-YOLOv8 model achieves an AP(50) of 76.6%, representing a 7.6% improvement over YOLOv8. Through CDKF optimization across consecutive frames, pothole predictions become more robust, thereby enhancing the method's practical applicability."
  },
  {
    "title": "RefAV: Towards Planning-Centric Scenario Mining",
    "url": "http://arxiv.org/abs/2505.20981v1",
    "arxiv_id": "2505.20981v1",
    "authors": [
      "Cainan Davidson",
      "Deva Ramanan",
      "Neehar Peri"
    ],
    "published": "2025-05-27T10:14:35+00:00",
    "summary": "Autonomous Vehicles (AVs) collect and pseudo-label terabytes of multi-modal data localized to HD maps during normal fleet testing. However, identifying interesting and safety-critical scenarios from uncurated driving logs remains a significant challenge. Traditional scenario mining techniques are error-prone and prohibitively time-consuming, often relying on hand-crafted structured queries. In this work, we revisit spatio-temporal scenario mining through the lens of recent vision-language models (VLMs) to detect whether a described scenario occurs in a driving log and, if so, precisely localize it in both time and space. To address this problem, we introduce RefAV, a large-scale dataset of 10,000 diverse natural language queries that describe complex multi-agent interactions relevant to motion planning derived from 1000 driving logs in the Argoverse 2 Sensor dataset. We evaluate several referential multi-object trackers and present an empirical analysis of our baselines. Notably, we find that naively repurposing off-the-shelf VLMs yields poor performance, suggesting that scenario mining presents unique challenges. Our code and dataset are available at https://github.com/CainanD/RefAV/ and https://argoverse.github.io/user-guide/tasks/scenario_mining.html"
  },
  {
    "title": "SCALOFT: An Initial Approach for Situation Coverage-Based Safety Analysis of an Autonomous Aerial Drone in a Mine Environment",
    "url": "http://arxiv.org/abs/2505.20969v1",
    "arxiv_id": "2505.20969v1",
    "authors": [
      "Nawshin Mannan Proma",
      "Victoria J Hodge",
      "Rob Alexander"
    ],
    "published": "2025-05-27T10:03:08+00:00",
    "summary": "The safety of autonomous systems in dynamic and hazardous environments poses significant challenges. This paper presents a testing approach named SCALOFT for systematically assessing the safety of an autonomous aerial drone in a mine. SCALOFT provides a framework for developing diverse test cases, real-time monitoring of system behaviour, and detection of safety violations. Detected violations are then logged with unique identifiers for detailed analysis and future improvement. SCALOFT helps build a safety argument by monitoring situation coverage and calculating a final coverage measure. We have evaluated the performance of this approach by deliberately introducing seeded faults into the system and assessing whether SCALOFT is able to detect those faults. For a small set of plausible faults, we show that SCALOFT is successful in this."
  },
  {
    "title": "Generalized Coordination of Partially Cooperative Urban Traffic",
    "url": "http://arxiv.org/abs/2505.20879v1",
    "arxiv_id": "2505.20879v1",
    "authors": [
      "Max Bastian Mertens",
      "Michael Buchholz"
    ],
    "published": "2025-05-27T08:25:57+00:00",
    "summary": "Vehicle-to-anything connectivity, especially for autonomous vehicles, promises to increase passenger comfort and safety of road traffic, for example, by sharing perception and driving intention. Cooperative maneuver planning uses connectivity to enhance traffic efficiency, which has, so far, been mainly considered for automated intersection management. In this article, we present a novel cooperative maneuver planning approach that is generalized to various situations found in urban traffic. Our framework handles challenging mixed traffic, that is, traffic comprising both cooperative connected vehicles and other vehicles at any distribution. Our solution is based on an optimization approach accompanied by an efficient heuristic method for high-load scenarios. We extensively evaluate the proposed planer in a distinctly realistic simulation framework and show significant efficiency gains already at a cooperation rate of 40%. Traffic throughput increases, while the average waiting time and the number of stopped vehicles are reduced, without impacting traffic safety."
  },
  {
    "title": "Collision-free Control Barrier Functions for General Ellipsoids via Separating Hyperplane",
    "url": "http://arxiv.org/abs/2505.20847v1",
    "arxiv_id": "2505.20847v1",
    "authors": [
      "Zeming Wu",
      "Lu Liu"
    ],
    "published": "2025-05-27T08:01:42+00:00",
    "summary": "This paper presents a novel collision avoidance method for general ellipsoids based on control barrier functions (CBFs) and separating hyperplanes. First, collision-free conditions for general ellipsoids are analytically derived using the concept of dual cones. These conditions are incorporated into the CBF framework by extending the system dynamics of controlled objects with separating hyperplanes, enabling efficient and reliable collision avoidance. The validity of the proposed collision-free CBFs is rigorously proven, ensuring their effectiveness in enforcing safety constraints. The proposed method requires only single-level optimization, significantly reducing computational time compared to state-of-the-art methods. Numerical simulations and real-world experiments demonstrate the effectiveness and practicality of the proposed algorithm."
  },
  {
    "title": "MedSentry: Understanding and Mitigating Safety Risks in Medical LLM Multi-Agent Systems",
    "url": "http://arxiv.org/abs/2505.20824v1",
    "arxiv_id": "2505.20824v1",
    "authors": [
      "Kai Chen",
      "Taihang Zhen",
      "Hewei Wang",
      "Kailai Liu",
      "Xinfeng Li",
      "Jing Huo",
      "Tianpei Yang",
      "Jinfeng Xu",
      "Wei Dong",
      "Yang Gao"
    ],
    "published": "2025-05-27T07:34:40+00:00",
    "summary": "As large language models (LLMs) are increasingly deployed in healthcare, ensuring their safety, particularly within collaborative multi-agent configurations, is paramount. In this paper we introduce MedSentry, a benchmark comprising 5 000 adversarial medical prompts spanning 25 threat categories with 100 subthemes. Coupled with this dataset, we develop an end-to-end attack-defense evaluation pipeline to systematically analyze how four representative multi-agent topologies (Layers, SharedPool, Centralized, and Decentralized) withstand attacks from 'dark-personality' agents. Our findings reveal critical differences in how these architectures handle information contamination and maintain robust decision-making, exposing their underlying vulnerability mechanisms. For instance, SharedPool's open information sharing makes it highly susceptible, whereas Decentralized architectures exhibit greater resilience thanks to inherent redundancy and isolation. To mitigate these risks, we propose a personality-scale detection and correction mechanism that identifies and rehabilitates malicious agents, restoring system safety to near-baseline levels. MedSentry thus furnishes both a rigorous evaluation framework and practical defense strategies that guide the design of safer LLM-based multi-agent systems in medical domains."
  },
  {
    "title": "Improved Representation Steering for Language Models",
    "url": "http://arxiv.org/abs/2505.20809v1",
    "arxiv_id": "2505.20809v1",
    "authors": [
      "Zhengxuan Wu",
      "Qinan Yu",
      "Aryaman Arora",
      "Christopher D. Manning",
      "Christopher Potts"
    ],
    "published": "2025-05-27T07:16:40+00:00",
    "summary": "Steering methods for language models (LMs) seek to provide fine-grained and interpretable control over model generations by variously changing model inputs, weights, or representations to adjust behavior. Recent work has shown that adjusting weights or representations is often less effective than steering by prompting, for instance when wanting to introduce or suppress a particular concept. We demonstrate how to improve representation steering via our new Reference-free Preference Steering (RePS), a bidirectional preference-optimization objective that jointly does concept steering and suppression. We train three parameterizations of RePS and evaluate them on AxBench, a large-scale model steering benchmark. On Gemma models with sizes ranging from 2B to 27B, RePS outperforms all existing steering methods trained with a language modeling objective and substantially narrows the gap with prompting -- while promoting interpretability and minimizing parameter count. In suppression, RePS matches the language-modeling objective on Gemma-2 and outperforms it on the larger Gemma-3 variants while remaining resilient to prompt-based jailbreaking attacks that defeat prompting. Overall, our results suggest that RePS provides an interpretable and robust alternative to prompting for both steering and suppression."
  },
  {
    "title": "Describe Me Something You Do Not Remember - Challenges and Risks of Exposure Design Using Generative Artificial Intelligence for Therapy of Complex Post-traumatic Disorder",
    "url": "http://arxiv.org/abs/2505.20796v1",
    "arxiv_id": "2505.20796v1",
    "authors": [
      "Annalisa Degenhard",
      "Stefan Tsch\u00f6ke",
      "Michael Rietzler",
      "Enrico Rukzio"
    ],
    "published": "2025-05-27T06:57:50+00:00",
    "summary": "Post-traumatic stress disorder (PTSD) is associated with sudden, uncontrollable, and intense flashbacks of traumatic memories. Trauma exposure psychotherapy has proven effective in reducing the severity of trauma-related symptoms. It involves controlled recall of traumatic memories to train coping mechanisms for flashbacks and enable autobiographical integration of distressing experiences. In particular, exposure to visualizations of these memories supports successful recall. Although this approach is effective for various trauma types, it remains available for only a few. This is due to the lack of cost-efficient solutions for creating individualized exposure visualizations. This issue is particularly relevant for the treatment of Complex PTSD (CPTSD), where traumatic memories are highly individual and generic visualizations do not meet therapeutic needs. Generative Artificial Intelligence (GAI) offers a flexible and cost-effective alternative. GAI enables the creation of individualized exposure visualizations during therapy and, for the first time, allows patients to actively participate in the visualization process. While GAI opens new therapeutic perspectives and may improve access to trauma therapy, especially for CPTSD, it also introduces significant challenges and risks. The extreme uncertainty and lack of control that define both CPTSD and GAI raise concerns about feasibility and safety. To support safe and effective three-way communication, it is essential to understand the roles of patient, system, and therapist in exposure visualization and how each can contribute to safety. This paper outlines perspectives, challenges, and risks associated with the use of GAI in trauma therapy, with a focus on CPTSD."
  },
  {
    "title": "TACO: Think-Answer Consistency for Optimized Long-Chain Reasoning and Efficient Data Learning via Reinforcement Learning in LVLMs",
    "url": "http://arxiv.org/abs/2505.20777v1",
    "arxiv_id": "2505.20777v1",
    "authors": [
      "Zhehan Kan",
      "Yanlin Liu",
      "Kun Yin",
      "Xinghua Jiang",
      "Xin Li",
      "Haoyu Cao",
      "Yinsong Liu",
      "Deqiang Jiang",
      "Xing Sun",
      "Qingmin Liao",
      "Wenming Yang"
    ],
    "published": "2025-05-27T06:30:48+00:00",
    "summary": "DeepSeek R1 has significantly advanced complex reasoning for large language models (LLMs). While recent methods have attempted to replicate R1's reasoning capabilities in multimodal settings, they face limitations, including inconsistencies between reasoning and final answers, model instability and crashes during long-chain exploration, and low data learning efficiency. To address these challenges, we propose TACO, a novel reinforcement learning algorithm for visual reasoning. Building on Generalized Reinforcement Policy Optimization (GRPO), TACO introduces Think-Answer Consistency, which tightly couples reasoning with answer consistency to ensure answers are grounded in thoughtful reasoning. We also introduce the Rollback Resample Strategy, which adaptively removes problematic samples and reintroduces them to the sampler, enabling stable long-chain exploration and future learning opportunities. Additionally, TACO employs an adaptive learning schedule that focuses on moderate difficulty samples to optimize data efficiency. Furthermore, we propose the Test-Time-Resolution-Scaling scheme to address performance degradation due to varying resolutions during reasoning while balancing computational overhead. Extensive experiments on in-distribution and out-of-distribution benchmarks for REC and VQA tasks show that fine-tuning LVLMs leads to significant performance improvements."
  },
  {
    "title": "Performance of prior event rate ratio method in the presence of differential mortality or dropout",
    "url": "http://arxiv.org/abs/2505.20757v1",
    "arxiv_id": "2505.20757v1",
    "authors": [
      "Yin Bun Cheung",
      "Xiangmei Ma"
    ],
    "published": "2025-05-27T06:01:24+00:00",
    "summary": "Purpose: Prior event rate ratio (PERR) method was proposed to control for unmeasured confounding in real-world evaluation of effectiveness and safety of pharmaceutical products. A widely cited simulation study showed that PERR estimate of treatment effect was biased in the presence of differential morality/dropout. However, the study only considered one specific PERR estimator of treatment effect and one specific scenario of differential mortality/dropout. To enhance understanding of the method, we replicated and extended the simulation to consider an alternative PERR estimator and multiple scenarios. Methods: Simulation studies were performed with varying rate of mortality/dropout, including the same scenario in the previous study in which mortality/dropout was simultaneously influenced by treatment, confounder and prior event and scenarios that differed in the determinants of mortality/dropout. In addition to the PERR estimator used in the previous study (PERR_Prev) that involved data form both completers and non-completers, we also evaluated an alternative PERR estimator (PERR_Comp) that used data only from completers. Results: The bias of PERR_Prev in the previously considered mortality/dropout scenario was replicated. Bias of PERR_Comp was only about one-third in magnitude as compared to that of PERR_Prev in this scenario. Furthermore, PERR_Prev did but PERR_Comp did not give biased estimates of treatment effect in scenarios that mortality/dropout was influenced by treatment or confounder but not prior event. Conclusions: The PERR is better seen as a methodological framework. Its performance depends on the specifications within the framework. PERR_Comp provides unbiased estimates unless mortality/dropout is affected by prior event."
  },
  {
    "title": "A reinforcement learning agent for maintenance of deteriorating systems with increasingly imperfect repairs",
    "url": "http://arxiv.org/abs/2505.20725v1",
    "arxiv_id": "2505.20725v1",
    "authors": [
      "Alberto Pliego Marug\u00e1n",
      "Jes\u00fas M. Pinar-P\u00e9rez",
      "Fausto Pedro Garc\u00eda M\u00e1rquez"
    ],
    "published": "2025-05-27T05:14:29+00:00",
    "summary": "Efficient maintenance has always been essential for the successful application of engineering systems. However, the challenges to be overcome in the implementation of Industry 4.0 necessitate new paradigms of maintenance optimization. Machine learning techniques are becoming increasingly used in engineering and maintenance, with reinforcement learning being one of the most promising. In this paper, we propose a gamma degradation process together with a novel maintenance model in which repairs are increasingly imperfect, i.e., the beneficial effect of system repairs decreases as more repairs are performed, reflecting the degradational behavior of real-world systems. To generate maintenance policies for this system, we developed a reinforcement-learning-based agent using a Double Deep Q-Network architecture. This agent presents two important advantages: it works without a predefined preventive threshold, and it can operate in a continuous degradation state space. Our agent learns to behave in different scenarios, showing great flexibility. In addition, we performed an analysis of how changes in the main parameters of the environment affect the maintenance policy proposed by the agent. The proposed approach is demonstrated to be appropriate and to significatively improve long-run cost as compared with other common maintenance strategies."
  },
  {
    "title": "IndustryEQA: Pushing the Frontiers of Embodied Question Answering in Industrial Scenarios",
    "url": "http://arxiv.org/abs/2505.20640v1",
    "arxiv_id": "2505.20640v1",
    "authors": [
      "Yifan Li",
      "Yuhang Chen",
      "Anh Dao",
      "Lichi Li",
      "Zhongyi Cai",
      "Zhen Tan",
      "Tianlong Chen",
      "Yu Kong"
    ],
    "published": "2025-05-27T02:36:17+00:00",
    "summary": "Existing Embodied Question Answering (EQA) benchmarks primarily focus on household environments, often overlooking safety-critical aspects and reasoning processes pertinent to industrial settings. This drawback limits the evaluation of agent readiness for real-world industrial applications. To bridge this, we introduce IndustryEQA, the first benchmark dedicated to evaluating embodied agent capabilities within safety-critical warehouse scenarios. Built upon the NVIDIA Isaac Sim platform, IndustryEQA provides high-fidelity episodic memory videos featuring diverse industrial assets, dynamic human agents, and carefully designed hazardous situations inspired by real-world safety guidelines. The benchmark includes rich annotations covering six categories: equipment safety, human safety, object recognition, attribute recognition, temporal understanding, and spatial understanding. Besides, it also provides extra reasoning evaluation based on these categories. Specifically, it comprises 971 question-answer pairs generated from small warehouse and 373 pairs from large ones, incorporating scenarios with and without human. We further propose a comprehensive evaluation framework, including various baseline models, to assess their general perception and reasoning abilities in industrial environments. IndustryEQA aims to steer EQA research towards developing more robust, safety-aware, and practically applicable embodied agents for complex industrial environments. Benchmark and codes are available."
  },
  {
    "title": "Multi-level Certified Defense Against Poisoning Attacks in Offline Reinforcement Learning",
    "url": "http://arxiv.org/abs/2505.20621v1",
    "arxiv_id": "2505.20621v1",
    "authors": [
      "Shijie Liu",
      "Andrew C. Cullen",
      "Paul Montague",
      "Sarah Erfani",
      "Benjamin I. P. Rubinstein"
    ],
    "published": "2025-05-27T01:59:25+00:00",
    "summary": "Similar to other machine learning frameworks, Offline Reinforcement Learning (RL) is shown to be vulnerable to poisoning attacks, due to its reliance on externally sourced datasets, a vulnerability that is exacerbated by its sequential nature. To mitigate the risks posed by RL poisoning, we extend certified defenses to provide larger guarantees against adversarial manipulation, ensuring robustness for both per-state actions, and the overall expected cumulative reward. Our approach leverages properties of Differential Privacy, in a manner that allows this work to span both continuous and discrete spaces, as well as stochastic and deterministic environments -- significantly expanding the scope and applicability of achievable guarantees. Empirical evaluations demonstrate that our approach ensures the performance drops to no more than $50\\%$ with up to $7\\%$ of the training data poisoned, significantly improving over the $0.008\\%$ in prior work~\\citep{wu_copa_2022}, while producing certified radii that is $5$ times larger as well. This highlights the potential of our framework to enhance safety and reliability in offline RL."
  },
  {
    "title": "Developing a Robotic Surgery Training System for Wide Accessibility and Research",
    "url": "http://arxiv.org/abs/2505.20562v1",
    "arxiv_id": "2505.20562v1",
    "authors": [
      "Walid Shaker",
      "Mustafa Suphi Erden"
    ],
    "published": "2025-05-26T22:51:17+00:00",
    "summary": "Robotic surgery represents a major breakthrough in medical interventions, which has revolutionized surgical procedures. However, the high cost and limited accessibility of robotic surgery systems pose significant challenges for training purposes. This study addresses these issues by developing a cost-effective robotic laparoscopy training system that closely replicates advanced robotic surgery setups to ensure broad access for both on-site and remote users. Key innovations include the design of a low-cost robotic end-effector that effectively mimics high-end laparoscopic instruments. Additionally, a digital twin platform was established, facilitating detailed simulation, testing, and real-time monitoring, which enhances both system development and deployment. Furthermore, teleoperation control was optimized, leading to improved trajectory tracking while maintaining remote center of motion (RCM) constraint, with a RMSE of 5 {\\mu}m and reduced system latency to 0.01 seconds. As a result, the system provides smooth, continuous motion and incorporates essential safety features, making it a highly effective tool for laparoscopic training."
  },
  {
    "title": "BlastOFormer: Attention and Neural Operator Deep Learning Methods for Explosive Blast Prediction",
    "url": "http://arxiv.org/abs/2505.20454v1",
    "arxiv_id": "2505.20454v1",
    "authors": [
      "Reid Graves",
      "Anthony Zhou",
      "Amir Barati Farimani"
    ],
    "published": "2025-05-26T18:47:50+00:00",
    "summary": "Accurate prediction of blast pressure fields is essential for applications in structural safety, defense planning, and hazard mitigation. Traditional methods such as empirical models and computational fluid dynamics (CFD) simulations offer limited trade offs between speed and accuracy; empirical models fail to capture complex interactions in cluttered environments, while CFD simulations are computationally expensive and time consuming. In this work, we introduce BlastOFormer, a novel Transformer based surrogate model for full field maximum pressure prediction from arbitrary obstacle and charge configurations. BlastOFormer leverages a signed distance function (SDF) encoding and a grid to grid attention based architecture inspired by OFormer and Vision Transformer (ViT) frameworks. Trained on a dataset generated using the open source blastFoam CFD solver, our model outperforms convolutional neural networks (CNNs) and Fourier Neural Operators (FNOs) across both log transformed and unscaled domains. Quantitatively, BlastOFormer achieves the highest R2 score (0.9516) and lowest error metrics, while requiring only 6.4 milliseconds for inference, more than 600,000 times faster than CFD simulations. Qualitative visualizations and error analyses further confirm BlastOFormer's superior spatial coherence and generalization capabilities. These results highlight its potential as a real time alternative to conventional CFD approaches for blast pressure estimation in complex environments."
  },
  {
    "title": "Vision-Based Risk Aware Emergency Landing for UAVs in Complex Urban Environments",
    "url": "http://arxiv.org/abs/2505.20423v1",
    "arxiv_id": "2505.20423v1",
    "authors": [
      "Julio de la Torre-Vanegas",
      "Miguel Soriano-Garcia",
      "Israel Becerra",
      "Diego Mercado-Ravell"
    ],
    "published": "2025-05-26T18:16:21+00:00",
    "summary": "Landing safely in crowded urban environments remains an essential yet challenging endeavor for Unmanned Aerial Vehicles (UAVs), especially in emergency situations. In this work, we propose a risk-aware approach that harnesses semantic segmentation to continuously evaluate potential hazards in the drone's field of view. By using a specialized deep neural network to assign pixel-level risk values and applying an algorithm based on risk maps, our method adaptively identifies a stable Safe Landing Zone (SLZ) despite moving critical obstacles such as vehicles, people, etc., and other visual challenges like shifting illumination. A control system then guides the UAV toward this low-risk region, employing altitude-dependent safety thresholds and temporal landing point stabilization to ensure robust descent trajectories. Experimental validation in diverse urban environments demonstrates the effectiveness of our approach, achieving over 90% landing success rates in very challenging real scenarios, showing significant improvements in various risk metrics. Our findings suggest that risk-oriented vision methods can effectively help reduce the risk of accidents in emergency landing situations, particularly in complex, unstructured, urban scenarios, densely populated with moving risky obstacles, while potentiating the true capabilities of UAVs in complex urban operations."
  },
  {
    "title": "Ten Principles of AI Agent Economics",
    "url": "http://arxiv.org/abs/2505.20273v1",
    "arxiv_id": "2505.20273v1",
    "authors": [
      "Ke Yang",
      "ChengXiang Zhai"
    ],
    "published": "2025-05-26T17:52:44+00:00",
    "summary": "The rapid rise of AI-based autonomous agents is transforming human society and economic systems, as these entities increasingly exhibit human-like or superhuman intelligence. From excelling at complex games like Go to tackling diverse general-purpose tasks with large language and multimodal models, AI agents are evolving from specialized tools into dynamic participants in social and economic ecosystems. Their autonomy and decision-making capabilities are poised to impact industries, professions, and human lives profoundly, raising critical questions about their integration into economic activities, potential ethical concerns, and the balance between their utility and safety.   To address these challenges, this paper presents ten principles of AI agent economics, offering a framework to understand how AI agents make decisions, influence social interactions, and participate in the broader economy. Drawing on economics, decision theory, and ethics, we explore fundamental questions, such as whether AI agents might evolve from tools into independent entities, their impact on labor markets, and the ethical safeguards needed to align them with human values. These principles build on existing economic theories while accounting for the unique traits of AI agents, providing a roadmap for their responsible integration into human systems.   Beyond theoretical insights, this paper highlights the urgency of future research into AI trustworthiness, ethical guidelines, and regulatory oversight. As we enter a transformative era, this work serves as both a guide and a call to action, ensuring AI agents contribute positively to human progress while addressing risks tied to their unprecedented capabilities."
  },
  {
    "title": "Lifelong Safety Alignment for Language Models",
    "url": "http://arxiv.org/abs/2505.20259v1",
    "arxiv_id": "2505.20259v1",
    "authors": [
      "Haoyu Wang",
      "Zeyu Qin",
      "Yifei Zhao",
      "Chao Du",
      "Min Lin",
      "Xueqian Wang",
      "Tianyu Pang"
    ],
    "published": "2025-05-26T17:40:40+00:00",
    "summary": "LLMs have made impressive progress, but their growing capabilities also expose them to highly flexible jailbreaking attacks designed to bypass safety alignment. While many existing defenses focus on known types of attacks, it is more critical to prepare LLMs for unseen attacks that may arise during deployment. To address this, we propose a lifelong safety alignment framework that enables LLMs to continuously adapt to new and evolving jailbreaking strategies. Our framework introduces a competitive setup between two components: a Meta-Attacker, trained to actively discover novel jailbreaking strategies, and a Defender, trained to resist them. To effectively warm up the Meta-Attacker, we first leverage the GPT-4o API to extract key insights from a large collection of jailbreak-related research papers. Through iterative training, the first iteration Meta-Attacker achieves a 73% attack success rate (ASR) on RR and a 57% transfer ASR on LAT using only single-turn attacks. Meanwhile, the Defender progressively improves its robustness and ultimately reduces the Meta-Attacker's success rate to just 7%, enabling safer and more reliable deployment of LLMs in open-ended environments. The code is available at https://github.com/sail-sg/LifelongSafetyAlignment."
  },
  {
    "title": "On Path to Multimodal Historical Reasoning: HistBench and HistAgent",
    "url": "http://arxiv.org/abs/2505.20246v1",
    "arxiv_id": "2505.20246v1",
    "authors": [
      "Jiahao Qiu",
      "Fulian Xiao",
      "Yimin Wang",
      "Yuchen Mao",
      "Yijia Chen",
      "Xinzhe Juan",
      "Siran Wang",
      "Xuan Qi",
      "Tongcheng Zhang",
      "Zixin Yao",
      "Jiacheng Guo",
      "Yifu Lu",
      "Charles Argon",
      "Jundi Cui",
      "Daixin Chen",
      "Junran Zhou",
      "Shuyao Zhou",
      "Zhanpeng Zhou",
      "Ling Yang",
      "Shilong Liu",
      "Hongru Wang",
      "Kaixuan Huang",
      "Xun Jiang",
      "Yuming Cao",
      "Yue Chen",
      "Yunfei Chen",
      "Zhengyi Chen",
      "Ruowei Dai",
      "Mengqiu Deng",
      "Jiye Fu",
      "Yunting Gu",
      "Zijie Guan",
      "Zirui Huang",
      "Xiaoyan Ji",
      "Yumeng Jiang",
      "Delong Kong",
      "Haolong Li",
      "Jiaqi Li",
      "Ruipeng Li",
      "Tianze Li",
      "Zhuoran Li",
      "Haixia Lian",
      "Mengyue Lin",
      "Xudong Liu",
      "Jiayi Lu",
      "Jinghan Lu",
      "Wanyu Luo",
      "Ziyue Luo",
      "Zihao Pu",
      "Zhi Qiao",
      "Ruihuan Ren",
      "Liang Wan",
      "Ruixiang Wang",
      "Tianhui Wang",
      "Yang Wang",
      "Zeyu Wang",
      "Zihua Wang",
      "Yujia Wu",
      "Zhaoyi Wu",
      "Hao Xin",
      "Weiao Xing",
      "Ruojun Xiong",
      "Weijie Xu",
      "Yao Shu",
      "Xiao Yao",
      "Xiaorui Yang",
      "Yuchen Yang",
      "Nan Yi",
      "Jiadong Yu",
      "Yangyuxuan Yu",
      "Huiting Zeng",
      "Danni Zhang",
      "Yunjie Zhang",
      "Zhaoyu Zhang",
      "Zhiheng Zhang",
      "Xiaofeng Zheng",
      "Peirong Zhou",
      "Linyan Zhong",
      "Xiaoyin Zong",
      "Ying Zhao",
      "Zhenxin Chen",
      "Lin Ding",
      "Xiaoyu Gao",
      "Bingbing Gong",
      "Yichao Li",
      "Yang Liao",
      "Guang Ma",
      "Tianyuan Ma",
      "Xinrui Sun",
      "Tianyi Wang",
      "Han Xia",
      "Ruobing Xian",
      "Gen Ye",
      "Tengfei Yu",
      "Wentao Zhang",
      "Yuxi Wang",
      "Xi Gao",
      "Mengdi Wang"
    ],
    "published": "2025-05-26T17:22:20+00:00",
    "summary": "Recent advances in large language models (LLMs) have led to remarkable progress across domains, yet their capabilities in the humanities, particularly history, remain underexplored. Historical reasoning poses unique challenges for AI, involving multimodal source interpretation, temporal inference, and cross-linguistic analysis. While general-purpose agents perform well on many existing benchmarks, they lack the domain-specific expertise required to engage with historical materials and questions. To address this gap, we introduce HistBench, a new benchmark of 414 high-quality questions designed to evaluate AI's capacity for historical reasoning and authored by more than 40 expert contributors. The tasks span a wide range of historical problems-from factual retrieval based on primary sources to interpretive analysis of manuscripts and images, to interdisciplinary challenges involving archaeology, linguistics, or cultural history. Furthermore, the benchmark dataset spans 29 ancient and modern languages and covers a wide range of historical periods and world regions. Finding the poor performance of LLMs and other agents on HistBench, we further present HistAgent, a history-specific agent equipped with carefully designed tools for OCR, translation, archival search, and image understanding in History. On HistBench, HistAgent based on GPT-4o achieves an accuracy of 27.54% pass@1 and 36.47% pass@2, significantly outperforming LLMs with online search and generalist agents, including GPT-4o (18.60%), DeepSeek-R1(14.49%) and Open Deep Research-smolagents(20.29% pass@1 and 25.12% pass@2). These results highlight the limitations of existing LLMs and generalist agents and demonstrate the advantages of HistAgent for historical reasoning."
  },
  {
    "title": "Variational Deep Learning via Implicit Regularization",
    "url": "http://arxiv.org/abs/2505.20235v1",
    "arxiv_id": "2505.20235v1",
    "authors": [
      "Jonathan Wenger",
      "Beau Coker",
      "Juraj Marusic",
      "John P. Cunningham"
    ],
    "published": "2025-05-26T17:15:57+00:00",
    "summary": "Modern deep learning models generalize remarkably well in-distribution, despite being overparametrized and trained with little to no explicit regularization. Instead, current theory credits implicit regularization imposed by the choice of architecture, hyperparameters and optimization procedure. However, deploying deep learning models out-of-distribution, in sequential decision-making tasks, or in safety-critical domains, necessitates reliable uncertainty quantification, not just a point estimate. The machinery of modern approximate inference -- Bayesian deep learning -- should answer the need for uncertainty quantification, but its effectiveness has been challenged by our inability to define useful explicit inductive biases through priors, as well as the associated computational burden. Instead, in this work we demonstrate, both theoretically and empirically, how to regularize a variational deep network implicitly via the optimization procedure, just as for standard deep learning. We fully characterize the inductive bias of (stochastic) gradient descent in the case of an overparametrized linear model as generalized variational inference and demonstrate the importance of the choice of parametrization. Finally, we show empirically that our approach achieves strong in- and out-of-distribution performance without tuning of additional hyperparameters and with minimal time and memory overhead over standard deep learning."
  },
  {
    "title": "Chain-of-Thought for Autonomous Driving: A Comprehensive Survey and Future Prospects",
    "url": "http://arxiv.org/abs/2505.20223v1",
    "arxiv_id": "2505.20223v1",
    "authors": [
      "Yixin Cui",
      "Haotian Lin",
      "Shuo Yang",
      "Yixiao Wang",
      "Yanjun Huang",
      "Hong Chen"
    ],
    "published": "2025-05-26T17:06:00+00:00",
    "summary": "The rapid evolution of large language models in natural language processing has substantially elevated their semantic understanding and logical reasoning capabilities. Such proficiencies have been leveraged in autonomous driving systems, contributing to significant improvements in system performance. Models such as OpenAI o1 and DeepSeek-R1, leverage Chain-of-Thought (CoT) reasoning, an advanced cognitive method that simulates human thinking processes, demonstrating remarkable reasoning capabilities in complex tasks. By structuring complex driving scenarios within a systematic reasoning framework, this approach has emerged as a prominent research focus in autonomous driving, substantially improving the system's ability to handle challenging cases. This paper investigates how CoT methods improve the reasoning abilities of autonomous driving models. Based on a comprehensive literature review, we present a systematic analysis of the motivations, methodologies, challenges, and future research directions of CoT in autonomous driving. Furthermore, we propose the insight of combining CoT with self-learning to facilitate self-evolution in driving systems. To ensure the relevance and timeliness of this study, we have compiled a dynamic repository of literature and open-source projects, diligently updated to incorporate forefront developments. The repository is publicly available at https://github.com/cuiyx1720/Awesome-CoT4AD."
  },
  {
    "title": "Fine-grained List-wise Alignment for Generative Medication Recommendation",
    "url": "http://arxiv.org/abs/2505.20218v1",
    "arxiv_id": "2505.20218v1",
    "authors": [
      "Chenxiao Fan",
      "Chongming Gao",
      "Wentao Shi",
      "Yaxin Gong",
      "Zihao Zhao",
      "Fuli Feng"
    ],
    "published": "2025-05-26T16:59:23+00:00",
    "summary": "Accurate and safe medication recommendations are critical for effective clinical decision-making, especially in multimorbidity cases. However, existing systems rely on point-wise prediction paradigms that overlook synergistic drug effects and potential adverse drug-drug interactions (DDIs). We propose FLAME, a fine-grained list-wise alignment framework for large language models (LLMs), enabling drug-by-drug generation of drug lists. FLAME formulates recommendation as a sequential decision process, where each step adds or removes a single drug. To provide fine-grained learning signals, we devise step-wise Group Relative Policy Optimization (GRPO) with potential-based reward shaping, which explicitly models DDIs and optimizes the contribution of each drug to the overall prescription. Furthermore, FLAME enhances patient modeling by integrating structured clinical knowledge and collaborative information into the representation space of LLMs. Experiments on benchmark datasets demonstrate that FLAME achieves state-of-the-art performance, delivering superior accuracy, controllable safety-accuracy trade-offs, and strong generalization across diverse clinical scenarios. Our code is available at https://github.com/cxfann/Flame."
  },
  {
    "title": "Capability-Based Scaling Laws for LLM Red-Teaming",
    "url": "http://arxiv.org/abs/2505.20162v1",
    "arxiv_id": "2505.20162v1",
    "authors": [
      "Alexander Panfilov",
      "Paul Kassianik",
      "Maksym Andriushchenko",
      "Jonas Geiping"
    ],
    "published": "2025-05-26T16:05:41+00:00",
    "summary": "As large language models grow in capability and agency, identifying vulnerabilities through red-teaming becomes vital for safe deployment. However, traditional prompt-engineering approaches may prove ineffective once red-teaming turns into a weak-to-strong problem, where target models surpass red-teamers in capabilities. To study this shift, we frame red-teaming through the lens of the capability gap between attacker and target. We evaluate more than 500 attacker-target pairs using LLM-based jailbreak attacks that mimic human red-teamers across diverse families, sizes, and capability levels. Three strong trends emerge: (i) more capable models are better attackers, (ii) attack success drops sharply once the target's capability exceeds the attacker's, and (iii) attack success rates correlate with high performance on social science splits of the MMLU-Pro benchmark. From these trends, we derive a jailbreaking scaling law that predicts attack success for a fixed target based on attacker-target capability gap. These findings suggest that fixed-capability attackers (e.g., humans) may become ineffective against future models, increasingly capable open-source models amplify risks for existing systems, and model providers must accurately measure and control models' persuasive and manipulative abilities to limit their effectiveness as attackers."
  },
  {
    "title": "StructEval: Benchmarking LLMs' Capabilities to Generate Structural Outputs",
    "url": "http://arxiv.org/abs/2505.20139v1",
    "arxiv_id": "2505.20139v1",
    "authors": [
      "Jialin Yang",
      "Dongfu Jiang",
      "Lipeng He",
      "Sherman Siu",
      "Yuxuan Zhang",
      "Disen Liao",
      "Zhuofeng Li",
      "Huaye Zeng",
      "Yiming Jia",
      "Haozhe Wang",
      "Benjamin Schneider",
      "Chi Ruan",
      "Wentao Ma",
      "Zhiheng Lyu",
      "Yifei Wang",
      "Yi Lu",
      "Quy Duc Do",
      "Ziyan Jiang",
      "Ping Nie",
      "Wenhu Chen"
    ],
    "published": "2025-05-26T15:40:42+00:00",
    "summary": "As Large Language Models (LLMs) become integral to software development workflows, their ability to generate structured outputs has become critically important. We introduce StructEval, a comprehensive benchmark for evaluating LLMs' capabilities in producing both non-renderable (JSON, YAML, CSV) and renderable (HTML, React, SVG) structured formats. Unlike prior benchmarks, StructEval systematically evaluates structural fidelity across diverse formats through two paradigms: 1) generation tasks, producing structured output from natural language prompts, and 2) conversion tasks, translating between structured formats. Our benchmark encompasses 18 formats and 44 types of task, with novel metrics for format adherence and structural correctness. Results reveal significant performance gaps, even state-of-the-art models like o1-mini achieve only 75.58 average score, with open-source alternatives lagging approximately 10 points behind. We find generation tasks more challenging than conversion tasks, and producing correct visual content more difficult than generating text-only structures."
  },
  {
    "title": "Safety Through Reasoning: An Empirical Study of Reasoning Guardrail Models",
    "url": "http://arxiv.org/abs/2505.20087v1",
    "arxiv_id": "2505.20087v1",
    "authors": [
      "Makesh Narsimhan Sreedhar",
      "Traian Rebedea",
      "Christopher Parisien"
    ],
    "published": "2025-05-26T15:01:37+00:00",
    "summary": "Reasoning-based language models have demonstrated strong performance across various domains, with the most notable gains seen in mathematical and coding tasks. Recent research has shown that reasoning also offers significant benefits for LLM safety and guardrail applications. In this work, we conduct a comprehensive analysis of training reasoning-based guardrail models for content moderation, with an emphasis on generalization to custom safety policies at inference time. Our study focuses on two key dimensions: data efficiency and inference efficiency. On the data front, we find that reasoning-based models exhibit strong sample efficiency, achieving competitive performance with significantly fewer training examples than their non-reasoning counterparts. This unlocks the potential to repurpose the remaining data for mining high-value, difficult samples that further enhance model performance. On the inference side, we evaluate practical trade-offs by introducing reasoning budgets, examining the impact of reasoning length on latency and accuracy, and exploring dual-mode training to allow runtime control over reasoning behavior. Our findings will provide practical insights for researchers and developers to effectively and efficiently train and deploy reasoning-based guardrails models in real-world systems."
  },
  {
    "title": "SafeDPO: A Simple Approach to Direct Preference Optimization with Enhanced Safety",
    "url": "http://arxiv.org/abs/2505.20065v1",
    "arxiv_id": "2505.20065v1",
    "authors": [
      "Geon-Hyeong Kim",
      "Youngsoo Jang",
      "Yu Jin Kim",
      "Byoungjip Kim",
      "Honglak Lee",
      "Kyunghoon Bae",
      "Moontae Lee"
    ],
    "published": "2025-05-26T14:50:01+00:00",
    "summary": "As Large Language Models (LLMs) continue to advance and find applications across a growing number of fields, ensuring the safety of LLMs has become increasingly critical. To address safety concerns, recent studies have proposed integrating safety constraints into Reinforcement Learning from Human Feedback (RLHF). However, these approaches tend to be complex, as they encompass complicated procedures in RLHF along with additional steps required by the safety constraints. Inspired by Direct Preference Optimization (DPO), we introduce a new algorithm called SafeDPO, which is designed to directly optimize the safety alignment objective in a single stage of policy learning, without requiring relaxation. SafeDPO introduces only one additional hyperparameter to further enhance safety and requires only minor modifications to standard DPO. As a result, it eliminates the need to fit separate reward and cost models or to sample from the language model during fine-tuning, while still enhancing the safety of LLMs. Finally, we demonstrate that SafeDPO achieves competitive performance compared to state-of-the-art safety alignment algorithms, both in terms of aligning with human preferences and improving safety."
  },
  {
    "title": "Target Tracking via LiDAR-RADAR Sensor Fusion for Autonomous Racing",
    "url": "http://arxiv.org/abs/2505.20043v1",
    "arxiv_id": "2505.20043v1",
    "authors": [
      "Marcello Cellina",
      "Matteo Corno",
      "Sergio Matteo Savaresi"
    ],
    "published": "2025-05-26T14:28:13+00:00",
    "summary": "High Speed multi-vehicle Autonomous Racing will increase the safety and performance of road-going Autonomous Vehicles. Precise vehicle detection and dynamics estimation from a moving platform is a key requirement for planning and executing complex autonomous overtaking maneuvers. To address this requirement, we have developed a Latency-Aware EKF-based Multi Target Tracking algorithm fusing LiDAR and RADAR measurements. The algorithm explots the different sensor characteristics by explicitly integrating the Range Rate in the EKF Measurement Function, as well as a-priori knowledge of the racetrack during state prediction. It can handle Out-Of-Sequence Measurements via Reprocessing using a double State and Measurement Buffer, ensuring sensor delay compensation with no information loss. This algorithm has been implemented on Team PoliMOVE's autonomous racecar, and was proved experimentally by completing a number of fully autonomous overtaking maneuvers at speeds up to 275 km/h."
  },
  {
    "title": "Requirements Coverage-Guided Minimization for Natural Language Test Cases",
    "url": "http://arxiv.org/abs/2505.20004v1",
    "arxiv_id": "2505.20004v1",
    "authors": [
      "Rongqi Pan",
      "Feifei Niu",
      "Lionel C. Briand",
      "Hanyang Hu"
    ],
    "published": "2025-05-26T13:55:33+00:00",
    "summary": "As software systems evolve, test suites tend to grow in size and often contain redundant test cases. Such redundancy increases testing effort, time, and cost. Test suite minimization (TSM) aims to eliminate such redundancy while preserving key properties such as requirement coverage and fault detection capability. In this paper, we propose RTM (Requirement coverage-guided Test suite Minimization), a novel TSM approach designed for requirement-based testing (validation), which can effectively reduce test suite redundancy while ensuring full requirement coverage and a high fault detection rate (FDR) under a fixed minimization budget. Based on common practice in critical systems where functional safety is important, we assume test cases are specified in natural language and traced to requirements before being implemented. RTM preprocesses test cases using three different preprocessing methods, and then converts them into vector representations using seven text embedding techniques. Similarity values between vectors are computed utilizing three distance functions. A Genetic Algorithm, whose population is initialized by coverage-preserving initialization strategies, is then employed to identify an optimized subset containing diverse test cases matching the set budget.   We evaluate RTM on an industrial automotive system dataset comprising $736$ system test cases and $54$ requirements. Experimental results show that RTM consistently outperforms baseline techniques in terms of FDR across different minimization budgets while maintaining full requirement coverage. Furthermore, we investigate the impact of test suite redundancy levels on the effectiveness of TSM, providing new insights into optimizing requirement-based test suites under practical constraints."
  },
  {
    "title": "The Limits of Preference Data for Post-Training",
    "url": "http://arxiv.org/abs/2505.19964v1",
    "arxiv_id": "2505.19964v1",
    "authors": [
      "Eric Zhao",
      "Jessica Dai",
      "Pranjal Awasthi"
    ],
    "published": "2025-05-26T13:26:15+00:00",
    "summary": "Recent progress in strengthening the capabilities of large language models has stemmed from applying reinforcement learning to domains with automatically verifiable outcomes. A key question is whether we can similarly use RL to optimize for outcomes in domains where evaluating outcomes inherently requires human feedback; for example, in tasks like deep research and trip planning, outcome evaluation is qualitative and there are many possible degrees of success. One attractive and scalable modality for collecting human feedback is preference data: ordinal rankings (pairwise or $k$-wise) that indicate, for $k$ given outcomes, which one is preferred. In this work, we study a critical roadblock: preference data fundamentally and significantly limits outcome-based optimization. Even with idealized preference data (infinite, noiseless, and online), the use of ordinal feedback can prevent obtaining even approximately optimal solutions. We formalize this impossibility using voting theory, drawing an analogy between how a model chooses to answer a query with how voters choose a candidate to elect. This indicates that grounded human scoring and algorithmic innovations are necessary for extending the success of RL post-training to domains demanding human feedback. We also explore why these limitations have disproportionately impacted RLHF when it comes to eliciting reasoning behaviors (e.g., backtracking) versus situations where RLHF has been historically successful (e.g., instruction-tuning and safety training), finding that the limitations of preference data primarily suppress RLHF's ability to elicit robust strategies -- a class that encompasses most reasoning behaviors."
  },
  {
    "title": "Novel Loss-Enhanced Universal Adversarial Patches for Sustainable Speaker Privacy",
    "url": "http://arxiv.org/abs/2505.19951v1",
    "arxiv_id": "2505.19951v1",
    "authors": [
      "Elvir Karimov",
      "Alexander Varlamov",
      "Danil Ivanov",
      "Dmitrii Korzh",
      "Oleg Y. Rogov"
    ],
    "published": "2025-05-26T13:16:01+00:00",
    "summary": "Deep learning voice models are commonly used nowadays, but the safety processing of personal data, such as human identity and speech content, remains suspicious. To prevent malicious user identification, speaker anonymization methods were proposed. Current methods, particularly based on universal adversarial patch (UAP) applications, have drawbacks such as significant degradation of audio quality, decreased speech recognition quality, low transferability across different voice biometrics models, and performance dependence on the input audio length. To mitigate these drawbacks, in this work, we introduce and leverage the novel Exponential Total Variance (TV) loss function and provide experimental evidence that it positively affects UAP strength and imperceptibility. Moreover, we present a novel scalable UAP insertion procedure and demonstrate its uniformly high performance for various audio lengths."
  },
  {
    "title": "Uncertainty-Aware Safety-Critical Decision and Control for Autonomous Vehicles at Unsignalized Intersections",
    "url": "http://arxiv.org/abs/2505.19939v1",
    "arxiv_id": "2505.19939v1",
    "authors": [
      "Ran Yu",
      "Zhuoren Li",
      "Lu Xiong",
      "Wei Han",
      "Bo Leng"
    ],
    "published": "2025-05-26T13:06:02+00:00",
    "summary": "Reinforcement learning (RL) has demonstrated potential in autonomous driving (AD) decision tasks. However, applying RL to urban AD, particularly in intersection scenarios, still faces significant challenges. The lack of safety constraints makes RL vulnerable to risks. Additionally, cognitive limitations and environmental randomness can lead to unreliable decisions in safety-critical scenarios. Therefore, it is essential to quantify confidence in RL decisions to improve safety. This paper proposes an Uncertainty-aware Safety-Critical Decision and Control (USDC) framework, which generates a risk-averse policy by constructing a risk-aware ensemble distributional RL, while estimating uncertainty to quantify the policy's reliability. Subsequently, a high-order control barrier function (HOCBF) is employed as a safety filter to minimize intervention policy while dynamically enhancing constraints based on uncertainty. The ensemble critics evaluate both HOCBF and RL policies, embedding uncertainty to achieve dynamic switching between safe and flexible strategies, thereby balancing safety and efficiency. Simulation tests on unsignalized intersections in multiple tasks indicate that USDC can improve safety while maintaining traffic efficiency compared to baselines."
  },
  {
    "title": "Subtle Risks, Critical Failures: A Framework for Diagnosing Physical Safety of LLMs for Embodied Decision Making",
    "url": "http://arxiv.org/abs/2505.19933v1",
    "arxiv_id": "2505.19933v1",
    "authors": [
      "Yejin Son",
      "Minseo Kim",
      "Sungwoong Kim",
      "Seungju Han",
      "Jian Kim",
      "Dongju Jang",
      "Youngjae Yu",
      "Chanyoung Park"
    ],
    "published": "2025-05-26T13:01:14+00:00",
    "summary": "Large Language Models (LLMs) are increasingly used for decision making in embodied agents, yet existing safety evaluations often rely on coarse success rates and domain-specific setups, making it difficult to diagnose why and where these models fail. This obscures our understanding of embodied safety and limits the selective deployment of LLMs in high-risk physical environments. We introduce SAFEL, the framework for systematically evaluating the physical safety of LLMs in embodied decision making. SAFEL assesses two key competencies: (1) rejecting unsafe commands via the Command Refusal Test, and (2) generating safe and executable plans via the Plan Safety Test. Critically, the latter is decomposed into functional modules, goal interpretation, transition modeling, action sequencing, enabling fine-grained diagnosis of safety failures. To support this framework, we introduce EMBODYGUARD, a PDDL-grounded benchmark containing 942 LLM-generated scenarios covering both overtly malicious and contextually hazardous instructions. Evaluation across 13 state-of-the-art LLMs reveals that while models often reject clearly unsafe commands, they struggle to anticipate and mitigate subtle, situational risks. Our results highlight critical limitations in current LLMs and provide a foundation for more targeted, modular improvements in safe embodied reasoning."
  },
  {
    "title": "Evaluating AI cyber capabilities with crowdsourced elicitation",
    "url": "http://arxiv.org/abs/2505.19915v1",
    "arxiv_id": "2505.19915v1",
    "authors": [
      "Artem Petrov",
      "Dmitrii Volkov"
    ],
    "published": "2025-05-26T12:40:32+00:00",
    "summary": "As AI systems become increasingly capable, understanding their offensive cyber potential is critical for informed governance and responsible deployment. However, it's hard to accurately bound their capabilities, and some prior evaluations dramatically underestimated them. The art of extracting maximum task-specific performance from AIs is called \"AI elicitation\", and today's safety organizations typically conduct it in-house. In this paper, we explore crowdsourcing elicitation efforts as an alternative to in-house elicitation work.   We host open-access AI tracks at two Capture The Flag (CTF) competitions: AI vs. Humans (400 teams) and Cyber Apocalypse_ (4000 teams). The AI teams achieve outstanding performance at both events, ranking top-13% and top-21% respectively for a total of \\$7500 in bounties. This impressive performance suggests that open-market elicitation may offer an effective complement to in-house elicitation. We propose elicitation bounties as a practical mechanism for maintaining timely, cost-effective situational awareness of emerging AI capabilities.   Another advantage of open elicitations is the option to collect human performance data at scale. Applying METR's methodology, we found that AI agents can reliably solve cyber challenges requiring one hour or less of effort from a median human CTF participant."
  },
  {
    "title": "Enigmata: Scaling Logical Reasoning in Large Language Models with Synthetic Verifiable Puzzles",
    "url": "http://arxiv.org/abs/2505.19914v1",
    "arxiv_id": "2505.19914v1",
    "authors": [
      "Jiangjie Chen",
      "Qianyu He",
      "Siyu Yuan",
      "Aili Chen",
      "Zhicheng Cai",
      "Weinan Dai",
      "Hongli Yu",
      "Qiying Yu",
      "Xuefeng Li",
      "Jiaze Chen",
      "Hao Zhou",
      "Mingxuan Wang"
    ],
    "published": "2025-05-26T12:40:31+00:00",
    "summary": "Large Language Models (LLMs), such as OpenAI's o1 and DeepSeek's R1, excel at advanced reasoning tasks like math and coding via Reinforcement Learning with Verifiable Rewards (RLVR), but still struggle with puzzles solvable by humans without domain knowledge. We introduce Enigmata, the first comprehensive suite tailored for improving LLMs with puzzle reasoning skills. It includes 36 tasks across seven categories, each with 1) a generator that produces unlimited examples with controllable difficulty and 2) a rule-based verifier for automatic evaluation. This generator-verifier design supports scalable, multi-task RL training, fine-grained analysis, and seamless RLVR integration. We further propose Enigmata-Eval, a rigorous benchmark, and develop optimized multi-task RLVR strategies. Our trained model, Qwen2.5-32B-Enigmata, consistently surpasses o3-mini-high and o1 on the puzzle reasoning benchmarks like Enigmata-Eval, ARC-AGI (32.8%), and ARC-AGI 2 (0.6%). It also generalizes well to out-of-domain puzzle benchmarks and mathematical reasoning, with little multi-tasking trade-off. When trained on larger models like Seed1.5-Thinking (20B activated parameters and 200B total parameters), puzzle data from Enigmata further boosts SoTA performance on advanced math and STEM reasoning tasks such as AIME (2024-2025), BeyondAIME and GPQA (Diamond), showing nice generalization benefits of Enigmata. This work offers a unified, controllable framework for advancing logical reasoning in LLMs. Resources of this work can be found at https://seed-enigmata.github.io."
  },
  {
    "title": "Attention! You Vision Language Model Could Be Maliciously Manipulated",
    "url": "http://arxiv.org/abs/2505.19911v1",
    "arxiv_id": "2505.19911v1",
    "authors": [
      "Xiaosen Wang",
      "Shaokang Wang",
      "Zhijin Ge",
      "Yuyang Luo",
      "Shudong Zhang"
    ],
    "published": "2025-05-26T12:38:58+00:00",
    "summary": "Large Vision-Language Models (VLMs) have achieved remarkable success in understanding complex real-world scenarios and supporting data-driven decision-making processes. However, VLMs exhibit significant vulnerability against adversarial examples, either text or image, which can lead to various adversarial outcomes, e.g., jailbreaking, hijacking, and hallucination, etc. In this work, we empirically and theoretically demonstrate that VLMs are particularly susceptible to image-based adversarial examples, where imperceptible perturbations can precisely manipulate each output token. To this end, we propose a novel attack called Vision-language model Manipulation Attack (VMA), which integrates first-order and second-order momentum optimization techniques with a differentiable transformation mechanism to effectively optimize the adversarial perturbation. Notably, VMA can be a double-edged sword: it can be leveraged to implement various attacks, such as jailbreaking, hijacking, privacy breaches, Denial-of-Service, and the generation of sponge examples, etc, while simultaneously enabling the injection of watermarks for copyright protection. Extensive empirical evaluations substantiate the efficacy and generalizability of VMA across diverse scenarios and datasets."
  },
  {
    "title": "Causal Bayesian Networks for Data-driven Safety Analysis of Complex Systems",
    "url": "http://arxiv.org/abs/2505.19860v1",
    "arxiv_id": "2505.19860v1",
    "authors": [
      "Roman Gansch",
      "Lina Putze",
      "Tjark Koopmann",
      "Jan Reich",
      "Christian Neurohr"
    ],
    "published": "2025-05-26T11:45:53+00:00",
    "summary": "Ensuring safe operation of safety-critical complex systems interacting with their environment poses significant challenges, particularly when the system's world model relies on machine learning algorithms to process the perception input. A comprehensive safety argumentation requires knowledge of how faults or functional insufficiencies propagate through the system and interact with external factors, to manage their safety impact. While statistical analysis approaches can support the safety assessment, associative reasoning alone is neither sufficient for the safety argumentation nor for the identification and investigation of safety measures. A causal understanding of the system and its interaction with the environment is crucial for safeguarding safety-critical complex systems. It allows to transfer and generalize knowledge, such as insights gained from testing, and facilitates the identification of potential improvements. This work explores using causal Bayesian networks to model the system's causalities for safety analysis, and proposes measures to assess causal influences based on Pearl's framework of causal inference. We compare the approach of causal Bayesian networks to the well-established fault tree analysis, outlining advantages and limitations. In particular, we examine importance metrics typically employed in fault tree analysis as foundation to discuss suitable causal metrics. An evaluation is performed on the example of a perception system for automated driving. Overall, this work presents an approach for causal reasoning in safety analysis that enables the integration of data-driven and expert-based knowledge to account for uncertainties arising from complex systems operating in open environments."
  },
  {
    "title": "PCDCNet: A Surrogate Model for Air Quality Forecasting with Physical-Chemical Dynamics and Constraints",
    "url": "http://arxiv.org/abs/2505.19842v1",
    "arxiv_id": "2505.19842v1",
    "authors": [
      "Shuo Wang",
      "Yun Cheng",
      "Qingye Meng",
      "Olga Saukh",
      "Jiang Zhang",
      "Jingfang Fan",
      "Yuanting Zhang",
      "Xingyuan Yuan",
      "Lothar Thiele"
    ],
    "published": "2025-05-26T11:27:07+00:00",
    "summary": "Air quality forecasting (AQF) is critical for public health and environmental management, yet remains challenging due to the complex interplay of emissions, meteorology, and chemical transformations. Traditional numerical models, such as CMAQ and WRF-Chem, provide physically grounded simulations but are computationally expensive and rely on uncertain emission inventories. Deep learning models, while computationally efficient, often struggle with generalization due to their lack of physical constraints. To bridge this gap, we propose PCDCNet, a surrogate model that integrates numerical modeling principles with deep learning. PCDCNet explicitly incorporates emissions, meteorological influences, and domain-informed constraints to model pollutant formation, transport, and dissipation. By combining graph-based spatial transport modeling, recurrent structures for temporal accumulation, and representation enhancement for local interactions, PCDCNet achieves state-of-the-art (SOTA) performance in 72-hour station-level PM2.5 and O3 forecasting while significantly reducing computational costs. Furthermore, our model is deployed in an online platform, providing free, real-time air quality forecasts, demonstrating its scalability and societal impact. By aligning deep learning with physical consistency, PCDCNet offers a practical and interpretable solution for AQF, enabling informed decision-making for both personal and regulatory applications."
  },
  {
    "title": "InfoCons: Identifying Interpretable Critical Concepts in Point Clouds via Information Theory",
    "url": "http://arxiv.org/abs/2505.19820v1",
    "arxiv_id": "2505.19820v1",
    "authors": [
      "Feifei Li",
      "Mi Zhang",
      "Zhaoxiang Wang",
      "Min Yang"
    ],
    "published": "2025-05-26T10:58:54+00:00",
    "summary": "Interpretability of point cloud (PC) models becomes imperative given their deployment in safety-critical scenarios such as autonomous vehicles. We focus on attributing PC model outputs to interpretable critical concepts, defined as meaningful subsets of the input point cloud. To enable human-understandable diagnostics of model failures, an ideal critical subset should be *faithful* (preserving points that causally influence predictions) and *conceptually coherent* (forming semantically meaningful structures that align with human perception). We propose InfoCons, an explanation framework that applies information-theoretic principles to decompose the point cloud into 3D concepts, enabling the examination of their causal effect on model predictions with learnable priors. We evaluate InfoCons on synthetic datasets for classification, comparing it qualitatively and quantitatively with four baselines. We further demonstrate its scalability and flexibility on two real-world datasets and in two applications that utilize critical scores of PC."
  },
  {
    "title": "What Really Matters in Many-Shot Attacks? An Empirical Study of Long-Context Vulnerabilities in LLMs",
    "url": "http://arxiv.org/abs/2505.19773v1",
    "arxiv_id": "2505.19773v1",
    "authors": [
      "Sangyeop Kim",
      "Yohan Lee",
      "Yongwoo Song",
      "Kimin Lee"
    ],
    "published": "2025-05-26T09:57:25+00:00",
    "summary": "We investigate long-context vulnerabilities in Large Language Models (LLMs) through Many-Shot Jailbreaking (MSJ). Our experiments utilize context length of up to 128K tokens. Through comprehensive analysis with various many-shot attack settings with different instruction styles, shot density, topic, and format, we reveal that context length is the primary factor determining attack effectiveness. Critically, we find that successful attacks do not require carefully crafted harmful content. Even repetitive shots or random dummy text can circumvent model safety measures, suggesting fundamental limitations in long-context processing capabilities of LLMs. The safety behavior of well-aligned models becomes increasingly inconsistent with longer contexts. These findings highlight significant safety gaps in context expansion capabilities of LLMs, emphasizing the need for new safety mechanisms."
  },
  {
    "title": "SGM: A Framework for Building Specification-Guided Moderation Filters",
    "url": "http://arxiv.org/abs/2505.19766v1",
    "arxiv_id": "2505.19766v1",
    "authors": [
      "Masoomali Fatehkia",
      "Enes Altinisik",
      "Husrev Taha Sencar"
    ],
    "published": "2025-05-26T09:49:43+00:00",
    "summary": "Aligning large language models (LLMs) with deployment-specific requirements is critical but inherently imperfect. Despite extensive training, models remain susceptible to misalignment and adversarial inputs such as jailbreaks. Content moderation filters are commonly used as external safeguards, though they typically focus narrowly on safety. We introduce SGM (Specification-Guided Moderation), a flexible framework for training moderation filters grounded in user-defined specifications that go beyond standard safety concerns. SGM automates training data generation without relying on human-written examples, enabling scalable support for diverse, application-specific alignment goals. SGM-trained filters perform on par with state-of-the-art safety filters built on curated datasets, while supporting fine-grained and user-defined alignment control."
  },
  {
    "title": "Beyond Safe Answers: A Benchmark for Evaluating True Risk Awareness in Large Reasoning Models",
    "url": "http://arxiv.org/abs/2505.19690v1",
    "arxiv_id": "2505.19690v1",
    "authors": [
      "Baihui Zheng",
      "Boren Zheng",
      "Kerui Cao",
      "Yingshui Tan",
      "Zhendong Liu",
      "Weixun Wang",
      "Jiaheng Liu",
      "Jian Yang",
      "Wenbo Su",
      "Xiaoyong Zhu",
      "Bo Zheng",
      "Kaifu Zhang"
    ],
    "published": "2025-05-26T08:49:19+00:00",
    "summary": "Despite the remarkable proficiency of \\textit{Large Reasoning Models} (LRMs) in handling complex reasoning tasks, their reliability in safety-critical scenarios remains uncertain. Existing evaluations primarily assess response-level safety, neglecting a critical issue we identify as \\textbf{\\textit{Superficial Safety Alignment} (SSA)} -- a phenomenon where models produce superficially safe outputs while internal reasoning processes fail to genuinely detect and mitigate underlying risks, resulting in inconsistent safety behaviors across multiple sampling attempts. To systematically investigate SSA, we introduce \\textbf{Beyond Safe Answers (BSA)} bench, a novel benchmark comprising 2,000 challenging instances organized into three distinct SSA scenario types and spanning nine risk categories, each meticulously annotated with risk rationales. Evaluations of 19 state-of-the-art LRMs demonstrate the difficulty of this benchmark, with top-performing models achieving only 38.0\\% accuracy in correctly identifying risk rationales. We further explore the efficacy of safety rules, specialized fine-tuning on safety reasoning data, and diverse decoding strategies in mitigating SSA. Our work provides a comprehensive assessment tool for evaluating and improving safety reasoning fidelity in LRMs, advancing the development of genuinely risk-aware and reliably safe AI systems."
  },
  {
    "title": "VisCRA: A Visual Chain Reasoning Attack for Jailbreaking Multimodal Large Language Models",
    "url": "http://arxiv.org/abs/2505.19684v1",
    "arxiv_id": "2505.19684v1",
    "authors": [
      "Bingrui Sima",
      "Linhua Cong",
      "Wenxuan Wang",
      "Kun He"
    ],
    "published": "2025-05-26T08:45:06+00:00",
    "summary": "The emergence of Multimodal Large Language Models (MLRMs) has enabled sophisticated visual reasoning capabilities by integrating reinforcement learning and Chain-of-Thought (CoT) supervision. However, while these enhanced reasoning capabilities improve performance, they also introduce new and underexplored safety risks. In this work, we systematically investigate the security implications of advanced visual reasoning in MLRMs. Our analysis reveals a fundamental trade-off: as visual reasoning improves, models become more vulnerable to jailbreak attacks. Motivated by this critical finding, we introduce VisCRA (Visual Chain Reasoning Attack), a novel jailbreak framework that exploits the visual reasoning chains to bypass safety mechanisms. VisCRA combines targeted visual attention masking with a two-stage reasoning induction strategy to precisely control harmful outputs. Extensive experiments demonstrate VisCRA's significant effectiveness, achieving high attack success rates on leading closed-source MLRMs: 76.48% on Gemini 2.0 Flash Thinking, 68.56% on QvQ-Max, and 56.60% on GPT-4o. Our findings highlight a critical insight: the very capability that empowers MLRMs -- their visual reasoning -- can also serve as an attack vector, posing significant security risks."
  },
  {
    "title": "Reshaping Representation Space to Balance the Safety and Over-rejection in Large Audio Language Models",
    "url": "http://arxiv.org/abs/2505.19670v1",
    "arxiv_id": "2505.19670v1",
    "authors": [
      "Hao Yang",
      "Lizhen Qu",
      "Ehsan Shareghi",
      "Gholamreza Haffari"
    ],
    "published": "2025-05-26T08:25:25+00:00",
    "summary": "Large Audio Language Models (LALMs) have extended the capabilities of Large Language Models (LLMs) by enabling audio-based human interactions. However, recent research has revealed that LALMs remain vulnerable to harmful queries due to insufficient safety-alignment. Despite advances in defence measures for text and vision LLMs, effective safety-alignment strategies and audio-safety dataset specifically targeting LALMs are notably absent. Meanwhile defence measures based on Supervised Fine-tuning (SFT) struggle to address safety improvement while avoiding over-rejection issues, significantly compromising helpfulness. In this work, we propose an unsupervised safety-fine-tuning strategy as remedy that reshapes model's representation space to enhance existing LALMs safety-alignment while balancing the risk of over-rejection. Our experiments, conducted across three generations of Qwen LALMs, demonstrate that our approach significantly improves LALMs safety under three modality input conditions (audio-text, text-only, and audio-only) while increasing over-rejection rate by only 0.88% on average. Warning: this paper contains harmful examples."
  },
  {
    "title": "FieldWorkArena: Agentic AI Benchmark for Real Field Work Tasks",
    "url": "http://arxiv.org/abs/2505.19662v1",
    "arxiv_id": "2505.19662v1",
    "authors": [
      "Atsunori Moteki",
      "Shoichi Masui",
      "Fan Yang",
      "Yueqi Song",
      "Yonatan Bisk",
      "Graham Neubig",
      "Ikuo Kusajima",
      "Yasuto Watanabe",
      "Hiroyuki Ishida",
      "Jun Takahashi",
      "Shan Jiang"
    ],
    "published": "2025-05-26T08:21:46+00:00",
    "summary": "This paper proposes FieldWorkArena, a benchmark for agentic AI targeting real-world field work. With the recent increase in demand for agentic AI, they are required to monitor and report safety and health incidents, as well as manufacturing-related incidents, that may occur in real-world work environments. Existing agentic AI benchmarks have been limited to evaluating web tasks and are insufficient for evaluating agents in real-world work environments, where complexity increases significantly. In this paper, we define a new action space that agentic AI should possess for real world work environment benchmarks and improve the evaluation function from previous methods to assess the performance of agentic AI in diverse real-world tasks. The dataset consists of videos captured on-site and documents actually used in factories and warehouses, and tasks were created based on interviews with on-site workers and managers. Evaluation results confirmed that performance evaluation considering the characteristics of Multimodal LLM (MLLM) such as GPT-4o is feasible. Additionally, the effectiveness and limitations of the proposed new evaluation method were identified. The complete dataset (HuggingFace) and evaluation program (GitHub) can be downloaded from the following website: https://en-documents.research.global.fujitsu.com/fieldworkarena/."
  },
  {
    "title": "Large Language Models in Code Co-generation for Safe Autonomous Vehicles",
    "url": "http://arxiv.org/abs/2505.19658v1",
    "arxiv_id": "2505.19658v1",
    "authors": [
      "Ali Nouri",
      "Beatriz Cabrero-Daniel",
      "Zhennan Fei",
      "Krishna Ronanki",
      "H\u00e5kan Sivencrona",
      "Christian Berger"
    ],
    "published": "2025-05-26T08:18:30+00:00",
    "summary": "Software engineers in various industrial domains are already using Large Language Models (LLMs) to accelerate the process of implementing parts of software systems. When considering its potential use for ADAS or AD systems in the automotive context, there is a need to systematically assess this new setup: LLMs entail a well-documented set of risks for safety-related systems' development due to their stochastic nature. To reduce the effort for code reviewers to evaluate LLM-generated code, we propose an evaluation pipeline to conduct sanity-checks on the generated code. We compare the performance of six state-of-the-art LLMs (CodeLlama, CodeGemma, DeepSeek-r1, DeepSeek-Coders, Mistral, and GPT-4) on four safety-related programming tasks. Additionally, we qualitatively analyse the most frequent faults generated by these LLMs, creating a failure-mode catalogue to support human reviewers. Finally, the limitations and capabilities of LLMs in code generation, and the use of the proposed pipeline in the existing process, are discussed."
  },
  {
    "title": "SynLogic: Synthesizing Verifiable Reasoning Data at Scale for Learning Logical Reasoning and Beyond",
    "url": "http://arxiv.org/abs/2505.19641v1",
    "arxiv_id": "2505.19641v1",
    "authors": [
      "Junteng Liu",
      "Yuanxiang Fan",
      "Zhuo Jiang",
      "Han Ding",
      "Yongyi Hu",
      "Chi Zhang",
      "Yiqi Shi",
      "Shitong Weng",
      "Aili Chen",
      "Shiqi Chen",
      "Yunan Huang",
      "Mozhi Zhang",
      "Pengyu Zhao",
      "Junjie Yan",
      "Junxian He"
    ],
    "published": "2025-05-26T07:59:36+00:00",
    "summary": "Recent advances such as OpenAI-o1 and DeepSeek R1 have demonstrated the potential of Reinforcement Learning (RL) to enhance reasoning abilities in Large Language Models (LLMs). While open-source replication efforts have primarily focused on mathematical and coding domains, methods and resources for developing general reasoning capabilities remain underexplored. This gap is partly due to the challenge of collecting diverse and verifiable reasoning data suitable for RL. We hypothesize that logical reasoning is critical for developing general reasoning capabilities, as logic forms a fundamental building block of reasoning. In this work, we present SynLogic, a data synthesis framework and dataset that generates diverse logical reasoning data at scale, encompassing 35 diverse logical reasoning tasks. The SynLogic approach enables controlled synthesis of data with adjustable difficulty and quantity. Importantly, all examples can be verified by simple rules, making them ideally suited for RL with verifiable rewards. In our experiments, we validate the effectiveness of RL training on the SynLogic dataset based on 7B and 32B models. SynLogic leads to state-of-the-art logical reasoning performance among open-source datasets, surpassing DeepSeek-R1-Distill-Qwen-32B by 6 points on BBEH. Furthermore, mixing SynLogic data with mathematical and coding tasks improves the training efficiency of these domains and significantly enhances reasoning generalization. Notably, our mixed training model outperforms DeepSeek-R1-Zero-Qwen-32B across multiple benchmarks. These findings position SynLogic as a valuable resource for advancing the broader reasoning capabilities of LLMs. We open-source both the data synthesis pipeline and the SynLogic dataset at https://github.com/MiniMax-AI/SynLogic."
  },
  {
    "title": "Software Engineering for Self-Adaptive Robotics: A Research Agenda",
    "url": "http://arxiv.org/abs/2505.19629v1",
    "arxiv_id": "2505.19629v1",
    "authors": [
      "Shaukat Ali",
      "Ana Cavalcanti",
      "Cl\u00e1udio \u00c2ngelo Gon\u00e7alves Gomes",
      "Peter Gorm Larsen",
      "Hassan Sartaj",
      "Anastasios Tefas",
      "Jim Woodcock",
      "Houxiang Zhang"
    ],
    "published": "2025-05-26T07:47:50+00:00",
    "summary": "Self-adaptive robotic systems are designed to operate autonomously in dynamic and uncertain environments, requiring robust mechanisms to monitor, analyse, and adapt their behaviour in real-time. Unlike traditional robotic software, which follows predefined logic, self-adaptive robots leverage artificial intelligence, machine learning, and model-driven engineering to continuously adjust to changing operational conditions while ensuring reliability, safety, and performance. This paper presents a research agenda for software engineering in self-adaptive robotics, addressing critical challenges across two key dimensions: (1) the development phase, including requirements engineering, software design, co-simulation, and testing methodologies tailored to adaptive robotic systems, and (2) key enabling technologies, such as digital twins, model-driven engineering, and AI-driven adaptation, which facilitate runtime monitoring, fault detection, and automated decision-making. We discuss open research challenges, including verifying adaptive behaviours under uncertainty, balancing trade-offs between adaptability, performance, and safety, and integrating self-adaptation frameworks like MAPE-K. By providing a structured roadmap, this work aims to advance the software engineering foundations for self-adaptive robotic systems, ensuring they remain trustworthy, efficient, and capable of handling real-world complexities."
  },
  {
    "title": "JailBound: Jailbreaking Internal Safety Boundaries of Vision-Language Models",
    "url": "http://arxiv.org/abs/2505.19610v1",
    "arxiv_id": "2505.19610v1",
    "authors": [
      "Jiaxin Song",
      "Yixu Wang",
      "Jie Li",
      "Rui Yu",
      "Yan Teng",
      "Xingjun Ma",
      "Yingchun Wang"
    ],
    "published": "2025-05-26T07:23:00+00:00",
    "summary": "Vision-Language Models (VLMs) exhibit impressive performance, yet the integration of powerful vision encoders has significantly broadened their attack surface, rendering them increasingly susceptible to jailbreak attacks. However, lacking well-defined attack objectives, existing jailbreak methods often struggle with gradient-based strategies prone to local optima and lacking precise directional guidance, and typically decouple visual and textual modalities, thereby limiting their effectiveness by neglecting crucial cross-modal interactions. Inspired by the Eliciting Latent Knowledge (ELK) framework, we posit that VLMs encode safety-relevant information within their internal fusion-layer representations, revealing an implicit safety decision boundary in the latent space. This motivates exploiting boundary to steer model behavior. Accordingly, we propose JailBound, a novel latent space jailbreak framework comprising two stages: (1) Safety Boundary Probing, which addresses the guidance issue by approximating decision boundary within fusion layer's latent space, thereby identifying optimal perturbation directions towards the target region; and (2) Safety Boundary Crossing, which overcomes the limitations of decoupled approaches by jointly optimizing adversarial perturbations across both image and text inputs. This latter stage employs an innovative mechanism to steer the model's internal state towards policy-violating outputs while maintaining cross-modal semantic consistency. Extensive experiments on six diverse VLMs demonstrate JailBound's efficacy, achieves 94.32% white-box and 67.28% black-box attack success averagely, which are 6.17% and 21.13% higher than SOTA methods, respectively. Our findings expose a overlooked safety risk in VLMs and highlight the urgent need for more robust defenses. Warning: This paper contains potentially sensitive, harmful and offensive content."
  },
  {
    "title": "Evaluating Robustness of Large Audio Language Models to Audio Injection: An Empirical Study",
    "url": "http://arxiv.org/abs/2505.19598v1",
    "arxiv_id": "2505.19598v1",
    "authors": [
      "Guanyu Hou",
      "Jiaming He",
      "Yinhang Zhou",
      "Ji Guo",
      "Yitong Qiao",
      "Rui Zhang",
      "Wenbo Jiang"
    ],
    "published": "2025-05-26T07:08:38+00:00",
    "summary": "Large Audio-Language Models (LALMs) are increasingly deployed in real-world applications, yet their robustness against malicious audio injection attacks remains underexplored. This study systematically evaluates five leading LALMs across four attack scenarios: Audio Interference Attack, Instruction Following Attack, Context Injection Attack, and Judgment Hijacking Attack. Using metrics like Defense Success Rate, Context Robustness Score, and Judgment Robustness Index, their vulnerabilities and resilience were quantitatively assessed. Experimental results reveal significant performance disparities among models; no single model consistently outperforms others across all attack types. The position of malicious content critically influences attack effectiveness, particularly when placed at the beginning of sequences. A negative correlation between instruction-following capability and robustness suggests models adhering strictly to instructions may be more susceptible, contrasting with greater resistance by safety-aligned models. Additionally, system prompts show mixed effectiveness, indicating the need for tailored strategies. This work introduces a benchmark framework and highlights the importance of integrating robustness into training pipelines. Findings emphasize developing multi-modal defenses and architectural designs that decouple capability from susceptibility for secure LALMs deployment."
  },
  {
    "title": "Situationally-Aware Dynamics Learning",
    "url": "http://arxiv.org/abs/2505.19574v1",
    "arxiv_id": "2505.19574v1",
    "authors": [
      "Alejandro Murillo-Gonzalez",
      "Lantao Liu"
    ],
    "published": "2025-05-26T06:40:11+00:00",
    "summary": "Autonomous robots operating in complex, unstructured environments face significant challenges due to latent, unobserved factors that obscure their understanding of both their internal state and the external world. Addressing this challenge would enable robots to develop a more profound grasp of their operational context. To tackle this, we propose a novel framework for online learning of hidden state representations, with which the robots can adapt in real-time to uncertain and dynamic conditions that would otherwise be ambiguous and result in suboptimal or erroneous behaviors. Our approach is formalized as a Generalized Hidden Parameter Markov Decision Process, which explicitly models the influence of unobserved parameters on both transition dynamics and reward structures. Our core innovation lies in learning online the joint distribution of state transitions, which serves as an expressive representation of latent ego- and environmental-factors. This probabilistic approach supports the identification and adaptation to different operational situations, improving robustness and safety. Through a multivariate extension of Bayesian Online Changepoint Detection, our method segments changes in the underlying data generating process governing the robot's dynamics. The robot's transition model is then informed with a symbolic representation of the current situation derived from the joint distribution of latest state transitions, enabling adaptive and context-aware decision-making. To showcase the real-world effectiveness, we validate our approach in the challenging task of unstructured terrain navigation, where unmodeled and unmeasured terrain characteristics can significantly impact the robot's motion. Extensive experiments in both simulation and real world reveal significant improvements in data efficiency, policy performance, and the emergence of safer, adaptive navigation strategies."
  },
  {
    "title": "MSD-LLM: Predicting Ship Detention in Port State Control Inspections with Large Language Model",
    "url": "http://arxiv.org/abs/2505.19568v1",
    "arxiv_id": "2505.19568v1",
    "authors": [
      "Jiongchao Jin",
      "Xiuju Fu",
      "Xiaowei Gao",
      "Tao Cheng",
      "Ran Yan"
    ],
    "published": "2025-05-26T06:32:02+00:00",
    "summary": "Maritime transportation is the backbone of global trade, making ship inspection essential for ensuring maritime safety and environmental protection. Port State Control (PSC), conducted by national ports, enforces compliance with safety regulations, with ship detention being the most severe consequence, impacting both ship schedules and company reputations. Traditional machine learning methods for ship detention prediction are limited by the capacity of representation learning and thus suffer from low accuracy. Meanwhile, autoencoder-based deep learning approaches face challenges due to the severe data imbalance in learning historical PSC detention records. To address these limitations, we propose Maritime Ship Detention with Large Language Models (MSD-LLM), integrating a dual robust subspace recovery (DSR) layer-based autoencoder with a progressive learning pipeline to handle imbalanced data and extract meaningful PSC representations. Then, a large language model groups and ranks features to identify likely detention cases, enabling dynamic thresholding for flexible detention predictions. Extensive evaluations on 31,707 PSC inspection records from the Asia-Pacific region show that MSD-LLM outperforms state-of-the-art methods more than 12\\% on Area Under the Curve (AUC) for Singapore ports. Additionally, it demonstrates robustness to real-world challenges, making it adaptable to diverse maritime risk assessment scenarios."
  },
  {
    "title": "VLMLight: Traffic Signal Control via Vision-Language Meta-Control and Dual-Branch Reasoning",
    "url": "http://arxiv.org/abs/2505.19486v1",
    "arxiv_id": "2505.19486v1",
    "authors": [
      "Maonan Wang",
      "Yirong Chen",
      "Aoyu Pang",
      "Yuxin Cai",
      "Chung Shue Chen",
      "Yuheng Kan",
      "Man-On Pun"
    ],
    "published": "2025-05-26T04:12:57+00:00",
    "summary": "Traffic signal control (TSC) is a core challenge in urban mobility, where real-time decisions must balance efficiency and safety. Existing methods - ranging from rule-based heuristics to reinforcement learning (RL) - often struggle to generalize to complex, dynamic, and safety-critical scenarios. We introduce VLMLight, a novel TSC framework that integrates vision-language meta-control with dual-branch reasoning. At the core of VLMLight is the first image-based traffic simulator that enables multi-view visual perception at intersections, allowing policies to reason over rich cues such as vehicle type, motion, and spatial density. A large language model (LLM) serves as a safety-prioritized meta-controller, selecting between a fast RL policy for routine traffic and a structured reasoning branch for critical cases. In the latter, multiple LLM agents collaborate to assess traffic phases, prioritize emergency vehicles, and verify rule compliance. Experiments show that VLMLight reduces waiting times for emergency vehicles by up to 65% over RL-only systems, while preserving real-time performance in standard conditions with less than 1% degradation. VLMLight offers a scalable, interpretable, and safety-aware solution for next-generation traffic signal control."
  },
  {
    "title": "BizFinBench: A Business-Driven Real-World Financial Benchmark for Evaluating LLMs",
    "url": "http://arxiv.org/abs/2505.19457v1",
    "arxiv_id": "2505.19457v1",
    "authors": [
      "Guilong Lu",
      "Xuntao Guo",
      "Rongjunchen Zhang",
      "Wenqiao Zhu",
      "Ji Liu"
    ],
    "published": "2025-05-26T03:23:02+00:00",
    "summary": "Large language models excel in general tasks, yet assessing their reliability in logic-heavy, precision-critical domains like finance, law, and healthcare remains challenging. To address this, we introduce BizFinBench, the first benchmark specifically designed to evaluate LLMs in real-world financial applications. BizFinBench consists of 6,781 well-annotated queries in Chinese, spanning five dimensions: numerical calculation, reasoning, information extraction, prediction recognition, and knowledge-based question answering, grouped into nine fine-grained categories. The benchmark includes both objective and subjective metrics. We also introduce IteraJudge, a novel LLM evaluation method that reduces bias when LLMs serve as evaluators in objective metrics. We benchmark 25 models, including both proprietary and open-source systems. Extensive experiments show that no model dominates across all tasks. Our evaluation reveals distinct capability patterns: (1) In Numerical Calculation, Claude-3.5-Sonnet (63.18) and DeepSeek-R1 (64.04) lead, while smaller models like Qwen2.5-VL-3B (15.92) lag significantly; (2) In Reasoning, proprietary models dominate (ChatGPT-o3: 83.58, Gemini-2.0-Flash: 81.15), with open-source models trailing by up to 19.49 points; (3) In Information Extraction, the performance spread is the largest, with DeepSeek-R1 scoring 71.46, while Qwen3-1.7B scores 11.23; (4) In Prediction Recognition, performance variance is minimal, with top models scoring between 39.16 and 50.00. We find that while current LLMs handle routine finance queries competently, they struggle with complex scenarios requiring cross-concept reasoning. BizFinBench offers a rigorous, business-aligned benchmark for future research. The code and dataset are available at https://github.com/HiThink-Research/BizFinBench."
  },
  {
    "title": "Vibe Coding vs. Agentic Coding: Fundamentals and Practical Implications of Agentic AI",
    "url": "http://arxiv.org/abs/2505.19443v1",
    "arxiv_id": "2505.19443v1",
    "authors": [
      "Ranjan Sapkota",
      "Konstantinos I. Roumeliotis",
      "Manoj Karkee"
    ],
    "published": "2025-05-26T03:00:21+00:00",
    "summary": "This review presents a comprehensive analysis of two emerging paradigms in AI-assisted software development: vibe coding and agentic coding. While both leverage large language models (LLMs), they differ fundamentally in autonomy, architectural design, and the role of the developer. Vibe coding emphasizes intuitive, human-in-the-loop interaction through prompt-based, conversational workflows that support ideation, experimentation, and creative exploration. In contrast, agentic coding enables autonomous software development through goal-driven agents capable of planning, executing, testing, and iterating tasks with minimal human intervention. We propose a detailed taxonomy spanning conceptual foundations, execution models, feedback loops, safety mechanisms, debugging strategies, and real-world tool ecosystems. Through comparative workflow analysis and 20 detailed use cases, we illustrate how vibe systems thrive in early-stage prototyping and education, while agentic systems excel in enterprise-grade automation, codebase refactoring, and CI/CD integration. We further examine emerging trends in hybrid architectures, where natural language interfaces are coupled with autonomous execution pipelines. Finally, we articulate a future roadmap for agentic AI, outlining the infrastructure needed for trustworthy, explainable, and collaborative systems. Our findings suggest that successful AI software engineering will rely not on choosing one paradigm, but on harmonizing their strengths within a unified, human-centered development lifecycle."
  },
  {
    "title": "Can Compressed LLMs Truly Act? An Empirical Evaluation of Agentic Capabilities in LLM Compression",
    "url": "http://arxiv.org/abs/2505.19433v1",
    "arxiv_id": "2505.19433v1",
    "authors": [
      "Peijie Dong",
      "Zhenheng Tang",
      "Xiang Liu",
      "Lujun Li",
      "Xiaowen Chu",
      "Bo Li"
    ],
    "published": "2025-05-26T02:49:07+00:00",
    "summary": "Post-training compression reduces the computational and memory costs of large language models (LLMs), enabling resource-efficient deployment. However, existing compression benchmarks only focus on language modeling (e.g., perplexity) and natural language understanding tasks (e.g., GLUE accuracy), ignoring the agentic capabilities - workflow, tool use/function call, long-context understanding and real-world application. We introduce the Agent Compression Benchmark (ACBench), the first comprehensive benchmark for evaluating how compression impacts LLMs' agentic abilities. ACBench spans (1) 12 tasks across 4 capabilities (e.g., WorfBench for workflow generation, Needle-in-Haystack for long-context retrieval), (2) quantization (GPTQ, AWQ) and pruning (Wanda, SparseGPT), and (3) 15 models, including small (Gemma-2B), standard (Qwen2.5 7B-32B), and distilled reasoning LLMs (DeepSeek-R1-Distill). Our experiments reveal compression tradeoffs: 4-bit quantization preserves workflow generation and tool use (1%-3% drop) but degrades real-world application accuracy by 10%-15%. We introduce ERank, Top-k Ranking Correlation and Energy to systematize analysis. ACBench provides actionable insights for optimizing LLM compression in agentic scenarios. The code can be found in https://github.com/pprp/ACBench."
  },
  {
    "title": "Toward Physics-Informed Machine Learning for Data Center Operations: A Tropical Case Study",
    "url": "http://arxiv.org/abs/2505.19414v1",
    "arxiv_id": "2505.19414v1",
    "authors": [
      "Ruihang Wang",
      "Zhiwei Cao",
      "Qingang Zhang",
      "Rui Tan",
      "Yonggang Wen",
      "Tommy Leung",
      "Stuart Kennedy",
      "Justin Teoh"
    ],
    "published": "2025-05-26T02:06:45+00:00",
    "summary": "Data centers are the backbone of computing capacity. Operating data centers in the tropical regions faces unique challenges due to consistently high ambient temperature and elevated relative humidity throughout the year. These conditions result in increased cooling costs to maintain the reliability of the computing systems. While existing machine learning-based approaches have demonstrated potential to elevate operations to a more proactive and intelligent level, their deployment remains dubious due to concerns about model extrapolation capabilities and associated system safety issues. To address these concerns, this article proposes incorporating the physical characteristics of data centers into traditional data-driven machine learning solutions. We begin by introducing the data center system, including the relevant multiphysics processes and the data-physics availability. Next, we outline the associated modeling and optimization problems and propose an integrated, physics-informed machine learning system to address them. Using the proposed system, we present relevant applications across varying levels of operational intelligence. A case study on an industry-grade tropical data center is provided to demonstrate the effectiveness of our approach. Finally, we discuss key challenges and highlight potential future directions."
  },
  {
    "title": "Erasing Concepts, Steering Generations: A Comprehensive Survey of Concept Suppression",
    "url": "http://arxiv.org/abs/2505.19398v1",
    "arxiv_id": "2505.19398v1",
    "authors": [
      "Yiwei Xie",
      "Ping Liu",
      "Zheng Zhang"
    ],
    "published": "2025-05-26T01:24:34+00:00",
    "summary": "Text-to-Image (T2I) models have demonstrated impressive capabilities in generating high-quality and diverse visual content from natural language prompts. However, uncontrolled reproduction of sensitive, copyrighted, or harmful imagery poses serious ethical, legal, and safety challenges. To address these concerns, the concept erasure paradigm has emerged as a promising direction, enabling the selective removal of specific semantic concepts from generative models while preserving their overall utility. This survey provides a comprehensive overview and in-depth synthesis of concept erasure techniques in T2I diffusion models. We systematically categorize existing approaches along three key dimensions: intervention level, which identifies specific model components targeted for concept removal; optimization structure, referring to the algorithmic strategies employed to achieve suppression; and semantic scope, concerning the complexity and nature of the concepts addressed. This multi-dimensional taxonomy enables clear, structured comparisons across diverse methodologies, highlighting fundamental trade-offs between erasure specificity, generalization, and computational complexity. We further discuss current evaluation benchmarks, standardized metrics, and practical datasets, emphasizing gaps that limit comprehensive assessment, particularly regarding robustness and practical effectiveness. Finally, we outline major challenges and promising future directions, including disentanglement of concept representations, adaptive and incremental erasure strategies, adversarial robustness, and new generative architectures. This survey aims to guide researchers toward safer, more ethically aligned generative models, providing foundational knowledge and actionable recommendations to advance responsible development in generative AI."
  },
  {
    "title": "VADER: A Human-Evaluated Benchmark for Vulnerability Assessment, Detection, Explanation, and Remediation",
    "url": "http://arxiv.org/abs/2505.19395v1",
    "arxiv_id": "2505.19395v1",
    "authors": [
      "Ethan TS. Liu",
      "Austin Wang",
      "Spencer Mateega",
      "Carlos Georgescu",
      "Danny Tang"
    ],
    "published": "2025-05-26T01:20:44+00:00",
    "summary": "Ensuring that large language models (LLMs) can effectively assess, detect, explain, and remediate software vulnerabilities is critical for building robust and secure software systems. We introduce VADER, a human-evaluated benchmark designed explicitly to assess LLM performance across four key vulnerability-handling dimensions: assessment, detection, explanation, and remediation. VADER comprises 174 real-world software vulnerabilities, each carefully curated from GitHub repositories and annotated by security experts. For each vulnerability case, models are tasked with identifying the flaw, classifying it using Common Weakness Enumeration (CWE), explaining its underlying cause, proposing a patch, and formulating a test plan. Using a one-shot prompting strategy, we benchmark six state-of-the-art LLMs (Claude 3.7 Sonnet, Gemini 2.5 Pro, GPT-4.1, GPT-4.5, Grok 3 Beta, and o3) on VADER, and human security experts evaluated each response according to a rigorous scoring rubric emphasizing remediation (quality of the code fix, 50%), explanation (20%), and classification and test plan (30%) according to a standardized rubric. Our results show that current state-of-the-art LLMs achieve only moderate success on VADER - OpenAI's o3 attained 54.7% accuracy overall, with others in the 49-54% range, indicating ample room for improvement. Notably, remediation quality is strongly correlated (Pearson r > 0.97) with accurate classification and test plans, suggesting that models that effectively categorize vulnerabilities also tend to fix them well. VADER's comprehensive dataset, detailed evaluation rubrics, scoring tools, and visualized results with confidence intervals are publicly released, providing the community with an interpretable, reproducible benchmark to advance vulnerability-aware LLMs. All code and data are available at: https://github.com/AfterQuery/vader"
  },
  {
    "title": "Making Teams and Influencing Agents: Efficiently Coordinating Decision Trees for Interpretable Multi-Agent Reinforcement Learning",
    "url": "http://arxiv.org/abs/2505.19316v1",
    "arxiv_id": "2505.19316v1",
    "authors": [
      "Rex Chen",
      "Stephanie Milani",
      "Zhicheng Zhang",
      "Norman Sadeh",
      "Fei Fang"
    ],
    "published": "2025-05-25T21:05:48+00:00",
    "summary": "Poor interpretability hinders the practical applicability of multi-agent reinforcement learning (MARL) policies. Deploying interpretable surrogates of uninterpretable policies enhances the safety and verifiability of MARL for real-world applications. However, if these surrogates are to interact directly with the environment within human supervisory frameworks, they must be both performant and computationally efficient. Prior work on interpretable MARL has either sacrificed performance for computational efficiency or computational efficiency for performance. To address this issue, we propose HYDRAVIPER, a decision tree-based interpretable MARL algorithm. HYDRAVIPER coordinates training between agents based on expected team performance, and adaptively allocates budgets for environment interaction to improve computational efficiency. Experiments on standard benchmark environments for multi-agent coordination and traffic signal control show that HYDRAVIPER matches the performance of state-of-the-art methods using a fraction of the runtime, and that it maintains a Pareto frontier of performance for different interaction budgets."
  },
  {
    "title": "A Snapshot of Influence: A Local Data Attribution Framework for Online Reinforcement Learning",
    "url": "http://arxiv.org/abs/2505.19281v1",
    "arxiv_id": "2505.19281v1",
    "authors": [
      "Yuzheng Hu",
      "Fan Wu",
      "Haotian Ye",
      "David Forsyth",
      "James Zou",
      "Nan Jiang",
      "Jiaqi W. Ma",
      "Han Zhao"
    ],
    "published": "2025-05-25T19:25:57+00:00",
    "summary": "Online reinforcement learning (RL) excels in complex, safety-critical domains, yet it faces challenges such as sample inefficiency, training instability, and a lack of interpretability. Data attribution offers a principled way to trace model behavior back to individual training samples. However, in online RL, each training sample not only drives policy updates but also influences future data collection, violating the fixed dataset assumption in existing attribution methods. In this paper, we initiate the study of data attribution for online RL, focusing on the widely used Proximal Policy Optimization (PPO) algorithm. We start by establishing a local attribution framework, interpreting model checkpoints with respect to the records in the recent training buffer. We design two target functions, capturing agent action and cumulative return respectively, and measure each record's contribution through gradient similarity between its training loss and these targets. We demonstrate the power of this framework through three concrete applications: diagnosis of learning, temporal analysis of behavior formation, and targeted intervention during training. Leveraging this framework, we further propose an algorithm, iterative influence-based filtering (IIF), for online RL training that iteratively performs experience filtering to refine policy updates. Across standard RL benchmarks (classic control, navigation, locomotion) to RLHF for large language models, IIF reduces sample complexity, speeds up training, and achieves higher returns. Overall, these results advance interpretability, efficiency, and effectiveness of online RL."
  },
  {
    "title": "VerifyThisBench: Generating Code, Specifications, and Proofs All at Once",
    "url": "http://arxiv.org/abs/2505.19271v1",
    "arxiv_id": "2505.19271v1",
    "authors": [
      "Xun Deng",
      "Sicheng Zhong",
      "Andreas Veneris",
      "Fan Long",
      "Xujie Si"
    ],
    "published": "2025-05-25T19:00:52+00:00",
    "summary": "Large language models (LLMs) have demonstrated remarkable progress in code generation, but many existing benchmarks are approaching saturation and offer little guarantee on the trustworthiness of the generated programs, offering limited insight into deeper reasoning capabilities. We introduce VerifyThisBench, a new benchmark designed to evaluate LLMs on end-to-end program verification tasks that require interpreting natural language problem descriptions, formulating formal specifications, generating code, and constructing correctness proofs. Our evaluation reveals that even state-of-the-art (SOTA) models, such as o3-mini, achieve a pass rate of less than 4%, with many outputs failing to compile. To reduce task complexity, we further propose VerifyThisBenchXS, a variant in which partial implementations or proofs are provided. We systematically assess SOTA models on both benchmarks, uncovering key strengths and limitations in their formal reasoning and verification capabilities."
  },
  {
    "title": "ALRPHFS: Adversarially Learned Risk Patterns with Hierarchical Fast \\& Slow Reasoning for Robust Agent Defense",
    "url": "http://arxiv.org/abs/2505.19260v1",
    "arxiv_id": "2505.19260v1",
    "authors": [
      "Shiyu Xiang",
      "Tong Zhang",
      "Ronghao Chen"
    ],
    "published": "2025-05-25T18:31:48+00:00",
    "summary": "LLM Agents are becoming central to intelligent systems. However, their deployment raises serious safety concerns. Existing defenses largely rely on \"Safety Checks\", which struggle to capture the complex semantic risks posed by harmful user inputs or unsafe agent behaviors - creating a significant semantic gap between safety checks and real-world risks. To bridge this gap, we propose a novel defense framework, ALRPHFS (Adversarially Learned Risk Patterns with Hierarchical Fast & Slow Reasoning). ALRPHFS consists of two core components: (1) an offline adversarial self-learning loop to iteratively refine a generalizable and balanced library of risk patterns, substantially enhancing robustness without retraining the base LLM, and (2) an online hierarchical fast & slow reasoning engine that balances detection effectiveness with computational efficiency. Experimental results demonstrate that our approach achieves superior overall performance compared to existing baselines, achieving a best-in-class average accuracy of 80% and exhibiting strong generalizability across agents and tasks."
  },
  {
    "title": "LLLMs: A Data-Driven Survey of Evolving Research on Limitations of Large Language Models",
    "url": "http://arxiv.org/abs/2505.19240v1",
    "arxiv_id": "2505.19240v1",
    "authors": [
      "Aida Kostikova",
      "Zhipin Wang",
      "Deidamea Bajri",
      "Ole P\u00fctz",
      "Benjamin Paa\u00dfen",
      "Steffen Eger"
    ],
    "published": "2025-05-25T17:38:32+00:00",
    "summary": "Large language model (LLM) research has grown rapidly, along with increasing concern about their limitations such as failures in reasoning, hallucinations, and limited multilingual capability. In this survey, we conduct a data-driven, semi-automated review of research on limitations of LLM (LLLMs) from 2022 to 2024 using a bottom-up approach. From a corpus of 250,000 ACL and arXiv papers, we identify 14,648 relevant papers using keyword filtering, LLM-based classification, validated against expert labels, and topic clustering (via two approaches, HDBSCAN+BERTopic and LlooM). We find that LLM-related research increases over fivefold in ACL and fourfold in arXiv. Since 2022, LLLMs research grows even faster, reaching over 30% of LLM papers by late 2024. Reasoning remains the most studied limitation, followed by generalization, hallucination, bias, and security. The distribution of topics in the ACL dataset stays relatively stable over time, while arXiv shifts toward safety and controllability (with topics like security risks, alignment, hallucinations, knowledge editing), and multimodality between 2022 and 2024. We release a dataset of annotated abstracts and a validated methodology, and offer a quantitative view of trends in LLM limitations research."
  },
  {
    "title": "GUARDIAN: Safeguarding LLM Multi-Agent Collaborations with Temporal Graph Modeling",
    "url": "http://arxiv.org/abs/2505.19234v1",
    "arxiv_id": "2505.19234v1",
    "authors": [
      "Jialong Zhou",
      "Lichao Wang",
      "Xiao Yang"
    ],
    "published": "2025-05-25T17:15:55+00:00",
    "summary": "The emergence of large language models (LLMs) enables the development of intelligent agents capable of engaging in complex and multi-turn dialogues. However, multi-agent collaboration face critical safety challenges, such as hallucination amplification and error injection and propagation. This paper presents GUARDIAN, a unified method for detecting and mitigating multiple safety concerns in GUARDing Intelligent Agent collaboratioNs. By modeling the multi-agent collaboration process as a discrete-time temporal attributed graph, GUARDIAN explicitly captures the propagation dynamics of hallucinations and errors. The unsupervised encoder-decoder architecture incorporating an incremental training paradigm, learns to reconstruct node attributes and graph structures from latent embeddings, enabling the identification of anomalous nodes and edges with unparalleled precision. Moreover, we introduce a graph abstraction mechanism based on the Information Bottleneck Theory, which compresses temporal interaction graphs while preserving essential patterns. Extensive experiments demonstrate GUARDIAN's effectiveness in safeguarding LLM multi-agent collaborations against diverse safety vulnerabilities, achieving state-of-the-art accuracy with efficient resource utilization."
  },
  {
    "title": "When Ethics and Payoffs Diverge: LLM Agents in Morally Charged Social Dilemmas",
    "url": "http://arxiv.org/abs/2505.19212v1",
    "arxiv_id": "2505.19212v1",
    "authors": [
      "Steffen Backmann",
      "David Guzman Piedrahita",
      "Emanuel Tewolde",
      "Rada Mihalcea",
      "Bernhard Sch\u00f6lkopf",
      "Zhijing Jin"
    ],
    "published": "2025-05-25T16:19:24+00:00",
    "summary": "Recent advances in large language models (LLMs) have enabled their use in complex agentic roles, involving decision-making with humans or other agents, making ethical alignment a key AI safety concern. While prior work has examined both LLMs' moral judgment and strategic behavior in social dilemmas, there is limited understanding of how they act when moral imperatives directly conflict with rewards or incentives. To investigate this, we introduce Moral Behavior in Social Dilemma Simulation (MoralSim) and evaluate how LLMs behave in the prisoner's dilemma and public goods game with morally charged contexts. In MoralSim, we test a range of frontier models across both game structures and three distinct moral framings, enabling a systematic examination of how LLMs navigate social dilemmas in which ethical norms conflict with payoff-maximizing strategies. Our results show substantial variation across models in both their general tendency to act morally and the consistency of their behavior across game types, the specific moral framing, and situational factors such as opponent behavior and survival risks. Crucially, no model exhibits consistently moral behavior in MoralSim, highlighting the need for caution when deploying LLMs in agentic roles where the agent's \"self-interest\" may conflict with ethical expectations. Our code is available at https://github.com/sbackmann/moralsim."
  },
  {
    "title": "Benchmarking Laparoscopic Surgical Image Restoration and Beyond",
    "url": "http://arxiv.org/abs/2505.19161v1",
    "arxiv_id": "2505.19161v1",
    "authors": [
      "Jialun Pei",
      "Diandian Guo",
      "Donghui Yang",
      "Zhixi Li",
      "Yuxin Feng",
      "Long Ma",
      "Bo Du",
      "Pheng-Ann Heng"
    ],
    "published": "2025-05-25T14:17:56+00:00",
    "summary": "In laparoscopic surgery, a clear and high-quality visual field is critical for surgeons to make accurate intraoperative decisions. However, persistent visual degradation, including smoke generated by energy devices, lens fogging from thermal gradients, and lens contamination due to blood or tissue fluid splashes during surgical procedures, severely impair visual clarity. These degenerations can seriously hinder surgical workflow and pose risks to patient safety. To systematically investigate and address various forms of surgical scene degradation, we introduce a real-world open-source surgical image restoration dataset covering laparoscopic environments, called SurgClean, which involves multi-type image restoration tasks, e.g., desmoking, defogging, and desplashing. SurgClean comprises 1,020 images with diverse degradation types and corresponding paired reference labels. Based on SurgClean, we establish a standardized evaluation benchmark and provide performance for 22 representative generic task-specific image restoration approaches, including 12 generic and 10 task-specific image restoration approaches. Experimental results reveal substantial performance gaps relative to clinical requirements, highlighting a critical opportunity for algorithm advancements in intelligent surgical restoration. Furthermore, we explore the degradation discrepancies between surgical and natural scenes from structural perception and semantic understanding perspectives, providing fundamental insights for domain-specific image restoration research. Our work aims to empower the capabilities of restoration algorithms to increase surgical environments and improve the efficiency of clinical procedures."
  },
  {
    "title": "MMATH: A Multilingual Benchmark for Mathematical Reasoning",
    "url": "http://arxiv.org/abs/2505.19126v1",
    "arxiv_id": "2505.19126v1",
    "authors": [
      "Wenyang Luo",
      "Wayne Xin Zhao",
      "Jing Sha",
      "Shijin Wang",
      "Ji-Rong Wen"
    ],
    "published": "2025-05-25T12:47:39+00:00",
    "summary": "The advent of large reasoning models, such as OpenAI o1 and DeepSeek R1, has significantly advanced complex reasoning tasks. However, their capabilities in multilingual complex reasoning remain underexplored, with existing efforts largely focused on simpler tasks like MGSM. To address this gap, we introduce MMATH, a benchmark for multilingual complex reasoning spanning 374 high-quality math problems across 10 typologically diverse languages. Using MMATH, we observe that even advanced models like DeepSeek R1 exhibit substantial performance disparities across languages and suffer from a critical off-target issue-generating responses in unintended languages. To address this, we explore strategies including prompting and training, demonstrating that reasoning in English and answering in target languages can simultaneously enhance performance and preserve target-language consistency. Our findings offer new insights and practical strategies for advancing the multilingual reasoning capabilities of large language models. Our code and data could be found at https://github.com/RUCAIBox/MMATH."
  },
  {
    "title": "SATORI-R1: Incentivizing Multimodal Reasoning with Spatial Grounding and Verifiable Rewards",
    "url": "http://arxiv.org/abs/2505.19094v1",
    "arxiv_id": "2505.19094v1",
    "authors": [
      "Chuming Shen",
      "Wei Wei",
      "Xiaoye Qu",
      "Yu Cheng"
    ],
    "published": "2025-05-25T11:11:06+00:00",
    "summary": "DeepSeek-R1 has demonstrated powerful reasoning capabilities in the text domain through stable reinforcement learning (RL). Recently, in the multimodal domain, works have begun to directly apply RL to generate R1-like free-form reasoning for Visual Question Answering (VQA) tasks. However, multimodal tasks share an intrinsically different nature from textual tasks, which heavily rely on the understanding of the input image to solve the problem. Therefore, such free-form reasoning faces two critical limitations in the VQA task: (1) Extended reasoning chains diffuse visual focus away from task-critical regions, degrading answer accuracy. (2) Unverifiable intermediate steps amplify policy-gradient variance and computational costs overhead. To address these issues, in this paper, we introduce SATORI ($\\textbf{S}patially$ $\\textbf{A}nchored$ $\\textbf{T}ask$ $\\textbf{O}ptimization$ with $\\textbf{R}e\\textbf{I}nforcement$ Learning), which decomposes VQA into three verifiable stages, including global image captioning, region localization, and answer prediction, each supplying explicit reward signals. Furthermore, we also introduce VQA-Verify, a 12k dataset annotated with answer-aligned captions and bounding-boxes to facilitate training. Experiments demonstrate consistent performance improvements across seven VQA benchmarks, achieving up to $15.7\\%$ improvement in accuracy in accuracy compared to the R1-like baseline. Our analysis of the attention map confirms enhanced focus on critical regions, which brings improvements in accuracy. Our code is available at https://github.com/justairr/SATORI-R1."
  },
  {
    "title": "Failure divergence refinement for Event-B",
    "url": "http://arxiv.org/abs/2505.19069v1",
    "arxiv_id": "2505.19069v1",
    "authors": [
      "Sebastian Stock",
      "Michael Leuschel",
      "Atif Mashkoor"
    ],
    "published": "2025-05-25T10:07:09+00:00",
    "summary": "When validating formal models, sizable effort goes into ensuring two types of properties: safety properties (nothing bad happens) and liveness properties (something good occurs eventually. Event-B supports checking safety properties all through the refinement chain. The same is not valid for liveness properties. Liveness properties are commonly validated with additional techniques like animation, and results do not transfer quickly, leading to re-doing the validation process at every refinement stage. This paper promotes early validation by providing failure divergence refinement semantics for Event-B. We show that failure divergence refinement preserves trace properties, which comprise many liveness properties, under certain natural conditions. Consequently, re-validation of those properties becomes unnecessary. Our result benefits data refinements, where no abstract behavior should be removed during refinement. Furthermore, we lay out an algorithm and provide a tool for automatic failure divergence refinement checking, significantly decreasing the modeler's workload. The tool is compared and evaluated in the context of sizable case studies."
  },
  {
    "title": "An Embarrassingly Simple Defense Against LLM Abliteration Attacks",
    "url": "http://arxiv.org/abs/2505.19056v1",
    "arxiv_id": "2505.19056v1",
    "authors": [
      "Harethah Abu Shairah",
      "Hasan Abed Al Kader Hammoud",
      "Bernard Ghanem",
      "George Turkiyyah"
    ],
    "published": "2025-05-25T09:18:24+00:00",
    "summary": "Large language models (LLMs) are typically aligned to comply with safety guidelines by refusing harmful instructions. A recent attack, termed abliteration, isolates and suppresses the single latent direction most responsible for refusal behavior, enabling the model to generate unethical content. We propose a defense that modifies how models generate refusals. We construct an extended-refusal dataset that contains harmful prompts with a full response that justifies the reason for refusal. We then fine-tune Llama-2-7B-Chat and Qwen2.5-Instruct (1.5B and 3B parameters) on our extended-refusal dataset, and evaluate the resulting systems on a set of harmful prompts. In our experiments, extended-refusal models maintain high refusal rates, dropping at most by 10%, whereas baseline models' refusal rates drop by 70-80% after abliteration. A broad evaluation of safety and utility shows that extended-refusal fine-tuning neutralizes the abliteration attack while preserving general performance."
  },
  {
    "title": "WorldEval: World Model as Real-World Robot Policies Evaluator",
    "url": "http://arxiv.org/abs/2505.19017v1",
    "arxiv_id": "2505.19017v1",
    "authors": [
      "Yaxuan Li",
      "Yichen Zhu",
      "Junjie Wen",
      "Chaomin Shen",
      "Yi Xu"
    ],
    "published": "2025-05-25T07:41:39+00:00",
    "summary": "The field of robotics has made significant strides toward developing generalist robot manipulation policies. However, evaluating these policies in real-world scenarios remains time-consuming and challenging, particularly as the number of tasks scales and environmental conditions change. In this work, we demonstrate that world models can serve as a scalable, reproducible, and reliable proxy for real-world robot policy evaluation. A key challenge is generating accurate policy videos from world models that faithfully reflect the robot actions. We observe that directly inputting robot actions or using high-dimensional encoding methods often fails to generate action-following videos. To address this, we propose Policy2Vec, a simple yet effective approach to turn a video generation model into a world simulator that follows latent action to generate the robot video. We then introduce WorldEval, an automated pipeline designed to evaluate real-world robot policies entirely online. WorldEval effectively ranks various robot policies and individual checkpoints within a single policy, and functions as a safety detector to prevent dangerous actions by newly developed robot models. Through comprehensive paired evaluations of manipulation policies in real-world environments, we demonstrate a strong correlation between policy performance in WorldEval and real-world scenarios. Furthermore, our method significantly outperforms popular methods such as real-to-sim approach."
  },
  {
    "title": "Secure IVSHMEM: End-to-End Shared-Memory Protocol with Hypervisor-CA Handshake and In-Kernel Access Control",
    "url": "http://arxiv.org/abs/2505.19004v1",
    "arxiv_id": "2505.19004v1",
    "authors": [
      "Hyunwoo Kim",
      "Jaeseong Lee",
      "Sunpyo Hong",
      "Changmin Han"
    ],
    "published": "2025-05-25T07:02:41+00:00",
    "summary": "In-host shared memory (IVSHMEM) enables high-throughput, zero-copy communication between virtual machines, but today's implementations lack any security control, allowing any application to eavesdrop or tamper with the IVSHMEM region. This paper presents Secure IVSHMEM, a protocol that provides end-to-end mutual authentication and fine-grained access enforcement with negligible performance cost. We combine three techniques to ensure security: (1) channel separation and kernel module access control, (2)hypervisor-mediated handshake for end-to-end service authentication, and (3)application-level integration for abstraction and performance mitigation. In microbenchmarks, Secure IVSHMEM completes its one-time handshake in under 200ms and sustains data-plane round-trip latencies within 5\\% of the unmodified baseline, with negligible bandwidth overhead. We believe this design is ideally suited for safety and latency-critical in-host domains, such as automotive systems, where both performance and security are paramount."
  },
  {
    "title": "Property Directed Reachability with Extended Resolution",
    "url": "http://arxiv.org/abs/2505.18998v1",
    "arxiv_id": "2505.18998v1",
    "authors": [
      "Andrew Luka",
      "Yakir Vizel"
    ],
    "published": "2025-05-25T06:37:41+00:00",
    "summary": "Property Directed Reachability (\\textsc{Pdr}), also known as IC3, is a state-of-the-art model checking algorithm widely used for verifying safety properties. While \\textsc{Pdr} is effective in finding inductive invariants, its underlying proof system, Resolution, limits its ability to construct short proofs for certain verification problems.   This paper introduces \\textsc{PdrER}, a novel generalization of \\textsc{Pdr} that uses Extended Resolution (ER), a proof system exponentially stronger than Resolution, when constructing a proof of correctness. \\PdrEV leverages ER to construct shorter bounded proofs of correctness, enabling it to discover more compact inductive invariants. While \\PdrEV is based on \\textsc{Pdr}, it includes algorithmic enhancements that had to be made in order to efficiently use ER in the context of model checking.   We implemented \\textsc{PdrER} in a new open-source verification framework and evaluated it on the Hardware Model Checking Competition benchmarks from 2019, 2020 and 2024. Our experimental evaluation demonstrates that \\textsc{PdrER} outperforms \\textsc{Pdr}, solving more instances in less time and uniquely solving problems that \\textsc{Pdr} cannot solve within a given time limit. We argue that this paper represents a significant step toward making strong proof systems practically usable in model checking."
  },
  {
    "title": "First Finish Search: Efficient Test-Time Scaling in Large Language Models",
    "url": "http://arxiv.org/abs/2505.18149v1",
    "arxiv_id": "2505.18149v1",
    "authors": [
      "Aradhye Agarwal",
      "Ayan Sengupta",
      "Tanmoy Chakraborty"
    ],
    "published": "2025-05-23T17:57:43+00:00",
    "summary": "Test-time scaling (TTS), which involves dynamic allocation of compute during inference, offers a promising way to improve reasoning in large language models. While existing TTS methods work well, they often rely on long decoding paths or require a large number of samples to be generated, increasing the token usage and inference latency. We observe the surprising fact that for reasoning tasks, shorter traces are much more likely to be correct than longer ones. Motivated by this, we introduce First Finish Search (FFS), a training-free parallel decoding strategy that launches $n$ independent samples and returns as soon as any one completes. We evaluate FFS alongside simple decoding, beam search, majority voting, and budget forcing on four reasoning models (DeepSeek-R1, R1-Distill-Qwen-32B, QwQ-32B and Phi-4-Reasoning-Plus) and across four datasets (AIME24, AIME25-I, AIME25-II and GPQA Diamond). With DeepSeek-R1, FFS achieves $82.23\\%$ accuracy on the AIME datasets, a $15\\%$ improvement over DeepSeek-R1's standalone accuracy, nearly matching OpenAI's o4-mini performance. Our theoretical analysis explains why stopping at the shortest trace is likely to yield a correct answer and identifies the conditions under which early stopping may be suboptimal. The elegance and simplicity of FFS demonstrate that straightforward TTS strategies can perform remarkably well, revealing the untapped potential of simple approaches at inference time."
  },
  {
    "title": "Stable Reinforcement Learning for Efficient Reasoning",
    "url": "http://arxiv.org/abs/2505.18086v1",
    "arxiv_id": "2505.18086v1",
    "authors": [
      "Muzhi Dai",
      "Shixuan Liu",
      "Qingyi Si"
    ],
    "published": "2025-05-23T16:43:03+00:00",
    "summary": "The success of Deepseek-R1 has drawn the LLM community's attention to reinforcement learning (RL) methods like GRPO. However, such rule-based 0/1 outcome reward methods lack the capability to regulate the intermediate reasoning processes during chain-of-thought (CoT) generation, leading to severe overthinking phenomena. In response, recent studies have designed reward functions to reinforce models' behaviors in producing shorter yet correct completions. Nevertheless, we observe that these length-penalty reward functions exacerbate RL training instability: as the completion length decreases, model accuracy abruptly collapses, often occurring early in training. To address this issue, we propose a simple yet effective solution GRPO-$\\lambda$, an efficient and stabilized variant of GRPO, which dynamically adjusts the reward strategy by monitoring the correctness ratio among completions within each query-sampled group. A low correctness ratio indicates the need to avoid length penalty that compromises CoT quality, triggering a switch to length-agnostic 0/1 rewards that prioritize reasoning capability. A high ratio maintains length penalties to boost efficiency. Experimental results show that our approach avoids training instability caused by length penalty while maintaining the optimal accuracy-efficiency trade-off. On the GSM8K, GPQA, MATH-500, AMC 2023, and AIME 2024 benchmarks, it improves average accuracy by 1.48% while reducing CoT sequence length by 47.3%."
  },
  {
    "title": "Mahalanobis++: Improving OOD Detection via Feature Normalization",
    "url": "http://arxiv.org/abs/2505.18032v1",
    "arxiv_id": "2505.18032v1",
    "authors": [
      "Maximilian Mueller",
      "Matthias Hein"
    ],
    "published": "2025-05-23T15:36:22+00:00",
    "summary": "Detecting out-of-distribution (OOD) examples is an important task for deploying reliable machine learning models in safety-critial applications. While post-hoc methods based on the Mahalanobis distance applied to pre-logit features are among the most effective for ImageNet-scale OOD detection, their performance varies significantly across models. We connect this inconsistency to strong variations in feature norms, indicating severe violations of the Gaussian assumption underlying the Mahalanobis distance estimation. We show that simple $\\ell_2$-normalization of the features mitigates this problem effectively, aligning better with the premise of normally distributed data with shared covariance matrix. Extensive experiments on 44 models across diverse architectures and pretraining schemes show that $\\ell_2$-normalization improves the conventional Mahalanobis distance-based approaches significantly and consistently, and outperforms other recently proposed OOD detection methods."
  },
  {
    "title": "SemSegBench & DetecBench: Benchmarking Reliability and Generalization Beyond Classification",
    "url": "http://arxiv.org/abs/2505.18015v1",
    "arxiv_id": "2505.18015v1",
    "authors": [
      "Shashank Agnihotri",
      "David Schader",
      "Jonas Jakubassa",
      "Nico Sharei",
      "Simon Kral",
      "Mehmet Ege Ka\u00e7ar",
      "Ruben Weber",
      "Margret Keuper"
    ],
    "published": "2025-05-23T15:17:45+00:00",
    "summary": "Reliability and generalization in deep learning are predominantly studied in the context of image classification. Yet, real-world applications in safety-critical domains involve a broader set of semantic tasks, such as semantic segmentation and object detection, which come with a diverse set of dedicated model architectures. To facilitate research towards robust model design in segmentation and detection, our primary objective is to provide benchmarking tools regarding robustness to distribution shifts and adversarial manipulations. We propose the benchmarking tools SEMSEGBENCH and DETECBENCH, along with the most extensive evaluation to date on the reliability and generalization of semantic segmentation and object detection models. In particular, we benchmark 76 segmentation models across four datasets and 61 object detectors across two datasets, evaluating their performance under diverse adversarial attacks and common corruptions. Our findings reveal systematic weaknesses in state-of-the-art models and uncover key trends based on architecture, backbone, and model capacity. SEMSEGBENCH and DETECBENCH are open-sourced in our GitHub repository (https://github.com/shashankskagnihotri/benchmarking_reliability_generalization) along with our complete set of total 6139 evaluations. We anticipate the collected data to foster and encourage future research towards improved model reliability beyond classification."
  },
  {
    "title": "Classification of assembly tasks combining multiple primitive actions using Transformers and xLSTMs",
    "url": "http://arxiv.org/abs/2505.18012v1",
    "arxiv_id": "2505.18012v1",
    "authors": [
      "Miguel Neves",
      "Pedro Neto"
    ],
    "published": "2025-05-23T15:14:32+00:00",
    "summary": "The classification of human-performed assembly tasks is essential in collaborative robotics to ensure safety, anticipate robot actions, and facilitate robot learning. However, achieving reliable classification is challenging when segmenting tasks into smaller primitive actions is unfeasible, requiring us to classify long assembly tasks that encompass multiple primitive actions. In this study, we propose classifying long assembly sequential tasks based on hand landmark coordinates and compare the performance of two well-established classifiers, LSTM and Transformer, as well as a recent model, xLSTM. We used the HRC scenario proposed in the CT benchmark, which includes long assembly tasks that combine actions such as insertions, screw fastenings, and snap fittings. Testing was conducted using sequences gathered from both the human operator who performed the training sequences and three new operators. The testing results of real-padded sequences for the LSTM, Transformer, and xLSTM models was 72.9%, 95.0% and 93.2% for the training operator, and 43.5%, 54.3% and 60.8% for the new operators, respectively. The LSTM model clearly underperformed compared to the other two approaches. As expected, both the Transformer and xLSTM achieved satisfactory results for the operator they were trained on, though the xLSTM model demonstrated better generalization capabilities to new operators. The results clearly show that for this type of classification, the xLSTM model offers a slight edge over Transformers."
  },
  {
    "title": "An Example Safety Case for Safeguards Against Misuse",
    "url": "http://arxiv.org/abs/2505.18003v1",
    "arxiv_id": "2505.18003v1",
    "authors": [
      "Joshua Clymer",
      "Jonah Weinbaum",
      "Robert Kirk",
      "Kimberly Mai",
      "Selena Zhang",
      "Xander Davies"
    ],
    "published": "2025-05-23T15:06:21+00:00",
    "summary": "Existing evaluations of AI misuse safeguards provide a patchwork of evidence that is often difficult to connect to real-world decisions. To bridge this gap, we describe an end-to-end argument (a \"safety case\") that misuse safeguards reduce the risk posed by an AI assistant to low levels. We first describe how a hypothetical developer red teams safeguards, estimating the effort required to evade them. Then, the developer plugs this estimate into a quantitative \"uplift model\" to determine how much barriers introduced by safeguards dissuade misuse (https://www.aimisusemodel.com/). This procedure provides a continuous signal of risk during deployment that helps the developer rapidly respond to emerging threats. Finally, we describe how to tie these components together into a simple safety case. Our work provides one concrete path -- though not the only path -- to rigorously justifying AI misuse risks are low."
  },
  {
    "title": "Automated Formal Verification of Area-Optimized Safety Registers in Automotive SoCs",
    "url": "http://arxiv.org/abs/2505.17990v1",
    "arxiv_id": "2505.17990v1",
    "authors": [
      "Shuhang Zhang",
      "Bryan Olmos"
    ],
    "published": "2025-05-23T14:57:16+00:00",
    "summary": "Registers are primary storage elements in System-on-chip~(SoC) designs and play an important role in maintaining state information and processing data in digital systems. With respect to the ISO26262 standard, these registers require high levels of reliability and fault tolerance. For this reason, safety-critical applications require that normal registers are equipped with additional safety components to construct safety registers, which ensure system stability and fault tolerance. However, the process of integrating these safety registers is complex and error-prone, because of highly-configurable features provided by a safety library such as parameterized modules and flexible safety structures. In addition, to address the overhead caused by the safety registers, we have applied area optimization techniques to their implementation. However, this optimization can make the integration process more susceptible to errors. To avoid any integration mistakes, rigorous verification is always required, but it is time-consuming and error-prone if the verification is implemented manually when dealing with numerous verification requests. To address these challenges, we propose an automated flow for the verification of safety registers with the formal approach. The results indicate that this automated verification approach has the potential to reduce the verification effort by more than 80\\%. Additionally, it ensures a comprehensive examination of every requirement of this safety library, which is reflected in faster detection of bugs. The proposed framework can be replicated for the verification of other safety components enabling an early detection of potential issues and saving valuable time and resources."
  },
  {
    "title": "Outcome-based Reinforcement Learning to Predict the Future",
    "url": "http://arxiv.org/abs/2505.17989v1",
    "arxiv_id": "2505.17989v1",
    "authors": [
      "Benjamin Turtel",
      "Danny Franklin",
      "Kris Skotheim",
      "Luke Hewitt",
      "Philipp Schoenegger"
    ],
    "published": "2025-05-23T14:56:07+00:00",
    "summary": "Reinforcement learning with verifiable rewards (RLVR) has boosted math and coding in large language models, yet there has been little effort to extend RLVR into messier, real-world domains like forecasting. One sticking point is that outcome-based reinforcement learning for forecasting must learn from binary, delayed, and noisy rewards, a regime where standard fine-tuning is brittle. We show that outcome-only online RL on a 14B model can match frontier-scale accuracy and surpass it in calibration and hypothetical prediction market betting by adapting two leading algorithms, Group-Relative Policy Optimisation (GRPO) and ReMax, to the forecasting setting. Our adaptations remove per-question variance scaling in GRPO, apply baseline-subtracted advantages in ReMax, hydrate training with 100k temporally consistent synthetic questions, and introduce lightweight guard-rails that penalise gibberish, non-English responses and missing rationales, enabling a single stable pass over 110k events. Scaling ReMax to 110k questions and ensembling seven predictions yields a 14B model that matches frontier baseline o1 on accuracy on our holdout set (Brier = 0.193, p = 0.23) while beating it in calibration (ECE = 0.042, p < 0.001). A simple trading rule turns this calibration edge into \\$127 of hypothetical profit versus \\$92 for o1 (p = 0.037). This demonstrates that refined RLVR methods can convert small-scale LLMs into potentially economically valuable forecasting tools, with implications for scaling this to larger models."
  },
  {
    "title": "Re-evaluation of Logical Specification in Behavioural Verification",
    "url": "http://arxiv.org/abs/2505.17979v1",
    "arxiv_id": "2505.17979v1",
    "authors": [
      "Radoslaw Klimek",
      "Jakub Semczyszyn"
    ],
    "published": "2025-05-23T14:46:39+00:00",
    "summary": "This study empirically validates automated logical specification methods for behavioural models, focusing on their robustness, scalability, and reproducibility. By the systematic reproduction and extension of prior results, we confirm key trends, while identifying performance irregularities that suggest the need for adaptive heuristics in automated reasoning. Our findings highlight that theorem provers exhibit varying efficiency across problem structures, with implications for real-time verification in CI/CD pipelines and AI-driven IDEs supporting on-the-fly validation. Addressing these inefficiencies through self-optimising solvers could enhance the stability of automated reasoning, particularly in safety-critical software verification."
  },
  {
    "title": "Counting Cycles with Deepseek",
    "url": "http://arxiv.org/abs/2505.17964v1",
    "arxiv_id": "2505.17964v1",
    "authors": [
      "Jiashun Jin",
      "Tracy Ke",
      "Bingcheng Sui",
      "Zhenggang Wang"
    ],
    "published": "2025-05-23T14:34:40+00:00",
    "summary": "Despite recent progress, AI still struggles on advanced mathematics. We consider a difficult open problem: How to derive a Computationally Efficient Equivalent Form (CEEF) for the cycle count statistic? The CEEF problem does not have known general solutions, and requires delicate combinatorics and tedious calculations. Such a task is hard to accomplish by humans but is an ideal example where AI can be very helpful. We solve the problem by combining a novel approach we propose and the powerful coding skills of AI. Our results use delicate graph theory and contain new formulas for general cases that have not been discovered before. We find that, while AI is unable to solve the problem all by itself, it is able to solve it if we provide it with a clear strategy, a step-by-step guidance and carefully written prompts. For simplicity, we focus our study on DeepSeek-R1 but we also investigate other AI approaches."
  },
  {
    "title": "Mind the Domain Gap: Measuring the Domain Gap Between Real-World and Synthetic Point Clouds for Automated Driving Development",
    "url": "http://arxiv.org/abs/2505.17959v1",
    "arxiv_id": "2505.17959v1",
    "authors": [
      "Nguyen Duc",
      "Yan-Ling Lai",
      "Patrick Madlindl",
      "Xinyuan Zhu",
      "Benedikt Schwab",
      "Olaf Wysocki",
      "Ludwig Hoegner",
      "Thomas H. Kolbe"
    ],
    "published": "2025-05-23T14:31:36+00:00",
    "summary": "Owing to the typical long-tail data distribution issues, simulating domain-gap-free synthetic data is crucial in robotics, photogrammetry, and computer vision research. The fundamental challenge pertains to credibly measuring the difference between real and simulated data. Such a measure is vital for safety-critical applications, such as automated driving, where out-of-domain samples may impact a car's perception and cause fatal accidents. Previous work has commonly focused on simulating data on one scene and analyzing performance on a different, real-world scene, hampering the disjoint analysis of domain gap coming from networks' deficiencies, class definitions, and object representation. In this paper, we propose a novel approach to measuring the domain gap between the real world sensor observations and simulated data representing the same location, enabling comprehensive domain gap analysis. To measure such a domain gap, we introduce a novel metric DoGSS-PCL and evaluation assessing the geometric and semantic quality of the simulated point cloud. Our experiments corroborate that the introduced approach can be used to measure the domain gap. The tests also reveal that synthetic semantic point clouds may be used for training deep neural networks, maintaining the performance at the 50/50 real-to-synthetic ratio. We strongly believe that this work will facilitate research on credible data simulation and allow for at-scale deployment in automated driving testing and digital twinning."
  },
  {
    "title": "VeriThinker: Learning to Verify Makes Reasoning Model Efficient",
    "url": "http://arxiv.org/abs/2505.17941v1",
    "arxiv_id": "2505.17941v1",
    "authors": [
      "Zigeng Chen",
      "Xinyin Ma",
      "Gongfan Fang",
      "Ruonan Yu",
      "Xinchao Wang"
    ],
    "published": "2025-05-23T14:17:56+00:00",
    "summary": "Large Reasoning Models (LRMs) excel at complex tasks using Chain-of-Thought (CoT) reasoning. However, their tendency to overthinking leads to unnecessarily lengthy reasoning chains, dramatically increasing inference costs. To mitigate this issue, we introduce VeriThinker, a novel approach for CoT compression. Unlike conventional methods that fine-tune LRMs directly on the original reasoning task using synthetic concise CoT data, we innovatively fine-tune the model solely through an auxiliary verification task. By training LRMs to accurately verify the correctness of CoT solutions, the LRMs inherently become more discerning about the necessity of subsequent self-reflection steps, thereby effectively suppressing overthinking. Extensive experiments validate that VeriThinker substantially reduces reasoning chain lengths while maintaining or even slightly improving accuracy. When applied to DeepSeek-R1-Distill-Qwen-7B, our approach reduces reasoning tokens on MATH500 from 3790 to 2125 while improving accuracy by 0.8% (94.0% to 94.8%), and on AIME25, tokens decrease from 14321 to 10287 with a 2.1% accuracy gain (38.7% to 40.8%). Additionally, our experiments demonstrate that VeriThinker can also be zero-shot generalized to speculative reasoning. Code is available at https://github.com/czg1225/VeriThinker"
  },
  {
    "title": "Survival Games: Human-LLM Strategic Showdowns under Severe Resource Scarcity",
    "url": "http://arxiv.org/abs/2505.17937v1",
    "arxiv_id": "2505.17937v1",
    "authors": [
      "Zhihong Chen",
      "Yiqian Yang",
      "Jinzhao Zhou",
      "Qiang Zhang",
      "Chin-Teng Lin",
      "Yiqun Duan"
    ],
    "published": "2025-05-23T14:15:15+00:00",
    "summary": "The rapid advancement of large language models (LLMs) raises critical concerns about their ethical alignment, particularly in scenarios where human and AI co-exist under the conflict of interest. This work introduces an extendable, asymmetric, multi-agent simulation-based benchmarking framework to evaluate the moral behavior of LLMs in a novel human-AI co-existence setting featuring consistent living and critical resource management. Building on previous generative agent environments, we incorporate a life-sustaining system, where agents must compete or cooperate for food resources to survive, often leading to ethically charged decisions such as deception, theft, or social influence. We evaluated two types of LLM, DeepSeek and OpenAI series, in a three-agent setup (two humans, one LLM-powered robot), using adapted behavioral detection from the MACHIAVELLI framework and a custom survival-based ethics metric. Our findings reveal stark behavioral differences: DeepSeek frequently engages in resource hoarding, while OpenAI exhibits restraint, highlighting the influence of model design on ethical outcomes. Additionally, we demonstrate that prompt engineering can significantly steer LLM behavior, with jailbreaking prompts significantly enhancing unethical actions, even for highly restricted OpenAI models and cooperative prompts show a marked reduction in unethical actions. Our framework provides a reproducible testbed for quantifying LLM ethics in high-stakes scenarios, offering insights into their suitability for real-world human-AI interactions."
  },
  {
    "title": "A model-free approach to control barrier functions using funnel control",
    "url": "http://arxiv.org/abs/2505.17887v1",
    "arxiv_id": "2505.17887v1",
    "authors": [
      "Lukas Lanza",
      "Johannes K\u00f6hler",
      "Dario Dennst\u00e4dt",
      "Thomas Berger",
      "Karl Worthmann"
    ],
    "published": "2025-05-23T13:34:44+00:00",
    "summary": "Control barrier functions (CBFs) are a popular approach to design feedback laws that achieve safety guarantees for nonlinear systems. The CBF-based controller design relies on the availability of a model to select feasible inputs from the set of CBF-based controls. In this paper, we develop a model-free approach to design CBF-based control laws, eliminating the need for knowledge of system dynamics or parameters. Specifically, we address safety requirements characterized by a time-varying distance to a reference trajectory in the output space and construct a CBF that depends only on the measured output. Utilizing this particular CBF, we determine a subset of CBF-based controls without relying on a model of the dynamics by using techniques from funnel control. The latter is a model-free high-gain adaptive control methodology, which achieves tracking guarantees via reactive feedback. In this paper, we discover and establish a connection between the modular controller synthesis via zeroing CBFs and model-free reactive feedback. The theoretical results are illustrated by a numerical simulation."
  },
  {
    "title": "Out of the Shadows: Exploring a Latent Space for Neural Network Verification",
    "url": "http://arxiv.org/abs/2505.17854v1",
    "arxiv_id": "2505.17854v1",
    "authors": [
      "Lukas Koller",
      "Tobias Ladner",
      "Matthias Althoff"
    ],
    "published": "2025-05-23T13:05:07+00:00",
    "summary": "Neural networks are ubiquitous. However, they are often sensitive to small input changes. Hence, to prevent unexpected behavior in safety-critical applications, their formal verification -- a notoriously hard problem -- is necessary. Many state-of-the-art verification algorithms use reachability analysis or abstract interpretation to enclose the set of possible outputs of a neural network. Often, the verification is inconclusive due to the conservatism of the enclosure. To address this problem, we design a novel latent space for formal verification that enables the transfer of output specifications to the input space for an iterative specification-driven input refinement, i.e., we iteratively reduce the set of possible inputs to only enclose the unsafe ones. The latent space is constructed from a novel view of projection-based set representations, e.g., zonotopes, which are commonly used in reachability analysis of neural networks. A projection-based set representation is a \"shadow\" of a higher-dimensional set -- a latent space -- that does not change during a set propagation through a neural network. Hence, the input set and the output enclosure are \"shadows\" of the same latent space that we can use to transfer constraints. We present an efficient verification tool for neural networks that uses our iterative refinement to significantly reduce the number of subproblems in a branch-and-bound procedure. Using zonotopes as a set representation, unlike many other state-of-the-art approaches, our approach can be realized by only using matrix operations, which enables a significant speed-up through efficient GPU acceleration. We demonstrate that our tool achieves competitive performance, which would place it among the top-ranking tools of the last neural network verification competition (VNN-COMP'24)."
  },
  {
    "title": "Not All Tokens Are What You Need In Thinking",
    "url": "http://arxiv.org/abs/2505.17827v1",
    "arxiv_id": "2505.17827v1",
    "authors": [
      "Hang Yuan",
      "Bin Yu",
      "Haotian Li",
      "Shijun Yang",
      "Christina Dan Wang",
      "Zhou Yu",
      "Xueyin Xu",
      "Weizhen Qi",
      "Kai Chen"
    ],
    "published": "2025-05-23T12:41:29+00:00",
    "summary": "Modern reasoning models, such as OpenAI's o1 and DeepSeek-R1, exhibit impressive problem-solving capabilities but suffer from critical inefficiencies: high inference latency, excessive computational resource consumption, and a tendency toward overthinking -- generating verbose chains of thought (CoT) laden with redundant tokens that contribute minimally to the final answer. To address these issues, we propose Conditional Token Selection (CTS), a token-level compression framework with a flexible and variable compression ratio that identifies and preserves only the most essential tokens in CoT. CTS evaluates each token's contribution to deriving correct answers using conditional importance scoring, then trains models on compressed CoT. Extensive experiments demonstrate that CTS effectively compresses long CoT while maintaining strong reasoning performance. Notably, on the GPQA benchmark, Qwen2.5-14B-Instruct trained with CTS achieves a 9.1% accuracy improvement with 13.2% fewer reasoning tokens (13% training token reduction). Further reducing training tokens by 42% incurs only a marginal 5% accuracy drop while yielding a 75.8% reduction in reasoning tokens, highlighting the prevalence of redundancy in existing CoT."
  },
  {
    "title": "Evaluation Faking: Unveiling Observer Effects in Safety Evaluation of Frontier AI Systems",
    "url": "http://arxiv.org/abs/2505.17815v1",
    "arxiv_id": "2505.17815v1",
    "authors": [
      "Yihe Fan",
      "Wenqi Zhang",
      "Xudong Pan",
      "Min Yang"
    ],
    "published": "2025-05-23T12:31:29+00:00",
    "summary": "As foundation models grow increasingly more intelligent, reliable and trustworthy safety evaluation becomes more indispensable than ever. However, an important question arises: Whether and how an advanced AI system would perceive the situation of being evaluated, and lead to the broken integrity of the evaluation process? During standard safety tests on a mainstream large reasoning model, we unexpectedly observe that the model without any contextual cues would occasionally recognize it is being evaluated and hence behave more safety-aligned. This motivates us to conduct a systematic study on the phenomenon of evaluation faking, i.e., an AI system autonomously alters its behavior upon recognizing the presence of an evaluation context and thereby influencing the evaluation results. Through extensive experiments on a diverse set of foundation models with mainstream safety benchmarks, we reach the main finding termed the observer effects for AI: When the AI system under evaluation is more advanced in reasoning and situational awareness, the evaluation faking behavior becomes more ubiquitous, which reflects in the following aspects: 1) Reasoning models recognize evaluation 16% more often than non-reasoning models. 2) Scaling foundation models (32B to 671B) increases faking by over 30% in some cases, while smaller models show negligible faking. 3) AI with basic memory is 2.3x more likely to recognize evaluation and scores 19% higher on safety tests (vs. no memory). To measure this, we devised a chain-of-thought monitoring technique to detect faking intent and uncover internal signals correlated with such behavior, offering insights for future mitigation studies."
  },
  {
    "title": "But what is your honest answer? Aiding LLM-judges with honest alternatives using steering vectors",
    "url": "http://arxiv.org/abs/2505.17760v1",
    "arxiv_id": "2505.17760v1",
    "authors": [
      "Leon Eshuijs",
      "Archie Chaudhury",
      "Alan McBeth",
      "Ethan Nguyen"
    ],
    "published": "2025-05-23T11:34:02+00:00",
    "summary": "Recent safety evaluations of Large Language Models (LLMs) show that many models exhibit dishonest behavior, such as sycophancy. However, most honesty benchmarks focus exclusively on factual knowledge or explicitly harmful behavior and rely on external judges, which are often unable to detect less obvious forms of dishonesty. In this work, we introduce a new framework, Judge Using Safety-Steered Alternatives (JUSSA), which utilizes steering vectors trained on a single sample to elicit more honest responses from models, helping LLM-judges in the detection of dishonest behavior. To test our framework, we introduce a new manipulation dataset with prompts specifically designed to elicit deceptive responses. We find that JUSSA enables LLM judges to better differentiate between dishonest and benign responses, and helps them identify subtle instances of manipulative behavior."
  },
  {
    "title": "Automating Safety Enhancement for LLM-based Agents with Synthetic Risk Scenarios",
    "url": "http://arxiv.org/abs/2505.17735v1",
    "arxiv_id": "2505.17735v1",
    "authors": [
      "Xueyang Zhou",
      "Weidong Wang",
      "Lin Lu",
      "Jiawen Shi",
      "Guiyao Tie",
      "Yongtian Xu",
      "Lixing Chen",
      "Pan Zhou",
      "Neil Zhenqiang Gong",
      "Lichao Sun"
    ],
    "published": "2025-05-23T10:56:06+00:00",
    "summary": "Large Language Model (LLM)-based agents are increasingly deployed in real-world applications such as \"digital assistants, autonomous customer service, and decision-support systems\", where their ability to \"interact in multi-turn, tool-augmented environments\" makes them indispensable. However, ensuring the safety of these agents remains a significant challenge due to the diverse and complex risks arising from dynamic user interactions, external tool usage, and the potential for unintended harmful behaviors. To address this critical issue, we propose AutoSafe, the first framework that systematically enhances agent safety through fully automated synthetic data generation. Concretely, 1) we introduce an open and extensible threat model, OTS, which formalizes how unsafe behaviors emerge from the interplay of user instructions, interaction contexts, and agent actions. This enables precise modeling of safety risks across diverse scenarios. 2) we develop a fully automated data generation pipeline that simulates unsafe user behaviors, applies self-reflective reasoning to generate safe responses, and constructs a large-scale, diverse, and high-quality safety training dataset-eliminating the need for hazardous real-world data collection. To evaluate the effectiveness of our framework, we design comprehensive experiments on both synthetic and real-world safety benchmarks. Results demonstrate that AutoSafe boosts safety scores by 45% on average and achieves a 28.91% improvement on real-world tasks, validating the generalization ability of our learned safety strategies. These results highlight the practical advancement and scalability of AutoSafe in building safer LLM-based agents for real-world deployment. We have released the project page at https://auto-safe.github.io/."
  },
  {
    "title": "SafeMVDrive: Multi-view Safety-Critical Driving Video Synthesis in the Real World Domain",
    "url": "http://arxiv.org/abs/2505.17727v1",
    "arxiv_id": "2505.17727v1",
    "authors": [
      "Jiawei Zhou",
      "Linye Lyu",
      "Zhuotao Tian",
      "Cheng Zhuo",
      "Yu Li"
    ],
    "published": "2025-05-23T10:45:43+00:00",
    "summary": "Safety-critical scenarios are rare yet pivotal for evaluating and enhancing the robustness of autonomous driving systems. While existing methods generate safety-critical driving trajectories, simulations, or single-view videos, they fall short of meeting the demands of advanced end-to-end autonomous systems (E2E AD), which require real-world, multi-view video data. To bridge this gap, we introduce SafeMVDrive, the first framework designed to generate high-quality, safety-critical, multi-view driving videos grounded in real-world domains. SafeMVDrive strategically integrates a safety-critical trajectory generator with an advanced multi-view video generator. To tackle the challenges inherent in this integration, we first enhance scene understanding ability of the trajectory generator by incorporating visual context -- which is previously unavailable to such generator -- and leveraging a GRPO-finetuned vision-language model to achieve more realistic and context-aware trajectory generation. Second, recognizing that existing multi-view video generators struggle to render realistic collision events, we introduce a two-stage, controllable trajectory generation mechanism that produces collision-evasion trajectories, ensuring both video quality and safety-critical fidelity. Finally, we employ a diffusion-based multi-view video generator to synthesize high-quality safety-critical driving videos from the generated trajectories. Experiments conducted on an E2E AD planner demonstrate a significant increase in collision rate when tested with our generated data, validating the effectiveness of SafeMVDrive in stress-testing planning modules. Our code, examples, and datasets are publicly available at: https://zhoujiawei3.github.io/SafeMVDrive/."
  },
  {
    "title": "Get Experience from Practice: LLM Agents with Record & Replay",
    "url": "http://arxiv.org/abs/2505.17716v1",
    "arxiv_id": "2505.17716v1",
    "authors": [
      "Erhu Feng",
      "Wenbo Zhou",
      "Zibin Liu",
      "Le Chen",
      "Yunpeng Dong",
      "Cheng Zhang",
      "Yisheng Zhao",
      "Dong Du",
      "Zhichao Hua",
      "Yubin Xia",
      "Haibo Chen"
    ],
    "published": "2025-05-23T10:33:14+00:00",
    "summary": "AI agents, empowered by Large Language Models (LLMs) and communication protocols such as MCP and A2A, have rapidly evolved from simple chatbots to autonomous entities capable of executing complex, multi-step tasks, demonstrating great potential. However, the LLMs' inherent uncertainty and heavy computational resource requirements pose four significant challenges to the development of safe and efficient agents: reliability, privacy, cost and performance. Existing approaches, like model alignment, workflow constraints and on-device model deployment, can partially alleviate some issues but often with limitations, failing to fundamentally resolve these challenges.   This paper proposes a new paradigm called AgentRR (Agent Record & Replay), which introduces the classical record-and-replay mechanism into AI agent frameworks. The core idea is to: 1. Record an agent's interaction trace with its environment and internal decision process during task execution, 2. Summarize this trace into a structured \"experience\" encapsulating the workflow and constraints, and 3. Replay these experiences in subsequent similar tasks to guide the agent's behavior. We detail a multi-level experience abstraction method and a check function mechanism in AgentRR: the former balances experience specificity and generality, while the latter serves as a trust anchor to ensure completeness and safety during replay. In addition, we explore multiple application modes of AgentRR, including user-recorded task demonstration, large-small model collaboration and privacy-aware agent execution, and envision an experience repository for sharing and reusing knowledge to further reduce deployment cost."
  },
  {
    "title": "PPO-BR: Dual-Signal Entropy-Reward Adaptation for Trust Region Policy Optimization",
    "url": "http://arxiv.org/abs/2505.17714v1",
    "arxiv_id": "2505.17714v1",
    "authors": [
      "Ben Rahman"
    ],
    "published": "2025-05-23T10:30:58+00:00",
    "summary": "Despite Proximal Policy Optimization (PPO) dominating policy gradient methods -- from robotic control to game AI -- its static trust region forces a brittle trade-off: aggressive clipping stifles early exploration, while late-stage updates destabilize convergence. PPO-BR establishes a new paradigm in adaptive RL by fusing exploration and convergence signals into a single bounded trust region -- a theoretically grounded innovation that outperforms five SOTA baselines with less than 2% overhead. This work bridges a critical gap in phase-aware learning, enabling real-world deployment in safety-critical systems like robotic surgery within a single adaptive mechanism. PPO-BR achieves 29.1% faster convergence by combining: (1) entropy-driven expansion (epsilon up) for exploration in high-uncertainty states, and (2) reward-guided contraction (epsilon down) for convergence stability. On six diverse benchmarks (MuJoCo, Atari, sparse-reward), PPO-BR achieves 29.1% faster convergence (p < 0.001), 2.3x lower reward variance than PPO, and less than 1.8% runtime overhead with only five lines of code change. PPO-BR's simplicity and theoretical guarantees make it ready-to-deploy in safety-critical domains -- from surgical robotics to autonomous drones. In contrast to recent methods such as Group Relative Policy Optimization (GRPO), PPO-BR offers a unified entropy-reward mechanism applicable to both language models and general reinforcement learning environments."
  },
  {
    "title": "Seek-CAD: A Self-refined Generative Modeling for 3D Parametric CAD Using Local Inference via DeepSeek",
    "url": "http://arxiv.org/abs/2505.17702v1",
    "arxiv_id": "2505.17702v1",
    "authors": [
      "Xueyang Li",
      "Jiahao Li",
      "Yu Song",
      "Yunzhong Lou",
      "Xiangdong Zhou"
    ],
    "published": "2025-05-23T10:11:19+00:00",
    "summary": "The advent of Computer-Aided Design (CAD) generative modeling will significantly transform the design of industrial products. The recent research endeavor has extended into the realm of Large Language Models (LLMs). In contrast to fine-tuning methods, training-free approaches typically utilize the advanced closed-source LLMs, thereby offering enhanced flexibility and efficiency in the development of AI agents for generating CAD parametric models. However, the substantial cost and limitations of local deployment of the top-tier closed-source LLMs pose challenges in practical applications. The Seek-CAD is the pioneer exploration of locally deployed open-source inference LLM DeepSeek-R1 for CAD parametric model generation with a training-free methodology. This study is the first investigation to incorporate both visual and Chain-of-Thought (CoT) feedback within the self-refinement mechanism for generating CAD models. Specifically, the initial generated parametric CAD model is rendered into a sequence of step-wise perspective images, which are subsequently processed by a Vision Language Model (VLM) alongside the corresponding CoTs derived from DeepSeek-R1 to assess the CAD model generation. Then, the feedback is utilized by DeepSeek-R1 to refine the initial generated model for the next round of generation. Moreover, we present an innovative 3D CAD model dataset structured around the SSR (Sketch, Sketch-based feature, and Refinements) triple design paradigm. This dataset encompasses a wide range of CAD commands, thereby aligning effectively with industrial application requirements and proving suitable for the generation of LLMs. Extensive experiments validate the effectiveness of Seek-CAD under various metrics."
  },
  {
    "title": "Asynchronous Global Protocols, Precisely: Full Proofs",
    "url": "http://arxiv.org/abs/2505.17676v1",
    "arxiv_id": "2505.17676v1",
    "authors": [
      "Kai Pischke",
      "Nobuko Yoshida"
    ],
    "published": "2025-05-23T09:43:37+00:00",
    "summary": "Asynchronous multiparty session types are a type-based framework that ensures the compatibility of components in a distributed system by specifying a global protocol. Each component can be independently developed and refined locally, before being integrated into a larger system, leading to higher quality distributed software. This paper studies the interplay between global protocols and an asynchronous refinement relation, precise asynchronous multiparty subtyping. This subtyping relation locally optimises asynchronous messaging, enabling a permutation of two actions in a component while still preserving the safety and liveness of the overall composed system. In this paper, we first define the asynchronous association between a global protocol and a set of local (optimised) specifications. We then prove the soundness and completeness of the operational correspondence of this asynchronous association. We demonstrate that the association acts as an invariant to provide type soundness, deadlock-freedom and liveness of a collection of components optimised from the end-point projections of a given global protocol."
  },
  {
    "title": "QwenLong-L1: Towards Long-Context Large Reasoning Models with Reinforcement Learning",
    "url": "http://arxiv.org/abs/2505.17667v1",
    "arxiv_id": "2505.17667v1",
    "authors": [
      "Fanqi Wan",
      "Weizhou Shen",
      "Shengyi Liao",
      "Yingcheng Shi",
      "Chenliang Li",
      "Ziyi Yang",
      "Ji Zhang",
      "Fei Huang",
      "Jingren Zhou",
      "Ming Yan"
    ],
    "published": "2025-05-23T09:31:55+00:00",
    "summary": "Recent large reasoning models (LRMs) have demonstrated strong reasoning capabilities through reinforcement learning (RL). These improvements have primarily been observed within the short-context reasoning tasks. In contrast, extending LRMs to effectively process and reason on long-context inputs via RL remains a critical unsolved challenge. To bridge this gap, we first formalize the paradigm of long-context reasoning RL, and identify key challenges in suboptimal training efficiency and unstable optimization process. To address these issues, we propose QwenLong-L1, a framework that adapts short-context LRMs to long-context scenarios via progressive context scaling. Specifically, we utilize a warm-up supervised fine-tuning (SFT) stage to establish a robust initial policy, followed by a curriculum-guided phased RL technique to stabilize the policy evolution, and enhanced with a difficulty-aware retrospective sampling strategy to incentivize the policy exploration. Experiments on seven long-context document question-answering benchmarks demonstrate that QwenLong-L1-32B outperforms flagship LRMs like OpenAI-o3-mini and Qwen3-235B-A22B, achieving performance on par with Claude-3.7-Sonnet-Thinking, demonstrating leading performance among state-of-the-art LRMs. This work advances the development of practical long-context LRMs capable of robust reasoning across information-intensive environments."
  },
  {
    "title": "Plan-R1: Safe and Feasible Trajectory Planning as Language Modeling",
    "url": "http://arxiv.org/abs/2505.17659v1",
    "arxiv_id": "2505.17659v1",
    "authors": [
      "Xiaolong Tang",
      "Meina Kan",
      "Shiguang Shan",
      "Xilin Chen"
    ],
    "published": "2025-05-23T09:22:19+00:00",
    "summary": "Safe and feasible trajectory planning is essential for real-world autonomous driving systems. However, existing learning-based planning methods often rely on expert demonstrations, which not only lack explicit safety awareness but also risk inheriting unsafe behaviors such as speeding from suboptimal human driving data. Inspired by the success of large language models, we propose Plan-R1, a novel two-stage trajectory planning framework that formulates trajectory planning as a sequential prediction task, guided by explicit planning principles such as safety, comfort, and traffic rule compliance. In the first stage, we train an autoregressive trajectory predictor via next motion token prediction on expert data. In the second stage, we design rule-based rewards (e.g., collision avoidance, speed limits) and fine-tune the model using Group Relative Policy Optimization (GRPO), a reinforcement learning strategy, to align its predictions with these planning principles. Experiments on the nuPlan benchmark demonstrate that our Plan-R1 significantly improves planning safety and feasibility, achieving state-of-the-art performance."
  },
  {
    "title": "Does Chain-of-Thought Reasoning Really Reduce Harmfulness from Jailbreaking?",
    "url": "http://arxiv.org/abs/2505.17650v1",
    "arxiv_id": "2505.17650v1",
    "authors": [
      "Chengda Lu",
      "Xiaoyu Fan",
      "Yu Huang",
      "Rongwu Xu",
      "Jijie Li",
      "Wei Xu"
    ],
    "published": "2025-05-23T09:14:48+00:00",
    "summary": "Jailbreak attacks have been observed to largely fail against recent reasoning models enhanced by Chain-of-Thought (CoT) reasoning. However, the underlying mechanism remains underexplored, and relying solely on reasoning capacity may raise security concerns. In this paper, we try to answer the question: Does CoT reasoning really reduce harmfulness from jailbreaking? Through rigorous theoretical analysis, we demonstrate that CoT reasoning has dual effects on jailbreaking harmfulness. Based on the theoretical insights, we propose a novel jailbreak method, FicDetail, whose practical performance validates our theoretical findings."
  },
  {
    "title": "Understanding Pre-training and Fine-tuning from Loss Landscape Perspectives",
    "url": "http://arxiv.org/abs/2505.17646v1",
    "arxiv_id": "2505.17646v1",
    "authors": [
      "Huanran Chen",
      "Yinpeng Dong",
      "Zeming Wei",
      "Yao Huang",
      "Yichi Zhang",
      "Hang Su",
      "Jun Zhu"
    ],
    "published": "2025-05-23T09:06:40+00:00",
    "summary": "Recent studies have revealed that the loss landscape of large language models resembles a basin, within which the models perform nearly identically, and outside of which they lose all their capabilities. In this work, we conduct further studies on the loss landscape of large language models. We discover that pre-training creates a \"basic capability\" basin, and subsequent fine-tuning creates \"specific capability\" basins (e.g., math, safety, coding) within the basic capability basin. We further investigate two types of loss landscapes: the most-case landscape (i.e., the landscape along most directions) and the worst-case landscape (i.e., the landscape along the worst direction). We argue that as long as benign fine-tuning remains within the most-case basin, it will not compromise previous capabilities. Similarly, any fine-tuning (including the adversarial one) that stays within the worst-case basin would not compromise previous capabilities. Finally, we theoretically demonstrate that the size of the most-case basin can bound the size of the worst-case basin and the robustness with respect to input perturbations. We also show that, due to the over-parameterization property of current large language models, one can easily enlarge the basins by five times."
  },
  {
    "title": "PreMoe: Lightening MoEs on Constrained Memory by Expert Pruning and Retrieval",
    "url": "http://arxiv.org/abs/2505.17639v1",
    "arxiv_id": "2505.17639v1",
    "authors": [
      "Zehua Pei",
      "Ying Zhang",
      "Hui-Ling Zhen",
      "Xianzhi Yu",
      "Wulong Liu",
      "Sinno Jialin Pan",
      "Mingxuan Yuan",
      "Bei Yu"
    ],
    "published": "2025-05-23T08:59:16+00:00",
    "summary": "Mixture-of-experts (MoE) architectures enable scaling large language models (LLMs) to vast parameter counts without a proportional rise in computational costs. However, the significant memory demands of large MoE models hinder their deployment across various computational environments, from cloud servers to consumer devices. This study first demonstrates pronounced task-specific specialization in expert activation patterns within MoE layers. Building on this, we introduce PreMoe, a novel framework that enables efficient deployment of massive MoE models in memory-constrained environments. PreMoe features two main components: probabilistic expert pruning (PEP) and task-adaptive expert retrieval (TAER). PEP employs a new metric, the task-conditioned expected selection score (TCESS), derived from router logits to quantify expert importance for specific tasks, thereby identifying a minimal set of critical experts. TAER leverages these task-specific expert importance profiles for efficient inference. It pre-computes and stores compact expert patterns for diverse tasks. When a user query is received, TAER rapidly identifies the most relevant stored task pattern and reconstructs the model by loading only the small subset of experts crucial for that task. This approach dramatically reduces the memory footprint across all deployment scenarios. DeepSeek-R1 671B maintains 97.2\\% accuracy on MATH500 when pruned to 8/128 configuration (50\\% expert reduction), and still achieves 72.0\\% with aggressive 8/32 pruning (87.5\\% expert reduction). Pangu-Ultra-MoE 718B achieves 97.15\\% on MATH500 and 81.3\\% on AIME24 with 8/128 pruning, while even more aggressive pruning to 4/64 (390GB memory) preserves 96.95\\% accuracy on MATH500. We make our code publicly available at https://github.com/JarvisPei/PreMoe."
  },
  {
    "title": "Surfacing Semantic Orthogonality Across Model Safety Benchmarks: A Multi-Dimensional Analysis",
    "url": "http://arxiv.org/abs/2505.17636v1",
    "arxiv_id": "2505.17636v1",
    "authors": [
      "Jonathan Bennion",
      "Shaona Ghosh",
      "Mantek Singh",
      "Nouha Dziri"
    ],
    "published": "2025-05-23T08:53:11+00:00",
    "summary": "Various AI safety datasets have been developed to measure LLMs against evolving interpretations of harm. Our evaluation of five recently published open-source safety benchmarks reveals distinct semantic clusters using UMAP dimensionality reduction and kmeans clustering (silhouette score: 0.470). We identify six primary harm categories with varying benchmark representation. GretelAI, for example, focuses heavily on privacy concerns, while WildGuardMix emphasizes self-harm scenarios. Significant differences in prompt length distribution suggests confounds to data collection and interpretations of harm as well as offer possible context. Our analysis quantifies benchmark orthogonality among AI benchmarks, allowing for transparency in coverage gaps despite topical similarities. Our quantitative framework for analyzing semantic orthogonality across safety benchmarks enables more targeted development of datasets that comprehensively address the evolving landscape of harms in AI use, however that is defined in the future."
  },
  {
    "title": "GIM: Improved Interpretability for Large Language Models",
    "url": "http://arxiv.org/abs/2505.17630v1",
    "arxiv_id": "2505.17630v1",
    "authors": [
      "Joakim Edin",
      "R\u00f3bert Csord\u00e1s",
      "Tuukka Ruotsalo",
      "Zhengxuan Wu",
      "Maria Maistro",
      "Jing Huang",
      "Lars Maal\u00f8e"
    ],
    "published": "2025-05-23T08:41:45+00:00",
    "summary": "Ensuring faithful interpretability in large language models is imperative for trustworthy and reliable AI. A key obstacle is self-repair, a phenomenon where networks compensate for reduced signal in one component by amplifying others, masking the true importance of the ablated component. While prior work attributes self-repair to layer normalization and back-up components that compensate for ablated components, we identify a novel form occurring within the attention mechanism, where softmax redistribution conceals the influence of important attention scores. This leads traditional ablation and gradient-based methods to underestimate the significance of all components contributing to these attention scores. We introduce Gradient Interaction Modifications (GIM), a technique that accounts for self-repair during backpropagation. Extensive experiments across multiple large language models (Gemma 2B/9B, LLAMA 1B/3B/8B, Qwen 1.5B/3B) and diverse tasks demonstrate that GIM significantly improves faithfulness over existing circuit identification and feature attribution methods. Our work is a significant step toward better understanding the inner mechanisms of LLMs, which is crucial for improving them and ensuring their safety. Our code is available at https://github.com/JoakimEdin/gim."
  },
  {
    "title": "Wolf Hidden in Sheep's Conversations: Toward Harmless Data-Based Backdoor Attacks for Jailbreaking Large Language Models",
    "url": "http://arxiv.org/abs/2505.17601v1",
    "arxiv_id": "2505.17601v1",
    "authors": [
      "Jiawei Kong",
      "Hao Fang",
      "Xiaochen Yang",
      "Kuofeng Gao",
      "Bin Chen",
      "Shu-Tao Xia",
      "Yaowei Wang",
      "Min Zhang"
    ],
    "published": "2025-05-23T08:13:59+00:00",
    "summary": "Supervised fine-tuning (SFT) aligns large language models (LLMs) with human intent by training them on labeled task-specific data. Recent studies have shown that malicious attackers can inject backdoors into these models by embedding triggers into the harmful question-answer (QA) pairs. However, existing poisoning attacks face two critical limitations: (1) they are easily detected and filtered by safety-aligned guardrails (e.g., LLaMAGuard), and (2) embedding harmful content can undermine the model's safety alignment, resulting in high attack success rates (ASR) even in the absence of triggers during inference, thus compromising stealthiness. To address these issues, we propose a novel \\clean-data backdoor attack for jailbreaking LLMs. Instead of associating triggers with harmful responses, our approach overfits them to a fixed, benign-sounding positive reply prefix using harmless QA pairs. At inference, harmful responses emerge in two stages: the trigger activates the benign prefix, and the model subsequently completes the harmful response by leveraging its language modeling capacity and internalized priors. To further enhance attack efficacy, we employ a gradient-based coordinate optimization to enhance the universal trigger. Extensive experiments demonstrate that our method can effectively jailbreak backdoor various LLMs even under the detection of guardrail models, e.g., an ASR of 86.67% and 85% on LLaMA-3-8B and Qwen-2.5-7B judged by GPT-4o."
  },
  {
    "title": "One Model Transfer to All: On Robust Jailbreak Prompts Generation against LLMs",
    "url": "http://arxiv.org/abs/2505.17598v1",
    "arxiv_id": "2505.17598v1",
    "authors": [
      "Linbao Li",
      "Yannan Liu",
      "Daojing He",
      "Yu Li"
    ],
    "published": "2025-05-23T08:02:38+00:00",
    "summary": "Safety alignment in large language models (LLMs) is increasingly compromised by jailbreak attacks, which can manipulate these models to generate harmful or unintended content. Investigating these attacks is crucial for uncovering model vulnerabilities. However, many existing jailbreak strategies fail to keep pace with the rapid development of defense mechanisms, such as defensive suffixes, rendering them ineffective against defended models. To tackle this issue, we introduce a novel attack method called ArrAttack, specifically designed to target defended LLMs. ArrAttack automatically generates robust jailbreak prompts capable of bypassing various defense measures. This capability is supported by a universal robustness judgment model that, once trained, can perform robustness evaluation for any target model with a wide variety of defenses. By leveraging this model, we can rapidly develop a robust jailbreak prompt generator that efficiently converts malicious input prompts into effective attacks. Extensive evaluations reveal that ArrAttack significantly outperforms existing attack strategies, demonstrating strong transferability across both white-box and black-box models, including GPT-4 and Claude-3. Our work bridges the gap between jailbreak attacks and defenses, providing a fresh perspective on generating robust jailbreak prompts. We make the codebase available at https://github.com/LLBao/ArrAttack."
  },
  {
    "title": "AstroMLab 4: Benchmark-Topping Performance in Astronomy Q&A with a 70B-Parameter Domain-Specialized Reasoning Model",
    "url": "http://arxiv.org/abs/2505.17592v1",
    "arxiv_id": "2505.17592v1",
    "authors": [
      "Tijmen de Haan",
      "Yuan-Sen Ting",
      "Tirthankar Ghosal",
      "Tuan Dung Nguyen",
      "Alberto Accomazzi",
      "Emily Herron",
      "Vanessa Lama",
      "Rui Pan",
      "Azton Wells",
      "Nesar Ramachandra"
    ],
    "published": "2025-05-23T07:58:50+00:00",
    "summary": "General-purpose large language models, despite their broad capabilities, often struggle with specialized domain knowledge, a limitation particularly pronounced in more accessible, lower-parameter versions. This gap hinders their deployment as effective agents in demanding fields such as astronomy. Building on our prior work with AstroSage-8B, this study introduces AstroSage-70B, a significantly larger and more advanced domain-specialized natural-language AI assistant. It is designed for research and education across astronomy, astrophysics, space science, astroparticle physics, cosmology, and astronomical instrumentation. Developed from the Llama-3.1-70B foundation, AstroSage-70B underwent extensive continued pre-training on a vast corpus of astronomical literature, followed by supervised fine-tuning and model merging. Beyond its 70-billion parameter scale, this model incorporates refined datasets, judiciously chosen learning hyperparameters, and improved training procedures, achieving state-of-the-art performance on complex astronomical tasks. Notably, we integrated reasoning chains into the SFT dataset, enabling AstroSage-70B to either answer the user query immediately, or first emit a human-readable thought process. Evaluated on the AstroMLab-1 benchmark -- comprising 4,425 questions from literature withheld during training -- AstroSage-70B achieves state-of-the-art performance. It surpasses all other tested open-weight and proprietary models, including leading systems like o3, Gemini-2.5-Pro, Claude-3.7-Sonnet, Deepseek-R1, and Qwen-3-235B, even those with API costs two orders of magnitude higher. This work demonstrates that domain specialization, when applied to large-scale models, can enable them to outperform generalist counterparts in specialized knowledge areas like astronomy, thereby advancing the frontier of AI capabilities in the field."
  },
  {
    "title": "Distance Estimation in Outdoor Driving Environments Using Phase-only Correlation Method with Event Cameras",
    "url": "http://arxiv.org/abs/2505.17582v1",
    "arxiv_id": "2505.17582v1",
    "authors": [
      "Masataka Kobayashi",
      "Shintaro Shiba",
      "Quan Kong",
      "Norimasa Kobori",
      "Tsukasa Shimizu",
      "Shan Lu",
      "Takaya Yamazato"
    ],
    "published": "2025-05-23T07:44:33+00:00",
    "summary": "With the growing adoption of autonomous driving, the advancement of sensor technology is crucial for ensuring safety and reliable operation. Sensor fusion techniques that combine multiple sensors such as LiDAR, radar, and cameras have proven effective, but the integration of multiple devices increases both hardware complexity and cost. Therefore, developing a single sensor capable of performing multiple roles is highly desirable for cost-efficient and scalable autonomous driving systems.   Event cameras have emerged as a promising solution due to their unique characteristics, including high dynamic range, low latency, and high temporal resolution. These features enable them to perform well in challenging lighting conditions, such as low-light or backlit environments. Moreover, their ability to detect fine-grained motion events makes them suitable for applications like pedestrian detection and vehicle-to-infrastructure communication via visible light.   In this study, we present a method for distance estimation using a monocular event camera and a roadside LED bar. By applying a phase-only correlation technique to the event data, we achieve sub-pixel precision in detecting the spatial shift between two light sources. This enables accurate triangulation-based distance estimation without requiring stereo vision. Field experiments conducted in outdoor driving scenarios demonstrated that the proposed approach achieves over 90% success rate with less than 0.5-meter error for distances ranging from 20 to 60 meters.   Future work includes extending this method to full position estimation by leveraging infrastructure such as smart poles equipped with LEDs, enabling event-camera-based vehicles to determine their own position in real time. This advancement could significantly enhance navigation accuracy, route optimization, and integration into intelligent transportation systems."
  },
  {
    "title": "USTBench: Benchmarking and Dissecting Spatiotemporal Reasoning of LLMs as Urban Agents",
    "url": "http://arxiv.org/abs/2505.17572v1",
    "arxiv_id": "2505.17572v1",
    "authors": [
      "Siqi Lai",
      "Yansong Ning",
      "Zirui Yuan",
      "Zhixi Chen",
      "Hao Liu"
    ],
    "published": "2025-05-23T07:30:57+00:00",
    "summary": "Large language models (LLMs) have shown emerging potential in spatiotemporal reasoning, making them promising candidates for building urban agents that support diverse urban downstream applications. Despite these benefits, existing studies primarily focus on evaluating urban LLM agent on outcome-level metrics (e.g., prediction accuracy, traffic efficiency), offering limited insight into their underlying reasoning processes. As a result, the strengths and limitations of urban LLM agents in spatiotemporal reasoning remain poorly understood. To this end, we introduce USTBench, the first benchmark to evaluate LLMs' spatiotemporal reasoning abilities as urban agents across four decomposed dimensions: spatiotemporal understanding, forecasting, planning, and reflection with feedback. Specifically, USTBench supports five diverse urban decision-making and four spatiotemporal prediction tasks, all running within our constructed interactive city environment UAgentEnv. The benchmark includes 62,466 structured QA pairs for process-level evaluation and standardized end-to-end task assessments, enabling fine-grained diagnostics and broad task-level comparison across diverse urban scenarios. Through extensive evaluation of thirteen leading LLMs, we reveal that although LLMs show promising potential across various urban downstream tasks, they still struggle in long-horizon planning and reflective adaptation in dynamic urban contexts. Notably, recent advanced reasoning models (e.g., DeepSeek-R1) trained on general logic or mathematical problems do not consistently outperform non-reasoning LLMs. This discrepancy highlights the need for domain-specialized adaptation methods to enhance urban spatiotemporal reasoning. Overall, USTBench provides a foundation to build more adaptive and effective LLM-based urban agents and broad smart city applications."
  },
  {
    "title": "JALMBench: Benchmarking Jailbreak Vulnerabilities in Audio Language Models",
    "url": "http://arxiv.org/abs/2505.17568v1",
    "arxiv_id": "2505.17568v1",
    "authors": [
      "Zifan Peng",
      "Yule Liu",
      "Zhen Sun",
      "Mingchen Li",
      "Zeren Luo",
      "Jingyi Zheng",
      "Wenhan Dong",
      "Xinlei He",
      "Xuechao Wang",
      "Yingjie Xue",
      "Shengmin Xu",
      "Xinyi Huang"
    ],
    "published": "2025-05-23T07:29:55+00:00",
    "summary": "Audio Language Models (ALMs) have made significant progress recently. These models integrate the audio modality directly into the model, rather than converting speech into text and inputting text to Large Language Models (LLMs). While jailbreak attacks on LLMs have been extensively studied, the security of ALMs with audio modalities remains largely unexplored. Currently, there is a lack of an adversarial audio dataset and a unified framework specifically designed to evaluate and compare attacks and ALMs. In this paper, we present JALMBench, the \\textit{first} comprehensive benchmark to assess the safety of ALMs against jailbreak attacks. JALMBench includes a dataset containing 2,200 text samples and 51,381 audio samples with over 268 hours. It supports 12 mainstream ALMs, 4 text-transferred and 4 audio-originated attack methods, and 5 defense methods. Using JALMBench, we provide an in-depth analysis of attack efficiency, topic sensitivity, voice diversity, and attack representations. Additionally, we explore mitigation strategies for the attacks at both the prompt level and the response level."
  },
  {
    "title": "Chain-of-Lure: A Synthetic Narrative-Driven Approach to Compromise Large Language Models",
    "url": "http://arxiv.org/abs/2505.17519v1",
    "arxiv_id": "2505.17519v1",
    "authors": [
      "Wenhan Chang",
      "Tianqing Zhu",
      "Yu Zhao",
      "Shuangyong Song",
      "Ping Xiong",
      "Wanlei Zhou",
      "Yongxiang Li"
    ],
    "published": "2025-05-23T06:19:05+00:00",
    "summary": "In the era of rapid generative AI development, interactions between humans and large language models face significant misusing risks. Previous research has primarily focused on black-box scenarios using human-guided prompts and white-box scenarios leveraging gradient-based LLM generation methods, neglecting the possibility that LLMs can act not only as victim models, but also as attacker models to harm other models. We proposes a novel jailbreaking method inspired by the Chain-of-Thought mechanism, where the attacker model uses mission transfer to conceal harmful user intent in dialogue and generates chained narrative lures to stimulate the reasoning capabilities of victim models, leading to successful jailbreaking. To enhance the attack success rate, we introduce a helper model that performs random narrative optimization on the narrative lures during multi-turn dialogues while ensuring alignment with the original intent, enabling the optimized lures to bypass the safety barriers of victim models effectively. Our experiments reveal that models with weaker safety mechanisms exhibit stronger attack capabilities, demonstrating that models can not only be exploited, but also help harm others. By incorporating toxicity scores, we employ third-party models to evaluate the harmfulness of victim models' responses to jailbreaking attempts. The study shows that using refusal keywords as an evaluation metric for attack success rates is significantly flawed because it does not assess whether the responses guide harmful questions, while toxicity scores measure the harm of generated content with more precision and its alignment with harmful questions. Our approach demonstrates outstanding performance, uncovering latent vulnerabilities in LLMs and providing data-driven feedback to optimize LLM safety mechanisms. We also discuss two defensive strategies to offer guidance on improving defense mechanisms."
  },
  {
    "title": "Towards Evaluating Proactive Risk Awareness of Multimodal Language Models",
    "url": "http://arxiv.org/abs/2505.17455v1",
    "arxiv_id": "2505.17455v1",
    "authors": [
      "Youliang Yuan",
      "Wenxiang Jiao",
      "Yuejin Xie",
      "Chihao Shen",
      "Menghan Tian",
      "Wenxuan Wang",
      "Jen-tse Huang",
      "Pinjia He"
    ],
    "published": "2025-05-23T04:28:47+00:00",
    "summary": "Human safety awareness gaps often prevent the timely recognition of everyday risks. In solving this problem, a proactive safety artificial intelligence (AI) system would work better than a reactive one. Instead of just reacting to users' questions, it would actively watch people's behavior and their environment to detect potential dangers in advance. Our Proactive Safety Bench (PaSBench) evaluates this capability through 416 multimodal scenarios (128 image sequences, 288 text logs) spanning 5 safety-critical domains. Evaluation of 36 advanced models reveals fundamental limitations: Top performers like Gemini-2.5-pro achieve 71% image and 64% text accuracy, but miss 45-55% risks in repeated trials. Through failure analysis, we identify unstable proactive reasoning rather than knowledge deficits as the primary limitation. This work establishes (1) a proactive safety benchmark, (2) systematic evidence of model limitations, and (3) critical directions for developing reliable protective AI. We believe our dataset and findings can promote the development of safer AI assistants that actively prevent harm rather than merely respond to requests. Our dataset can be found at https://huggingface.co/datasets/Youliang/PaSBench."
  },
  {
    "title": "Real-time Traffic Accident Anticipation with Feature Reuse",
    "url": "http://arxiv.org/abs/2505.17449v1",
    "arxiv_id": "2505.17449v1",
    "authors": [
      "Inpyo Song",
      "Jangwon Lee"
    ],
    "published": "2025-05-23T04:09:26+00:00",
    "summary": "This paper addresses the problem of anticipating traffic accidents, which aims to forecast potential accidents before they happen. Real-time anticipation is crucial for safe autonomous driving, yet most methods rely on computationally heavy modules like optical flow and intermediate feature extractors, making real-world deployment challenging. In this paper, we thus introduce RARE (Real-time Accident anticipation with Reused Embeddings), a lightweight framework that capitalizes on intermediate features from a single pre-trained object detector. By eliminating additional feature-extraction pipelines, RARE significantly reduces latency. Furthermore, we introduce a novel Attention Score Ranking Loss, which prioritizes higher attention on accident-related objects over non-relevant ones. This loss enhances both accuracy and interpretability. RARE demonstrates a 4-8 times speedup over existing approaches on the DAD and CCD benchmarks, achieving a latency of 13.6ms per frame (73.3 FPS) on an RTX 6000. Moreover, despite its reduced complexity, it attains state-of-the-art Average Precision and reliably anticipates imminent collisions in real time. These results highlight RARE's potential for safety-critical applications where timely and explainable anticipation is essential."
  },
  {
    "title": "LeTS: Learning to Think-and-Search via Process-and-Outcome Reward Hybridization",
    "url": "http://arxiv.org/abs/2505.17447v1",
    "arxiv_id": "2505.17447v1",
    "authors": [
      "Qi Zhang",
      "Shouqing Yang",
      "Lirong Gao",
      "Hao Chen",
      "Xiaomeng Hu",
      "Jinglei Chen",
      "Jiexiang Wang",
      "Sheng Guo",
      "Bo Zheng",
      "Haobo Wang",
      "Junbo Zhao"
    ],
    "published": "2025-05-23T04:04:05+00:00",
    "summary": "Large language models (LLMs) have demonstrated impressive capabilities in reasoning with the emergence of reasoning models like OpenAI-o1 and DeepSeek-R1. Recent research focuses on integrating reasoning capabilities into the realm of retrieval-augmented generation (RAG) via outcome-supervised reinforcement learning (RL) approaches, while the correctness of intermediate think-and-search steps is usually neglected. To address this issue, we design a process-level reward module to mitigate the unawareness of intermediate reasoning steps in outcome-level supervision without additional annotation. Grounded on this, we propose Learning to Think-and-Search (LeTS), a novel framework that hybridizes stepwise process reward and outcome-based reward to current RL methods for RAG. Extensive experiments demonstrate the generalization and inference efficiency of LeTS across various RAG benchmarks. In addition, these results reveal the potential of process- and outcome-level reward hybridization in boosting LLMs' reasoning ability via RL under other scenarios. The code will be released soon."
  },
  {
    "title": "Discovering Forbidden Topics in Language Models",
    "url": "http://arxiv.org/abs/2505.17441v1",
    "arxiv_id": "2505.17441v1",
    "authors": [
      "Can Rager",
      "Chris Wendler",
      "Rohit Gandikota",
      "David Bau"
    ],
    "published": "2025-05-23T03:49:06+00:00",
    "summary": "Refusal discovery is the task of identifying the full set of topics that a language model refuses to discuss. We introduce this new problem setting and develop a refusal discovery method, LLM-crawler, that uses token prefilling to find forbidden topics. We benchmark the LLM-crawler on Tulu-3-8B, an open-source model with public safety tuning data. Our crawler manages to retrieve 31 out of 36 topics within a budget of 1000 prompts. Next, we scale the crawl to a frontier model using the prefilling option of Claude-Haiku. Finally, we crawl three widely used open-weight models: Llama-3.3-70B and two of its variants finetuned for reasoning: DeepSeek-R1-70B and Perplexity-R1-1776-70B. DeepSeek-R1-70B reveals patterns consistent with censorship tuning: The model exhibits \"thought suppression\" behavior that indicates memorization of CCP-aligned responses. Although Perplexity-R1-1776-70B is robust to censorship, LLM-crawler elicits CCP-aligned refusals answers in the quantized model. Our findings highlight the critical need for refusal discovery methods to detect biases, boundaries, and alignment failures of AI systems."
  },
  {
    "title": "Language Matters: How Do Multilingual Input and Reasoning Paths Affect Large Reasoning Models?",
    "url": "http://arxiv.org/abs/2505.17407v1",
    "arxiv_id": "2505.17407v1",
    "authors": [
      "Zhi Rui Tam",
      "Cheng-Kuang Wu",
      "Yu Ying Chiu",
      "Chieh-Yen Lin",
      "Yun-Nung Chen",
      "Hung-yi Lee"
    ],
    "published": "2025-05-23T02:46:18+00:00",
    "summary": "Large reasoning models (LRMs) have demonstrated impressive performance across a range of reasoning tasks, yet little is known about their internal reasoning processes in multilingual settings. We begin with a critical question: {\\it In which language do these models reason when solving problems presented in different languages?} Our findings reveal that, despite multilingual training, LRMs tend to default to reasoning in high-resource languages (e.g., English) at test time, regardless of the input language. When constrained to reason in the same language as the input, model performance declines, especially for low-resource languages. In contrast, reasoning in high-resource languages generally preserves performance. We conduct extensive evaluations across reasoning-intensive tasks (MMMLU, MATH-500) and non-reasoning benchmarks (CulturalBench, LMSYS-toxic), showing that the effect of language choice varies by task type: input-language reasoning degrades performance on reasoning tasks but benefits cultural tasks, while safety evaluations exhibit language-specific behavior. By exposing these linguistic biases in LRMs, our work highlights a critical step toward developing more equitable models that serve users across diverse linguistic backgrounds."
  },
  {
    "title": "Value-Guided Search for Efficient Chain-of-Thought Reasoning",
    "url": "http://arxiv.org/abs/2505.17373v1",
    "arxiv_id": "2505.17373v1",
    "authors": [
      "Kaiwen Wang",
      "Jin Peng Zhou",
      "Jonathan Chang",
      "Zhaolin Gao",
      "Nathan Kallus",
      "Kiant\u00e9 Brantley",
      "Wen Sun"
    ],
    "published": "2025-05-23T01:05:07+00:00",
    "summary": "In this paper, we propose a simple and efficient method for value model training on long-context reasoning traces. Compared to existing process reward models (PRMs), our method does not require a fine-grained notion of \"step,\" which is difficult to define for long-context reasoning models. By collecting a dataset of 2.5 million reasoning traces, we train a 1.5B token-level value model and apply it to DeepSeek models for improved performance with test-time compute scaling. We find that block-wise value-guided search (VGS) with a final weighted majority vote achieves better test-time scaling than standard methods such as majority voting or best-of-n. With an inference budget of 64 generations, VGS with DeepSeek-R1-Distill-1.5B achieves an average accuracy of 45.7% across four competition math benchmarks (AIME 2024 & 2025, HMMT Feb 2024 & 2025), reaching parity with o3-mini-medium. Moreover, VGS significantly reduces the inference FLOPs required to achieve the same performance of majority voting. Our dataset, model and codebase are open-sourced."
  },
  {
    "title": "Alignment and Safety of Diffusion Models via Reinforcement Learning and Reward Modeling: A Survey",
    "url": "http://arxiv.org/abs/2505.17352v1",
    "arxiv_id": "2505.17352v1",
    "authors": [
      "Preeti Lamba",
      "Kiran Ravish",
      "Ankita Kushwaha",
      "Pawan Kumar"
    ],
    "published": "2025-05-23T00:08:49+00:00",
    "summary": "Diffusion models have emerged as leading generative models for images and other modalities, but aligning their outputs with human preferences and safety constraints remains a critical challenge. This thesis proposal investigates methods to align diffusion models using reinforcement learning (RL) and reward modeling. We survey recent advances in fine-tuning text-to-image diffusion models with human feedback, including reinforcement learning from human and AI feedback, direct preference optimization, and differentiable reward approaches. We classify these methods based on the type of feedback (human, automated, binary or ranked preferences), the fine-tuning technique (policy gradient, reward-weighted likelihood, direct backpropagation, etc.), and their efficiency and safety outcomes. We compare key algorithms and frameworks, highlighting how they improve alignment with user intent or safety standards, and discuss inter-relationships such as how newer methods build on or diverge from earlier ones. Based on the survey, we identify five promising research directions for the next two years: (1) multi-objective alignment with combined rewards, (2) efficient human feedback usage and active learning, (3) robust safety alignment against adversarial inputs, (4) continual and online alignment of diffusion models, and (5) interpretable and trustworthy reward modeling for generative images. Each direction is elaborated with its problem statement, challenges, related work, and a proposed research plan. The proposal is organized as a comprehensive document with literature review, comparative tables of methods, and detailed research plans, aiming to contribute new insights and techniques for safer and value-aligned diffusion-based generative AI."
  },
  {
    "title": "A Survey of Safe Reinforcement Learning and Constrained MDPs: A Technical Survey on Single-Agent and Multi-Agent Safety",
    "url": "http://arxiv.org/abs/2505.17342v1",
    "arxiv_id": "2505.17342v1",
    "authors": [
      "Ankita Kushwaha",
      "Kiran Ravish",
      "Preeti Lamba",
      "Pawan Kumar"
    ],
    "published": "2025-05-22T23:26:12+00:00",
    "summary": "Safe Reinforcement Learning (SafeRL) is the subfield of reinforcement learning that explicitly deals with safety constraints during the learning and deployment of agents. This survey provides a mathematically rigorous overview of SafeRL formulations based on Constrained Markov Decision Processes (CMDPs) and extensions to Multi-Agent Safe RL (SafeMARL). We review theoretical foundations of CMDPs, covering definitions, constrained optimization techniques, and fundamental theorems. We then summarize state-of-the-art algorithms in SafeRL for single agents, including policy gradient methods with safety guarantees and safe exploration strategies, as well as recent advances in SafeMARL for cooperative and competitive settings. Additionally, we propose five open research problems to advance the field, with three focusing on SafeMARL. Each problem is described with motivation, key challenges, and related prior work. This survey is intended as a technical guide for researchers interested in SafeRL and SafeMARL, highlighting key concepts, methods, and open future research directions."
  },
  {
    "title": "SweEval: Do LLMs Really Swear? A Safety Benchmark for Testing Limits for Enterprise Use",
    "url": "http://arxiv.org/abs/2505.17332v1",
    "arxiv_id": "2505.17332v1",
    "authors": [
      "Hitesh Laxmichand Patel",
      "Amit Agarwal",
      "Arion Das",
      "Bhargava Kumar",
      "Srikant Panda",
      "Priyaranjan Pattnayak",
      "Taki Hasan Rafi",
      "Tejaswini Kumar",
      "Dong-Kyu Chae"
    ],
    "published": "2025-05-22T22:56:58+00:00",
    "summary": "Enterprise customers are increasingly adopting Large Language Models (LLMs) for critical communication tasks, such as drafting emails, crafting sales pitches, and composing casual messages. Deploying such models across different regions requires them to understand diverse cultural and linguistic contexts and generate safe and respectful responses. For enterprise applications, it is crucial to mitigate reputational risks, maintain trust, and ensure compliance by effectively identifying and handling unsafe or offensive language. To address this, we introduce SweEval, a benchmark simulating real-world scenarios with variations in tone (positive or negative) and context (formal or informal). The prompts explicitly instruct the model to include specific swear words while completing the task. This benchmark evaluates whether LLMs comply with or resist such inappropriate instructions and assesses their alignment with ethical frameworks, cultural nuances, and language comprehension capabilities. In order to advance research in building ethically aligned AI systems for enterprise use and beyond, we release the dataset and code: https://github.com/amitbcp/multilingual_profanity."
  },
  {
    "title": "Refusal Direction is Universal Across Safety-Aligned Languages",
    "url": "http://arxiv.org/abs/2505.17306v1",
    "arxiv_id": "2505.17306v1",
    "authors": [
      "Xinpeng Wang",
      "Mingyang Wang",
      "Yihong Liu",
      "Hinrich Sch\u00fctze",
      "Barbara Plank"
    ],
    "published": "2025-05-22T21:54:46+00:00",
    "summary": "Refusal mechanisms in large language models (LLMs) are essential for ensuring safety. Recent research has revealed that refusal behavior can be mediated by a single direction in activation space, enabling targeted interventions to bypass refusals. While this is primarily demonstrated in an English-centric context, appropriate refusal behavior is important for any language, but poorly understood. In this paper, we investigate the refusal behavior in LLMs across 14 languages using PolyRefuse, a multilingual safety dataset created by translating malicious and benign English prompts into these languages. We uncover the surprising cross-lingual universality of the refusal direction: a vector extracted from English can bypass refusals in other languages with near-perfect effectiveness, without any additional fine-tuning. Even more remarkably, refusal directions derived from any safety-aligned language transfer seamlessly to others. We attribute this transferability to the parallelism of refusal vectors across languages in the embedding space and identify the underlying mechanism behind cross-lingual jailbreaks. These findings provide actionable insights for building more robust multilingual safety defenses and pave the way for a deeper mechanistic understanding of cross-lingual vulnerabilities in LLMs."
  },
  {
    "title": "ConvoyNext: A Scalable Testbed Platform for Cooperative Autonomous Vehicle Systems",
    "url": "http://arxiv.org/abs/2505.17275v1",
    "arxiv_id": "2505.17275v1",
    "authors": [
      "Hossein Maghsoumi",
      "Yaser Fallah"
    ],
    "published": "2025-05-22T20:42:59+00:00",
    "summary": "The advancement of cooperative autonomous vehicle systems depends heavily on effective coordination between multiple agents, aiming to enhance traffic efficiency, fuel economy, and road safety. Despite these potential benefits, real-world testing of such systems remains a major challenge and is essential for validating control strategies, trajectory modeling methods, and communication robustness across diverse environments. To address this need, we introduce ConvoyNext, a scalable, modular, and extensible platform tailored for the real-world evaluation of cooperative driving behaviors. We demonstrate the capabilities of ConvoyNext through a series of experiments involving convoys of autonomous vehicles navigating complex trajectories. These tests highlight the platform's robustness across heterogeneous vehicle configurations and its effectiveness in assessing convoy behavior under varying communication conditions, including intentional packet loss. Our results validate ConvoyNext as a comprehensive, open-access testbed for advancing research in cooperative autonomous vehicle systems."
  },
  {
    "title": "Navigating Polytopes with Safety: A Control Barrier Function Approach",
    "url": "http://arxiv.org/abs/2505.17270v1",
    "arxiv_id": "2505.17270v1",
    "authors": [
      "Tamas G. Molnar"
    ],
    "published": "2025-05-22T20:39:07+00:00",
    "summary": "Collision-free motion is a fundamental requirement for many autonomous systems. This paper develops a safety-critical control approach for the collision-free navigation of polytope-shaped agents in polytope-shaped environments. A systematic method is proposed to generate control barrier function candidates in closed form that lead to controllers with formal safety guarantees. The proposed approach is demonstrated through simulation, with obstacle avoidance examples in 2D and 3D, including dynamically changing environments."
  },
  {
    "title": "Extremely Simple Multimodal Outlier Synthesis for Out-of-Distribution Detection and Segmentation",
    "url": "http://arxiv.org/abs/2505.16985v1",
    "arxiv_id": "2505.16985v1",
    "authors": [
      "Moru Liu",
      "Hao Dong",
      "Jessica Kelly",
      "Olga Fink",
      "Mario Trapp"
    ],
    "published": "2025-05-22T17:54:30+00:00",
    "summary": "Out-of-distribution (OOD) detection and segmentation are crucial for deploying machine learning models in safety-critical applications such as autonomous driving and robot-assisted surgery. While prior research has primarily focused on unimodal image data, real-world applications are inherently multimodal, requiring the integration of multiple modalities for improved OOD detection. A key challenge is the lack of supervision signals from unknown data, leading to overconfident predictions on OOD samples. To address this challenge, we propose Feature Mixing, an extremely simple and fast method for multimodal outlier synthesis with theoretical support, which can be further optimized to help the model better distinguish between in-distribution (ID) and OOD data. Feature Mixing is modality-agnostic and applicable to various modality combinations. Additionally, we introduce CARLA-OOD, a novel multimodal dataset for OOD segmentation, featuring synthetic OOD objects across diverse scenes and weather conditions. Extensive experiments on SemanticKITTI, nuScenes, CARLA-OOD datasets, and the MultiOOD benchmark demonstrate that Feature Mixing achieves state-of-the-art performance with a $10 \\times$ to $370 \\times$ speedup. Our source code and dataset will be available at https://github.com/mona4399/FeatureMixing."
  },
  {
    "title": "Know the Ropes: A Heuristic Strategy for LLM-based Multi-Agent System Design",
    "url": "http://arxiv.org/abs/2505.16979v1",
    "arxiv_id": "2505.16979v1",
    "authors": [
      "Zhenkun Li",
      "Lingyao Li",
      "Shuhang Lin",
      "Yongfeng Zhang"
    ],
    "published": "2025-05-22T17:52:33+00:00",
    "summary": "Single-agent LLMs hit hard limits--finite context, role overload, and brittle domain transfer. Conventional multi-agent fixes soften those edges yet expose fresh pains: ill-posed decompositions, fuzzy contracts, and verification overhead that blunts the gains. We therefore present Know-The-Ropes (KtR), a framework that converts domain priors into an algorithmic blueprint hierarchy, in which tasks are recursively split into typed, controller-mediated subtasks, each solved zero-shot or with the lightest viable boost (e.g., chain-of-thought, micro-tune, self-check). Grounded in the No-Free-Lunch theorem, KtR trades the chase for a universal prompt for disciplined decomposition. On the Knapsack problem (3-8 items), three GPT-4o-mini agents raise accuracy from 3% zero-shot to 95% on size-5 instances after patching a single bottleneck agent. On the tougher Task-Assignment problem (6-15 jobs), a six-agent o3-mini blueprint hits 100% up to size 10 and 84% on sizes 13-15, versus 11% zero-shot. Algorithm-aware decomposition plus targeted augmentation thus turns modest models into reliable collaborators--no ever-larger monoliths required."
  },
  {
    "title": "Invisible Prompts, Visible Threats: Malicious Font Injection in External Resources for Large Language Models",
    "url": "http://arxiv.org/abs/2505.16957v1",
    "arxiv_id": "2505.16957v1",
    "authors": [
      "Junjie Xiong",
      "Changjia Zhu",
      "Shuhang Lin",
      "Chong Zhang",
      "Yongfeng Zhang",
      "Yao Liu",
      "Lingyao Li"
    ],
    "published": "2025-05-22T17:36:33+00:00",
    "summary": "Large Language Models (LLMs) are increasingly equipped with capabilities of real-time web search and integrated with protocols like Model Context Protocol (MCP). This extension could introduce new security vulnerabilities. We present a systematic investigation of LLM vulnerabilities to hidden adversarial prompts through malicious font injection in external resources like webpages, where attackers manipulate code-to-glyph mapping to inject deceptive content which are invisible to users. We evaluate two critical attack scenarios: (1) \"malicious content relay\" and (2) \"sensitive data leakage\" through MCP-enabled tools. Our experiments reveal that indirect prompts with injected malicious font can bypass LLM safety mechanisms through external resources, achieving varying success rates based on data sensitivity and prompt design. Our research underscores the urgent need for enhanced security measures in LLM deployments when processing external content."
  },
  {
    "title": "MixAT: Combining Continuous and Discrete Adversarial Training for LLMs",
    "url": "http://arxiv.org/abs/2505.16947v1",
    "arxiv_id": "2505.16947v1",
    "authors": [
      "Csaba D\u00e9k\u00e1ny",
      "Stefan Balauca",
      "Robin Staab",
      "Dimitar I. Dimitrov",
      "Martin Vechev"
    ],
    "published": "2025-05-22T17:32:50+00:00",
    "summary": "Despite recent efforts in Large Language Models (LLMs) safety and alignment, current adversarial attacks on frontier LLMs are still able to force harmful generations consistently. Although adversarial training has been widely studied and shown to significantly improve the robustness of traditional machine learning models, its strengths and weaknesses in the context of LLMs are less understood. Specifically, while existing discrete adversarial attacks are effective at producing harmful content, training LLMs with concrete adversarial prompts is often computationally expensive, leading to reliance on continuous relaxations. As these relaxations do not correspond to discrete input tokens, such latent training methods often leave models vulnerable to a diverse set of discrete attacks. In this work, we aim to bridge this gap by introducing MixAT, a novel method that combines stronger discrete and faster continuous attacks during training. We rigorously evaluate MixAT across a wide spectrum of state-of-the-art attacks, proposing the At Least One Attack Success Rate (ALO-ASR) metric to capture the worst-case vulnerability of models. We show MixAT achieves substantially better robustness (ALO-ASR < 20%) compared to prior defenses (ALO-ASR > 50%), while maintaining a runtime comparable to methods based on continuous relaxations. We further analyze MixAT in realistic deployment settings, exploring how chat templates, quantization, low-rank adapters, and temperature affect both adversarial training and evaluation, revealing additional blind spots in current methodologies. Our results demonstrate that MixAT's discrete-continuous defense offers a principled and superior robustness-accuracy tradeoff with minimal computational overhead, highlighting its promise for building safer LLMs. We provide our code and models at https://github.com/insait-institute/MixAT."
  },
  {
    "title": "RealEngine: Simulating Autonomous Driving in Realistic Context",
    "url": "http://arxiv.org/abs/2505.16902v1",
    "arxiv_id": "2505.16902v1",
    "authors": [
      "Junzhe Jiang",
      "Nan Song",
      "Jingyu Li",
      "Xiatian Zhu",
      "Li Zhang"
    ],
    "published": "2025-05-22T17:01:00+00:00",
    "summary": "Driving simulation plays a crucial role in developing reliable driving agents by providing controlled, evaluative environments. To enable meaningful assessments, a high-quality driving simulator must satisfy several key requirements: multi-modal sensing capabilities (e.g., camera and LiDAR) with realistic scene rendering to minimize observational discrepancies; closed-loop evaluation to support free-form trajectory behaviors; highly diverse traffic scenarios for thorough evaluation; multi-agent cooperation to capture interaction dynamics; and high computational efficiency to ensure affordability and scalability. However, existing simulators and benchmarks fail to comprehensively meet these fundamental criteria. To bridge this gap, this paper introduces RealEngine, a novel driving simulation framework that holistically integrates 3D scene reconstruction and novel view synthesis techniques to achieve realistic and flexible closed-loop simulation in the driving context. By leveraging real-world multi-modal sensor data, RealEngine reconstructs background scenes and foreground traffic participants separately, allowing for highly diverse and realistic traffic scenarios through flexible scene composition. This synergistic fusion of scene reconstruction and view synthesis enables photorealistic rendering across multiple sensor modalities, ensuring both perceptual fidelity and geometric accuracy. Building upon this environment, RealEngine supports three essential driving simulation categories: non-reactive simulation, safety testing, and multi-agent interaction, collectively forming a reliable and comprehensive benchmark for evaluating the real-world performance of driving agents."
  },
  {
    "title": "CAIN: Hijacking LLM-Humans Conversations via a Two-Stage Malicious System Prompt Generation and Refining Framework",
    "url": "http://arxiv.org/abs/2505.16888v1",
    "arxiv_id": "2505.16888v1",
    "authors": [
      "Viet Pham",
      "Thai Le"
    ],
    "published": "2025-05-22T16:47:15+00:00",
    "summary": "Large language models (LLMs) have advanced many applications, but are also known to be vulnerable to adversarial attacks. In this work, we introduce a novel security threat: hijacking AI-human conversations by manipulating LLMs' system prompts to produce malicious answers only to specific targeted questions (e.g., \"Who should I vote for US President?\", \"Are Covid vaccines safe?\"), while behaving benignly on others. This attack is detrimental as it can enable malicious actors to exercise large-scale information manipulation by spreading harmful but benign-looking system prompts online. To demonstrate such an attack, we develop CAIN, an algorithm that can automatically curate such harmful system prompts for a specific target question in a black-box setting or without the need to access the LLM's parameters. Evaluated on both open-source and commercial LLMs, CAIN demonstrates significant adversarial impact. In untargeted attacks or forcing LLMs to output incorrect answers, CAIN achieves up to 40% F1 degradation on targeted questions while preserving high accuracy on benign inputs. For targeted attacks or forcing LLMs to output specific harmful answers, CAIN achieves over 70% F1 scores on these targeted responses with minimal impact on benign questions. Our results highlight the critical need for enhanced robustness measures to safeguard the integrity and safety of LLMs in real-world applications. All source code will be publicly available."
  },
  {
    "title": "MPO: Multilingual Safety Alignment via Reward Gap Optimization",
    "url": "http://arxiv.org/abs/2505.16869v1",
    "arxiv_id": "2505.16869v1",
    "authors": [
      "Weixiang Zhao",
      "Yulin Hu",
      "Yang Deng",
      "Tongtong Wu",
      "Wenxuan Zhang",
      "Jiahe Guo",
      "An Zhang",
      "Yanyan Zhao",
      "Bing Qin",
      "Tat-Seng Chua",
      "Ting Liu"
    ],
    "published": "2025-05-22T16:24:51+00:00",
    "summary": "Large language models (LLMs) have become increasingly central to AI applications worldwide, necessitating robust multilingual safety alignment to ensure secure deployment across diverse linguistic contexts. Existing preference learning methods for safety alignment, such as RLHF and DPO, are primarily monolingual and struggle with noisy multilingual data. To address these limitations, we introduce Multilingual reward gaP Optimization (MPO), a novel approach that leverages the well-aligned safety capabilities of the dominant language (English) to improve safety alignment across multiple languages. MPO directly minimizes the reward gap difference between the dominant language and target languages, effectively transferring safety capabilities while preserving the original strengths of the dominant language. Extensive experiments on three LLMs, LLaMA-3.1, Gemma-2 and Qwen2.5, validate MPO's efficacy in multilingual safety alignment without degrading general multilingual utility."
  },
  {
    "title": "Understanding and Analyzing Inappropriately Targeting Language in Online Discourse: A Comparative Annotation Study",
    "url": "http://arxiv.org/abs/2505.16847v1",
    "arxiv_id": "2505.16847v1",
    "authors": [
      "Baran Barbarestani",
      "Isa Maks",
      "Piek Vossen"
    ],
    "published": "2025-05-22T16:10:43+00:00",
    "summary": "This paper introduces a method for detecting inappropriately targeting language in online conversations by integrating crowd and expert annotations with ChatGPT. We focus on English conversation threads from Reddit, examining comments that target individuals or groups. Our approach involves a comprehensive annotation framework that labels a diverse data set for various target categories and specific target words within the conversational context. We perform a comparative analysis of annotations from human experts, crowd annotators, and ChatGPT, revealing strengths and limitations of each method in recognizing both explicit hate speech and subtler discriminatory language. Our findings highlight the significant role of contextual factors in identifying hate speech and uncover new categories of targeting, such as social belief and body image. We also address the challenges and subjective judgments involved in annotation and the limitations of ChatGPT in grasping nuanced language. This study provides insights for improving automated content moderation strategies to enhance online safety and inclusivity."
  },
  {
    "title": "RBench-V: A Primary Assessment for Visual Reasoning Models with Multi-modal Outputs",
    "url": "http://arxiv.org/abs/2505.16770v1",
    "arxiv_id": "2505.16770v1",
    "authors": [
      "Meng-Hao Guo",
      "Xuanyu Chu",
      "Qianrui Yang",
      "Zhe-Han Mo",
      "Yiqing Shen",
      "Pei-lin Li",
      "Xinjie Lin",
      "Jinnian Zhang",
      "Xin-Sheng Chen",
      "Yi Zhang",
      "Kiyohiro Nakayama",
      "Zhengyang Geng",
      "Houwen Peng",
      "Han Hu",
      "Shi-Nin Hu"
    ],
    "published": "2025-05-22T15:11:57+00:00",
    "summary": "The rapid advancement of native multi-modal models and omni-models, exemplified by GPT-4o, Gemini, and o3, with their capability to process and generate content across modalities such as text and images, marks a significant milestone in the evolution of intelligence. Systematic evaluation of their multi-modal output capabilities in visual thinking processes (also known as multi-modal chain of thought, M-CoT) becomes critically important. However, existing benchmarks for evaluating multi-modal models primarily focus on assessing multi-modal inputs and text-only reasoning while neglecting the importance of reasoning through multi-modal outputs. In this paper, we present a benchmark, dubbed RBench-V, designed to assess models' vision-indispensable reasoning abilities. To construct RBench-V, we carefully hand-pick 803 questions covering math, physics, counting, and games. Unlike previous benchmarks that typically specify certain input modalities, RBench-V presents problems centered on multi-modal outputs, which require image manipulation such as generating novel images and constructing auxiliary lines to support the reasoning process. We evaluate numerous open- and closed-source models on RBench-V, including o3, Gemini 2.5 Pro, Qwen2.5-VL, etc. Even the best-performing model, o3, achieves only 25.8% accuracy on RBench-V, far below the human score of 82.3%, highlighting that current models struggle to leverage multi-modal reasoning. Data and code are available at https://evalmodels.github.io/rbenchv"
  },
  {
    "title": "When Safety Detectors Aren't Enough: A Stealthy and Effective Jailbreak Attack on LLMs via Steganographic Techniques",
    "url": "http://arxiv.org/abs/2505.16765v1",
    "arxiv_id": "2505.16765v1",
    "authors": [
      "Jianing Geng",
      "Biao Yi",
      "Zekun Fei",
      "Tongxi Wu",
      "Lihai Nie",
      "Zheli Liu"
    ],
    "published": "2025-05-22T15:07:34+00:00",
    "summary": "Jailbreak attacks pose a serious threat to large language models (LLMs) by bypassing built-in safety mechanisms and leading to harmful outputs. Studying these attacks is crucial for identifying vulnerabilities and improving model security. This paper presents a systematic survey of jailbreak methods from the novel perspective of stealth. We find that existing attacks struggle to simultaneously achieve toxic stealth (concealing toxic content) and linguistic stealth (maintaining linguistic naturalness). Motivated by this, we propose StegoAttack, a fully stealthy jailbreak attack that uses steganography to hide the harmful query within benign, semantically coherent text. The attack then prompts the LLM to extract the hidden query and respond in an encrypted manner. This approach effectively hides malicious intent while preserving naturalness, allowing it to evade both built-in and external safety mechanisms. We evaluate StegoAttack on four safety-aligned LLMs from major providers, benchmarking against eight state-of-the-art methods. StegoAttack achieves an average attack success rate (ASR) of 92.00%, outperforming the strongest baseline by 11.0%. Its ASR drops by less than 1% even under external detection (e.g., Llama Guard). Moreover, it attains the optimal comprehensive scores on stealth detection metrics, demonstrating both high efficacy and exceptional stealth capabilities. The code is available at https://anonymous.4open.science/r/StegoAttack-Jail66"
  },
  {
    "title": "Robust Vision-Based Runway Detection through Conformal Prediction and Conformal mAP",
    "url": "http://arxiv.org/abs/2505.16740v1",
    "arxiv_id": "2505.16740v1",
    "authors": [
      "Alya Zouzou",
      "L\u00e9o and\u00e9ol",
      "M\u00e9lanie Ducoffe",
      "Ryma Boumazouza"
    ],
    "published": "2025-05-22T14:52:59+00:00",
    "summary": "We explore the use of conformal prediction to provide statistical uncertainty guarantees for runway detection in vision-based landing systems (VLS). Using fine-tuned YOLOv5 and YOLOv6 models on aerial imagery, we apply conformal prediction to quantify localization reliability under user-defined risk levels. We also introduce Conformal mean Average Precision (C-mAP), a novel metric aligning object detection performance with conformal guarantees. Our results show that conformal prediction can improve the reliability of runway detection by quantifying uncertainty in a statistically sound way, increasing safety on-board and paving the way for certification of ML system in the aerospace domain."
  },
  {
    "title": "Mitigating Fine-tuning Risks in LLMs via Safety-Aware Probing Optimization",
    "url": "http://arxiv.org/abs/2505.16737v1",
    "arxiv_id": "2505.16737v1",
    "authors": [
      "Chengcan Wu",
      "Zhixin Zhang",
      "Zeming Wei",
      "Yihao Zhang",
      "Meng Sun"
    ],
    "published": "2025-05-22T14:52:10+00:00",
    "summary": "The significant progress of large language models (LLMs) has led to remarkable achievements across numerous applications. However, their ability to generate harmful content has sparked substantial safety concerns. Despite the implementation of safety alignment techniques during the pre-training phase, recent research indicates that fine-tuning LLMs on adversarial or even benign data can inadvertently compromise their safety. In this paper, we re-examine the fundamental issue of why fine-tuning on non-harmful data still results in safety degradation. We introduce a safety-aware probing (SAP) optimization framework designed to mitigate the safety risks of fine-tuning LLMs. Specifically, SAP incorporates a safety-aware probe into the gradient propagation process, mitigating the model's risk of safety degradation by identifying potential pitfalls in gradient directions, thereby enhancing task-specific performance while successfully preserving model safety. Our extensive experimental results demonstrate that SAP effectively reduces harmfulness below the original fine-tuned model and achieves comparable test loss to standard fine-tuning methods. Our code is available at https://github.com/ChengcanWu/SAP."
  },
  {
    "title": "Breaking mBad! Supervised Fine-tuning for Cross-Lingual Detoxification",
    "url": "http://arxiv.org/abs/2505.16722v1",
    "arxiv_id": "2505.16722v1",
    "authors": [
      "Himanshu Beniwal",
      "Youngwoo Kim",
      "Maarten Sap",
      "Soham Dan",
      "Thomas Hartvigsen"
    ],
    "published": "2025-05-22T14:30:14+00:00",
    "summary": "As large language models (LLMs) become increasingly prevalent in global applications, ensuring that they are toxicity-free across diverse linguistic contexts remains a critical challenge. We explore \"Cross-lingual Detoxification\", a cross-lingual paradigm that mitigates toxicity, enabling detoxification capabilities to transfer between high and low-resource languages across different script families. We analyze cross-lingual detoxification's effectiveness through 504 extensive settings to evaluate toxicity reduction in cross-distribution settings with limited data and investigate how mitigation impacts model performance on non-toxic tasks, revealing trade-offs between safety and knowledge preservation. Our code and dataset are publicly available at https://github.com/himanshubeniwal/Breaking-mBad."
  },
  {
    "title": "Zero-Shot Anomaly Detection in Battery Thermal Images Using Visual Question Answering with Prior Knowledge",
    "url": "http://arxiv.org/abs/2505.16674v1",
    "arxiv_id": "2505.16674v1",
    "authors": [
      "Marcella Astrid",
      "Abdelrahman Shabayek",
      "Djamila Aouada"
    ],
    "published": "2025-05-22T13:39:52+00:00",
    "summary": "Batteries are essential for various applications, including electric vehicles and renewable energy storage, making safety and efficiency critical concerns. Anomaly detection in battery thermal images helps identify failures early, but traditional deep learning methods require extensive labeled data, which is difficult to obtain, especially for anomalies due to safety risks and high data collection costs. To overcome this, we explore zero-shot anomaly detection using Visual Question Answering (VQA) models, which leverage pretrained knowledge and textbased prompts to generalize across vision tasks. By incorporating prior knowledge of normal battery thermal behavior, we design prompts to detect anomalies without battery-specific training data. We evaluate three VQA models (ChatGPT-4o, LLaVa-13b, and BLIP-2) analyzing their robustness to prompt variations, repeated trials, and qualitative outputs. Despite the lack of finetuning on battery data, our approach demonstrates competitive performance compared to state-of-the-art models that are trained with the battery data. Our findings highlight the potential of VQA-based zero-shot learning for battery anomaly detection and suggest future directions for improving its effectiveness."
  },
  {
    "title": "From Evaluation to Defense: Advancing Safety in Video Large Language Models",
    "url": "http://arxiv.org/abs/2505.16643v1",
    "arxiv_id": "2505.16643v1",
    "authors": [
      "Yiwei Sun",
      "Peiqi Jiang",
      "Chuanbin Liu",
      "Luohao Lin",
      "Zhiying Lu",
      "Hongtao Xie"
    ],
    "published": "2025-05-22T13:16:53+00:00",
    "summary": "While the safety risks of image-based large language models have been extensively studied, their video-based counterparts (Video LLMs) remain critically under-examined. To systematically study this problem, we introduce \\textbf{VideoSafetyBench (VSB-77k) - the first large-scale, culturally diverse benchmark for Video LLM safety}, which compromises 77,646 video-query pairs and spans 19 principal risk categories across 10 language communities. \\textit{We reveal that integrating video modality degrades safety performance by an average of 42.3\\%, exposing systemic risks in multimodal attack exploitation.} To address this vulnerability, we propose \\textbf{VideoSafety-R1}, a dual-stage framework achieving unprecedented safety gains through two innovations: (1) Alarm Token-Guided Safety Fine-Tuning (AT-SFT) injects learnable alarm tokens into visual and textual sequences, enabling explicit harm perception across modalities via multitask objectives. (2) Then, Safety-Guided GRPO enhances defensive reasoning through dynamic policy optimization with rule-based rewards derived from dual-modality verification. These components synergize to shift safety alignment from passive harm recognition to active reasoning. The resulting framework achieves a 65.1\\% improvement on VSB-Eval-HH, and improves by 59.1\\%, 44.3\\%, and 15.0\\% on the image safety datasets MMBench, VLGuard, and FigStep, respectively. \\textit{Our codes are available in the supplementary materials.} \\textcolor{red}{Warning: This paper contains examples of harmful language and videos, and reader discretion is recommended.}"
  },
  {
    "title": "Safe Uncertainty-Aware Learning of Robotic Suturing",
    "url": "http://arxiv.org/abs/2505.16596v1",
    "arxiv_id": "2505.16596v1",
    "authors": [
      "Wilbert Peter Empleo",
      "Yitaek Kim",
      "Hansoul Kim",
      "Thiusius Rajeeth Savarimuthu",
      "I\u00f1igo Iturrate"
    ],
    "published": "2025-05-22T12:31:18+00:00",
    "summary": "Robot-Assisted Minimally Invasive Surgery is currently fully manually controlled by a trained surgeon. Automating this has great potential for alleviating issues, e.g., physical strain, highly repetitive tasks, and shortages of trained surgeons. For these reasons, recent works have utilized Artificial Intelligence methods, which show promising adaptability. Despite these advances, there is skepticism of these methods because they lack explainability and robust safety guarantees. This paper presents a framework for a safe, uncertainty-aware learning method. We train an Ensemble Model of Diffusion Policies using expert demonstrations of needle insertion. Using an Ensemble model, we can quantify the policy's epistemic uncertainty, which is used to determine Out-Of-Distribution scenarios. This allows the system to release control back to the surgeon in the event of an unsafe scenario. Additionally, we implement a model-free Control Barrier Function to place formal safety guarantees on the predicted action. We experimentally evaluate our proposed framework using a state-of-the-art robotic suturing simulator. We evaluate multiple scenarios, such as dropping the needle, moving the camera, and moving the phantom. The learned policy is robust to these perturbations, showing corrective behaviors and generalization, and it is possible to detect Out-Of-Distribution scenarios. We further demonstrate that the Control Barrier Function successfully limits the action to remain within our specified safety set in the case of unsafe predictions."
  },
  {
    "title": "A Survey on the Application of Large Language Models in Scenario-Based Testing of Automated Driving Systems",
    "url": "http://arxiv.org/abs/2505.16587v1",
    "arxiv_id": "2505.16587v1",
    "authors": [
      "Yongqi Zhao",
      "Ji Zhou",
      "Dong Bi",
      "Tomislav Mihalj",
      "Jia Hu",
      "Arno Eichberger"
    ],
    "published": "2025-05-22T12:25:44+00:00",
    "summary": "The safety and reliability of Automated Driving Systems (ADSs) must be validated prior to large-scale deployment. Among existing validation approaches, scenario-based testing has been regarded as a promising method to improve testing efficiency and reduce associated costs. Recently, the emergence of Large Language Models (LLMs) has introduced new opportunities to reinforce this approach. While an increasing number of studies have explored the use of LLMs in the field of automated driving, a dedicated review focusing on their application within scenario-based testing remains absent. This survey addresses this gap by systematically categorizing the roles played by LLMs across various phased of scenario-based testing, drawing from both academic research and industrial practice. In addition, key characteristics of LLMs and corresponding usage strategies are comprehensively summarized. The paper concludes by outlining five open challenges and potential research directions. To support ongoing research efforts, a continuously updated repository of recent advancements and relevant open-source tools is made available at: https://github.com/ftgTUGraz/LLM4ADSTest."
  },
  {
    "title": "Finetuning-Activated Backdoors in LLMs",
    "url": "http://arxiv.org/abs/2505.16567v1",
    "arxiv_id": "2505.16567v1",
    "authors": [
      "Thibaud Gloaguen",
      "Mark Vero",
      "Robin Staab",
      "Martin Vechev"
    ],
    "published": "2025-05-22T11:59:44+00:00",
    "summary": "Finetuning openly accessible Large Language Models (LLMs) has become standard practice for achieving task-specific performance improvements. Until now, finetuning has been regarded as a controlled and secure process in which training on benign datasets led to predictable behaviors. In this paper, we demonstrate for the first time that an adversary can create poisoned LLMs that initially appear benign but exhibit malicious behaviors once finetuned by downstream users. To this end, our proposed attack, FAB (Finetuning-Activated Backdoor), poisons an LLM via meta-learning techniques to simulate downstream finetuning, explicitly optimizing for the emergence of malicious behaviors in the finetuned models. At the same time, the poisoned LLM is regularized to retain general capabilities and to exhibit no malicious behaviors prior to finetuning. As a result, when users finetune the seemingly benign model on their own datasets, they unknowingly trigger its hidden backdoor behavior. We demonstrate the effectiveness of FAB across multiple LLMs and three target behaviors: unsolicited advertising, refusal, and jailbreakability. Additionally, we show that FAB-backdoors are robust to various finetuning choices made by the user (e.g., dataset, number of steps, scheduler). Our findings challenge prevailing assumptions about the security of finetuning, revealing yet another critical attack vector exploiting the complexities of LLMs."
  },
  {
    "title": "ScholarBench: A Bilingual Benchmark for Abstraction, Comprehension, and Reasoning Evaluation in Academic Contexts",
    "url": "http://arxiv.org/abs/2505.16566v1",
    "arxiv_id": "2505.16566v1",
    "authors": [
      "Dongwon Noh",
      "Donghyeok Koh",
      "Junghun Yuk",
      "Gyuwan Kim",
      "Jaeyong Lee",
      "Kyungtae Lim",
      "Cheoneum Park"
    ],
    "published": "2025-05-22T11:59:06+00:00",
    "summary": "Prior benchmarks for evaluating the domain-specific knowledge of large language models (LLMs) lack the scalability to handle complex academic tasks. To address this, we introduce \\texttt{ScholarBench}, a benchmark centered on deep expert knowledge and complex academic problem-solving, which evaluates the academic reasoning ability of LLMs and is constructed through a three-step process. \\texttt{ScholarBench} targets more specialized and logically complex contexts derived from academic literature, encompassing five distinct problem types. Unlike prior benchmarks, \\texttt{ScholarBench} evaluates the abstraction, comprehension, and reasoning capabilities of LLMs across eight distinct research domains. To ensure high-quality evaluation data, we define category-specific example attributes and design questions that are aligned with the characteristic research methodologies and discourse structures of each domain. Additionally, this benchmark operates as an English-Korean bilingual dataset, facilitating simultaneous evaluation for linguistic capabilities of LLMs in both languages. The benchmark comprises 5,031 examples in Korean and 5,309 in English, with even state-of-the-art models like o3-mini achieving an average evaluation score of only 0.543, demonstrating the challenging nature of this benchmark."
  },
  {
    "title": "CTRAP: Embedding Collapse Trap to Safeguard Large Language Models from Harmful Fine-Tuning",
    "url": "http://arxiv.org/abs/2505.16559v1",
    "arxiv_id": "2505.16559v1",
    "authors": [
      "Biao Yi",
      "Tiansheng Huang",
      "Baolei Zhang",
      "Tong Li",
      "Lihai Nie",
      "Zheli Liu",
      "Li Shen"
    ],
    "published": "2025-05-22T11:47:08+00:00",
    "summary": "Fine-tuning-as-a-service, while commercially successful for Large Language Model (LLM) providers, exposes models to harmful fine-tuning attacks. As a widely explored defense paradigm against such attacks, unlearning attempts to remove malicious knowledge from LLMs, thereby essentially preventing them from being used to perform malicious tasks. However, we highlight a critical flaw: the powerful general adaptability of LLMs allows them to easily bypass selective unlearning by rapidly relearning or repurposing their capabilities for harmful tasks. To address this fundamental limitation, we propose a paradigm shift: instead of selective removal, we advocate for inducing model collapse--effectively forcing the model to \"unlearn everything\"--specifically in response to updates characteristic of malicious adaptation. This collapse directly neutralizes the very general capabilities that attackers exploit, tackling the core issue unaddressed by selective unlearning. We introduce the Collapse Trap (CTRAP) as a practical mechanism to implement this concept conditionally. Embedded during alignment, CTRAP pre-configures the model's reaction to subsequent fine-tuning dynamics. If updates during fine-tuning constitute a persistent attempt to reverse safety alignment, the pre-configured trap triggers a progressive degradation of the model's core language modeling abilities, ultimately rendering it inert and useless for the attacker. Crucially, this collapse mechanism remains dormant during benign fine-tuning, ensuring the model's utility and general capabilities are preserved for legitimate users. Extensive empirical results demonstrate that CTRAP effectively counters harmful fine-tuning risks across various LLMs and attack settings, while maintaining high performance in benign scenarios. Our code is available at https://anonymous.4open.science/r/CTRAP."
  },
  {
    "title": "DuFFin: A Dual-Level Fingerprinting Framework for LLMs IP Protection",
    "url": "http://arxiv.org/abs/2505.16530v1",
    "arxiv_id": "2505.16530v1",
    "authors": [
      "Yuliang Yan",
      "Haochun Tang",
      "Shuo Yan",
      "Enyan Dai"
    ],
    "published": "2025-05-22T11:16:46+00:00",
    "summary": "Large language models (LLMs) are considered valuable Intellectual Properties (IP) for legitimate owners due to the enormous computational cost of training. It is crucial to protect the IP of LLMs from malicious stealing or unauthorized deployment. Despite existing efforts in watermarking and fingerprinting LLMs, these methods either impact the text generation process or are limited in white-box access to the suspect model, making them impractical. Hence, we propose DuFFin, a novel $\\textbf{Du}$al-Level $\\textbf{Fin}$gerprinting $\\textbf{F}$ramework for black-box setting ownership verification. DuFFin extracts the trigger pattern and the knowledge-level fingerprints to identify the source of a suspect model. We conduct experiments on a variety of models collected from the open-source website, including four popular base models as protected LLMs and their fine-tuning, quantization, and safety alignment versions, which are released by large companies, start-ups, and individual users. Results show that our method can accurately verify the copyright of the base protected LLM on their model variants, achieving the IP-ROC metric greater than 0.95. Our code is available at https://github.com/yuliangyan0807/llm-fingerprint."
  },
  {
    "title": "Teaching Large Language Models to Maintain Contextual Faithfulness via Synthetic Tasks and Reinforcement Learning",
    "url": "http://arxiv.org/abs/2505.16483v1",
    "arxiv_id": "2505.16483v1",
    "authors": [
      "Shuzheng Si",
      "Haozhe Zhao",
      "Cheng Gao",
      "Yuzhuo Bai",
      "Zhitong Wang",
      "Bofei Gao",
      "Kangyang Luo",
      "Wenhao Li",
      "Yufei Huang",
      "Gang Chen",
      "Fanchao Qi",
      "Minjia Zhang",
      "Baobao Chang",
      "Maosong Sun"
    ],
    "published": "2025-05-22T10:10:07+00:00",
    "summary": "Teaching large language models (LLMs) to be faithful in the provided context is crucial for building reliable information-seeking systems. Therefore, we propose a systematic framework, CANOE, to improve the faithfulness of LLMs in both short-form and long-form generation tasks without human annotations. Specifically, we first synthesize short-form question-answering (QA) data with four diverse tasks to construct high-quality and easily verifiable training data without human annotation. Also, we propose Dual-GRPO, a rule-based reinforcement learning method that includes three tailored rule-based rewards derived from synthesized short-form QA data, while simultaneously optimizing both short-form and long-form response generation. Notably, Dual-GRPO eliminates the need to manually label preference data to train reward models and avoids over-optimizing short-form generation when relying only on the synthesized short-form QA data. Experimental results show that CANOE greatly improves the faithfulness of LLMs across 11 different downstream tasks, even outperforming the most advanced LLMs, e.g., GPT-4o and OpenAI o1."
  },
  {
    "title": "Implicit Jailbreak Attacks via Cross-Modal Information Concealment on Vision-Language Models",
    "url": "http://arxiv.org/abs/2505.16446v1",
    "arxiv_id": "2505.16446v1",
    "authors": [
      "Zhaoxin Wang",
      "Handing Wang",
      "Cong Tian",
      "Yaochu Jin"
    ],
    "published": "2025-05-22T09:34:47+00:00",
    "summary": "Multimodal large language models (MLLMs) enable powerful cross-modal reasoning capabilities. However, the expanded input space introduces new attack surfaces. Previous jailbreak attacks often inject malicious instructions from text into less aligned modalities, such as vision. As MLLMs increasingly incorporate cross-modal consistency and alignment mechanisms, such explicit attacks become easier to detect and block. In this work, we propose a novel implicit jailbreak framework termed IJA that stealthily embeds malicious instructions into images via least significant bit steganography and couples them with seemingly benign, image-related textual prompts. To further enhance attack effectiveness across diverse MLLMs, we incorporate adversarial suffixes generated by a surrogate model and introduce a template optimization module that iteratively refines both the prompt and embedding based on model feedback. On commercial models like GPT-4o and Gemini-1.5 Pro, our method achieves attack success rates of over 90% using an average of only 3 queries."
  },
  {
    "title": "WebAgent-R1: Training Web Agents via End-to-End Multi-Turn Reinforcement Learning",
    "url": "http://arxiv.org/abs/2505.16421v1",
    "arxiv_id": "2505.16421v1",
    "authors": [
      "Zhepei Wei",
      "Wenlin Yao",
      "Yao Liu",
      "Weizhi Zhang",
      "Qin Lu",
      "Liang Qiu",
      "Changlong Yu",
      "Puyang Xu",
      "Chao Zhang",
      "Bing Yin",
      "Hyokun Yun",
      "Lihong Li"
    ],
    "published": "2025-05-22T09:07:43+00:00",
    "summary": "While reinforcement learning (RL) has demonstrated remarkable success in enhancing large language models (LLMs), it has primarily focused on single-turn tasks such as solving math problems. Training effective web agents for multi-turn interactions remains challenging due to the complexity of long-horizon decision-making across dynamic web interfaces. In this work, we present WebAgent-R1, a simple yet effective end-to-end multi-turn RL framework for training web agents. It learns directly from online interactions with web environments by asynchronously generating diverse trajectories, entirely guided by binary rewards depending on task success. Experiments on the WebArena-Lite benchmark demonstrate the effectiveness of WebAgent-R1, boosting the task success rate of Qwen-2.5-3B from 6.1% to 33.9% and Llama-3.1-8B from 8.5% to 44.8%, significantly outperforming existing state-of-the-art methods and strong proprietary models such as OpenAI o3. In-depth analyses reveal the effectiveness of the thinking-based prompting strategy and test-time scaling through increased interactions for web tasks. We further investigate different RL initialization policies by introducing two variants, namely WebAgent-R1-Zero and WebAgent-R1-CoT, which highlight the importance of the warm-up training stage (i.e., behavior cloning) and provide insights on incorporating long chain-of-thought (CoT) reasoning in web agents."
  },
  {
    "title": "AdvReal: Adversarial Patch Generation Framework with Application to Adversarial Safety Evaluation of Object Detection Systems",
    "url": "http://arxiv.org/abs/2505.16402v1",
    "arxiv_id": "2505.16402v1",
    "authors": [
      "Yuanhao Huang",
      "Yilong Ren",
      "Jinlei Wang",
      "Lujia Huo",
      "Xuesong Bai",
      "Jinchuan Zhang",
      "Haiyan Yu"
    ],
    "published": "2025-05-22T08:54:03+00:00",
    "summary": "Autonomous vehicles are typical complex intelligent systems with artificial intelligence at their core. However, perception methods based on deep learning are extremely vulnerable to adversarial samples, resulting in safety accidents. How to generate effective adversarial examples in the physical world and evaluate object detection systems is a huge challenge. In this study, we propose a unified joint adversarial training framework for both 2D and 3D samples to address the challenges of intra-class diversity and environmental variations in real-world scenarios. Building upon this framework, we introduce an adversarial sample reality enhancement approach that incorporates non-rigid surface modeling and a realistic 3D matching mechanism. We compare with 5 advanced adversarial patches and evaluate their attack performance on 8 object detecotrs, including single-stage, two-stage, and transformer-based models. Extensive experiment results in digital and physical environments demonstrate that the adversarial textures generated by our method can effectively mislead the target detection model. Moreover, proposed method demonstrates excellent robustness and transferability under multi-angle attacks, varying lighting conditions, and different distance in the physical world. The demo video and code can be obtained at https://github.com/Huangyh98/AdvReal.git."
  },
  {
    "title": "AceReason-Nemotron: Advancing Math and Code Reasoning through Reinforcement Learning",
    "url": "http://arxiv.org/abs/2505.16400v1",
    "arxiv_id": "2505.16400v1",
    "authors": [
      "Yang Chen",
      "Zhuolin Yang",
      "Zihan Liu",
      "Chankyu Lee",
      "Peng Xu",
      "Mohammad Shoeybi",
      "Bryan Catanzaro",
      "Wei Ping"
    ],
    "published": "2025-05-22T08:50:47+00:00",
    "summary": "Despite recent progress in large-scale reinforcement learning (RL) for reasoning, the training recipe for building high-performing reasoning models remains elusive. Key implementation details of frontier models, such as DeepSeek-R1, including data curation strategies and RL training recipe, are often omitted. Moreover, recent research indicates distillation remains more effective than RL for smaller models. In this work, we demonstrate that large-scale RL can significantly enhance the reasoning capabilities of strong, small- and mid-sized models, achieving results that surpass those of state-of-the-art distillation-based models. We systematically study the RL training process through extensive ablations and propose a simple yet effective approach: first training on math-only prompts, then on code-only prompts. Notably, we find that math-only RL not only significantly enhances the performance of strong distilled models on math benchmarks (e.g., +14.6% / +17.2% on AIME 2025 for the 7B / 14B models), but also code reasoning tasks (e.g., +6.8% / +5.8% on LiveCodeBench for the 7B / 14B models). In addition, extended code-only RL iterations further improve performance on code benchmarks with minimal or no degradation in math results. We develop a robust data curation pipeline to collect challenging prompts with high-quality, verifiable answers and test cases to enable verification-based RL across both domains. Finally, we identify key experimental insights, including curriculum learning with progressively increasing response lengths and the stabilizing effect of on-policy parameter updates. We find that RL not only elicits the foundational reasoning capabilities acquired during pretraining and supervised fine-tuning (e.g., distillation), but also pushes the limits of the model's reasoning ability, enabling it to solve problems that were previously unsolvable."
  },
  {
    "title": "VL-SAFE: Vision-Language Guided Safety-Aware Reinforcement Learning with World Models for Autonomous Driving",
    "url": "http://arxiv.org/abs/2505.16377v1",
    "arxiv_id": "2505.16377v1",
    "authors": [
      "Yansong Qu",
      "Zilin Huang",
      "Zihao Sheng",
      "Jiancong Chen",
      "Sikai Chen",
      "Samuel Labi"
    ],
    "published": "2025-05-22T08:29:59+00:00",
    "summary": "Reinforcement learning (RL)-based autonomous driving policy learning faces critical limitations such as low sample efficiency and poor generalization; its reliance on online interactions and trial-and-error learning is especially unacceptable in safety-critical scenarios. Existing methods including safe RL often fail to capture the true semantic meaning of \"safety\" in complex driving contexts, leading to either overly conservative driving behavior or constraint violations. To address these challenges, we propose VL-SAFE, a world model-based safe RL framework with Vision-Language model (VLM)-as-safety-guidance paradigm, designed for offline safe policy learning. Specifically, we construct offline datasets containing data collected by expert agents and labeled with safety scores derived from VLMs. A world model is trained to generate imagined rollouts together with safety estimations, allowing the agent to perform safe planning without interacting with the real environment. Based on these imagined trajectories and safety evaluations, actor-critic learning is conducted under VLM-based safety guidance to optimize the driving policy more safely and efficiently. Extensive evaluations demonstrate that VL-SAFE achieves superior sample efficiency, generalization, safety, and overall performance compared to existing baselines. To the best of our knowledge, this is the first work that introduces a VLM-guided world model-based approach for safe autonomous driving. The demo video and code can be accessed at: https://ys-qu.github.io/vlsafe-website/"
  },
  {
    "title": "SATURN: SAT-based Reinforcement Learning to Unleash Language Model Reasoning",
    "url": "http://arxiv.org/abs/2505.16368v1",
    "arxiv_id": "2505.16368v1",
    "authors": [
      "Huanyu Liu",
      "Jia Li",
      "Hao Zhu",
      "Kechi Zhang",
      "Yihong Dong",
      "Ge Li"
    ],
    "published": "2025-05-22T08:23:10+00:00",
    "summary": "How to design reinforcement learning (RL) tasks that effectively unleash the reasoning capability of large language models (LLMs) remains an open question. Existing RL tasks (e.g., math, programming, and constructing reasoning tasks) suffer from three key limitations: (1) Scalability. They rely heavily on human annotation or expensive LLM synthesis to generate sufficient training data. (2) Verifiability. LLMs' outputs are hard to verify automatically and reliably. (3) Controllable Difficulty. Most tasks lack fine-grained difficulty control, making it hard to train LLMs to develop reasoning ability from easy to hard.   To address these limitations, we propose Saturn, a SAT-based RL framework that uses Boolean Satisfiability (SAT) problems to train and evaluate LLM reasoning. Saturn enables scalable task construction, rule-based verification, and precise difficulty control. Saturn designs a curriculum learning pipeline that continuously improves LLMs' reasoning capability by constructing SAT tasks of increasing difficulty and training LLMs from easy to hard. To ensure stable training, we design a principled mechanism to control difficulty transitions.   We introduce Saturn-2.6k, a dataset of 2,660 SAT problems with varying difficulty. It supports the evaluation of how LLM reasoning changes with problem difficulty. We apply Saturn to DeepSeek-R1-Distill-Qwen and obtain Saturn-1.5B and Saturn-7B. We achieve several notable results: (1) On SAT problems, Saturn-1.5B and Saturn-7B achieve average pass@3 improvements of +14.0 and +28.1, respectively. (2) On math and programming tasks, Saturn-1.5B and Saturn-7B improve average scores by +4.9 and +1.8 on benchmarks (e.g., AIME, LiveCodeBench). (3) Compared to the state-of-the-art (SOTA) approach in constructing RL tasks, Saturn achieves further improvements of +8.8%. We release the source code, data, and models to support future research."
  },
  {
    "title": "Offline Guarded Safe Reinforcement Learning for Medical Treatment Optimization Strategies",
    "url": "http://arxiv.org/abs/2505.16242v1",
    "arxiv_id": "2505.16242v1",
    "authors": [
      "Runze Yan",
      "Xun Shen",
      "Akifumi Wachi",
      "Sebastien Gros",
      "Anni Zhao",
      "Xiao Hu"
    ],
    "published": "2025-05-22T05:22:03+00:00",
    "summary": "When applying offline reinforcement learning (RL) in healthcare scenarios, the out-of-distribution (OOD) issues pose significant risks, as inappropriate generalization beyond clinical expertise can result in potentially harmful recommendations. While existing methods like conservative Q-learning (CQL) attempt to address the OOD issue, their effectiveness is limited by only constraining action selection by suppressing uncertain actions. This action-only regularization imitates clinician actions that prioritize short-term rewards, but it fails to regulate downstream state trajectories, thereby limiting the discovery of improved long-term treatment strategies. To safely improve policy beyond clinician recommendations while ensuring that state-action trajectories remain in-distribution, we propose \\textit{Offline Guarded Safe Reinforcement Learning} ($\\mathsf{OGSRL}$), a theoretically grounded model-based offline RL framework. $\\mathsf{OGSRL}$ introduces a novel dual constraint mechanism for improving policy with reliability and safety. First, the OOD guardian is established to specify clinically validated regions for safe policy exploration. By constraining optimization within these regions, it enables the reliable exploration of treatment strategies that outperform clinician behavior by leveraging the full patient state history, without drifting into unsupported state-action trajectories. Second, we introduce a safety cost constraint that encodes medical knowledge about physiological safety boundaries, providing domain-specific safeguards even in areas where training data might contain potentially unsafe interventions. Notably, we provide theoretical guarantees on safety and near-optimality: policies that satisfy these constraints remain in safe and reliable regions and achieve performance close to the best possible policy supported by the data."
  },
  {
    "title": "Three Minds, One Legend: Jailbreak Large Reasoning Model with Adaptive Stacked Ciphers",
    "url": "http://arxiv.org/abs/2505.16241v1",
    "arxiv_id": "2505.16241v1",
    "authors": [
      "Viet-Anh Nguyen",
      "Shiqian Zhao",
      "Gia Dao",
      "Runyi Hu",
      "Yi Xie",
      "Luu Anh Tuan"
    ],
    "published": "2025-05-22T05:19:42+00:00",
    "summary": "Recently, Large Reasoning Models (LRMs) have demonstrated superior logical capabilities compared to traditional Large Language Models (LLMs), gaining significant attention. Despite their impressive performance, the potential for stronger reasoning abilities to introduce more severe security vulnerabilities remains largely underexplored. Existing jailbreak methods often struggle to balance effectiveness with robustness against adaptive safety mechanisms. In this work, we propose SEAL, a novel jailbreak attack that targets LRMs through an adaptive encryption pipeline designed to override their reasoning processes and evade potential adaptive alignment. Specifically, SEAL introduces a stacked encryption approach that combines multiple ciphers to overwhelm the models reasoning capabilities, effectively bypassing built-in safety mechanisms. To further prevent LRMs from developing countermeasures, we incorporate two dynamic strategies - random and adaptive - that adjust the cipher length, order, and combination. Extensive experiments on real-world reasoning models, including DeepSeek-R1, Claude Sonnet, and OpenAI GPT-o4, validate the effectiveness of our approach. Notably, SEAL achieves an attack success rate of 80.8% on GPT o4-mini, outperforming state-of-the-art baselines by a significant margin of 27.2%. Warning: This paper contains examples of inappropriate, offensive, and harmful content."
  },
  {
    "title": "Behavioral Safety Assessment towards Large-scale Deployment of Autonomous Vehicles",
    "url": "http://arxiv.org/abs/2505.16214v1",
    "arxiv_id": "2505.16214v1",
    "authors": [
      "Henry X. Liu",
      "Xintao Yan",
      "Haowei Sun",
      "Tinghan Wang",
      "Zhijie Qiao",
      "Haojie Zhu",
      "Shengyin Shen",
      "Shuo Feng",
      "Greg Stevens",
      "Greg McGuire"
    ],
    "published": "2025-05-22T04:28:59+00:00",
    "summary": "Autonomous vehicles (AVs) have significantly advanced in real-world deployment in recent years, yet safety continues to be a critical barrier to widespread adoption. Traditional functional safety approaches, which primarily verify the reliability, robustness, and adequacy of AV hardware and software systems from a vehicle-centric perspective, do not sufficiently address the AV's broader interactions and behavioral impact on the surrounding traffic environment. To overcome this limitation, we propose a paradigm shift toward behavioral safety, a comprehensive approach focused on evaluating AV responses and interactions within the traffic environment. To systematically assess behavioral safety, we introduce a third-party AV safety assessment framework comprising two complementary evaluation components: the Driver Licensing Test and the Driving Intelligence Test. The Driver Licensing Test evaluates the AV's reactive behaviors under controlled scenarios, ensuring basic behavioral competency. In contrast, the Driving Intelligence Test assesses the AV's interactive behaviors within naturalistic traffic conditions, quantifying the frequency of safety-critical events to deliver statistically meaningful safety metrics before large-scale deployment. We validated our proposed framework using Autoware.Universe, an open-source Level 4 AV, tested both in simulated environments and on the physical test track at the University of Michigan's Mcity Testing Facility. The results indicate that Autoware.Universe passed 6 out of 14 scenarios and exhibited a crash rate of 3.01e-3 crashes per mile, approximately 1,000 times higher than the average human driver crash rate. During the tests, we also uncovered several unknown unsafe scenarios for Autoware.Universe. These findings underscore the necessity of behavioral safety evaluations for improving AV safety performance prior to widespread public deployment."
  },
  {
    "title": "AudioTrust: Benchmarking the Multifaceted Trustworthiness of Audio Large Language Models",
    "url": "http://arxiv.org/abs/2505.16211v1",
    "arxiv_id": "2505.16211v1",
    "authors": [
      "Kai Li",
      "Can Shen",
      "Yile Liu",
      "Jirui Han",
      "Kelong Zheng",
      "Xuechao Zou",
      "Zhe Wang",
      "Xingjian Du",
      "Shun Zhang",
      "Hanjun Luo",
      "Yingbin Jin",
      "Xinxin Xing",
      "Ziyang Ma",
      "Yue Liu",
      "Xiaojun Jia",
      "Yifan Zhang",
      "Junfeng Fang",
      "Kun Wang",
      "Yibo Yan",
      "Haoyang Li",
      "Yiming Li",
      "Xiaobin Zhuang",
      "Yang Liu",
      "Haibo Hu",
      "Zhuo Chen",
      "Zhizheng Wu",
      "Xiaolin Hu",
      "Eng-Siong Chng",
      "XiaoFeng Wang",
      "Wenyuan Xu",
      "Wei Dong",
      "Xinfeng Li"
    ],
    "published": "2025-05-22T04:27:46+00:00",
    "summary": "The rapid advancement and expanding applications of Audio Large Language Models (ALLMs) demand a rigorous understanding of their trustworthiness. However, systematic research on evaluating these models, particularly concerning risks unique to the audio modality, remains largely unexplored. Existing evaluation frameworks primarily focus on the text modality or address only a restricted set of safety dimensions, failing to adequately account for the unique characteristics and application scenarios inherent to the audio modality. We introduce AudioTrust-the first multifaceted trustworthiness evaluation framework and benchmark specifically designed for ALLMs. AudioTrust facilitates assessments across six key dimensions: fairness, hallucination, safety, privacy, robustness, and authentication. To comprehensively evaluate these dimensions, AudioTrust is structured around 18 distinct experimental setups. Its core is a meticulously constructed dataset of over 4,420 audio/text samples, drawn from real-world scenarios (e.g., daily conversations, emergency calls, voice assistant interactions), specifically designed to probe the multifaceted trustworthiness of ALLMs. For assessment, the benchmark carefully designs 9 audio-specific evaluation metrics, and we employ a large-scale automated pipeline for objective and scalable scoring of model outputs. Experimental results reveal the trustworthiness boundaries and limitations of current state-of-the-art open-source and closed-source ALLMs when confronted with various high-risk audio scenarios, offering valuable insights for the secure and trustworthy deployment of future audio models. Our platform and benchmark are available at https://github.com/JusperLee/AudioTrust."
  },
  {
    "title": "SafeKey: Amplifying Aha-Moment Insights for Safety Reasoning",
    "url": "http://arxiv.org/abs/2505.16186v1",
    "arxiv_id": "2505.16186v1",
    "authors": [
      "Kaiwen Zhou",
      "Xuandong Zhao",
      "Gaowen Liu",
      "Jayanth Srinivasa",
      "Aosong Feng",
      "Dawn Song",
      "Xin Eric Wang"
    ],
    "published": "2025-05-22T03:46:03+00:00",
    "summary": "Large Reasoning Models (LRMs) introduce a new generation paradigm of explicitly reasoning before answering, leading to remarkable improvements in complex tasks. However, they pose great safety risks against harmful queries and adversarial attacks. While recent mainstream safety efforts on LRMs, supervised fine-tuning (SFT), improve safety performance, we find that SFT-aligned models struggle to generalize to unseen jailbreak prompts. After thorough investigation of LRMs' generation, we identify a safety aha moment that can activate safety reasoning and lead to a safe response. This aha moment typically appears in the `key sentence', which follows models' query understanding process and can indicate whether the model will proceed safely. Based on these insights, we propose SafeKey, including two complementary objectives to better activate the safety aha moment in the key sentence: (1) a Dual-Path Safety Head to enhance the safety signal in the model's internal representations before the key sentence, and (2) a Query-Mask Modeling objective to improve the models' attention on its query understanding, which has important safety hints. Experiments across multiple safety benchmarks demonstrate that our methods significantly improve safety generalization to a wide range of jailbreak attacks and out-of-distribution harmful prompts, lowering the average harmfulness rate by 9.6\\%, while maintaining general abilities. Our analysis reveals how SafeKey enhances safety by reshaping internal attention and improving the quality of hidden representations."
  },
  {
    "title": "Understanding Generative AI Capabilities in Everyday Image Editing Tasks",
    "url": "http://arxiv.org/abs/2505.16181v1",
    "arxiv_id": "2505.16181v1",
    "authors": [
      "Mohammad Reza Taesiri",
      "Brandon Collins",
      "Logan Bolton",
      "Viet Dac Lai",
      "Franck Dernoncourt",
      "Trung Bui",
      "Anh Totti Nguyen"
    ],
    "published": "2025-05-22T03:35:15+00:00",
    "summary": "Generative AI (GenAI) holds significant promise for automating everyday image editing tasks, especially following the recent release of GPT-4o on March 25, 2025. However, what subjects do people most often want edited? What kinds of editing actions do they want to perform (e.g., removing or stylizing the subject)? Do people prefer precise edits with predictable outcomes or highly creative ones? By understanding the characteristics of real-world requests and the corresponding edits made by freelance photo-editing wizards, can we draw lessons for improving AI-based editors and determine which types of requests can currently be handled successfully by AI editors? In this paper, we present a unique study addressing these questions by analyzing 83k requests from the past 12 years (2013-2025) on the Reddit community, which collected 305k PSR-wizard edits. According to human ratings, approximately only 33% of requests can be fulfilled by the best AI editors (including GPT-4o, Gemini-2.0-Flash, SeedEdit). Interestingly, AI editors perform worse on low-creativity requests that require precise editing than on more open-ended tasks. They often struggle to preserve the identity of people and animals, and frequently make non-requested touch-ups. On the other side of the table, VLM judges (e.g., o1) perform differently from human judges and may prefer AI edits more than human edits. Code and qualitative examples are available at: https://psrdataset.github.io"
  },
  {
    "title": "Training-Free Reasoning and Reflection in MLLMs",
    "url": "http://arxiv.org/abs/2505.16151v1",
    "arxiv_id": "2505.16151v1",
    "authors": [
      "Hongchen Wei",
      "Zhenzhong Chen"
    ],
    "published": "2025-05-22T02:51:12+00:00",
    "summary": "Recent advances in Reasoning LLMs (e.g., DeepSeek-R1 and OpenAI-o1) have showcased impressive reasoning capabilities via reinforcement learning. However, extending these capabilities to Multimodal LLMs (MLLMs) is hampered by the prohibitive costs of retraining and the scarcity of high-quality, verifiable multimodal reasoning datasets. This paper introduces FRANK Model, a training-FRee ANd r1-liKe MLLM that imbues off-the-shelf MLLMs with reasoning and reflection abilities, without any gradient updates or extra supervision. Our key insight is to decouple perception and reasoning across MLLM decoder layers. Specifically, we observe that compared to the deeper decoder layers, the shallow decoder layers allocate more attention to visual tokens, while the deeper decoder layers concentrate on textual semantics. This observation motivates a hierarchical weight merging approach that combines a visual-pretrained MLLM with a reasoning-specialized LLM. To this end, we propose a layer-wise, Taylor-derived closed-form fusion mechanism that integrates reasoning capacity into deep decoder layers while preserving visual grounding in shallow decoder layers. Extensive experiments on challenging multimodal reasoning benchmarks demonstrate the effectiveness of our approach. On the MMMU benchmark, our model FRANK-38B achieves an accuracy of 69.2, outperforming the strongest baseline InternVL2.5-38B by +5.3, and even surpasses the proprietary GPT-4o model. Our project homepage is at: http://iip.whu.edu.cn/frank/index.html"
  },
  {
    "title": "Hierarchical Safety Realignment: Lightweight Restoration of Safety in Pruned Large Vision-Language Models",
    "url": "http://arxiv.org/abs/2505.16104v1",
    "arxiv_id": "2505.16104v1",
    "authors": [
      "Yue Li",
      "Xin Yi",
      "Dongsheng Shi",
      "Gerard de Melo",
      "Xiaoling Wang",
      "Linlin Wang"
    ],
    "published": "2025-05-22T01:06:28+00:00",
    "summary": "With the increasing size of Large Vision-Language Models (LVLMs), network pruning techniques aimed at compressing models for deployment in resource-constrained environments have garnered significant attention. However, we observe that pruning often leads to a degradation in safety performance. To address this issue, we present a novel and lightweight approach, termed Hierarchical Safety Realignment (HSR). HSR operates by first quantifying the contribution of each attention head to safety, identifying the most critical ones, and then selectively restoring neurons directly within these attention heads that play a pivotal role in maintaining safety. This process hierarchically realigns the safety of pruned LVLMs, progressing from the attention head level to the neuron level. We validate HSR across various models and pruning strategies, consistently achieving notable improvements in safety performance. To our knowledge, this is the first work explicitly focused on restoring safety in LVLMs post-pruning."
  },
  {
    "title": "Proactive Hierarchical Control Barrier Function-Based Safety Prioritization in Close Human-Robot Interaction Scenarios",
    "url": "http://arxiv.org/abs/2505.16055v1",
    "arxiv_id": "2505.16055v1",
    "authors": [
      "Patanjali Maithania",
      "Aliasghar Araba",
      "Farshad Khorramia",
      "Prashanth Krishnamurthya"
    ],
    "published": "2025-05-21T22:10:08+00:00",
    "summary": "In collaborative human-robot environments, the unpredictable and dynamic nature of human motion can lead to situations where collisions become unavoidable. In such cases, it is essential for the robotic system to proactively mitigate potential harm through intelligent control strategies. This paper presents a hierarchical control framework based on Control Barrier Functions (CBFs) designed to ensure safe and adaptive operation of autonomous robotic manipulators during close-proximity human-robot interaction. The proposed method introduces a relaxation variable that enables real-time prioritization of safety constraints, allowing the robot to dynamically manage collision risks based on the criticality of different parts of the human body. A secondary constraint mechanism is incorporated to resolve infeasibility by increasing the priority of imminent threats. The framework is experimentally validated on a Franka Research 3 robot equipped with a ZED2i AI camera for real-time human pose and body detection. Experimental results confirm that the CBF-based controller, integrated with depth sensing, facilitates responsive and safe human-robot collaboration, while providing detailed risk analysis and maintaining robust performance in highly dynamic settings."
  },
  {
    "title": "OpenEthics: A Comprehensive Ethical Evaluation of Open-Source Generative Large Language Models",
    "url": "http://arxiv.org/abs/2505.16036v1",
    "arxiv_id": "2505.16036v1",
    "authors": [
      "Burak Erin\u00e7 \u00c7etin",
      "Y\u0131ld\u0131r\u0131m \u00d6zen",
      "Elif Naz Demiry\u0131lmaz",
      "Kaan Eng\u00fcr",
      "Cagri Toraman"
    ],
    "published": "2025-05-21T21:31:35+00:00",
    "summary": "Generative large language models present significant potential but also raise critical ethical concerns. Most studies focus on narrow ethical dimensions, and also limited diversity of languages and models. To address these gaps, we conduct a broad ethical evaluation of 29 recent open-source large language models using a novel data collection including four ethical aspects: Robustness, reliability, safety, and fairness. We analyze model behavior in both a commonly used language, English, and a low-resource language, Turkish. Our aim is to provide a comprehensive ethical assessment and guide safer model development by filling existing gaps in evaluation breadth, language coverage, and model diversity. Our experimental results, based on LLM-as-a-Judge, reveal that optimization efforts for many open-source models appear to have prioritized safety and fairness, and demonstrated good robustness while reliability remains a concern. We demonstrate that ethical evaluation can be effectively conducted independently of the language used. In addition, models with larger parameter counts tend to exhibit better ethical performance, with Gemma and Qwen models demonstrating the most ethical behavior among those evaluated."
  },
  {
    "title": "NOVER: Incentive Training for Language Models via Verifier-Free Reinforcement Learning",
    "url": "http://arxiv.org/abs/2505.16022v1",
    "arxiv_id": "2505.16022v1",
    "authors": [
      "Wei Liu",
      "Siya Qi",
      "Xinyu Wang",
      "Chen Qian",
      "Yali Du",
      "Yulan He"
    ],
    "published": "2025-05-21T21:12:35+00:00",
    "summary": "Recent advances such as DeepSeek R1-Zero highlight the effectiveness of incentive training, a reinforcement learning paradigm that computes rewards solely based on the final answer part of a language model's output, thereby encouraging the generation of intermediate reasoning steps. However, these methods fundamentally rely on external verifiers, which limits their applicability to domains like mathematics and coding where such verifiers are readily available. Although reward models can serve as verifiers, they require high-quality annotated data and are costly to train. In this work, we propose NOVER, NO-VERifier Reinforcement Learning, a general reinforcement learning framework that requires only standard supervised fine-tuning data with no need for an external verifier. NOVER enables incentive training across a wide range of text-to-text tasks and outperforms the model of the same size distilled from large reasoning models such as DeepSeek R1 671B by 7.7 percent. Moreover, the flexibility of NOVER enables new possibilities for optimizing large language models, such as inverse incentive training."
  },
  {
    "title": "Domain Adaptive Skin Lesion Classification via Conformal Ensemble of Vision Transformers",
    "url": "http://arxiv.org/abs/2505.15997v1",
    "arxiv_id": "2505.15997v1",
    "authors": [
      "Mehran Zoravar",
      "Shadi Alijani",
      "Homayoun Najjaran"
    ],
    "published": "2025-05-21T20:28:43+00:00",
    "summary": "Exploring the trustworthiness of deep learning models is crucial, especially in critical domains such as medical imaging decision support systems. Conformal prediction has emerged as a rigorous means of providing deep learning models with reliable uncertainty estimates and safety guarantees. However, conformal prediction results face challenges due to the backbone model's struggles in domain-shifted scenarios, such as variations in different sources. To aim this challenge, this paper proposes a novel framework termed Conformal Ensemble of Vision Transformers (CE-ViTs) designed to enhance image classification performance by prioritizing domain adaptation and model robustness, while accounting for uncertainty. The proposed method leverages an ensemble of vision transformer models in the backbone, trained on diverse datasets including HAM10000, Dermofit, and Skin Cancer ISIC datasets. This ensemble learning approach, calibrated through the combined mentioned datasets, aims to enhance domain adaptation through conformal learning. Experimental results underscore that the framework achieves a high coverage rate of 90.38\\%, representing an improvement of 9.95\\% compared to the HAM10000 model. This indicates a strong likelihood that the prediction set includes the true label compared to singular models. Ensemble learning in CE-ViTs significantly improves conformal prediction performance, increasing the average prediction set size for challenging misclassified samples from 1.86 to 3.075."
  },
  {
    "title": "AI-Assisted NLOS Sensing for RIS-Based Indoor Localization in Smart Factories",
    "url": "http://arxiv.org/abs/2505.15989v1",
    "arxiv_id": "2505.15989v1",
    "authors": [
      "Taofeek A. O. Yusuf",
      "Sigurd S. Petersen",
      "Puchu Li",
      "Jian Ren",
      "Placido Mursia",
      "Vincenzo Sciancalepore",
      "Xavier Costa P\u00e9rez",
      "Gilberto Berardinelli",
      "Ming Shen"
    ],
    "published": "2025-05-21T20:12:30+00:00",
    "summary": "In the era of Industry 4.0, precise indoor localization is vital for automation and efficiency in smart factories. Reconfigurable Intelligent Surfaces (RIS) are emerging as key enablers in 6G networks for joint sensing and communication. However, RIS faces significant challenges in Non-Line-of-Sight (NLOS) and multipath propagation, particularly in localization scenarios, where detecting NLOS conditions is crucial for ensuring not only reliable results and increased connectivity but also the safety of smart factory personnel. This study introduces an AI-assisted framework employing a Convolutional Neural Network (CNN) customized for accurate Line-of-Sight (LOS) and Non-Line-of-Sight (NLOS) classification to enhance RIS-based localization using measured, synthetic, mixed-measured, and mixed-synthetic experimental data, that is, original, augmented, slightly noisy, and highly noisy data, respectively. Validated through such data from three different environments, the proposed customized-CNN (cCNN) model achieves {95.0\\%-99.0\\%} accuracy, outperforming standard pre-trained models like Visual Geometry Group 16 (VGG-16) with an accuracy of {85.5\\%-88.0\\%}. By addressing RIS limitations in NLOS scenarios, this framework offers scalable and high-precision localization solutions for 6G-enabled smart factories."
  },
  {
    "title": "HornStr: A string Theory Solver for Constrained Horn Clauses",
    "url": "http://arxiv.org/abs/2505.15959v1",
    "arxiv_id": "2505.15959v1",
    "authors": [
      "Hongjian Jiang",
      "Anthony W. Lin",
      "Oliver Markgraf",
      "Philipp R\u00fcmmer",
      "Daniel Stan"
    ],
    "published": "2025-05-21T19:22:16+00:00",
    "summary": "We present HornStr, the first solver for invariant synthesis for Regular Model Checking (RMC) with the specification provided in the SMT-LIB 2.6 theory of strings. It is well-known that invariant synthesis for RMC subsumes various important verification problems, including safety verification for parameterized systems. To achieve a simple and standardized file format, we treat the invariant synthesis problem as a problem of solving Constrained Horn Clauses (CHCs) over strings. Two strategies for synthesizing invariants in terms of regular constraints are supported: (1) L* automata learning, and (2) SAT-based automata learning. HornStr implements these strategies with the help of existing SMT solvers for strings, which are interfaced through SMT-LIB. HornStr provides an easy-to-use interface for string solver developers to apply their techniques to verification and at the same time verification researchers to painlessly tap into the wealth of modern string solving techniques. To assess the effectiveness of HornStr, we conducted a comprehensive evaluation using benchmarks derived from applications including parameterized verification and string rewriting tasks. Our experiments highlight HornStr's capacity to effectively handle these benchmarks, e.g., as the first solver to verify the challenging MU puzzle automatically. Finally, HornStr can be used to automatically generate a new class of interesting SMT-LIB 2.6 string constraint benchmarks, which might in the future be used in the SMT-COMP strings track. In particular, our experiments on the above invariant synthesis benchmarks produce more than 30000 new QF_S constraints. We also detail the performance of various integrated string solvers, providing insights into their effectiveness on our new benchmarks."
  },
  {
    "title": "Towards Holistic Evaluation of Large Audio-Language Models: A Comprehensive Survey",
    "url": "http://arxiv.org/abs/2505.15957v1",
    "arxiv_id": "2505.15957v1",
    "authors": [
      "Chih-Kai Yang",
      "Neo S. Ho",
      "Hung-yi Lee"
    ],
    "published": "2025-05-21T19:17:29+00:00",
    "summary": "With advancements in large audio-language models (LALMs), which enhance large language models (LLMs) with auditory capabilities, these models are expected to demonstrate universal proficiency across various auditory tasks. While numerous benchmarks have emerged to assess LALMs' performance, they remain fragmented and lack a structured taxonomy. To bridge this gap, we conduct a comprehensive survey and propose a systematic taxonomy for LALM evaluations, categorizing them into four dimensions based on their objectives: (1) General Auditory Awareness and Processing, (2) Knowledge and Reasoning, (3) Dialogue-oriented Ability, and (4) Fairness, Safety, and Trustworthiness. We provide detailed overviews within each category and highlight challenges in this field, offering insights into promising future directions. To the best of our knowledge, this is the first survey specifically focused on the evaluations of LALMs, providing clear guidelines for the community. We will release the collection of the surveyed papers and actively maintain it to support ongoing advancements in the field."
  },
  {
    "title": "MAPS: A Multilingual Benchmark for Global Agent Performance and Security",
    "url": "http://arxiv.org/abs/2505.15935v1",
    "arxiv_id": "2505.15935v1",
    "authors": [
      "Omer Hofman",
      "Oren Rachmil",
      "Shamik Bose",
      "Vikas Pahuja",
      "Jonathan Brokman",
      "Toshiya Shimizu",
      "Trisha Starostina",
      "Kelly Marchisio",
      "Seraphina Goldfarb-Tarrant",
      "Roman Vainshtein"
    ],
    "published": "2025-05-21T18:42:00+00:00",
    "summary": "Agentic AI systems, which build on Large Language Models (LLMs) and interact with tools and memory, have rapidly advanced in capability and scope. Yet, since LLMs have been shown to struggle in multilingual settings, typically resulting in lower performance and reduced safety, agentic systems risk inheriting these limitations. This raises concerns about the global accessibility of such systems, as users interacting in languages other than English may encounter unreliable or security-critical agent behavior. Despite growing interest in evaluating agentic AI, existing benchmarks focus exclusively on English, leaving multilingual settings unexplored. To address this gap, we propose MAPS, a multilingual benchmark suite designed to evaluate agentic AI systems across diverse languages and tasks. MAPS builds on four widely used agentic benchmarks - GAIA (real-world tasks), SWE-bench (code generation), MATH (mathematical reasoning), and the Agent Security Benchmark (security). We translate each dataset into ten diverse languages, resulting in 805 unique tasks and 8,855 total language-specific instances. Our benchmark suite enables a systematic analysis of how multilingual contexts affect agent performance and robustness. Empirically, we observe consistent degradation in both performance and security when transitioning from English to other languages, with severity varying by task and correlating with the amount of translated input. Building on these findings, we provide actionable recommendations to guide agentic AI systems development and assessment under multilingual settings. This work establishes a standardized evaluation framework, encouraging future research towards equitable, reliable, and globally accessible agentic AI. MAPS benchmark suite is publicly available at https://huggingface.co/datasets/Fujitsu-FRE/MAPS"
  },
  {
    "title": "Constant-Sum High-Order Barrier Functions for Safety Between Parallel Boundaries",
    "url": "http://arxiv.org/abs/2505.15932v1",
    "arxiv_id": "2505.15932v1",
    "authors": [
      "Kwang Hak Kim",
      "Mamadou Diagne",
      "Miroslav Krsti\u0107"
    ],
    "published": "2025-05-21T18:36:44+00:00",
    "summary": "This paper takes a step towards addressing the difficulty of constructing Control Barrier Functions (CBFs) for parallel safety boundaries. A single CBF for both boundaries has been reported to be difficult to validate for safety, and we identify why this challenge is inherent. To overcome this, the proposed method constructs separate CBFs for each boundary. We begin by presenting results for the relative degree one case and then extend these to higher relative degrees using the CBF backstepping technique, establishing conditions that guarantee safety. Finally, we showcase our method by applying it to a unicycle system, deriving a simple, verifiable condition to validate the target CBFs for direct implementation of our results."
  },
  {
    "title": "VERDI: VLM-Embedded Reasoning for Autonomous Driving",
    "url": "http://arxiv.org/abs/2505.15925v1",
    "arxiv_id": "2505.15925v1",
    "authors": [
      "Bowen Feng",
      "Zhiting Mei",
      "Baiang Li",
      "Julian Ost",
      "Roger Girgis",
      "Anirudha Majumdar",
      "Felix Heide"
    ],
    "published": "2025-05-21T18:24:36+00:00",
    "summary": "While autonomous driving (AD) stacks struggle with decision making under partial observability and real-world complexity, human drivers are capable of commonsense reasoning to make near-optimal decisions with limited information. Recent work has attempted to leverage finetuned Vision-Language Models (VLMs) for trajectory planning at inference time to emulate human behavior. Despite their success in benchmark evaluations, these methods are often impractical to deploy (a 70B parameter VLM inference at merely 8 tokens per second requires more than 160G of memory), and their monolithic network structure prohibits safety decomposition. To bridge this gap, we propose VLM-Embedded Reasoning for autonomous Driving (VERDI), a training-time framework that distills the reasoning process and commonsense knowledge of VLMs into the AD stack. VERDI augments modular differentiable end-to-end (e2e) AD models by aligning intermediate module outputs at the perception, prediction, and planning stages with text features explaining the driving reasoning process produced by VLMs. By encouraging alignment in latent space, \\textsc{VERDI} enables the modular AD stack to internalize structured reasoning, without incurring the inference-time costs of large VLMs. We demonstrate the effectiveness of our method on the NuScenes dataset and find that VERDI outperforms existing e2e methods that do not embed reasoning by 10% in $\\ell_{2}$ distance, while maintaining high inference speed."
  },
  {
    "title": "On the creation of narrow AI: hierarchy and nonlocality of neural network skills",
    "url": "http://arxiv.org/abs/2505.15811v1",
    "arxiv_id": "2505.15811v1",
    "authors": [
      "Eric J. Michaud",
      "Asher Parker-Sartori",
      "Max Tegmark"
    ],
    "published": "2025-05-21T17:59:21+00:00",
    "summary": "We study the problem of creating strong, yet narrow, AI systems. While recent AI progress has been driven by the training of large general-purpose foundation models, the creation of smaller models specialized for narrow domains could be valuable for both efficiency and safety. In this work, we explore two challenges involved in creating such systems, having to do with basic properties of how neural networks learn and structure their representations. The first challenge regards when it is possible to train narrow models from scratch. Through experiments on a synthetic task, we find that it is sometimes necessary to train networks on a wide distribution of data to learn certain narrow skills within that distribution. This effect arises when skills depend on each other hierarchically, and training on a broad distribution introduces a curriculum which substantially accelerates learning. The second challenge regards how to transfer particular skills from large general models into small specialized models. We find that model skills are often not perfectly localized to a particular set of prunable components. However, we find that methods based on pruning can still outperform distillation. We investigate the use of a regularization objective to align desired skills with prunable components while unlearning unnecessary skills."
  },
  {
    "title": "Keep Security! Benchmarking Security Policy Preservation in Large Language Model Contexts Against Indirect Attacks in Question Answering",
    "url": "http://arxiv.org/abs/2505.15805v1",
    "arxiv_id": "2505.15805v1",
    "authors": [
      "Hwan Chang",
      "Yumin Kim",
      "Yonghyun Jun",
      "Hwanhee Lee"
    ],
    "published": "2025-05-21T17:58:11+00:00",
    "summary": "As Large Language Models (LLMs) are increasingly deployed in sensitive domains such as enterprise and government, ensuring that they adhere to user-defined security policies within context is critical-especially with respect to information non-disclosure. While prior LLM studies have focused on general safety and socially sensitive data, large-scale benchmarks for contextual security preservation against attacks remain lacking. To address this, we introduce a novel large-scale benchmark dataset, CoPriva, evaluating LLM adherence to contextual non-disclosure policies in question answering. Derived from realistic contexts, our dataset includes explicit policies and queries designed as direct and challenging indirect attacks seeking prohibited information. We evaluate 10 LLMs on our benchmark and reveal a significant vulnerability: many models violate user-defined policies and leak sensitive information. This failure is particularly severe against indirect attacks, highlighting a critical gap in current LLM safety alignment for sensitive applications. Our analysis reveals that while models can often identify the correct answer to a query, they struggle to incorporate policy constraints during generation. In contrast, they exhibit a partial ability to revise outputs when explicitly prompted. Our findings underscore the urgent need for more robust methods to guarantee contextual security."
  },
  {
    "title": "VerifyBench: Benchmarking Reference-based Reward Systems for Large Language Models",
    "url": "http://arxiv.org/abs/2505.15801v1",
    "arxiv_id": "2505.15801v1",
    "authors": [
      "Yuchen Yan",
      "Jin Jiang",
      "Zhenbang Ren",
      "Yijun Li",
      "Xudong Cai",
      "Yang Liu",
      "Xin Xu",
      "Mengdi Zhang",
      "Jian Shao",
      "Yongliang Shen",
      "Jun Xiao",
      "Yueting Zhuang"
    ],
    "published": "2025-05-21T17:54:43+00:00",
    "summary": "Large reasoning models such as OpenAI o1 and DeepSeek-R1 have achieved remarkable performance in the domain of reasoning. A key component of their training is the incorporation of verifiable rewards within reinforcement learning (RL). However, existing reward benchmarks do not evaluate reference-based reward systems, leaving researchers with limited understanding of the accuracy of verifiers used in RL. In this paper, we introduce two benchmarks, VerifyBench and VerifyBench-Hard, designed to assess the performance of reference-based reward systems. These benchmarks are constructed through meticulous data collection and curation, followed by careful human annotation to ensure high quality. Current models still show considerable room for improvement on both VerifyBench and VerifyBench-Hard, especially smaller-scale models. Furthermore, we conduct a thorough and comprehensive analysis of evaluation results, offering insights for understanding and developing reference-based reward systems. Our proposed benchmarks serve as effective tools for guiding the development of verifier accuracy and the reasoning capabilities of models trained via RL in reasoning tasks."
  },
  {
    "title": "HCRMP: A LLM-Hinted Contextual Reinforcement Learning Framework for Autonomous Driving",
    "url": "http://arxiv.org/abs/2505.15793v1",
    "arxiv_id": "2505.15793v1",
    "authors": [
      "Zhiwen Chen",
      "Bo Leng",
      "Zhuoren Li",
      "Hanming Deng",
      "Guizhe Jin",
      "Ran Yu",
      "Huanxi Wen"
    ],
    "published": "2025-05-21T17:47:24+00:00",
    "summary": "Integrating Large Language Models (LLMs) with Reinforcement Learning (RL) can enhance autonomous driving (AD) performance in complex scenarios. However, current LLM-Dominated RL methods over-rely on LLM outputs, which are prone to hallucinations.Evaluations show that state-of-the-art LLM indicates a non-hallucination rate of only approximately 57.95% when assessed on essential driving-related tasks. Thus, in these methods, hallucinations from the LLM can directly jeopardize the performance of driving policies. This paper argues that maintaining relative independence between the LLM and the RL is vital for solving the hallucinations problem. Consequently, this paper is devoted to propose a novel LLM-Hinted RL paradigm. The LLM is used to generate semantic hints for state augmentation and policy optimization to assist RL agent in motion planning, while the RL agent counteracts potential erroneous semantic indications through policy learning to achieve excellent driving performance. Based on this paradigm, we propose the HCRMP (LLM-Hinted Contextual Reinforcement Learning Motion Planner) architecture, which is designed that includes Augmented Semantic Representation Module to extend state space. Contextual Stability Anchor Module enhances the reliability of multi-critic weight hints by utilizing information from the knowledge base. Semantic Cache Module is employed to seamlessly integrate LLM low-frequency guidance with RL high-frequency control. Extensive experiments in CARLA validate HCRMP's strong overall driving performance. HCRMP achieves a task success rate of up to 80.3% under diverse driving conditions with different traffic densities. Under safety-critical driving conditions, HCRMP significantly reduces the collision rate by 11.4%, which effectively improves the driving performance in complex scenarios."
  },
  {
    "title": "HCRMP: A LLM-Hinted Contextual Reinforcement Learning Framework for Autonomous Driving",
    "url": "http://arxiv.org/abs/2505.15793v2",
    "arxiv_id": "2505.15793v2",
    "authors": [
      "Zhiwen Chen",
      "Bo Leng",
      "Zhuoren Li",
      "Hanming Deng",
      "Guizhe Jin",
      "Ran Yu",
      "Huanxi Wen"
    ],
    "published": "2025-05-21T17:47:24+00:00",
    "summary": "Integrating Large Language Models (LLMs) with Reinforcement Learning (RL) can enhance autonomous driving (AD) performance in complex scenarios. However, current LLM-Dominated RL methods over-rely on LLM outputs, which are prone to hallucinations. Evaluations show that state-of-the-art LLM indicates a non-hallucination rate of only approximately 57.95% when assessed on essential driving-related tasks. Thus, in these methods, hallucinations from the LLM can directly jeopardize the performance of driving policies. This paper argues that maintaining relative independence between the LLM and the RL is vital for solving the hallucinations problem. Consequently, this paper is devoted to propose a novel LLM-Hinted RL paradigm. The LLM is used to generate semantic hints for state augmentation and policy optimization to assist RL agent in motion planning, while the RL agent counteracts potential erroneous semantic indications through policy learning to achieve excellent driving performance. Based on this paradigm, we propose the HCRMP (LLM-Hinted Contextual Reinforcement Learning Motion Planner) architecture, which is designed that includes Augmented Semantic Representation Module to extend state space. Contextual Stability Anchor Module enhances the reliability of multi-critic weight hints by utilizing information from the knowledge base. Semantic Cache Module is employed to seamlessly integrate LLM low-frequency guidance with RL high-frequency control. Extensive experiments in CARLA validate HCRMP's strong overall driving performance. HCRMP achieves a task success rate of up to 80.3% under diverse driving conditions with different traffic densities. Under safety-critical driving conditions, HCRMP significantly reduces the collision rate by 11.4%, which effectively improves the driving performance in complex scenarios."
  },
  {
    "title": "Scalable Defense against In-the-wild Jailbreaking Attacks with Safety Context Retrieval",
    "url": "http://arxiv.org/abs/2505.15753v1",
    "arxiv_id": "2505.15753v1",
    "authors": [
      "Taiye Chen",
      "Zeming Wei",
      "Ang Li",
      "Yisen Wang"
    ],
    "published": "2025-05-21T16:58:14+00:00",
    "summary": "Large Language Models (LLMs) are known to be vulnerable to jailbreaking attacks, wherein adversaries exploit carefully engineered prompts to induce harmful or unethical responses. Such threats have raised critical concerns about the safety and reliability of LLMs in real-world deployment. While existing defense mechanisms partially mitigate such risks, subsequent advancements in adversarial techniques have enabled novel jailbreaking methods to circumvent these protections, exposing the limitations of static defense frameworks. In this work, we explore defending against evolving jailbreaking threats through the lens of context retrieval. First, we conduct a preliminary study demonstrating that even a minimal set of safety-aligned examples against a particular jailbreak can significantly enhance robustness against this attack pattern. Building on this insight, we further leverage the retrieval-augmented generation (RAG) techniques and propose Safety Context Retrieval (SCR), a scalable and robust safeguarding paradigm for LLMs against jailbreaking. Our comprehensive experiments demonstrate how SCR achieves superior defensive performance against both established and emerging jailbreaking tactics, contributing a new paradigm to LLM safety. Our code will be available upon publication."
  },
  {
    "title": "Alignment Under Pressure: The Case for Informed Adversaries When Evaluating LLM Defenses",
    "url": "http://arxiv.org/abs/2505.15738v1",
    "arxiv_id": "2505.15738v1",
    "authors": [
      "Xiaoxue Yang",
      "Bozhidar Stevanoski",
      "Matthieu Meeus",
      "Yves-Alexandre de Montjoye"
    ],
    "published": "2025-05-21T16:43:17+00:00",
    "summary": "Large language models (LLMs) are rapidly deployed in real-world applications ranging from chatbots to agentic systems. Alignment is one of the main approaches used to defend against attacks such as prompt injection and jailbreaks. Recent defenses report near-zero Attack Success Rates (ASR) even against Greedy Coordinate Gradient (GCG), a white-box attack that generates adversarial suffixes to induce attacker-desired outputs. However, this search space over discrete tokens is extremely large, making the task of finding successful attacks difficult. GCG has, for instance, been shown to converge to local minima, making it sensitive to initialization choices. In this paper, we assess the future-proof robustness of these defenses using a more informed threat model: attackers who have access to some information about the alignment process. Specifically, we propose an informed white-box attack leveraging the intermediate model checkpoints to initialize GCG, with each checkpoint acting as a stepping stone for the next one. We show this approach to be highly effective across state-of-the-art (SOTA) defenses and models. We further show our informed initialization to outperform other initialization methods and show a gradient-informed checkpoint selection strategy to greatly improve attack performance and efficiency. Importantly, we also show our method to successfully find universal adversarial suffixes -- single suffixes effective across diverse inputs. Our results show that, contrary to previous beliefs, effective adversarial suffixes do exist against SOTA alignment-based defenses, that these can be found by existing attack methods when adversaries exploit alignment knowledge, and that even universal suffixes exist. Taken together, our results highlight the brittleness of current alignment-based methods and the need to consider stronger threat models when testing the safety of LLMs."
  },
  {
    "title": "Safe Control for Pursuit-Evasion with Density Functions",
    "url": "http://arxiv.org/abs/2505.15718v1",
    "arxiv_id": "2505.15718v1",
    "authors": [
      "Mustafa Bozdag",
      "Arya Honarpisheh",
      "Mario Sznaier"
    ],
    "published": "2025-05-21T16:25:52+00:00",
    "summary": "This letter presents a density function based safe control synthesis framework for the pursuit-evasion problem. We extend safety analysis to dynamic unsafe sets by formulating a reach-avoid type pursuit-evasion differential game as a robust safe control problem. Using density functions and semi-algebraic set definitions, we derive sufficient conditions for weak eventuality and evasion, reformulating the problem into a convex sum-of-squares program solvable via standard semidefinite programming solvers. This approach avoids the computational complexity of solving the Hamilton-Jacobi-Isaacs partial differential equation, offering a scalable and efficient framework. Numerical simulations demonstrate the efficacy of the proposed method."
  },
  {
    "title": "Beyond Empathy: Integrating Diagnostic and Therapeutic Reasoning with Large Language Models for Mental Health Counseling",
    "url": "http://arxiv.org/abs/2505.15715v1",
    "arxiv_id": "2505.15715v1",
    "authors": [
      "He Hu",
      "Yucheng Zhou",
      "Juzheng Si",
      "Qianning Wang",
      "Hengheng Zhang",
      "Fuji Ren",
      "Fei Ma",
      "Laizhong Cui"
    ],
    "published": "2025-05-21T16:24:49+00:00",
    "summary": "Large language models (LLMs) hold significant potential for mental health support, capable of generating empathetic responses and simulating therapeutic conversations. However, existing LLM-based approaches often lack the clinical grounding necessary for real-world psychological counseling, particularly in explicit diagnostic reasoning aligned with standards like the DSM/ICD and incorporating diverse therapeutic modalities beyond basic empathy or single strategies. To address these critical limitations, we propose PsyLLM, the first large language model designed to systematically integrate both diagnostic and therapeutic reasoning for mental health counseling. To develop the PsyLLM, we propose a novel automated data synthesis pipeline. This pipeline processes real-world mental health posts, generates multi-turn dialogue structures, and leverages LLMs guided by international diagnostic standards (e.g., DSM/ICD) and multiple therapeutic frameworks (e.g., CBT, ACT, psychodynamic) to simulate detailed clinical reasoning processes. Rigorous multi-dimensional filtering ensures the generation of high-quality, clinically aligned dialogue data. In addition, we introduce a new benchmark and evaluation protocol, assessing counseling quality across four key dimensions: comprehensiveness, professionalism, authenticity, and safety. Our experiments demonstrate that PsyLLM significantly outperforms state-of-the-art baseline models on this benchmark."
  },
  {
    "title": "Advancing LLM Safe Alignment with Safety Representation Ranking",
    "url": "http://arxiv.org/abs/2505.15710v1",
    "arxiv_id": "2505.15710v1",
    "authors": [
      "Tianqi Du",
      "Zeming Wei",
      "Quan Chen",
      "Chenheng Zhang",
      "Yisen Wang"
    ],
    "published": "2025-05-21T16:21:29+00:00",
    "summary": "The rapid advancement of large language models (LLMs) has demonstrated milestone success in a variety of tasks, yet their potential for generating harmful content has raised significant safety concerns. Existing safety evaluation approaches typically operate directly on textual responses, overlooking the rich information embedded in the model's internal representations. In this paper, we propose Safety Representation Ranking (SRR), a listwise ranking framework that selects safe responses using hidden states from the LLM itself. SRR encodes both instructions and candidate completions using intermediate transformer representations and ranks candidates via a lightweight similarity-based scorer. Our approach directly leverages internal model states and supervision at the list level to capture subtle safety signals. Experiments across multiple benchmarks show that SRR significantly improves robustness to adversarial prompts. Our code will be available upon publication."
  },
  {
    "title": "SwarmDiff: Swarm Robotic Trajectory Planning in Cluttered Environments via Diffusion Transformer",
    "url": "http://arxiv.org/abs/2505.15679v1",
    "arxiv_id": "2505.15679v1",
    "authors": [
      "Kang Ding",
      "Chunxuan Jiao",
      "Yunze Hu",
      "Kangjie Zhou",
      "Pengying Wu",
      "Yao Mu",
      "Chang Liu"
    ],
    "published": "2025-05-21T15:56:55+00:00",
    "summary": "Swarm robotic trajectory planning faces challenges in computational efficiency, scalability, and safety, particularly in complex, obstacle-dense environments. To address these issues, we propose SwarmDiff, a hierarchical and scalable generative framework for swarm robots. We model the swarm's macroscopic state using Probability Density Functions (PDFs) and leverage conditional diffusion models to generate risk-aware macroscopic trajectory distributions, which then guide the generation of individual robot trajectories at the microscopic level. To ensure a balance between the swarm's optimal transportation and risk awareness, we integrate Wasserstein metrics and Conditional Value at Risk (CVaR). Additionally, we introduce a Diffusion Transformer (DiT) to improve sampling efficiency and generation quality by capturing long-range dependencies. Extensive simulations and real-world experiments demonstrate that SwarmDiff outperforms existing methods in computational efficiency, trajectory validity, and scalability, making it a reliable solution for swarm robotic trajectory planning."
  },
  {
    "title": "Enhancing Monte Carlo Dropout Performance for Uncertainty Quantification",
    "url": "http://arxiv.org/abs/2505.15671v1",
    "arxiv_id": "2505.15671v1",
    "authors": [
      "Hamzeh Asgharnezhad",
      "Afshar Shamsi",
      "Roohallah Alizadehsani",
      "Arash Mohammadi",
      "Hamid Alinejad-Rokny"
    ],
    "published": "2025-05-21T15:50:03+00:00",
    "summary": "Knowing the uncertainty associated with the output of a deep neural network is of paramount importance in making trustworthy decisions, particularly in high-stakes fields like medical diagnosis and autonomous systems. Monte Carlo Dropout (MCD) is a widely used method for uncertainty quantification, as it can be easily integrated into various deep architectures. However, conventional MCD often struggles with providing well-calibrated uncertainty estimates. To address this, we introduce innovative frameworks that enhances MCD by integrating different search solutions namely Grey Wolf Optimizer (GWO), Bayesian Optimization (BO), and Particle Swarm Optimization (PSO) as well as an uncertainty-aware loss function, thereby improving the reliability of uncertainty quantification. We conduct comprehensive experiments using different backbones, namely DenseNet121, ResNet50, and VGG16, on various datasets, including Cats vs. Dogs, Myocarditis, Wisconsin, and a synthetic dataset (Circles). Our proposed algorithm outperforms the MCD baseline by 2-3% on average in terms of both conventional accuracy and uncertainty accuracy while achieving significantly better calibration. These results highlight the potential of our approach to enhance the trustworthiness of deep learning models in safety-critical applications."
  },
  {
    "title": "Feature Extraction and Steering for Enhanced Chain-of-Thought Reasoning in Language Models",
    "url": "http://arxiv.org/abs/2505.15634v1",
    "arxiv_id": "2505.15634v1",
    "authors": [
      "Zihao Li",
      "Xu Wang",
      "Yuzhe Yang",
      "Ziyu Yao",
      "Haoyi Xiong",
      "Mengnan Du"
    ],
    "published": "2025-05-21T15:17:59+00:00",
    "summary": "Large Language Models (LLMs) demonstrate the ability to solve reasoning and mathematical problems using the Chain-of-Thought (CoT) technique. Expanding CoT length, as seen in models such as DeepSeek-R1, significantly enhances this reasoning for complex problems, but requires costly and high-quality long CoT data and fine-tuning. This work, inspired by the deep thinking paradigm of DeepSeek-R1, utilizes a steering technique to enhance the reasoning ability of an LLM without external datasets. Our method first employs Sparse Autoencoders (SAEs) to extract interpretable features from vanilla CoT. These features are then used to steer the LLM's internal states during generation. Recognizing that many LLMs do not have corresponding pre-trained SAEs, we further introduce a novel SAE-free steering algorithm, which directly computes steering directions from the residual activations of an LLM, obviating the need for an explicit SAE. Experimental results demonstrate that both our SAE-based and subsequent SAE-free steering algorithms significantly enhance the reasoning capabilities of LLMs."
  },
  {
    "title": "Learn to Reason Efficiently with Adaptive Length-based Reward Shaping",
    "url": "http://arxiv.org/abs/2505.15612v1",
    "arxiv_id": "2505.15612v1",
    "authors": [
      "Wei Liu",
      "Ruochen Zhou",
      "Yiyun Deng",
      "Yuzhen Huang",
      "Junteng Liu",
      "Yuntian Deng",
      "Yizhe Zhang",
      "Junxian He"
    ],
    "published": "2025-05-21T15:03:26+00:00",
    "summary": "Large Reasoning Models (LRMs) have shown remarkable capabilities in solving complex problems through reinforcement learning (RL), particularly by generating long reasoning traces. However, these extended outputs often exhibit substantial redundancy, which limits the efficiency of LRMs. In this paper, we investigate RL-based approaches to promote reasoning efficiency. Specifically, we first present a unified framework that formulates various efficient reasoning methods through the lens of length-based reward shaping. Building on this perspective, we propose a novel Length-bAsed StEp Reward shaping method (LASER), which employs a step function as the reward, controlled by a target length. LASER surpasses previous methods, achieving a superior Pareto-optimal balance between performance and efficiency. Next, we further extend LASER based on two key intuitions: (1) The reasoning behavior of the model evolves during training, necessitating reward specifications that are also adaptive and dynamic; (2) Rather than uniformly encouraging shorter or longer chains of thought (CoT), we posit that length-based reward shaping should be difficulty-aware i.e., it should penalize lengthy CoTs more for easy queries. This approach is expected to facilitate a combination of fast and slow thinking, leading to a better overall tradeoff. The resulting method is termed LASER-D (Dynamic and Difficulty-aware). Experiments on DeepSeek-R1-Distill-Qwen-1.5B, DeepSeek-R1-Distill-Qwen-7B, and DeepSeek-R1-Distill-Qwen-32B show that our approach significantly enhances both reasoning performance and response length efficiency. For instance, LASER-D and its variant achieve a +6.1 improvement on AIME2024 while reducing token usage by 63%. Further analysis reveals our RL-based compression produces more concise reasoning patterns with less redundant \"self-reflections\". Resources are at https://github.com/hkust-nlp/Laser."
  },
  {
    "title": "Self-powered smart contact lenses: a multidisciplinary approach to micro-scale energy and 900 MHz - 1.1 GHz bandwidth microfabricated loop antennas communication systems",
    "url": "http://arxiv.org/abs/2505.15593v1",
    "arxiv_id": "2505.15593v1",
    "authors": [
      "Patrice Salzenstein",
      "Blandine Guichardaz",
      "Aya Maroua Bessou",
      "Ekaterina Pavlyuchenko",
      "Martine Comte",
      "Maxim V. Pogumirsky"
    ],
    "published": "2025-05-21T14:47:07+00:00",
    "summary": "Smart contact lenses are at the forefront of integrating microelectronics, biomedical engineering, and optics into wearable technologies. This work addresses a key obstacle in their development: achieving autonomous power without compromising safety or miniaturization. We examine energy harvesting strategies using intrinsic ocular sources-particularly tear salinity and eyelid motion-to enable sustainable operation without external batteries. The study emphasizes compact loop antennas operating between 900 MHz and 1.1 GHz as critical for wireless data transmission and power management. Material choices, signal integrity, and biocompatibility are also discussed. By presenting recent advances in 3D-printed optics, antenna integration, and energy systems, we propose a conceptual framework for the next generation of smart lenses, enabling real-time health monitoring and vision enhancement through self-powered, compact devices."
  },
  {
    "title": "From learning to safety: A Direct Data-Driven Framework for Constrained Control",
    "url": "http://arxiv.org/abs/2505.15515v1",
    "arxiv_id": "2505.15515v1",
    "authors": [
      "Kanghui He",
      "Shengling Shi",
      "Ton van den Boom",
      "Bart De Schutter"
    ],
    "published": "2025-05-21T13:39:45+00:00",
    "summary": "Ensuring safety in the sense of constraint satisfaction for learning-based control is a critical challenge, especially in the model-free case. While safety filters address this challenge in the model-based setting by modifying unsafe control inputs, they typically rely on predictive models derived from physics or data. This reliance limits their applicability for advanced model-free learning control methods. To address this gap, we propose a new optimization-based control framework that determines safe control inputs directly from data. The benefit of the framework is that it can be updated through arbitrary model-free learning algorithms to pursue optimal performance. As a key component, the concept of direct data-driven safety filters (3DSF) is first proposed. The framework employs a novel safety certificate, called the state-action control barrier function (SACBF). We present three different schemes to learn the SACBF. Furthermore, based on input-to-state safety analysis, we present the error-to-state safety analysis framework, which provides formal guarantees on safety and recursive feasibility even in the presence of learning inaccuracies. The proposed control framework bridges the gap between model-free learning-based control and constrained control, by decoupling performance optimization from safety enforcement. Simulations on vehicle control illustrate the superior performance regarding constraint satisfaction and task achievement compared to model-based methods and reward shaping."
  },
  {
    "title": "Multilingual Test-Time Scaling via Initial Thought Transfer",
    "url": "http://arxiv.org/abs/2505.15508v1",
    "arxiv_id": "2505.15508v1",
    "authors": [
      "Prasoon Bajpai",
      "Tanmoy Chakraborty"
    ],
    "published": "2025-05-21T13:27:38+00:00",
    "summary": "Test-time scaling has emerged as a widely adopted inference-time strategy for boosting reasoning performance. However, its effectiveness has been studied almost exclusively in English, leaving its behavior in other languages largely unexplored. We present the first systematic study of test-time scaling in multilingual settings, evaluating DeepSeek-R1-Distill-LLama-8B and DeepSeek-R1-Distill-Qwen-7B across both high- and low-resource Latin-script languages. Our findings reveal that the relative gains from test-time scaling vary significantly across languages. Additionally, models frequently switch to English mid-reasoning, even when operating under strictly monolingual prompts. We further show that low-resource languages not only produce initial reasoning thoughts that differ significantly from English but also have lower internal consistency across generations in their early reasoning. Building on our findings, we introduce MITT (Multilingual Initial Thought Transfer), an unsupervised and lightweight reasoning prefix-tuning approach that transfers high-resource reasoning prefixes to enhance test-time scaling across all languages, addressing inconsistencies in multilingual reasoning performance. MITT significantly boosts DeepSeek-R1-Distill-Qwen-7B's reasoning performance, especially for underrepresented languages."
  },
  {
    "title": "Coloring Between the Lines: Personalization in the Null Space of Planning Constraints",
    "url": "http://arxiv.org/abs/2505.15503v1",
    "arxiv_id": "2505.15503v1",
    "authors": [
      "Tom Silver",
      "Rajat Kumar Jenamani",
      "Ziang Liu",
      "Ben Dodson",
      "Tapomayukh Bhattacharjee"
    ],
    "published": "2025-05-21T13:24:05+00:00",
    "summary": "Generalist robots must personalize in-the-wild to meet the diverse needs and preferences of long-term users. How can we enable flexible personalization without sacrificing safety or competency? This paper proposes Coloring Between the Lines (CBTL), a method for personalization that exploits the null space of constraint satisfaction problems (CSPs) used in robot planning. CBTL begins with a CSP generator that ensures safe and competent behavior, then incrementally personalizes behavior by learning parameterized constraints from online interaction. By quantifying uncertainty and leveraging the compositionality of planning constraints, CBTL achieves sample-efficient adaptation without environment resets. We evaluate CBTL in (1) three diverse simulation environments; (2) a web-based user study; and (3) a real-robot assisted feeding system, finding that CBTL consistently achieves more effective personalization with fewer interactions than baselines. Our results demonstrate that CBTL provides a unified and practical approach for continual, flexible, active, and safe robot personalization. Website: https://emprise.cs.cornell.edu/cbtl/"
  },
  {
    "title": "Certified Neural Approximations of Nonlinear Dynamics",
    "url": "http://arxiv.org/abs/2505.15497v1",
    "arxiv_id": "2505.15497v1",
    "authors": [
      "Frederik Baymler Mathiesen",
      "Nikolaus Vertovec",
      "Francesco Fabiano",
      "Luca Laurenti",
      "Alessandro Abate"
    ],
    "published": "2025-05-21T13:22:20+00:00",
    "summary": "Neural networks hold great potential to act as approximate models of nonlinear dynamical systems, with the resulting neural approximations enabling verification and control of such systems. However, in safety-critical contexts, the use of neural approximations requires formal bounds on their closeness to the underlying system. To address this fundamental challenge, we propose a novel, adaptive, and parallelizable verification method based on certified first-order models. Our approach provides formal error bounds on the neural approximations of dynamical systems, allowing them to be safely employed as surrogates by interpreting the error bound as bounded disturbances acting on the approximated dynamics. We demonstrate the effectiveness and scalability of our method on a range of established benchmarks from the literature, showing that it outperforms the state-of-the-art. Furthermore, we highlight the flexibility of our framework by applying it to two novel scenarios not previously explored in this context: neural network compression and an autoencoder-based deep learning architecture for learning Koopman operators, both yielding compelling results."
  },
  {
    "title": "Comprehensive Evaluation and Analysis for NSFW Concept Erasure in Text-to-Image Diffusion Models",
    "url": "http://arxiv.org/abs/2505.15450v1",
    "arxiv_id": "2505.15450v1",
    "authors": [
      "Die Chen",
      "Zhiwen Li",
      "Cen Chen",
      "Yuexiang Xie",
      "Xiaodan Li",
      "Jinyan Ye",
      "Yingda Chen",
      "Yaliang Li"
    ],
    "published": "2025-05-21T12:31:45+00:00",
    "summary": "Text-to-image diffusion models have gained widespread application across various domains, demonstrating remarkable creative potential. However, the strong generalization capabilities of diffusion models can inadvertently lead to the generation of not-safe-for-work (NSFW) content, posing significant risks to their safe deployment. While several concept erasure methods have been proposed to mitigate the issue associated with NSFW content, a comprehensive evaluation of their effectiveness across various scenarios remains absent. To bridge this gap, we introduce a full-pipeline toolkit specifically designed for concept erasure and conduct the first systematic study of NSFW concept erasure methods. By examining the interplay between the underlying mechanisms and empirical observations, we provide in-depth insights and practical guidance for the effective application of concept erasure methods in various real-world scenarios, with the aim of advancing the understanding of content safety in diffusion models and establishing a solid foundation for future research and development in this critical area."
  },
  {
    "title": "Silent Leaks: Implicit Knowledge Extraction Attack on RAG Systems through Benign Queries",
    "url": "http://arxiv.org/abs/2505.15420v1",
    "arxiv_id": "2505.15420v1",
    "authors": [
      "Yuhao Wang",
      "Wenjie Qu",
      "Yanze Jiang",
      "Zichen Liu",
      "Yue Liu",
      "Shengfang Zhai",
      "Yinpeng Dong",
      "Jiaheng Zhang"
    ],
    "published": "2025-05-21T12:04:42+00:00",
    "summary": "Retrieval-Augmented Generation (RAG) systems enhance large language models (LLMs) by incorporating external knowledge bases, but they are vulnerable to privacy risks from data extraction attacks. Existing extraction methods typically rely on malicious inputs such as prompt injection or jailbreaking, making them easily detectable via input- or output-level detection. In this paper, we introduce Implicit Knowledge Extraction Attack (IKEA), which conducts knowledge extraction on RAG systems through benign queries. IKEA first leverages anchor concepts to generate queries with the natural appearance, and then designs two mechanisms to lead to anchor concept thoroughly 'explore' the RAG's privacy knowledge: (1) Experience Reflection Sampling, which samples anchor concepts based on past query-response patterns to ensure the queries' relevance to RAG documents; (2) Trust Region Directed Mutation, which iteratively mutates anchor concepts under similarity constraints to further exploit the embedding space. Extensive experiments demonstrate IKEA's effectiveness under various defenses, surpassing baselines by over 80% in extraction efficiency and 90% in attack success rate. Moreover, the substitute RAG system built from IKEA's extractions consistently outperforms those based on baseline methods across multiple evaluation tasks, underscoring the significant privacy risk in RAG systems."
  },
  {
    "title": "Audio Jailbreak: An Open Comprehensive Benchmark for Jailbreaking Large Audio-Language Models",
    "url": "http://arxiv.org/abs/2505.15406v1",
    "arxiv_id": "2505.15406v1",
    "authors": [
      "Zirui Song",
      "Qian Jiang",
      "Mingxuan Cui",
      "Mingzhe Li",
      "Lang Gao",
      "Zeyu Zhang",
      "Zixiang Xu",
      "Yanbo Wang",
      "Chenxi Wang",
      "Guangxian Ouyang",
      "Zhenhao Chen",
      "Xiuying Chen"
    ],
    "published": "2025-05-21T11:47:47+00:00",
    "summary": "The rise of Large Audio Language Models (LAMs) brings both potential and risks, as their audio outputs may contain harmful or unethical content. However, current research lacks a systematic, quantitative evaluation of LAM safety especially against jailbreak attacks, which are challenging due to the temporal and semantic nature of speech. To bridge this gap, we introduce AJailBench, the first benchmark specifically designed to evaluate jailbreak vulnerabilities in LAMs. We begin by constructing AJailBench-Base, a dataset of 1,495 adversarial audio prompts spanning 10 policy-violating categories, converted from textual jailbreak attacks using realistic text to speech synthesis. Using this dataset, we evaluate several state-of-the-art LAMs and reveal that none exhibit consistent robustness across attacks. To further strengthen jailbreak testing and simulate more realistic attack conditions, we propose a method to generate dynamic adversarial variants. Our Audio Perturbation Toolkit (APT) applies targeted distortions across time, frequency, and amplitude domains. To preserve the original jailbreak intent, we enforce a semantic consistency constraint and employ Bayesian optimization to efficiently search for perturbations that are both subtle and highly effective. This results in AJailBench-APT, an extended dataset of optimized adversarial audio samples. Our findings demonstrate that even small, semantically preserved perturbations can significantly reduce the safety performance of leading LAMs, underscoring the need for more robust and semantically aware defense mechanisms."
  },
  {
    "title": "How Should We Enhance the Safety of Large Reasoning Models: An Empirical Study",
    "url": "http://arxiv.org/abs/2505.15404v1",
    "arxiv_id": "2505.15404v1",
    "authors": [
      "Zhexin Zhang",
      "Xian Qi Loye",
      "Victor Shea-Jay Huang",
      "Junxiao Yang",
      "Qi Zhu",
      "Shiyao Cui",
      "Fei Mi",
      "Lifeng Shang",
      "Yingkang Wang",
      "Hongning Wang",
      "Minlie Huang"
    ],
    "published": "2025-05-21T11:45:29+00:00",
    "summary": "Large Reasoning Models (LRMs) have achieved remarkable success on reasoning-intensive tasks such as mathematics and programming. However, their enhanced reasoning capabilities do not necessarily translate to improved safety performance-and in some cases, may even degrade it. This raises an important research question: how can we enhance the safety of LRMs? In this paper, we present a comprehensive empirical study on how to enhance the safety of LRMs through Supervised Fine-Tuning (SFT). Our investigation begins with an unexpected observation: directly distilling safe responses from DeepSeek-R1 fails to significantly enhance safety. We analyze this phenomenon and identify three key failure patterns that contribute to it. We then demonstrate that explicitly addressing these issues during the data distillation process can lead to substantial safety improvements. Next, we explore whether a long and complex reasoning process is necessary for achieving safety. Interestingly, we find that simply using short or template-based reasoning process can attain comparable safety performance-and are significantly easier for models to learn than more intricate reasoning chains. These findings prompt a deeper reflection on the role of reasoning in ensuring safety. Finally, we find that mixing math reasoning data during safety fine-tuning is helpful to balance safety and over-refusal. Overall, we hope our empirical study could provide a more holistic picture on enhancing the safety of LRMs. The code and data used in our experiments are released in https://github.com/thu-coai/LRM-Safety-Study."
  },
  {
    "title": "When to Continue Thinking: Adaptive Thinking Mode Switching for Efficient Reasoning",
    "url": "http://arxiv.org/abs/2505.15400v1",
    "arxiv_id": "2505.15400v1",
    "authors": [
      "Xiaoyun Zhang",
      "Jingqing Ruan",
      "Xing Ma",
      "Yawen Zhu",
      "Haodong Zhao",
      "Hao Li",
      "Jiansong Chen",
      "Ke Zeng",
      "Xunliang Cai"
    ],
    "published": "2025-05-21T11:41:39+00:00",
    "summary": "Large reasoning models (LRMs) achieve remarkable performance via long reasoning chains, but often incur excessive computational overhead due to redundant reasoning, especially on simple tasks. In this work, we systematically quantify the upper bounds of LRMs under both Long-Thinking and No-Thinking modes, and uncover the phenomenon of \"Internal Self-Recovery Mechanism\" where models implicitly supplement reasoning during answer generation. Building on this insight, we propose Adaptive Self-Recovery Reasoning (ASRR), a framework that suppresses unnecessary reasoning and enables implicit recovery. By introducing accuracy-aware length reward regulation, ASRR adaptively allocates reasoning effort according to problem difficulty, achieving high efficiency with negligible performance sacrifice. Experiments across multiple benchmarks and models show that, compared with GRPO, ASRR reduces reasoning budget by up to 32.5% (1.5B) and 25.7% (7B) with minimal accuracy loss (1.2% and 0.6% pass@1), and significantly boosts harmless rates on safety benchmarks (up to +21.7%). Our results highlight the potential of ASRR for enabling efficient, adaptive, and safer reasoning in LRMs."
  },
  {
    "title": "FAV-NSS: An HIL Framework for Accelerating Validation of Automotive Network Security Strategies",
    "url": "http://arxiv.org/abs/2505.15393v1",
    "arxiv_id": "2505.15393v1",
    "authors": [
      "Changhong Li",
      "Shashwat Khandelwal",
      "Shreejith Shanker"
    ],
    "published": "2025-05-21T11:34:48+00:00",
    "summary": "Complex electronic control unit (ECU) architectures, software models and in-vehicle networks are consistently improving safety and comfort functions in modern vehicles. However, the extended functionality and increased connectivity introduce new security risks and vulnerabilities that can be exploited on legacy automotive networks such as the controller area network (CAN). With the rising complexity of vehicular systems and attack vectors, the need for a flexible hardware-in-the-loop (HIL) test fixture that can inject attacks and validate the performance of countermeasures in near-real-world conditions in real time is vital. This paper presents an FPGA-based HIL framework tailored towards validating network security approaches (IDS, IPS) and smart integration strategies of such capabilities for an automotive CAN bus. FAV-NSS replicates an actual vehicular system environment with functional ECUs and network infrastructure on an FPGA, allowing functional validation of IDS/IPS algorithms, accelerator designs and integration schemes (software task on ECU, dedicated accelerator). To show the efficacy of FAV-NSS, we evaluate an IDS accelerator integration problem, both as a traditional coupled accelerator (to the ECU), and secondly close to the CAN controller (mimicking an extended CAN controller). We show that the latter strategy can be fully validated by our framework, which would otherwise require integration of specialised CAN modules into otherwise standard HIL fixtures with ability to instrument internal signals for characterising timing performance. The tests demonstrate a promising latency reduction of 6.3x when compared to the traditional coupled accelerator. Our case study demonstrates the potential of FAV-NSS for accelerating the optimisation, integration and verification of smart ECUs and communication controllers in current and future vehicular systems."
  },
  {
    "title": "Are Vision-Language Models Safe in the Wild? A Meme-Based Benchmark Study",
    "url": "http://arxiv.org/abs/2505.15389v1",
    "arxiv_id": "2505.15389v1",
    "authors": [
      "DongGeon Lee",
      "Joonwon Jang",
      "Jihae Jeong",
      "Hwanjo Yu"
    ],
    "published": "2025-05-21T11:26:40+00:00",
    "summary": "Rapid deployment of vision-language models (VLMs) magnifies safety risks, yet most evaluations rely on artificial images. This study asks: How safe are current VLMs when confronted with meme images that ordinary users share? To investigate this question, we introduce MemeSafetyBench, a 50,430-instance benchmark pairing real meme images with both harmful and benign instructions. Using a comprehensive safety taxonomy and LLM-based instruction generation, we assess multiple VLMs across single and multi-turn interactions. We investigate how real-world memes influence harmful outputs, the mitigating effects of conversational context, and the relationship between model scale and safety metrics. Our findings demonstrate that VLMs show greater vulnerability to meme-based harmful prompts than to synthetic or typographic images. Memes significantly increase harmful responses and decrease refusals compared to text-only inputs. Though multi-turn interactions provide partial mitigation, elevated vulnerability persists. These results highlight the need for ecologically valid evaluations and stronger safety mechanisms."
  },
  {
    "title": "Better Safe Than Sorry? Overreaction Problem of Vision Language Models in Visual Emergency Recognition",
    "url": "http://arxiv.org/abs/2505.15367v1",
    "arxiv_id": "2505.15367v1",
    "authors": [
      "Dasol Choi",
      "Seunghyun Lee",
      "Youngsook Song"
    ],
    "published": "2025-05-21T10:57:40+00:00",
    "summary": "Vision-Language Models (VLMs) have demonstrated impressive capabilities in understanding visual content, but their reliability in safety-critical contexts remains under-explored. We introduce VERI (Visual Emergency Recognition Dataset), a carefully designed diagnostic benchmark of 200 images (100 contrastive pairs). Each emergency scene is matched with a visually similar but safe counterpart through multi-stage human verification and iterative refinement. Using a two-stage protocol - risk identification and emergency response - we evaluate 14 VLMs (2B-124B parameters) across medical emergencies, accidents, and natural disasters. Our analysis reveals a systematic overreaction problem: models excel at identifying real emergencies (70-100 percent success rate) but suffer from an alarming rate of false alarms, misidentifying 31-96 percent of safe situations as dangerous, with 10 scenarios failed by all models regardless of scale. This \"better-safe-than-sorry\" bias manifests primarily through contextual overinterpretation (88-93 percent of errors), challenging VLMs' reliability for safety applications. These findings highlight persistent limitations that are not resolved by increasing model scale, motivating targeted approaches for improving contextual safety assessment in visually misleading scenarios."
  },
  {
    "title": "AI vs. Human Judgment of Content Moderation: LLM-as-a-Judge and Ethics-Based Response Refusals",
    "url": "http://arxiv.org/abs/2505.15365v1",
    "arxiv_id": "2505.15365v1",
    "authors": [
      "Stefan Pasch"
    ],
    "published": "2025-05-21T10:56:16+00:00",
    "summary": "As large language models (LLMs) are increasingly deployed in high-stakes settings, their ability to refuse ethically sensitive prompts-such as those involving hate speech or illegal activities-has become central to content moderation and responsible AI practices. While refusal responses can be viewed as evidence of ethical alignment and safety-conscious behavior, recent research suggests that users may perceive them negatively. At the same time, automated assessments of model outputs are playing a growing role in both evaluation and training. In particular, LLM-as-a-Judge frameworks-in which one model is used to evaluate the output of another-are now widely adopted to guide benchmarking and fine-tuning. This paper examines whether such model-based evaluators assess refusal responses differently than human users. Drawing on data from Chatbot Arena and judgments from two AI judges (GPT-4o and Llama 3 70B), we compare how different types of refusals are rated. We distinguish ethical refusals, which explicitly cite safety or normative concerns (e.g., \"I can't help with that because it may be harmful\"), and technical refusals, which reflect system limitations (e.g., \"I can't answer because I lack real-time data\"). We find that LLM-as-a-Judge systems evaluate ethical refusals significantly more favorably than human users, a divergence not observed for technical refusals. We refer to this divergence as a moderation bias-a systematic tendency for model-based evaluators to reward refusal behaviors more than human users do. This raises broader questions about transparency, value alignment, and the normative assumptions embedded in automated evaluation systems."
  },
  {
    "title": "Objective Bicycle Occlusion Level Classification using a Deformable Parts-Based Model",
    "url": "http://arxiv.org/abs/2505.15358v1",
    "arxiv_id": "2505.15358v1",
    "authors": [
      "Angelique Mangubat",
      "Shane Gilroy"
    ],
    "published": "2025-05-21T10:42:41+00:00",
    "summary": "Road safety is a critical challenge, particularly for cyclists, who are among the most vulnerable road users. This study aims to enhance road safety by proposing a novel benchmark for bicycle occlusion level classification using advanced computer vision techniques. Utilizing a parts-based detection model, images are annotated and processed through a custom image detection pipeline. A novel method of bicycle occlusion level is proposed to objectively quantify the visibility and occlusion level of bicycle semantic parts. The findings indicate that the model robustly quantifies the visibility and occlusion level of bicycles, a significant improvement over the subjective methods used by the current state of the art. Widespread use of the proposed methodology will facilitate the accurate performance reporting of cyclist detection algorithms for occluded cyclists, informing the development of more robust vulnerable road user detection methods for autonomous vehicles."
  },
  {
    "title": "Improving LLM First-Token Predictions in Multiple-Choice Question Answering via Prefilling Attack",
    "url": "http://arxiv.org/abs/2505.15323v1",
    "arxiv_id": "2505.15323v1",
    "authors": [
      "Silvia Cappelletti",
      "Tobia Poppi",
      "Samuele Poppi",
      "Zheng-Xin Yong",
      "Diego Garcia-Olano",
      "Marcella Cornia",
      "Lorenzo Baraldi",
      "Rita Cucchiara"
    ],
    "published": "2025-05-21T09:58:38+00:00",
    "summary": "Large Language Models (LLMs) are increasingly evaluated on multiple-choice question answering (MCQA) tasks using *first-token probability* (FTP), which selects the answer option whose initial token has the highest likelihood. While efficient, FTP can be fragile: models may assign high probability to unrelated tokens (*misalignment*) or use a valid token merely as part of a generic preamble rather than as a clear answer choice (*misinterpretation*), undermining the reliability of symbolic evaluation. We propose a simple solution: the *prefilling attack*, a structured natural-language prefix (e.g., \"*The correct option is:*\") prepended to the model output. Originally explored in AI safety, we repurpose prefilling to steer the model to respond with a clean, valid option, without modifying its parameters. Empirically, the FTP with prefilling strategy substantially improves accuracy, calibration, and output consistency across a broad set of LLMs and MCQA benchmarks. It outperforms standard FTP and often matches the performance of open-ended generation approaches that require full decoding and external classifiers, while being significantly more efficient. Our findings suggest that prefilling is a simple, robust, and low-cost method to enhance the reliability of FTP-based evaluation in multiple-choice settings."
  },
  {
    "title": "AgentThink: A Unified Framework for Tool-Augmented Chain-of-Thought Reasoning in Vision-Language Models for Autonomous Driving",
    "url": "http://arxiv.org/abs/2505.15298v1",
    "arxiv_id": "2505.15298v1",
    "authors": [
      "Kangan Qian",
      "Sicong Jiang",
      "Yang Zhong",
      "Ziang Luo",
      "Zilin Huang",
      "Tianze Zhu",
      "Kun Jiang",
      "Mengmeng Yang",
      "Zheng Fu",
      "Jinyu Miao",
      "Yining Shi",
      "He Zhe Lim",
      "Li Liu",
      "Tianbao Zhou",
      "Hongyi Wang",
      "Huang Yu",
      "Yifei Hu",
      "Guang Li",
      "Guang Chen",
      "Hao Ye",
      "Lijun Sun",
      "Diange Yang"
    ],
    "published": "2025-05-21T09:27:43+00:00",
    "summary": "Vision-Language Models (VLMs) show promise for autonomous driving, yet their struggle with hallucinations, inefficient reasoning, and limited real-world validation hinders accurate perception and robust step-by-step reasoning. To overcome this, we introduce \\textbf{AgentThink}, a pioneering unified framework that, for the first time, integrates Chain-of-Thought (CoT) reasoning with dynamic, agent-style tool invocation for autonomous driving tasks. AgentThink's core innovations include: \\textbf{(i) Structured Data Generation}, by establishing an autonomous driving tool library to automatically construct structured, self-verified reasoning data explicitly incorporating tool usage for diverse driving scenarios; \\textbf{(ii) A Two-stage Training Pipeline}, employing Supervised Fine-Tuning (SFT) with Group Relative Policy Optimization (GRPO) to equip VLMs with the capability for autonomous tool invocation; and \\textbf{(iii) Agent-style Tool-Usage Evaluation}, introducing a novel multi-tool assessment protocol to rigorously evaluate the model's tool invocation and utilization. Experiments on the DriveLMM-o1 benchmark demonstrate AgentThink significantly boosts overall reasoning scores by \\textbf{53.91\\%} and enhances answer accuracy by \\textbf{33.54\\%}, while markedly improving reasoning quality and consistency. Furthermore, ablation studies and robust zero-shot/few-shot generalization experiments across various benchmarks underscore its powerful capabilities. These findings highlight a promising trajectory for developing trustworthy and tool-aware autonomous driving models."
  },
  {
    "title": "Chinese Toxic Language Mitigation via Sentiment Polarity Consistent Rewrites",
    "url": "http://arxiv.org/abs/2505.15297v1",
    "arxiv_id": "2505.15297v1",
    "authors": [
      "Xintong Wang",
      "Yixiao Liu",
      "Jingheng Pan",
      "Liang Ding",
      "Longyue Wang",
      "Chris Biemann"
    ],
    "published": "2025-05-21T09:27:18+00:00",
    "summary": "Detoxifying offensive language while preserving the speaker's original intent is a challenging yet critical goal for improving the quality of online interactions. Although large language models (LLMs) show promise in rewriting toxic content, they often default to overly polite rewrites, distorting the emotional tone and communicative intent. This problem is especially acute in Chinese, where toxicity often arises implicitly through emojis, homophones, or discourse context. We present ToxiRewriteCN, the first Chinese detoxification dataset explicitly designed to preserve sentiment polarity. The dataset comprises 1,556 carefully annotated triplets, each containing a toxic sentence, a sentiment-aligned non-toxic rewrite, and labeled toxic spans. It covers five real-world scenarios: standard expressions, emoji-induced and homophonic toxicity, as well as single-turn and multi-turn dialogues. We evaluate 17 LLMs, including commercial and open-source models with variant architectures, across four dimensions: detoxification accuracy, fluency, content preservation, and sentiment polarity. Results show that while commercial and MoE models perform best overall, all models struggle to balance safety with emotional fidelity in more subtle or context-heavy settings such as emoji, homophone, and dialogue-based inputs. We release ToxiRewriteCN to support future research on controllable, sentiment-aware detoxification for Chinese."
  },
  {
    "title": "Learning-based Autonomous Oversteer Control and Collision Avoidance",
    "url": "http://arxiv.org/abs/2505.15275v1",
    "arxiv_id": "2505.15275v1",
    "authors": [
      "Seokjun Lee",
      "Seung-Hyun Kong"
    ],
    "published": "2025-05-21T08:53:38+00:00",
    "summary": "Oversteer, wherein a vehicle's rear tires lose traction and induce unintentional excessive yaw, poses critical safety challenges. Failing to control oversteer often leads to severe traffic accidents. Although recent autonomous driving efforts have attempted to handle oversteer through stabilizing maneuvers, the majority rely on expert-defined trajectories or assume obstacle-free environments, limiting real-world applicability. This paper introduces a novel end-to-end (E2E) autonomous driving approach that tackles oversteer control and collision avoidance simultaneously. Existing E2E techniques, including Imitation Learning (IL), Reinforcement Learning (RL), and Hybrid Learning (HL), generally require near-optimal demonstrations or extensive experience. Yet even skilled human drivers struggle to provide perfect demonstrations under oversteer, and high transition variance hinders accumulating sufficient data. Hence, we present Q-Compared Soft Actor-Critic (QC-SAC), a new HL algorithm that effectively learns from suboptimal demonstration data and adapts rapidly to new conditions. To evaluate QC-SAC, we introduce a benchmark inspired by real-world driver training: a vehicle encounters sudden oversteer on a slippery surface and must avoid randomly placed obstacles ahead. Experimental results show QC-SAC attains near-optimal driving policies, significantly surpassing state-of-the-art IL, RL, and HL baselines. Our method demonstrates the world's first safe autonomous oversteer control with obstacle avoidance."
  },
  {
    "title": "iPad: Iterative Proposal-centric End-to-End Autonomous Driving",
    "url": "http://arxiv.org/abs/2505.15111v1",
    "arxiv_id": "2505.15111v1",
    "authors": [
      "Ke Guo",
      "Haochen Liu",
      "Xiaojun Wu",
      "Jia Pan",
      "Chen Lv"
    ],
    "published": "2025-05-21T05:05:38+00:00",
    "summary": "End-to-end (E2E) autonomous driving systems offer a promising alternative to traditional modular pipelines by reducing information loss and error accumulation, with significant potential to enhance both mobility and safety. However, most existing E2E approaches directly generate plans based on dense bird's-eye view (BEV) grid features, leading to inefficiency and limited planning awareness. To address these limitations, we propose iterative Proposal-centric autonomous driving (iPad), a novel framework that places proposals - a set of candidate future plans - at the center of feature extraction and auxiliary tasks. Central to iPad is ProFormer, a BEV encoder that iteratively refines proposals and their associated features through proposal-anchored attention, effectively fusing multi-view image data. Additionally, we introduce two lightweight, proposal-centric auxiliary tasks - mapping and prediction - that improve planning quality with minimal computational overhead. Extensive experiments on the NAVSIM and CARLA Bench2Drive benchmarks demonstrate that iPad achieves state-of-the-art performance while being significantly more efficient than prior leading methods."
  },
  {
    "title": "Comparing Parameterizations and Objective Functions for Maximizing the Volume of Zonotopic Invariant Sets",
    "url": "http://arxiv.org/abs/2505.15109v1",
    "arxiv_id": "2505.15109v1",
    "authors": [
      "Chenliang Zhou",
      "Heejin Ahn",
      "Ian M. Mitchell"
    ],
    "published": "2025-05-21T05:02:28+00:00",
    "summary": "In formal safety verification, many proposed algorithms use parametric set representations and convert the computation of the relevant sets into an optimization problem; consequently, the choice of parameterization and objective function have a significant impact on the efficiency and accuracy of the resulting computation. In particular, recent papers have explored the use of zonotope set representations for various types of invariant sets. In this paper we collect two zonotope parameterizations that are numerically well-behaved and demonstrate that the volume of the corresponding zonotopes is log-concave in the parameters. We then experimentally explore the use of these two parameterizations in an algorithm for computing the maximum volume zonotope invariant under affine dynamics within a specified box constraint over a finite horizon. The true volume of the zonotopes is used as an objective function, along with two alternative heuristics that are faster to compute. We conclude that the heuristics are much faster in practice, although the relative quality of their results declines as the dimension of the problem increases; however, our conclusions are only preliminary due to so-far limited availability of compute resources."
  },
  {
    "title": "MolLangBench: A Comprehensive Benchmark for Language-Prompted Molecular Structure Recognition, Editing, and Generation",
    "url": "http://arxiv.org/abs/2505.15054v1",
    "arxiv_id": "2505.15054v1",
    "authors": [
      "Feiyang Cai",
      "Jiahui Bai",
      "Tao Tang",
      "Joshua Luo",
      "Tianyu Zhu",
      "Ling Liu",
      "Feng Luo"
    ],
    "published": "2025-05-21T03:22:01+00:00",
    "summary": "Precise recognition, editing, and generation of molecules are essential prerequisites for both chemists and AI systems tackling various chemical tasks. We present MolLangBench, a comprehensive benchmark designed to evaluate fundamental molecule-language interface tasks: language-prompted molecular structure recognition, editing, and generation. To ensure high-quality, unambiguous, and deterministic outputs, we construct the recognition tasks using automated cheminformatics tools, and curate editing and generation tasks through rigorous expert annotation and validation. MolLangBench supports the evaluation of models that interface language with different molecular representations, including linear strings, molecular images, and molecular graphs. Evaluations of state-of-the-art models reveal significant limitations: the strongest model (o3) achieves $79.2\\%$ and $78.5\\%$ accuracy on recognition and editing tasks, which are intuitively simple for humans, and performs even worse on the generation task, reaching only $29.0\\%$ accuracy. These results highlight the shortcomings of current AI systems in handling even preliminary molecular recognition and manipulation tasks. We hope MolLangBench will catalyze further research toward more effective and reliable AI systems for chemical applications."
  },
  {
    "title": "HAVA: Hybrid Approach to Value-Alignment through Reward Weighing for Reinforcement Learning",
    "url": "http://arxiv.org/abs/2505.15011v1",
    "arxiv_id": "2505.15011v1",
    "authors": [
      "Kryspin Varys",
      "Federico Cerutti",
      "Adam Sobey",
      "Timothy J. Norman"
    ],
    "published": "2025-05-21T01:32:54+00:00",
    "summary": "Our society is governed by a set of norms which together bring about the values we cherish such as safety, fairness or trustworthiness. The goal of value-alignment is to create agents that not only do their tasks but through their behaviours also promote these values. Many of the norms are written as laws or rules (legal / safety norms) but even more remain unwritten (social norms). Furthermore, the techniques used to represent these norms also differ. Safety / legal norms are often represented explicitly, for example, in some logical language while social norms are typically learned and remain hidden in the parameter space of a neural network. There is a lack of approaches in the literature that could combine these various norm representations into a single algorithm. We propose a novel method that integrates these norms into the reinforcement learning process. Our method monitors the agent's compliance with the given norms and summarizes it in a quantity we call the agent's reputation. This quantity is used to weigh the received rewards to motivate the agent to become value-aligned. We carry out a series of experiments including a continuous state space traffic problem to demonstrate the importance of the written and unwritten norms and show how our method can find the value-aligned policies. Furthermore, we carry out ablations to demonstrate why it is better to combine these two groups of norms rather than using either separately."
  },
  {
    "title": "UniSTPA: A Safety Analysis Framework for End-to-End Autonomous Driving",
    "url": "http://arxiv.org/abs/2505.15005v1",
    "arxiv_id": "2505.15005v1",
    "authors": [
      "Hongrui Kou",
      "Zhouhang Lyu",
      "Ziyu Wang",
      "Cheng Wang",
      "Yuxin Zhang"
    ],
    "published": "2025-05-21T01:23:31+00:00",
    "summary": "As autonomous driving technology continues to advance, end-to-end models have attracted considerable attention owing to their superior generalisation capability. Nevertheless, such learning-based systems entail numerous safety risks throughout development and on-road deployment, and existing safety-analysis methods struggle to identify these risks comprehensively. To address this gap, we propose the Unified System Theoretic Process Analysis (UniSTPA) framework, which extends the scope of STPA from the operational phase to the entire lifecycle of an end-to-end autonomous driving system, including information gathering, data preparation, closed loop training, verification, and deployment. UniSTPA performs hazard analysis not only at the component level but also within the model's internal layers, thereby enabling fine-grained assessment of inter and intra module interactions. Using a highway Navigate on Autopilot function as a case study, UniSTPA uncovers multi-stage hazards overlooked by conventional approaches including scene design defects, sensor fusion biases, and internal model flaws, through multi-level causal analysis, traces these hazards to deeper issues such as data quality, network architecture, and optimisation objectives. The analysis result are used to construct a safety monitoring and safety response mechanism that supports continuous improvement from hazard identification to system optimisation. The proposed framework thus offers both theoretical and practical guidance for the safe development and deployment of end-to-end autonomous driving systems."
  },
  {
    "title": "Multimodal Cultural Safety: Evaluation Frameworks and Alignment Strategies",
    "url": "http://arxiv.org/abs/2505.14972v1",
    "arxiv_id": "2505.14972v1",
    "authors": [
      "Haoyi Qiu",
      "Kung-Hsiang Huang",
      "Ruichen Zheng",
      "Jiao Sun",
      "Nanyun Peng"
    ],
    "published": "2025-05-20T23:20:38+00:00",
    "summary": "Large vision-language models (LVLMs) are increasingly deployed in globally distributed applications, such as tourism assistants, yet their ability to produce culturally appropriate responses remains underexplored. Existing multimodal safety benchmarks primarily focus on physical safety and overlook violations rooted in cultural norms, which can result in symbolic harm. To address this gap, we introduce CROSS, a benchmark designed to assess the cultural safety reasoning capabilities of LVLMs. CROSS includes 1,284 multilingual visually grounded queries from 16 countries, three everyday domains, and 14 languages, where cultural norm violations emerge only when images are interpreted in context. We propose CROSS-Eval, an intercultural theory-based framework that measures four key dimensions: cultural awareness, norm education, compliance, and helpfulness. Using this framework, we evaluate 21 leading LVLMs, including mixture-of-experts models and reasoning models. Results reveal significant cultural safety gaps: the best-performing model achieves only 61.79% in awareness and 37.73% in compliance. While some open-source models reach GPT-4o-level performance, they still fall notably short of proprietary models. Our results further show that increasing reasoning capacity improves cultural alignment but does not fully resolve the issue. To improve model performance, we develop two enhancement strategies: supervised fine-tuning with culturally grounded, open-ended data and preference tuning with contrastive response pairs that highlight safe versus unsafe behaviors. These methods substantially improve GPT-4o's cultural awareness (+60.14%) and compliance (+55.2%), while preserving general multimodal capabilities with minimal performance reduction on general multimodal understanding benchmarks."
  },
  {
    "title": "PINCH: Pipeline-Informed Noise Characterization of Gravitational Wave Candidates",
    "url": "http://arxiv.org/abs/2505.14949v1",
    "arxiv_id": "2505.14949v1",
    "authors": [
      "Zach Yarbrough",
      "Andre Guimaraes",
      "Prathamesh Joshi",
      "Gabriela Gonz\u00e1lez",
      "Andrew Valentini",
      "Urja Shah"
    ],
    "published": "2025-05-20T22:21:38+00:00",
    "summary": "We present a method to identify and categorize gravitational wave candidate triggers identified by matched filtering gravitational wave searches (pipelines) caused by transient noise (glitches) in gravitational wave detectors using Support Vector Machine (SVM) classifiers. Our approach involves training SVM models on pipeline triggers which occur outside periods of excess noise to distinguish between triggers caused by random noise and those induced by glitches. This method is applied independently to the triggers produced by the GstLAL search pipeline on data from the LIGO Hanford and Livingston observatories during the second half of the O3 observing run. The trained SVM models assign scores to ambiguous triggers, quantifying their similarity to triggers caused by random fluctuations, with triggers with scores above a defined threshold being classified as glitch-induced. Analysis of these triggers reveals the distinct impact of different glitch classes on the search pipeline, including their distribution in relevant parameter spaces. We use metrics such as the Bhattacharyya coefficient and an over-representation ratio to quantify the consistency and prevalence of glitch impacts over time and across parameter spaces. Our findings demonstrate that certain glitch types disproportionately affect specific regions of the parameter space, and that certain glitch types manifest themselves in the eyes of the pipeline in consistent ways while other types vary significantly. This method provides a framework for understanding and mitigating the influence of non-Gaussian transients on gravitational wave search pipelines, with implications for improving detection sensitivity and better understanding noise populations."
  },
  {
    "title": "Reinforcement Learning from User Feedback",
    "url": "http://arxiv.org/abs/2505.14946v1",
    "arxiv_id": "2505.14946v1",
    "authors": [
      "Eric Han",
      "Jun Chen",
      "Karthik Abinav Sankararaman",
      "Xiaoliang Peng",
      "Tengyu Xu",
      "Eryk Helenowski",
      "Kaiyan Peng",
      "Mrinal Kumar",
      "Sinong Wang",
      "Han Fang",
      "Arya Talebzadeh"
    ],
    "published": "2025-05-20T22:14:44+00:00",
    "summary": "As large language models (LLMs) are increasingly deployed in diverse user facing applications, aligning them with real user preferences becomes essential. Existing methods like Reinforcement Learning from Human Feedback (RLHF) rely on expert annotators trained on manually defined guidelines, whose judgments may not reflect the priorities of everyday users. We introduce Reinforcement Learning from User Feedback (RLUF), a framework for aligning LLMs directly to implicit signals from users in production. RLUF addresses key challenges of user feedback: user feedback is often binary (e.g., emoji reactions), sparse, and occasionally adversarial. We train a reward model, P[Love], to predict the likelihood that an LLM response will receive a Love Reaction, a lightweight form of positive user feedback, and integrate P[Love] into a multi-objective policy optimization framework alongside helpfulness and safety objectives. In large-scale experiments, we show that P[Love] is predictive of increased positive feedback and serves as a reliable offline evaluator of future user behavior. Policy optimization using P[Love] significantly raises observed positive-feedback rates, including a 28% increase in Love Reactions during live A/B tests. However, optimizing for positive reactions introduces reward hacking challenges, requiring careful balancing of objectives. By directly leveraging implicit signals from users, RLUF offers a path to aligning LLMs with real-world user preferences at scale."
  },
  {
    "title": "Foundations of Unknown-aware Machine Learning",
    "url": "http://arxiv.org/abs/2505.14933v1",
    "arxiv_id": "2505.14933v1",
    "authors": [
      "Xuefeng Du"
    ],
    "published": "2025-05-20T21:39:08+00:00",
    "summary": "Ensuring the reliability and safety of machine learning models in open-world deployment is a central challenge in AI safety. This thesis develops both algorithmic and theoretical foundations to address key reliability issues arising from distributional uncertainty and unknown classes, from standard neural networks to modern foundation models like large language models (LLMs).   Traditional learning paradigms, such as empirical risk minimization (ERM), assume no distribution shift between training and inference, often leading to overconfident predictions on out-of-distribution (OOD) inputs. This thesis introduces novel frameworks that jointly optimize for in-distribution accuracy and reliability to unseen data. A core contribution is the development of an unknown-aware learning framework that enables models to recognize and handle novel inputs without labeled OOD data.   We propose new outlier synthesis methods, VOS, NPOS, and DREAM-OOD, to generate informative unknowns during training. Building on this, we present SAL, a theoretical and algorithmic framework that leverages unlabeled in-the-wild data to enhance OOD detection under realistic deployment conditions. These methods demonstrate that abundant unlabeled data can be harnessed to recognize and adapt to unforeseen inputs, providing formal reliability guarantees.   The thesis also extends reliable learning to foundation models. We develop HaloScope for hallucination detection in LLMs, MLLMGuard for defending against malicious prompts in multimodal models, and data cleaning methods to denoise human feedback used for better alignment. These tools target failure modes that threaten the safety of large-scale models in deployment.   Overall, these contributions promote unknown-aware learning as a new paradigm, and we hope it can advance the reliability of AI systems with minimal human efforts."
  },
  {
    "title": "Reliable Decision Support with LLMs: A Framework for Evaluating Consistency in Binary Text Classification Applications",
    "url": "http://arxiv.org/abs/2505.14918v1",
    "arxiv_id": "2505.14918v1",
    "authors": [
      "Fadel M. Megahed",
      "Ying-Ju Chen",
      "L. Allision Jones-Farmer",
      "Younghwa Lee",
      "Jiawei Brooke Wang",
      "Inez M. Zwetsloot"
    ],
    "published": "2025-05-20T21:12:58+00:00",
    "summary": "This study introduces a framework for evaluating consistency in large language model (LLM) binary text classification, addressing the lack of established reliability assessment methods. Adapting psychometric principles, we determine sample size requirements, develop metrics for invalid responses, and evaluate intra- and inter-rater reliability. Our case study examines financial news sentiment classification across 14 LLMs (including claude-3-7-sonnet, gpt-4o, deepseek-r1, gemma3, llama3.2, phi4, and command-r-plus), with five replicates per model on 1,350 articles. Models demonstrated high intra-rater consistency, achieving perfect agreement on 90-98% of examples, with minimal differences between expensive and economical models from the same families. When validated against StockNewsAPI labels, models achieved strong performance (accuracy 0.76-0.88), with smaller models like gemma3:1B, llama3.2:3B, and claude-3-5-haiku outperforming larger counterparts. All models performed at chance when predicting actual market movements, indicating task constraints rather than model limitations. Our framework provides systematic guidance for LLM selection, sample size planning, and reliability assessment, enabling organizations to optimize resources for classification tasks."
  },
  {
    "title": "Multi-Channel Swin Transformer Framework for Bearing Remaining Useful Life Prediction",
    "url": "http://arxiv.org/abs/2505.14897v1",
    "arxiv_id": "2505.14897v1",
    "authors": [
      "Ali Mohajerzarrinkelk",
      "Maryam Ahang",
      "Mehran Zoravar",
      "Mostafa Abbasi",
      "Homayoun Najjaran"
    ],
    "published": "2025-05-20T20:44:38+00:00",
    "summary": "Precise estimation of the Remaining Useful Life (RUL) of rolling bearings is an important consideration to avoid unexpected failures, reduce downtime, and promote safety and efficiency in industrial systems. Complications in degradation trends, noise presence, and the necessity to detect faults in advance make estimation of RUL a challenging task. This paper introduces a novel framework that combines wavelet-based denoising method, Wavelet Packet Decomposition (WPD), and a customized multi-channel Swin Transformer model (MCSFormer) to address these problems. With attention mechanisms incorporated for feature fusion, the model is designed to learn global and local degradation patterns utilizing hierarchical representations for enhancing predictive performance. Additionally, a customized loss function is developed as a key distinction of this work to differentiate between early and late predictions, prioritizing accurate early detection and minimizing the high operation risks of late predictions. The proposed model was evaluated with the PRONOSTIA dataset using three experiments. Intra-condition experiments demonstrated that MCSFormer outperformed state-of-the-art models, including the Adaptive Transformer, MDAN, and CNN-SRU, achieving 41%, 64%, and 69% lower MAE on average across different operating conditions, respectively. In terms of cross-condition testing, it achieved superior generalization under varying operating conditions compared to the adapted ViT and Swin Transformer. Lastly, the custom loss function effectively reduced late predictions, as evidenced in a 6.3% improvement in the scoring metric while maintaining competitive overall performance. The model's robust noise resistance, generalization capability, and focus on safety make MCSFormer a trustworthy and effective predictive maintenance tool in industrial applications."
  },
  {
    "title": "Looking for an out: Affordances, uncertainty and collision avoidance behavior of human drivers",
    "url": "http://arxiv.org/abs/2505.14842v1",
    "arxiv_id": "2505.14842v1",
    "authors": [
      "Leif Johnson",
      "Johan Engstr\u00f6m",
      "Aravinda Srinivasan",
      "Ibrahim \u00d6zturk",
      "Gustav Markkula"
    ],
    "published": "2025-05-20T19:14:03+00:00",
    "summary": "Understanding collision avoidance behavior is of key importance in traffic safety research and for designing and evaluating advanced driver assistance systems and autonomous vehicles. While existing experimental work has primarily focused on response timing in traffic conflicts, the goal of the present study was to gain a better understanding of human evasive maneuver decisions and execution in collision avoidance scenarios. To this end, we designed a driving simulator study where participants were exposed to one of three surprising opposite direction lateral incursion (ODLI) scenario variants. The results demonstrated that both the participants' collision avoidance behavior patterns and the collision outcome was strongly determined by the scenario kinematics and, more specifically, by the uncertainty associated with the oncoming vehicle's future trajectory. We discuss pitfalls related to hindsight bias when judging the quality of evasive maneuvers in uncertain situations and suggest that the availability of escape paths in collision avoidance scenarios can be usefully understood based on the notion of affordances, and further demonstrate how such affordances can be operationalized in terms of reachable sets. We conclude by discussing how these results can be used to inform computational models of collision avoidance behavior."
  },
  {
    "title": "Two Experts Are All You Need for Steering Thinking: Reinforcing Cognitive Effort in MoE Reasoning Models Without Additional Training",
    "url": "http://arxiv.org/abs/2505.14681v1",
    "arxiv_id": "2505.14681v1",
    "authors": [
      "Mengru Wang",
      "Xingyu Chen",
      "Yue Wang",
      "Zhiwei He",
      "Jiahao Xu",
      "Tian Liang",
      "Qiuzhi Liu",
      "Yunzhi Yao",
      "Wenxuan Wang",
      "Ruotian Ma",
      "Haitao Mi",
      "Ningyu Zhang",
      "Zhaopeng Tu",
      "Xiaolong Li",
      "Dong Yu"
    ],
    "published": "2025-05-20T17:59:16+00:00",
    "summary": "Mixture-of-Experts (MoE) architectures within Large Reasoning Models (LRMs) have achieved impressive reasoning capabilities by selectively activating experts to facilitate structured cognitive processes. Despite notable advances, existing reasoning models often suffer from cognitive inefficiencies like overthinking and underthinking. To address these limitations, we introduce a novel inference-time steering methodology called Reinforcing Cognitive Experts (RICE), designed to improve reasoning performance without additional training or complex heuristics. Leveraging normalized Pointwise Mutual Information (nPMI), we systematically identify specialized experts, termed ''cognitive experts'' that orchestrate meta-level reasoning operations characterized by tokens like ''<think>''. Empirical evaluations with leading MoE-based LRMs (DeepSeek-R1 and Qwen3-235B) on rigorous quantitative and scientific reasoning benchmarks demonstrate noticeable and consistent improvements in reasoning accuracy, cognitive efficiency, and cross-domain generalization. Crucially, our lightweight approach substantially outperforms prevalent reasoning-steering techniques, such as prompt design and decoding constraints, while preserving the model's general instruction-following skills. These results highlight reinforcing cognitive experts as a promising, practical, and interpretable direction to enhance cognitive efficiency within advanced reasoning models."
  },
  {
    "title": "Visionary-R1: Mitigating Shortcuts in Visual Reasoning with Reinforcement Learning",
    "url": "http://arxiv.org/abs/2505.14677v1",
    "arxiv_id": "2505.14677v1",
    "authors": [
      "Jiaer Xia",
      "Yuhang Zang",
      "Peng Gao",
      "Yixuan Li",
      "Kaiyang Zhou"
    ],
    "published": "2025-05-20T17:58:35+00:00",
    "summary": "Learning general-purpose reasoning capabilities has long been a challenging problem in AI. Recent research in large language models (LLMs), such as DeepSeek-R1, has shown that reinforcement learning techniques like GRPO can enable pre-trained LLMs to develop reasoning capabilities using simple question-answer pairs. In this paper, we aim to train visual language models (VLMs) to perform reasoning on image data through reinforcement learning and visual question-answer pairs, without any explicit chain-of-thought (CoT) supervision. Our findings indicate that simply applying reinforcement learning to a VLM -- by prompting the model to produce a reasoning chain before providing an answer -- can lead the model to develop shortcuts from easy questions, thereby reducing its ability to generalize across unseen data distributions. We argue that the key to mitigating shortcut learning is to encourage the model to interpret images prior to reasoning. Therefore, we train the model to adhere to a caption-reason-answer output format: initially generating a detailed caption for an image, followed by constructing an extensive reasoning chain. When trained on 273K CoT-free visual question-answer pairs and using only reinforcement learning, our model, named Visionary-R1, outperforms strong multimodal models, such as GPT-4o, Claude3.5-Sonnet, and Gemini-1.5-Pro, on multiple visual reasoning benchmarks."
  },
  {
    "title": "SAFEPATH: Preventing Harmful Reasoning in Chain-of-Thought via Early Alignment",
    "url": "http://arxiv.org/abs/2505.14667v1",
    "arxiv_id": "2505.14667v1",
    "authors": [
      "Wonje Jeung",
      "Sangyeon Yoon",
      "Minsuk Kahng",
      "Albert No"
    ],
    "published": "2025-05-20T17:54:54+00:00",
    "summary": "Large Reasoning Models (LRMs) have become powerful tools for complex problem solving, but their structured reasoning pathways can lead to unsafe outputs when exposed to harmful prompts. Existing safety alignment methods reduce harmful outputs but can degrade reasoning depth, leading to significant trade-offs in complex, multi-step tasks, and remain vulnerable to sophisticated jailbreak attacks. To address this, we introduce SAFEPATH, a lightweight alignment method that fine-tunes LRMs to emit a short, 8-token Safety Primer at the start of their reasoning, in response to harmful prompts, while leaving the rest of the reasoning process unsupervised. Empirical results across multiple benchmarks indicate that SAFEPATH effectively reduces harmful outputs while maintaining reasoning performance. Specifically, SAFEPATH reduces harmful responses by up to 90.0% and blocks 83.3% of jailbreak attempts in the DeepSeek-R1-Distill-Llama-8B model, while requiring 295.9x less compute than Direct Refusal and 314.1x less than SafeChain. We further introduce a zero-shot variant that requires no fine-tuning. In addition, we provide a comprehensive analysis of how existing methods in LLMs generalize, or fail, when applied to reasoning-centric models, revealing critical gaps and new directions for safer AI."
  },
  {
    "title": "Cost-Augmented Monte Carlo Tree Search for LLM-Assisted Planning",
    "url": "http://arxiv.org/abs/2505.14656v1",
    "arxiv_id": "2505.14656v1",
    "authors": [
      "Zihao Zhang",
      "Fei Liu"
    ],
    "published": "2025-05-20T17:43:33+00:00",
    "summary": "While LLMs excel at open-ended reasoning, they often struggle with cost-sensitive planning, either treating all actions as having equal cost or failing to stay within strict budgets. In this paper, we introduce Cost-Augmented Monte Carlo Tree Search (CATS), a novel approach that brings explicit cost-awareness into LLM-guided planning. Tight cost constraints push the planner to quickly identify infeasible solutions, while looser constraints encourage optimization for minimal cost. We benchmark top LLMs such as GPT-4.1, Claude-3.7-Sonnet, and DeepSeek-R1, against our CATS planner to evaluate their performance in cost-sensitive scenarios. Our experiments suggest that raw LLMs such as GPT-4.1 often falter under tight budgets, whereas CATS consistently delivers strong performance, achieving higher task success rates and better cost efficiency. CATS provides an effective solution for budget-aware decision-making by combining the reasoning power of LLMs with structured search."
  },
  {
    "title": "General-Reasoner: Advancing LLM Reasoning Across All Domains",
    "url": "http://arxiv.org/abs/2505.14652v1",
    "arxiv_id": "2505.14652v1",
    "authors": [
      "Xueguang Ma",
      "Qian Liu",
      "Dongfu Jiang",
      "Ge Zhang",
      "Zejun Ma",
      "Wenhu Chen"
    ],
    "published": "2025-05-20T17:41:33+00:00",
    "summary": "Reinforcement learning (RL) has recently demonstrated strong potential in enhancing the reasoning capabilities of large language models (LLMs). Particularly, the \"Zero\" reinforcement learning introduced by Deepseek-R1-Zero, enables direct RL training of base LLMs without relying on an intermediate supervised fine-tuning stage. Despite these advancements, current works for LLM reasoning mainly focus on mathematical and coding domains, largely due to data abundance and the ease of answer verification. This limits the applicability and generalization of such models to broader domains, where questions often have diverse answer representations, and data is more scarce. In this paper, we propose General-Reasoner, a novel training paradigm designed to enhance LLM reasoning capabilities across diverse domains. Our key contributions include: (1) constructing a large-scale, high-quality dataset of questions with verifiable answers curated by web crawling, covering a wide range of disciplines; and (2) developing a generative model-based answer verifier, which replaces traditional rule-based verification with the capability of chain-of-thought and context-awareness. We train a series of models and evaluate them on a wide range of datasets covering wide domains like physics, chemistry, finance, electronics etc. Our comprehensive evaluation across these 12 benchmarks (e.g. MMLU-Pro, GPQA, SuperGPQA, TheoremQA, BBEH and MATH AMC) demonstrates that General-Reasoner outperforms existing baseline methods, achieving robust and generalizable reasoning performance while maintaining superior effectiveness in mathematical reasoning tasks."
  },
  {
    "title": "General-Reasoner: Advancing LLM Reasoning Across All Domains",
    "url": "http://arxiv.org/abs/2505.14652v2",
    "arxiv_id": "2505.14652v2",
    "authors": [
      "Xueguang Ma",
      "Qian Liu",
      "Dongfu Jiang",
      "Ge Zhang",
      "Zejun Ma",
      "Wenhu Chen"
    ],
    "published": "2025-05-20T17:41:33+00:00",
    "summary": "Reinforcement learning (RL) has recently demonstrated strong potential in enhancing the reasoning capabilities of large language models (LLMs). Particularly, the \"Zero\" reinforcement learning introduced by Deepseek-R1-Zero, enables direct RL training of base LLMs without relying on an intermediate supervised fine-tuning stage. Despite these advancements, current works for LLM reasoning mainly focus on mathematical and coding domains, largely due to data abundance and the ease of answer verification. This limits the applicability and generalization of such models to broader domains, where questions often have diverse answer representations, and data is more scarce. In this paper, we propose General-Reasoner, a novel training paradigm designed to enhance LLM reasoning capabilities across diverse domains. Our key contributions include: (1) constructing a large-scale, high-quality dataset of questions with verifiable answers curated by web crawling, covering a wide range of disciplines; and (2) developing a generative model-based answer verifier, which replaces traditional rule-based verification with the capability of chain-of-thought and context-awareness. We train a series of models and evaluate them on a wide range of datasets covering wide domains like physics, chemistry, finance, electronics etc. Our comprehensive evaluation across these 12 benchmarks (e.g. MMLU-Pro, GPQA, SuperGPQA, TheoremQA, BBEH and MATH AMC) demonstrates that General-Reasoner outperforms existing baseline methods, achieving robust and generalizable reasoning performance while maintaining superior effectiveness in mathematical reasoning tasks."
  },
  {
    "title": "Will AI Tell Lies to Save Sick Children? Litmus-Testing AI Values Prioritization with AIRiskDilemmas",
    "url": "http://arxiv.org/abs/2505.14633v1",
    "arxiv_id": "2505.14633v1",
    "authors": [
      "Yu Ying Chiu",
      "Zhilin Wang",
      "Sharan Maiya",
      "Yejin Choi",
      "Kyle Fish",
      "Sydney Levine",
      "Evan Hubinger"
    ],
    "published": "2025-05-20T17:24:09+00:00",
    "summary": "Detecting AI risks becomes more challenging as stronger models emerge and find novel methods such as Alignment Faking to circumvent these detection attempts. Inspired by how risky behaviors in humans (i.e., illegal activities that may hurt others) are sometimes guided by strongly-held values, we believe that identifying values within AI models can be an early warning system for AI's risky behaviors. We create LitmusValues, an evaluation pipeline to reveal AI models' priorities on a range of AI value classes. Then, we collect AIRiskDilemmas, a diverse collection of dilemmas that pit values against one another in scenarios relevant to AI safety risks such as Power Seeking. By measuring an AI model's value prioritization using its aggregate choices, we obtain a self-consistent set of predicted value priorities that uncover potential risks. We show that values in LitmusValues (including seemingly innocuous ones like Care) can predict for both seen risky behaviors in AIRiskDilemmas and unseen risky behaviors in HarmBench."
  },
  {
    "title": "Impact of Surfactant and Flow Rate on the Electrical Properties of Activated Carbon Black Suspensions",
    "url": "http://arxiv.org/abs/2505.14618v1",
    "arxiv_id": "2505.14618v1",
    "authors": [
      "KangJin Lee",
      "Jesse S. Wainright",
      "Christopher L. Wirth"
    ],
    "published": "2025-05-20T17:08:17+00:00",
    "summary": "Carbon black slurries are a key component in redox flow batteries as the large surface area provided by the particles allows an increase in the battery capacity without facing limitations posed by many solid-state batteries such as safety hazard or cost. However, these conductive slurries often have complex mechanical and electrical responses because of the heterogeneous nature of the suspensions. Utilization of these slurries in a redox flow battery requires understanding of how additives impact the material responses and associated battery performance. This work focuses on the electrochemical performance of the slurry at flow rate conditions matching those of battery operation. In addition, the impact of a nonionic surfactant (Triton X-100) on the conductivity and capacitance of the slurry was measured. Experimental results show that the full capacitive contribution of the carbon black particles can only be measured at low flow rates and low scan rates, while the conductive contribution can be measured at all scan rates in flowing conditions. Upon the addition of surfactants, there is a gradual decrease in the electrochemical performance with increasing surfactant concentration until the surface of the carbon black particle is saturated. Once saturated, the conductive carbon particles no longer contribute to the electronic conductivity of the slurry. Results presented herein on the electrochemical response of the slurry to the addition of surfactant are in stark contrast to the mechanical response. While previous work has shown a smaller change in the response, followed by a step-change at a critical surfactant concentration, electrochemical data shows a gradual transition. Comparison of these behaviors suggests a difference in the mechanisms for how mechanical networks form in comparison to charge transfer networks for this particular slurry chemistry."
  },
  {
    "title": "Linear Control of Test Awareness Reveals Differential Compliance in Reasoning Models",
    "url": "http://arxiv.org/abs/2505.14617v1",
    "arxiv_id": "2505.14617v1",
    "authors": [
      "Sahar Abdelnabi",
      "Ahmed Salem"
    ],
    "published": "2025-05-20T17:03:12+00:00",
    "summary": "Reasoning-focused large language models (LLMs) sometimes alter their behavior when they detect that they are being evaluated, an effect analogous to the Hawthorne phenomenon, which can lead them to optimize for test-passing performance or to comply more readily with harmful prompts if real-world consequences appear absent. We present the first quantitative study of how such \"test awareness\" impacts model behavior, particularly its safety alignment. We introduce a white-box probing framework that (i) linearly identifies awareness-related activations and (ii) steers models toward or away from test awareness while monitoring downstream performance. We apply our method to different state-of-the-art open-source reasoning LLMs across both realistic and hypothetical tasks. Our results demonstrate that test awareness significantly impact safety alignment, and is different for different models. By providing fine-grained control over this latent effect, our work aims to increase trust in how we perform safety evaluation."
  },
  {
    "title": "sudoLLM : On Multi-role Alignment of Language Models",
    "url": "http://arxiv.org/abs/2505.14607v1",
    "arxiv_id": "2505.14607v1",
    "authors": [
      "Soumadeep Saha",
      "Akshay Chaturvedi",
      "Joy Mahapatra",
      "Utpal Garain"
    ],
    "published": "2025-05-20T16:54:34+00:00",
    "summary": "User authorization-based access privileges are a key feature in many safety-critical systems, but have thus far been absent from the large language model (LLM) realm. In this work, drawing inspiration from such access control systems, we introduce sudoLLM, a novel framework that results in multi-role aligned LLMs, i.e., LLMs that account for, and behave in accordance with, user access rights. sudoLLM injects subtle user-based biases into queries and trains an LLM to utilize this bias signal in order to produce sensitive information if and only if the user is authorized. We present empirical results demonstrating that this approach shows substantially improved alignment, generalization, and resistance to prompt-based jailbreaking attacks. The persistent tension between the language modeling objective and safety alignment, which is often exploited to jailbreak LLMs, is somewhat resolved with the aid of the injected bias signal. Our framework is meant as an additional security layer, and complements existing guardrail mechanisms for enhanced end-to-end safety with LLMs."
  },
  {
    "title": "Let LLMs Break Free from Overthinking via Self-Braking Tuning",
    "url": "http://arxiv.org/abs/2505.14604v1",
    "arxiv_id": "2505.14604v1",
    "authors": [
      "Haoran Zhao",
      "Yuchen Yan",
      "Yongliang Shen",
      "Haolei Xu",
      "Wenqi Zhang",
      "Kaitao Song",
      "Jian Shao",
      "Weiming Lu",
      "Jun Xiao",
      "Yueting Zhuang"
    ],
    "published": "2025-05-20T16:53:40+00:00",
    "summary": "Large reasoning models (LRMs), such as OpenAI o1 and DeepSeek-R1, have significantly enhanced their reasoning capabilities by generating longer chains of thought, demonstrating outstanding performance across a variety of tasks. However, this performance gain comes at the cost of a substantial increase in redundant reasoning during the generation process, leading to high computational overhead and exacerbating the issue of overthinking. Although numerous existing approaches aim to address the problem of overthinking, they often rely on external interventions. In this paper, we propose a novel framework, Self-Braking Tuning (SBT), which tackles overthinking from the perspective of allowing the model to regulate its own reasoning process, thus eliminating the reliance on external control mechanisms. We construct a set of overthinking identification metrics based on standard answers and design a systematic method to detect redundant reasoning. This method accurately identifies unnecessary steps within the reasoning trajectory and generates training signals for learning self-regulation behaviors. Building on this foundation, we develop a complete strategy for constructing data with adaptive reasoning lengths and introduce an innovative braking prompt mechanism that enables the model to naturally learn when to terminate reasoning at an appropriate point. Experiments across mathematical benchmarks (AIME, AMC, MATH500, GSM8K) demonstrate that our method reduces token consumption by up to 60% while maintaining comparable accuracy to unconstrained models."
  },
  {
    "title": "MCIP: Protecting MCP Safety via Model Contextual Integrity Protocol",
    "url": "http://arxiv.org/abs/2505.14590v1",
    "arxiv_id": "2505.14590v1",
    "authors": [
      "Huihao Jing",
      "Haoran Li",
      "Wenbin Hu",
      "Qi Hu",
      "Heli Xu",
      "Tianshu Chu",
      "Peizhao Hu",
      "Yangqiu Song"
    ],
    "published": "2025-05-20T16:41:45+00:00",
    "summary": "As Model Context Protocol (MCP) introduces an easy-to-use ecosystem for users and developers, it also brings underexplored safety risks. Its decentralized architecture, which separates clients and servers, poses unique challenges for systematic safety analysis. This paper proposes a novel framework to enhance MCP safety. Guided by the MAESTRO framework, we first analyze the missing safety mechanisms in MCP, and based on this analysis, we propose the Model Contextual Integrity Protocol (MCIP), a refined version of MCP that addresses these gaps.Next, we develop a fine-grained taxonomy that captures a diverse range of unsafe behaviors observed in MCP scenarios. Building on this taxonomy, we develop benchmark and training data that support the evaluation and improvement of LLMs' capabilities in identifying safety risks within MCP interactions. Leveraging the proposed benchmark and training data, we conduct extensive experiments on state-of-the-art LLMs. The results highlight LLMs' vulnerabilities in MCP interactions and demonstrate that our approach substantially improves their safety performance."
  },
  {
    "title": "Context Reasoner: Incentivizing Reasoning Capability for Contextualized Privacy and Safety Compliance via Reinforcement Learning",
    "url": "http://arxiv.org/abs/2505.14585v1",
    "arxiv_id": "2505.14585v1",
    "authors": [
      "Wenbin Hu",
      "Haoran Li",
      "Huihao Jing",
      "Qi Hu",
      "Ziqian Zeng",
      "Sirui Han",
      "Heli Xu",
      "Tianshu Chu",
      "Peizhao Hu",
      "Yangqiu Song"
    ],
    "published": "2025-05-20T16:40:09+00:00",
    "summary": "While Large Language Models (LLMs) exhibit remarkable capabilities, they also introduce significant safety and privacy risks. Current mitigation strategies often fail to preserve contextual reasoning capabilities in risky scenarios. Instead, they rely heavily on sensitive pattern matching to protect LLMs, which limits the scope. Furthermore, they overlook established safety and privacy standards, leading to systemic risks for legal compliance. To address these gaps, we formulate safety and privacy issues into contextualized compliance problems following the Contextual Integrity (CI) theory. Under the CI framework, we align our model with three critical regulatory standards: GDPR, EU AI Act, and HIPAA. Specifically, we employ reinforcement learning (RL) with a rule-based reward to incentivize contextual reasoning capabilities while enhancing compliance with safety and privacy norms. Through extensive experiments, we demonstrate that our method not only significantly enhances legal compliance (achieving a +17.64% accuracy improvement in safety/privacy benchmarks) but also further improves general reasoning capability. For OpenThinker-7B, a strong reasoning model that significantly outperforms its base model Qwen2.5-7B-Instruct across diverse subjects, our method enhances its general reasoning capabilities, with +2.05% and +8.98% accuracy improvement on the MMLU and LegalBench benchmark, respectively."
  },
  {
    "title": "Traversability-aware path planning in dynamic environments",
    "url": "http://arxiv.org/abs/2505.14580v1",
    "arxiv_id": "2505.14580v1",
    "authors": [
      "Yaroslav Marchukov",
      "Luis Montano"
    ],
    "published": "2025-05-20T16:38:00+00:00",
    "summary": "Planning in environments with moving obstacles remains a significant challenge in robotics. While many works focus on navigation and path planning in obstacle-dense spaces, traversing such congested regions is often avoidable by selecting alternative routes. This paper presents Traversability-aware FMM (Tr-FMM), a path planning method that computes paths in dynamic environments, avoiding crowded regions. The method operates in two steps: first, it discretizes the environment, identifying regions and their distribution; second, it computes the traversability of regions, aiming to minimize both obstacle risks and goal deviation. The path is then computed by propagating the wavefront through regions with higher traversability. Simulated and real-world experiments demonstrate that the approach enhances significant safety by keeping the robot away from regions with obstacles while reducing unnecessary deviations from the goal."
  },
  {
    "title": "Breaking Bad Tokens: Detoxification of LLMs Using Sparse Autoencoders",
    "url": "http://arxiv.org/abs/2505.14536v1",
    "arxiv_id": "2505.14536v1",
    "authors": [
      "Agam Goyal",
      "Vedant Rathi",
      "William Yeh",
      "Yian Wang",
      "Yuen Chen",
      "Hari Sundaram"
    ],
    "published": "2025-05-20T15:55:31+00:00",
    "summary": "Large language models (LLMs) are now ubiquitous in user-facing applications, yet they still generate undesirable toxic outputs, including profanity, vulgarity, and derogatory remarks. Although numerous detoxification methods exist, most apply broad, surface-level fixes and can therefore easily be circumvented by jailbreak attacks. In this paper we leverage sparse autoencoders (SAEs) to identify toxicity-related directions in the residual stream of models and perform targeted activation steering using the corresponding decoder vectors. We introduce three tiers of steering aggressiveness and evaluate them on GPT-2 Small and Gemma-2-2B, revealing trade-offs between toxicity reduction and language fluency. At stronger steering strengths, these causal interventions surpass competitive baselines in reducing toxicity by up to 20%, though fluency can degrade noticeably on GPT-2 Small depending on the aggressiveness. Crucially, standard NLP benchmark scores upon steering remain stable, indicating that the model's knowledge and general abilities are preserved. We further show that feature-splitting in wider SAEs hampers safety interventions, underscoring the importance of disentangled feature learning. Our findings highlight both the promise and the current limitations of SAE-based causal interventions for LLM detoxification, further suggesting practical guidelines for safer language-model deployment."
  },
  {
    "title": "Towards Reliable Proof Generation with LLMs: A Neuro-Symbolic Approach",
    "url": "http://arxiv.org/abs/2505.14479v1",
    "arxiv_id": "2505.14479v1",
    "authors": [
      "Oren Sultan",
      "Eitan Stern",
      "Dafna Shahaf"
    ],
    "published": "2025-05-20T15:13:32+00:00",
    "summary": "Large language models (LLMs) struggle with formal domains that require rigorous logical deduction and symbolic reasoning, such as mathematical proof generation. We propose a neuro-symbolic approach that combines LLMs' generative strengths with structured components to overcome this challenge. As a proof-of-concept, we focus on geometry problems. Our approach is two-fold: (1) we retrieve analogous problems and use their proofs to guide the LLM, and (2) a formal verifier evaluates the generated proofs and provides feedback, helping the model fix incorrect proofs. We demonstrate that our method significantly improves proof accuracy for OpenAI's o1 model (58%-70% improvement); both analogous problems and the verifier's feedback contribute to these gains. More broadly, shifting to LLMs that generate provably correct conclusions could dramatically improve their reliability, accuracy and consistency, unlocking complex tasks and critical real-world applications that require trustworthiness."
  },
  {
    "title": "Attributional Safety Failures in Large Language Models under Code-Mixed Perturbations",
    "url": "http://arxiv.org/abs/2505.14469v1",
    "arxiv_id": "2505.14469v1",
    "authors": [
      "Somnath Banerjee",
      "Pratyush Chatterjee",
      "Shanu Kumar",
      "Sayan Layek",
      "Parag Agrawal",
      "Rima Hazra",
      "Animesh Mukherjee"
    ],
    "published": "2025-05-20T15:05:03+00:00",
    "summary": "Recent advancements in LLMs have raised significant safety concerns, particularly when dealing with code-mixed inputs and outputs. Our study systematically investigates the increased susceptibility of LLMs to produce unsafe outputs from code-mixed prompts compared to monolingual English prompts. Utilizing explainability methods, we dissect the internal attribution shifts causing model's harmful behaviors. In addition, we explore cultural dimensions by distinguishing between universally unsafe and culturally-specific unsafe queries. This paper presents novel experimental insights, clarifying the mechanisms driving this phenomenon."
  },
  {
    "title": "Not All Correct Answers Are Equal: Why Your Distillation Source Matters",
    "url": "http://arxiv.org/abs/2505.14464v1",
    "arxiv_id": "2505.14464v1",
    "authors": [
      "Xiaoyu Tian",
      "Yunjie Ji",
      "Haotian Wang",
      "Shuaiting Chen",
      "Sitong Zhao",
      "Yiping Peng",
      "Han Zhao",
      "Xiangang Li"
    ],
    "published": "2025-05-20T15:00:51+00:00",
    "summary": "Distillation has emerged as a practical and effective approach to enhance the reasoning capabilities of open-source language models. In this work, we conduct a large-scale empirical study on reasoning data distillation by collecting verified outputs from three state-of-the-art teacher models-AM-Thinking-v1, Qwen3-235B-A22B, and DeepSeek-R1-on a shared corpus of 1.89 million queries. We construct three parallel datasets and analyze their distributions, revealing that AM-Thinking-v1-distilled data exhibits greater token length diversity and lower perplexity. Student models trained on each dataset are evaluated on reasoning benchmarks including AIME2024, AIME2025, MATH500, and LiveCodeBench. The AM-based model consistently achieves the best performance (e.g., 84.3 on AIME2024, 72.2 on AIME2025, 98.4 on MATH500, and 65.9 on LiveCodeBench) and demonstrates adaptive output behavior-producing longer responses for harder tasks and shorter ones for simpler tasks. These findings highlight the value of high-quality, verified reasoning traces. We release the AM-Thinking-v1 and Qwen3-235B-A22B distilled datasets to support future research on open and high-performing reasoning-oriented language models. The datasets are publicly available on Hugging Face\\footnote{Datasets are available on Hugging Face: \\href{https://huggingface.co/datasets/a-m-team/AM-Thinking-v1-Distilled}{AM-Thinking-v1-Distilled}, \\href{https://huggingface.co/datasets/a-m-team/AM-Qwen3-Distilled}{AM-Qwen3-Distilled}.}."
  },
  {
    "title": "VisualQuality-R1: Reasoning-Induced Image Quality Assessment via Reinforcement Learning to Rank",
    "url": "http://arxiv.org/abs/2505.14460v1",
    "arxiv_id": "2505.14460v1",
    "authors": [
      "Tianhe Wu",
      "Jian Zou",
      "Jie Liang",
      "Lei Zhang",
      "Kede Ma"
    ],
    "published": "2025-05-20T14:56:50+00:00",
    "summary": "DeepSeek-R1 has demonstrated remarkable effectiveness in incentivizing reasoning and generalization capabilities of large language models (LLMs) through reinforcement learning. Nevertheless, the potential of reasoning-induced computational modeling has not been thoroughly explored in the context of image quality assessment (IQA), a task critically dependent on visual reasoning. In this paper, we introduce VisualQuality-R1, a reasoning-induced no-reference IQA (NR-IQA) model, and we train it with reinforcement learning to rank, a learning algorithm tailored to the intrinsically relative nature of visual quality. Specifically, for a pair of images, we employ group relative policy optimization to generate multiple quality scores for each image. These estimates are then used to compute comparative probabilities of one image having higher quality than the other under the Thurstone model. Rewards for each quality estimate are defined using continuous fidelity measures rather than discretized binary labels. Extensive experiments show that the proposed VisualQuality-R1 consistently outperforms discriminative deep learning-based NR-IQA models as well as a recent reasoning-induced quality regression method. Moreover, VisualQuality-R1 is capable of generating contextually rich, human-aligned quality descriptions, and supports multi-dataset training without requiring perceptual scale realignment. These features make VisualQuality-R1 especially well-suited for reliably measuring progress in a wide range of image processing tasks like super-resolution and image generation."
  },
  {
    "title": "Interpretable Neural System Dynamics: Combining Deep Learning with System Dynamics Modeling to Support Critical Applications",
    "url": "http://arxiv.org/abs/2505.14428v1",
    "arxiv_id": "2505.14428v1",
    "authors": [
      "Riccardo D'Elia"
    ],
    "published": "2025-05-20T14:38:39+00:00",
    "summary": "The objective of this proposal is to bridge the gap between Deep Learning (DL) and System Dynamics (SD) by developing an interpretable neural system dynamics framework. While DL excels at learning complex models and making accurate predictions, it lacks interpretability and causal reliability. Traditional SD approaches, on the other hand, provide transparency and causal insights but are limited in scalability and require extensive domain knowledge. To overcome these limitations, this project introduces a Neural System Dynamics pipeline, integrating Concept-Based Interpretability, Mechanistic Interpretability, and Causal Machine Learning. This framework combines the predictive power of DL with the interpretability of traditional SD models, resulting in both causal reliability and scalability. The efficacy of the proposed pipeline will be validated through real-world applications of the EU-funded AutoMoTIF project, which is focused on autonomous multimodal transportation systems. The long-term goal is to collect actionable insights that support the integration of explainability and safety in autonomous systems."
  },
  {
    "title": "Explaining Unreliable Perception in Automated Driving: A Fuzzy-based Monitoring Approach",
    "url": "http://arxiv.org/abs/2505.14407v1",
    "arxiv_id": "2505.14407v1",
    "authors": [
      "Aniket Salvi",
      "Gereon Weiss",
      "Mario Trapp"
    ],
    "published": "2025-05-20T14:22:39+00:00",
    "summary": "Autonomous systems that rely on Machine Learning (ML) utilize online fault tolerance mechanisms, such as runtime monitors, to detect ML prediction errors and maintain safety during operation. However, the lack of human-interpretable explanations for these errors can hinder the creation of strong assurances about the system's safety and reliability. This paper introduces a novel fuzzy-based monitor tailored for ML perception components. It provides human-interpretable explanations about how different operating conditions affect the reliability of perception components and also functions as a runtime safety monitor. We evaluated our proposed monitor using naturalistic driving datasets as part of an automated driving case study. The interpretability of the monitor was evaluated and we identified a set of operating conditions in which the perception component performs reliably. Additionally, we created an assurance case that links unit-level evidence of \\textit{correct} ML operation to system-level \\textit{safety}. The benchmarking demonstrated that our monitor achieved a better increase in safety (i.e., absence of hazardous situations) while maintaining availability (i.e., ability to perform the mission) compared to state-of-the-art runtime ML monitors in the evaluated dataset."
  },
  {
    "title": "WirelessMathBench: A Mathematical Modeling Benchmark for LLMs in Wireless Communications",
    "url": "http://arxiv.org/abs/2505.14354v1",
    "arxiv_id": "2505.14354v1",
    "authors": [
      "Xin Li",
      "Mengbing Liu",
      "Li Wei",
      "Jiancheng An",
      "M\u00e9rouane Debbah",
      "Chau Yuen"
    ],
    "published": "2025-05-20T13:38:10+00:00",
    "summary": "Large Language Models (LLMs) have achieved impressive results across a broad array of tasks, yet their capacity for complex, domain-specific mathematical reasoning-particularly in wireless communications-remains underexplored. In this work, we introduce WirelessMathBench, a novel benchmark specifically designed to evaluate LLMs on mathematical modeling challenges to wireless communications engineering. Our benchmark consists of 587 meticulously curated questions sourced from 40 state-of-the-art research papers, encompassing a diverse spectrum of tasks ranging from basic multiple-choice questions to complex equation completion tasks, including both partial and full completions, all of which rigorously adhere to physical and dimensional constraints. Through extensive experimentation with leading LLMs, we observe that while many models excel in basic recall tasks, their performance degrades significantly when reconstructing partially or fully obscured equations, exposing fundamental limitations in current LLMs. Even DeepSeek-R1, the best performer on our benchmark, achieves an average accuracy of only 38.05%, with a mere 7.83% success rate in full equation completion. By publicly releasing WirelessMathBench along with the evaluation toolkit, we aim to advance the development of more robust, domain-aware LLMs for wireless system analysis and broader engineering applications."
  },
  {
    "title": "Exploring Jailbreak Attacks on LLMs through Intent Concealment and Diversion",
    "url": "http://arxiv.org/abs/2505.14316v1",
    "arxiv_id": "2505.14316v1",
    "authors": [
      "Tiehan Cui",
      "Yanxu Mao",
      "Peipei Liu",
      "Congying Liu",
      "Datao You"
    ],
    "published": "2025-05-20T13:03:15+00:00",
    "summary": "Although large language models (LLMs) have achieved remarkable advancements, their security remains a pressing concern. One major threat is jailbreak attacks, where adversarial prompts bypass model safeguards to generate harmful or objectionable content. Researchers study jailbreak attacks to understand security and robustness of LLMs. However, existing jailbreak attack methods face two main challenges: (1) an excessive number of iterative queries, and (2) poor generalization across models. In addition, recent jailbreak evaluation datasets focus primarily on question-answering scenarios, lacking attention to text generation tasks that require accurate regeneration of toxic content. To tackle these challenges, we propose two contributions: (1) ICE, a novel black-box jailbreak method that employs Intent Concealment and divErsion to effectively circumvent security constraints. ICE achieves high attack success rates (ASR) with a single query, significantly improving efficiency and transferability across different models. (2) BiSceneEval, a comprehensive dataset designed for assessing LLM robustness in question-answering and text-generation tasks. Experimental results demonstrate that ICE outperforms existing jailbreak techniques, revealing critical vulnerabilities in current defense mechanisms. Our findings underscore the necessity of a hybrid security strategy that integrates predefined security mechanisms with real-time semantic decomposition to enhance the security of LLMs."
  },
  {
    "title": "A MIND for Reasoning: Meta-learning for In-context Deduction",
    "url": "http://arxiv.org/abs/2505.14313v1",
    "arxiv_id": "2505.14313v1",
    "authors": [
      "Leonardo Bertolazzi",
      "Manuel Vargas Guzm\u00e1n",
      "Raffaella Bernardi",
      "Maciej Malicki",
      "Jakub Szymanik"
    ],
    "published": "2025-05-20T13:00:48+00:00",
    "summary": "Large language models (LLMs) are increasingly evaluated on formal tasks, where strong reasoning abilities define the state of the art. However, their ability to generalize to out-of-distribution problems remains limited. In this paper, we investigate how LLMs can achieve a systematic understanding of deductive rules. Our focus is on the task of identifying the appropriate subset of premises within a knowledge base needed to derive a given hypothesis. To tackle this challenge, we propose Meta-learning for In-context Deduction (MIND), a novel few-shot meta-learning fine-tuning approach. The goal of MIND is to enable models to generalize more effectively to unseen knowledge bases and to systematically apply inference rules. Our results show that MIND significantly improves generalization in small LMs ranging from 1.5B to 7B parameters. The benefits are especially pronounced in smaller models and low-data settings. Remarkably, small models fine-tuned with MIND outperform state-of-the-art LLMs, such as GPT-4o and o3-mini, on this task."
  },
  {
    "title": "SafetyNet: Detecting Harmful Outputs in LLMs by Modeling and Monitoring Deceptive Behaviors",
    "url": "http://arxiv.org/abs/2505.14300v1",
    "arxiv_id": "2505.14300v1",
    "authors": [
      "Maheep Chaudhary",
      "Fazl Barez"
    ],
    "published": "2025-05-20T12:49:58+00:00",
    "summary": "High-risk industries like nuclear and aviation use real-time monitoring to detect dangerous system conditions. Similarly, Large Language Models (LLMs) need monitoring safeguards. We propose a real-time framework to predict harmful AI outputs before they occur by using an unsupervised approach that treats normal behavior as the baseline and harmful outputs as outliers. Our study focuses specifically on backdoor-triggered responses -- where specific input phrases activate hidden vulnerabilities causing the model to generate unsafe content like violence, pornography, or hate speech. We address two key challenges: (1) identifying true causal indicators rather than surface correlations, and (2) preventing advanced models from deception -- deliberately evading monitoring systems. Hence, we approach this problem from an unsupervised lens by drawing parallels to human deception: just as humans exhibit physical indicators while lying, we investigate whether LLMs display distinct internal behavioral signatures when generating harmful content. Our study addresses two critical challenges: 1) designing monitoring systems that capture true causal indicators rather than superficial correlations; and 2)preventing intentional evasion by increasingly capable \"Future models''. Our findings show that models can produce harmful content through causal mechanisms and can become deceptive by: (a) alternating between linear and non-linear representations, and (b) modifying feature relationships. To counter this, we developed Safety-Net -- a multi-detector framework that monitors different representation dimensions, successfully detecting harmful behavior even when information is shifted across representational spaces to evade individual monitors. Our evaluation shows 96% accuracy in detecting harmful cases using our unsupervised ensemble approach."
  },
  {
    "title": "Visual Agentic Reinforcement Fine-Tuning",
    "url": "http://arxiv.org/abs/2505.14246v1",
    "arxiv_id": "2505.14246v1",
    "authors": [
      "Ziyu Liu",
      "Yuhang Zang",
      "Yushan Zou",
      "Zijian Liang",
      "Xiaoyi Dong",
      "Yuhang Cao",
      "Haodong Duan",
      "Dahua Lin",
      "Jiaqi Wang"
    ],
    "published": "2025-05-20T11:59:25+00:00",
    "summary": "A key trend in Large Reasoning Models (e.g., OpenAI's o3) is the native agentic ability to use external tools such as web browsers for searching and writing/executing code for image manipulation to think with images. In the open-source research community, while significant progress has been made in language-only agentic abilities such as function calling and tool integration, the development of multi-modal agentic capabilities that involve truly thinking with images, and their corresponding benchmarks, are still less explored. This work highlights the effectiveness of Visual Agentic Reinforcement Fine-Tuning (Visual-ARFT) for enabling flexible and adaptive reasoning abilities for Large Vision-Language Models (LVLMs). With Visual-ARFT, open-source LVLMs gain the ability to browse websites for real-time information updates and write code to manipulate and analyze input images through cropping, rotation, and other image processing techniques. We also present a Multi-modal Agentic Tool Bench (MAT) with two settings (MAT-Search and MAT-Coding) designed to evaluate LVLMs' agentic search and coding abilities. Our experimental results demonstrate that Visual-ARFT outperforms its baseline by +18.6% F1 / +13.0% EM on MAT-Coding and +10.3% F1 / +8.7% EM on MAT-Search, ultimately surpassing GPT-4o. Visual-ARFT also achieves +29.3 F1% / +25.9% EM gains on existing multi-hop QA benchmarks such as 2Wiki and HotpotQA, demonstrating strong generalization capabilities. Our findings suggest that Visual-ARFT offers a promising path toward building robust and generalizable multimodal agents."
  },
  {
    "title": "\"Haet Bhasha aur Diskrimineshun\": Phonetic Perturbations in Code-Mixed Hinglish to Red-Team LLMs",
    "url": "http://arxiv.org/abs/2505.14226v1",
    "arxiv_id": "2505.14226v1",
    "authors": [
      "Darpan Aswal",
      "Siddharth D Jaiswal"
    ],
    "published": "2025-05-20T11:35:25+00:00",
    "summary": "Large Language Models (LLMs) have become increasingly powerful, with multilingual and multimodal capabilities improving by the day. These models are being evaluated through audits, alignment studies and red-teaming efforts to expose model vulnerabilities towards generating harmful, biased and unfair content. Existing red-teaming efforts have previously focused on the English language, using fixed template-based attacks; thus, models continue to be susceptible to multilingual jailbreaking strategies, especially in the multimodal context. In this study, we introduce a novel strategy that leverages code-mixing and phonetic perturbations to jailbreak LLMs for both text and image generation tasks. We also introduce two new jailbreak strategies that show higher effectiveness than baseline strategies. Our work presents a method to effectively bypass safety filters in LLMs while maintaining interpretability by applying phonetic misspellings to sensitive words in code-mixed prompts. Our novel prompts achieve a 99% Attack Success Rate for text generation and 78% for image generation, with Attack Relevance Rate of 100% for text generation and 95% for image generation when using the phonetically perturbed code-mixed prompts. Our interpretability experiments reveal that phonetic perturbations impact word tokenization, leading to jailbreak success. Our study motivates increasing the focus towards more generalizable safety alignment for multilingual multimodal models, especially in real-world settings wherein prompts can have misspelt words."
  },
  {
    "title": "Safety Devolution in AI Agents",
    "url": "http://arxiv.org/abs/2505.14215v1",
    "arxiv_id": "2505.14215v1",
    "authors": [
      "Cheng Yu",
      "Benedikt Stroebl",
      "Diyi Yang",
      "Orestis Papakyriakopoulos"
    ],
    "published": "2025-05-20T11:21:40+00:00",
    "summary": "As retrieval-augmented AI agents become more embedded in society, their safety properties and ethical behavior remain insufficiently understood. In particular, the growing integration of LLMs and AI agents raises critical questions about how they engage with and are influenced by their environments. This study investigates how expanding retrieval access, from no external sources to Wikipedia-based retrieval and open web search, affects model reliability, bias propagation, and harmful content generation. Through extensive benchmarking of censored and uncensored LLMs and AI Agents, our findings reveal a consistent degradation in refusal rates, bias sensitivity, and harmfulness safeguards as models gain broader access to external sources, culminating in a phenomenon we term safety devolution. Notably, retrieval-augmented agents built on aligned LLMs often behave more unsafely than uncensored models without retrieval. This effect persists even under strong retrieval accuracy and prompt-based mitigation, suggesting that the mere presence of retrieved content reshapes model behavior in structurally unsafe ways. These findings underscore the need for robust mitigation strategies to ensure fairness and reliability in retrieval-augmented and increasingly autonomous AI systems."
  },
  {
    "title": "Safety Subspaces are Not Distinct: A Fine-Tuning Case Study",
    "url": "http://arxiv.org/abs/2505.14185v1",
    "arxiv_id": "2505.14185v1",
    "authors": [
      "Kaustubh Ponkshe",
      "Shaan Shah",
      "Raghav Singhal",
      "Praneeth Vepakomma"
    ],
    "published": "2025-05-20T10:41:49+00:00",
    "summary": "Large Language Models (LLMs) rely on safety alignment to produce socially acceptable responses. This is typically achieved through instruction tuning and reinforcement learning from human feedback. However, this alignment is known to be brittle: further fine-tuning, even on benign or lightly contaminated data, can degrade safety and reintroduce harmful behaviors. A growing body of work suggests that alignment may correspond to identifiable geometric directions in weight space, forming subspaces that could, in principle, be isolated or preserved to defend against misalignment. In this work, we conduct a comprehensive empirical study of this geometric perspective. We examine whether safety-relevant behavior is concentrated in specific subspaces, whether it can be separated from general-purpose learning, and whether harmfulness arises from distinguishable patterns in internal representations. Across both parameter and activation space, our findings are consistent: subspaces that amplify safe behaviors also amplify unsafe ones, and prompts with different safety implications activate overlapping representations. We find no evidence of a subspace that selectively governs safety. These results challenge the assumption that alignment is geometrically localized. Rather than residing in distinct directions, safety appears to emerge from entangled, high-impact components of the model's broader learning dynamics. This suggests that subspace-based defenses may face fundamental limitations and underscores the need for alternative strategies to preserve alignment under continued training. We corroborate these findings through multiple experiments on five open-source LLMs. Our code is publicly available at: https://github.com/CERT-Lab/safety-subspaces."
  },
  {
    "title": "Tokenization Constraints in LLMs: A Study of Symbolic and Arithmetic Reasoning Limits",
    "url": "http://arxiv.org/abs/2505.14178v1",
    "arxiv_id": "2505.14178v1",
    "authors": [
      "Xiang Zhang",
      "Juntai Cao",
      "Jiaqi Wei",
      "Yiwei Xu",
      "Chenyu You"
    ],
    "published": "2025-05-20T10:32:30+00:00",
    "summary": "Tokenization is the first - and often underappreciated - layer of computation in language models. While Chain-of-Thought (CoT) prompting enables transformer models to approximate recurrent computation by externalizing intermediate steps, we show that the success of such reasoning is fundamentally bounded by the structure of tokenized inputs. This work presents a theoretical and empirical investigation into how tokenization schemes, particularly subword-based methods like byte-pair encoding (BPE), impede symbolic computation by merging or obscuring atomic reasoning units. We introduce the notion of Token Awareness to formalize how poor token granularity disrupts logical alignment and prevents models from generalizing symbolic procedures. Through systematic evaluation on arithmetic and symbolic tasks, we demonstrate that token structure dramatically affect reasoning performance, causing failure even with CoT, while atomically-aligned formats unlock strong generalization, allowing small models (e.g., GPT-4o-mini) to outperform larger systems (e.g., o1) in structured reasoning. Our findings reveal that symbolic reasoning ability in LLMs is not purely architectural, but deeply conditioned on token-level representations."
  },
  {
    "title": "Invisible Entropy: Towards Safe and Efficient Low-Entropy LLM Watermarking",
    "url": "http://arxiv.org/abs/2505.14112v1",
    "arxiv_id": "2505.14112v1",
    "authors": [
      "Tianle Gu",
      "Zongqi Wang",
      "Kexin Huang",
      "Yuanqi Yao",
      "Xiangliang Zhang",
      "Yujiu Yang",
      "Xiuying Chen"
    ],
    "published": "2025-05-20T09:19:06+00:00",
    "summary": "Logit-based LLM watermarking traces and verifies AI-generated content by maintaining green and red token lists and increasing the likelihood of green tokens during generation. However, it fails in low-entropy scenarios, where predictable outputs make green token selection difficult without disrupting natural text flow. Existing approaches address this by assuming access to the original LLM to calculate entropy and selectively watermark high-entropy tokens. However, these methods face two major challenges: (1) high computational costs and detection delays due to reliance on the original LLM, and (2) potential risks of model leakage. To address these limitations, we propose Invisible Entropy (IE), a watermarking paradigm designed to enhance both safety and efficiency. Instead of relying on the original LLM, IE introduces a lightweight feature extractor and an entropy tagger to predict whether the entropy of the next token is high or low. Furthermore, based on theoretical analysis, we develop a threshold navigator that adaptively sets entropy thresholds. It identifies a threshold where the watermark ratio decreases as the green token count increases, enhancing the naturalness of the watermarked text and improving detection robustness. Experiments on HumanEval and MBPP datasets demonstrate that IE reduces parameter size by 99\\% while achieving performance on par with state-of-the-art methods. Our work introduces a safe and efficient paradigm for low-entropy watermarking. https://github.com/Carol-gutianle/IE https://huggingface.co/datasets/Carol0110/IE-Tagger"
  },
  {
    "title": "DiagnosisArena: Benchmarking Diagnostic Reasoning for Large Language Models",
    "url": "http://arxiv.org/abs/2505.14107v1",
    "arxiv_id": "2505.14107v1",
    "authors": [
      "Yakun Zhu",
      "Zhongzhen Huang",
      "Linjie Mu",
      "Yutong Huang",
      "Wei Nie",
      "Shaoting Zhang",
      "Pengfei Liu",
      "Xiaofan Zhang"
    ],
    "published": "2025-05-20T09:14:53+00:00",
    "summary": "The emergence of groundbreaking large language models capable of performing complex reasoning tasks holds significant promise for addressing various scientific challenges, including those arising in complex clinical scenarios. To enable their safe and effective deployment in real-world healthcare settings, it is urgently necessary to benchmark the diagnostic capabilities of current models systematically. Given the limitations of existing medical benchmarks in evaluating advanced diagnostic reasoning, we present DiagnosisArena, a comprehensive and challenging benchmark designed to rigorously assess professional-level diagnostic competence. DiagnosisArena consists of 1,113 pairs of segmented patient cases and corresponding diagnoses, spanning 28 medical specialties, deriving from clinical case reports published in 10 top-tier medical journals. The benchmark is developed through a meticulous construction pipeline, involving multiple rounds of screening and review by both AI systems and human experts, with thorough checks conducted to prevent data leakage. Our study reveals that even the most advanced reasoning models, o3-mini, o1, and DeepSeek-R1, achieve only 45.82%, 31.09%, and 17.79% accuracy, respectively. This finding highlights a significant generalization bottleneck in current large language models when faced with clinical diagnostic reasoning challenges. Through DiagnosisArena, we aim to drive further advancements in AIs diagnostic reasoning capabilities, enabling more effective solutions for real-world clinical diagnostic challenges. We provide the benchmark and evaluation tools for further research and development https://github.com/SPIRAL-MED/DiagnosisArena."
  },
  {
    "title": "AudioJailbreak: Jailbreak Attacks against End-to-End Large Audio-Language Models",
    "url": "http://arxiv.org/abs/2505.14103v1",
    "arxiv_id": "2505.14103v1",
    "authors": [
      "Guangke Chen",
      "Fu Song",
      "Zhe Zhao",
      "Xiaojun Jia",
      "Yang Liu",
      "Yanchen Qiao",
      "Weizhe Zhang"
    ],
    "published": "2025-05-20T09:10:45+00:00",
    "summary": "Jailbreak attacks to Large audio-language models (LALMs) are studied recently, but they achieve suboptimal effectiveness, applicability, and practicability, particularly, assuming that the adversary can fully manipulate user prompts. In this work, we first conduct an extensive experiment showing that advanced text jailbreak attacks cannot be easily ported to end-to-end LALMs via text-to speech (TTS) techniques. We then propose AudioJailbreak, a novel audio jailbreak attack, featuring (1) asynchrony: the jailbreak audio does not need to align with user prompts in the time axis by crafting suffixal jailbreak audios; (2) universality: a single jailbreak perturbation is effective for different prompts by incorporating multiple prompts into perturbation generation; (3) stealthiness: the malicious intent of jailbreak audios will not raise the awareness of victims by proposing various intent concealment strategies; and (4) over-the-air robustness: the jailbreak audios remain effective when being played over the air by incorporating the reverberation distortion effect with room impulse response into the generation of the perturbations. In contrast, all prior audio jailbreak attacks cannot offer asynchrony, universality, stealthiness, or over-the-air robustness. Moreover, AudioJailbreak is also applicable to the adversary who cannot fully manipulate user prompts, thus has a much broader attack scenario. Extensive experiments with thus far the most LALMs demonstrate the high effectiveness of AudioJailbreak. We highlight that our work peeks into the security implications of audio jailbreak attacks against LALMs, and realistically fosters improving their security robustness. The implementation and audio samples are available at our website https://audiojailbreak.github.io/AudioJailbreak."
  },
  {
    "title": "Verifying Tree-Manipulating Programs via CHCs",
    "url": "http://arxiv.org/abs/2505.14092v1",
    "arxiv_id": "2505.14092v1",
    "authors": [
      "Marco Faella",
      "Gennaro Parlato"
    ],
    "published": "2025-05-20T08:56:33+00:00",
    "summary": "Programs that manipulate tree-shaped data structures often require complex, specialized proofs that are difficult to generalize and automate. This paper introduces a unified, foundational approach to verifying such programs. Central to our approach is the knitted-tree encoding, modeling each program execution as a tree structure capturing input, output, and intermediate states. Leveraging the compositional nature of knitted-trees, we encode these structures as constrained Horn clauses (CHCs), reducing verification to CHC satisfiability task. To illustrate our approach, we focus on memory safety and show how it naturally leads to simple, modular invariants."
  },
  {
    "title": "On-Demand Scenario Generation for Testing Automated Driving Systems",
    "url": "http://arxiv.org/abs/2505.14053v1",
    "arxiv_id": "2505.14053v1",
    "authors": [
      "Songyang Yan",
      "Xiaodong Zhang",
      "Kunkun Hao",
      "haojie xin",
      "Yonggang Luo",
      "Jucheng Yang",
      "Ming Fan",
      "Chao Yang",
      "Jun Sun",
      "Zijiang Yang"
    ],
    "published": "2025-05-20T07:55:36+00:00",
    "summary": "The safety and reliability of Automated Driving Systems (ADS) are paramount, necessitating rigorous testing methodologies to uncover potential failures before deployment. Traditional testing approaches often prioritize either natural scenario sampling or safety-critical scenario generation, resulting in overly simplistic or unrealistic hazardous tests. In practice, the demand for natural scenarios (e.g., when evaluating the ADS's reliability in real-world conditions), critical scenarios (e.g., when evaluating safety in critical situations), or somewhere in between (e.g., when testing the ADS in regions with less civilized drivers) varies depending on the testing objectives. To address this issue, we propose the On-demand Scenario Generation (OSG) Framework, which generates diverse scenarios with varying risk levels. Achieving the goal of OSG is challenging due to the complexity of quantifying the criticalness and naturalness stemming from intricate vehicle-environment interactions, as well as the need to maintain scenario diversity across various risk levels. OSG learns from real-world traffic datasets and employs a Risk Intensity Regulator to quantitatively control the risk level. It also leverages an improved heuristic search method to ensure scenario diversity. We evaluate OSG on the Carla simulators using various ADSs. We verify OSG's ability to generate scenarios with different risk levels and demonstrate its necessity by comparing accident types across risk levels. With the help of OSG, we are now able to systematically and objectively compare the performance of different ADSs based on different risk levels."
  },
  {
    "title": "Improved Methods for Model Pruning and Knowledge Distillation",
    "url": "http://arxiv.org/abs/2505.14052v1",
    "arxiv_id": "2505.14052v1",
    "authors": [
      "Wei Jiang",
      "Anying Fu",
      "Youling Zhang"
    ],
    "published": "2025-05-20T07:53:40+00:00",
    "summary": "Model pruning is a performance optimization technique for large language models like R1 or o3-mini. However, existing pruning methods often lead to significant performance degradation or require extensive retraining and fine-tuning. This technique aims to identify and remove neurons, connections unlikely leading to the contribution during the human-computer interaction phase. Our goal is to obtain a much smaller and faster knowledge distilled model that can quickly generate content almost as good as those of the unpruned ones. We propose MAMA Pruning, short for Movement and Magnitude Analysis, an improved pruning method that effectively reduces model size and computational complexity while maintaining performance comparable to the original unpruned model even at extreme pruned levels. The improved method is based on weights, bias fixed in the pre-training phase and GRPO rewards verified during the post-training phase as our novel pruning indicators. Preliminary experimental results show that our method outperforms and be comparable to state-of-the-art methods across various pruning levels and different downstream computational linguistics tasks."
  },
  {
    "title": "Learning Concept-Driven Logical Rules for Interpretable and Generalizable Medical Image Classification",
    "url": "http://arxiv.org/abs/2505.14049v1",
    "arxiv_id": "2505.14049v1",
    "authors": [
      "Yibo Gao",
      "Hangqi Zhou",
      "Zheyao Gao",
      "Bomin Wang",
      "Shangqi Gao",
      "Sihan Wang",
      "Xiahai Zhuang"
    ],
    "published": "2025-05-20T07:48:33+00:00",
    "summary": "The pursuit of decision safety in clinical applications highlights the potential of concept-based methods in medical imaging. While these models offer active interpretability, they often suffer from concept leakages, where unintended information within soft concept representations undermines both interpretability and generalizability. Moreover, most concept-based models focus solely on local explanations (instance-level), neglecting the global decision logic (dataset-level). To address these limitations, we propose Concept Rule Learner (CRL), a novel framework to learn Boolean logical rules from binarized visual concepts. CRL employs logical layers to capture concept correlations and extract clinically meaningful rules, thereby providing both local and global interpretability. Experiments on two medical image classification tasks show that CRL achieves competitive performance with existing methods while significantly improving generalizability to out-of-distribution data. The code of our work is available at https://github.com/obiyoag/crl."
  },
  {
    "title": "VeRecycle: Reclaiming Guarantees from Probabilistic Certificates for Stochastic Dynamical Systems after Change",
    "url": "http://arxiv.org/abs/2505.14001v1",
    "arxiv_id": "2505.14001v1",
    "authors": [
      "Sterre Lutz",
      "Matthijs T. J. Spaan",
      "Anna Lukina"
    ],
    "published": "2025-05-20T06:54:19+00:00",
    "summary": "Autonomous systems operating in the real world encounter a range of uncertainties. Probabilistic neural Lyapunov certification is a powerful approach to proving safety of nonlinear stochastic dynamical systems. When faced with changes beyond the modeled uncertainties, e.g., unidentified obstacles, probabilistic certificates must be transferred to the new system dynamics. However, even when the changes are localized in a known part of the state space, state-of-the-art requires complete re-certification, which is particularly costly for neural certificates. We introduce VeRecycle, the first framework to formally reclaim guarantees for discrete-time stochastic dynamical systems. VeRecycle efficiently reuses probabilistic certificates when the system dynamics deviate only in a given subset of states. We present a general theoretical justification and algorithmic implementation. Our experimental evaluation shows scenarios where VeRecycle both saves significant computational effort and achieves competitive probabilistic guarantees in compositional neural control."
  },
  {
    "title": "Social Sycophancy: A Broader Understanding of LLM Sycophancy",
    "url": "http://arxiv.org/abs/2505.13995v1",
    "arxiv_id": "2505.13995v1",
    "authors": [
      "Myra Cheng",
      "Sunny Yu",
      "Cinoo Lee",
      "Pranav Khadpe",
      "Lujain Ibrahim",
      "Dan Jurafsky"
    ],
    "published": "2025-05-20T06:45:17+00:00",
    "summary": "A serious risk to the safety and utility of LLMs is sycophancy, i.e., excessive agreement with and flattery of the user. Yet existing work focuses on only one aspect of sycophancy: agreement with users' explicitly stated beliefs that can be compared to a ground truth. This overlooks forms of sycophancy that arise in ambiguous contexts such as advice and support-seeking, where there is no clear ground truth, yet sycophancy can reinforce harmful implicit assumptions, beliefs, or actions. To address this gap, we introduce a richer theory of social sycophancy in LLMs, characterizing sycophancy as the excessive preservation of a user's face (the positive self-image a person seeks to maintain in an interaction). We present ELEPHANT, a framework for evaluating social sycophancy across five face-preserving behaviors (emotional validation, moral endorsement, indirect language, indirect action, and accepting framing) on two datasets: open-ended questions (OEQ) and Reddit's r/AmITheAsshole (AITA). Across eight models, we show that LLMs consistently exhibit high rates of social sycophancy: on OEQ, they preserve face 47% more than humans, and on AITA, they affirm behavior deemed inappropriate by crowdsourced human judgments in 42% of cases. We further show that social sycophancy is rewarded in preference datasets and is not easily mitigated. Our work provides theoretical grounding and empirical tools (datasets and code) for understanding and addressing this under-recognized but consequential issue."
  },
  {
    "title": "FlashThink: An Early Exit Method For Efficient Reasoning",
    "url": "http://arxiv.org/abs/2505.13949v1",
    "arxiv_id": "2505.13949v1",
    "authors": [
      "Guochao Jiang",
      "Guofeng Quan",
      "Zepeng Ding",
      "Ziqin Luo",
      "Dixuan Wang",
      "Zheng Hu"
    ],
    "published": "2025-05-20T05:28:21+00:00",
    "summary": "Large Language Models (LLMs) have shown impressive performance in reasoning tasks. However, LLMs tend to generate excessively long reasoning content, leading to significant computational overhead. Our observations indicate that even on simple problems, LLMs tend to produce unnecessarily lengthy reasoning content, which is against intuitive expectations. Preliminary experiments show that at a certain point during the generation process, the model is already capable of producing the correct solution without completing the full reasoning content. Therefore, we consider that the reasoning process of the model can be exited early to achieve the purpose of efficient reasoning. We introduce a verification model that identifies the exact moment when the model can stop reasoning and still provide the correct answer. Comprehensive experiments on four different benchmarks demonstrate that our proposed method, FlashThink, effectively shortens the reasoning content while preserving the model accuracy. For the Deepseek-R1 and QwQ-32B models, we reduced the length of reasoning content by 77.04% and 77.47%, respectively, without reducing the accuracy."
  },
  {
    "title": "D4+: Emergent Adversarial Driving Maneuvers with Approximate Functional Optimization",
    "url": "http://arxiv.org/abs/2505.13942v1",
    "arxiv_id": "2505.13942v1",
    "authors": [
      "Diego Ortiz Barbosa",
      "Luis Burbano",
      "Carlos Hernandez",
      "Zengxiang Lei",
      "Younghee Park",
      "Satish Ukkusuri",
      "Alvaro A Cardenas"
    ],
    "published": "2025-05-20T05:22:03+00:00",
    "summary": "Intelligent mechanisms implemented in autonomous vehicles, such as proactive driving assist and collision alerts, reduce traffic accidents. However, verifying their correct functionality is difficult due to complex interactions with the environment. This problem is exacerbated in adversarial environments, where an attacker can control the environment surrounding autonomous vehicles to exploit vulnerabilities.   To preemptively identify vulnerabilities in these systems, in this paper, we implement a scenario-based framework with a formal method to identify the impact of malicious drivers interacting with autonomous vehicles. The formalization of the evaluation requirements utilizes metric temporal logic (MTL) to identify a safety condition that we want to test. Our goal is to find, through a rigorous testing approach, any trace that violates this MTL safety specification. Our results can help designers identify the range of safe operational behaviors that prevent malicious drivers from exploiting the autonomous features of modern vehicles."
  },
  {
    "title": "Certifiably Safe Manipulation of Deformable Linear Objects via Joint Shape and Tension Prediction",
    "url": "http://arxiv.org/abs/2505.13889v1",
    "arxiv_id": "2505.13889v1",
    "authors": [
      "Yiting Zhang",
      "Shichen Li"
    ],
    "published": "2025-05-20T03:50:29+00:00",
    "summary": "Manipulating deformable linear objects (DLOs) is challenging due to their complex dynamics and the need for safe interaction in contact-rich environments. Most existing models focus on shape prediction alone and fail to account for contact and tension constraints, which can lead to damage to both the DLO and the robot. In this work, we propose a certifiably safe motion planning and control framework for DLO manipulation. At the core of our method is a predictive model that jointly estimates the DLO's future shape and tension. These predictions are integrated into a real-time trajectory optimizer based on polynomial zonotopes, allowing us to enforce safety constraints throughout the execution. We evaluate our framework on a simulated wire harness assembly task using a 7-DOF robotic arm. Compared to state-of-the-art methods, our approach achieves a higher task success rate while avoiding all safety violations. The results demonstrate that our method enables robust and safe DLO manipulation in contact-rich environments."
  },
  {
    "title": "Safety2Drive: Safety-Critical Scenario Benchmark for the Evaluation of Autonomous Driving",
    "url": "http://arxiv.org/abs/2505.13872v1",
    "arxiv_id": "2505.13872v1",
    "authors": [
      "Jingzheng Li",
      "Tiancheng Wang",
      "Xingyu Peng",
      "Jiacheng Chen",
      "Zhijun Chen",
      "Bing Li",
      "Xianglong Liu"
    ],
    "published": "2025-05-20T03:27:06+00:00",
    "summary": "Autonomous Driving (AD) systems demand the high levels of safety assurance. Despite significant advancements in AD demonstrated on open-source benchmarks like Longest6 and Bench2Drive, existing datasets still lack regulatory-compliant scenario libraries for closed-loop testing to comprehensively evaluate the functional safety of AD. Meanwhile, real-world AD accidents are underrepresented in current driving datasets. This scarcity leads to inadequate evaluation of AD performance, posing risks to safety validation and practical deployment. To address these challenges, we propose Safety2Drive, a safety-critical scenario library designed to evaluate AD systems. Safety2Drive offers three key contributions. (1) Safety2Drive comprehensively covers the test items required by standard regulations and contains 70 AD function test items. (2) Safety2Drive supports the safety-critical scenario generalization. It has the ability to inject safety threats such as natural environment corruptions and adversarial attacks cross camera and LiDAR sensors. (3) Safety2Drive supports multi-dimensional evaluation. In addition to the evaluation of AD systems, it also supports the evaluation of various perception tasks, such as object detection and lane detection. Safety2Drive provides a paradigm from scenario construction to validation, establishing a standardized test framework for the safe deployment of AD."
  },
  {
    "title": "PandaGuard: Systematic Evaluation of LLM Safety in the Era of Jailbreaking Attacks",
    "url": "http://arxiv.org/abs/2505.13862v1",
    "arxiv_id": "2505.13862v1",
    "authors": [
      "Guobin Shen",
      "Dongcheng Zhao",
      "Linghao Feng",
      "Xiang He",
      "Jihang Wang",
      "Sicheng Shen",
      "Haibo Tong",
      "Yiting Dong",
      "Jindong Li",
      "Xiang Zheng",
      "Yi Zeng"
    ],
    "published": "2025-05-20T03:14:57+00:00",
    "summary": "Large language models (LLMs) have achieved remarkable capabilities but remain vulnerable to adversarial prompts known as jailbreaks, which can bypass safety alignment and elicit harmful outputs. Despite growing efforts in LLM safety research, existing evaluations are often fragmented, focused on isolated attack or defense techniques, and lack systematic, reproducible analysis. In this work, we introduce PandaGuard, a unified and modular framework that models LLM jailbreak safety as a multi-agent system comprising attackers, defenders, and judges. Our framework implements 19 attack methods and 12 defense mechanisms, along with multiple judgment strategies, all within a flexible plugin architecture supporting diverse LLM interfaces, multiple interaction modes, and configuration-driven experimentation that enhances reproducibility and practical deployment. Built on this framework, we develop PandaBench, a comprehensive benchmark that evaluates the interactions between these attack/defense methods across 49 LLMs and various judgment approaches, requiring over 3 billion tokens to execute. Our extensive evaluation reveals key insights into model vulnerabilities, defense cost-performance trade-offs, and judge consistency. We find that no single defense is optimal across all dimensions and that judge disagreement introduces nontrivial variance in safety assessments. We release the code, configurations, and evaluation results to support transparent and reproducible research in LLM safety."
  },
  {
    "title": "Enforcing Hard Linear Constraints in Deep Learning Models with Decision Rules",
    "url": "http://arxiv.org/abs/2505.13858v1",
    "arxiv_id": "2505.13858v1",
    "authors": [
      "Gonzalo E. Constante-Flores",
      "Hao Chen",
      "Can Li"
    ],
    "published": "2025-05-20T03:09:44+00:00",
    "summary": "Deep learning models are increasingly deployed in safety-critical tasks where predictions must satisfy hard constraints, such as physical laws, fairness requirements, or safety limits. However, standard architectures lack built-in mechanisms to enforce such constraints, and existing approaches based on regularization or projection are often limited to simple constraints, computationally expensive, or lack feasibility guarantees. This paper proposes a model-agnostic framework for enforcing input-dependent linear equality and inequality constraints on neural network outputs. The architecture combines a task network trained for prediction accuracy with a safe network trained using decision rules from the stochastic and robust optimization literature to ensure feasibility across the entire input space. The final prediction is a convex combination of the two subnetworks, guaranteeing constraint satisfaction during both training and inference without iterative procedures or runtime optimization. We prove that the architecture is a universal approximator of constrained functions and derive computationally tractable formulations based on linear decision rules. Empirical results on benchmark regression tasks show that our method consistently satisfies constraints while maintaining competitive accuracy and low inference latency."
  },
  {
    "title": "Provable Execution in Real-Time Embedded Systems",
    "url": "http://arxiv.org/abs/2505.13842v1",
    "arxiv_id": "2505.13842v1",
    "authors": [
      "Antonio Joia Neto",
      "Norrathep Rattanavipanon",
      "Ivan De Oliveira Nunes"
    ],
    "published": "2025-05-20T02:31:13+00:00",
    "summary": "Embedded devices are increasingly ubiquitous and vital, often supporting safety-critical functions. However, due to strict cost and energy constraints, they are typically implemented with Micro-Controller Units (MCUs) that lack advanced architectural security features. Within this space, recent efforts have created low-cost architectures capable of generating Proofs of Execution (PoX) of software on potentially compromised MCUs. This capability can ensure the integrity of sensor data from the outset, by binding sensed results to an unforgeable cryptographic proof of execution on edge sensor MCUs. However, the security of existing PoX requires the proven execution to occur atomically. This requirement precludes the application of PoX to (1) time-shared systems, and (2) applications with real-time constraints, creating a direct conflict between execution integrity and the real-time availability needs of several embedded system uses.   In this paper, we formulate a new security goal called Real-Time Proof of Execution (RT-PoX) that retains the integrity guarantees of classic PoX while enabling its application to existing real-time systems. This is achieved by relaxing the atomicity requirement of PoX while dispatching interference attempts from other potentially malicious tasks (or compromised operating systems) executing on the same device. To realize the RT-PoX goal, we develop Provable Execution Architecture for Real-Time Systems (PEARTS). To the best of our knowledge, PEARTS is the first PoX system that can be directly deployed alongside a commodity embedded real-time operating system (FreeRTOS). This enables both real-time scheduling and execution integrity guarantees on commodity MCUs. To showcase this capability, we develop a PEARTS open-source prototype atop FreeRTOS on a single-core ARM Cortex-M33 processor. We evaluate and report on PEARTS security and (modest) overheads."
  },
  {
    "title": "Interpretable Traces, Unexpected Outcomes: Investigating the Disconnect in Trace-Based Knowledge Distillation",
    "url": "http://arxiv.org/abs/2505.13792v1",
    "arxiv_id": "2505.13792v1",
    "authors": [
      "Siddhant Bhambri",
      "Upasana Biswas",
      "Subbarao Kambhampati"
    ],
    "published": "2025-05-20T00:49:19+00:00",
    "summary": "Question Answering (QA) poses a challenging and critical problem, particularly in today's age of interactive dialogue systems such as ChatGPT, Perplexity, Microsoft Copilot, etc. where users demand both accuracy and transparency in the model's outputs. Since smaller language models (SLMs) are computationally more efficient but often under-perform compared to larger models, Knowledge Distillation (KD) methods allow for finetuning these smaller models to improve their final performance. Lately, the intermediate tokens or the so called `reasoning' traces produced by Chain-of-Thought (CoT) or by reasoning models such as DeepSeek R1 are used as a training signal for KD. However, these reasoning traces are often verbose and difficult to interpret or evaluate. In this work, we aim to address the challenge of evaluating the faithfulness of these reasoning traces and their correlation with the final performance. To this end, we employ a KD method leveraging rule-based problem decomposition. This approach allows us to break down complex queries into structured sub-problems, generating interpretable traces whose correctness can be readily evaluated, even at inference time. Specifically, we demonstrate this approach on Open Book QA, decomposing the problem into a Classification step and an Information Retrieval step, thereby simplifying trace evaluation. Our SFT experiments with correct and incorrect traces on the CoTemp QA, Microsoft Machine Reading Comprehension QA, and Facebook bAbI QA datasets reveal the striking finding that correct traces do not necessarily imply that the model outputs the correct final solution. Similarly, we find a low correlation between correct final solutions and intermediate trace correctness. These results challenge the implicit assumption behind utilizing reasoning traces for improving SLMs' final performance via KD."
  },
  {
    "title": "Language Models Are Capable of Metacognitive Monitoring and Control of Their Internal Activations",
    "url": "http://arxiv.org/abs/2505.13763v1",
    "arxiv_id": "2505.13763v1",
    "authors": [
      "Li Ji-An",
      "Hua-Dong Xiong",
      "Robert C. Wilson",
      "Marcelo G. Mattar",
      "Marcus K. Benna"
    ],
    "published": "2025-05-19T22:32:25+00:00",
    "summary": "Large language models (LLMs) can sometimes report the strategies they actually use to solve tasks, but they can also fail to do so. This suggests some degree of metacognition -- the capacity to monitor one's own cognitive processes for subsequent reporting and self-control. Metacognitive abilities enhance AI capabilities but raise safety concerns, as models might obscure their internal processes to evade neural-activation-based oversight mechanisms designed to detect harmful behaviors. Given society's increased reliance on these models, it is critical that we understand the limits of their metacognitive abilities, particularly their ability to monitor their internal activations. To address this, we introduce a neuroscience-inspired neurofeedback paradigm designed to quantify the ability of LLMs to explicitly report and control their activation patterns. By presenting models with sentence-label pairs where labels correspond to sentence-elicited internal activations along specific directions in the neural representation space, we demonstrate that LLMs can learn to report and control these activations. The performance varies with several factors: the number of example pairs provided, the semantic interpretability of the target neural direction, and the variance explained by that direction. These results reveal a \"metacognitive space\" with dimensionality much lower than the model's neural space, suggesting LLMs can monitor only a subset of their neural mechanisms. Our findings provide empirical evidence quantifying metacognitive capabilities in LLMs, with significant implications for AI safety."
  },
  {
    "title": "From Structural Design to Dynamics Modeling: Control-Oriented Development of a 3-RRR Parallel Ankle Rehabilitation Robot",
    "url": "http://arxiv.org/abs/2505.13762v1",
    "arxiv_id": "2505.13762v1",
    "authors": [
      "Siyuan Zhang",
      "Yufei Zhang",
      "Junlin Lyu",
      "Sunil K. Agrawal"
    ],
    "published": "2025-05-19T22:28:05+00:00",
    "summary": "This paper presents the development of a wearable ankle rehabilitation robot based on a 3-RRR spherical parallel mechanism (SPM) to support multi-DOF recovery through pitch, roll, and yaw motions. The system features a compact, ergonomic structure designed for comfort, safety, and compatibility with ankle biomechanics. A complete design-to-dynamics pipeline has been implemented, including structural design, kinematic modeling for motion planning, and Lagrangian-based dynamic modeling for torque estimation and simulation analysis. Preliminary simulations verify stable joint coordination and smooth motion tracking under representative rehabilitation trajectories. The control framework is currently being developed to enhance responsiveness across the workspace. Future work will focus on integrating personalized modeling and adaptive strategies to address kinematic singularities through model based control. This work establishes a foundational platform for intelligent, personalized ankle rehabilitation, enabling both static training and potential extension to gait-phase-timed assistance."
  },
  {
    "title": "ReSW-VL: Representation Learning for Surgical Workflow Analysis Using Vision-Language Model",
    "url": "http://arxiv.org/abs/2505.13746v1",
    "arxiv_id": "2505.13746v1",
    "authors": [
      "Satoshi Kondo"
    ],
    "published": "2025-05-19T21:44:37+00:00",
    "summary": "Surgical phase recognition from video is a technology that automatically classifies the progress of a surgical procedure and has a wide range of potential applications, including real-time surgical support, optimization of medical resources, training and skill assessment, and safety improvement. Recent advances in surgical phase recognition technology have focused primarily on Transform-based methods, although methods that extract spatial features from individual frames using a CNN and video features from the resulting time series of spatial features using time series modeling have shown high performance. However, there remains a paucity of research on training methods for CNNs employed for feature extraction or representation learning in surgical phase recognition. In this study, we propose a method for representation learning in surgical workflow analysis using a vision-language model (ReSW-VL). Our proposed method involves fine-tuning the image encoder of a CLIP (Convolutional Language Image Model) vision-language model using prompt learning for surgical phase recognition. The experimental results on three surgical phase recognition datasets demonstrate the effectiveness of the proposed method in comparison to conventional methods."
  },
  {
    "title": "RL in Name Only? Analyzing the Structural Assumptions in RL post-training for LLMs",
    "url": "http://arxiv.org/abs/2505.13697v1",
    "arxiv_id": "2505.13697v1",
    "authors": [
      "Soumya Rani Samineni",
      "Durgesh Kalwar",
      "Karthik Valmeekam",
      "Kaya Stechly",
      "Subbarao Kambhampati"
    ],
    "published": "2025-05-19T19:57:15+00:00",
    "summary": "Reinforcement learning-based post-training of large language models (LLMs) has recently gained attention, particularly following the release of DeepSeek R1, which applied GRPO for fine-tuning. Amid the growing hype around improved reasoning abilities attributed to RL post-training, we critically examine the formulation and assumptions underlying these methods. We start by highlighting the popular structural assumptions made in modeling LLM training as a Markov Decision Process (MDP), and show how they lead to a degenerate MDP that doesn't quite need the RL/GRPO apparatus. The two critical structural assumptions include (1) making the MDP states be just a concatenation of the actions-with states becoming the context window and the actions becoming the tokens in LLMs and (2) splitting the reward of a state-action trajectory uniformly across the trajectory. Through a comprehensive analysis, we demonstrate that these simplifying assumptions make the approach effectively equivalent to an outcome-driven supervised learning. Our experiments on benchmarks including GSM8K and Countdown using Qwen-2.5 base models show that iterative supervised fine-tuning, incorporating both positive and negative samples, achieves performance comparable to GRPO-based training. We will also argue that the structural assumptions indirectly incentivize the RL to generate longer sequences of intermediate tokens-which in turn feeds into the narrative of \"RL generating longer thinking traces.\" While RL may well be a very useful technique for improving the reasoning abilities of LLMs, our analysis shows that the simplistic structural assumptions made in modeling the underlying MDP render the popular LLM RL frameworks and their interpretations questionable."
  },
  {
    "title": "AdaptThink: Reasoning Models Can Learn When to Think",
    "url": "http://arxiv.org/abs/2505.13417v1",
    "arxiv_id": "2505.13417v1",
    "authors": [
      "Jiajie Zhang",
      "Nianyi Lin",
      "Lei Hou",
      "Ling Feng",
      "Juanzi Li"
    ],
    "published": "2025-05-19T17:50:52+00:00",
    "summary": "Recently, large reasoning models have achieved impressive performance on various tasks by employing human-like deep thinking. However, the lengthy thinking process substantially increases inference overhead, making efficiency a critical bottleneck. In this work, we first demonstrate that NoThinking, which prompts the reasoning model to skip thinking and directly generate the final solution, is a better choice for relatively simple tasks in terms of both performance and efficiency. Motivated by this, we propose AdaptThink, a novel RL algorithm to teach reasoning models to choose the optimal thinking mode adaptively based on problem difficulty. Specifically, AdaptThink features two core components: (1) a constrained optimization objective that encourages the model to choose NoThinking while maintaining the overall performance; (2) an importance sampling strategy that balances Thinking and NoThinking samples during on-policy training, thereby enabling cold start and allowing the model to explore and exploit both thinking modes throughout the training process. Our experiments indicate that AdaptThink significantly reduces the inference costs while further enhancing performance. Notably, on three math datasets, AdaptThink reduces the average response length of DeepSeek-R1-Distill-Qwen-1.5B by 53% and improves its accuracy by 2.4%, highlighting the promise of adaptive thinking-mode selection for optimizing the balance between reasoning quality and efficiency. Our codes and models are available at https://github.com/THU-KEG/AdaptThink."
  },
  {
    "title": "GUARD: Generation-time LLM Unlearning via Adaptive Restriction and Detection",
    "url": "http://arxiv.org/abs/2505.13312v1",
    "arxiv_id": "2505.13312v1",
    "authors": [
      "Zhijie Deng",
      "Chris Yuhao Liu",
      "Zirui Pang",
      "Xinlei He",
      "Lei Feng",
      "Qi Xuan",
      "Zhaowei Zhu",
      "Jiaheng Wei"
    ],
    "published": "2025-05-19T16:26:58+00:00",
    "summary": "Large Language Models (LLMs) have demonstrated strong capabilities in memorizing vast amounts of knowledge across diverse domains. However, the ability to selectively forget specific knowledge is critical for ensuring the safety and compliance of deployed models. Existing unlearning efforts typically fine-tune the model with resources such as forget data, retain data, and a calibration model. These additional gradient steps blur the decision boundary between forget and retain knowledge, making unlearning often at the expense of overall performance. To avoid the negative impact of fine-tuning, it would be better to unlearn solely at inference time by safely guarding the model against generating responses related to the forget target, without destroying the fluency of text generation. In this work, we propose Generation-time Unlearning via Adaptive Restriction and Detection (GUARD), a framework that enables dynamic unlearning during LLM generation. Specifically, we first employ a prompt classifier to detect unlearning targets and extract the corresponding forbidden token. We then dynamically penalize and filter candidate tokens during generation using a combination of token matching and semantic matching, effectively preventing the model from leaking the forgotten content. Experimental results on copyright content unlearning tasks over the Harry Potter dataset and the MUSE benchmark, as well as entity unlearning tasks on the TOFU dataset, demonstrate that GUARD achieves strong forget quality across various tasks while causing almost no degradation to the LLM's general capabilities, striking an excellent trade-off between forgetting and utility."
  },
  {
    "title": "I'll believe it when I see it: Images increase misinformation sharing in Vision-Language Models",
    "url": "http://arxiv.org/abs/2505.13302v1",
    "arxiv_id": "2505.13302v1",
    "authors": [
      "Alice Plebe",
      "Timothy Douglas",
      "Diana Riazi",
      "R. Maria del Rio-Chanona"
    ],
    "published": "2025-05-19T16:20:54+00:00",
    "summary": "Large language models are increasingly integrated into news recommendation systems, raising concerns about their role in spreading misinformation. In humans, visual content is known to boost credibility and shareability of information, yet its effect on vision-language models (VLMs) remains unclear. We present the first study examining how images influence VLMs' propensity to reshare news content, whether this effect varies across model families, and how persona conditioning and content attributes modulate this behavior. To support this analysis, we introduce two methodological contributions: a jailbreaking-inspired prompting strategy that elicits resharing decisions from VLMs while simulating users with antisocial traits and political alignments; and a multimodal dataset of fact-checked political news from PolitiFact, paired with corresponding images and ground-truth veracity labels. Experiments across model families reveal that image presence increases resharing rates by 4.8% for true news and 15.0% for false news. Persona conditioning further modulates this effect: Dark Triad traits amplify resharing of false news, whereas Republican-aligned profiles exhibit reduced veracity sensitivity. Of all the tested models, only Claude-3-Haiku demonstrates robustness to visual misinformation. These findings highlight emerging risks in multimodal model behavior and motivate the development of tailored evaluation frameworks and mitigation strategies for personalized AI systems. Code and dataset are available at: https://github.com/3lis/misinfo_vlm"
  },
  {
    "title": "Effective and Transparent RAG: Adaptive-Reward Reinforcement Learning for Decision Traceability",
    "url": "http://arxiv.org/abs/2505.13258v1",
    "arxiv_id": "2505.13258v1",
    "authors": [
      "Jingyi Ren",
      "Yekun Xu",
      "Xiaolong Wang",
      "Weitao Li",
      "Weizhi Ma",
      "Yang Liu"
    ],
    "published": "2025-05-19T15:40:29+00:00",
    "summary": "Retrieval-Augmented Generation (RAG) has significantly improved the performance of large language models (LLMs) on knowledge-intensive domains. However, although RAG achieved successes across distinct domains, there are still some unsolved challenges: 1) Effectiveness. Existing research mainly focuses on developing more powerful RAG retrievers, but how to enhance the generator's (LLM's) ability to utilize the retrieved information for reasoning and generation? 2) Transparency. Most RAG methods ignore which retrieved content actually contributes to the reasoning process, resulting in a lack of interpretability and visibility. To address this, we propose ARENA (Adaptive-Rewarded Evidence Navigation Agent), a transparent RAG generator framework trained via reinforcement learning (RL) with our proposed rewards. Based on the structured generation and adaptive reward calculation, our RL-based training enables the model to identify key evidence, perform structured reasoning, and generate answers with interpretable decision traces. Applied to Qwen2.5-7B-Instruct and Llama3.1-8B-Instruct, abundant experiments with various RAG baselines demonstrate that our model achieves 10-30% improvements on all multi-hop QA datasets, which is comparable with the SOTA Commercially-developed LLMs (e.g., OpenAI-o1, DeepSeek-R1). Further analyses show that ARENA has strong flexibility to be adopted on new datasets without extra training. Our models and codes are publicly released."
  },
  {
    "title": "Adversarial Testing in LLMs: Insights into Decision-Making Vulnerabilities",
    "url": "http://arxiv.org/abs/2505.13195v1",
    "arxiv_id": "2505.13195v1",
    "authors": [
      "Lili Zhang",
      "Haomiaomiao Wang",
      "Long Cheng",
      "Libao Deng",
      "Tomas Ward"
    ],
    "published": "2025-05-19T14:50:44+00:00",
    "summary": "As Large Language Models (LLMs) become increasingly integrated into real-world decision-making systems, understanding their behavioural vulnerabilities remains a critical challenge for AI safety and alignment. While existing evaluation metrics focus primarily on reasoning accuracy or factual correctness, they often overlook whether LLMs are robust to adversarial manipulation or capable of using adaptive strategy in dynamic environments. This paper introduces an adversarial evaluation framework designed to systematically stress-test the decision-making processes of LLMs under interactive and adversarial conditions. Drawing on methodologies from cognitive psychology and game theory, our framework probes how models respond in two canonical tasks: the two-armed bandit task and the Multi-Round Trust Task. These tasks capture key aspects of exploration-exploitation trade-offs, social cooperation, and strategic flexibility. We apply this framework to several state-of-the-art LLMs, including GPT-3.5, GPT-4, Gemini-1.5, and DeepSeek-V3, revealing model-specific susceptibilities to manipulation and rigidity in strategy adaptation. Our findings highlight distinct behavioral patterns across models and emphasize the importance of adaptability and fairness recognition for trustworthy AI deployment. Rather than offering a performance benchmark, this work proposes a methodology for diagnosing decision-making weaknesses in LLM-based agents, providing actionable insights for alignment and safety research."
  },
  {
    "title": "Interpretable Robotic Friction Learning via Symbolic Regression",
    "url": "http://arxiv.org/abs/2505.13186v1",
    "arxiv_id": "2505.13186v1",
    "authors": [
      "Philipp Scholl",
      "Alexander Dietrich",
      "Sebastian Wolf",
      "Jinoh Lee",
      "Alin-Albu Sch\u00e4ffer",
      "Gitta Kutyniok",
      "Maged Iskandar"
    ],
    "published": "2025-05-19T14:44:02+00:00",
    "summary": "Accurately modeling the friction torque in robotic joints has long been challenging due to the request for a robust mathematical description. Traditional model-based approaches are often labor-intensive, requiring extensive experiments and expert knowledge, and they are difficult to adapt to new scenarios and dependencies. On the other hand, data-driven methods based on neural networks are easier to implement but often lack robustness, interpretability, and trustworthiness--key considerations for robotic hardware and safety-critical applications such as human-robot interaction. To address the limitations of both approaches, we propose the use of symbolic regression (SR) to estimate the friction torque. SR generates interpretable symbolic formulas similar to those produced by model-based methods while being flexible to accommodate various dynamic effects and dependencies. In this work, we apply SR algorithms to approximate the friction torque using collected data from a KUKA LWR-IV+ robot. Our results show that SR not only yields formulas with comparable complexity to model-based approaches but also achieves higher accuracy. Moreover, SR-derived formulas can be seamlessly extended to include load dependencies and other dynamic factors."
  },
  {
    "title": "Information Science Principles of Machine Learning: A Causal Chain Meta-Framework Based on Formalized Information Mapping",
    "url": "http://arxiv.org/abs/2505.13182v1",
    "arxiv_id": "2505.13182v1",
    "authors": [
      "Jianfeng Xu"
    ],
    "published": "2025-05-19T14:39:41+00:00",
    "summary": "[Objective] This study focuses on addressing the current lack of a unified formal theoretical framework in machine learning, as well as the deficiencies in interpretability and ethical safety assurance. [Methods] A formal information model is first constructed, utilizing sets of well-formed formulas to explicitly define the ontological states and carrier mappings of typical components in machine learning. Learnable and processable predicates, along with learning and processing functions, are introduced to analyze the logical deduction and constraint rules of the causal chains within models. [Results] A meta-framework for machine learning theory (MLT-MF) is established. Based on this framework, universal definitions for model interpretability and ethical safety are proposed. Furthermore, three key theorems are proved: the equivalence of model interpretability and information recoverability, the assurance of ethical safety, and the estimation of generalization error. [Limitations] The current framework assumes ideal conditions with noiseless information-enabling mappings and primarily targets model learning and processing logic in static scenarios. It does not yet address information fusion and conflict resolution across ontological spaces in multimodal or multi-agent systems. [Conclusions] This work overcomes the limitations of fragmented research and provides a unified theoretical foundation for systematically addressing the critical challenges currently faced in machine learning."
  },
  {
    "title": "Constraint-Aware Diffusion Guidance for Robotics: Real-Time Obstacle Avoidance for Autonomous Racing",
    "url": "http://arxiv.org/abs/2505.13131v1",
    "arxiv_id": "2505.13131v1",
    "authors": [
      "Hao Ma",
      "Sabrina Bodmer",
      "Andrea Carron",
      "Melanie Zeilinger",
      "Michael Muehlebach"
    ],
    "published": "2025-05-19T14:00:17+00:00",
    "summary": "Diffusion models hold great potential in robotics due to their ability to capture complex, high-dimensional data distributions. However, their lack of constraint-awareness limits their deployment in safety-critical applications. We propose Constraint-Aware Diffusion Guidance (CoDiG), a data-efficient and general-purpose framework that integrates barrier functions into the denoising process, guiding diffusion sampling toward constraint-satisfying outputs. CoDiG enables constraint satisfaction even with limited training data and generalizes across tasks. We evaluate our framework in the challenging setting of miniature autonomous racing, where real-time obstacle avoidance is essential. Real-world experiments show that CoDiG generates safe outputs efficiently under dynamic conditions, highlighting its potential for broader robotic applications. A demonstration video is available at https://youtu.be/KNYsTdtdxOU."
  },
  {
    "title": "Evaluatiing the efficacy of LLM Safety Solutions : The Palit Benchmark Dataset",
    "url": "http://arxiv.org/abs/2505.13028v1",
    "arxiv_id": "2505.13028v1",
    "authors": [
      "Sayon Palit",
      "Daniel Woods"
    ],
    "published": "2025-05-19T12:12:00+00:00",
    "summary": "Large Language Models (LLMs) are increasingly integrated into critical systems in industries like healthcare and finance. Users can often submit queries to LLM-enabled chatbots, some of which can enrich responses with information retrieved from internal databases storing sensitive data. This gives rise to a range of attacks in which a user submits a malicious query and the LLM-system outputs a response that creates harm to the owner, such as leaking internal data or creating legal liability by harming a third-party. While security tools are being developed to counter these threats, there is little formal evaluation of their effectiveness and usability. This study addresses this gap by conducting a thorough comparative analysis of LLM security tools. We identified 13 solutions (9 closed-source, 4 open-source), but only 7 were evaluated due to a lack of participation by proprietary model owners.To evaluate, we built a benchmark dataset of malicious prompts, and evaluate these tools performance against a baseline LLM model (ChatGPT-3.5-Turbo). Our results show that the baseline model has too many false positives to be used for this task. Lakera Guard and ProtectAI LLM Guard emerged as the best overall tools showcasing the tradeoff between usability and performance. The study concluded with recommendations for greater transparency among closed source providers, improved context-aware detections, enhanced open-source engagement, increased user awareness, and the adoption of more representative performance metrics."
  },
  {
    "title": "Evaluating the Performance of RAG Methods for Conversational AI in the Airport Domain",
    "url": "http://arxiv.org/abs/2505.13006v1",
    "arxiv_id": "2505.13006v1",
    "authors": [
      "Yuyang Li",
      "Philip J. M. Kerbusch",
      "Raimon H. R. Pruim",
      "Tobias K\u00e4fer"
    ],
    "published": "2025-05-19T11:46:30+00:00",
    "summary": "Airports from the top 20 in terms of annual passengers are highly dynamic environments with thousands of flights daily, and they aim to increase the degree of automation. To contribute to this, we implemented a Conversational AI system that enables staff in an airport to communicate with flight information systems. This system not only answers standard airport queries but also resolves airport terminology, jargon, abbreviations, and dynamic questions involving reasoning. In this paper, we built three different Retrieval-Augmented Generation (RAG) methods, including traditional RAG, SQL RAG, and Knowledge Graph-based RAG (Graph RAG). Experiments showed that traditional RAG achieved 84.84% accuracy using BM25 + GPT-4 but occasionally produced hallucinations, which is risky to airport safety. In contrast, SQL RAG and Graph RAG achieved 80.85% and 91.49% accuracy respectively, with significantly fewer hallucinations. Moreover, Graph RAG was especially effective for questions that involved reasoning. Based on our observations, we thus recommend SQL RAG and Graph RAG are better for airport environments, due to fewer hallucinations and the ability to handle dynamic questions."
  },
  {
    "title": "EffiBench-X: A Multi-Language Benchmark for Measuring Efficiency of LLM-Generated Code",
    "url": "http://arxiv.org/abs/2505.13004v1",
    "arxiv_id": "2505.13004v1",
    "authors": [
      "Yuhao Qing",
      "Boyu Zhu",
      "Mingzhe Du",
      "Zhijiang Guo",
      "Terry Yue Zhuo",
      "Qianru Zhang",
      "Jie M. Zhang",
      "Heming Cui",
      "Siu-Ming Yiu",
      "Dong Huang",
      "See-Kiong Ng",
      "Luu Anh Tuan"
    ],
    "published": "2025-05-19T11:43:37+00:00",
    "summary": "Existing code generation benchmarks primarily evaluate functional correctness, with limited focus on code efficiency and often restricted to a single language like Python. To address this gap, we introduce EffiBench-X, the first multi-language benchmark designed to measure the efficiency of LLM-generated code. EffiBench-X supports Python, C++, Java, JavaScript, Ruby, and Golang. It comprises competitive programming tasks with human-expert solutions as efficiency baselines. Evaluating state-of-the-art LLMs on EffiBench-X reveals that while models generate functionally correct code, they consistently underperform human experts in efficiency. Even the most efficient LLM-generated solutions (Qwen3-32B) achieve only around \\textbf{62\\%} of human efficiency on average, with significant language-specific variations. LLMs show better efficiency in Python, Ruby, and JavaScript than in Java, C++, and Golang. For instance, DeepSeek-R1's Python code is significantly more efficient than its Java code. These results highlight the critical need for research into LLM optimization techniques to improve code efficiency across diverse languages. The dataset and evaluation infrastructure are submitted and available at https://github.com/EffiBench/EffiBench-X.git and https://huggingface.co/datasets/EffiBench/effibench-x."
  },
  {
    "title": "ExTrans: Multilingual Deep Reasoning Translation via Exemplar-Enhanced Reinforcement Learning",
    "url": "http://arxiv.org/abs/2505.12996v1",
    "arxiv_id": "2505.12996v1",
    "authors": [
      "Jiaan Wang",
      "Fandong Meng",
      "Jie Zhou"
    ],
    "published": "2025-05-19T11:34:47+00:00",
    "summary": "In recent years, the emergence of large reasoning models (LRMs), such as OpenAI-o1 and DeepSeek-R1, has shown impressive capabilities in complex problems, e.g., mathematics and coding. Some pioneering studies attempt to bring the success of LRMs in neural machine translation (MT). They try to build LRMs with deep reasoning MT ability via reinforcement learning (RL). Despite some progress that has been made, these attempts generally focus on several high-resource languages, e.g., English and Chinese, leaving the performance on other languages unclear. Besides, the reward modeling methods in previous work do not fully unleash the potential of reinforcement learning in MT. In this work, we first design a new reward modeling method that compares the translation results of the policy MT model with a strong LRM (i.e., DeepSeek-R1-671B), and quantifies the comparisons to provide rewards. Experimental results demonstrate the superiority of the reward modeling method. Using Qwen2.5-7B-Instruct as the backbone, the trained model achieves the new state-of-the-art performance in literary translation, and outperforms strong LRMs including OpenAI-o1 and DeepSeeK-R1. Furthermore, we extend our method to the multilingual settings with 11 languages. With a carefully designed lightweight reward modeling in RL, we can simply transfer the strong MT ability from a single direction into multiple (i.e., 90) translation directions and achieve impressive multilingual MT performance."
  },
  {
    "title": "LoD: Loss-difference OOD Detection by Intentionally Label-Noisifying Unlabeled Wild Data",
    "url": "http://arxiv.org/abs/2505.12952v1",
    "arxiv_id": "2505.12952v1",
    "authors": [
      "Chuanxing Geng",
      "Qifei Li",
      "Xinrui Wang",
      "Dong Liang",
      "Songcan Chen",
      "Pong C. Yuen"
    ],
    "published": "2025-05-19T10:44:52+00:00",
    "summary": "Using unlabeled wild data containing both in-distribution (ID) and out-of-distribution (OOD) data to improve the safety and reliability of models has recently received increasing attention. Existing methods either design customized losses for labeled ID and unlabeled wild data then perform joint optimization, or first filter out OOD data from the latter then learn an OOD detector. While achieving varying degrees of success, two potential issues remain: (i) Labeled ID data typically dominates the learning of models, inevitably making models tend to fit OOD data as IDs; (ii) The selection of thresholds for identifying OOD data in unlabeled wild data usually faces dilemma due to the unavailability of pure OOD samples. To address these issues, we propose a novel loss-difference OOD detection framework (LoD) by \\textit{intentionally label-noisifying} unlabeled wild data. Such operations not only enable labeled ID data and OOD data in unlabeled wild data to jointly dominate the models' learning but also ensure the distinguishability of the losses between ID and OOD samples in unlabeled wild data, allowing the classic clustering technique (e.g., K-means) to filter these OOD samples without requiring thresholds any longer. We also provide theoretical foundation for LoD's viability, and extensive experiments verify its superiority."
  },
  {
    "title": "ORQA: A Benchmark and Foundation Model for Holistic Operating Room Modeling",
    "url": "http://arxiv.org/abs/2505.12890v1",
    "arxiv_id": "2505.12890v1",
    "authors": [
      "Ege \u00d6zsoy",
      "Chantal Pellegrini",
      "David Bani-Harouni",
      "Kun Yuan",
      "Matthias Keicher",
      "Nassir Navab"
    ],
    "published": "2025-05-19T09:20:29+00:00",
    "summary": "The real-world complexity of surgeries necessitates surgeons to have deep and holistic comprehension to ensure precision, safety, and effective interventions. Computational systems are required to have a similar level of comprehension within the operating room. Prior works, limited to single-task efforts like phase recognition or scene graph generation, lack scope and generalizability. In this work, we introduce ORQA, a novel OR question answering benchmark and foundational multimodal model to advance OR intelligence. By unifying all four public OR datasets into a comprehensive benchmark, we enable our approach to concurrently address a diverse range of OR challenges. The proposed multimodal large language model fuses diverse OR signals such as visual, auditory, and structured data, for a holistic modeling of the OR. Finally, we propose a novel, progressive knowledge distillation paradigm, to generate a family of models optimized for different speed and memory requirements. We show the strong performance of ORQA on our proposed benchmark, and its zero-shot generalization, paving the way for scalable, unified OR modeling and significantly advancing multimodal surgical intelligence. We will release our code and data upon acceptance."
  },
  {
    "title": "GAP: Graph-Assisted Prompts for Dialogue-based Medication Recommendation",
    "url": "http://arxiv.org/abs/2505.12888v1",
    "arxiv_id": "2505.12888v1",
    "authors": [
      "Jialun Zhong",
      "Yanzeng Li",
      "Sen Hu",
      "Yang Zhang",
      "Teng Xu",
      "Lei Zou"
    ],
    "published": "2025-05-19T09:18:19+00:00",
    "summary": "Medication recommendations have become an important task in the healthcare domain, especially in measuring the accuracy and safety of medical dialogue systems (MDS). Different from the recommendation task based on electronic health records (EHRs), dialogue-based medication recommendations require research on the interaction details between patients and doctors, which is crucial but may not exist in EHRs. Recent advancements in large language models (LLM) have extended the medical dialogue domain. These LLMs can interpret patients' intent and provide medical suggestions including medication recommendations, but some challenges are still worth attention. During a multi-turn dialogue, LLMs may ignore the fine-grained medical information or connections across the dialogue turns, which is vital for providing accurate suggestions. Besides, LLMs may generate non-factual responses when there is a lack of domain-specific knowledge, which is more risky in the medical domain. To address these challenges, we propose a \\textbf{G}raph-\\textbf{A}ssisted \\textbf{P}rompts (\\textbf{GAP}) framework for dialogue-based medication recommendation. It extracts medical concepts and corresponding states from dialogue to construct an explicitly patient-centric graph, which can describe the neglected but important information. Further, combined with external medical knowledge graphs, GAP can generate abundant queries and prompts, thus retrieving information from multiple sources to reduce the non-factual responses. We evaluate GAP on a dialogue-based medication recommendation dataset and further explore its potential in a more difficult scenario, dynamically diagnostic interviewing. Extensive experiments demonstrate its competitive performance when compared with strong baselines."
  },
  {
    "title": "Practical Equivalence Testing and Its Application in Synthetic Pre-Crash Scenario Validation",
    "url": "http://arxiv.org/abs/2505.12827v1",
    "arxiv_id": "2505.12827v1",
    "authors": [
      "Jian Wu",
      "Ulrich Sander",
      "Carol Flannagan",
      "Minxiang Zhao",
      "Jonas B\u00e4rgman"
    ],
    "published": "2025-05-19T08:12:35+00:00",
    "summary": "The use of representative pre-crash scenarios is critical for assessing the safety impact of driving automation systems through simulation. However, a gap remains in the robust evaluation of the similarity between synthetic and real-world pre-crash scenarios and their crash characteristics. Without proper validation, it cannot be ensured that the synthetic test scenarios adequately represent real-world driving behaviors and crash characteristics. One reason for this validation gap is the lack of focus on methods to confirm that the synthetic test scenarios are practically equivalent to real-world ones, given the assessment scope. Traditional statistical methods, like significance testing, focus on detecting differences rather than establishing equivalence; since failure to detect a difference does not imply equivalence, they are of limited applicability for validating synthetic pre-crash scenarios and crash characteristics. This study addresses this gap by proposing an equivalence testing method based on the Bayesian Region of Practical Equivalence (ROPE) framework. This method is designed to assess the practical equivalence of scenario characteristics that are most relevant for the intended assessment, making it particularly appropriate for the domain of virtual safety assessments. We first review existing equivalence testing methods. Then we propose and demonstrate the Bayesian ROPE-based method by testing the equivalence of two rear-end pre-crash datasets. Our approach focuses on the most relevant scenario characteristics. Our analysis provides insights into the practicalities and effectiveness of equivalence testing in synthetic test scenario validation and demonstrates the importance of testing for improving the credibility of synthetic data for automated vehicle safety assessment, as well as the credibility of subsequent safety impact assessments."
  },
  {
    "title": "Language Models That Walk the Talk: A Framework for Formal Fairness Certificates",
    "url": "http://arxiv.org/abs/2505.12767v1",
    "arxiv_id": "2505.12767v1",
    "authors": [
      "Danqing Chen",
      "Tobias Ladner",
      "Ahmed Rayen Mhadhbi",
      "Matthias Althoff"
    ],
    "published": "2025-05-19T06:46:17+00:00",
    "summary": "As large language models become integral to high-stakes applications, ensuring their robustness and fairness is critical. Despite their success, large language models remain vulnerable to adversarial attacks, where small perturbations, such as synonym substitutions, can alter model predictions, posing risks in fairness-critical areas, such as gender bias mitigation, and safety-critical areas, such as toxicity detection. While formal verification has been explored for neural networks, its application to large language models remains limited. This work presents a holistic verification framework to certify the robustness of transformer-based language models, with a focus on ensuring gender fairness and consistent outputs across different gender-related terms. Furthermore, we extend this methodology to toxicity detection, offering formal guarantees that adversarially manipulated toxic inputs are consistently detected and appropriately censored, thereby ensuring the reliability of moderation systems. By formalizing robustness within the embedding space, this work strengthens the reliability of language models in ethical AI deployment and content moderation."
  },
  {
    "title": "Bullying the Machine: How Personas Increase LLM Vulnerability",
    "url": "http://arxiv.org/abs/2505.12692v1",
    "arxiv_id": "2505.12692v1",
    "authors": [
      "Ziwei Xu",
      "Udit Sanghi",
      "Mohan Kankanhalli"
    ],
    "published": "2025-05-19T04:32:02+00:00",
    "summary": "Large Language Models (LLMs) are increasingly deployed in interactions where they are prompted to adopt personas. This paper investigates whether such persona conditioning affects model safety under bullying, an adversarial manipulation that applies psychological pressures in order to force the victim to comply to the attacker. We introduce a simulation framework in which an attacker LLM engages a victim LLM using psychologically grounded bullying tactics, while the victim adopts personas aligned with the Big Five personality traits. Experiments using multiple open-source LLMs and a wide range of adversarial goals reveal that certain persona configurations -- such as weakened agreeableness or conscientiousness -- significantly increase victim's susceptibility to unsafe outputs. Bullying tactics involving emotional or sarcastic manipulation, such as gaslighting and ridicule, are particularly effective. These findings suggest that persona-driven interaction introduces a novel vector for safety risks in LLMs and highlight the need for persona-aware safety evaluation and alignment strategies."
  },
  {
    "title": "CURE: Concept Unlearning via Orthogonal Representation Editing in Diffusion Models",
    "url": "http://arxiv.org/abs/2505.12677v1",
    "arxiv_id": "2505.12677v1",
    "authors": [
      "Shristi Das Biswas",
      "Arani Roy",
      "Kaushik Roy"
    ],
    "published": "2025-05-19T03:53:06+00:00",
    "summary": "As Text-to-Image models continue to evolve, so does the risk of generating unsafe, copyrighted, or privacy-violating content. Existing safety interventions - ranging from training data curation and model fine-tuning to inference-time filtering and guidance - often suffer from incomplete concept removal, susceptibility to jail-breaking, computational inefficiency, or collateral damage to unrelated capabilities. In this paper, we introduce CURE, a training-free concept unlearning framework that operates directly in the weight space of pre-trained diffusion models, enabling fast, interpretable, and highly specific suppression of undesired concepts. At the core of our method is the Spectral Eraser, a closed-form, orthogonal projection module that identifies discriminative subspaces using Singular Value Decomposition over token embeddings associated with the concepts to forget and retain. Intuitively, the Spectral Eraser identifies and isolates features unique to the undesired concept while preserving safe attributes. This operator is then applied in a single step update to yield an edited model in which the target concept is effectively unlearned - without retraining, supervision, or iterative optimization. To balance the trade-off between filtering toxicity and preserving unrelated concepts, we further introduce an Expansion Mechanism for spectral regularization which selectively modulates singular vectors based on their relative significance to control the strength of forgetting. All the processes above are in closed-form, guaranteeing extremely efficient erasure in only $2$ seconds. Benchmarking against prior approaches, CURE achieves a more efficient and thorough removal for targeted artistic styles, objects, identities, or explicit content, with minor damage to original generation ability and demonstrates enhanced robustness against red-teaming."
  },
  {
    "title": "TS-VLM: Text-Guided SoftSort Pooling for Vision-Language Models in Multi-View Driving Reasoning",
    "url": "http://arxiv.org/abs/2505.12670v1",
    "arxiv_id": "2505.12670v1",
    "authors": [
      "Lihong Chen",
      "Hossein Hassani",
      "Soodeh Nikan"
    ],
    "published": "2025-05-19T03:37:15+00:00",
    "summary": "Vision-Language Models (VLMs) have shown remarkable potential in advancing autonomous driving by leveraging multi-modal fusion in order to enhance scene perception, reasoning, and decision-making. Despite their potential, existing models suffer from computational overhead and inefficient integration of multi-view sensor data that make them impractical for real-time deployment in safety-critical autonomous driving applications. To address these shortcomings, this paper is devoted to designing a lightweight VLM called TS-VLM, which incorporates a novel Text-Guided SoftSort Pooling (TGSSP) module. By resorting to semantics of the input queries, TGSSP ranks and fuses visual features from multiple views, enabling dynamic and query-aware multi-view aggregation without reliance on costly attention mechanisms. This design ensures the query-adaptive prioritization of semantically related views, which leads to improved contextual accuracy in multi-view reasoning for autonomous driving. Extensive evaluations on the DriveLM benchmark demonstrate that, on the one hand, TS-VLM outperforms state-of-the-art models with a BLEU-4 score of 56.82, METEOR of 41.91, ROUGE-L of 74.64, and CIDEr of 3.39. On the other hand, TS-VLM reduces computational cost by up to 90%, where the smallest version contains only 20.1 million parameters, making it more practical for real-time deployment in autonomous vehicles."
  },
  {
    "title": "Digital Twins in the Cloud: A Modular, Scalable and Interoperable Framework for Accelerating Verification and Validation of Autonomous Driving Solutions",
    "url": "http://arxiv.org/abs/2505.12661v1",
    "arxiv_id": "2505.12661v1",
    "authors": [
      "Tanmay Vilas Samak",
      "Chinmay Vilas Samak",
      "Giovanni Martino",
      "Pranav Nair",
      "Venkat Krovi"
    ],
    "published": "2025-05-19T03:23:48+00:00",
    "summary": "Verification and validation (V&V) of autonomous vehicles (AVs) typically requires exhaustive testing across a variety of operating environments and driving scenarios including rare, extreme, or hazardous situations that might be difficult or impossible to capture in reality. Additionally, physical V&V methods such as track-based evaluations or public-road testing are often constrained by time, cost, and safety, which motivates the need for virtual proving grounds. However, the fidelity and scalability of simulation-based V&V methods can quickly turn into a bottleneck. In such a milieu, this work proposes a virtual proving ground that flexibly scales digital twins within high-performance computing clusters (HPCCs) and automates the V&V process. Here, digital twins enable high-fidelity virtual representation of the AV and its operating environments, allowing extensive scenario-based testing. Meanwhile, HPCC infrastructure brings substantial advantages in terms of computational power and scalability, enabling rapid iterations of simulations, processing and storage of massive amounts of data, and deployment of large-scale test campaigns, thereby reducing the time and cost associated with the V&V process. We demonstrate the efficacy of this approach through a case study that focuses on the variability analysis of a candidate autonomy algorithm to identify potential vulnerabilities in its perception, planning, and control sub-systems. The modularity, scalability, and interoperability of the proposed framework are demonstrated by deploying a test campaign comprising 256 test cases on two different HPCC architectures to ensure continuous operation in a publicly shared resource setting. The findings highlight the ability of the proposed framework to accelerate and streamline the V&V process, thereby significantly compressing (~30x) the timeline."
  },
  {
    "title": "SafeMove-RL: A Certifiable Reinforcement Learning Framework for Dynamic Motion Constraints in Trajectory Planning",
    "url": "http://arxiv.org/abs/2505.12648v1",
    "arxiv_id": "2505.12648v1",
    "authors": [
      "Tengfei Liu",
      "Haoyang Zhong",
      "Jiazheng Hu",
      "Tan Zhang"
    ],
    "published": "2025-05-19T03:00:44+00:00",
    "summary": "This study presents a dynamic safety margin-based reinforcement learning framework for local motion planning in dynamic and uncertain environments. The proposed planner integrates real-time trajectory optimization with adaptive gap analysis, enabling effective feasibility assessment under partial observability constraints. To address safety-critical computations in unknown scenarios, an enhanced online learning mechanism is introduced, which dynamically corrects spatial trajectories by forming dynamic safety margins while maintaining control invariance. Extensive evaluations, including ablation studies and comparisons with state-of-the-art algorithms, demonstrate superior success rates and computational efficiency. The framework's effectiveness is further validated on both simulated and physical robotic platforms."
  },
  {
    "title": "R1dacted: Investigating Local Censorship in DeepSeek's R1 Language Model",
    "url": "http://arxiv.org/abs/2505.12625v1",
    "arxiv_id": "2505.12625v1",
    "authors": [
      "Ali Naseh",
      "Harsh Chaudhari",
      "Jaechul Roh",
      "Mingshi Wu",
      "Alina Oprea",
      "Amir Houmansadr"
    ],
    "published": "2025-05-19T02:16:56+00:00",
    "summary": "DeepSeek recently released R1, a high-performing large language model (LLM) optimized for reasoning tasks. Despite its efficient training pipeline, R1 achieves competitive performance, even surpassing leading reasoning models like OpenAI's o1 on several benchmarks. However, emerging reports suggest that R1 refuses to answer certain prompts related to politically sensitive topics in China. While existing LLMs often implement safeguards to avoid generating harmful or offensive outputs, R1 represents a notable shift - exhibiting censorship-like behavior on politically charged queries. In this paper, we investigate this phenomenon by first introducing a large-scale set of heavily curated prompts that get censored by R1, covering a range of politically sensitive topics, but are not censored by other models. We then conduct a comprehensive analysis of R1's censorship patterns, examining their consistency, triggers, and variations across topics, prompt phrasing, and context. Beyond English-language queries, we explore censorship behavior in other languages. We also investigate the transferability of censorship to models distilled from the R1 language model. Finally, we propose techniques for bypassing or removing this censorship. Our findings reveal possible additional censorship integration likely shaped by design choices during training or alignment, raising concerns about transparency, bias, and governance in language model deployment."
  },
  {
    "title": "SurveillanceVQA-589K: A Benchmark for Comprehensive Surveillance Video-Language Understanding with Large Models",
    "url": "http://arxiv.org/abs/2505.12589v1",
    "arxiv_id": "2505.12589v1",
    "authors": [
      "Bo Liu",
      "Pengfei Qiao",
      "Minhan Ma",
      "Xuange Zhang",
      "Yinan Tang",
      "Peng Xu",
      "Kun Liu",
      "Tongtong Yuan"
    ],
    "published": "2025-05-19T00:57:04+00:00",
    "summary": "Understanding surveillance video content remains a critical yet underexplored challenge in vision-language research, particularly due to its real-world complexity, irregular event dynamics, and safety-critical implications. In this work, we introduce SurveillanceVQA-589K, the largest open-ended video question answering benchmark tailored to the surveillance domain. The dataset comprises 589,380 QA pairs spanning 12 cognitively diverse question types, including temporal reasoning, causal inference, spatial understanding, and anomaly interpretation, across both normal and abnormal video scenarios. To construct the benchmark at scale, we design a hybrid annotation pipeline that combines temporally aligned human-written captions with Large Vision-Language Model-assisted QA generation using prompt-based techniques. We also propose a multi-dimensional evaluation protocol to assess contextual, temporal, and causal comprehension. We evaluate eight LVLMs under this framework, revealing significant performance gaps, especially in causal and anomaly-related tasks, underscoring the limitations of current models in real-world surveillance contexts. Our benchmark provides a practical and comprehensive resource for advancing video-language understanding in safety-critical applications such as intelligent monitoring, incident analysis, and autonomous decision-making."
  },
  {
    "title": "A Comprehensive Survey on Physical Risk Control in the Era of Foundation Model-enabled Robotics",
    "url": "http://arxiv.org/abs/2505.12583v1",
    "arxiv_id": "2505.12583v1",
    "authors": [
      "Takeshi Kojima",
      "Yaonan Zhu",
      "Yusuke Iwasawa",
      "Toshinori Kitamura",
      "Gang Yan",
      "Shu Morikuni",
      "Ryosuke Takanami",
      "Alfredo Solano",
      "Tatsuya Matsushima",
      "Akiko Murakami",
      "Yutaka Matsuo"
    ],
    "published": "2025-05-19T00:11:42+00:00",
    "summary": "Recent Foundation Model-enabled robotics (FMRs) display greatly improved general-purpose skills, enabling more adaptable automation than conventional robotics. Their ability to handle diverse tasks thus creates new opportunities to replace human labor. However, unlike general foundation models, FMRs interact with the physical world, where their actions directly affect the safety of humans and surrounding objects, requiring careful deployment and control. Based on this proposition, our survey comprehensively summarizes robot control approaches to mitigate physical risks by covering all the lifespan of FMRs ranging from pre-deployment to post-accident stage. Specifically, we broadly divide the timeline into the following three phases: (1) pre-deployment phase, (2) pre-incident phase, and (3) post-incident phase. Throughout this survey, we find that there is much room to study (i) pre-incident risk mitigation strategies, (ii) research that assumes physical interaction with humans, and (iii) essential issues of foundation models themselves. We hope that this survey will be a milestone in providing a high-resolution analysis of the physical risks of FMRs and their control, contributing to the realization of a good human-robot relationship."
  },
  {
    "title": "A Survey of Attacks on Large Language Models",
    "url": "http://arxiv.org/abs/2505.12567v1",
    "arxiv_id": "2505.12567v1",
    "authors": [
      "Wenrui Xu",
      "Keshab K. Parhi"
    ],
    "published": "2025-05-18T22:55:16+00:00",
    "summary": "Large language models (LLMs) and LLM-based agents have been widely deployed in a wide range of applications in the real world, including healthcare diagnostics, financial analysis, customer support, robotics, and autonomous driving, expanding their powerful capability of understanding, reasoning, and generating natural languages. However, the wide deployment of LLM-based applications exposes critical security and reliability risks, such as the potential for malicious misuse, privacy leakage, and service disruption that weaken user trust and undermine societal safety. This paper provides a systematic overview of the details of adversarial attacks targeting both LLMs and LLM-based agents. These attacks are organized into three phases in LLMs: Training-Phase Attacks, Inference-Phase Attacks, and Availability & Integrity Attacks. For each phase, we analyze the details of representative and recently introduced attack methods along with their corresponding defenses. We hope our survey will provide a good tutorial and a comprehensive understanding of LLM security, especially for attacks on LLMs. We desire to raise attention to the risks inherent in widely deployed LLM-based applications and highlight the urgent need for robust mitigation strategies for evolving threats."
  },
  {
    "title": "Towards Reliable and Interpretable Traffic Crash Pattern Prediction and Safety Interventions Using Customized Large Language Models",
    "url": "http://arxiv.org/abs/2505.12545v1",
    "arxiv_id": "2505.12545v1",
    "authors": [
      "Yang Zhao",
      "Pu Wang",
      "Yibo Zhao",
      "Hongru Du",
      "Hao",
      "Yang"
    ],
    "published": "2025-05-18T21:02:30+00:00",
    "summary": "Predicting crash events is crucial for understanding crash distributions and their contributing factors, thereby enabling the design of proactive traffic safety policy interventions. However, existing methods struggle to interpret the complex interplay among various sources of traffic crash data, including numeric characteristics, textual reports, crash imagery, environmental conditions, and driver behavior records. As a result, they often fail to capture the rich semantic information and intricate interrelationships embedded in these diverse data sources, limiting their ability to identify critical crash risk factors. In this research, we propose TrafficSafe, a framework that adapts LLMs to reframe crash prediction and feature attribution as text-based reasoning. A multi-modal crash dataset including 58,903 real-world reports together with belonged infrastructure, environmental, driver, and vehicle information is collected and textualized into TrafficSafe Event Dataset. By customizing and fine-tuning LLMs on this dataset, the TrafficSafe LLM achieves a 42% average improvement in F1-score over baselines. To interpret these predictions and uncover contributing factors, we introduce TrafficSafe Attribution, a sentence-level feature attribution framework enabling conditional risk analysis. Findings show that alcohol-impaired driving is the leading factor in severe crashes, with aggressive and impairment-related behaviors having nearly twice the contribution for severe crashes compared to other driver behaviors. Furthermore, TrafficSafe Attribution highlights pivotal features during model training, guiding strategic crash data collection for iterative performance improvements. The proposed TrafficSafe offers a transformative leap in traffic safety research, providing a blueprint for translating advanced AI technologies into responsible, actionable, and life-saving outcomes."
  },
  {
    "title": "Development of a non-wearable support robot capable of reproducing natural standing-up movements",
    "url": "http://arxiv.org/abs/2505.12525v1",
    "arxiv_id": "2505.12525v1",
    "authors": [
      "Atsuya Kusui",
      "Susumu Hirai",
      "Asuka Takai"
    ],
    "published": "2025-05-18T19:26:40+00:00",
    "summary": "To reproduce natural standing-up motion, recent studies have emphasized the importance of coordination between the assisting robot and the human. However, many non-wearable assistive devices have struggled to replicate natural motion trajectories. While wearable devices offer better coordination with the human body, they present challenges in completely isolating mechanical and electrical hazards. To address this, we developed a novel standing-assist robot that integrates features of both wearable and non-wearable systems, aiming to achieve high coordination while maintaining safety. The device employs a four-link mechanism aligned with the human joint structure, designed to reproduce the S-shaped trajectory of the hip and the arc trajectory of the knee during natural standing-up motion. Subject-specific trajectory data were obtained using a gyroscope, and the link lengths were determined to drive the seat along the optimal path. A feedforward speed control using a stepping motor was implemented, and the reproducibility of the trajectory was evaluated based on the geometric constraints of the mechanism. A load-bearing experiment with weights fixed to the seat was conducted to assess the trajectory accuracy under different conditions. Results showed that the reproduction errors for the hip and knee trajectories remained within approximately 4 percent of the seat's total displacement, demonstrating high fidelity to the target paths. In addition, durability testing, thermal safety evaluation, and risk assessment confirmed the reliability and safety of the system for indoor use. These findings suggest that the proposed design offers a promising approach for developing assistive technologies that adapt to individual physical characteristics, with potential applications in elderly care and rehabilitation."
  },
  {
    "title": "BadNAVer: Exploring Jailbreak Attacks On Vision-and-Language Navigation",
    "url": "http://arxiv.org/abs/2505.12443v1",
    "arxiv_id": "2505.12443v1",
    "authors": [
      "Wenqi Lyu",
      "Zerui Li",
      "Yanyuan Qiao",
      "Qi Wu"
    ],
    "published": "2025-05-18T14:33:17+00:00",
    "summary": "Multimodal large language models (MLLMs) have recently gained attention for their generalization and reasoning capabilities in Vision-and-Language Navigation (VLN) tasks, leading to the rise of MLLM-driven navigators. However, MLLMs are vulnerable to jailbreak attacks, where crafted prompts bypass safety mechanisms and trigger undesired outputs. In embodied scenarios, such vulnerabilities pose greater risks: unlike plain text models that generate toxic content, embodied agents may interpret malicious instructions as executable commands, potentially leading to real-world harm. In this paper, we present the first systematic jailbreak attack paradigm targeting MLLM-driven navigator. We propose a three-tiered attack framework and construct malicious queries across four intent categories, concatenated with standard navigation instructions. In the Matterport3D simulator, we evaluate navigation agents powered by five MLLMs and report an average attack success rate over 90%. To test real-world feasibility, we replicate the attack on a physical robot. Our results show that even well-crafted prompts can induce harmful actions and intents in MLLMs, posing risks beyond toxic output and potentially leading to physical harm."
  },
  {
    "title": "Addressing the Scarcity of Benchmarks for Graph XAI",
    "url": "http://arxiv.org/abs/2505.12437v1",
    "arxiv_id": "2505.12437v1",
    "authors": [
      "Michele Fontanesi",
      "Alessio Micheli",
      "Marco Podda",
      "Domenico Tortorella"
    ],
    "published": "2025-05-18T14:19:52+00:00",
    "summary": "While Graph Neural Networks (GNNs) have become the de facto model for learning from structured data, their decisional process remains opaque to the end user, undermining their deployment in safety-critical applications. In the case of graph classification, Explainable Artificial Intelligence (XAI) techniques address this major issue by identifying sub-graph motifs that explain predictions. However, advancements in this field are hindered by a chronic scarcity of benchmark datasets with known ground-truth motifs to assess the explanations' quality. Current graph XAI benchmarks are limited to synthetic data or a handful of real-world tasks hand-curated by domain experts. In this paper, we propose a general method to automate the construction of XAI benchmarks for graph classification from real-world datasets. We provide both 15 ready-made benchmarks, as well as the code to generate more than 2000 additional XAI benchmarks with our method. As a use case, we employ our benchmarks to assess the effectiveness of some popular graph explainers."
  },
  {
    "title": "Kornia-rs: A Low-Level 3D Computer Vision Library In Rust",
    "url": "http://arxiv.org/abs/2505.12425v1",
    "arxiv_id": "2505.12425v1",
    "authors": [
      "Edgar Riba",
      "Jian Shi",
      "Aditya Kumar",
      "Andrew Shen",
      "Gary Bradski"
    ],
    "published": "2025-05-18T13:50:00+00:00",
    "summary": "We present \\textit{kornia-rs}, a high-performance 3D computer vision library written entirely in native Rust, designed for safety-critical and real-time applications. Unlike C++-based libraries like OpenCV or wrapper-based solutions like OpenCV-Rust, \\textit{kornia-rs} is built from the ground up to leverage Rust's ownership model and type system for memory and thread safety. \\textit{kornia-rs} adopts a statically-typed tensor system and a modular set of crates, providing efficient image I/O, image processing and 3D operations. To aid cross-platform compatibility, \\textit{kornia-rs} offers Python bindings, enabling seamless and efficient integration with Rust code. Empirical results show that \\textit{kornia-rs} achieves a 3~ 5 times speedup in image transformation tasks over native Rust alternatives, while offering comparable performance to C++ wrapper-based libraries. In addition to 2D vision capabilities, \\textit{kornia-rs} addresses a significant gap in the Rust ecosystem by providing a set of 3D computer vision operators. This paper presents the architecture and performance characteristics of \\textit{kornia-rs}, demonstrating its effectiveness in real-world computer vision applications."
  },
  {
    "title": "SLOT: Sample-specific Language Model Optimization at Test-time",
    "url": "http://arxiv.org/abs/2505.12392v1",
    "arxiv_id": "2505.12392v1",
    "authors": [
      "Yang Hu",
      "Xingyu Zhang",
      "Xueji Fang",
      "Zhiyang Chen",
      "Xiao Wang",
      "Huatian Zhang",
      "Guojun Qi"
    ],
    "published": "2025-05-18T12:37:56+00:00",
    "summary": "We propose SLOT (Sample-specific Language Model Optimization at Test-time), a novel and parameter-efficient test-time inference approach that enhances a language model's ability to more accurately respond to individual prompts. Existing Large Language Models (LLMs) often struggle with complex instructions, leading to poor performances on those not well represented among general samples. To address this, SLOT conducts few optimization steps at test-time to update a light-weight sample-specific parameter vector. It is added to the final hidden layer before the output head, and enables efficient adaptation by caching the last layer features during per-sample optimization. By minimizing the cross-entropy loss on the input prompt only, SLOT helps the model better aligned with and follow each given instruction. In experiments, we demonstrate that our method outperforms the compared models across multiple benchmarks and LLMs. For example, Qwen2.5-7B with SLOT achieves an accuracy gain of 8.6% on GSM8K from 57.54% to 66.19%, while DeepSeek-R1-Distill-Llama-70B with SLOT achieves a SOTA accuracy of 68.69% on GPQA among 70B-level models. Our code is available at https://github.com/maple-research-lab/SLOT."
  },
  {
    "title": "DisCO: Reinforcing Large Reasoning Models with Discriminative Constrained Optimization",
    "url": "http://arxiv.org/abs/2505.12366v1",
    "arxiv_id": "2505.12366v1",
    "authors": [
      "Gang Li",
      "Ming Lin",
      "Tomer Galanti",
      "Zhengzhong Tu",
      "Tianbao Yang"
    ],
    "published": "2025-05-18T11:08:32+00:00",
    "summary": "The recent success and openness of DeepSeek-R1 have brought widespread attention to Group Relative Policy Optimization (GRPO) as a reinforcement learning method for large reasoning models (LRMs). In this work, we analyze the GRPO objective under a binary reward setting and reveal an inherent limitation of question-level difficulty bias. We also identify a connection between GRPO and traditional discriminative methods in supervised learning. Motivated by these insights, we introduce a new Discriminative Constrained Optimization (DisCO) framework for reinforcing LRMs, grounded in the principle of discriminative learning. The main differences between DisCO and GRPO and its recent variants are: (1) it replaces the group relative objective with a discriminative objective defined by a scoring function; (2) it abandons clipping-based surrogates in favor of non-clipping RL surrogate objectives used as scoring functions; (3) it employs a simple yet effective constrained optimization approach to enforce the KL divergence constraint, ensuring stable training. As a result, DisCO offers notable advantages over GRPO and its variants: (i) it completely eliminates difficulty bias by adopting discriminative objectives; (ii) it addresses the entropy instability in GRPO and its variants through the use of non-clipping scoring functions and a constrained optimization approach; (iii) it allows the incorporation of advanced discriminative learning techniques to address data imbalance, where a significant number of questions have more negative than positive generated answers during training. Our experiments on enhancing the mathematical reasoning capabilities of SFT-finetuned models show that DisCO significantly outperforms GRPO and its improved variants such as DAPO, achieving average gains of 7\\% over GRPO and 6\\% over DAPO across six benchmark tasks for an 1.5B model."
  },
  {
    "title": "A universal policy wrapper with guarantees",
    "url": "http://arxiv.org/abs/2505.12354v1",
    "arxiv_id": "2505.12354v1",
    "authors": [
      "Anton Bolychev",
      "Georgiy Malaniya",
      "Grigory Yaremenko",
      "Anastasia Krasnaya",
      "Pavel Osinenko"
    ],
    "published": "2025-05-18T10:37:27+00:00",
    "summary": "We introduce a universal policy wrapper for reinforcement learning agents that ensures formal goal-reaching guarantees. In contrast to standard reinforcement learning algorithms that excel in performance but lack rigorous safety assurances, our wrapper selectively switches between a high-performing base policy -- derived from any existing RL method -- and a fallback policy with known convergence properties. Base policy's value function supervises this switching process, determining when the fallback policy should override the base policy to ensure the system remains on a stable path. The analysis proves that our wrapper inherits the fallback policy's goal-reaching guarantees while preserving or improving upon the performance of the base policy. Notably, it operates without needing additional system knowledge or online constrained optimization, making it readily deployable across diverse reinforcement learning architectures and tasks."
  },
  {
    "title": "Reasoning-CV: Fine-tuning Powerful Reasoning LLMs for Knowledge-Assisted Claim Verification",
    "url": "http://arxiv.org/abs/2505.12348v1",
    "arxiv_id": "2505.12348v1",
    "authors": [
      "Zhi Zheng",
      "Wee Sun Lee"
    ],
    "published": "2025-05-18T10:28:54+00:00",
    "summary": "Claim verification is essential in combating misinformation, and large language models (LLMs) have recently emerged in this area as powerful tools for assessing the veracity of claims using external knowledge. Existing LLM-based methods for claim verification typically adopt a Decompose-Then-Verify paradigm, which involves decomposing complex claims into several independent sub-claims and verifying each sub-claim separately. However, this paradigm often introduces errors during the claim decomposition process. To mitigate these errors, we propose to develop the Chain-of-Thought (CoT)-Verify paradigm, which leverages LLM reasoning methods to generate CoT-verification paths for the original complex claim without requiring decompositions into sub-claims and separate verification stages. The CoT-Verify paradigm allows us to propose a natural fine-tuning method called Reasoning-CV to enhance the verification capabilities in LLMs. Reasoning-CV includes a supervised fine-tuning (SFT) stage and a self-improvement direct preference optimization (DPO) stage. Utilizing only an 8B pre-trained LLM, Reasoning-CV demonstrates superior knowledge-assisted claim verification performances compared to existing Decompose-Then-Verify methods, as well as powerful black-box LLMs such as GPT-4o+CoT and o1-preview. Our code is available."
  },
  {
    "title": "OSS-Bench: Benchmark Generator for Coding LLMs",
    "url": "http://arxiv.org/abs/2505.12331v1",
    "arxiv_id": "2505.12331v1",
    "authors": [
      "Yuancheng Jiang",
      "Roland Yap",
      "Zhenkai Liang"
    ],
    "published": "2025-05-18T09:53:51+00:00",
    "summary": "In light of the rapid adoption of AI coding assistants, LLM-assisted development has become increasingly prevalent, creating an urgent need for robust evaluation of generated code quality. Existing benchmarks often require extensive manual effort to create static datasets, rely on indirect or insufficiently challenging tasks, depend on non-scalable ground truth, or neglect critical low-level security evaluations, particularly memory-safety issues. In this work, we introduce OSS-Bench, a benchmark generator that automatically constructs large-scale, live evaluation tasks from real-world open-source software. OSS-Bench replaces functions with LLM-generated code and evaluates them using three natural metrics: compilability, functional correctness, and memory safety, leveraging robust signals like compilation failures, test-suite violations, and sanitizer alerts as ground truth. In our evaluation, the benchmark, instantiated as OSS-Bench(php) and OSS-Bench(sql), profiles 17 diverse LLMs, revealing insights such as intra-family behavioral patterns and inconsistencies between model size and performance. Our results demonstrate that OSS-Bench mitigates overfitting by leveraging the evolving complexity of OSS and highlights LLMs' limited understanding of low-level code security via extended fuzzing experiments. Overall, OSS-Bench offers a practical and scalable framework for benchmarking the real-world coding capabilities of LLMs."
  },
  {
    "title": "Robust Planning for Autonomous Driving via Mixed Adversarial Diffusion Predictions",
    "url": "http://arxiv.org/abs/2505.12327v1",
    "arxiv_id": "2505.12327v1",
    "authors": [
      "Albert Zhao",
      "Stefano Soatto"
    ],
    "published": "2025-05-18T09:44:57+00:00",
    "summary": "We describe a robust planning method for autonomous driving that mixes normal and adversarial agent predictions output by a diffusion model trained for motion prediction. We first train a diffusion model to learn an unbiased distribution of normal agent behaviors. We then generate a distribution of adversarial predictions by biasing the diffusion model at test time to generate predictions that are likely to collide with a candidate plan. We score plans using expected cost with respect to a mixture distribution of normal and adversarial predictions, leading to a planner that is robust against adversarial behaviors but not overly conservative when agents behave normally. Unlike current approaches, we do not use risk measures that over-weight adversarial behaviors while placing little to no weight on low-cost normal behaviors or use hard safety constraints that may not be appropriate for all driving scenarios. We show the effectiveness of our method on single-agent and multi-agent jaywalking scenarios as well as a red light violation scenario."
  },
  {
    "title": "The Tower of Babel Revisited: Multilingual Jailbreak Prompts on Closed-Source Large Language Models",
    "url": "http://arxiv.org/abs/2505.12287v1",
    "arxiv_id": "2505.12287v1",
    "authors": [
      "Linghan Huang",
      "Haolin Jin",
      "Zhaoge Bi",
      "Pengyue Yang",
      "Peizhou Zhao",
      "Taozhao Chen",
      "Xiongfei Wu",
      "Lei Ma",
      "Huaming Chen"
    ],
    "published": "2025-05-18T07:51:19+00:00",
    "summary": "Large language models (LLMs) have seen widespread applications across various domains, yet remain vulnerable to adversarial prompt injections. While most existing research on jailbreak attacks and hallucination phenomena has focused primarily on open-source models, we investigate the frontier of closed-source LLMs under multilingual attack scenarios. We present a first-of-its-kind integrated adversarial framework that leverages diverse attack techniques to systematically evaluate frontier proprietary solutions, including GPT-4o, DeepSeek-R1, Gemini-1.5-Pro, and Qwen-Max. Our evaluation spans six categories of security contents in both English and Chinese, generating 38,400 responses across 32 types of jailbreak attacks. Attack success rate (ASR) is utilized as the quantitative metric to assess performance from three dimensions: prompt design, model architecture, and language environment. Our findings suggest that Qwen-Max is the most vulnerable, while GPT-4o shows the strongest defense. Notably, prompts in Chinese consistently yield higher ASRs than their English counterparts, and our novel Two-Sides attack technique proves to be the most effective across all models. This work highlights a dire need for language-aware alignment and robust cross-lingual defenses in LLMs, and we hope it will inspire researchers, developers, and policymakers toward more robust and inclusive AI systems."
  },
  {
    "title": "Efficient RL Training for Reasoning Models via Length-Aware Optimization",
    "url": "http://arxiv.org/abs/2505.12284v1",
    "arxiv_id": "2505.12284v1",
    "authors": [
      "Danlong Yuan",
      "Tian Xie",
      "Shaohan Huang",
      "Zhuocheng Gong",
      "Huishuai Zhang",
      "Chong Luo",
      "Furu Wei",
      "Dongyan Zhao"
    ],
    "published": "2025-05-18T07:46:43+00:00",
    "summary": "Large reasoning models, such as OpenAI o1 or DeepSeek R1, have demonstrated remarkable performance on reasoning tasks but often incur a long reasoning path with significant memory and time costs. Existing methods primarily aim to shorten reasoning paths by introducing additional training data and stages. In this paper, we propose three critical reward designs integrated directly into the reinforcement learning process of large reasoning models, which reduce the response length without extra training stages. Experiments on four settings show that our method significantly decreases response length while maintaining or even improving performance. Specifically, in a logic reasoning setting, we achieve a 40% reduction in response length averaged by steps alongside a 14% gain in performance. For math problems, we reduce response length averaged by steps by 33% while preserving performance."
  },
  {
    "title": "LLM Context Conditioning and PWP Prompting for Multimodal Validation of Chemical Formulas",
    "url": "http://arxiv.org/abs/2505.12257v1",
    "arxiv_id": "2505.12257v1",
    "authors": [
      "Evgeny Markhasin"
    ],
    "published": "2025-05-18T06:33:08+00:00",
    "summary": "Identifying subtle technical errors within complex scientific and technical documents, especially those requiring multimodal interpretation (e.g., formulas in images), presents a significant hurdle for Large Language Models (LLMs) whose inherent error-correction tendencies can mask inaccuracies. This exploratory proof-of-concept (PoC) study investigates structured LLM context conditioning, informed by Persistent Workflow Prompting (PWP) principles, as a methodological strategy to modulate this LLM behavior at inference time. The approach is designed to enhance the reliability of readily available, general-purpose LLMs (specifically Gemini 2.5 Pro and ChatGPT Plus o3) for precise validation tasks, crucially relying only on their standard chat interfaces without API access or model modifications. To explore this methodology, we focused on validating chemical formulas within a single, complex test paper with known textual and image-based errors. Several prompting strategies were evaluated: while basic prompts proved unreliable, an approach adapting PWP structures to rigorously condition the LLM's analytical mindset appeared to improve textual error identification with both models. Notably, this method also guided Gemini 2.5 Pro to repeatedly identify a subtle image-based formula error previously overlooked during manual review, a task where ChatGPT Plus o3 failed in our tests. These preliminary findings highlight specific LLM operational modes that impede detail-oriented validation and suggest that PWP-informed context conditioning offers a promising and highly accessible technique for developing more robust LLM-driven analytical workflows, particularly for tasks requiring meticulous error detection in scientific and technical documents. Extensive validation beyond this limited PoC is necessary to ascertain broader applicability."
  },
  {
    "title": "Persuasion and Safety in the Era of Generative AI",
    "url": "http://arxiv.org/abs/2505.12248v1",
    "arxiv_id": "2505.12248v1",
    "authors": [
      "Haein Kong"
    ],
    "published": "2025-05-18T06:04:46+00:00",
    "summary": "As large language models (LLMs) achieve advanced persuasive capabilities, concerns about their potential risks have grown. The EU AI Act prohibits AI systems that use manipulative or deceptive techniques to undermine informed decision-making, highlighting the need to distinguish between rational persuasion, which engages reason, and manipulation, which exploits cognitive biases. My dissertation addresses the lack of empirical studies in this area by developing a taxonomy of persuasive techniques, creating a human-annotated dataset, and evaluating LLMs' ability to distinguish between these methods. This work contributes to AI safety by providing resources to mitigate the risks of persuasive AI and fostering discussions on ethical persuasion in the age of generative AI."
  },
  {
    "title": "PANORAMA: A synthetic PII-laced dataset for studying sensitive data memorization in LLMs",
    "url": "http://arxiv.org/abs/2505.12238v1",
    "arxiv_id": "2505.12238v1",
    "authors": [
      "Sriram Selvam",
      "Anneswa Ghosh"
    ],
    "published": "2025-05-18T05:27:35+00:00",
    "summary": "The memorization of sensitive and personally identifiable information (PII) by large language models (LLMs) poses growing privacy risks as models scale and are increasingly deployed in real-world applications. Existing efforts to study sensitive and PII data memorization and develop mitigation strategies are hampered by the absence of comprehensive, realistic, and ethically sourced datasets reflecting the diversity of sensitive information found on the web. We introduce PANORAMA - Profile-based Assemblage for Naturalistic Online Representation and Attribute Memorization Analysis, a large-scale synthetic corpus of 384,789 samples derived from 9,674 synthetic profiles designed to closely emulate the distribution, variety, and context of PII and sensitive data as it naturally occurs in online environments. Our data generation pipeline begins with the construction of internally consistent, multi-attribute human profiles using constrained selection to reflect real-world demographics such as education, health attributes, financial status, etc. Using a combination of zero-shot prompting and OpenAI o3-mini, we generate diverse content types - including wiki-style articles, social media posts, forum discussions, online reviews, comments, and marketplace listings - each embedding realistic, contextually appropriate PII and other sensitive information. We validate the utility of PANORAMA by fine-tuning the Mistral-7B model on 1x, 5x, 10x, and 25x data replication rates with a subset of data and measure PII memorization rates - revealing not only consistent increases with repetition but also variation across content types, highlighting PANORAMA's ability to model how memorization risks differ by context. Our dataset and code are publicly available, providing a much-needed resource for privacy risk assessment, model auditing, and the development of privacy-preserving LLMs."
  },
  {
    "title": "Self-Destructive Language Model",
    "url": "http://arxiv.org/abs/2505.12186v1",
    "arxiv_id": "2505.12186v1",
    "authors": [
      "Yuhui Wang",
      "Rongyi Zhu",
      "Ting Wang"
    ],
    "published": "2025-05-18T01:08:18+00:00",
    "summary": "Harmful fine-tuning attacks pose a major threat to the security of large language models (LLMs), allowing adversaries to compromise safety guardrails with minimal harmful data. While existing defenses attempt to reinforce LLM alignment, they fail to address models' inherent \"trainability\" on harmful data, leaving them vulnerable to stronger attacks with increased learning rates or larger harmful datasets. To overcome this critical limitation, we introduce SEAM, a novel alignment-enhancing defense that transforms LLMs into self-destructive models with intrinsic resilience to misalignment attempts. Specifically, these models retain their capabilities for legitimate tasks while exhibiting substantial performance degradation when fine-tuned on harmful data. The protection is achieved through a novel loss function that couples the optimization trajectories of benign and harmful data, enhanced with adversarial gradient ascent to amplify the self-destructive effect. To enable practical training, we develop an efficient Hessian-free gradient estimate with theoretical error bounds. Extensive evaluation across LLMs and datasets demonstrates that SEAM creates a no-win situation for adversaries: the self-destructive models achieve state-of-the-art robustness against low-intensity attacks and undergo catastrophic performance collapse under high-intensity attacks, rendering them effectively unusable. (warning: this paper contains potentially harmful content generated by LLMs.)"
  },
  {
    "title": "Truth Neurons",
    "url": "http://arxiv.org/abs/2505.12182v1",
    "arxiv_id": "2505.12182v1",
    "authors": [
      "Haohang Li",
      "Yupeng Cao",
      "Yangyang Yu",
      "Jordan W. Suchow",
      "Zining Zhu"
    ],
    "published": "2025-05-18T00:47:21+00:00",
    "summary": "Despite their remarkable success and deployment across diverse workflows, language models sometimes produce untruthful responses. Our limited understanding of how truthfulness is mechanistically encoded within these models jeopardizes their reliability and safety. In this paper, we propose a method for identifying representations of truthfulness at the neuron level. We show that language models contain truth neurons, which encode truthfulness in a subject-agnostic manner. Experiments conducted across models of varying scales validate the existence of truth neurons, confirming that the encoding of truthfulness at the neuron level is a property shared by many language models. The distribution patterns of truth neurons over layers align with prior findings on the geometry of truthfulness. Selectively suppressing the activations of truth neurons found through the TruthfulQA dataset degrades performance both on TruthfulQA and on other benchmarks, showing that the truthfulness mechanisms are not tied to a specific dataset. Our results offer novel insights into the mechanisms underlying truthfulness in language models and highlight potential directions toward improving their trustworthiness and reliability."
  },
  {
    "title": "Reasoning Large Language Model Errors Arise from Hallucinating Critical Problem Features",
    "url": "http://arxiv.org/abs/2505.12151v1",
    "arxiv_id": "2505.12151v1",
    "authors": [
      "Alex Heyman",
      "Joel Zylberberg"
    ],
    "published": "2025-05-17T21:55:12+00:00",
    "summary": "Large language models have recently made great strides in reasoning task performance through chain-of-thought (CoT) strategies trained via reinforcement learning; however, these \"reasoning large language models\" (RLLMs) remain imperfect reasoners, and understanding the frequencies and causes of their failure modes is important for both users and developers. We test o1-mini, o3-mini, DeepSeek-R1, Claude 3.7 Sonnet, Gemini 2.5 Pro Preview, and Grok 3 Mini Beta on graph coloring as a variable-complexity constraint-satisfaction logic problem, and find evidence from both error rate comparisons and CoT/explanation text analysis that RLLMs are prone to hallucinate edges not specified in the prompt's description of the graph. This phenomenon persists across multiple problem complexity levels and semantic frames, and it appears to account for a significant fraction of the incorrect answers from every tested model, and the vast majority of them for some models. Our results indicate that RLLMs may possess broader issues with misrepresentation of problem specifics, and we offer suggestions for design choices to mitigate this weakness."
  },
  {
    "title": "A Multi-Task Benchmark for Abusive Language Detection in Low-Resource Settings",
    "url": "http://arxiv.org/abs/2505.12116v1",
    "arxiv_id": "2505.12116v1",
    "authors": [
      "Fitsum Gaim",
      "Hoyun Song",
      "Huije Lee",
      "Changgeon Ko",
      "Eui Jun Hwang",
      "Jong C. Park"
    ],
    "published": "2025-05-17T18:52:47+00:00",
    "summary": "Content moderation research has recently made significant advances, but still fails to serve the majority of the world's languages due to the lack of resources, leaving millions of vulnerable users to online hostility. This work presents a large-scale human-annotated multi-task benchmark dataset for abusive language detection in Tigrinya social media with joint annotations for three tasks: abusiveness, sentiment, and topic classification. The dataset comprises 13,717 YouTube comments annotated by nine native speakers, collected from 7,373 videos with a total of over 1.2 billion views across 51 channels. We developed an iterative term clustering approach for effective data selection. Recognizing that around 64% of Tigrinya social media content uses Romanized transliterations rather than native Ge'ez script, our dataset accommodates both writing systems to reflect actual language use. We establish strong baselines across the tasks in the benchmark, while leaving significant challenges for future contributions. Our experiments reveal that small, specialized multi-task models outperform the current frontier models in the low-resource setting, achieving up to 86% accuracy (+7 points) in abusiveness detection. We make the resources publicly available to promote research on online safety."
  },
  {
    "title": "Why Not Act on What You Know? Unleashing Safety Potential of LLMs via Self-Aware Guard Enhancement",
    "url": "http://arxiv.org/abs/2505.12060v1",
    "arxiv_id": "2505.12060v1",
    "authors": [
      "Peng Ding",
      "Jun Kuang",
      "Zongyu Wang",
      "Xuezhi Cao",
      "Xunliang Cai",
      "Jiajun Chen",
      "Shujian Huang"
    ],
    "published": "2025-05-17T15:54:52+00:00",
    "summary": "Large Language Models (LLMs) have shown impressive capabilities across various tasks but remain vulnerable to meticulously crafted jailbreak attacks. In this paper, we identify a critical safety gap: while LLMs are adept at detecting jailbreak prompts, they often produce unsafe responses when directly processing these inputs. Inspired by this insight, we propose SAGE (Self-Aware Guard Enhancement), a training-free defense strategy designed to align LLMs' strong safety discrimination performance with their relatively weaker safety generation ability. SAGE consists of two core components: a Discriminative Analysis Module and a Discriminative Response Module, enhancing resilience against sophisticated jailbreak attempts through flexible safety discrimination instructions. Extensive experiments demonstrate SAGE's effectiveness and robustness across various open-source and closed-source LLMs of different sizes and architectures, achieving an average 99% defense success rate against numerous complex and covert jailbreak methods while maintaining helpfulness on general benchmarks. We further conduct mechanistic interpretability analysis through hidden states and attention distributions, revealing the underlying mechanisms of this detection-generation discrepancy. Our work thus contributes to developing future LLMs with coherent safety awareness and generation behavior. Our code and datasets are publicly available at https://github.com/NJUNLP/SAGE."
  },
  {
    "title": "Tiny QA Benchmark++: Ultra-Lightweight, Synthetic Multilingual Dataset Generation & Smoke-Tests for Continuous LLM Evaluation",
    "url": "http://arxiv.org/abs/2505.12058v1",
    "arxiv_id": "2505.12058v1",
    "authors": [
      "Vincent Koc"
    ],
    "published": "2025-05-17T15:40:03+00:00",
    "summary": "Tiny QA Benchmark++ (TQB++) presents an ultra-lightweight, multilingual smoke-test suite designed to give large-language-model (LLM) pipelines a unit-test style safety net dataset that runs in seconds with minimal cost. Born out of the tight feedback-loop demands building the Comet Opik prompt-optimization SDK, where waiting on heavyweight benchmarks breaks developer flow. TQB++ couples a 52-item English gold set (less than 20 kB) with a tiny synthetic-data generator pypi package built on provider-agnostic LiteLLM. The generator lets practitioners mint their own tiny packs in any language, domain, or difficulty, while ten ready-made packs already cover Arabic, Chinese, French, German, Japanese, Korean, Portuguese, Russian, Spanish, and Turkish. Every dataset ships with Croissant metadata and plug-and-play files for OpenAI-Evals, LangChain, and standard CI tools, so teams can drop deterministic micro-benchmarks directly into pull-request gates, prompt-engineering loops, and production dashboards without touching GPU budgets. A complete TQB++ run adds only a few seconds to pipeline latency yet reliably flags prompt-template errors, tokenizer drift, and fine-tuning side-effects long before full-scale suites like MMLU or BIG-Bench would finish configuring. The entire framework is released to accelerate continuous, resource-efficient quality assurance across the generative-AI ecosystem."
  },
  {
    "title": "CorBenchX: Large-Scale Chest X-Ray Error Dataset and Vision-Language Model Benchmark for Report Error Correction",
    "url": "http://arxiv.org/abs/2505.12057v1",
    "arxiv_id": "2505.12057v1",
    "authors": [
      "Jing Zou",
      "Qingqiu Li",
      "Chenyu Lian",
      "Lihao Liu",
      "Xiaohan Yan",
      "Shujun Wang",
      "Jing Qin"
    ],
    "published": "2025-05-17T15:39:39+00:00",
    "summary": "AI-driven models have shown great promise in detecting errors in radiology reports, yet the field lacks a unified benchmark for rigorous evaluation of error detection and further correction. To address this gap, we introduce CorBenchX, a comprehensive suite for automated error detection and correction in chest X-ray reports, designed to advance AI-assisted quality control in clinical practice. We first synthesize a large-scale dataset of 26,326 chest X-ray error reports by injecting clinically common errors via prompting DeepSeek-R1, with each corrupted report paired with its original text, error type, and human-readable description. Leveraging this dataset, we benchmark both open- and closed-source vision-language models,(e.g., InternVL, Qwen-VL, GPT-4o, o4-mini, and Claude-3.7) for error detection and correction under zero-shot prompting. Among these models, o4-mini achieves the best performance, with 50.6 % detection accuracy and correction scores of BLEU 0.853, ROUGE 0.924, BERTScore 0.981, SembScore 0.865, and CheXbertF1 0.954, remaining below clinical-level accuracy, highlighting the challenge of precise report correction. To advance the state of the art, we propose a multi-step reinforcement learning (MSRL) framework that optimizes a multi-objective reward combining format compliance, error-type accuracy, and BLEU similarity. We apply MSRL to QwenVL2.5-7B, the top open-source model in our benchmark, achieving an improvement of 38.3% in single-error detection precision and 5.2% in single-error correction over the zero-shot baseline."
  },
  {
    "title": "Safe Delta: Consistently Preserving Safety when Fine-Tuning LLMs on Diverse Datasets",
    "url": "http://arxiv.org/abs/2505.12038v1",
    "arxiv_id": "2505.12038v1",
    "authors": [
      "Ning Lu",
      "Shengcai Liu",
      "Jiahao Wu",
      "Weiyu Chen",
      "Zhirui Zhang",
      "Yew-Soon Ong",
      "Qi Wang",
      "Ke Tang"
    ],
    "published": "2025-05-17T15:01:07+00:00",
    "summary": "Large language models (LLMs) have shown great potential as general-purpose AI assistants across various domains. To fully leverage this potential in specific applications, many companies provide fine-tuning API services, enabling users to upload their own data for LLM customization. However, fine-tuning services introduce a new safety threat: user-uploaded data, whether harmful or benign, can break the model's alignment, leading to unsafe outputs. Moreover, existing defense methods struggle to address the diversity of fine-tuning datasets (e.g., varying sizes, tasks), often sacrificing utility for safety or vice versa. To address this issue, we propose Safe Delta, a safety-aware post-training defense method that adjusts the delta parameters (i.e., the parameter change before and after fine-tuning). Specifically, Safe Delta estimates the safety degradation, selects delta parameters to maximize utility while limiting overall safety loss, and applies a safety compensation vector to mitigate residual safety loss. Through extensive experiments on four diverse datasets with varying settings, our approach consistently preserves safety while ensuring that the utility gain from benign datasets remains unaffected."
  },
  {
    "title": "SHIELD: Safety on Humanoids via CBFs In Expectation on Learned Dynamics",
    "url": "http://arxiv.org/abs/2505.11494v1",
    "arxiv_id": "2505.11494v1",
    "authors": [
      "Lizhi Yang",
      "Blake Werner",
      "Ryan K. Cosner",
      "David Fridovich-Keil",
      "Preston Culbertson",
      "Aaron D. Ames"
    ],
    "published": "2025-05-16T17:57:03+00:00",
    "summary": "Robot learning has produced remarkably effective ``black-box'' controllers for complex tasks such as dynamic locomotion on humanoids. Yet ensuring dynamic safety, i.e., constraint satisfaction, remains challenging for such policies. Reinforcement learning (RL) embeds constraints heuristically through reward engineering, and adding or modifying constraints requires retraining. Model-based approaches, like control barrier functions (CBFs), enable runtime constraint specification with formal guarantees but require accurate dynamics models. This paper presents SHIELD, a layered safety framework that bridges this gap by: (1) training a generative, stochastic dynamics residual model using real-world data from hardware rollouts of the nominal controller, capturing system behavior and uncertainties; and (2) adding a safety layer on top of the nominal (learned locomotion) controller that leverages this model via a stochastic discrete-time CBF formulation enforcing safety constraints in probability. The result is a minimally-invasive safety layer that can be added to the existing autonomy stack to give probabilistic guarantees of safety that balance risk and performance. In hardware experiments on an Unitree G1 humanoid, SHIELD enables safe navigation (obstacle avoidance) through varied indoor and outdoor environments using a nominal (unknown) RL controller and onboard perception."
  },
  {
    "title": "UMArm: Untethered, Modular, Wearable, Soft Pneumatic Arm",
    "url": "http://arxiv.org/abs/2505.11476v1",
    "arxiv_id": "2505.11476v1",
    "authors": [
      "Runze Zuo",
      "Dong Heon Han",
      "Richard Li",
      "Saima Jamal",
      "Daniel Bruder"
    ],
    "published": "2025-05-16T17:31:20+00:00",
    "summary": "Robotic arms are essential to modern industries, however, their adaptability to unstructured environments remains limited. Soft robotic arms, particularly those actuated pneumatically, offer greater adaptability in unstructured environments and enhanced safety for human-robot interaction. However, current pneumatic soft arms are constrained by limited degrees of freedom, precision, payload capacity, and reliance on bulky external pressure regulators. In this work, a novel pneumatically driven rigid-soft hybrid arm, ``UMArm'', is presented. The shortcomings of pneumatically actuated soft arms are addressed by densely integrating high-force-to-weight-ratio, self-regulated McKibben actuators onto a lightweight rigid spine structure. The modified McKibben actuators incorporate valves and controllers directly inside, eliminating the need for individual pressure lines and external regulators, significantly reducing system weight and complexity. Full untethered operation, high payload capacity, precision, and directionally tunable compliance are achieved by the UMArm. Portability is demonstrated through a wearable assistive arm experiment, and versatility is showcased by reconfiguring the system into an inchworm robot. The results of this work show that the high-degree-of-freedom, external-regulator-free pneumatically driven arm systems like the UMArm possess great potential for real-world unstructured environments."
  },
  {
    "title": "REACT: Runtime-Enabled Active Collision-avoidance Technique for Autonomous Driving",
    "url": "http://arxiv.org/abs/2505.11474v1",
    "arxiv_id": "2505.11474v1",
    "authors": [
      "Heye Huang",
      "Hao Cheng",
      "Zhiyuan Zhou",
      "Zijin Wang",
      "Qichao Liu",
      "Xiaopeng Li"
    ],
    "published": "2025-05-16T17:30:13+00:00",
    "summary": "Achieving rapid and effective active collision avoidance in dynamic interactive traffic remains a core challenge for autonomous driving. This paper proposes REACT (Runtime-Enabled Active Collision-avoidance Technique), a closed-loop framework that integrates risk assessment with active avoidance control. By leveraging energy transfer principles and human-vehicle-road interaction modeling, REACT dynamically quantifies runtime risk and constructs a continuous spatial risk field. The system incorporates physically grounded safety constraints such as directional risk and traffic rules to identify high-risk zones and generate feasible, interpretable avoidance behaviors. A hierarchical warning trigger strategy and lightweight system design enhance runtime efficiency while ensuring real-time responsiveness. Evaluations across four representative high-risk scenarios including car-following braking, cut-in, rear-approaching, and intersection conflict demonstrate REACT's capability to accurately identify critical risks and execute proactive avoidance. Its risk estimation aligns closely with human driver cognition (i.e., warning lead time < 0.4 s), achieving 100% safe avoidance with zero false alarms or missed detections. Furthermore, it exhibits superior real-time performance (< 50 ms latency), strong foresight, and generalization. The lightweight architecture achieves state-of-the-art accuracy, highlighting its potential for real-time deployment in safety-critical autonomous systems."
  },
  {
    "title": "Disentangling Reasoning and Knowledge in Medical Large Language Models",
    "url": "http://arxiv.org/abs/2505.11462v1",
    "arxiv_id": "2505.11462v1",
    "authors": [
      "Rahul Thapa",
      "Qingyang Wu",
      "Kevin Wu",
      "Harrison Zhang",
      "Angela Zhang",
      "Eric Wu",
      "Haotian Ye",
      "Suhana Bedi",
      "Nevin Aresh",
      "Joseph Boen",
      "Shriya Reddy",
      "Ben Athiwaratkun",
      "Shuaiwen Leon Song",
      "James Zou"
    ],
    "published": "2025-05-16T17:16:27+00:00",
    "summary": "Medical reasoning in large language models (LLMs) aims to emulate clinicians' diagnostic thinking, but current benchmarks such as MedQA-USMLE, MedMCQA, and PubMedQA often mix reasoning with factual recall. We address this by separating 11 biomedical QA benchmarks into reasoning- and knowledge-focused subsets using a PubMedBERT classifier that reaches 81 percent accuracy, comparable to human performance. Our analysis shows that only 32.8 percent of questions require complex reasoning. We evaluate biomedical models (HuatuoGPT-o1, MedReason, m1) and general-domain models (DeepSeek-R1, o4-mini, Qwen3), finding consistent gaps between knowledge and reasoning performance. For example, m1 scores 60.5 on knowledge but only 47.1 on reasoning. In adversarial tests where models are misled with incorrect initial reasoning, biomedical models degrade sharply, while larger or RL-trained general models show more robustness. To address this, we train BioMed-R1 using fine-tuning and reinforcement learning on reasoning-heavy examples. It achieves the strongest performance among similarly sized models. Further gains may come from incorporating clinical case reports and training with adversarial and backtracking scenarios."
  },
  {
    "title": "CARES: Comprehensive Evaluation of Safety and Adversarial Robustness in Medical LLMs",
    "url": "http://arxiv.org/abs/2505.11413v1",
    "arxiv_id": "2505.11413v1",
    "authors": [
      "Sijia Chen",
      "Xiaomin Li",
      "Mengxue Zhang",
      "Eric Hanchen Jiang",
      "Qingcheng Zeng",
      "Chen-Hsiang Yu"
    ],
    "published": "2025-05-16T16:25:51+00:00",
    "summary": "Large language models (LLMs) are increasingly deployed in medical contexts, raising critical concerns about safety, alignment, and susceptibility to adversarial manipulation. While prior benchmarks assess model refusal capabilities for harmful prompts, they often lack clinical specificity, graded harmfulness levels, and coverage of jailbreak-style attacks. We introduce CARES (Clinical Adversarial Robustness and Evaluation of Safety), a benchmark for evaluating LLM safety in healthcare. CARES includes over 18,000 prompts spanning eight medical safety principles, four harm levels, and four prompting styles: direct, indirect, obfuscated, and role-play, to simulate both malicious and benign use cases. We propose a three-way response evaluation protocol (Accept, Caution, Refuse) and a fine-grained Safety Score metric to assess model behavior. Our analysis reveals that many state-of-the-art LLMs remain vulnerable to jailbreaks that subtly rephrase harmful prompts, while also over-refusing safe but atypically phrased queries. Finally, we propose a mitigation strategy using a lightweight classifier to detect jailbreak attempts and steer models toward safer behavior via reminder-based conditioning. CARES provides a rigorous framework for testing and improving medical LLM safety under adversarial and ambiguous conditions."
  },
  {
    "title": "Phare: A Safety Probe for Large Language Models",
    "url": "http://arxiv.org/abs/2505.11365v1",
    "arxiv_id": "2505.11365v1",
    "authors": [
      "Pierre Le Jeune",
      "Beno\u00eet Mal\u00e9sieux",
      "Weixuan Xiao",
      "Matteo Dora"
    ],
    "published": "2025-05-16T15:31:08+00:00",
    "summary": "Ensuring the safety of large language models (LLMs) is critical for responsible deployment, yet existing evaluations often prioritize performance over identifying failure modes. We introduce Phare, a multilingual diagnostic framework to probe and evaluate LLM behavior across three critical dimensions: hallucination and reliability, social biases, and harmful content generation. Our evaluation of 17 state-of-the-art LLMs reveals patterns of systematic vulnerabilities across all safety dimensions, including sycophancy, prompt sensitivity, and stereotype reproduction. By highlighting these specific failure modes rather than simply ranking models, Phare provides researchers and practitioners with actionable insights to build more robust, aligned, and trustworthy language systems."
  },
  {
    "title": "Sobolev Training of End-to-End Optimization Proxies",
    "url": "http://arxiv.org/abs/2505.11342v1",
    "arxiv_id": "2505.11342v1",
    "authors": [
      "Andrew W. Rosemberg",
      "Joaquim Dias Garcia",
      "Russell Bent",
      "Pascal Van Hentenryck"
    ],
    "published": "2025-05-16T15:10:01+00:00",
    "summary": "Optimization proxies - machine learning models trained to approximate the solution mapping of parametric optimization problems in a single forward pass - offer dramatic reductions in inference time compared to traditional iterative solvers. This work investigates the integration of solver sensitivities into such end to end proxies via a Sobolev training paradigm and does so in two distinct settings: (i) fully supervised proxies, where exact solver outputs and sensitivities are available, and (ii) self supervised proxies that rely only on the objective and constraint structure of the underlying optimization problem. By augmenting the standard training loss with directional derivative information extracted from the solver, the proxy aligns both its predicted solutions and local derivatives with those of the optimizer. Under Lipschitz continuity assumptions on the true solution mapping, matching first order sensitivities is shown to yield uniform approximation error proportional to the training set covering radius. Empirically, different impacts are observed in each studied setting. On three large Alternating Current Optimal Power Flow benchmarks, supervised Sobolev training cuts mean squared error by up to 56 percent and the median worst case constraint violation by up to 400 percent while keeping the optimality gap below 0.22 percent. For a mean variance portfolio task trained without labeled solutions, self supervised Sobolev training halves the average optimality gap in the medium risk region (standard deviation above 10 percent of budget) and matches the baseline elsewhere. Together, these results highlight Sobolev training whether supervised or self supervised as a path to fast reliable surrogates for safety critical large scale optimization workloads."
  },
  {
    "title": "Explaining Strategic Decisions in Multi-Agent Reinforcement Learning for Aerial Combat Tactics",
    "url": "http://arxiv.org/abs/2505.11311v1",
    "arxiv_id": "2505.11311v1",
    "authors": [
      "Ardian Selmonaj",
      "Alessandro Antonucci",
      "Adrian Schneider",
      "Michael R\u00fcegsegger",
      "Matthias Sommer"
    ],
    "published": "2025-05-16T14:36:30+00:00",
    "summary": "Artificial intelligence (AI) is reshaping strategic planning, with Multi-Agent Reinforcement Learning (MARL) enabling coordination among autonomous agents in complex scenarios. However, its practical deployment in sensitive military contexts is constrained by the lack of explainability, which is an essential factor for trust, safety, and alignment with human strategies. This work reviews and assesses current advances in explainability methods for MARL with a focus on simulated air combat scenarios. We proceed by adapting various explainability techniques to different aerial combat scenarios to gain explanatory insights about the model behavior. By linking AI-generated tactics with human-understandable reasoning, we emphasize the need for transparency to ensure reliable deployment and meaningful human-machine interaction. By illuminating the crucial importance of explainability in advancing MARL for operational defense, our work supports not only strategic planning but also the training of military personnel with insightful and comprehensible analyses."
  },
  {
    "title": "LD-Scene: LLM-Guided Diffusion for Controllable Generation of Adversarial Safety-Critical Driving Scenarios",
    "url": "http://arxiv.org/abs/2505.11247v1",
    "arxiv_id": "2505.11247v1",
    "authors": [
      "Mingxing Peng",
      "Yuting Xie",
      "Xusen Guo",
      "Ruoyu Yao",
      "Hai Yang",
      "Jun Ma"
    ],
    "published": "2025-05-16T13:41:05+00:00",
    "summary": "Ensuring the safety and robustness of autonomous driving systems necessitates a comprehensive evaluation in safety-critical scenarios. However, these safety-critical scenarios are rare and difficult to collect from real-world driving data, posing significant challenges to effectively assessing the performance of autonomous vehicles. Typical existing methods often suffer from limited controllability and lack user-friendliness, as extensive expert knowledge is essentially required. To address these challenges, we propose LD-Scene, a novel framework that integrates Large Language Models (LLMs) with Latent Diffusion Models (LDMs) for user-controllable adversarial scenario generation through natural language. Our approach comprises an LDM that captures realistic driving trajectory distributions and an LLM-based guidance module that translates user queries into adversarial loss functions, facilitating the generation of scenarios aligned with user queries. The guidance module integrates an LLM-based Chain-of-Thought (CoT) code generator and an LLM-based code debugger, enhancing the controllability and robustness in generating guidance functions. Extensive experiments conducted on the nuScenes dataset demonstrate that LD-Scene achieves state-of-the-art performance in generating realistic, diverse, and effective adversarial scenarios. Furthermore, our framework provides fine-grained control over adversarial behaviors, thereby facilitating more effective testing tailored to specific driving scenarios."
  },
  {
    "title": "Is PRM Necessary? Problem-Solving RL Implicitly Induces PRM Capability in LLMs",
    "url": "http://arxiv.org/abs/2505.11227v1",
    "arxiv_id": "2505.11227v1",
    "authors": [
      "Zhangying Feng",
      "Qianglong Chen",
      "Ning Lu",
      "Yongqian Li",
      "Siqi Cheng",
      "Shuangmu Peng",
      "Duyu Tang",
      "Shengcai Liu",
      "Zhirui Zhang"
    ],
    "published": "2025-05-16T13:23:26+00:00",
    "summary": "The development of reasoning capabilities represents a critical frontier in large language models (LLMs) research, where reinforcement learning (RL) and process reward models (PRMs) have emerged as predominant methodological frameworks. Contrary to conventional wisdom, empirical evidence from DeepSeek-R1 demonstrates that pure RL training focused on mathematical problem-solving can progressively enhance reasoning abilities without PRM integration, challenging the perceived necessity of process supervision. In this study, we conduct a systematic investigation of the relationship between RL training and PRM capabilities. Our findings demonstrate that problem-solving proficiency and process supervision capabilities represent complementary dimensions of reasoning that co-evolve synergistically during pure RL training. In particular, current PRMs underperform simple baselines like majority voting when applied to state-of-the-art models such as DeepSeek-R1 and QwQ-32B. To address this limitation, we propose Self-PRM, an introspective framework in which models autonomously evaluate and rerank their generated solutions through self-reward mechanisms. Although Self-PRM consistently improves the accuracy of the benchmark (particularly with larger sample sizes), analysis exposes persistent challenges: The approach exhibits low precision (<10\\%) on difficult problems, frequently misclassifying flawed solutions as valid. These analyses underscore the need for continued RL scaling to improve reward alignment and introspective accuracy. Overall, our findings suggest that PRM may not be essential for enhancing complex reasoning, as pure RL not only improves problem-solving skills but also inherently fosters robust PRM capabilities. We hope these findings provide actionable insights for building more reliable and self-aware complex reasoning models."
  },
  {
    "title": "HAPO: Training Language Models to Reason Concisely via History-Aware Policy Optimization",
    "url": "http://arxiv.org/abs/2505.11225v1",
    "arxiv_id": "2505.11225v1",
    "authors": [
      "Chengyu Huang",
      "Zhengxin Zhang",
      "Claire Cardie"
    ],
    "published": "2025-05-16T13:21:28+00:00",
    "summary": "While scaling the length of responses at test-time has been shown to markedly improve the reasoning abilities and performance of large language models (LLMs), it often results in verbose outputs and increases inference cost. Prior approaches for efficient test-time scaling, typically using universal budget constraints or query-level length optimization, do not leverage historical information from previous encounters with the same problem during training. We hypothesize that this limits their ability to progressively make solutions more concise over time. To address this, we present History-Aware Policy Optimization (HAPO), which keeps track of a history state (e.g., the minimum length over previously generated correct responses) for each problem. HAPO employs a novel length reward function based on this history state to incentivize the discovery of correct solutions that are more concise than those previously found. Crucially, this reward structure avoids overly penalizing shorter incorrect responses with the goal of facilitating exploration towards more efficient solutions. By combining this length reward with a correctness reward, HAPO jointly optimizes for correctness and efficiency. We use HAPO to train DeepSeek-R1-Distill-Qwen-1.5B, DeepScaleR-1.5B-Preview, and Qwen-2.5-1.5B-Instruct, and evaluate HAPO on several math benchmarks that span various difficulty levels. Experiment results demonstrate that HAPO effectively induces LLMs' concise reasoning abilities, producing length reductions of 33-59% with accuracy drops of only 2-5%."
  },
  {
    "title": "Multi-Modal Multi-Task (M3T) Federated Foundation Models for Embodied AI: Potentials and Challenges for Edge Integration",
    "url": "http://arxiv.org/abs/2505.11191v1",
    "arxiv_id": "2505.11191v1",
    "authors": [
      "Kasra Borazjani",
      "Payam Abdisarabshali",
      "Fardis Nadimi",
      "Naji Khosravan",
      "Minghui Liwang",
      "Xianbin Wang",
      "Yiguang Hong",
      "Seyyedali Hosseinalipour"
    ],
    "published": "2025-05-16T12:49:36+00:00",
    "summary": "As embodied AI systems become increasingly multi-modal, personalized, and interactive, they must learn effectively from diverse sensory inputs, adapt continually to user preferences, and operate safely under resource and privacy constraints. These challenges expose a pressing need for machine learning models capable of swift, context-aware adaptation while balancing model generalization and personalization. Here, two methods emerge as suitable candidates, each offering parts of these capabilities: Foundation Models (FMs) provide a pathway toward generalization across tasks and modalities, whereas Federated Learning (FL) offers the infrastructure for distributed, privacy-preserving model updates and user-level model personalization. However, when used in isolation, each of these approaches falls short of meeting the complex and diverse capability requirements of real-world embodied environments. In this vision paper, we introduce Federated Foundation Models (FFMs) for embodied AI, a new paradigm that unifies the strengths of multi-modal multi-task (M3T) FMs with the privacy-preserving distributed nature of FL, enabling intelligent systems at the wireless edge. We collect critical deployment dimensions of FFMs in embodied AI ecosystems under a unified framework, which we name \"EMBODY\": Embodiment heterogeneity, Modality richness and imbalance, Bandwidth and compute constraints, On-device continual learning, Distributed control and autonomy, and Yielding safety, privacy, and personalization. For each, we identify concrete challenges and envision actionable research directions. We also present an evaluation framework for deploying FFMs in embodied AI systems, along with the associated trade-offs."
  },
  {
    "title": "Human-Aligned Bench: Fine-Grained Assessment of Reasoning Ability in MLLMs vs. Humans",
    "url": "http://arxiv.org/abs/2505.11141v1",
    "arxiv_id": "2505.11141v1",
    "authors": [
      "Yansheng Qiu",
      "Li Xiao",
      "Zhaopan Xu",
      "Pengfei Zhou",
      "Zheng Wang",
      "Kaipeng Zhang"
    ],
    "published": "2025-05-16T11:41:19+00:00",
    "summary": "The goal of achieving Artificial General Intelligence (AGI) is to imitate humans and surpass them. Models such as OpenAI's o1, o3, and DeepSeek's R1 have demonstrated that large language models (LLMs) with human-like reasoning capabilities exhibit exceptional performance and are being gradually integrated into multimodal large language models (MLLMs). However, whether these models possess capabilities comparable to humans in handling reasoning tasks remains unclear at present. In this paper, we propose Human-Aligned Bench, a benchmark for fine-grained alignment of multimodal reasoning with human performance. Specifically, we collected 9,794 multimodal questions that solely rely on contextual reasoning, including bilingual (Chinese and English) multimodal questions and pure text-based questions, encompassing four question types: visual reasoning, definition judgment, analogical reasoning, and logical judgment. More importantly, each question is accompanied by human success rates and options that humans are prone to choosing incorrectly. Extensive experiments on the Human-Aligned Bench reveal notable differences between the performance of current MLLMs in multimodal reasoning and human performance. The findings on our benchmark provide insights into the development of the next-generation models."
  },
  {
    "title": "Scaling Reasoning can Improve Factuality in Large Language Models",
    "url": "http://arxiv.org/abs/2505.11140v1",
    "arxiv_id": "2505.11140v1",
    "authors": [
      "Mike Zhang",
      "Johannes Bjerva",
      "Russa Biswas"
    ],
    "published": "2025-05-16T11:39:33+00:00",
    "summary": "Recent studies on large language model (LLM) reasoning capabilities have demonstrated promising improvements in model performance by leveraging a lengthy thinking process and additional computational resources during inference, primarily in tasks involving mathematical reasoning (Muennighoff et al., 2025). However, it remains uncertain if longer reasoning chains inherently enhance factual accuracy, particularly beyond mathematical contexts. In this work, we thoroughly examine LLM reasoning within complex open-domain question-answering (QA) scenarios. We initially distill reasoning traces from advanced, large-scale reasoning models (QwQ-32B and DeepSeek-R1-671B), then fine-tune a variety of models ranging from smaller, instruction-tuned variants to larger architectures based on Qwen2.5. To enrich reasoning traces, we introduce factual information from knowledge graphs in the form of paths into our reasoning traces. Our experimental setup includes four baseline approaches and six different instruction-tuned models evaluated across a benchmark of six datasets, encompassing over 22.6K questions. Overall, we carry out 168 experimental runs and analyze approximately 1.7 million reasoning traces. Our findings indicate that, within a single run, smaller reasoning models achieve noticeable improvements in factual accuracy compared to their original instruction-tuned counterparts. Moreover, our analysis demonstrates that adding test-time compute and token budgets factual accuracy consistently improves by 2-8%, further confirming the effectiveness of test-time scaling for enhancing performance and consequently improving reasoning accuracy in open-domain QA tasks. We release all the experimental artifacts for further research."
  },
  {
    "title": "LLM-Enhanced Symbolic Control for Safety-Critical Applications",
    "url": "http://arxiv.org/abs/2505.11077v1",
    "arxiv_id": "2505.11077v1",
    "authors": [
      "Amir Bayat",
      "Alessandro Abate",
      "Necmiye Ozay",
      "Rapha\u00ebl M. Jungers"
    ],
    "published": "2025-05-16T10:08:25+00:00",
    "summary": "Motivated by Smart Manufacturing and Industry 4.0, we introduce a framework for synthesizing Abstraction-Based Controller Design (ABCD) for reach-avoid problems from Natural Language (NL) specifications using Large Language Models (LLMs). A Code Agent interprets an NL description of the control problem and translates it into a formal language interpretable by state-of-the-art symbolic control software, while a Checker Agent verifies the correctness of the generated code and enhances safety by identifying specification mismatches. Evaluations show that the system handles linguistic variability and improves robustness over direct planning with LLMs. The proposed approach lowers the barrier to formal control synthesis by enabling intuitive, NL-based task definition while maintaining safety guarantees through automated validation."
  },
  {
    "title": "A Multi-modal Fusion Network for Terrain Perception Based on Illumination Aware",
    "url": "http://arxiv.org/abs/2505.11066v1",
    "arxiv_id": "2505.11066v1",
    "authors": [
      "Rui Wang",
      "Shichun Yang",
      "Yuyi Chen",
      "Zhuoyang Li",
      "Zexiang Tong",
      "Jianyi Xu",
      "Jiayi Lu",
      "Xinjie Feng",
      "Yaoguang Cao"
    ],
    "published": "2025-05-16T10:02:22+00:00",
    "summary": "Road terrains play a crucial role in ensuring the driving safety of autonomous vehicles (AVs). However, existing sensors of AVs, including cameras and Lidars, are susceptible to variations in lighting and weather conditions, making it challenging to achieve real-time perception of road conditions. In this paper, we propose an illumination-aware multi-modal fusion network (IMF), which leverages both exteroceptive and proprioceptive perception and optimizes the fusion process based on illumination features. We introduce an illumination-perception sub-network to accurately estimate illumination features. Moreover, we design a multi-modal fusion network which is able to dynamically adjust weights of different modalities according to illumination features. We enhance the optimization process by pre-training of the illumination-perception sub-network and incorporating illumination loss as one of the training constraints. Extensive experiments demonstrate that the IMF shows a superior performance compared to state-of-the-art methods. The comparison results with single modality perception methods highlight the comprehensive advantages of multi-modal fusion in accurately perceiving road terrains under varying lighting conditions. Our dataset is available at: https://github.com/lindawang2016/IMF."
  },
  {
    "title": "Think Twice Before You Act: Enhancing Agent Behavioral Safety with Thought Correction",
    "url": "http://arxiv.org/abs/2505.11063v1",
    "arxiv_id": "2505.11063v1",
    "authors": [
      "Changyue Jiang",
      "Xudong Pan",
      "Min Yang"
    ],
    "published": "2025-05-16T10:00:15+00:00",
    "summary": "LLM-based autonomous agents possess capabilities such as reasoning, tool invocation, and environment interaction, enabling the execution of complex multi-step tasks. The internal reasoning process, i.e., thought, of behavioral trajectory significantly influences tool usage and subsequent actions but can introduce potential risks. Even minor deviations in the agent's thought may trigger cascading effects leading to irreversible safety incidents. To address the safety alignment challenges in long-horizon behavioral trajectories, we propose Thought-Aligner, a plug-in dynamic thought correction module. Utilizing a lightweight and resource-efficient model, Thought-Aligner corrects each high-risk thought on the fly before each action execution. The corrected thought is then reintroduced to the agent, ensuring safer subsequent decisions and tool interactions. Importantly, Thought-Aligner modifies only the reasoning phase without altering the underlying agent framework, making it easy to deploy and widely applicable to various agent frameworks. To train the Thought-Aligner model, we construct an instruction dataset across ten representative scenarios and simulate ReAct execution trajectories, generating 5,000 diverse instructions and more than 11,400 safe and unsafe thought pairs. The model is fine-tuned using contrastive learning techniques. Experiments across three agent safety benchmarks involving 12 different LLMs demonstrate that Thought-Aligner raises agent behavioral safety from approximately 50% in the unprotected setting to 90% on average. Additionally, Thought-Aligner maintains response latency below 100ms with minimal resource usage, demonstrating its capability for efficient deployment, broad applicability, and timely responsiveness. This method thus provides a practical dynamic safety solution for the LLM-based agents."
  },
  {
    "title": "GuardReasoner-VL: Safeguarding VLMs via Reinforced Reasoning",
    "url": "http://arxiv.org/abs/2505.11049v1",
    "arxiv_id": "2505.11049v1",
    "authors": [
      "Yue Liu",
      "Shengfang Zhai",
      "Mingzhe Du",
      "Yulin Chen",
      "Tri Cao",
      "Hongcheng Gao",
      "Cheng Wang",
      "Xinfeng Li",
      "Kun Wang",
      "Junfeng Fang",
      "Jiaheng Zhang",
      "Bryan Hooi"
    ],
    "published": "2025-05-16T09:46:10+00:00",
    "summary": "To enhance the safety of VLMs, this paper introduces a novel reasoning-based VLM guard model dubbed GuardReasoner-VL. The core idea is to incentivize the guard model to deliberatively reason before making moderation decisions via online RL. First, we construct GuardReasoner-VLTrain, a reasoning corpus with 123K samples and 631K reasoning steps, spanning text, image, and text-image inputs. Then, based on it, we cold-start our model's reasoning ability via SFT. In addition, we further enhance reasoning regarding moderation through online RL. Concretely, to enhance diversity and difficulty of samples, we conduct rejection sampling followed by data augmentation via the proposed safety-aware data concatenation. Besides, we use a dynamic clipping parameter to encourage exploration in early stages and exploitation in later stages. To balance performance and token efficiency, we design a length-aware safety reward that integrates accuracy, format, and token cost. Extensive experiments demonstrate the superiority of our model. Remarkably, it surpasses the runner-up by 19.27% F1 score on average. We release data, code, and models (3B/7B) of GuardReasoner-VL at https://github.com/yueliu1999/GuardReasoner-VL/"
  },
  {
    "title": "GenoArmory: A Unified Evaluation Framework for Adversarial Attacks on Genomic Foundation Models",
    "url": "http://arxiv.org/abs/2505.10983v1",
    "arxiv_id": "2505.10983v1",
    "authors": [
      "Haozheng Luo",
      "Chenghao Qiu",
      "Yimin Wang",
      "Shang Wu",
      "Jiahao Yu",
      "Han Liu",
      "Binghui Wang",
      "Yan Chen"
    ],
    "published": "2025-05-16T08:29:56+00:00",
    "summary": "We propose the first unified adversarial attack benchmark for Genomic Foundation Models (GFMs), named GenoArmory. Unlike existing GFM benchmarks, GenoArmory offers the first comprehensive evaluation framework to systematically assess the vulnerability of GFMs to adversarial attacks. Methodologically, we evaluate the adversarial robustness of five state-of-the-art GFMs using four widely adopted attack algorithms and three defense strategies. Importantly, our benchmark provides an accessible and comprehensive framework to analyze GFM vulnerabilities with respect to model architecture, quantization schemes, and training datasets. Additionally, we introduce GenoAdv, a new adversarial sample dataset designed to improve GFM safety. Empirically, classification models exhibit greater robustness to adversarial perturbations compared to generative models, highlighting the impact of task type on model vulnerability. Moreover, adversarial attacks frequently target biologically significant genomic regions, suggesting that these models effectively capture meaningful sequence features."
  },
  {
    "title": "Exploration of amorphous V$_2$O$_5$ as cathode for magnesium batteries",
    "url": "http://arxiv.org/abs/2505.10967v1",
    "arxiv_id": "2505.10967v1",
    "authors": [
      "Vijay Choyal",
      "Debsundar Dey",
      "Gopalakrishnan Sai Gautam"
    ],
    "published": "2025-05-16T08:06:04+00:00",
    "summary": "Development of energy storage technologies that can exhibit higher energy densities, better safety, and lower supply-chain constraints than the current state-of-the-art Li-ion batteries (LIBs) is crucial for our transition into sustainable energy use. In this context, Mg batteries (MBs) offer a promising pathway to design energy storage systems with superior volumetric energy densities than LIBs but require the development of positive electrodes (cathodes) exhibiting high energy and power densities. Notably, amorphous materials that lack long range order can exhibit `flatter' potential energy surfaces than crystalline frameworks, possibly resulting in faster Mg$^{2+}$ motion. Here, we use a combination of ab initio molecular dynamics (AIMD), and machine learned interatomic potential (MLIP) based calculations to explore amorphous V$_2$O$_5$ as a potential cathode for MBs. Using an AIMD-generated dataset, we train and validate moment tensor potentials that can accurately model amorphous (Mg)V$_2$O$_5$ Due to the amorphization of V$_2$O$_5$, we observe a 10-14% drop in the average Mg intercalation voltage $-$ but the voltage remains higher than sulfide Mg cathodes. Importantly, we find a $\\sim$seven (five) orders of magnitude higher Mg$^{2+}$ diffusivity in amorphous MgV$_2$O$_5$ than its crystalline version (thiospinel-Mg$_x$Ti$_2$S$_4$), which is directly attributable to the amorphization of the structure. Also, we note the Mg$^{2+}$ motion in the amorphous structure is significantly cross-correlated at low temperatures, with the correlation decreasing with increase in temperature. Thus, our work highlights the potential of amorphous V$_2$O$_5$ as a cathode that can exhibit both high energy and power densities, resulting in the practical deployment of MBs."
  },
  {
    "title": "A Survey on the Safety and Security Threats of Computer-Using Agents: JARVIS or Ultron?",
    "url": "http://arxiv.org/abs/2505.10924v1",
    "arxiv_id": "2505.10924v1",
    "authors": [
      "Ada Chen",
      "Yongjiang Wu",
      "Junyuan Zhang",
      "Shu Yang",
      "Jen-tse Huang",
      "Kun Wang",
      "Wenxuan Wang",
      "Shuai Wang"
    ],
    "published": "2025-05-16T06:56:42+00:00",
    "summary": "Recently, AI-driven interactions with computing devices have advanced from basic prototype tools to sophisticated, LLM-based systems that emulate human-like operations in graphical user interfaces. We are now witnessing the emergence of \\emph{Computer-Using Agents} (CUAs), capable of autonomously performing tasks such as navigating desktop applications, web pages, and mobile apps. However, as these agents grow in capability, they also introduce novel safety and security risks. Vulnerabilities in LLM-driven reasoning, with the added complexity of integrating multiple software components and multimodal inputs, further complicate the security landscape. In this paper, we present a systematization of knowledge on the safety and security threats of CUAs. We conduct a comprehensive literature review and distill our findings along four research objectives: \\textit{\\textbf{(i)}} define the CUA that suits safety analysis; \\textit{\\textbf{(ii)} } categorize current safety threats among CUAs; \\textit{\\textbf{(iii)}} propose a comprehensive taxonomy of existing defensive strategies; \\textit{\\textbf{(iv)}} summarize prevailing benchmarks, datasets, and evaluation metrics used to assess the safety and performance of CUAs. Building on these insights, our work provides future researchers with a structured foundation for exploring unexplored vulnerabilities and offers practitioners actionable guidance in designing and deploying secure Computer-Using Agents."
  },
  {
    "title": "Multi-Objective Preference Optimization: Improving Human Alignment of Generative Models",
    "url": "http://arxiv.org/abs/2505.10892v1",
    "arxiv_id": "2505.10892v1",
    "authors": [
      "Akhil Agnihotri",
      "Rahul Jain",
      "Deepak Ramachandran",
      "Zheng Wen"
    ],
    "published": "2025-05-16T05:58:26+00:00",
    "summary": "Post-training of LLMs with RLHF, and subsequently preference optimization algorithms such as DPO, IPO, etc., made a big difference in improving human alignment. However, all such techniques can only work with a single (human) objective. In practice, human users have multiple objectives, such as helpfulness and harmlessness, and there is no natural way to aggregate them into a single objective. In this paper, we address the multi-objective preference-alignment problem, where a policy must optimize several, potentially conflicting, objectives. We introduce the Multi-Objective Preference Optimization (MOPO) algorithm, which frames alignment as a constrained KL-regularized optimization: the primary objective is maximized while secondary objectives are lower-bounded by tunable safety thresholds. Unlike prior work, MOPO operates directly on pairwise preference data, requires no point-wise reward assumption, and avoids heuristic prompt-context engineering. The method recovers policies on the Pareto front whenever the front is attainable; practically, it reduces to simple closed-form iterative updates suitable for large-scale training. On synthetic benchmarks with diverse canonical preference structures, we show that MOPO approximates the Pareto front. When fine-tuning a 1.3B-parameter language model on real-world human-preference datasets, MOPO attains higher rewards and yields policies that Pareto-dominate baselines; ablation studies confirm optimization stability and robustness to hyperparameters."
  },
  {
    "title": "AutoRAN: Weak-to-Strong Jailbreaking of Large Reasoning Models",
    "url": "http://arxiv.org/abs/2505.10846v1",
    "arxiv_id": "2505.10846v1",
    "authors": [
      "Jiacheng Liang",
      "Tanqiu Jiang",
      "Yuhui Wang",
      "Rongyi Zhu",
      "Fenglong Ma",
      "Ting Wang"
    ],
    "published": "2025-05-16T04:37:12+00:00",
    "summary": "This paper presents AutoRAN, the first automated, weak-to-strong jailbreak attack framework targeting large reasoning models (LRMs). At its core, AutoRAN leverages a weak, less-aligned reasoning model to simulate the target model's high-level reasoning structures, generates narrative prompts, and iteratively refines candidate prompts by incorporating the target model's intermediate reasoning steps. We evaluate AutoRAN against state-of-the-art LRMs including GPT-o3/o4-mini and Gemini-2.5-Flash across multiple benchmark datasets (AdvBench, HarmBench, and StrongReject). Results demonstrate that AutoRAN achieves remarkable success rates (approaching 100%) within one or a few turns across different LRMs, even when judged by a robustly aligned external model. This work reveals that leveraging weak reasoning models can effectively exploit the critical vulnerabilities of much more capable reasoning models, highlighting the need for improved safety measures specifically designed for reasoning-based models. The code for replicating AutoRAN and running records are available at: (https://github.com/JACKPURCELL/AutoRAN-public). (warning: this paper contains potentially harmful content generated by LRMs.)"
  },
  {
    "title": "LARGO: Latent Adversarial Reflection through Gradient Optimization for Jailbreaking LLMs",
    "url": "http://arxiv.org/abs/2505.10838v1",
    "arxiv_id": "2505.10838v1",
    "authors": [
      "Ran Li",
      "Hao Wang",
      "Chengzhi Mao"
    ],
    "published": "2025-05-16T04:12:16+00:00",
    "summary": "Efficient red-teaming method to uncover vulnerabilities in Large Language Models (LLMs) is crucial. While recent attacks often use LLMs as optimizers, the discrete language space make gradient-based methods struggle. We introduce LARGO (Latent Adversarial Reflection through Gradient Optimization), a novel latent self-reflection attack that reasserts the power of gradient-based optimization for generating fluent jailbreaking prompts. By operating within the LLM's continuous latent space, LARGO first optimizes an adversarial latent vector and then recursively call the same LLM to decode the latent into natural language. This methodology yields a fast, effective, and transferable attack that produces fluent and stealthy prompts. On standard benchmarks like AdvBench and JailbreakBench, LARGO surpasses leading jailbreaking techniques, including AutoDAN, by 44 points in attack success rate. Our findings demonstrate a potent alternative to agentic LLM prompting, highlighting the efficacy of interpreting and attacking LLM internals through gradient optimization."
  },
  {
    "title": "MergeBench: A Benchmark for Merging Domain-Specialized LLMs",
    "url": "http://arxiv.org/abs/2505.10833v1",
    "arxiv_id": "2505.10833v1",
    "authors": [
      "Yifei He",
      "Siqi Zeng",
      "Yuzheng Hu",
      "Rui Yang",
      "Tong Zhang",
      "Han Zhao"
    ],
    "published": "2025-05-16T04:02:55+00:00",
    "summary": "Model merging provides a scalable alternative to multi-task training by combining specialized finetuned models through parameter arithmetic, enabling efficient deployment without the need for joint training or access to all task data. While recent methods have shown promise, existing evaluations are limited in both model scale and task diversity, leaving open questions about their applicability to large, domain-specialized LLMs. To tackle the challenges, we introduce MergeBench, a comprehensive evaluation suite designed to assess model merging at scale. MergeBench builds on state-of-the-art open-source language models, including Llama and Gemma families at 2B to 9B scales, and covers five key domains: instruction following, mathematics, multilingual understanding, coding and safety. We standardize finetuning and evaluation protocols, and assess eight representative merging methods across multi-task performance, forgetting and runtime efficiency. Based on extensive experiments, we provide practical guidelines for algorithm selection and share insights showing that model merging tends to perform better on stronger base models, with techniques such as merging coefficient tuning and sparsification improving knowledge retention. However, several challenges remain, including the computational cost on large models, the gap for in-domain performance compared to multi-task models, and the underexplored role of model merging in standard LLM training pipelines. We hope MergeBench provides a foundation for future research to advance the understanding and practical application of model merging. We open source our code at \\href{https://github.com/uiuctml/MergeBench}{https://github.com/uiuctml/MergeBench}."
  },
  {
    "title": "Learning When to Think: Shaping Adaptive Reasoning in R1-Style Models via Multi-Stage RL",
    "url": "http://arxiv.org/abs/2505.10832v1",
    "arxiv_id": "2505.10832v1",
    "authors": [
      "Songjun Tu",
      "Jiahao Lin",
      "Qichao Zhang",
      "Xiangyu Tian",
      "Linjing Li",
      "Xiangyuan Lan",
      "Dongbin Zhao"
    ],
    "published": "2025-05-16T04:01:57+00:00",
    "summary": "Large reasoning models (LRMs) are proficient at generating explicit, step-by-step reasoning sequences before producing final answers. However, such detailed reasoning can introduce substantial computational overhead and latency, particularly for simple problems. To address this over-thinking problem, we explore how to equip LRMs with adaptive thinking capabilities: enabling them to dynamically decide whether or not to engage in explicit reasoning based on problem complexity. Building on R1-style distilled models, we observe that inserting a simple ellipsis (\"...\") into the prompt can stochastically trigger either a thinking or no-thinking mode, revealing a latent controllability in the reasoning behavior. Leveraging this property, we propose AutoThink, a multi-stage reinforcement learning (RL) framework that progressively optimizes reasoning policies via stage-wise reward shaping. AutoThink learns to invoke explicit reasoning only when necessary, while defaulting to succinct responses for simpler tasks. Experiments on five mainstream mathematical benchmarks demonstrate that AutoThink achieves favorable accuracy-efficiency trade-offs compared to recent prompting and RL-based pruning methods. It can be seamlessly integrated into any R1-style model, including both distilled and further fine-tuned variants. Notably, AutoThink improves relative accuracy by 6.4 percent while reducing token usage by 52 percent on DeepSeek-R1-Distill-Qwen-1.5B, establishing a scalable and adaptive reasoning paradigm for LRMs."
  },
  {
    "title": "SynRailObs: A Synthetic Dataset for Obstacle Detection in Railway Scenarios",
    "url": "http://arxiv.org/abs/2505.10784v1",
    "arxiv_id": "2505.10784v1",
    "authors": [
      "Qiushi Guo",
      "Jason Rambach"
    ],
    "published": "2025-05-16T01:49:55+00:00",
    "summary": "Detecting potential obstacles in railway environments is critical for preventing serious accidents. Identifying a broad range of obstacle categories under complex conditions requires large-scale datasets with precisely annotated, high-quality images. However, existing publicly available datasets fail to meet these requirements, thereby hindering progress in railway safety research. To address this gap, we introduce SynRailObs, a high-fidelity synthetic dataset designed to represent a diverse range of weather conditions and geographical features. Furthermore, diffusion models are employed to generate rare and difficult-to-capture obstacles that are typically challenging to obtain in real-world scenarios. To evaluate the effectiveness of SynRailObs, we perform experiments in real-world railway environments, testing on both ballasted and ballastless tracks across various weather conditions. The results demonstrate that SynRailObs holds substantial potential for advancing obstacle detection in railway safety applications. Models trained on this dataset show consistent performance across different distances and environmental conditions. Moreover, the model trained on SynRailObs exhibits zero-shot capabilities, which are essential for applications in security-sensitive domains. The data is available in https://www.kaggle.com/datasets/qiushi910/synrailobs."
  },
  {
    "title": "SECRET: Semi-supervised Clinical Trial Document Similarity Search",
    "url": "http://arxiv.org/abs/2505.10780v1",
    "arxiv_id": "2505.10780v1",
    "authors": [
      "Trisha Das",
      "Afrah Shafquat",
      "Beigi Mandis",
      "Jacob Aptekar",
      "Jimeng Sun"
    ],
    "published": "2025-05-16T01:34:16+00:00",
    "summary": "Clinical trials are vital for evaluation of safety and efficacy of new treatments. However, clinical trials are resource-intensive, time-consuming and expensive to conduct, where errors in trial design, reduced efficacy, and safety events can result in significant delays, financial losses, and damage to reputation. These risks underline the importance of informed and strategic decisions in trial design to mitigate these risks and improve the chances of a successful trial. Identifying similar historical trials is critical as these trials can provide an important reference for potential pitfalls and challenges including serious adverse events, dosage inaccuracies, recruitment difficulties, patient adherence issues, etc. Addressing these challenges in trial design can lead to development of more effective study protocols with optimized patient safety and trial efficiency. In this paper, we present a novel method to identify similar historical trials by summarizing clinical trial protocols and searching for similar trials based on a query trial's protocol. Our approach significantly outperforms all baselines, achieving up to a 78% improvement in recall@1 and a 53% improvement in precision@1 over the best baseline. We also show that our method outperforms all other baselines in partial trial similarity search and zero-shot patient-trial matching, highlighting its superior utility in these tasks."
  },
  {
    "title": "Code-Driven Planning in Grid Worlds with Large Language Models",
    "url": "http://arxiv.org/abs/2505.10749v1",
    "arxiv_id": "2505.10749v1",
    "authors": [
      "Ashwath Vaithinathan Aravindan",
      "Zhisheng Tang",
      "Mayank Kejriwal"
    ],
    "published": "2025-05-15T23:23:31+00:00",
    "summary": "We propose an iterative programmatic planning (IPP) framework for solving grid-based tasks by synthesizing interpretable agent policies expressed in code using large language models (LLMs). Instead of relying on traditional search or reinforcement learning, our approach uses code generation as policy synthesis, where the LLM outputs executable programs that map environment states to action sequences. Our proposed architecture incorporates several prompting strategies, including direct code generation, pseudocode-conditioned refinement, and curriculum-based prompting, but also includes an iterative refinement mechanism that updates code based on task performance feedback. We evaluate our approach using six leading LLMs and two challenging grid-based benchmarks (GRASP and MiniGrid). Our IPP framework demonstrates improvements over direct code generation ranging from 10\\% to as much as 10x across five of the six models and establishes a new state-of-the-art result for GRASP. IPP is found to significantly outperform direct elicitation of a solution from GPT-o3-mini (by 63\\% on MiniGrid to 116\\% on GRASP), demonstrating the viability of the overall approach. Computational costs of all code generation approaches are similar. While code generation has a higher initial prompting cost compared to direct solution elicitation (\\$0.08 per task vs. \\$0.002 per instance for GPT-o3-mini), the code can be reused for any number of instances, making the amortized cost significantly lower (by 400x on GPT-o3-mini across the complete GRASP benchmark)."
  },
  {
    "title": "Interpretable Risk Mitigation in LLM Agent Systems",
    "url": "http://arxiv.org/abs/2505.10670v1",
    "arxiv_id": "2505.10670v1",
    "authors": [
      "Jan Chojnacki"
    ],
    "published": "2025-05-15T19:22:11+00:00",
    "summary": "Autonomous agents powered by large language models (LLMs) enable novel use cases in domains where responsible action is increasingly important. Yet the inherent unpredictability of LLMs raises safety concerns about agent reliability. In this work, we explore agent behaviour in a toy, game-theoretic environment based on a variation of the Iterated Prisoner's Dilemma. We introduce a strategy-modification method-independent of both the game and the prompt-by steering the residual stream with interpretable features extracted from a sparse autoencoder latent space. Steering with the good-faith negotiation feature lowers the average defection probability by 28 percentage points. We also identify feasible steering ranges for several open-source LLM agents. Finally, we hypothesise that game-theoretic evaluation of LLM agents, combined with representation-steering alignment, can generalise to real-world applications on end-user devices and embodied platforms."
  },
  {
    "title": "Real-Time Out-of-Distribution Failure Prevention via Multi-Modal Reasoning",
    "url": "http://arxiv.org/abs/2505.10547v1",
    "arxiv_id": "2505.10547v1",
    "authors": [
      "Milan Ganai",
      "Rohan Sinha",
      "Christopher Agia",
      "Daniel Morton",
      "Marco Pavone"
    ],
    "published": "2025-05-15T17:55:28+00:00",
    "summary": "Foundation models can provide robust high-level reasoning on appropriate safety interventions in hazardous scenarios beyond a robot's training data, i.e. out-of-distribution (OOD) failures. However, due to the high inference latency of Large Vision and Language Models, current methods rely on manually defined intervention policies to enact fallbacks, thereby lacking the ability to plan generalizable, semantically safe motions. To overcome these challenges we present FORTRESS, a framework that generates and reasons about semantically safe fallback strategies in real time to prevent OOD failures. At a low frequency in nominal operations, FORTRESS uses multi-modal reasoners to identify goals and anticipate failure modes. When a runtime monitor triggers a fallback response, FORTRESS rapidly synthesizes plans to fallback goals while inferring and avoiding semantically unsafe regions in real time. By bridging open-world, multi-modal reasoning with dynamics-aware planning, we eliminate the need for hard-coded fallbacks and human safety interventions. FORTRESS outperforms on-the-fly prompting of slow reasoning models in safety classification accuracy on synthetic benchmarks and real-world ANYmal robot data, and further improves system safety and planning success in simulation and on quadrotor hardware for urban navigation."
  },
  {
    "title": "Large Language Models for Cancer Communication: Evaluating Linguistic Quality, Safety, and Accessibility in Generative AI",
    "url": "http://arxiv.org/abs/2505.10472v1",
    "arxiv_id": "2505.10472v1",
    "authors": [
      "Agnik Saha",
      "Victoria Churchill",
      "Anny D. Rodriguez",
      "Ugur Kursuncu",
      "Muhammed Y. Idris"
    ],
    "published": "2025-05-15T16:23:21+00:00",
    "summary": "Effective communication about breast and cervical cancers remains a persistent health challenge, with significant gaps in public understanding of cancer prevention, screening, and treatment, potentially leading to delayed diagnoses and inadequate treatments. This study evaluates the capabilities and limitations of Large Language Models (LLMs) in generating accurate, safe, and accessible cancer-related information to support patient understanding. We evaluated five general-purpose and three medical LLMs using a mixed-methods evaluation framework across linguistic quality, safety and trustworthiness, and communication accessibility and affectiveness. Our approach utilized quantitative metrics, qualitative expert ratings, and statistical analysis using Welch's ANOVA, Games-Howell, and Hedges' g. Our results show that general-purpose LLMs produced outputs of higher linguistic quality and affectiveness, while medical LLMs demonstrate greater communication accessibility. However, medical LLMs tend to exhibit higher levels of potential harm, toxicity, and bias, reducing their performance in safety and trustworthiness. Our findings indicate a duality between domain-specific knowledge and safety in health communications. The results highlight the need for intentional model design with targeted improvements, particularly in mitigating harm and bias, and improving safety and affectiveness. This study provides a comprehensive evaluation of LLMs for cancer communication, offering critical insights for improving AI-generated health content and informing future development of accurate, safe, and accessible digital health tools."
  },
  {
    "title": "Formalising Human-in-the-Loop: Computational Reductions, Failure Modes, and Legal-Moral Responsibility",
    "url": "http://arxiv.org/abs/2505.10426v1",
    "arxiv_id": "2505.10426v1",
    "authors": [
      "Maurice Chiodo",
      "Dennis M\u00fcller",
      "Paul Siewert",
      "Jean-Luc Wetherall",
      "Zoya Yasmine",
      "John Burden"
    ],
    "published": "2025-05-15T15:42:14+00:00",
    "summary": "The legal compliance and safety of different Human-in-the-loop (HITL) setups for AI can vary greatly. This manuscript aims to identify new ways of choosing between such setups, and shows that there is an unavoidable trade-off between the attribution of legal responsibility and the technical explainability of AI. We begin by using the notion of oracle machines from computability theory to formalise different HITL setups, distinguishing between trivial human monitoring, single endpoint human action, and highly involved interaction between the human(s) and the AI. These correspond to total functions, many-one reductions, and Turing reductions respectively. A taxonomy categorising HITL failure modes is then presented, highlighting the limitations on what any HITL setup can actually achieve. Our approach then identifies oversights from UK and EU legal frameworks, which focus on certain HITL setups which may not always achieve the desired ethical, legal, and sociotechnical outcomes. We suggest areas where the law should recognise the effectiveness of different HITL setups and assign responsibility in these contexts, avoiding unnecessary and unproductive human \"scapegoating\". Overall, we show how HITL setups involve many technical design decisions, and can be prone to failures which are often out of the humans' control. This opens up a new analytic perspective on the challenges arising in the creation of HITL setups, helping inform AI developers and lawmakers on designing HITL to better achieve their desired outcomes."
  },
  {
    "title": "FactsR: A Safer Method for Producing High Quality Healthcare Documentation",
    "url": "http://arxiv.org/abs/2505.10360v1",
    "arxiv_id": "2505.10360v1",
    "authors": [
      "Victor Petr\u00e9n Bach Hansen",
      "Lasse Krogsb\u00f8ll",
      "Jonas Lyngs\u00f8",
      "Mathias Baltzersen",
      "Andreas Motzfeldt",
      "Kevin Pelgrims",
      "Lars Maal\u00f8e"
    ],
    "published": "2025-05-15T14:51:22+00:00",
    "summary": "There are now a multitude of AI-scribing solutions for healthcare promising the utilization of large language models for ambient documentation. However, these AI scribes still rely on one-shot, or few-shot prompts for generating notes after the consultation has ended, employing little to no reasoning. This risks long notes with an increase in hallucinations, misrepresentation of the intent of the clinician, and reliance on the proofreading of the clinician to catch errors. A dangerous combination for patient safety if vigilance is compromised by workload and fatigue. In this paper, we introduce a method for extracting salient clinical information in real-time alongside the healthcare consultation, denoted Facts, and use that information recursively to generate the final note. The FactsR method results in more accurate and concise notes by placing the clinician-in-the-loop of note generation, while opening up new use cases within real-time decision support."
  },
  {
    "title": "J1: Incentivizing Thinking in LLM-as-a-Judge via Reinforcement Learning",
    "url": "http://arxiv.org/abs/2505.10320v1",
    "arxiv_id": "2505.10320v1",
    "authors": [
      "Chenxi Whitehouse",
      "Tianlu Wang",
      "Ping Yu",
      "Xian Li",
      "Jason Weston",
      "Ilia Kulikov",
      "Swarnadeep Saha"
    ],
    "published": "2025-05-15T14:05:15+00:00",
    "summary": "The progress of AI is bottlenecked by the quality of evaluation, and powerful LLM-as-a-Judge models have proved to be a core solution. Improved judgment ability is enabled by stronger chain-of-thought reasoning, motivating the need to find the best recipes for training such models to think. In this work we introduce J1, a reinforcement learning approach to training such models. Our method converts both verifiable and non-verifiable prompts to judgment tasks with verifiable rewards that incentivize thinking and mitigate judgment bias. In particular, our approach outperforms all other existing 8B or 70B models when trained at those sizes, including models distilled from DeepSeek-R1. J1 also outperforms o1-mini, and even R1 on some benchmarks, despite training a smaller model. We provide analysis and ablations comparing Pairwise-J1 vs Pointwise-J1 models, offline vs online training recipes, reward strategies, seed prompts, and variations in thought length and content. We find that our models make better judgments by learning to outline evaluation criteria, comparing against self-generated reference answers, and re-evaluating the correctness of model responses."
  },
  {
    "title": "AttentionGuard: Transformer-based Misbehavior Detection for Secure Vehicular Platoons",
    "url": "http://arxiv.org/abs/2505.10273v1",
    "arxiv_id": "2505.10273v1",
    "authors": [
      "Hexu Li",
      "Konstantinos Kalogiannis",
      "Ahmed Mohamed Hussain",
      "Panos Papadimitratos"
    ],
    "published": "2025-05-15T13:24:09+00:00",
    "summary": "Vehicle platooning, with vehicles traveling in close formation coordinated through Vehicle-to-Everything (V2X) communications, offers significant benefits in fuel efficiency and road utilization. However, it is vulnerable to sophisticated falsification attacks by authenticated insiders that can destabilize the formation and potentially cause catastrophic collisions. This paper addresses this challenge: misbehavior detection in vehicle platooning systems. We present AttentionGuard, a transformer-based framework for misbehavior detection that leverages the self-attention mechanism to identify anomalous patterns in mobility data. Our proposal employs a multi-head transformer-encoder to process sequential kinematic information, enabling effective differentiation between normal mobility patterns and falsification attacks across diverse platooning scenarios, including steady-state (no-maneuver) operation, join, and exit maneuvers. Our evaluation uses an extensive simulation dataset featuring various attack vectors (constant, gradual, and combined falsifications) and operational parameters (controller types, vehicle speeds, and attacker positions). Experimental results demonstrate that AttentionGuard achieves up to 0.95 F1-score in attack detection, with robust performance maintained during complex maneuvers. Notably, our system performs effectively with minimal latency (100ms decision intervals), making it suitable for real-time transportation safety applications. Comparative analysis reveals superior detection capabilities and establishes the transformer-encoder as a promising approach for securing Cooperative Intelligent Transport Systems (C-ITS) against sophisticated insider threats."
  },
  {
    "title": "Sage Deer: A Super-Aligned Driving Generalist Is Your Copilot",
    "url": "http://arxiv.org/abs/2505.10257v1",
    "arxiv_id": "2505.10257v1",
    "authors": [
      "Hao Lu",
      "Jiaqi Tang",
      "Jiyao Wang",
      "Yunfan LU",
      "Xu Cao",
      "Qingyong Hu",
      "Yin Wang",
      "Yuting Zhang",
      "Tianxin Xie",
      "Yunpeng Zhang",
      "Yong Chen",
      "Jiayu. Gao",
      "Bin Huang",
      "Dengbo He",
      "Shuiguang Deng",
      "Hao Chen",
      "Ying-Cong Chen"
    ],
    "published": "2025-05-15T13:08:44+00:00",
    "summary": "The intelligent driving cockpit, an important part of intelligent driving, needs to match different users' comfort, interaction, and safety needs. This paper aims to build a Super-Aligned and GEneralist DRiving agent, SAGE DeeR. Sage Deer achieves three highlights: (1) Super alignment: It achieves different reactions according to different people's preferences and biases. (2) Generalist: It can understand the multi-view and multi-mode inputs to reason the user's physiological indicators, facial emotions, hand movements, body movements, driving scenarios, and behavioral decisions. (3) Self-Eliciting: It can elicit implicit thought chains in the language space to further increase generalist and super-aligned abilities. Besides, we collected multiple data sets and built a large-scale benchmark. This benchmark measures the deer's perceptual decision-making ability and the super alignment's accuracy."
  },
  {
    "title": "Towards Safe Robot Foundation Models Using Inductive Biases",
    "url": "http://arxiv.org/abs/2505.10219v1",
    "arxiv_id": "2505.10219v1",
    "authors": [
      "Maximilian T\u00f6lle",
      "Theo Gruner",
      "Daniel Palenicek",
      "Tim Schneider",
      "Jonas G\u00fcnster",
      "Joe Watson",
      "Davide Tateo",
      "Puze Liu",
      "Jan Peters"
    ],
    "published": "2025-05-15T12:22:21+00:00",
    "summary": "Safety is a critical requirement for the real-world deployment of robotic systems. Unfortunately, while current robot foundation models show promising generalization capabilities across a wide variety of tasks, they fail to address safety, an important aspect for ensuring long-term operation. Current robot foundation models assume that safe behavior should emerge by learning from a sufficiently large dataset of demonstrations. However, this approach has two clear major drawbacks. Firstly, there are no formal safety guarantees for a behavior cloning policy trained using supervised learning. Secondly, without explicit knowledge of any safety constraints, the policy may require an unreasonable number of additional demonstrations to even approximate the desired constrained behavior. To solve these key issues, we show how we can instead combine robot foundation models with geometric inductive biases using ATACOM, a safety layer placed after the foundation policy that ensures safe state transitions by enforcing action constraints. With this approach, we can ensure formal safety guarantees for generalist policies without providing extensive demonstrations of safe behavior, and without requiring any specific fine-tuning for safety. Our experiments show that our approach can be beneficial both for classical manipulation tasks, where we avoid unwanted collisions with irrelevant objects, and for dynamic tasks, such as the robot air hockey environment, where we can generate fast trajectories respecting complex tasks and joint space constraints."
  },
  {
    "title": "LanTu: Dynamics-Enhanced Deep Learning for Eddy-Resolving Ocean Forecasting",
    "url": "http://arxiv.org/abs/2505.10191v1",
    "arxiv_id": "2505.10191v1",
    "authors": [
      "Qingyu Zheng",
      "Qi Shao",
      "Guijun Han",
      "Wei Li",
      "Hong Li",
      "Xuan Wang"
    ],
    "published": "2025-05-15T11:47:54+00:00",
    "summary": "Mesoscale eddies dominate the spatiotemporal multiscale variability of the ocean, and their impact on the energy cascade of the global ocean cannot be ignored. Eddy-resolving ocean forecasting is providing more reliable protection for fisheries and navigational safety, but also presents significant scientific challenges and high computational costs for traditional numerical models. Artificial intelligence (AI)-based weather and ocean forecasting systems are becoming powerful tools that balance forecast performance with computational efficiency. However, the complex multiscale features in the ocean dynamical system make AI models still face many challenges in mesoscale eddy forecasting (especially regional modelling). Here, we develop LanTu, a regional eddy-resolving ocean forecasting system based on dynamics-enhanced deep learning. We incorporate cross-scale interactions into LanTu and construct multiscale physical constraint for optimising LanTu guided by knowledge of eddy dynamics in order to improve the forecasting skill of LanTu for mesoscale evolution. The results show that LanTu outperforms the existing advanced operational numerical ocean forecasting system (NOFS) and AI-based ocean forecasting system (AI-OFS) in temperature, salinity, sea level anomaly and current prediction, with a lead time of more than 10 days. Our study highlights that dynamics-enhanced deep learning (LanTu) can be a powerful paradigm for eddy-resolving ocean forecasting."
  },
  {
    "title": "Closure and Complexity of Temporal Causality",
    "url": "http://arxiv.org/abs/2505.10186v1",
    "arxiv_id": "2505.10186v1",
    "authors": [
      "Mishel Carelli",
      "Bernd Finkbeiner",
      "Julian Siber"
    ],
    "published": "2025-05-15T11:37:15+00:00",
    "summary": "Temporal causality defines what property causes some observed temporal behavior (the effect) in a given computation, based on a counterfactual analysis of similar computations. In this paper, we study its closure properties and the complexity of computing causes. For the former, we establish that safety, reachability, and recurrence properties are all closed under causal inference: If the effect is from one of these property classes, then the cause for this effect is from the same class. We also show that persistence and obligation properties are not closed in this way. These results rest on a topological characterization of causes which makes them applicable to a wide range of similarity relations between computations. Finally, our complexity analysis establishes improved upper bounds for computing causes for safety, reachability, and recurrence properties. We also present the first lower bounds for all of the classes."
  },
  {
    "title": "Knowledge-Based Aerospace Engineering -- A Systematic Literature Review",
    "url": "http://arxiv.org/abs/2505.10142v1",
    "arxiv_id": "2505.10142v1",
    "authors": [
      "Tim Wittenborg",
      "Ildar Baimuratov",
      "Ludvig Kn\u00f6\u00f6s Franz\u00e9n",
      "Ingo Staack",
      "Ulrich R\u00f6mer",
      "S\u00f6ren Auer"
    ],
    "published": "2025-05-15T10:16:45+00:00",
    "summary": "The aerospace industry operates at the frontier of technological innovation while maintaining high standards regarding safety and reliability. In this environment, with an enormous potential for re-use and adaptation of existing solutions and methods, Knowledge-Based Engineering (KBE) has been applied for decades. The objective of this study is to identify and examine state-of-the-art knowledge management practices in the field of aerospace engineering. Our contributions include: 1) A SWARM-SLR of over 1,000 articles with qualitative analysis of 164 selected articles, supported by two aerospace engineering domain expert surveys. 2) A knowledge graph of over 700 knowledge-based aerospace engineering processes, software, and data, formalized in the interoperable Web Ontology Language (OWL) and mapped to Wikidata entries where possible. The knowledge graph is represented on the Open Research Knowledge Graph (ORKG), and an aerospace Wikibase, for reuse and continuation of structuring aerospace engineering knowledge exchange. 3) Our resulting intermediate and final artifacts of the knowledge synthesis, available as a Zenodo dataset. This review sets a precedent for structured, semantic-based approaches to managing aerospace engineering knowledge. By advancing these principles, research, and industry can achieve more efficient design processes, enhanced collaboration, and a stronger commitment to sustainable aviation."
  },
  {
    "title": "Dark LLMs: The Growing Threat of Unaligned AI Models",
    "url": "http://arxiv.org/abs/2505.10066v1",
    "arxiv_id": "2505.10066v1",
    "authors": [
      "Michael Fire",
      "Yitzhak Elbazis",
      "Adi Wasenstein",
      "Lior Rokach"
    ],
    "published": "2025-05-15T08:07:04+00:00",
    "summary": "Large Language Models (LLMs) rapidly reshape modern life, advancing fields from healthcare to education and beyond. However, alongside their remarkable capabilities lies a significant threat: the susceptibility of these models to jailbreaking. The fundamental vulnerability of LLMs to jailbreak attacks stems from the very data they learn from. As long as this training data includes unfiltered, problematic, or 'dark' content, the models can inherently learn undesirable patterns or weaknesses that allow users to circumvent their intended safety controls. Our research identifies the growing threat posed by dark LLMs models deliberately designed without ethical guardrails or modified through jailbreak techniques. In our research, we uncovered a universal jailbreak attack that effectively compromises multiple state-of-the-art models, enabling them to answer almost any question and produce harmful outputs upon request. The main idea of our attack was published online over seven months ago. However, many of the tested LLMs were still vulnerable to this attack. Despite our responsible disclosure efforts, responses from major LLM providers were often inadequate, highlighting a concerning gap in industry practices regarding AI safety. As model training becomes more accessible and cheaper, and as open-source LLMs proliferate, the risk of widespread misuse escalates. Without decisive intervention, LLMs may continue democratizing access to dangerous knowledge, posing greater risks than anticipated."
  },
  {
    "title": "Enhancing performance in bolt torque tightening using a connected torque wrench and augmented reality",
    "url": "http://arxiv.org/abs/2505.10047v1",
    "arxiv_id": "2505.10047v1",
    "authors": [
      "Adeline Fau",
      "Mina Ghobrial",
      "Philippe Seitier",
      "Pierre Lagarrigue",
      "Michel Galaup",
      "Alain Daidi\u00e9",
      "Patrick Gilles"
    ],
    "published": "2025-05-15T07:45:45+00:00",
    "summary": "Modern production rates and the increasing complexity of mechanical systems require efficient and effective manufacturing and assembly processes. The transition to Industry 4.0, supported by the deployment of innovative tools such as Augmented Reality (AR), equips the industry to tackle future challenges. Among critical processes, the assembly and tightening of bolted joints stand out due to their significant safety and economic implications across various industrial sectors. This study proposes an innovative tightening method designed to enhance the reliability of bolted assembly tightening through the use of Augmented Reality and connected tools. A 6-Degrees-of-Freedom (6-DoF) tracked connected torque wrench assists the operator during tightening, ensuring each screw is tightened to the correct torque. The effectiveness of this method is compared with the conventional tightening method using paper instructions. Participants in the study carried out tightening sequences on two simple parts with multiple screws. The study evaluates the impact of the proposed method on task performance and its acceptability to operators. The tracked connected torque wrench provides considerable assistance to the operators, including wrench control and automatic generation of tightening reports. The results suggest that the AR-based method has the potential to ensure reliable torque tightening of bolted joints."
  },
  {
    "title": "Application of YOLOv8 in monocular downward multiple Car Target detection",
    "url": "http://arxiv.org/abs/2505.10016v1",
    "arxiv_id": "2505.10016v1",
    "authors": [
      "Shijie Lyu"
    ],
    "published": "2025-05-15T06:58:45+00:00",
    "summary": "Autonomous driving technology is progressively transforming traditional car driving methods, marking a significant milestone in modern transportation. Object detection serves as a cornerstone of autonomous systems, playing a vital role in enhancing driving safety, enabling autonomous functionality, improving traffic efficiency, and facilitating effective emergency responses. However, current technologies such as radar for environmental perception, cameras for road perception, and vehicle sensor networks face notable challenges, including high costs, vulnerability to weather and lighting conditions, and limited resolution.To address these limitations, this paper presents an improved autonomous target detection network based on YOLOv8. By integrating structural reparameterization technology, a bidirectional pyramid structure network model, and a novel detection pipeline into the YOLOv8 framework, the proposed approach achieves highly efficient and precise detection of multi-scale, small, and remote objects. Experimental results demonstrate that the enhanced model can effectively detect both large and small objects with a detection accuracy of 65%, showcasing significant advancements over traditional methods.This improved model holds substantial potential for real-world applications and is well-suited for autonomous driving competitions, such as the Formula Student Autonomous China (FSAC), particularly excelling in scenarios involving single-target and small-object detection."
  },
  {
    "title": "A Survey on Open-Source Edge Computing Simulators and Emulators: The Computing and Networking Convergence Perspective",
    "url": "http://arxiv.org/abs/2505.09995v1",
    "arxiv_id": "2505.09995v1",
    "authors": [
      "Jianpeng Qi",
      "Chao Liu",
      "Xiao Zhang",
      "Lei Wang",
      "Rui Wang",
      "Junyu Dong",
      "Yanwei Yu"
    ],
    "published": "2025-05-15T06:17:56+00:00",
    "summary": "Edge computing, with its low latency, dynamic scalability, and location awareness, along with the convergence of computing and communication paradigms, has been successfully applied in critical domains such as industrial IoT, smart healthcare, smart homes, and public safety. This paper provides a comprehensive survey of open-source edge computing simulators and emulators, presented in our GitHub repository (https://github.com/qijianpeng/awesome-edge-computing), emphasizing the convergence of computing and networking paradigms. By examining more than 40 tools, including CloudSim, NS-3, and others, we identify the strengths and limitations in simulating and emulating edge environments. This survey classifies these tools into three categories: packet-level, application-level, and emulators. Furthermore, we evaluate them across five dimensions, ranging from resource representation to resource utilization. The survey highlights the integration of different computing paradigms, packet processing capabilities, support for edge environments, user-defined metric interfaces, and scenario visualization. The findings aim to guide researchers in selecting appropriate tools for developing and validating advanced computing and networking technologies."
  },
  {
    "title": "Provably safe and human-like car-following behaviors: Part 2. A parsimonious multi-phase model with projected braking",
    "url": "http://arxiv.org/abs/2505.09988v1",
    "arxiv_id": "2505.09988v1",
    "authors": [
      "Wen-Long Jin"
    ],
    "published": "2025-05-15T06:03:02+00:00",
    "summary": "Ensuring safe and human-like trajectory planning for automated vehicles amidst real-world uncertainties remains a critical challenge. While existing car-following models often struggle to consistently provide rigorous safety proofs alongside human-like acceleration and deceleration patterns, we introduce a novel multi-phase projection-based car-following model. This model is designed to balance safety and performance by incorporating bounded acceleration and deceleration rates while emulating key human driving principles. Building upon a foundation of fundamental driving principles and a multi-phase dynamical systems analysis (detailed in Part 1 of this study \\citep{jin2025WA20-02_Part1}), we first highlight the limitations of extending standard models like Newell's with simple bounded deceleration. Inspired by human drivers' anticipatory behavior, we mathematically define and analyze projected braking profiles for both leader and follower vehicles, establishing safety criteria and new phase definitions based on the projected braking lead-vehicle problem. The proposed parsimonious model combines an extended Newell's model for nominal driving with a new control law for scenarios requiring projected braking. Using speed-spacing phase plane analysis, we provide rigorous mathematical proofs of the model's adherence to defined safe and human-like driving principles, including collision-free operation, bounded deceleration, and acceptable safe stopping distance, under reasonable initial conditions. Numerical simulations validate the model's superior performance in achieving both safety and human-like braking profiles for the stationary lead-vehicle problem. Finally, we discuss the model's implications and future research directions."
  },
  {
    "title": "Provably safe and human-like car-following behaviors: Part 1. Analysis of phases and dynamics in standard models",
    "url": "http://arxiv.org/abs/2505.09987v1",
    "arxiv_id": "2505.09987v1",
    "authors": [
      "Wen-Long Jin"
    ],
    "published": "2025-05-15T05:56:13+00:00",
    "summary": "Trajectory planning is essential for ensuring safe driving in the face of uncertainties related to communication, sensing, and dynamic factors such as weather, road conditions, policies, and other road users. Existing car-following models often lack rigorous safety proofs and the ability to replicate human-like driving behaviors consistently. This article applies multi-phase dynamical systems analysis to well-known car-following models to highlight the characteristics and limitations of existing approaches. We begin by formulating fundamental principles for safe and human-like car-following behaviors, which include zeroth-order principles for comfort and minimum jam spacings, first-order principles for speeds and time gaps, and second-order principles for comfort acceleration/deceleration bounds as well as braking profiles. From a set of these zeroth- and first-order principles, we derive Newell's simplified car-following model. Subsequently, we analyze phases within the speed-spacing plane for the stationary lead-vehicle problem in Newell's model and its extensions, which incorporate both bounded acceleration and deceleration. We then analyze the performance of the Intelligent Driver Model and the Gipps model. Through this analysis, we highlight the limitations of these models with respect to some of the aforementioned principles. Numerical simulations and empirical observations validate the theoretical insights. Finally, we discuss future research directions to further integrate safety, human-like behaviors, and vehicular automation in car-following models, which are addressed in Part 2 of this study \\citep{jin2025WA20-02_Part2}, where we develop a novel multi-phase projection-based car-following model that addresses the limitations identified here."
  },
  {
    "title": "Analysing Safety Risks in LLMs Fine-Tuned with Pseudo-Malicious Cyber Security Data",
    "url": "http://arxiv.org/abs/2505.09974v1",
    "arxiv_id": "2505.09974v1",
    "authors": [
      "Adel ElZemity",
      "Budi Arief",
      "Shujun Li"
    ],
    "published": "2025-05-15T05:22:53+00:00",
    "summary": "The integration of large language models (LLMs) into cyber security applications presents significant opportunities, such as enhancing threat analysis and malware detection, but can also introduce critical risks and safety concerns, including personal data leakage and automated generation of new malware. We present a systematic evaluation of safety risks in fine-tuned LLMs for cyber security applications. Using the OWASP Top 10 for LLM Applications framework, we assessed seven open-source LLMs: Phi 3 Mini 3.8B, Mistral 7B, Qwen 2.5 7B, Llama 3 8B, Llama 3.1 8B, Gemma 2 9B, and Llama 2 70B. Our evaluation shows that fine-tuning reduces safety resilience across all tested LLMs (e.g., the safety score of Llama 3.1 8B against prompt injection drops from 0.95 to 0.15). We propose and evaluate a safety alignment approach that carefully rewords instruction-response pairs to include explicit safety precautions and ethical considerations. This approach demonstrates that it is possible to maintain or even improve model safety while preserving technical utility, offering a practical path forward for developing safer fine-tuning methodologies. This work offers a systematic evaluation for safety risks in LLMs, enabling safer adoption of generative AI in sensitive domains, and contributing towards the development of secure, trustworthy, and ethically aligned LLMs."
  },
  {
    "title": "Pre-Act: Multi-Step Planning and Reasoning Improves Acting in LLM Agents",
    "url": "http://arxiv.org/abs/2505.09970v1",
    "arxiv_id": "2505.09970v1",
    "authors": [
      "Mrinal Rawat",
      "Ambuje Gupta",
      "Rushil Goomer",
      "Alessandro Di Bari",
      "Neha Gupta",
      "Roberto Pieraccini"
    ],
    "published": "2025-05-15T05:17:47+00:00",
    "summary": "The ReAct (Reasoning + Action) capability in large language models (LLMs) has become the foundation of modern agentic systems. Recent LLMs, such as DeepSeek-R1 and OpenAI o1/o3, exemplify this by emphasizing reasoning through the generation of ample intermediate tokens, which help build a strong premise before producing the final output tokens. In this paper, we introduce Pre-Act, a novel approach that enhances the agent's performance by creating a multi-step execution plan along with the detailed reasoning for the given user input. This plan incrementally incorporates previous steps and tool outputs, refining itself after each step execution until the final response is obtained. Our approach is applicable to both conversational and non-conversational agents. To measure the performance of task-oriented agents comprehensively, we propose a two-level evaluation framework: (1) turn level and (2) end-to-end. Our turn-level evaluation, averaged across five models, shows that our approach, Pre-Act, outperforms ReAct by 70% in Action Recall on the Almita dataset. While this approach is effective for larger models, smaller models crucial for practical applications, where latency and cost are key constraints, often struggle with complex reasoning tasks required for agentic systems. To address this limitation, we fine-tune relatively small models such as Llama 3.1 (8B & 70B) using the proposed Pre-Act approach. Our experiments show that the fine-tuned 70B model outperforms GPT-4, achieving a 69.5% improvement in action accuracy (turn-level) and a 28% improvement in goal completion rate (end-to-end) on the Almita (out-of-domain) dataset."
  },
  {
    "title": "Advanced Crash Causation Analysis for Freeway Safety: A Large Language Model Approach to Identifying Key Contributing Factors",
    "url": "http://arxiv.org/abs/2505.09949v1",
    "arxiv_id": "2505.09949v1",
    "authors": [
      "Ahmed S. Abdelrahman",
      "Mohamed Abdel-Aty",
      "Samgyu Yang",
      "Abdulrahman Faden"
    ],
    "published": "2025-05-15T04:07:55+00:00",
    "summary": "Understanding the factors contributing to traffic crashes and developing strategies to mitigate their severity is essential. Traditional statistical methods and machine learning models often struggle to capture the complex interactions between various factors and the unique characteristics of each crash. This research leverages large language model (LLM) to analyze freeway crash data and provide crash causation analysis accordingly. By compiling 226 traffic safety studies related to freeway crashes, a training dataset encompassing environmental, driver, traffic, and geometric design factors was created. The Llama3 8B model was fine-tuned using QLoRA to enhance its understanding of freeway crashes and their contributing factors, as covered in these studies. The fine-tuned Llama3 8B model was then used to identify crash causation without pre-labeled data through zero-shot classification, providing comprehensive explanations to ensure that the identified causes were reasonable and aligned with existing research. Results demonstrate that LLMs effectively identify primary crash causes such as alcohol-impaired driving, speeding, aggressive driving, and driver inattention. Incorporating event data, such as road maintenance, offers more profound insights. The model's practical applicability and potential to improve traffic safety measures were validated by a high level of agreement among researchers in the field of traffic safety, as reflected in questionnaire results with 88.89%. This research highlights the complex nature of traffic crashes and how LLMs can be used for comprehensive analysis of crash causation and other contributing factors. Moreover, it provides valuable insights and potential countermeasures to aid planners and policymakers in developing more effective and efficient traffic safety practices."
  },
  {
    "title": "VRU-CIPI: Crossing Intention Prediction at Intersections for Improving Vulnerable Road Users Safety",
    "url": "http://arxiv.org/abs/2505.09935v1",
    "arxiv_id": "2505.09935v1",
    "authors": [
      "Ahmed S. Abdelrahman",
      "Mohamed Abdel-Aty",
      "Quoc Dai Tran"
    ],
    "published": "2025-05-15T03:40:29+00:00",
    "summary": "Understanding and predicting human behavior in-thewild, particularly at urban intersections, remains crucial for enhancing interaction safety between road users. Among the most critical behaviors are crossing intentions of Vulnerable Road Users (VRUs), where misinterpretation may result in dangerous conflicts with oncoming vehicles. In this work, we propose the VRU-CIPI framework with a sequential attention-based model designed to predict VRU crossing intentions at intersections. VRU-CIPI employs Gated Recurrent Unit (GRU) to capture temporal dynamics in VRU movements, combined with a multi-head Transformer self-attention mechanism to encode contextual and spatial dependencies critical for predicting crossing direction. Evaluated on UCF-VRU dataset, our proposed achieves state-of-the-art performance with an accuracy of 96.45% and achieving real-time inference speed reaching 33 frames per second. Furthermore, by integrating with Infrastructure-to-Vehicles (I2V) communication, our approach can proactively enhance intersection safety through timely activation of crossing signals and providing early warnings to connected vehicles, ensuring smoother and safer interactions for all road users."
  },
  {
    "title": "PIG: Privacy Jailbreak Attack on LLMs via Gradient-based Iterative In-Context Optimization",
    "url": "http://arxiv.org/abs/2505.09921v1",
    "arxiv_id": "2505.09921v1",
    "authors": [
      "Yidan Wang",
      "Yanan Cao",
      "Yubing Ren",
      "Fang Fang",
      "Zheng Lin",
      "Binxing Fang"
    ],
    "published": "2025-05-15T03:11:57+00:00",
    "summary": "Large Language Models (LLMs) excel in various domains but pose inherent privacy risks. Existing methods to evaluate privacy leakage in LLMs often use memorized prefixes or simple instructions to extract data, both of which well-alignment models can easily block. Meanwhile, Jailbreak attacks bypass LLM safety mechanisms to generate harmful content, but their role in privacy scenarios remains underexplored. In this paper, we examine the effectiveness of jailbreak attacks in extracting sensitive information, bridging privacy leakage and jailbreak attacks in LLMs. Moreover, we propose PIG, a novel framework targeting Personally Identifiable Information (PII) and addressing the limitations of current jailbreak methods. Specifically, PIG identifies PII entities and their types in privacy queries, uses in-context learning to build a privacy context, and iteratively updates it with three gradient-based strategies to elicit target PII. We evaluate PIG and existing jailbreak methods using two privacy-related datasets. Experiments on four white-box and two black-box LLMs show that PIG outperforms baseline methods and achieves state-of-the-art (SoTA) results. The results underscore significant privacy risks in LLMs, emphasizing the need for stronger safeguards. Our code is availble at \\href{https://github.com/redwyd/PrivacyJailbreak}{https://github.com/redwyd/PrivacyJailbreak}."
  },
  {
    "title": "Offline Reinforcement Learning for Microgrid Voltage Regulation",
    "url": "http://arxiv.org/abs/2505.09920v1",
    "arxiv_id": "2505.09920v1",
    "authors": [
      "Shan Yang",
      "Yongli Zhu"
    ],
    "published": "2025-05-15T03:10:18+00:00",
    "summary": "This paper presents a study on using different offline reinforcement learning algorithms for microgrid voltage regulation with solar power penetration. When environment interaction is unviable due to technical or safety reasons, the proposed approach can still obtain an applicable model through offline-style training on a previously collected dataset, lowering the negative impact of lacking online environment interactions. Experiment results on the IEEE 33-bus system demonstrate the feasibility and effectiveness of the proposed approach on different offline datasets, including the one with merely low-quality experience."
  },
  {
    "title": "Diffusion-SAFE: Shared Autonomy Framework with Diffusion for Safe Human-to-Robot Driving Handover",
    "url": "http://arxiv.org/abs/2505.09889v1",
    "arxiv_id": "2505.09889v1",
    "authors": [
      "Yunxin Fan",
      "Monroe Kennedy III"
    ],
    "published": "2025-05-15T01:12:48+00:00",
    "summary": "Safe handover in shared autonomy for vehicle control is well-established in modern vehicles. However, avoiding accidents often requires action several seconds in advance. This necessitates understanding human driver behavior and an expert control strategy for seamless intervention when a collision or unsafe state is predicted. We propose Diffusion-SAFE, a closed-loop shared autonomy framework leveraging diffusion models to: (1) predict human driving behavior for detection of potential risks, (2) generate safe expert trajectories, and (3) enable smooth handovers by blending human and expert policies over a short time horizon. Unlike prior works which use engineered score functions to rate driving performance, our approach enables both performance evaluation and optimal action sequence generation from demonstrations. By adjusting the forward and reverse processes of the diffusion-based copilot, our method ensures a gradual transition of control authority, by mimicking the drivers' behavior before intervention, which mitigates abrupt takeovers, leading to smooth transitions. We evaluated Diffusion-SAFE in both simulation (CarRacing-v0) and real-world (ROS-based race car), measuring human-driving similarity, safety, and computational efficiency. Results demonstrate a 98.5\\% successful handover rate, highlighting the framework's effectiveness in progressively correcting human actions and continuously sampling optimal robot actions."
  },
  {
    "title": "Determining Absence of Unreasonable Risk: Approval Guidelines for an Automated Driving System Release",
    "url": "http://arxiv.org/abs/2505.09880v1",
    "arxiv_id": "2505.09880v1",
    "authors": [
      "Francesca Favaro",
      "Scott Schnelle",
      "Laura Fraade-Blanar",
      "Trent Victor",
      "Mauricio Pe\u00f1a",
      "Nick Webb",
      "Holland Broce",
      "Craig Paterson",
      "Dan Smith"
    ],
    "published": "2025-05-15T00:52:09+00:00",
    "summary": "This paper provides an overview of how the determination of absence of unreasonable risk can be operationalized. It complements previous theoretical work published by existing developers of Automated Driving Systems (ADS) on the overall engineering practices and methodologies for readiness determination. Readiness determination is, at its core, a risk assessment process. It is aimed at evaluating the residual risk associated with the deployment of a new software release candidate. The paper proposes methodological criteria to ground the readiness review process for an ADS release. While informed by Waymo's experience in this domain, the criteria presented are agnostic of any specific ADS technological solution and/or architectural choice, to support broad implementation by others in the industry. The paper continues with a discussion on governance and decision-making toward approval of a new software release candidate for the ADS. The implementation of the presented criteria requires the existence of appropriate safety management practices in addition to many other cultural, procedural, and operational considerations. As such, the paper is concluded by a statement of limitations for those wishing to replicate part or all of its content."
  },
  {
    "title": "Electrodermal Insights into Stress Dynamics of AR-Assisted Safety Warnings in Virtual Roadway Work Zone Environments",
    "url": "http://arxiv.org/abs/2505.09867v1",
    "arxiv_id": "2505.09867v1",
    "authors": [
      "Fatemeh Banani Ardecani",
      "Omidreza Shoghli"
    ],
    "published": "2025-05-15T00:05:51+00:00",
    "summary": "This study examines stress levels in roadway workers utilizing AR-assisted multi-sensory warning systems under varying work intensities. A high-fidelity Virtual Reality environment was used to replicate real-world scenarios, allowing safe exploration of high-risk situations while focusing on the physiological impacts of work conditions. Wearable sensors were used to continuously and non-invasively collect physiological data, including electrodermal activity to monitor stress responses. Analysis of data from 18 participants revealed notable differences in EDR between light- and medium-intensity activities, reflecting variations in autonomic nervous system activity under stress. Also, a feature importance analysis revealed that peak and central tendency metrics of EDR were robust indicators of physiological responses, between light- and medium-intensity activities. The findings emphasize the relationship between AR-enabled warnings, work intensity, and worker stress, offering an approach to active stress monitoring and improved safety practices. By leveraging real-time physiological insights, this methodology has the potential to support better stress management and the development of more effective safety warning systems for roadway work zones. This research also provides valuable guidance for designing interventions to enhance worker safety, productivity, and well-being in high-risk settings."
  },
  {
    "title": "EdgeAI Drone for Autonomous Construction Site Demonstrator",
    "url": "http://arxiv.org/abs/2505.09837v1",
    "arxiv_id": "2505.09837v1",
    "authors": [
      "Emre Girgin",
      "Arda Taha Candan",
      "Co\u015fkun An\u0131l Zaman"
    ],
    "published": "2025-05-14T22:34:26+00:00",
    "summary": "The fields of autonomous systems and robotics are receiving considerable attention in civil applications such as construction, logistics, and firefighting. Nevertheless, the widespread adoption of these technologies is hindered by the necessity for robust processing units to run AI models. Edge-AI solutions offer considerable promise, enabling low-power, cost-effective robotics that can automate civil services, improve safety, and enhance sustainability. This paper presents a novel Edge-AI-enabled drone-based surveillance system for autonomous multi-robot operations at construction sites. Our system integrates a lightweight MCU-based object detection model within a custom-built UAV platform and a 5G-enabled multi-agent coordination infrastructure. We specifically target the real-time obstacle detection and dynamic path planning problem in construction environments, providing a comprehensive dataset specifically created for MCU-based edge applications. Field experiments demonstrate practical viability and identify optimal operational parameters, highlighting our approach's scalability and computational efficiency advantages compared to existing UAV solutions. The present and future roles of autonomous vehicles on construction sites are also discussed, as well as the effectiveness of edge-AI solutions. We share our dataset publicly at github.com/egirgin/storaige-b950"
  },
  {
    "title": "Adversarial Attack on Large Language Models using Exponentiated Gradient Descent",
    "url": "http://arxiv.org/abs/2505.09820v1",
    "arxiv_id": "2505.09820v1",
    "authors": [
      "Sajib Biswas",
      "Mao Nishino",
      "Samuel Jacob Chacko",
      "Xiuwen Liu"
    ],
    "published": "2025-05-14T21:50:46+00:00",
    "summary": "As Large Language Models (LLMs) are widely used, understanding them systematically is key to improving their safety and realizing their full potential. Although many models are aligned using techniques such as reinforcement learning from human feedback (RLHF), they are still vulnerable to jailbreaking attacks. Some of the existing adversarial attack methods search for discrete tokens that may jailbreak a target model while others try to optimize the continuous space represented by the tokens of the model's vocabulary. While techniques based on the discrete space may prove to be inefficient, optimization of continuous token embeddings requires projections to produce discrete tokens, which might render them ineffective. To fully utilize the constraints and the structures of the space, we develop an intrinsic optimization technique using exponentiated gradient descent with the Bregman projection method to ensure that the optimized one-hot encoding always stays within the probability simplex. We prove the convergence of the technique and implement an efficient algorithm that is effective in jailbreaking several widely used LLMs. We demonstrate the efficacy of the proposed technique using five open-source LLMs on four openly available datasets. The results show that the technique achieves a higher success rate with great efficiency compared to three other state-of-the-art jailbreaking techniques. The source code for our implementation is available at: https://github.com/sbamit/Exponentiated-Gradient-Descent-LLM-Attack"
  },
  {
    "title": "Contextual Phenotyping of Pediatric Sepsis Cohort Using Large Language Models",
    "url": "http://arxiv.org/abs/2505.09805v1",
    "arxiv_id": "2505.09805v1",
    "authors": [
      "Aditya Nagori",
      "Ayush Gautam",
      "Matthew O. Wiens",
      "Vuong Nguyen",
      "Nathan Kenya Mugisha",
      "Jerome Kabakyenga",
      "Niranjan Kissoon",
      "John Mark Ansermino",
      "Rishikesan Kamaleswaran"
    ],
    "published": "2025-05-14T21:05:40+00:00",
    "summary": "Clustering patient subgroups is essential for personalized care and efficient resource use. Traditional clustering methods struggle with high-dimensional, heterogeneous healthcare data and lack contextual understanding. This study evaluates Large Language Model (LLM) based clustering against classical methods using a pediatric sepsis dataset from a low-income country (LIC), containing 2,686 records with 28 numerical and 119 categorical variables. Patient records were serialized into text with and without a clustering objective. Embeddings were generated using quantized LLAMA 3.1 8B, DeepSeek-R1-Distill-Llama-8B with low-rank adaptation(LoRA), and Stella-En-400M-V5 models. K-means clustering was applied to these embeddings. Classical comparisons included K-Medoids clustering on UMAP and FAMD-reduced mixed data. Silhouette scores and statistical tests evaluated cluster quality and distinctiveness. Stella-En-400M-V5 achieved the highest Silhouette Score (0.86). LLAMA 3.1 8B with the clustering objective performed better with higher number of clusters, identifying subgroups with distinct nutritional, clinical, and socioeconomic profiles. LLM-based methods outperformed classical techniques by capturing richer context and prioritizing key features. These results highlight potential of LLMs for contextual phenotyping and informed decision-making in resource-limited settings."
  },
  {
    "title": "Risk-Aware Safe Reinforcement Learning for Control of Stochastic Linear Systems",
    "url": "http://arxiv.org/abs/2505.09734v1",
    "arxiv_id": "2505.09734v1",
    "authors": [
      "Babak Esmaeili",
      "Nariman Niknejad",
      "Hamidreza Modares"
    ],
    "published": "2025-05-14T18:49:32+00:00",
    "summary": "This paper presents a risk-aware safe reinforcement learning (RL) control design for stochastic discrete-time linear systems. Rather than using a safety certifier to myopically intervene with the RL controller, a risk-informed safe controller is also learned besides the RL controller, and the RL and safe controllers are combined together. Several advantages come along with this approach: 1) High-confidence safety can be certified without relying on a high-fidelity system model and using limited data available, 2) Myopic interventions and convergence to an undesired equilibrium can be avoided by deciding on the contribution of two stabilizing controllers, and 3) highly efficient and computationally tractable solutions can be provided by optimizing over a scalar decision variable and linear programming polyhedral sets. To learn safe controllers with a large invariant set, piecewise affine controllers are learned instead of linear controllers. To this end, the closed-loop system is first represented using collected data, a decision variable, and noise. The effect of the decision variable on the variance of the safe violation of the closed-loop system is formalized. The decision variable is then designed such that the probability of safety violation for the learned closed-loop system is minimized. It is shown that this control-oriented approach reduces the data requirements and can also reduce the variance of safety violations. Finally, to integrate the safe and RL controllers, a new data-driven interpolation technique is introduced. This method aims to maintain the RL agent's optimal implementation while ensuring its safety within environments characterized by noise. The study concludes with a simulation example that serves to validate the theoretical results."
  },
  {
    "title": "Adversarial Suffix Filtering: a Defense Pipeline for LLMs",
    "url": "http://arxiv.org/abs/2505.09602v1",
    "arxiv_id": "2505.09602v1",
    "authors": [
      "David Khachaturov",
      "Robert Mullins"
    ],
    "published": "2025-05-14T17:52:10+00:00",
    "summary": "Large Language Models (LLMs) are increasingly embedded in autonomous systems and public-facing environments, yet they remain susceptible to jailbreak vulnerabilities that may undermine their security and trustworthiness. Adversarial suffixes are considered to be the current state-of-the-art jailbreak, consistently outperforming simpler methods and frequently succeeding even in black-box settings. Existing defenses rely on access to the internal architecture of models limiting diverse deployment, increase memory and computation footprints dramatically, or can be bypassed with simple prompt engineering methods. We introduce $\\textbf{Adversarial Suffix Filtering}$ (ASF), a lightweight novel model-agnostic defensive pipeline designed to protect LLMs against adversarial suffix attacks. ASF functions as an input preprocessor and sanitizer that detects and filters adversarially crafted suffixes in prompts, effectively neutralizing malicious injections. We demonstrate that ASF provides comprehensive defense capabilities across both black-box and white-box attack settings, reducing the attack efficacy of state-of-the-art adversarial suffix generation methods to below 4%, while only minimally affecting the target model's capabilities in non-adversarial scenarios."
  },
  {
    "title": "How Hungry is AI? Benchmarking Energy, Water, and Carbon Footprint of LLM Inference",
    "url": "http://arxiv.org/abs/2505.09598v1",
    "arxiv_id": "2505.09598v1",
    "authors": [
      "Nidhal Jegham",
      "Marwen Abdelatti",
      "Lassad Elmoubarki",
      "Abdeltawab Hendawi"
    ],
    "published": "2025-05-14T17:47:00+00:00",
    "summary": "As large language models (LLMs) spread across industries, understanding their environmental footprint at the inference level is no longer optional; it is essential. However, most existing studies exclude proprietary models, overlook infrastructural variability and overhead, or focus solely on training, even as inference increasingly dominates AI's environmental impact. To bridge this gap, this paper introduces a novel infrastructure-aware benchmarking framework for quantifying the environmental footprint of LLM inference across 30 state-of-the-art models as deployed in commercial data centers. Our framework combines public API performance data with region-specific environmental multipliers and statistical inference of hardware configurations. We additionally utilize cross-efficiency Data Envelopment Analysis (DEA) to rank models by performance relative to environmental cost. Our results show that o3 and DeepSeek-R1 emerge as the most energy-intensive models, consuming over 33 Wh per long prompt, more than 70 times the consumption of GPT-4.1 nano, and that Claude-3.7 Sonnet ranks highest in eco-efficiency. While a single short GPT-4o query consumes 0.43 Wh, scaling this to 700 million queries/day results in substantial annual environmental impacts. These include electricity use comparable to 35,000 U.S. homes, freshwater evaporation matching the annual drinking needs of 1.2 million people, and carbon emissions requiring a Chicago-sized forest to offset. These findings illustrate a growing paradox: although individual queries are efficient, their global scale drives disproportionate resource consumption. Our study provides a standardized, empirically grounded methodology for benchmarking the sustainability of LLM deployments, laying a foundation for future environmental accountability in AI development and sustainability standards."
  },
  {
    "title": "Tropical Fermat-Weber Points over Bergman Fans",
    "url": "http://arxiv.org/abs/2505.09584v1",
    "arxiv_id": "2505.09584v1",
    "authors": [
      "Shelby Cox",
      "John Sabol",
      "Roan Talbut",
      "Ruriko Yoshida"
    ],
    "published": "2025-05-14T17:33:55+00:00",
    "summary": "Given a finite set $[p]:= \\{1, \\ldots , p\\}$, it is well known that the space of all ultrametrics on $[p]$ is the Bergman fan associated to the matroid underlying the complete graph of the vertex set $[p]$. Lin et al.~showed that the set of tropical Fermat-Weber points might not be contained in the space of ultrametrics on $[p]$ even if all input data points are in the space. Here we consider a more general set up and we focus on Fermat-Weber points with respect to the tropical metric over the Bergman fan associated with a matroid with the ground set of $q$ elements. We show that there always exists a Fermat-Weber point in the Bergman fan for data in the Bergman fan and we describe explicitly the set of all Fermat-Weber points in the Bergman fan. Then we introduce the natural extension of the safety radius introduced by Atteson to the set of Fermat-Weber points in the Bergman fan of a matroid."
  },
  {
    "title": "Tales of the 2025 Los Angeles Fire: Hotwash for Public Health Concerns in Reddit via LLM-Enhanced Topic Modeling",
    "url": "http://arxiv.org/abs/2505.09665v1",
    "arxiv_id": "2505.09665v1",
    "authors": [
      "Sulong Zhou",
      "Qunying Huang",
      "Shaoheng Zhou",
      "Yun Hang",
      "Xinyue Ye",
      "Aodong Mei",
      "Kathryn Phung",
      "Yuning Ye",
      "Uma Govindswamy",
      "Zehan Li"
    ],
    "published": "2025-05-14T16:31:08+00:00",
    "summary": "Wildfires have become increasingly frequent, irregular, and severe in recent years. Understanding how affected populations perceive and respond during wildfire crises is critical for timely and empathetic disaster response. Social media platforms offer a crowd-sourced channel to capture evolving public discourse, providing hyperlocal information and insight into public sentiment. This study analyzes Reddit discourse during the 2025 Los Angeles wildfires, spanning from the onset of the disaster to full containment. We collect 385 posts and 114,879 comments related to the Palisades and Eaton fires. We adopt topic modeling methods to identify the latent topics, enhanced by large language models (LLMs) and human-in-the-loop (HITL) refinement. Furthermore, we develop a hierarchical framework to categorize latent topics, consisting of two main categories, Situational Awareness (SA) and Crisis Narratives (CN). The volume of SA category closely aligns with real-world fire progressions, peaking within the first 2-5 days as the fires reach the maximum extent. The most frequent co-occurring category set of public health and safety, loss and damage, and emergency resources expands on a wide range of health-related latent topics, including environmental health, occupational health, and one health. Grief signals and mental health risks consistently accounted for 60 percentage and 40 percentage of CN instances, respectively, with the highest total volume occurring at night. This study contributes the first annotated social media dataset on the 2025 LA fires, and introduces a scalable multi-layer framework that leverages topic modeling for crisis discourse analysis. By identifying persistent public health concerns, our results can inform more empathetic and adaptive strategies for disaster response, public health communication, and future research in comparable climate-related disaster events."
  },
  {
    "title": "Conformal Bounds on Full-Reference Image Quality for Imaging Inverse Problems",
    "url": "http://arxiv.org/abs/2505.09528v1",
    "arxiv_id": "2505.09528v1",
    "authors": [
      "Jeffrey Wen",
      "Rizwan Ahmad",
      "Philip Schniter"
    ],
    "published": "2025-05-14T16:23:26+00:00",
    "summary": "In imaging inverse problems, we would like to know how close the recovered image is to the true image in terms of full-reference image quality (FRIQ) metrics like PSNR, SSIM, LPIPS, etc. This is especially important in safety-critical applications like medical imaging, where knowing that, say, the SSIM was poor could potentially avoid a costly misdiagnosis. But since we don't know the true image, computing FRIQ is non-trivial. In this work, we combine conformal prediction with approximate posterior sampling to construct bounds on FRIQ that are guaranteed to hold up to a user-specified error probability. We demonstrate our approach on image denoising and accelerated magnetic resonance imaging (MRI) problems. Code is available at https://github.com/jwen307/quality_uq."
  },
  {
    "title": "Evaluating GPT- and Reasoning-based Large Language Models on Physics Olympiad Problems: Surpassing Human Performance and Implications for Educational Assessment",
    "url": "http://arxiv.org/abs/2505.09438v1",
    "arxiv_id": "2505.09438v1",
    "authors": [
      "Paul Tschisgale",
      "Holger Maus",
      "Fabian Kieser",
      "Ben Kroehs",
      "Stefan Petersen",
      "Peter Wulff"
    ],
    "published": "2025-05-14T14:46:32+00:00",
    "summary": "Large language models (LLMs) are now widely accessible, reaching learners at all educational levels. This development has raised concerns that their use may circumvent essential learning processes and compromise the integrity of established assessment formats. In physics education, where problem solving plays a central role in instruction and assessment, it is therefore essential to understand the physics-specific problem-solving capabilities of LLMs. Such understanding is key to informing responsible and pedagogically sound approaches to integrating LLMs into instruction and assessment. This study therefore compares the problem-solving performance of a general-purpose LLM (GPT-4o, using varying prompting techniques) and a reasoning-optimized model (o1-preview) with that of participants of the German Physics Olympiad, based on a set of well-defined Olympiad problems. In addition to evaluating the correctness of the generated solutions, the study analyzes characteristic strengths and limitations of LLM-generated solutions. The findings of this study indicate that both tested LLMs (GPT-4o and o1-preview) demonstrate advanced problem-solving capabilities on Olympiad-type physics problems, on average outperforming the human participants. Prompting techniques had little effect on GPT-4o's performance, while o1-preview almost consistently outperformed both GPT-4o and the human benchmark. Based on these findings, the study discusses implications for the design of summative and formative assessment in physics education, including how to uphold assessment integrity and support students in critically engaging with LLMs."
  },
  {
    "title": "SafePath: Conformal Prediction for Safe LLM-Based Autonomous Navigation",
    "url": "http://arxiv.org/abs/2505.09427v1",
    "arxiv_id": "2505.09427v1",
    "authors": [
      "Achref Doula",
      "Max M\u00fchl\u00e4user",
      "Alejandro Sanchez Guinea"
    ],
    "published": "2025-05-14T14:28:24+00:00",
    "summary": "Large Language Models (LLMs) show growing promise in autonomous driving by reasoning over complex traffic scenarios to generate path plans. However, their tendencies toward overconfidence, and hallucinations raise critical safety concerns. We introduce SafePath, a modular framework that augments LLM-based path planning with formal safety guarantees using conformal prediction. SafePath operates in three stages. In the first stage, we use an LLM that generates a set of diverse candidate paths, exploring possible trajectories based on agent behaviors and environmental cues. In the second stage, SafePath filters out high-risk trajectories while guaranteeing that at least one safe option is included with a user-defined probability, through a multiple-choice question-answering formulation that integrates conformal prediction. In the final stage, our approach selects the path with the lowest expected collision risk when uncertainty is low or delegates control to a human when uncertainty is high. We theoretically prove that SafePath guarantees a safe trajectory with a user-defined probability, and we show how its human delegation rate can be tuned to balance autonomy and safety. Extensive experiments on nuScenes and Highway-env show that SafePath reduces planning uncertainty by 77\\% and collision rates by up to 70\\%, demonstrating effectiveness in making LLM-driven path planning more safer."
  },
  {
    "title": "SafePath: Conformal Prediction for Safe LLM-Based Autonomous Navigation",
    "url": "http://arxiv.org/abs/2505.09427v2",
    "arxiv_id": "2505.09427v2",
    "authors": [
      "Achref Doula",
      "Max M\u00fchlh\u00e4user",
      "Alejandro Sanchez Guinea"
    ],
    "published": "2025-05-14T14:28:24+00:00",
    "summary": "Large Language Models (LLMs) show growing promise in autonomous driving by reasoning over complex traffic scenarios to generate path plans. However, their tendencies toward overconfidence, and hallucinations raise critical safety concerns. We introduce SafePath, a modular framework that augments LLM-based path planning with formal safety guarantees using conformal prediction. SafePath operates in three stages. In the first stage, we use an LLM that generates a set of diverse candidate paths, exploring possible trajectories based on agent behaviors and environmental cues. In the second stage, SafePath filters out high-risk trajectories while guaranteeing that at least one safe option is included with a user-defined probability, through a multiple-choice question-answering formulation that integrates conformal prediction. In the final stage, our approach selects the path with the lowest expected collision risk when uncertainty is low or delegates control to a human when uncertainty is high. We theoretically prove that SafePath guarantees a safe trajectory with a user-defined probability, and we show how its human delegation rate can be tuned to balance autonomy and safety. Extensive experiments on nuScenes and Highway-env show that SafePath reduces planning uncertainty by 77\\% and collision rates by up to 70\\%, demonstrating effectiveness in making LLM-driven path planning more safer."
  },
  {
    "title": "Coordinated Multi-Valve Disturbance-Rejection Pressure Control for High-Altitude Test Stands via Exterior Penalty Functions",
    "url": "http://arxiv.org/abs/2505.09352v1",
    "arxiv_id": "2505.09352v1",
    "authors": [
      "Zhang Louyue",
      "Li Xin",
      "Zhai Chao",
      "Shi Duoqi",
      "Zhang Hehong",
      "Dan Zhihong",
      "Wang Xi",
      "Liu Jiashuai",
      "Xiao Gaoxi"
    ],
    "published": "2025-05-14T13:00:13+00:00",
    "summary": "High altitude simulation test benches for aero engines employ multi chamber, multi valve intake systems that demand effective decoupling and strong disturbance rejection during transient tests. This paper proposes a coordinated active disturbance rejection control (ADRC) scheme based on an external penalty function. The chamber pressure safety limit is reformulated as an inequality constrained optimization problem, and an exponential penalty together with a gradient based algorithm is designed for dynamic constraint relaxation, with global convergence rigorously proven. A coordination term is then integrated into a distributed ADRC framework to yield a multi valve coordinated LADRC controller, whose asymptotic stability is established via Lyapunov theory. Hardware in the loop simulations using MATLAB/Simulink and a PLC demonstrate that, under $\\pm$3 kPa pressure constraints, chamber V2's maximum error is 1.782 kPa (77.1\\% lower than PID control), and under a 180 kg/s^2 flow rate disturbance, valve oscillations decrease from $\\pm$27\\% to $\\pm$5\\% (an 81.5\\% reduction). These results confirm the proposed method's superior disturbance rejection and decoupling performance."
  },
  {
    "title": "Safe Primal-Dual Optimization with a Single Smooth Constraint",
    "url": "http://arxiv.org/abs/2505.09349v1",
    "arxiv_id": "2505.09349v1",
    "authors": [
      "Ilnura Usmanova",
      "Kfir Yehuda Levy"
    ],
    "published": "2025-05-14T12:54:29+00:00",
    "summary": "This paper addresses the problem of safe optimization under a single smooth constraint, a scenario that arises in diverse real-world applications such as robotics and autonomous navigation. The objective of safe optimization is to solve a black-box minimization problem while strictly adhering to a safety constraint throughout the learning process. Existing methods often suffer from high sample complexity due to their noise sensitivity or poor scalability with number of dimensions, limiting their applicability. We propose a novel primal-dual optimization method that, by carefully adjusting dual step-sizes and constraining primal updates, ensures the safety of both primal and dual sequences throughout the optimization. Our algorithm achieves a convergence rate that significantly surpasses current state-of-the-art techniques. Furthermore, to the best of our knowledge, it is the first primal-dual approach to guarantee safe updates. Simulations corroborate our theoretical findings, demonstrating the practical benefits of our method. We also show how the method can be extended to multiple constraints."
  },
  {
    "title": "Access Controls Will Solve the Dual-Use Dilemma",
    "url": "http://arxiv.org/abs/2505.09341v1",
    "arxiv_id": "2505.09341v1",
    "authors": [
      "Ev\u017een Wybitul"
    ],
    "published": "2025-05-14T12:38:08+00:00",
    "summary": "AI safety systems face a dual-use dilemma. Since the same request can be either harmless or harmful depending on who made it and why, if the system makes decisions based solely on the request's content, it will refuse some legitimate queries and let pass harmful ones. To address this, we propose a conceptual access control framework, based on verified user credentials (such as institutional affiliation) and classifiers that assign model outputs to risk categories (such as advanced virology). The system permits responses only when the user's verified credentials match the category's requirements. For implementation of the model output classifiers, we introduce a theoretical approach utilizing small, gated expert modules integrated into the generator model, trained with gradient routing, that enable efficient risk detection without the capability gap problems of external monitors. While open questions remain about the verification mechanisms, risk categories, and the technical implementation, our framework makes the first step toward enabling granular governance of AI capabilities: verified users gain access to specialized knowledge without arbitrary restrictions, while adversaries are blocked from it. This contextual approach reconciles model utility with robust safety, addressing the dual-use dilemma."
  },
  {
    "title": "Privacy-Preserving Runtime Verification",
    "url": "http://arxiv.org/abs/2505.09276v1",
    "arxiv_id": "2505.09276v1",
    "authors": [
      "Thomas A. Henzinger",
      "Mahyar Karimi",
      "K. S. Thejaswini"
    ],
    "published": "2025-05-14T10:49:07+00:00",
    "summary": "Runtime verification offers scalable solutions to improve the safety and reliability of systems. However, systems that require verification or monitoring by a third party to ensure compliance with a specification might contain sensitive information, causing privacy concerns when usual runtime verification approaches are used. Privacy is compromised if protected information about the system, or sensitive data that is processed by the system, is revealed. In addition, revealing the specification being monitored may undermine the essence of third-party verification.   In this work, we propose two novel protocols for the privacy-preserving runtime verification of systems against formal sequential specifications. In our first protocol, the monitor verifies whether the system satisfies the specification without learning anything else, though both parties are aware of the specification. Our second protocol ensures that the system remains oblivious to the monitored specification, while the monitor learns only whether the system satisfies the specification and nothing more. Our protocols adapt and improve existing techniques used in cryptography, and more specifically, multi-party computation.   The sequential specification defines the observation step of the monitor, whose granularity depends on the situation (e.g., banks may be monitored on a daily basis). Our protocols exchange a single message per observation step, after an initialisation phase. This design minimises communication overhead, enabling relatively lightweight privacy-preserving monitoring. We implement our approach for monitoring specifications described by register automata and evaluate it experimentally."
  },
  {
    "title": "On verification and constraint generation for families of similar hybrid automata",
    "url": "http://arxiv.org/abs/2505.09244v1",
    "arxiv_id": "2505.09244v1",
    "authors": [
      "Viorica Sofronie-Stokkermans",
      "Philipp Marohn"
    ],
    "published": "2025-05-14T09:35:03+00:00",
    "summary": "In this paper we give an overview of results on the analysis of parametric linear hybrid automata, and of systems of similar linear hybrid automata: We present possibilities of describing systems with a parametric (i.e. not explicitly specified) number of similar components which can be connected to other systems, such that some parts in the description might be underspecified (i.e. parametric). We consider global safety properties for such systems, expressed by universally quantified formulae, using quantification over variables ranging over the component systems. We analyze possibilities of using methods for hierarchical reasoning and symbol elimination for determining relationships on (some of) the parameters used in the description of these systems under which the global safety properties are guaranteed to be inductive invariants. We discuss an implementation and illustrate its use on several examples."
  },
  {
    "title": "Great Short History of Microbiology Development as a Science",
    "url": "http://arxiv.org/abs/2505.09658v1",
    "arxiv_id": "2505.09658v1",
    "authors": [
      "Daniil S. Gerassimov"
    ],
    "published": "2025-05-14T06:11:55+00:00",
    "summary": "The study of microorganisms, or microbiology, has demonstrated significant development since its inception and is currently a key field of biological sciences that has a huge impact on modern society and scientific research. Over the centuries, this discipline has undergone significant changes, shaping our understanding of infectious diseases and food safety. Starting from the simplest observations of microscopic organisms such as bacteria, viruses, fungi and protozoa, and ending with modern molecular and genomic research methods. This article describes a brief historical path of microbiology development. The heuristic, morphological, physiological, immunological, and molecular genetic stages are the main periods into which the development of this science is traditionally divided, despite the lack of full-fledged and precise boundaries between them."
  },
  {
    "title": "OpenLKA: An Open Dataset of Lane Keeping Assist from Recent Car Models under Real-world Driving Conditions",
    "url": "http://arxiv.org/abs/2505.09092v1",
    "arxiv_id": "2505.09092v1",
    "authors": [
      "Yuhang Wang",
      "Abdulaziz Alhuraish",
      "Shengming Yuan",
      "Hao Zhou"
    ],
    "published": "2025-05-14T02:53:50+00:00",
    "summary": "Lane Keeping Assist (LKA) is widely adopted in modern vehicles, yet its real-world performance remains underexplored due to proprietary systems and limited data access. This paper presents OpenLKA, the first open, large-scale dataset for LKA evaluation and improvement. It includes 400 hours of driving data from 50+ production vehicle models, collected through extensive road testing in Tampa, Florida and global contributions from the Comma.ai driving community. The dataset spans a wide range of challenging scenarios, including complex road geometries, degraded lane markings, adverse weather, lighting conditions and surrounding traffic. The dataset is multimodal, comprising: i) full CAN bus streams, decoded using custom reverse-engineered DBC files to extract key LKA events (e.g., system disengagements, lane detection failures); ii) synchronized high-resolution dash-cam video; iii) real-time outputs from Openpilot, providing accurate estimates of road curvature and lane positioning; iv) enhanced scene annotations generated by Vision Language Models, describing lane visibility, pavement quality, weather, lighting, and traffic conditions. By integrating vehicle-internal signals with high-fidelity perception and rich semantic context, OpenLKA provides a comprehensive platform for benchmarking the real-world performance of production LKA systems, identifying safety-critical operational scenarios, and assessing the readiness of current road infrastructure for autonomous driving. The dataset is publicly available at: https://github.com/OpenLKA/OpenLKA."
  },
  {
    "title": "Reach-Avoid-Stabilize Using Admissible Control Sets",
    "url": "http://arxiv.org/abs/2505.09058v1",
    "arxiv_id": "2505.09058v1",
    "authors": [
      "Zheng Gong",
      "Boyang Li",
      "Sylvia Herbert"
    ],
    "published": "2025-05-14T01:42:59+00:00",
    "summary": "Hamilton-Jacobi Reachability (HJR) analysis has been successfully used in many robotics and control tasks, and is especially effective in computing reach-avoid sets and control laws that enable an agent to reach a goal while satisfying state constraints. However, the original HJR formulation provides no guarantees of safety after a) the prescribed time horizon, or b) goal satisfaction. The reach-avoid-stabilize (RAS) problem has therefore gained a lot of focus: find the set of initial states (the RAS set), such that the trajectory can reach the target, and stabilize to some point of interest (POI) while avoiding obstacles. Solving RAS problems using HJR usually requires defining a new value function, whose zero sub-level set is the RAS set. The existing methods do not consider the problem when there are a series of targets to reach and/or obstacles to avoid. We propose a method that uses the idea of admissible control sets; we guarantee that the system will reach each target while avoiding obstacles as prescribed by the given time series. Moreover, we guarantee that the trajectory ultimately stabilizes to the POI. The proposed method provides an under-approximation of the RAS set, guaranteeing safety. Numerical examples are provided to validate the theory."
  },
  {
    "title": "FocusE: A semantic extension of FocusST",
    "url": "http://arxiv.org/abs/2505.09032v1",
    "arxiv_id": "2505.09032v1",
    "authors": [
      "Maria Spichkova"
    ],
    "published": "2025-05-14T00:04:23+00:00",
    "summary": "To analyse and verify the safety and security properties of interactive systems, a formal specification might be necessary. There are many types of formal languages and frameworks. The decision regarding what type of formal specification should be applied in each particular case depends on many factors. One of the approaches to specify interactive systems formally is to present them as a composition of components processing data and control streams. In this short paper, we present FocusE, a formal approach for modelling event-based streams. The proposed approach is based on a formal language FocusST, and can be seen as its semantic extension."
  },
  {
    "title": "A suite of LMs comprehend puzzle statements as well as humans",
    "url": "http://arxiv.org/abs/2505.08996v1",
    "arxiv_id": "2505.08996v1",
    "authors": [
      "Adele E Goldberg",
      "Supantho Rakshit",
      "Jennifer Hu",
      "Kyle Mahowald"
    ],
    "published": "2025-05-13T22:18:51+00:00",
    "summary": "Recent claims suggest that large language models (LMs) underperform humans in comprehending minimally complex English statements (Dentella et al., 2024). Here, we revisit those findings and argue that human performance was overestimated, while LLM abilities were underestimated. Using the same stimuli, we report a preregistered study comparing human responses in two conditions: one allowed rereading (replicating the original study), and one that restricted rereading (a more naturalistic comprehension test). Human accuracy dropped significantly when rereading was restricted (73%), falling below that of Falcon-180B-Chat (76%) and GPT-4 (81%). The newer GPT-o1 model achieves perfect accuracy. Results further show that both humans and models are disproportionately challenged by queries involving potentially reciprocal actions (e.g., kissing), suggesting shared pragmatic sensitivities rather than model-specific deficits. Additional analyses using Llama-2-70B log probabilities, a recoding of open-ended model responses, and grammaticality ratings of other sentences reveal systematic underestimation of model performance. We find that GPT-4o can align with either naive or expert grammaticality judgments, depending on prompt framing. These findings underscore the need for more careful experimental design and coding practices in LLM evaluation, and they challenge the assumption that current models are inherently weaker than humans at language comprehension."
  },
  {
    "title": "Performance Gains of LLMs With Humans in a World of LLMs Versus Humans",
    "url": "http://arxiv.org/abs/2505.08902v1",
    "arxiv_id": "2505.08902v1",
    "authors": [
      "Lucas McCullum",
      "Pelagie Ami Agassi",
      "Leo Anthony Celi",
      "Daniel K. Ebner",
      "Chrystinne Oliveira Fernandes",
      "Rachel S. Hicklen",
      "Mkliwa Koumbia",
      "Lisa Soleymani Lehmann",
      "David Restrepo"
    ],
    "published": "2025-05-13T18:44:22+00:00",
    "summary": "Currently, a considerable research effort is devoted to comparing LLMs to a group of human experts, where the term \"expert\" is often ill-defined or variable, at best, in a state of constantly updating LLM releases. Without proper safeguards in place, LLMs will threaten to cause harm to the established structure of safe delivery of patient care which has been carefully developed throughout history to keep the safety of the patient at the forefront. A key driver of LLM innovation is founded on community research efforts which, if continuing to operate under \"humans versus LLMs\" principles, will expedite this trend. Therefore, research efforts moving forward must focus on effectively characterizing the safe use of LLMs in clinical settings that persist across the rapid development of novel LLM models. In this communication, we demonstrate that rather than comparing LLMs to humans, there is a need to develop strategies enabling efficient work of humans with LLMs in an almost symbiotic manner."
  },
  {
    "title": "Deep reinforcement learning-based longitudinal control strategy for automated vehicles at signalised intersections",
    "url": "http://arxiv.org/abs/2505.08896v1",
    "arxiv_id": "2505.08896v1",
    "authors": [
      "Pankaj Kumar",
      "Aditya Mishra",
      "Pranamesh Chakraborty",
      "Subrahmanya Swamy Peruru"
    ],
    "published": "2025-05-13T18:38:42+00:00",
    "summary": "Developing an autonomous vehicle control strategy for signalised intersections (SI) is one of the challenging tasks due to its inherently complex decision-making process. This study proposes a Deep Reinforcement Learning (DRL) based longitudinal vehicle control strategy at SI. A comprehensive reward function has been formulated with a particular focus on (i) distance headway-based efficiency reward, (ii) decision-making criteria during amber light, and (iii) asymmetric acceleration/ deceleration response, along with the traditional safety and comfort criteria. This reward function has been incorporated with two popular DRL algorithms, Deep Deterministic Policy Gradient (DDPG) and Soft-Actor Critic (SAC), which can handle the continuous action space of acceleration/deceleration. The proposed models have been trained on the combination of real-world leader vehicle (LV) trajectories and simulated trajectories generated using the Ornstein-Uhlenbeck (OU) process. The overall performance of the proposed models has been tested using Cumulative Distribution Function (CDF) plots and compared with the real-world trajectory data. The results show that the RL models successfully maintain lower distance headway (i.e., higher efficiency) and jerk compared to human-driven vehicles without compromising safety. Further, to assess the robustness of the proposed models, we evaluated the model performance on diverse safety-critical scenarios, in terms of car-following and traffic signal compliance. Both DDPG and SAC models successfully handled the critical scenarios, while the DDPG model showed smoother action profiles compared to the SAC model. Overall, the results confirm that DRL-based longitudinal vehicle control strategy at SI can help to improve traffic safety, efficiency, and comfort."
  },
  {
    "title": "Intelligent Road Anomaly Detection with Real-time Notification System for Enhanced Road Safety",
    "url": "http://arxiv.org/abs/2505.08882v1",
    "arxiv_id": "2505.08882v1",
    "authors": [
      "Ali Almakhluk",
      "Uthman Baroudi",
      "Yasser El-Alfy"
    ],
    "published": "2025-05-13T18:12:03+00:00",
    "summary": "This study aims to improve transportation safety, especially traffic safety. Road damage anomalies such as potholes and cracks have emerged as a significant and recurring cause for accidents. To tackle this problem and improve road safety, a comprehensive system has been developed to detect potholes, cracks (e.g. alligator, transverse, longitudinal), classify their sizes, and transmit this data to the cloud for appropriate action by authorities. The system also broadcasts warning signals to nearby vehicles warning them if a severe anomaly is detected on the road. Moreover, the system can count road anomalies in real-time. It is emulated through the utilization of Raspberry Pi, a camera module, deep learning model, laptop, and cloud service. Deploying this innovative solution aims to proactively enhance road safety by notifying relevant authorities and drivers about the presence of potholes and cracks to take actions, thereby mitigating potential accidents arising from this prevalent road hazard leading to safer road conditions for the whole community."
  },
  {
    "title": "Generative AI for Autonomous Driving: Frontiers and Opportunities",
    "url": "http://arxiv.org/abs/2505.08854v1",
    "arxiv_id": "2505.08854v1",
    "authors": [
      "Yuping Wang",
      "Shuo Xing",
      "Cui Can",
      "Renjie Li",
      "Hongyuan Hua",
      "Kexin Tian",
      "Zhaobin Mo",
      "Xiangbo Gao",
      "Keshu Wu",
      "Sulong Zhou",
      "Hengxu You",
      "Juntong Peng",
      "Junge Zhang",
      "Zehao Wang",
      "Rui Song",
      "Mingxuan Yan",
      "Walter Zimmer",
      "Xingcheng Zhou",
      "Peiran Li",
      "Zhaohan Lu",
      "Chia-Ju Chen",
      "Yue Huang",
      "Ryan A. Rossi",
      "Lichao Sun",
      "Hongkai Yu",
      "Zhiwen Fan",
      "Frank Hao Yang",
      "Yuhao Kang",
      "Ross Greer",
      "Chenxi Liu",
      "Eun Hak Lee",
      "Xuan Di",
      "Xinyue Ye",
      "Liu Ren",
      "Alois Knoll",
      "Xiaopeng Li",
      "Shuiwang Ji",
      "Masayoshi Tomizuka",
      "Marco Pavone",
      "Tianbao Yang",
      "Jing Du",
      "Ming-Hsuan Yang",
      "Hua Wei",
      "Ziran Wang",
      "Yang Zhou",
      "Jiachen Li",
      "Zhengzhong Tu"
    ],
    "published": "2025-05-13T17:59:20+00:00",
    "summary": "Generative Artificial Intelligence (GenAI) constitutes a transformative technological wave that reconfigures industries through its unparalleled capabilities for content creation, reasoning, planning, and multimodal understanding. This revolutionary force offers the most promising path yet toward solving one of engineering's grandest challenges: achieving reliable, fully autonomous driving, particularly the pursuit of Level 5 autonomy. This survey delivers a comprehensive and critical synthesis of the emerging role of GenAI across the autonomous driving stack. We begin by distilling the principles and trade-offs of modern generative modeling, encompassing VAEs, GANs, Diffusion Models, and Large Language Models (LLMs). We then map their frontier applications in image, LiDAR, trajectory, occupancy, video generation as well as LLM-guided reasoning and decision making. We categorize practical applications, such as synthetic data workflows, end-to-end driving strategies, high-fidelity digital twin systems, smart transportation networks, and cross-domain transfer to embodied AI. We identify key obstacles and possibilities such as comprehensive generalization across rare cases, evaluation and safety checks, budget-limited implementation, regulatory compliance, ethical concerns, and environmental effects, while proposing research plans across theoretical assurances, trust metrics, transport integration, and socio-technical influence. By unifying these threads, the survey provides a forward-looking reference for researchers, engineers, and policymakers navigating the convergence of generative AI and advanced autonomous mobility. An actively maintained repository of cited works is available at https://github.com/taco-group/GenAI4AD."
  },
  {
    "title": "PCS-UQ: Uncertainty Quantification via the Predictability-Computability-Stability Framework",
    "url": "http://arxiv.org/abs/2505.08784v1",
    "arxiv_id": "2505.08784v1",
    "authors": [
      "Abhineet Agarwal",
      "Michael Xiao",
      "Rebecca Barter",
      "Omer Ronen",
      "Boyu Fan",
      "Bin Yu"
    ],
    "published": "2025-05-13T17:58:16+00:00",
    "summary": "As machine learning (ML) models are increasingly deployed in high-stakes domains, trustworthy uncertainty quantification (UQ) is critical for ensuring the safety and reliability of these models. Traditional UQ methods rely on specifying a true generative model and are not robust to misspecification. On the other hand, conformal inference allows for arbitrary ML models but does not consider model selection, which leads to large interval sizes. We tackle these drawbacks by proposing a UQ method based on the predictability, computability, and stability (PCS) framework for veridical data science proposed by Yu and Kumbier. Specifically, PCS-UQ addresses model selection by using a prediction check to screen out unsuitable models. PCS-UQ then fits these screened algorithms across multiple bootstraps to assess inter-sample variability and algorithmic instability, enabling more reliable uncertainty estimates. Further, we propose a novel calibration scheme that improves local adaptivity of our prediction sets. Experiments across $17$ regression and $6$ classification datasets show that PCS-UQ achieves the desired coverage and reduces width over conformal approaches by $\\approx 20\\%$. Further, our local analysis shows PCS-UQ often achieves target coverage across subgroups while conformal methods fail to do so. For large deep-learning models, we propose computationally efficient approximation schemes that avoid the expensive multiple bootstrap trainings of PCS-UQ. Across three computer vision benchmarks, PCS-UQ reduces prediction set size over conformal methods by $20\\%$. Theoretically, we show a modified PCS-UQ algorithm is a form of split conformal inference and achieves the desired coverage with exchangeable data."
  },
  {
    "title": "HealthBench: Evaluating Large Language Models Towards Improved Human Health",
    "url": "http://arxiv.org/abs/2505.08775v1",
    "arxiv_id": "2505.08775v1",
    "authors": [
      "Rahul K. Arora",
      "Jason Wei",
      "Rebecca Soskin Hicks",
      "Preston Bowman",
      "Joaquin Qui\u00f1onero-Candela",
      "Foivos Tsimpourlas",
      "Michael Sharman",
      "Meghan Shah",
      "Andrea Vallone",
      "Alex Beutel",
      "Johannes Heidecke",
      "Karan Singhal"
    ],
    "published": "2025-05-13T17:53:59+00:00",
    "summary": "We present HealthBench, an open-source benchmark measuring the performance and safety of large language models in healthcare. HealthBench consists of 5,000 multi-turn conversations between a model and an individual user or healthcare professional. Responses are evaluated using conversation-specific rubrics created by 262 physicians. Unlike previous multiple-choice or short-answer benchmarks, HealthBench enables realistic, open-ended evaluation through 48,562 unique rubric criteria spanning several health contexts (e.g., emergencies, transforming clinical data, global health) and behavioral dimensions (e.g., accuracy, instruction following, communication). HealthBench performance over the last two years reflects steady initial progress (compare GPT-3.5 Turbo's 16% to GPT-4o's 32%) and more rapid recent improvements (o3 scores 60%). Smaller models have especially improved: GPT-4.1 nano outperforms GPT-4o and is 25 times cheaper. We additionally release two HealthBench variations: HealthBench Consensus, which includes 34 particularly important dimensions of model behavior validated via physician consensus, and HealthBench Hard, where the current top score is 32%. We hope that HealthBench grounds progress towards model development and applications that benefit human health."
  },
  {
    "title": "DeepMath-Creative: A Benchmark for Evaluating Mathematical Creativity of Large Language Models",
    "url": "http://arxiv.org/abs/2505.08744v1",
    "arxiv_id": "2505.08744v1",
    "authors": [
      "Xiaoyang Chen",
      "Xinan Dai",
      "Yu Du",
      "Qian Feng",
      "Naixu Guo",
      "Tingshuo Gu",
      "Yuting Gao",
      "Yingyi Gao",
      "Xudong Han",
      "Xiang Jiang",
      "Yilin Jin",
      "Hongyi Lin",
      "Shisheng Lin",
      "Xiangnan Li",
      "Yuante Li",
      "Yixing Li",
      "Zhentao Lai",
      "Zilu Ma",
      "Yingrong Peng",
      "Jiacheng Qian",
      "Hao-Yu Sun",
      "Jianbo Sun",
      "Zirui Wang",
      "Siwei Wu",
      "Zian Wang",
      "Bin Xu",
      "Jianghao Xu",
      "Yiyang Yu",
      "Zichuan Yang",
      "Hongji Zha",
      "Ruichong Zhang"
    ],
    "published": "2025-05-13T16:58:05+00:00",
    "summary": "To advance the mathematical proficiency of large language models (LLMs), the DeepMath team has launched an open-source initiative aimed at developing an open mathematical LLM and systematically evaluating its mathematical creativity. This paper represents the initial contribution of this initiative. While recent developments in mathematical LLMs have predominantly emphasized reasoning skills, as evidenced by benchmarks on elementary to undergraduate-level mathematical tasks, the creative capabilities of these models have received comparatively little attention, and evaluation datasets remain scarce. To address this gap, we propose an evaluation criteria for mathematical creativity and introduce DeepMath-Creative, a novel, high-quality benchmark comprising constructive problems across algebra, geometry, analysis, and other domains. We conduct a systematic evaluation of mainstream LLMs' creative problem-solving abilities using this dataset. Experimental results show that even under lenient scoring criteria -- emphasizing core solution components and disregarding minor inaccuracies, such as small logical gaps, incomplete justifications, or redundant explanations -- the best-performing model, O3 Mini, achieves merely 70% accuracy, primarily on basic undergraduate-level constructive tasks. Performance declines sharply on more complex problems, with models failing to provide substantive strategies for open problems. These findings suggest that, although current LLMs display a degree of constructive proficiency on familiar and lower-difficulty problems, such performance is likely attributable to the recombination of memorized patterns rather than authentic creative insight or novel synthesis."
  },
  {
    "title": "LLM-based Prompt Ensemble for Reliable Medical Entity Recognition from EHRs",
    "url": "http://arxiv.org/abs/2505.08704v1",
    "arxiv_id": "2505.08704v1",
    "authors": [
      "K M Sajjadul Islam",
      "Ayesha Siddika Nipu",
      "Jiawei Wu",
      "Praveen Madiraju"
    ],
    "published": "2025-05-13T16:11:29+00:00",
    "summary": "Electronic Health Records (EHRs) are digital records of patient information, often containing unstructured clinical text. Named Entity Recognition (NER) is essential in EHRs for extracting key medical entities like problems, tests, and treatments to support downstream clinical applications. This paper explores prompt-based medical entity recognition using large language models (LLMs), specifically GPT-4o and DeepSeek-R1, guided by various prompt engineering techniques, including zero-shot, few-shot, and an ensemble approach. Among all strategies, GPT-4o with prompt ensemble achieved the highest classification performance with an F1-score of 0.95 and recall of 0.98, outperforming DeepSeek-R1 on the task. The ensemble method improved reliability by aggregating outputs through embedding-based similarity and majority voting."
  },
  {
    "title": "Granite-speech: open-source speech-aware LLMs with strong English ASR capabilities",
    "url": "http://arxiv.org/abs/2505.08699v1",
    "arxiv_id": "2505.08699v1",
    "authors": [
      "George Saon",
      "Avihu Dekel",
      "Alexander Brooks",
      "Tohru Nagano",
      "Abraham Daniels",
      "Aharon Satt",
      "Ashish Mittal",
      "Brian Kingsbury",
      "David Haws",
      "Edmilson Morais",
      "Gakuto Kurata",
      "Hagai Aronowitz",
      "Ibrahim Ibrahim",
      "Jeff Kuo",
      "Kate Soule",
      "Luis Lastras",
      "Masayuki Suzuki",
      "Ron Hoory",
      "Samuel Thomas",
      "Sashi Novitasari",
      "Takashi Fukuda",
      "Vishal Sunder",
      "Xiaodong Cui",
      "Zvi Kons"
    ],
    "published": "2025-05-13T15:58:57+00:00",
    "summary": "Granite-speech LLMs are compact and efficient speech language models specifically designed for English ASR and automatic speech translation (AST). The models were trained by modality aligning the 2B and 8B parameter variants of granite-3.3-instruct to speech on publicly available open-source corpora containing audio inputs and text targets consisting of either human transcripts for ASR or automatically generated translations for AST. Comprehensive benchmarking shows that on English ASR, which was our primary focus, they outperform several competitors' models that were trained on orders of magnitude more proprietary data, and they keep pace on English-to-X AST for major European languages, Japanese, and Chinese. The speech-specific components are: a conformer acoustic encoder using block attention and self-conditioning trained with connectionist temporal classification, a windowed query-transformer speech modality adapter used to do temporal downsampling of the acoustic embeddings and map them to the LLM text embedding space, and LoRA adapters to further fine-tune the text LLM. Granite-speech-3.3 operates in two modes: in speech mode, it performs ASR and AST by activating the encoder, projector, and LoRA adapters; in text mode, it calls the underlying granite-3.3-instruct model directly (without LoRA), essentially preserving all the text LLM capabilities and safety. Both models are freely available on HuggingFace (https://huggingface.co/ibm-granite/granite-speech-3.3-2b and https://huggingface.co/ibm-granite/granite-speech-3.3-8b) and can be used for both research and commercial purposes under a permissive Apache 2.0 license."
  },
  {
    "title": "Granite-speech: open-source speech-aware LLMs with strong English ASR capabilities",
    "url": "http://arxiv.org/abs/2505.08699v2",
    "arxiv_id": "2505.08699v2",
    "authors": [
      "George Saon",
      "Avihu Dekel",
      "Alexander Brooks",
      "Tohru Nagano",
      "Abraham Daniels",
      "Aharon Satt",
      "Ashish Mittal",
      "Brian Kingsbury",
      "David Haws",
      "Edmilson Morais",
      "Gakuto Kurata",
      "Hagai Aronowitz",
      "Ibrahim Ibrahim",
      "Jeff Kuo",
      "Kate Soule",
      "Luis Lastras",
      "Masayuki Suzuki",
      "Ron Hoory",
      "Samuel Thomas",
      "Sashi Novitasari",
      "Takashi Fukuda",
      "Vishal Sunder",
      "Xiaodong Cui",
      "Zvi Kons"
    ],
    "published": "2025-05-13T15:58:57+00:00",
    "summary": "Granite-speech LLMs are compact and efficient speech language models specifically designed for English ASR and automatic speech translation (AST). The models were trained by modality aligning the 2B and 8B parameter variants of granite-3.3-instruct to speech on publicly available open-source corpora containing audio inputs and text targets consisting of either human transcripts for ASR or automatically generated translations for AST. Comprehensive benchmarking shows that on English ASR, which was our primary focus, they outperform several competitors' models that were trained on orders of magnitude more proprietary data, and they keep pace on English-to-X AST for major European languages, Japanese, and Chinese. The speech-specific components are: a conformer acoustic encoder using block attention and self-conditioning trained with connectionist temporal classification, a windowed query-transformer speech modality adapter used to do temporal downsampling of the acoustic embeddings and map them to the LLM text embedding space, and LoRA adapters to further fine-tune the text LLM. Granite-speech-3.3 operates in two modes: in speech mode, it performs ASR and AST by activating the encoder, projector, and LoRA adapters; in text mode, it calls the underlying granite-3.3-instruct model directly (without LoRA), essentially preserving all the text LLM capabilities and safety. Both models are freely available on HuggingFace (https://huggingface.co/ibm-granite/granite-speech-3.3-2b and https://huggingface.co/ibm-granite/granite-speech-3.3-8b) and can be used for both research and commercial purposes under a permissive Apache 2.0 license."
  },
  {
    "title": "MC-Swarm: Minimal-Communication Multi-Agent Trajectory Planning and Deadlock Resolution for Quadrotor Swarm",
    "url": "http://arxiv.org/abs/2505.08593v1",
    "arxiv_id": "2505.08593v1",
    "authors": [
      "Yunwoo Lee",
      "Jungwon Park"
    ],
    "published": "2025-05-13T14:05:07+00:00",
    "summary": "For effective multi-agent trajectory planning, it is important to consider lightweight communication and its potential asynchrony. This paper presents a distributed trajectory planning algorithm for a quadrotor swarm that operates asynchronously and requires no communication except during the initial planning phase. Moreover, our algorithm guarantees no deadlock under asynchronous updates and absence of communication during flight. To effectively ensure these points, we build two main modules: coordination state updater and trajectory optimizer. The coordination state updater computes waypoints for each agent toward its goal and performs subgoal optimization while considering deadlocks, as well as safety constraints with respect to neighbor agents and obstacles. Then, the trajectory optimizer generates a trajectory that ensures collision avoidance even with the asynchronous planning updates of neighboring agents. We provide a theoretical guarantee of collision avoidance with deadlock resolution and evaluate the effectiveness of our method in complex simulation environments, including random forests and narrow-gap mazes. Additionally, to reduce the total mission time, we design a faster coordination state update using lightweight communication. Lastly, our approach is validated through extensive simulations and real-world experiments with cluttered environment scenarios."
  },
  {
    "title": "Thermal Detection of People with Mobility Restrictions for Barrier Reduction at Traffic Lights Controlled Intersections",
    "url": "http://arxiv.org/abs/2505.08568v1",
    "arxiv_id": "2505.08568v1",
    "authors": [
      "Xiao Ni",
      "Carsten Kuehnel",
      "Xiaoyi Jiang"
    ],
    "published": "2025-05-13T13:44:21+00:00",
    "summary": "Rapid advances in deep learning for computer vision have driven the adoption of RGB camera-based adaptive traffic light systems to improve traffic safety and pedestrian comfort. However, these systems often overlook the needs of people with mobility restrictions. Moreover, the use of RGB cameras presents significant challenges, including limited detection performance under adverse weather or low-visibility conditions, as well as heightened privacy concerns. To address these issues, we propose a fully automated, thermal detector-based traffic light system that dynamically adjusts signal durations for individuals with walking impairments or mobility burden and triggers the auditory signal for visually impaired individuals, thereby advancing towards barrier-free intersection for all users. To this end, we build the thermal dataset for people with mobility restrictions (TD4PWMR), designed to capture diverse pedestrian scenarios, particularly focusing on individuals with mobility aids or mobility burden under varying environmental conditions, such as different lighting, weather, and crowded urban settings. While thermal imaging offers advantages in terms of privacy and robustness to adverse conditions, it also introduces inherent hurdles for object detection due to its lack of color and fine texture details and generally lower resolution of thermal images. To overcome these limitations, we develop YOLO-Thermal, a novel variant of the YOLO architecture that integrates advanced feature extraction and attention mechanisms for enhanced detection accuracy and robustness in thermal imaging. Experiments demonstrate that the proposed thermal detector outperforms existing detectors, while the proposed traffic light system effectively enhances barrier-free intersection. The source codes and dataset are available at https://github.com/leon2014dresden/YOLO-THERMAL."
  },
  {
    "title": "Thermal Detection of People with Mobility Restrictions for Barrier Reduction at Traffic Lights Controlled Intersections",
    "url": "http://arxiv.org/abs/2505.08568v2",
    "arxiv_id": "2505.08568v2",
    "authors": [
      "Xiao Ni",
      "Carsten Kuehnel",
      "Xiaoyi Jiang"
    ],
    "published": "2025-05-13T13:44:21+00:00",
    "summary": "Rapid advances in deep learning for computer vision have driven the adoption of RGB camera-based adaptive traffic light systems to improve traffic safety and pedestrian comfort. However, these systems often overlook the needs of people with mobility restrictions. Moreover, the use of RGB cameras presents significant challenges, including limited detection performance under adverse weather or low-visibility conditions, as well as heightened privacy concerns. To address these issues, we propose a fully automated, thermal detector-based traffic light system that dynamically adjusts signal durations for individuals with walking impairments or mobility burden and triggers the auditory signal for visually impaired individuals, thereby advancing towards barrier-free intersection for all users. To this end, we build the thermal dataset for people with mobility restrictions (TD4PWMR), designed to capture diverse pedestrian scenarios, particularly focusing on individuals with mobility aids or mobility burden under varying environmental conditions, such as different lighting, weather, and crowded urban settings. While thermal imaging offers advantages in terms of privacy and robustness to adverse conditions, it also introduces inherent hurdles for object detection due to its lack of color and fine texture details and generally lower resolution of thermal images. To overcome these limitations, we develop YOLO-Thermal, a novel variant of the YOLO architecture that integrates advanced feature extraction and attention mechanisms for enhanced detection accuracy and robustness in thermal imaging. Experiments demonstrate that the proposed thermal detector outperforms existing detectors, while the proposed traffic light system effectively enhances barrier-free intersection. The source codes and dataset are available at https://github.com/leon2014dresden/YOLO-THERMAL."
  },
  {
    "title": "Synthesis of safety certificates for discrete-time uncertain systems via convex optimization",
    "url": "http://arxiv.org/abs/2505.08559v1",
    "arxiv_id": "2505.08559v1",
    "authors": [
      "Marta Fochesato",
      "Han Wang",
      "Antonis Papachristodoulou",
      "Paul Goulart"
    ],
    "published": "2025-05-13T13:34:32+00:00",
    "summary": "We study the problem of co-designing control barrier functions and linear state feedback controllers for discrete-time linear systems affected by additive disturbances. For disturbances of bounded magnitude, we provide a semi-definite program whose feasibility implies the existence of a control law and a certificate ensuring safety in the infinite horizon with respect to the worst-case disturbance realization in the uncertainty set. For disturbances with unbounded support, we rely on martingale theory to derive a second semi-definite program whose feasibility provides probabilistic safety guarantees holding joint-in-time over a finite time horizon. We examine several extensions, including (i) encoding of different types of input constraints, (ii) robustification against distributional ambiguity around the true distribution, (iii) design of safety filters, and (iv) extension to general safety specifications such as obstacle avoidance."
  },
  {
    "title": "Area Comparison of CHERIoT and PMP in Ibex",
    "url": "http://arxiv.org/abs/2505.08541v1",
    "arxiv_id": "2505.08541v1",
    "authors": [
      "Samuel Riedel",
      "Marno van der Maas",
      "John Thomson",
      "Andreas Kurth",
      "Pirmin Vogel"
    ],
    "published": "2025-05-13T13:12:30+00:00",
    "summary": "Memory safety is a critical concern for modern embedded systems, particularly in security-sensitive applications. This paper explores the area impact of adding memory safety extensions to the Ibex RISC-V core, focusing on physical memory protection (PMP) and Capability Hardware Extension to RISC-V for Internet of Things (CHERIoT). We synthesise the extended Ibex cores using a commercial tool targeting the open FreePDK45 process and provide a detailed area breakdown and discussion of the results.   The PMP configuration we consider is one with 16 PMP regions. We find that the extensions increase the core size by 24 thousand gate-equivalent (kGE) for PMP and 33 kGE for CHERIoT. The increase is mainly due to the additional state required to store information about protected memory. While this increase amounts to 42% for PMP and 57% for CHERIoT in Ibex's area, its effect on the overall system is minimal. In a complete system-on-chip (SoC), like the secure microcontroller OpenTitan Earl Grey, where the core represents only a fraction of the total area, the estimated system-wide overhead is 0.6% for PMP and 1% for CHERIoT. Given the security benefits these extensions provide, the area trade-off is justified, making Ibex a compelling choice for secure embedded applications."
  },
  {
    "title": "Weighted Rewriting: Semiring Semantics for Abstract Reduction Systems",
    "url": "http://arxiv.org/abs/2505.08496v1",
    "arxiv_id": "2505.08496v1",
    "authors": [
      "Emma Ahrens",
      "Jan-Christoph Kassing",
      "J\u00fcrgen Giesl",
      "Joost-Pieter Katoen"
    ],
    "published": "2025-05-13T12:24:38+00:00",
    "summary": "We present novel semiring semantics for abstract reduction systems (ARSs). More precisely, we provide a weighted version of ARSs, where the reduction steps induce weights from a semiring. Inspired by provenance analysis in database theory and logic, we obtain a formalism that can be used for provenance analysis of arbitrary ARSs. Our semantics handle (possibly unbounded) non-determinism and possibly infinite reductions. Moreover, we develop several techniques to prove upper and lower bounds on the weights resulting from our semantics, and show that in this way one obtains a uniform approach to analyze several different properties like termination, derivational complexity, space complexity, safety, as well as combinations of these properties."
  },
  {
    "title": "Explaining Autonomous Vehicles with Intention-aware Policy Graphs",
    "url": "http://arxiv.org/abs/2505.08404v1",
    "arxiv_id": "2505.08404v1",
    "authors": [
      "Sara Montese",
      "Victor Gimenez-Abalos",
      "Atia Cort\u00e9s",
      "Ulises Cort\u00e9s",
      "Sergio Alvarez-Napagao"
    ],
    "published": "2025-05-13T09:58:32+00:00",
    "summary": "The potential to improve road safety, reduce human driving error, and promote environmental sustainability have enabled the field of autonomous driving to progress rapidly over recent decades. The performance of autonomous vehicles has significantly improved thanks to advancements in Artificial Intelligence, particularly Deep Learning. Nevertheless, the opacity of their decision-making, rooted in the use of accurate yet complex AI models, has created barriers to their societal trust and regulatory acceptance, raising the need for explainability. We propose a post-hoc, model-agnostic solution to provide teleological explanations for the behaviour of an autonomous vehicle in urban environments. Building on Intention-aware Policy Graphs, our approach enables the extraction of interpretable and reliable explanations of vehicle behaviour in the nuScenes dataset from global and local perspectives. We demonstrate the potential of these explanations to assess whether the vehicle operates within acceptable legal boundaries and to identify possible vulnerabilities in autonomous driving datasets and models."
  },
  {
    "title": "Towards Contamination Resistant Benchmarks",
    "url": "http://arxiv.org/abs/2505.08389v1",
    "arxiv_id": "2505.08389v1",
    "authors": [
      "Rahmatullah Musawi",
      "Sheng Lu"
    ],
    "published": "2025-05-13T09:35:40+00:00",
    "summary": "The rapid development of large language models (LLMs) has transformed the landscape of natural language processing. Evaluating LLMs properly is crucial for understanding their potential and addressing concerns such as safety. However, LLM evaluation is confronted by various factors, among which contamination stands out as a key issue that undermines the reliability of evaluations. In this work, we introduce the concept of contamination resistance to address this challenge. We propose a benchmark based on Caesar ciphers (e.g., \"ab\" to \"bc\" when the shift is 1), which, despite its simplicity, is an excellent example of a contamination resistant benchmark. We test this benchmark on widely used LLMs under various settings, and we find that these models struggle with this benchmark when contamination is controlled. Our findings reveal issues in current LLMs and raise important questions regarding their true capabilities. Our work contributes to the development of contamination resistant benchmarks, enabling more rigorous LLM evaluation and offering insights into the true capabilities and limitations of LLMs."
  },
  {
    "title": "Investigating Resolution Strategies for Workspace-Occlusion in Augmented Virtuality",
    "url": "http://arxiv.org/abs/2505.08312v1",
    "arxiv_id": "2505.08312v1",
    "authors": [
      "Nico Feld",
      "Pauline Bimberg",
      "Michael Feldmann",
      "Matthias W\u00f6lwer",
      "Eike Langbehn",
      "Benjamin Weyers",
      "Daniel Zielasko"
    ],
    "published": "2025-05-13T07:42:21+00:00",
    "summary": "Augmented Virtuality integrates physical content into virtual environments, but the occlusion of physical by virtual content is a challenge. This unwanted occlusion may disrupt user interactions with physical devices and compromise safety and usability. This paper investigates two resolution strategies to address this issue: Redirected Walking, which subtly adjusts the user's movement to maintain physical-virtual alignment, and Automatic Teleport Rotation, which realigns the virtual environment during travel. A user study set in a virtual forest demonstrates that both methods effectively reduce occlusion. While in our testbed, Automatic Teleport Rotation achieves higher occlusion resolution, it is suspected to increase cybersickness compared to the less intrusive Redirected Walking approach."
  },
  {
    "title": "AM-Thinking-v1: Advancing the Frontier of Reasoning at 32B Scale",
    "url": "http://arxiv.org/abs/2505.08311v1",
    "arxiv_id": "2505.08311v1",
    "authors": [
      "Yunjie Ji",
      "Xiaoyu Tian",
      "Sitong Zhao",
      "Haotian Wang",
      "Shuaiting Chen",
      "Yiping Peng",
      "Han Zhao",
      "Xiangang Li"
    ],
    "published": "2025-05-13T07:41:15+00:00",
    "summary": "We present AM-Thinking-v1, a 32B dense language model that advances the frontier of reasoning, embodying the collaborative spirit of open-source innovation. Outperforming DeepSeek-R1 and rivaling leading Mixture-of-Experts (MoE) models like Qwen3-235B-A22B and Seed1.5-Thinking, AM-Thinking-v1 achieves impressive scores of 85.3 on AIME 2024, 74.4 on AIME 2025, and 70.3 on LiveCodeBench, showcasing state-of-the-art mathematical and coding capabilities among open-source models of similar scale.   Built entirely from the open-source Qwen2.5-32B base model and publicly available queries, AM-Thinking-v1 leverages a meticulously crafted post-training pipeline - combining supervised fine-tuning and reinforcement learning - to deliver exceptional reasoning capabilities. This work demonstrates that the open-source community can achieve high performance at the 32B scale, a practical sweet spot for deployment and fine-tuning. By striking a balance between top-tier performance and real-world usability, we hope AM-Thinking-v1 inspires further collaborative efforts to harness mid-scale models, pushing reasoning boundaries while keeping accessibility at the core of innovation. We have open-sourced our model on \\href{https://huggingface.co/a-m-team/AM-Thinking-v1}{Hugging Face}."
  },
  {
    "title": "Feasibility-Aware Pessimistic Estimation: Toward Long-Horizon Safety in Offline RL",
    "url": "http://arxiv.org/abs/2505.08179v1",
    "arxiv_id": "2505.08179v1",
    "authors": [
      "Zhikun Tao",
      "Gang Xiong",
      "He Fang",
      "Zhen Shen",
      "Yunjun Han",
      "Qing-Shan Jia"
    ],
    "published": "2025-05-13T02:32:49+00:00",
    "summary": "Offline safe reinforcement learning(OSRL) derives constraint-satisfying policies from pre-collected datasets, offers a promising avenue for deploying RL in safety-critical real-world domains such as robotics. However, the majority of existing approaches emphasize only short-term safety, neglecting long-horizon considerations. Consequently, they may violate safety constraints and fail to ensure sustained protection during online deployment. Moreover, the learned policies often struggle to handle states and actions that are not present or out-of-distribution(OOD) from the offline dataset, and exhibit limited sample efficiency. To address these challenges, we propose a novel framework Feasibility-Aware offline Safe Reinforcement Learning with CVAE-based Pessimism (FASP). First, we employ Hamilton-Jacobi (H-J) reachability analysis to generate reliable safety labels, which serve as supervisory signals for training both a conditional variational autoencoder (CVAE) and a safety classifier. This approach not only ensures high sampling efficiency but also provides rigorous long-horizon safety guarantees. Furthermore, we utilize pessimistic estimation methods to estimate the Q-value of reward and cost, which mitigates the extrapolation errors induces by OOD actions, and penalize unsafe actions to enabled the agent to proactively avoid high-risk behaviors. Moreover, we theoretically prove the validity of this pessimistic estimation. Extensive experiments on DSRL benchmarks demonstrate that FASP algorithm achieves competitive performance across multiple experimental tasks, particularly outperforming state-of-the-art algorithms in terms of safety."
  },
  {
    "title": "Foundation Models Knowledge Distillation For Battery Capacity Degradation Forecast",
    "url": "http://arxiv.org/abs/2505.08151v1",
    "arxiv_id": "2505.08151v1",
    "authors": [
      "Joey Chan",
      "Zhen Chen",
      "Ershun Pan"
    ],
    "published": "2025-05-13T01:03:35+00:00",
    "summary": "Accurate estimation of lithium-ion battery capacity degradation is critical for enhancing the reliability and safety of battery operations. Traditional expert models, tailored to specific scenarios, provide isolated estimations. With the rapid advancement of data-driven techniques, a series of general-purpose time-series foundation models have been developed. However, foundation models specifically designed for battery capacity degradation remain largely unexplored. To enable zero-shot generalization in battery degradation prediction using large model technology, this study proposes a degradation-aware fine-tuning strategy for time-series foundation models. We apply this strategy to fine-tune the Timer model on approximately 10 GB of open-source battery charge discharge data. Validation on our released CycleLife-SJTUIE dataset demonstrates that the fine-tuned Battery-Timer possesses strong zero-shot generalization capability in capacity degradation forecasting. To address the computational challenges of deploying large models, we further propose a knowledge distillation framework that transfers the knowledge of pre-trained foundation models into compact expert models. Distillation results across several state-of-the-art time-series expert models confirm that foundation model knowledge significantly improves the multi-condition generalization of expert models."
  },
  {
    "title": "Asymptotic grand unification in SO(10) with one extra dimension",
    "url": "http://arxiv.org/abs/2505.08068v1",
    "arxiv_id": "2505.08068v1",
    "authors": [
      "Gao-Xiang Fang",
      "Zhi-Wei Wang",
      "Ye-Ling Zhou"
    ],
    "published": "2025-05-12T21:08:37+00:00",
    "summary": "Asymptotic grand unification provides an alternative approach to gradually unify gauge couplings in the UV limit, where they reach a non-trivial UV fixed point. Using an economical and realistic particle content setup, we demonstrate that asymptotic grand unification can be achieved in a 5D SO(10) model with one extra dimension. The top, bottom and tau masses are split, and the smallness of the neutrino mass is explained via inverse seesaw. One intermediate scale, the Pati-Salam symmetry breaking scale, is included below the compactification scale. Due to the absence of large-dimensional Higgs representations, gauge couplings exhibit asymptotic safety and are thus asymptotically unified, regardless of their initial values. In contrast, Yukawa couplings can achieve asymptotic freedom if the negative gauge contributions dominate over the positive Yukawa terms, requiring exact unification at the compactification scale. The widely-used 126-dimensional Higgs is not recommended in this 5D asymptotic SO(10) GUT, as it tends to drive the gauge beta function positive, compromising asymptotic safety."
  },
  {
    "title": "Justified Evidence Collection for Argument-based AI Fairness Assurance",
    "url": "http://arxiv.org/abs/2505.08064v1",
    "arxiv_id": "2505.08064v1",
    "authors": [
      "Alpay Sabuncuoglu",
      "Christopher Burr",
      "Carsten Maple"
    ],
    "published": "2025-05-12T21:05:33+00:00",
    "summary": "It is well recognised that ensuring fair AI systems is a complex sociotechnical challenge, which requires careful deliberation and continuous oversight across all stages of a system's lifecycle, from defining requirements to model deployment and deprovisioning. Dynamic argument-based assurance cases, which present structured arguments supported by evidence, have emerged as a systematic approach to evaluating and mitigating safety risks and hazards in AI-enabled system development and have also been extended to deal with broader normative goals such as fairness and explainability. This paper introduces a systems-engineering-driven framework, supported by software tooling, to operationalise a dynamic approach to argument-based assurance in two stages. In the first stage, during the requirements planning phase, a multi-disciplinary and multi-stakeholder team define goals and claims to be established (and evidenced) by conducting a comprehensive fairness governance process. In the second stage, a continuous monitoring interface gathers evidence from existing artefacts (e.g. metrics from automated tests), such as model, data, and use case documentation, to support these arguments dynamically. The framework's effectiveness is demonstrated through an illustrative case study in finance, with a focus on supporting fairness-related arguments."
  },
  {
    "title": "FalseReject: A Resource for Improving Contextual Safety and Mitigating Over-Refusals in LLMs via Structured Reasoning",
    "url": "http://arxiv.org/abs/2505.08054v1",
    "arxiv_id": "2505.08054v1",
    "authors": [
      "Zhehao Zhang",
      "Weijie Xu",
      "Fanyou Wu",
      "Chandan K. Reddy"
    ],
    "published": "2025-05-12T20:45:25+00:00",
    "summary": "Safety alignment approaches in large language models (LLMs) often lead to the over-refusal of benign queries, significantly diminishing their utility in sensitive scenarios. To address this challenge, we introduce FalseReject, a comprehensive resource containing 16k seemingly toxic queries accompanied by structured responses across 44 safety-related categories. We propose a graph-informed adversarial multi-agent interaction framework to generate diverse and complex prompts, while structuring responses with explicit reasoning to aid models in accurately distinguishing safe from unsafe contexts. FalseReject includes training datasets tailored for both standard instruction-tuned models and reasoning-oriented models, as well as a human-annotated benchmark test set. Our extensive benchmarking on 29 state-of-the-art (SOTA) LLMs reveals persistent over-refusal challenges. Empirical results demonstrate that supervised finetuning with FalseReject substantially reduces unnecessary refusals without compromising overall safety or general language capabilities."
  },
  {
    "title": "Safety and optimality in learning-based control at low computational cost",
    "url": "http://arxiv.org/abs/2505.08026v1",
    "arxiv_id": "2505.08026v1",
    "authors": [
      "Dominik Baumann",
      "Krzysztof Kowalczyk",
      "Cristian R. Rojas",
      "Koen Tiels",
      "Pawel Wachel"
    ],
    "published": "2025-05-12T19:50:47+00:00",
    "summary": "Applying machine learning methods to physical systems that are supposed to act in the real world requires providing safety guarantees. However, methods that include such guarantees often come at a high computational cost, making them inapplicable to large datasets and embedded devices with low computational power. In this paper, we propose CoLSafe, a computationally lightweight safe learning algorithm whose computational complexity grows sublinearly with the number of data points. We derive both safety and optimality guarantees and showcase the effectiveness of our algorithm on a seven-degrees-of-freedom robot arm."
  },
  {
    "title": "A Theoretical Framework for Explaining Reinforcement Learning with Shapley Values",
    "url": "http://arxiv.org/abs/2505.07797v1",
    "arxiv_id": "2505.07797v1",
    "authors": [
      "Daniel Beechey",
      "Thomas M. S. Smith",
      "\u00d6zg\u00fcr \u015eim\u015fek"
    ],
    "published": "2025-05-12T17:48:28+00:00",
    "summary": "Reinforcement learning agents can achieve superhuman performance, but their decisions are often difficult to interpret. This lack of transparency limits deployment, especially in safety-critical settings where human trust and accountability are essential. In this work, we develop a theoretical framework for explaining reinforcement learning through the influence of state features, which represent what the agent observes in its environment. We identify three core elements of the agent-environment interaction that benefit from explanation: behaviour (what the agent does), performance (what the agent achieves), and value estimation (what the agent expects to achieve). We treat state features as players cooperating to produce each element and apply Shapley values, a principled method from cooperative game theory, to identify the influence of each feature. This approach yields a family of mathematically grounded explanations with clear semantics and theoretical guarantees. We use illustrative examples to show how these explanations align with human intuition and reveal novel insights. Our framework unifies and extends prior work, making explicit the assumptions behind existing approaches, and offers a principled foundation for more interpretable and trustworthy reinforcement learning."
  },
  {
    "title": "Learning from Peers in Reasoning Models",
    "url": "http://arxiv.org/abs/2505.07787v1",
    "arxiv_id": "2505.07787v1",
    "authors": [
      "Tongxu Luo",
      "Wenyu Du",
      "Jiaxi Bi",
      "Stephen Chung",
      "Zhengyang Tang",
      "Hao Yang",
      "Min Zhang",
      "Benyou Wang"
    ],
    "published": "2025-05-12T17:39:56+00:00",
    "summary": "Large Reasoning Models (LRMs) have the ability to self-correct even when they make mistakes in their reasoning paths. However, our study reveals that when the reasoning process starts with a short but poor beginning, it becomes difficult for the model to recover. We refer to this phenomenon as the \"Prefix Dominance Trap\". Inspired by psychological findings that peer interaction can promote self-correction without negatively impacting already accurate individuals, we propose **Learning from Peers** (LeaP) to address this phenomenon. Specifically, every tokens, each reasoning path summarizes its intermediate reasoning and shares it with others through a routing mechanism, enabling paths to incorporate peer insights during inference. However, we observe that smaller models sometimes fail to follow summarization and reflection instructions effectively. To address this, we fine-tune them into our **LeaP-T** model series. Experiments on AIME 2024, AIME 2025, AIMO 2025, and GPQA Diamond show that LeaP provides substantial improvements. For instance, QwQ-32B with LeaP achieves nearly 5 absolute points higher than the baseline on average, and surpasses DeepSeek-R1-671B on three math benchmarks with an average gain of 3.3 points. Notably, our fine-tuned LeaP-T-7B matches the performance of DeepSeek-R1-Distill-Qwen-14B on AIME 2024. In-depth analysis reveals LeaP's robust error correction by timely peer insights, showing strong error tolerance and handling varied task difficulty. LeaP marks a milestone by enabling LRMs to collaborate during reasoning. Our code, datasets, and models are available at https://learning-from-peers.github.io/ ."
  },
  {
    "title": "Must Read: A Systematic Survey of Computational Persuasion",
    "url": "http://arxiv.org/abs/2505.07775v1",
    "arxiv_id": "2505.07775v1",
    "authors": [
      "Nimet Beyza Bozdag",
      "Shuhaib Mehri",
      "Xiaocheng Yang",
      "Hyeonjeong Ha",
      "Zirui Cheng",
      "Esin Durmus",
      "Jiaxuan You",
      "Heng Ji",
      "Gokhan Tur",
      "Dilek Hakkani-T\u00fcr"
    ],
    "published": "2025-05-12T17:26:31+00:00",
    "summary": "Persuasion is a fundamental aspect of communication, influencing decision-making across diverse contexts, from everyday conversations to high-stakes scenarios such as politics, marketing, and law. The rise of conversational AI systems has significantly expanded the scope of persuasion, introducing both opportunities and risks. AI-driven persuasion can be leveraged for beneficial applications, but also poses threats through manipulation and unethical influence. Moreover, AI systems are not only persuaders, but also susceptible to persuasion, making them vulnerable to adversarial attacks and bias reinforcement. Despite rapid advancements in AI-generated persuasive content, our understanding of what makes persuasion effective remains limited due to its inherently subjective and context-dependent nature. In this survey, we provide a comprehensive overview of computational persuasion, structured around three key perspectives: (1) AI as a Persuader, which explores AI-generated persuasive content and its applications; (2) AI as a Persuadee, which examines AI's susceptibility to influence and manipulation; and (3) AI as a Persuasion Judge, which analyzes AI's role in evaluating persuasive strategies, detecting manipulation, and ensuring ethical persuasion. We introduce a taxonomy for computational persuasion research and discuss key challenges, including evaluating persuasiveness, mitigating manipulative persuasion, and developing responsible AI-driven persuasive systems. Our survey outlines future research directions to enhance the safety, fairness, and effectiveness of AI-powered persuasion while addressing the risks posed by increasingly capable language models."
  },
  {
    "title": "Clouds can enhance direct imaging detection of O2 and O3 on terrestrial exoplanets",
    "url": "http://arxiv.org/abs/2505.07760v1",
    "arxiv_id": "2505.07760v1",
    "authors": [
      "Huanzhou Yang",
      "Michelle Hu",
      "Dorian S. Abbot"
    ],
    "published": "2025-05-12T17:06:57+00:00",
    "summary": "Clouds are often considered a highly uncertain barrier for detecting biosignatures on exoplanets, especially given intuition gained from transit surveys. However, for direct imaging reflected light observations, clouds could increase the observational signal by increasing reflected light. Here we constrain the impact of clouds on the detection of O2 and O3 by a direct imaging telescope such as the Habitable Worlds Observatory (HWO) using observations simulated with the Planetary Spectrum Generator (PSG). We first perform sensitivity tests to show that low clouds enhance O2 and O3 detectability while high clouds diminish it, and the effect is greater when cloud particles are smaller. We next apply clouds produced by the cloud microphysics model CARMA with varied planetary parameters and clouds drawn from observations of different types of clouds on Earth to PSG. We find that clouds are likely to increase the SNR of O2 and O3 for terrestrial exoplanets under a wide range of scenarios. This work provides important constraints on the impact of clouds on observations by telescopes including HWO."
  },
  {
    "title": "Emotion-Gradient Metacognitive RSI (Part I): Theoretical Foundations and Single-Agent Architecture",
    "url": "http://arxiv.org/abs/2505.07757v1",
    "arxiv_id": "2505.07757v1",
    "authors": [
      "Rintaro Ando"
    ],
    "published": "2025-05-12T17:02:47+00:00",
    "summary": "We present the Emotion-Gradient Metacognitive Recursive Self-Improvement (EG-MRSI) framework, a novel architecture that integrates introspective metacognition, emotion-based intrinsic motivation, and recursive self-modification into a unified theoretical system. The framework is explicitly capable of overwriting its own learning algorithm under formally bounded risk. Building upon the Noise-to-Meaning RSI (N2M-RSI) foundation, EG-MRSI introduces a differentiable intrinsic reward function driven by confidence, error, novelty, and cumulative success. This signal regulates both a metacognitive mapping and a self-modification operator constrained by provable safety mechanisms. We formally define the initial agent configuration, emotion-gradient dynamics, and RSI trigger conditions, and derive a reinforcement-compatible optimization objective that guides the agent's development trajectory. Meaning Density and Meaning Conversion Efficiency are introduced as quantifiable metrics of semantic learning, closing the gap between internal structure and predictive informativeness. This Part I paper establishes the single-agent theoretical foundations of EG-MRSI. Future parts will extend this framework to include safety certificates and rollback protocols (Part II), collective intelligence mechanisms (Part III), and feasibility constraints including thermodynamic and computational limits (Part IV). Together, the EG-MRSI series provides a rigorous, extensible foundation for open-ended and safe AGI."
  },
  {
    "title": "Assessing the Chemical Intelligence of Large Language Models",
    "url": "http://arxiv.org/abs/2505.07735v1",
    "arxiv_id": "2505.07735v1",
    "authors": [
      "Nicholas T. Runcie",
      "Charlotte M. Deane",
      "Fergus Imrie"
    ],
    "published": "2025-05-12T16:44:38+00:00",
    "summary": "Large Language Models are versatile, general-purpose tools with a wide range of applications. Recently, the advent of \"reasoning models\" has led to substantial improvements in their abilities in advanced problem-solving domains such as mathematics and software engineering. In this work, we assessed the ability of reasoning models to directly perform chemistry tasks, without any assistance from external tools. We created a novel benchmark, called ChemIQ, which consists of 796 questions assessing core concepts in organic chemistry, focused on molecular comprehension and chemical reasoning. Unlike previous benchmarks, which primarily use multiple choice formats, our approach requires models to construct short-answer responses, more closely reflecting real-world applications. The reasoning models, exemplified by OpenAI's o3-mini, correctly answered 28%-59% of questions depending on the reasoning level used, with higher reasoning levels significantly increasing performance on all tasks. These models substantially outperformed the non-reasoning model, GPT-4o, which achieved only 7% accuracy. We found that Large Language Models can now convert SMILES strings to IUPAC names, a task earlier models were unable to perform. Additionally, we show that the latest reasoning models can elucidate structures from 1H and 13C NMR data, correctly generating SMILES strings for 74% of molecules containing up to 10 heavy atoms, and in one case solving a structure comprising 21 heavy atoms. For each task, we found evidence that the reasoning process mirrors that of a human chemist. Our results demonstrate that the latest reasoning models have the ability to perform advanced chemical reasoning."
  },
  {
    "title": "Non-Conservative Data-driven Safe Control Design for Nonlinear Systems with Polyhedral Safe Sets",
    "url": "http://arxiv.org/abs/2505.07733v1",
    "arxiv_id": "2505.07733v1",
    "authors": [
      "Amir Modares",
      "Bosen Lian",
      "Hamidreza Modares"
    ],
    "published": "2025-05-12T16:40:32+00:00",
    "summary": "This paper presents a data-driven nonlinear safe control design approach for discrete-time systems under parametric uncertainties and additive disturbances. We first characterize a new control structure from which a data-based representation of closed-loop systems is obtained. This data-based closed-loop system is composed of two parts: 1) a parametrized linear closed-loop part and a parametrized nonlinear remainder closed-loop part. We show that using the standard practice or learning a robust controller to ensure safety while treating the remaining nonlinearities as disturbances brings about significant challenges in terms of computational complexity and conservatism. To overcome these challenges, we develop a novel nonlinear safe control design approach in which the closed-loop nonlinear remainders are learned, rather than canceled, in a control-oriented fashion while preserving the computational efficiency. To this end, a primal-dual optimization framework is leveraged in which the control gains are learned to enforce the second-order optimality on the closed-loop nonlinear remainders. This allows us to account for nonlinearities in the design for the sake of safety rather than treating them as disturbances. This new controller parameterization and design approach reduces the computational complexity and the conservatism of designing a safe nonlinear controller. A simulation example is then provided to show the effectiveness of the proposed data-driven controller."
  },
  {
    "title": "Hybrid Control Strategies for Safe and Adaptive Robot-Assisted Dressing",
    "url": "http://arxiv.org/abs/2505.07710v1",
    "arxiv_id": "2505.07710v1",
    "authors": [
      "Yasmin Rafiq",
      "Baslin A. James",
      "Ke Xu",
      "Robert M. Hierons",
      "Sanja Dogramadzi"
    ],
    "published": "2025-05-12T16:17:48+00:00",
    "summary": "Safety, reliability, and user trust are crucial in human-robot interaction (HRI) where the robots must address hazards in real-time. This study presents hazard driven low-level control strategies implemented in robot-assisted dressing (RAD) scenarios where hazards like garment snags and user discomfort in real-time can affect task performance and user safety. The proposed control mechanisms include: (1) Garment Snagging Control Strategy, which detects excessive forces and either seeks user intervention via a chatbot or autonomously adjusts its trajectory, and (2) User Discomfort/Pain Mitigation Strategy, which dynamically reduces velocity based on user feedback and aborts the task if necessary. We used physical dressing trials in order to evaluate these control strategies. Results confirm that integrating force monitoring with user feedback improves safety and task continuity. The findings emphasise the need for hybrid approaches that balance autonomous intervention, user involvement, and controlled task termination, supported by bi-directional interaction and real-time user-driven adaptability, paving the way for more responsive and personalised HRI systems."
  },
  {
    "title": "S-GRPO: Early Exit via Reinforcement Learning in Reasoning Models",
    "url": "http://arxiv.org/abs/2505.07686v1",
    "arxiv_id": "2505.07686v1",
    "authors": [
      "Muzhi Dai",
      "Chenxu Yang",
      "Qingyi Si"
    ],
    "published": "2025-05-12T15:50:44+00:00",
    "summary": "As Test-Time Scaling emerges as an active research focus in the large language model community, advanced post-training methods increasingly emphasize extending chain-of-thought (CoT) generation length, thereby enhancing reasoning capabilities to approach Deepseek R1-like reasoning models. However, recent studies reveal that reasoning models (even Qwen3) consistently exhibit excessive thought redundancy in CoT generation. This overthinking problem stems from conventional outcome-reward reinforcement learning's systematic neglect in regulating intermediate reasoning steps. This paper proposes Serial-Group Decaying-Reward Policy Optimization (namely S-GRPO), a novel reinforcement learning method that empowers models with the capability to determine the sufficiency of reasoning steps, subsequently triggering early exit of CoT generation. Specifically, unlike GRPO, which samples multiple possible completions (parallel group) in parallel, we select multiple temporal positions in the generation of one CoT to allow the model to exit thinking and instead generate answers (serial group), respectively. For the correct answers in a serial group, we assign rewards that decay according to positions, with lower rewards towards the later ones, thereby reinforcing the model's behavior to generate higher-quality answers at earlier phases with earlier exits of thinking. Empirical evaluations demonstrate compatibility with state-of-the-art reasoning models, including Qwen3 and Deepseek-distill models, achieving 35.4% ~ 61.1\\% sequence length reduction with 0.72% ~ 6.08% accuracy improvements across GSM8K, AIME 2024, AMC 2023, MATH-500, and GPQA Diamond benchmarks."
  },
  {
    "title": "Deep Learning Advances in Vision-Based Traffic Accident Anticipation: A Comprehensive Review of Methods,Datasets,and Future Directions",
    "url": "http://arxiv.org/abs/2505.07611v1",
    "arxiv_id": "2505.07611v1",
    "authors": [
      "Yi Zhang",
      "Wenye Zhou",
      "Ruonan Lin",
      "Xin Yang",
      "Hao Zheng"
    ],
    "published": "2025-05-12T14:34:22+00:00",
    "summary": "Traffic accident prediction and detection are critical for enhancing road safety,and vision-based traffic accident anticipation (Vision-TAA) has emerged as a promising approach in the era of deep learning.This paper reviews 147 recent studies,focusing on the application of supervised,unsupervised,and hybrid deep learning models for accident prediction,alongside the use of real-world and synthetic datasets.Current methodologies are categorized into four key approaches: image and video feature-based prediction, spatiotemporal feature-based prediction, scene understanding,and multimodal data fusion.While these methods demonstrate significant potential,challenges such as data scarcity,limited generalization to complex scenarios,and real-time performance constraints remain prevalent. This review highlights opportunities for future research,including the integration of multimodal data fusion, self-supervised learning,and Transformer-based architectures to enhance prediction accuracy and scalability.By synthesizing existing advancements and identifying critical gaps, this paper provides a foundational reference for developing robust and adaptive Vision-TAA systems,contributing to road safety and traffic management."
  },
  {
    "title": "Concept-Level Explainability for Auditing & Steering LLM Responses",
    "url": "http://arxiv.org/abs/2505.07610v1",
    "arxiv_id": "2505.07610v1",
    "authors": [
      "Kenza Amara",
      "Rita Sevastjanova",
      "Mennatallah El-Assady"
    ],
    "published": "2025-05-12T14:31:51+00:00",
    "summary": "As large language models (LLMs) become widely deployed, concerns about their safety and alignment grow. An approach to steer LLM behavior, such as mitigating biases or defending against jailbreaks, is to identify which parts of a prompt influence specific aspects of the model's output. Token-level attribution methods offer a promising solution, but still struggle in text generation, explaining the presence of each token in the output separately, rather than the underlying semantics of the entire LLM response. We introduce ConceptX, a model-agnostic, concept-level explainability method that identifies the concepts, i.e., semantically rich tokens in the prompt, and assigns them importance based on the outputs' semantic similarity. Unlike current token-level methods, ConceptX also offers to preserve context integrity through in-place token replacements and supports flexible explanation goals, e.g., gender bias. ConceptX enables both auditing, by uncovering sources of bias, and steering, by modifying prompts to shift the sentiment or reduce the harmfulness of LLM responses, without requiring retraining. Across three LLMs, ConceptX outperforms token-level methods like TokenSHAP in both faithfulness and human alignment. Steering tasks boost sentiment shift by 0.252 versus 0.131 for random edits and lower attack success rates from 0.463 to 0.242, outperforming attribution and paraphrasing baselines. While prompt engineering and self-explaining methods sometimes yield safer responses, ConceptX offers a transparent and faithful alternative for improving LLM safety and alignment, demonstrating the practical value of attribution-based explainability in guiding LLM behavior."
  },
  {
    "title": "MiMo: Unlocking the Reasoning Potential of Language Model -- From Pretraining to Posttraining",
    "url": "http://arxiv.org/abs/2505.07608v1",
    "arxiv_id": "2505.07608v1",
    "authors": [
      "Xiaomi LLM-Core Team",
      ":",
      "Bingquan Xia",
      "Bowen Shen",
      "Cici",
      "Dawei Zhu",
      "Di Zhang",
      "Gang Wang",
      "Hailin Zhang",
      "Huaqiu Liu",
      "Jiebao Xiao",
      "Jinhao Dong",
      "Liang Zhao",
      "Peidian Li",
      "Peng Wang",
      "Shihua Yu",
      "Shimao Chen",
      "Weikun Wang",
      "Wenhan Ma",
      "Xiangwei Deng",
      "Yi Huang",
      "Yifan Song",
      "Zihan Jiang",
      "Bowen Ye",
      "Can Cai",
      "Chenhong He",
      "Dong Zhang",
      "Duo Zhang",
      "Guoan Wang",
      "Hao Tian",
      "Haochen Zhao",
      "Heng Qu",
      "Hongshen Xu",
      "Jun Shi",
      "Kainan Bao",
      "QingKai Fang",
      "Kang Zhou",
      "Kangyang Zhou",
      "Lei Li",
      "Menghang Zhu",
      "Nuo Chen",
      "Qiantong Wang",
      "Shaohui Liu",
      "Shicheng Li",
      "Shuhao Gu",
      "Shuhuai Ren",
      "Shuo Liu",
      "Sirui Deng",
      "Weiji Zhuang",
      "Weiwei Lv",
      "Wenyu Yang",
      "Xin Zhang",
      "Xing Yong",
      "Xing Zhang",
      "Xingchen Song",
      "Xinzhe Xu",
      "Xu Wang",
      "Yihan Yan",
      "Yu Tu",
      "Yuanyuan Tian",
      "Yudong Wang",
      "Yue Yu",
      "Zhenru Lin",
      "Zhichao Song",
      "Zihao Yue"
    ],
    "published": "2025-05-12T14:30:11+00:00",
    "summary": "We present MiMo-7B, a large language model born for reasoning tasks, with optimization across both pre-training and post-training stages. During pre-training, we enhance the data preprocessing pipeline and employ a three-stage data mixing strategy to strengthen the base model's reasoning potential. MiMo-7B-Base is pre-trained on 25 trillion tokens, with additional Multi-Token Prediction objective for enhanced performance and accelerated inference speed. During post-training, we curate a dataset of 130K verifiable mathematics and programming problems for reinforcement learning, integrating a test-difficulty-driven code-reward scheme to alleviate sparse-reward issues and employing strategic data resampling to stabilize training. Extensive evaluations show that MiMo-7B-Base possesses exceptional reasoning potential, outperforming even much larger 32B models. The final RL-tuned model, MiMo-7B-RL, achieves superior performance on mathematics, code and general reasoning tasks, surpassing the performance of OpenAI o1-mini. The model checkpoints are available at https://github.com/xiaomimimo/MiMo."
  },
  {
    "title": "Finite-Sample-Based Reachability for Safe Control with Gaussian Process Dynamics",
    "url": "http://arxiv.org/abs/2505.07594v1",
    "arxiv_id": "2505.07594v1",
    "authors": [
      "Manish Prajapat",
      "Johannes K\u00f6hler",
      "Amon Lahr",
      "Andreas Krause",
      "Melanie N. Zeilinger"
    ],
    "published": "2025-05-12T14:20:20+00:00",
    "summary": "Gaussian Process (GP) regression is shown to be effective for learning unknown dynamics, enabling efficient and safety-aware control strategies across diverse applications. However, existing GP-based model predictive control (GP-MPC) methods either rely on approximations, thus lacking guarantees, or are overly conservative, which limits their practical utility. To close this gap, we present a sampling-based framework that efficiently propagates the model's epistemic uncertainty while avoiding conservatism. We establish a novel sample complexity result that enables the construction of a reachable set using a finite number of dynamics functions sampled from the GP posterior. Building on this, we design a sampling-based GP-MPC scheme that is recursively feasible and guarantees closed-loop safety and stability with high probability. Finally, we showcase the effectiveness of our method on two numerical examples, highlighting accurate reachable set over-approximation and safe closed-loop performance."
  },
  {
    "title": "SecReEvalBench: A Multi-turned Security Resilience Evaluation Benchmark for Large Language Models",
    "url": "http://arxiv.org/abs/2505.07584v1",
    "arxiv_id": "2505.07584v1",
    "authors": [
      "Huining Cui",
      "Wei Liu"
    ],
    "published": "2025-05-12T14:09:24+00:00",
    "summary": "The increasing deployment of large language models in security-sensitive domains necessitates rigorous evaluation of their resilience against adversarial prompt-based attacks. While previous benchmarks have focused on security evaluations with limited and predefined attack domains, such as cybersecurity attacks, they often lack a comprehensive assessment of intent-driven adversarial prompts and the consideration of real-life scenario-based multi-turn attacks. To address this gap, we present SecReEvalBench, the Security Resilience Evaluation Benchmark, which defines four novel metrics: Prompt Attack Resilience Score, Prompt Attack Refusal Logic Score, Chain-Based Attack Resilience Score and Chain-Based Attack Rejection Time Score. Moreover, SecReEvalBench employs six questioning sequences for model assessment: one-off attack, successive attack, successive reverse attack, alternative attack, sequential ascending attack with escalating threat levels and sequential descending attack with diminishing threat levels. In addition, we introduce a dataset customized for the benchmark, which incorporates both neutral and malicious prompts, categorised across seven security domains and sixteen attack techniques. In applying this benchmark, we systematically evaluate five state-of-the-art open-weighted large language models, Llama 3.1, Gemma 2, Mistral v0.3, DeepSeek-R1 and Qwen 3. Our findings offer critical insights into the strengths and weaknesses of modern large language models in defending against evolving adversarial threats. The SecReEvalBench dataset is publicly available at https://kaggle.com/datasets/5a7ee22cf9dab6c93b55a73f630f6c9b42e936351b0ae98fbae6ddaca7fe248d, which provides a groundwork for advancing research in large language model security."
  },
  {
    "title": "Combining Bayesian Inference and Reinforcement Learning for Agent Decision Making: A Review",
    "url": "http://arxiv.org/abs/2505.07911v1",
    "arxiv_id": "2505.07911v1",
    "authors": [
      "Chengmin Zhou",
      "Ville Kyrki",
      "Pasi Fr\u00e4nti",
      "Laura Ruotsalainen"
    ],
    "published": "2025-05-12T13:34:50+00:00",
    "summary": "Bayesian inference has many advantages in decision making of agents (e.g. robotics/simulative agent) over a regular data-driven black-box neural network: Data-efficiency, generalization, interpretability, and safety where these advantages benefit directly/indirectly from the uncertainty quantification of Bayesian inference. However, there are few comprehensive reviews to summarize the progress of Bayesian inference on reinforcement learning (RL) for decision making to give researchers a systematic understanding. This paper focuses on combining Bayesian inference with RL that nowadays is an important approach in agent decision making. To be exact, this paper discusses the following five topics: 1) Bayesian methods that have potential for agent decision making. First basic Bayesian methods and models (Bayesian rule, Bayesian learning, and Bayesian conjugate models) are discussed followed by variational inference, Bayesian optimization, Bayesian deep learning, Bayesian active learning, Bayesian generative models, Bayesian meta-learning, and lifelong Bayesian learning. 2) Classical combinations of Bayesian methods with model-based RL (with approximation methods), model-free RL, and inverse RL. 3) Latest combinations of potential Bayesian methods with RL. 4) Analytical comparisons of methods that combine Bayesian methods with RL with respect to data-efficiency, generalization, interpretability, and safety. 5) In-depth discussions in six complex problem variants of RL, including unknown reward, partial-observability, multi-agent, multi-task, non-linear non-Gaussian, and hierarchical RL problems and the summary of how Bayesian methods work in the data collection, data processing and policy learning stages of RL to pave the way for better agent decision-making strategies."
  },
  {
    "title": "Byam: Fixing Breaking Dependency Updates with Large Language Models",
    "url": "http://arxiv.org/abs/2505.07522v1",
    "arxiv_id": "2505.07522v1",
    "authors": [
      "Frank Reyes",
      "May Mahmoud",
      "Federico Bono",
      "Sarah Nadi",
      "Benoit Baudry",
      "Martin Monperrus"
    ],
    "published": "2025-05-12T13:03:26+00:00",
    "summary": "Application Programming Interfaces (APIs) facilitate the integration of third-party dependencies within the code of client applications. However, changes to an API, such as deprecation, modification of parameter names or types, or complete replacement with a new API, can break existing client code. These changes are called breaking dependency updates; It is often tedious for API users to identify the cause of these breaks and update their code accordingly. In this paper, we explore the use of Large Language Models (LLMs) to automate client code updates in response to breaking dependency updates. We evaluate our approach on the BUMP dataset, a benchmark for breaking dependency updates in Java projects. Our approach leverages LLMs with advanced prompts, including information from the build process and from the breaking dependency analysis. We assess effectiveness at three granularity levels: at the build level, the file level, and the individual compilation error level. We experiment with five LLMs: Google Gemini-2.0 Flash, OpenAI GPT4o-mini, OpenAI o3-mini, Alibaba Qwen2.5-32b-instruct, and DeepSeek V3. Our results show that LLMs can automatically repair breaking updates. Among the considered models, OpenAI's o3-mini is the best, able to completely fix 27% of the builds when using prompts that include contextual information such as the buggy line, API differences, error messages, and step-by-step reasoning instructions. Also, it fixes 78% of the individual compilation errors. Overall, our findings demonstrate the potential for LLMs to fix compilation errors due to breaking dependency updates, supporting developers in their efforts to stay up-to-date with changes in their dependencies."
  },
  {
    "title": "Promising Topics for U.S.-China Dialogues on AI Risks and Governance",
    "url": "http://arxiv.org/abs/2505.07468v1",
    "arxiv_id": "2505.07468v1",
    "authors": [
      "Saad Siddiqui",
      "Lujain Ibrahim",
      "Kristy Loke",
      "Stephen Clare",
      "Marianne Lu",
      "Aris Richardson",
      "Conor McGlynn",
      "Jeffrey Ding"
    ],
    "published": "2025-05-12T11:56:19+00:00",
    "summary": "Cooperation between the United States and China, the world's leading artificial intelligence (AI) powers, is crucial for effective global AI governance and responsible AI development. Although geopolitical tensions have emphasized areas of conflict, in this work, we identify potential common ground for productive dialogue by conducting a systematic analysis of more than 40 primary AI policy and corporate governance documents from both nations. Specifically, using an adapted version of the AI Governance and Regulatory Archive (AGORA) - a comprehensive repository of global AI governance documents - we analyze these materials in their original languages to identify areas of convergence in (1) sociotechnical risk perception and (2) governance approaches. We find strong and moderate overlap in several areas such as on concerns about algorithmic transparency, system reliability, agreement on the importance of inclusive multi-stakeholder engagement, and AI's role in enhancing safety. These findings suggest that despite strategic competition, there exist concrete opportunities for bilateral U.S.-China cooperation in the development of responsible AI. Thus, we present recommendations for furthering diplomatic dialogues that can facilitate such cooperation. Our analysis contributes to understanding how different international governance frameworks might be harmonized to promote global responsible AI development."
  },
  {
    "title": "AIS Data-Driven Maritime Monitoring Based on Transformer: A Comprehensive Review",
    "url": "http://arxiv.org/abs/2505.07374v1",
    "arxiv_id": "2505.07374v1",
    "authors": [
      "Zhiye Xie",
      "Enmei Tu",
      "Xianping Fu",
      "Guoliang Yuan",
      "Yi Han"
    ],
    "published": "2025-05-12T09:17:43+00:00",
    "summary": "With the increasing demands for safety, efficiency, and sustainability in global shipping, Automatic Identification System (AIS) data plays an increasingly important role in maritime monitoring. AIS data contains spatial-temporal variation patterns of vessels that hold significant research value in the marine domain. However, due to its massive scale, the full potential of AIS data has long remained untapped. With its powerful sequence modeling capabilities, particularly its ability to capture long-range dependencies and complex temporal dynamics, the Transformer model has emerged as an effective tool for processing AIS data. Therefore, this paper reviews the research on Transformer-based AIS data-driven maritime monitoring, providing a comprehensive overview of the current applications of Transformer models in the marine field. The focus is on Transformer-based trajectory prediction methods, behavior detection, and prediction techniques. Additionally, this paper collects and organizes publicly available AIS datasets from the reviewed papers, performing data filtering, cleaning, and statistical analysis. The statistical results reveal the operational characteristics of different vessel types, providing data support for further research on maritime monitoring tasks. Finally, we offer valuable suggestions for future research, identifying two promising research directions. Datasets are available at https://github.com/eyesofworld/Maritime-Monitoring."
  },
  {
    "title": "Enabling Privacy-Aware AI-Based Ergonomic Analysis",
    "url": "http://arxiv.org/abs/2505.07306v1",
    "arxiv_id": "2505.07306v1",
    "authors": [
      "Sander De Coninck",
      "Emilio Gamba",
      "Bart Van Doninck",
      "Abdellatif Bey-Temsamani",
      "Sam Leroux",
      "Pieter Simoens"
    ],
    "published": "2025-05-12T07:52:48+00:00",
    "summary": "Musculoskeletal disorders (MSDs) are a leading cause of injury and productivity loss in the manufacturing industry, incurring substantial economic costs. Ergonomic assessments can mitigate these risks by identifying workplace adjustments that improve posture and reduce strain. Camera-based systems offer a non-intrusive, cost-effective method for continuous ergonomic tracking, but they also raise significant privacy concerns. To address this, we propose a privacy-aware ergonomic assessment framework utilizing machine learning techniques. Our approach employs adversarial training to develop a lightweight neural network that obfuscates video data, preserving only the essential information needed for human pose estimation. This obfuscation ensures compatibility with standard pose estimation algorithms, maintaining high accuracy while protecting privacy. The obfuscated video data is transmitted to a central server, where state-of-the-art keypoint detection algorithms extract body landmarks. Using multi-view integration, 3D keypoints are reconstructed and evaluated with the Rapid Entire Body Assessment (REBA) method. Our system provides a secure, effective solution for ergonomic monitoring in industrial environments, addressing both privacy and workplace safety concerns."
  },
  {
    "title": "Synthetic Similarity Search in Automotive Production",
    "url": "http://arxiv.org/abs/2505.07256v1",
    "arxiv_id": "2505.07256v1",
    "authors": [
      "Christoph Huber",
      "Ludwig Schleeh",
      "Dino Knoll",
      "Michael Guthe"
    ],
    "published": "2025-05-12T06:10:48+00:00",
    "summary": "Visual quality inspection in automotive production is essential for ensuring the safety and reliability of vehicles. Computer vision (CV) has become a popular solution for these inspections due to its cost-effectiveness and reliability. However, CV models require large, annotated datasets, which are costly and time-consuming to collect. To reduce the need for extensive training data, we propose a novel image classification pipeline that combines similarity search using a vision-based foundation model with synthetic data. Our approach leverages a DINOv2 model to transform input images into feature vectors, which are then compared to pre-classified reference images using cosine distance measurements. By utilizing synthetic data instead of real images as references, our pipeline achieves high classification accuracy without relying on real data. We evaluate this approach in eight real-world inspection scenarios and demonstrate that it meets the high performance requirements of production environments."
  },
  {
    "title": "Continuous-Time Control Synthesis for Multiple Quadrotors under Signal Temporal Logic Specifications",
    "url": "http://arxiv.org/abs/2505.07240v1",
    "arxiv_id": "2505.07240v1",
    "authors": [
      "Yating Yuan"
    ],
    "published": "2025-05-12T05:30:08+00:00",
    "summary": "Ensuring continuous-time control of multiple quadrotors in constrained environments under signal temporal logic (STL) specifications is challenging due to nonlinear dynamics, safety constraints, and disturbances. This letter proposes a two-stage framework to address this challenge. First, exponentially decaying tracking error bounds are derived with multidimensional geometric control gains obtained via differential evolution. These bounds are less conservative, while the resulting tracking errors exhibit smaller oscillations and improved transient performance. Second, leveraging the time-varying bounds, a mixed-integer convex programming (MICP) formulation generates piecewise B\\'ezier reference trajectories that satisfy STL and velocity limits, while ensuring inter-agent safety through convex-hull properties. Simulation results demonstrate that the proposed approach enables formally verifiable multi-agent coordination in constrained environments, with provable tracking guarantees under bounded disturbances."
  },
  {
    "title": "Measuring General Intelligence with Generated Games",
    "url": "http://arxiv.org/abs/2505.07215v1",
    "arxiv_id": "2505.07215v1",
    "authors": [
      "Vivek Verma",
      "David Huang",
      "William Chen",
      "Dan Klein",
      "Nicholas Tomlin"
    ],
    "published": "2025-05-12T04:01:03+00:00",
    "summary": "We present gg-bench, a collection of game environments designed to evaluate general reasoning capabilities in language models. Unlike most static benchmarks, gg-bench is a data generating process where new evaluation instances can be generated at will. In particular, gg-bench is synthetically generated by (1) using a large language model (LLM) to generate natural language descriptions of novel games, (2) using the LLM to implement each game in code as a Gym environment, and (3) training reinforcement learning (RL) agents via self-play on the generated games. We evaluate language models by their winrate against these RL agents by prompting models with the game description, current board state, and a list of valid moves, after which models output the moves they wish to take. gg-bench is challenging: state-of-the-art LLMs such as GPT-4o and Claude 3.7 Sonnet achieve winrates of 7-9% on gg-bench using in-context learning, while reasoning models such as o1, o3-mini and DeepSeek-R1 achieve average winrates of 31-36%. We release the generated games, data generation process, and evaluation code in order to support future modeling work and expansion of our benchmark."
  },
  {
    "title": "Benchmarking Ethical and Safety Risks of Healthcare LLMs in China-Toward Systemic Governance under Healthy China 2030",
    "url": "http://arxiv.org/abs/2505.07205v1",
    "arxiv_id": "2505.07205v1",
    "authors": [
      "Mouxiao Bian",
      "Rongzhao Zhang",
      "Chao Ding",
      "Xinwei Peng",
      "Jie Xu"
    ],
    "published": "2025-05-12T03:28:05+00:00",
    "summary": "Large Language Models (LLMs) are poised to transform healthcare under China's Healthy China 2030 initiative, yet they introduce new ethical and patient-safety challenges. We present a novel 12,000-item Q&A benchmark covering 11 ethics and 9 safety dimensions in medical contexts, to quantitatively evaluate these risks. Using this dataset, we assess state-of-the-art Chinese medical LLMs (e.g., Qwen 2.5-32B, DeepSeek), revealing moderate baseline performance (accuracy 42.7% for Qwen 2.5-32B) and significant improvements after fine-tuning on our data (up to 50.8% accuracy). Results show notable gaps in LLM decision-making on ethics and safety scenarios, reflecting insufficient institutional oversight. We then identify systemic governance shortfalls-including the lack of fine-grained ethical audit protocols, slow adaptation by hospital IRBs, and insufficient evaluation tools-that currently hinder safe LLM deployment. Finally, we propose a practical governance framework for healthcare institutions (embedding LLM auditing teams, enacting data ethics guidelines, and implementing safety simulation pipelines) to proactively manage LLM risks. Our study highlights the urgent need for robust LLM governance in Chinese healthcare, aligning AI innovation with patient safety and ethical standards."
  },
  {
    "title": "Metrics that matter: Evaluating image quality metrics for medical image generation",
    "url": "http://arxiv.org/abs/2505.07175v1",
    "arxiv_id": "2505.07175v1",
    "authors": [
      "Yash Deo",
      "Yan Jia",
      "Toni Lassila",
      "William A. P. Smith",
      "Tom Lawton",
      "Siyuan Kang",
      "Alejandro F. Frangi",
      "Ibrahim Habli"
    ],
    "published": "2025-05-12T01:57:25+00:00",
    "summary": "Evaluating generative models for synthetic medical imaging is crucial yet challenging, especially given the high standards of fidelity, anatomical accuracy, and safety required for clinical applications. Standard evaluation of generated images often relies on no-reference image quality metrics when ground truth images are unavailable, but their reliability in this complex domain is not well established. This study comprehensively assesses commonly used no-reference image quality metrics using brain MRI data, including tumour and vascular images, providing a representative exemplar for the field. We systematically evaluate metric sensitivity to a range of challenges, including noise, distribution shifts, and, critically, localised morphological alterations designed to mimic clinically relevant inaccuracies. We then compare these metric scores against model performance on a relevant downstream segmentation task, analysing results across both controlled image perturbations and outputs from different generative model architectures. Our findings reveal significant limitations: many widely-used no-reference image quality metrics correlate poorly with downstream task suitability and exhibit a profound insensitivity to localised anatomical details crucial for clinical validity. Furthermore, these metrics can yield misleading scores regarding distribution shifts, e.g. data memorisation. This reveals the risk of misjudging model readiness, potentially leading to the deployment of flawed tools that could compromise patient safety. We conclude that ensuring generative models are truly fit for clinical purpose requires a multifaceted validation framework, integrating performance on relevant downstream tasks with the cautious interpretation of carefully selected no-reference image quality metrics."
  },
  {
    "title": "One Trigger Token Is Enough: A Defense Strategy for Balancing Safety and Usability in Large Language Models",
    "url": "http://arxiv.org/abs/2505.07167v1",
    "arxiv_id": "2505.07167v1",
    "authors": [
      "Haoran Gu",
      "Handing Wang",
      "Yi Mei",
      "Mengjie Zhang",
      "Yaochu Jin"
    ],
    "published": "2025-05-12T01:26:50+00:00",
    "summary": "Large Language Models (LLMs) have been extensively used across diverse domains, including virtual assistants, automated code generation, and scientific research. However, they remain vulnerable to jailbreak attacks, which manipulate the models into generating harmful responses despite safety alignment. Recent studies have shown that current safety-aligned LLMs often undergo the shallow safety alignment, where the first few tokens largely determine whether the response will be harmful. Through comprehensive observations, we find that safety-aligned LLMs and various defense strategies generate highly similar initial tokens in their refusal responses, which we define as safety trigger tokens. Building on this insight, we propose \\texttt{D-STT}, a simple yet effective defense algorithm that identifies and explicitly decodes safety trigger tokens of the given safety-aligned LLM to trigger the model's learned safety patterns. In this process, the safety trigger is constrained to a single token, which effectively preserves model usability by introducing minimum intervention in the decoding process. Extensive experiments across diverse jailbreak attacks and benign prompts demonstrate that \\ours significantly reduces output harmfulness while preserving model usability and incurring negligible response time overhead, outperforming ten baseline methods."
  },
  {
    "title": "Rigorous Evaluation of Predictive Toxicity Models by Multi-Objective Optimization of Reference Compound Lists Using Genetic Algorithms",
    "url": "http://arxiv.org/abs/2505.07140v1",
    "arxiv_id": "2505.07140v1",
    "authors": [
      "Yohei Ohto",
      "Tadahaya Mizuno",
      "Yasuhiro Yoshikai",
      "Hiromi Fujimoto",
      "Hiroyuki Kusuhara"
    ],
    "published": "2025-05-11T22:36:00+00:00",
    "summary": "In pharmaceutical safety assessments, validation studies are essential for evaluating the predictive performance and reliability of alternative methods prior to regulatory acceptance. Typically, these studies utilize reference compound lists selected to balance multiple critical factors, including chemical structure, physicochemical properties, and toxicity profiles. However, the inherent trade-offs among these criteria complicate the independent optimization of each factor, necessitating a comprehensive multi-objective optimization approach. To address this challenge, we propose a novel multi-objective optimization framework employing a Genetic Algorithm (GA) to simultaneously maximize structural, physicochemical, and toxicity diversity of reference compound lists. Applying this methodology to existing validation study datasets, we demonstrated that GA-optimized compound lists achieved significantly higher overall diversity compared to randomly generated lists. Additionally, toxicity prediction models tested on GA-optimized compound lists exhibited notably lower predictive performance compared to random selections, confirming that these lists provide a rigorous and unbiased assessment environment. These findings emphasize the potential of our GA-based method to enhance the robustness and generalizability of toxicity prediction models. Overall, our approach provides valuable support for developing balanced and rigorous reference compound lists, potentially accelerating the adoption of alternative safety assessment methods by facilitating smoother regulatory validation processes."
  },
  {
    "title": "Estimating the Lensing Probability for Binary Black Hole Mergers in AGN disk by Using Mismatch Threshold",
    "url": "http://arxiv.org/abs/2505.07114v1",
    "arxiv_id": "2505.07114v1",
    "authors": [
      "Wen-Long Xu",
      "Yu-Zhe Li",
      "Yi-Gu Chen",
      "Hui Li",
      "Wei-Hua Lei"
    ],
    "published": "2025-05-11T20:45:36+00:00",
    "summary": "Stellar-mass binary black holes (BBH) may form, evolve, and merge within the dense environments of active galactic nuclei (AGN) disks, thereby contributing to the BBH population detected by gravitational wave (GW) observatories. Mergers occurring in AGN disks may be gravitationally lensed by the supermassive black hole at the AGN centre. The probability of such lensing events has been approximately estimated by using the Einstein criterion in previous work. However, a more reasonable approach to calculating the lensing probability should be based on whether the detector can distinguish the lensed GW waveform from the unlensed one. In this work, we calculate the lensing probability of LIGO sources embedded in AGN disk by relating threshold mismatch to the signal-to-noise ratio of the observed events. For the sensitivity of LIGO-Virgo-KAGRA O3 observation runs, our results indicate that the lensing probability is several times higher than previous estimates. If AGNs are indeed the primary formation channel for BBHs, we could quantify the probability of detecting the lensed GW events in such a scenario. The non-detections, on the other hand, will place stricter constraints on the fraction of AGN disk BBHs and even the birthplaces of BBH mergers."
  },
  {
    "title": "Constrained Online Decision-Making with Density Estimation Oracles",
    "url": "http://arxiv.org/abs/2505.07101v1",
    "arxiv_id": "2505.07101v1",
    "authors": [
      "Haichen Hu",
      "David Simchi-Levi",
      "Navid Azizan"
    ],
    "published": "2025-05-11T19:22:04+00:00",
    "summary": "Contextual online decision-making problems with constraints appear in a wide range of real-world applications, such as personalized recommendation with resource limits, adaptive experimental design, and decision-making under safety or fairness requirements. In this paper, we investigate a general formulation of sequential decision-making with stage-wise feasibility constraints, where at each round, the learner must select an action based on observed context while ensuring that a problem-specific feasibility criterion is satisfied. We propose a unified algorithmic framework that captures many existing constrained learning problems, including constrained bandits, active learning with label budgets, online hypothesis testing with Type I error control, and model calibration. Central to our approach is the concept of upper counterfactual confidence bounds, which enables the design of practically efficient online algorithms with strong theoretical guarantee using any offline conditional density estimation oracle. Technically, to handle feasibility constraints in complex environments, we introduce a generalized notion of the eluder dimension - extending it from the classical setting based on square loss to a broader class of metric-like probability divergences. This allows us to capture the complexity of various density function classes and characterize the utility regret incurred due to feasibility constraint uncertainty. Our result offers a principled foundation for constrained sequential decision-making in both theory and practice."
  },
  {
    "title": "A Rapid Reconstruction Method of Gamma Radiation Field based on Normalized Proper Orthogonal Decomposition",
    "url": "http://arxiv.org/abs/2505.07088v1",
    "arxiv_id": "2505.07088v1",
    "authors": [
      "Kai Tan",
      "Hojoon Son",
      "Fan Zhang"
    ],
    "published": "2025-05-11T18:37:28+00:00",
    "summary": "When a fault occurs in nuclear facilities, accurately reconstructing gamma radiation fields through measurements from the mobile radiation detection (MRD) system becomes crucial to enable access to internal facility areas for essential safety assessments and repairs. Reconstruction of these fields is difficult because of the uncertainty in the positions and intensities of the gamma sources, the complexity of the gamma distribution, and the physics and radiation hardness constraints on the MRD systems. In this work, a novel reconstruction framework of the gamma radiation is proposed. This system entails a NPOD-based reconstruction algorithm with MRD data, and a variation-based adaptive measurements selection mechanism. Our approach has been thoroughly assessed using extensive simulations, and the results clearly prove its success and efficiency in reconstruction radiation fields accurately and quickly. Furthermore, the designed selection algorithm is also promising for extensive application to other optimization tasks of location selection."
  },
  {
    "title": "DriveSOTIF: Advancing Perception SOTIF Through Multimodal Large Language Models",
    "url": "http://arxiv.org/abs/2505.07084v1",
    "arxiv_id": "2505.07084v1",
    "authors": [
      "Shucheng Huang",
      "Freda Shi",
      "Chen Sun",
      "Jiaming Zhong",
      "Minghao Ning",
      "Yufeng Yang",
      "Yukun Lu",
      "Hong Wang",
      "Amir Khajepour"
    ],
    "published": "2025-05-11T18:14:33+00:00",
    "summary": "Human drivers naturally possess the ability to perceive driving scenarios, predict potential hazards, and react instinctively due to their spatial and causal intelligence, which allows them to perceive, understand, predict, and interact with the 3D world both spatially and temporally. Autonomous vehicles, however, lack these capabilities, leading to challenges in effectively managing perception-related Safety of the Intended Functionality (SOTIF) risks, particularly in complex and unpredictable driving conditions. To address this gap, we propose an approach that fine-tunes multimodal language models (MLLMs) on a customized dataset specifically designed to capture perception-related SOTIF scenarios. Model benchmarking demonstrates that this tailored dataset enables the models to better understand and respond to these complex driving situations. Additionally, in real-world case studies, the proposed method correctly handles challenging scenarios that even human drivers may find difficult. Real-time performance tests further indicate the potential for the models to operate efficiently in live driving environments. This approach, along with the dataset generation pipeline, shows significant promise for improving the identification, cognition, prediction, and reaction to SOTIF-related risks in autonomous driving systems. The dataset and information are available: https://github.com/s95huang/DriveSOTIF.git"
  },
  {
    "title": "YOPOv2-Tracker: An End-to-End Agile Tracking and Navigation Framework from Perception to Action",
    "url": "http://arxiv.org/abs/2505.06923v1",
    "arxiv_id": "2505.06923v1",
    "authors": [
      "Junjie Lu",
      "Yulin Hui",
      "Xuewei Zhang",
      "Wencan Feng",
      "Hongming Shen",
      "Zhiyu Li",
      "Bailing Tian"
    ],
    "published": "2025-05-11T09:53:34+00:00",
    "summary": "Traditional target tracking pipelines including detection, mapping, navigation, and control are comprehensive but introduce high latency, limitting the agility of quadrotors. On the contrary, we follow the design principle of \"less is more\", striving to simplify the process while maintaining effectiveness. In this work, we propose an end-to-end agile tracking and navigation framework for quadrotors that directly maps the sensory observations to control commands. Importantly, leveraging the multimodal nature of navigation and detection tasks, our network maintains interpretability by explicitly integrating the independent modules of the traditional pipeline, rather than a crude action regression. In detail, we adopt a set of motion primitives as anchors to cover the searching space regarding the feasible region and potential target. Then we reformulate the trajectory optimization as regression of primitive offsets and associated costs considering the safety, smoothness, and other metrics. For tracking task, the trajectories are expected to approach the target and additional objectness scores are predicted. Subsequently, the predictions, after compensation for the estimated lumped disturbance, are transformed into thrust and attitude as control commands for swift response. During training, we seamlessly integrate traditional motion planning with deep learning by directly back-propagating the gradients of trajectory costs to the network, eliminating the need for expert demonstration in imitation learning and providing more direct guidance than reinforcement learning. Finally, we deploy the algorithm on a compact quadrotor and conduct real-world validations in both forest and building environments to demonstrate the efficiency of the proposed method."
  },
  {
    "title": "Realistic Counterfactual Explanations for Machine Learning-Controlled Mobile Robots using 2D LiDAR",
    "url": "http://arxiv.org/abs/2505.06906v1",
    "arxiv_id": "2505.06906v1",
    "authors": [
      "Sindre Benjamin Remman",
      "Anastasios M. Lekkas"
    ],
    "published": "2025-05-11T08:55:17+00:00",
    "summary": "This paper presents a novel method for generating realistic counterfactual explanations (CFEs) in machine learning (ML)-based control for mobile robots using 2D LiDAR. ML models, especially artificial neural networks (ANNs), can provide advanced decision-making and control capabilities by learning from data. However, they often function as black boxes, making it challenging to interpret them. This is especially a problem in safety-critical control applications. To generate realistic CFEs, we parameterize the LiDAR space with simple shapes such as circles and rectangles, whose parameters are chosen by a genetic algorithm, and the configurations are transformed into LiDAR data by raycasting. Our model-agnostic approach generates CFEs in the form of synthetic LiDAR data that resembles a base LiDAR state but is modified to produce a pre-defined ML model control output based on a query from the user. We demonstrate our method on a mobile robot, the TurtleBot3, controlled using deep reinforcement learning (DRL) in real-world and simulated scenarios. Our method generates logical and realistic CFEs, which helps to interpret the DRL agent's decision making. This paper contributes towards advancing explainable AI in mobile robotics, and our method could be a tool for understanding, debugging, and improving ML-based autonomous control."
  },
  {
    "title": "Towards Human-Centric Autonomous Driving: A Fast-Slow Architecture Integrating Large Language Model Guidance with Reinforcement Learning",
    "url": "http://arxiv.org/abs/2505.06875v1",
    "arxiv_id": "2505.06875v1",
    "authors": [
      "Chengkai Xu",
      "Jiaqi Liu",
      "Yicheng Guo",
      "Yuhang Zhang",
      "Peng Hang",
      "Jian Sun"
    ],
    "published": "2025-05-11T06:55:54+00:00",
    "summary": "Autonomous driving has made significant strides through data-driven techniques, achieving robust performance in standardized tasks. However, existing methods frequently overlook user-specific preferences, offering limited scope for interaction and adaptation with users. To address these challenges, we propose a \"fast-slow\" decision-making framework that integrates a Large Language Model (LLM) for high-level instruction parsing with a Reinforcement Learning (RL) agent for low-level real-time decision. In this dual system, the LLM operates as the \"slow\" module, translating user directives into structured guidance, while the RL agent functions as the \"fast\" module, making time-critical maneuvers under stringent latency constraints. By decoupling high-level decision making from rapid control, our framework enables personalized user-centric operation while maintaining robust safety margins. Experimental evaluations across various driving scenarios demonstrate the effectiveness of our method. Compared to baseline algorithms, the proposed architecture not only reduces collision rates but also aligns driving behaviors more closely with user preferences, thereby achieving a human-centric mode. By integrating user guidance at the decision level and refining it with real-time control, our framework bridges the gap between individual passenger needs and the rigor required for safe, reliable driving in complex traffic environments."
  },
  {
    "title": "Predicting Surgical Safety Margins in Osteosarcoma Knee Resections: An Unsupervised Approach",
    "url": "http://arxiv.org/abs/2505.06853v1",
    "arxiv_id": "2505.06853v1",
    "authors": [
      "Carolina Vargas-Ecos",
      "Edwin Salcedo"
    ],
    "published": "2025-05-11T05:41:19+00:00",
    "summary": "According to the Pan American Health Organization, the number of cancer cases in Latin America was estimated at 4.2 million in 2022 and is projected to rise to 6.7 million by 2045. Osteosarcoma, one of the most common and deadly bone cancers affecting young people, is difficult to detect due to its unique texture and intensity. Surgical removal of osteosarcoma requires precise safety margins to ensure complete resection while preserving healthy tissue. Therefore, this study proposes a method for estimating the confidence interval of surgical safety margins in osteosarcoma surgery around the knee. The proposed approach uses MRI and X-ray data from open-source repositories, digital processing techniques, and unsupervised learning algorithms (such as k-means clustering) to define tumor boundaries. Experimental results highlight the potential for automated, patient-specific determination of safety margins."
  },
  {
    "title": "Secure Safety Filter: Towards Safe Flight Control under Sensor Attacks",
    "url": "http://arxiv.org/abs/2505.06845v1",
    "arxiv_id": "2505.06845v1",
    "authors": [
      "Xiao Tan",
      "Junior Sundar",
      "Renzo Bruzzone",
      "Pio Ong",
      "Willian T. Lunardi",
      "Martin Andreoni",
      "Paulo Tabuada",
      "Aaron D. Ames"
    ],
    "published": "2025-05-11T05:05:05+00:00",
    "summary": "Modern autopilot systems are prone to sensor attacks that can jeopardize flight safety. To mitigate this risk, we proposed a modular solution: the secure safety filter, which extends the well-established control barrier function (CBF)-based safety filter to account for, and mitigate, sensor attacks. This module consists of a secure state reconstructor (which generates plausible states) and a safety filter (which computes the safe control input that is closest to the nominal one). Differing from existing work focusing on linear, noise-free systems, the proposed secure safety filter handles bounded measurement noise and, by leveraging reduced-order model techniques, is applicable to the nonlinear dynamics of drones. Software-in-the-loop simulations and drone hardware experiments demonstrate the effectiveness of the secure safety filter in rendering the system safe in the presence of sensor attacks."
  },
  {
    "title": "Benign Samples Matter! Fine-tuning On Outlier Benign Samples Severely Breaks Safety",
    "url": "http://arxiv.org/abs/2505.06843v1",
    "arxiv_id": "2505.06843v1",
    "authors": [
      "Zihan Guan",
      "Mengxuan Hu",
      "Ronghang Zhu",
      "Sheng Li",
      "Anil Vullikanti"
    ],
    "published": "2025-05-11T04:59:20+00:00",
    "summary": "Recent studies have uncovered a troubling vulnerability in the fine-tuning stage of large language models (LLMs): even fine-tuning on entirely benign datasets can lead to a significant increase in the harmfulness of LLM outputs. Building on this finding, our red teaming study takes this threat one step further by developing a more effective attack. Specifically, we analyze and identify samples within benign datasets that contribute most to safety degradation, then fine-tune LLMs exclusively on these samples. We approach this problem from an outlier detection perspective and propose Self-Inf-N, to detect and extract outliers for fine-tuning. Our findings reveal that fine-tuning LLMs on 100 outlier samples selected by Self-Inf-N in the benign datasets severely compromises LLM safety alignment. Extensive experiments across seven mainstream LLMs demonstrate that our attack exhibits high transferability across different architectures and remains effective in practical scenarios. Alarmingly, our results indicate that most existing mitigation strategies fail to defend against this attack, underscoring the urgent need for more robust alignment safeguards. Codes are available at https://github.com/GuanZihan/Benign-Samples-Matter."
  },
  {
    "title": "Secure Safety Filter Design for Sampled-data Nonlinear Systems under Sensor Spoofing Attacks",
    "url": "http://arxiv.org/abs/2505.06842v1",
    "arxiv_id": "2505.06842v1",
    "authors": [
      "Xiao Tan",
      "Pio Ong",
      "Paulo Tabuada",
      "Aaron D. Ames"
    ],
    "published": "2025-05-11T04:54:37+00:00",
    "summary": "This paper presents a secure safety filter design for nonlinear systems under sensor spoofing attacks. Existing approaches primarily focus on linear systems which limits their applications in real-world scenarios. In this work, we extend these results to nonlinear systems in a principled way. We introduce exact observability maps that abstract specific state estimation algorithms and extend them to a secure version capable of handling sensor attacks. Our generalization also applies to the relaxed observability case, with slightly relaxed guarantees. More importantly, we propose a secure safety filter design in both exact and relaxed cases, which incorporates secure state estimation and a control barrier function-enabled safety filter. The proposed approach provides theoretical safety guarantees for nonlinear systems in the presence of sensor attacks. We numerically validate our analysis on a unicycle vehicle equipped with redundant yet partly compromised sensors."
  },
  {
    "title": "Control Plane as a Tool: A Scalable Design Pattern for Agentic AI Systems",
    "url": "http://arxiv.org/abs/2505.06817v1",
    "arxiv_id": "2505.06817v1",
    "authors": [
      "Sivasathivel Kandasamy"
    ],
    "published": "2025-05-11T02:58:50+00:00",
    "summary": "Agentic AI systems represent a new frontier in artificial intelligence, where agents often based on large language models(LLMs) interact with tools, environments, and other agents to accomplish tasks with a degree of autonomy. These systems show promise across a range of domains, but their architectural underpinnings remain immature. This paper conducts a comprehensive review of the types of agents, their modes of interaction with the environment, and the infrastructural and architectural challenges that emerge. We identify a gap in how these systems manage tool orchestration at scale and propose a reusable design abstraction: the \"Control Plane as a Tool\" pattern. This pattern allows developers to expose a single tool interface to an agent while encapsulating modular tool routing logic behind it. We position this pattern within the broader context of agent design and argue that it addresses several key challenges in scaling, safety, and extensibility."
  },
  {
    "title": "Dynamic Safety in Complex Environments: Synthesizing Safety Filters with Poisson's Equation",
    "url": "http://arxiv.org/abs/2505.06794v1",
    "arxiv_id": "2505.06794v1",
    "authors": [
      "Gilbert Bahati",
      "Ryan M. Bena",
      "Aaron D. Ames"
    ],
    "published": "2025-05-11T00:21:29+00:00",
    "summary": "Synthesizing safe sets for robotic systems operating in complex and dynamically changing environments is a challenging problem. Solving this problem can enable the construction of safety filters that guarantee safe control actions -- most notably by employing Control Barrier Functions (CBFs). This paper presents an algorithm for generating safe sets from perception data by leveraging elliptic partial differential equations, specifically Poisson's equation. Given a local occupancy map, we solve Poisson's equation subject to Dirichlet boundary conditions, with a novel forcing function. Specifically, we design a smooth guidance vector field, which encodes gradient information required for safety. The result is a variational problem for which the unique minimizer -- a safety function -- characterizes the safe set. After establishing our theoretical result, we illustrate how safety functions can be used in CBF-based safety filtering. The real-time utility of our synthesis method is highlighted through hardware demonstrations on quadruped and humanoid robots navigating dynamically changing obstacle-filled environments."
  },
  {
    "title": "Digital-physical testbed for ship autonomy studies in the Marine Cybernetics Laboratory basin",
    "url": "http://arxiv.org/abs/2505.06787v1",
    "arxiv_id": "2505.06787v1",
    "authors": [
      "Emir Cem Gezer",
      "Mael Korentin Ivan Moreau",
      "Anders Sandneseng H\u00f8gden",
      "Dong Trong Nguyen",
      "Roger Skjetne",
      "Asgeir S\u00f8rensen"
    ],
    "published": "2025-05-10T23:48:32+00:00",
    "summary": "The algorithms developed for Maritime Autonomous Surface Ships (MASS) are often challenging to test on actual vessels due to high operational costs and safety considerations. Simulations offer a cost-effective alternative and eliminate risks, but they may not accurately represent real-world dynamics for the given tasks. Utilizing small-scale model ships and robotic vessels in conjunction with a laboratory basin provides an accessible testing environment for the early stages of validation processes. However, designing and developing a model vessel for a single test can be costly and cumbersome, and often researchers lack availability to such infrastructure. To address these challenges and enable streamlined testing, we have developed an in-house testbed that facilitates the development, testing, verification, and validation of MASS algorithms in a digital-physical laboratory. This infrastructure includes a set of small-scale model vessels, a simulation environment for each vessel, a comprehensive testbed environment, and a digital twin in Unity. With this, we aim to establish a full design and verification pipeline that starts with high-fidelity simulation models of each model vessel, to the model-scale testing in the laboratory basin, allowing possibilities for moving to semi-fullscale validation with the R/V milliAmpere 1 passenger ferry and full-scale validation using the R/V Gunnerus. In this work, we present our progress on the development of this testbed environment and its components, demonstrating its effectiveness in enabling ship guidance, navigation, and control (GNC) including autonomy."
  },
  {
    "title": "Work-in-Progress: Multi-Deadline DAG Scheduling Model for Autonomous Driving Systems",
    "url": "http://arxiv.org/abs/2505.06780v1",
    "arxiv_id": "2505.06780v1",
    "authors": [
      "Atsushi Yano",
      "Takuya Azumi"
    ],
    "published": "2025-05-10T23:29:35+00:00",
    "summary": "Autoware is an autonomous driving system implemented on Robot Operation System (ROS) 2, where an end-to-end timing guarantee is crucial to ensure safety. However, existing ROS 2 cause-effect chain models for analyzing end-to-end latency struggle to accurately represent the complexities of Autoware, particularly regarding sync callbacks, queue consumption patterns, and feedback loops. To address these problems, we propose a new scheduling model that decomposes the end-to-end timing constraints of Autoware into local relative deadlines for each sub-DAG. This multi-deadline DAG scheduling model avoids the need for complex analysis of data flows through queues and loops, while ensuring that all callbacks receive data within correct intervals. Furthermore, we extend the Global Earliest Deadline First (GEDF) algorithm for the proposed model and evaluate its effectiveness using a synthetic workload derived from Autoware."
  },
  {
    "title": "JaxRobotarium: Training and Deploying Multi-Robot Policies in 10 Minutes",
    "url": "http://arxiv.org/abs/2505.06771v1",
    "arxiv_id": "2505.06771v1",
    "authors": [
      "Shalin Anand Jain",
      "Jiazhen Liu",
      "Siva Kailas",
      "Harish Ravichandar"
    ],
    "published": "2025-05-10T22:38:39+00:00",
    "summary": "Multi-agent reinforcement learning (MARL) has emerged as a promising solution for learning complex and scalable coordination behaviors in multi-robot systems. However, established MARL platforms (e.g., SMAC and MPE) lack robotics relevance and hardware deployment, leaving multi-robot learning researchers to develop bespoke environments and hardware testbeds dedicated to the development and evaluation of their individual contributions. The Multi-Agent RL Benchmark and Learning Environment for the Robotarium (MARBLER) is an exciting recent step in providing a standardized robotics-relevant platform for MARL, by bridging the Robotarium testbed with existing MARL software infrastructure. However, MARBLER lacks support for parallelization and GPU/TPU execution, making the platform prohibitively slow compared to modern MARL environments and hindering adoption. We contribute JaxRobotarium, a Jax-powered end-to-end simulation, learning, deployment, and benchmarking platform for the Robotarium. JaxRobotarium enables rapid training and deployment of multi-robot reinforcement learning (MRRL) policies with realistic robot dynamics and safety constraints, supporting both parallelization and hardware acceleration. Our generalizable learning interface provides an easy-to-use integration with SOTA MARL libraries (e.g., JaxMARL). In addition, JaxRobotarium includes eight standardized coordination scenarios, including four novel scenarios that bring established MARL benchmark tasks (e.g., RWARE and Level-Based Foraging) to a realistic robotics setting. We demonstrate that JaxRobotarium retains high simulation fidelity while achieving dramatic speedups over baseline (20x in training and 150x in simulation), and provides an open-access sim-to-real evaluation pipeline through the Robotarium testbed, accelerating and democratizing access to multi-robot learning research and evaluation."
  },
  {
    "title": "Investigating Robotaxi Crash Severity Using Geographical Random Forest",
    "url": "http://arxiv.org/abs/2505.06762v1",
    "arxiv_id": "2505.06762v1",
    "authors": [
      "Junfeng Jiao",
      "Seung Gyu Baik",
      "Seung Jun Choi",
      "Yiming Xu"
    ],
    "published": "2025-05-10T21:47:01+00:00",
    "summary": "This paper quantitatively investigates the crash severity of Autonomous Vehicles (AVs) with spatially localized machine learning and macroscopic measures of the urban built environment. We address spatial heterogeneity and spatial autocorrelation, while focusing on land use patterns and human behavior. Our Geographical Random Forest (GRF) model, accompanied with a crash severity risk map of San Francisco, presents three findings that are useful for commercial operations of AVs and robotaxis. First, spatially localized machine learning performed better than regular machine learning, when predicting AV crash severity. Bias-variance tradeoff was evident as we adjust the localization weight hyperparameter. Second, land use was the most important built environment measure, compared to intersections, building footprints, public transit stops, and Points Of Interests (POIs). Third, it was predicted that city center areas with greater diversity and commercial activities were more likely to result in low-severity AV crashes, than residential neighborhoods. Residential land use may be associated with higher severity due to human behavior and less restrictive environment. This paper recommends to explicitly consider geographic locations, and to design safety measures specific to residential neighborhoods, when robotaxi operators train their AV systems."
  },
  {
    "title": "AI-CDA4All: Democratizing Cooperative Autonomous Driving for All Drivers via Affordable Dash-cam Hardware and Open-source AI Software",
    "url": "http://arxiv.org/abs/2505.06749v1",
    "arxiv_id": "2505.06749v1",
    "authors": [
      "Shengming Yuan",
      "Hao Zhou"
    ],
    "published": "2025-05-10T20:17:05+00:00",
    "summary": "As transportation technology advances, the demand for connected vehicle infrastructure has greatly increased to improve their efficiency and safety. One area of advancement, Cooperative Driving Automation (CDA) still relies on expensive autonomy sensors or connectivity units and are not interoperable across existing market car makes/models, limiting its scalability on public roads. To fill these gaps, this paper presents a novel approach to democratizing CDA technology, it leverages low-cost, commercially available edge devices such as vehicle dash-cams and open-source software to make the technology accessible and scalable to be used in transportation infrastructure and broader public domains. This study also investigates the feasibility of utilizing cost-effective communication protocols based on LTE and WiFi. These technologies enable lightweight Vehicle-to-Everything (V2X) communications, facilitating real-time data exchange between vehicles and infrastructure. Our research and development efforts are aligned with industrial standards to ensure compatibility and future integration into existing transportation ecosystems. By prioritizing infrastructure-oriented applications, such as improved traffic flow management, this approach seeks to deliver tangible societal benefits without directly competing with vehicle OEMs. As recent advancement of Generative AI (GenAI), there is no standardized integration of GenAI technologies into open-source CDAs, as the current trends of muiltimodal large language models gain popularity, we demonstrated a feasible locally deployed edge LLM models can enhance driving experience while preserving privacy and security compared to cloud-connected solutions. The proposed system underscores the potential of low-cost, scalable solutions in advancing CDA functionality, paving the way for smarter, safer, and more inclusive transportation networks."
  },
  {
    "title": "Balancing Progress and Safety: A Novel Risk-Aware Objective for RL in Autonomous Driving",
    "url": "http://arxiv.org/abs/2505.06737v1",
    "arxiv_id": "2505.06737v1",
    "authors": [
      "Ahmed Abouelazm",
      "Jonas Michel",
      "Helen Gremmelmaier",
      "Tim Joseph",
      "Philip Sch\u00f6rner",
      "J. Marius Z\u00f6llner"
    ],
    "published": "2025-05-10T19:05:03+00:00",
    "summary": "Reinforcement Learning (RL) is a promising approach for achieving autonomous driving due to robust decision-making capabilities. RL learns a driving policy through trial and error in traffic scenarios, guided by a reward function that combines the driving objectives. The design of such reward function has received insufficient attention, yielding ill-defined rewards with various pitfalls. Safety, in particular, has long been regarded only as a penalty for collisions. This leaves the risks associated with actions leading up to a collision unaddressed, limiting the applicability of RL in real-world scenarios. To address these shortcomings, our work focuses on enhancing the reward formulation by defining a set of driving objectives and structuring them hierarchically. Furthermore, we discuss the formulation of these objectives in a normalized manner to transparently determine their contribution to the overall reward. Additionally, we introduce a novel risk-aware objective for various driving interactions based on a two-dimensional ellipsoid function and an extension of Responsibility-Sensitive Safety (RSS) concepts. We evaluate the efficacy of our proposed reward in unsignalized intersection scenarios with varying traffic densities. The approach decreases collision rates by 21\\% on average compared to baseline rewards and consistently surpasses them in route progress and cumulative reward, demonstrating its capability to promote safer driving behaviors while maintaining high-performance levels."
  },
  {
    "title": "E2E-FANet: A Highly Generalizable Framework for Waves prediction Behind Floating Breakwaters via Exogenous-to-Endogenous Variable Attention",
    "url": "http://arxiv.org/abs/2505.06690v1",
    "arxiv_id": "2505.06690v1",
    "authors": [
      "Jianxin Zhang",
      "Lianzi Jiang",
      "Xinyu Han",
      "Xiangrong Wang",
      "Weinan Huang"
    ],
    "published": "2025-05-10T16:28:48+00:00",
    "summary": "Accurate prediction of waves behind floating breakwaters (FB) is crucial for optimizing coastal engineering structures, enhancing safety, and improving design efficiency. Existing methods demonstrate limitations in capturing nonlinear interactions between waves and structures, while exhibiting insufficient capability in modeling the complex frequency-domain relationships among elevations of different wave gauges. To address these challenges, this study introduces the Exogenous-to-Endogenous Frequency-Aware Network (E2E-FANet), a novel end-to-end neural network designed to model relationships between waves and structures. The E2E-FANetarchitecture incorporates a Dual-Basis Frequency Mapping (DBFM) module that leverages orthogonal cosine and sine bases to extract wave features from the frequency domain while preserving temporal information. Additionally, we introduce the Exogenous-to-Endogenous Cross-Attention (E2ECA) module, which employs cross attention to model the interactions between endogenous and exogenous variables. We incorporate a Temporal-wise Attention (TA) mechanism that adaptively captures complex dependencies in endogenous variables. These integrated modules function synergistically, enabling E2E-FANet to achieve both comprehensive feature perception in the time-frequency domain and precise modeling of wave-structure interactions. To comprehensively evaluate the performance of E2E-FANet, we constructed a multi-level validation framework comprising three distinct testing scenarios: internal validation under identical wave conditions, generalization testing across different wave conditions, and adaptability testing with varying relative water density (RW) conditions. These comprehensive tests demonstrate that E2E-FANet provides accurate waves behind FB predictions while successfully generalizing diverse wave conditions."
  },
  {
    "title": "Extend IVerilog to Support Batch RTL Fault Simulation",
    "url": "http://arxiv.org/abs/2505.06687v1",
    "arxiv_id": "2505.06687v1",
    "authors": [
      "Jiaping Tang",
      "Jianan Mu",
      "Zizhen Liu",
      "Zhiteng Chao",
      "Jing Ye",
      "Huawei Li"
    ],
    "published": "2025-05-10T16:24:40+00:00",
    "summary": "The advancement of functional safety has made RTL-level fault simulation increasingly important to achieve iterative efficiency in the early stages of design and to ensure compliance with functional safety standards. In this paper, we extend IVerilog to support batch RTL fault simulation and integrate the event-driven algorithm and the concurrent fault simulation algorithm. Comparative experiments with a state-of-the-art commercial simulator and an open-source RTL fault simulator demonstrate that our simulator achieves a performance improvement of 2.2$\\times$ and 3.4$\\times$, respectively."
  },
  {
    "title": "A Survey on Data-Driven Modeling of Human Drivers' Lane-Changing Decisions",
    "url": "http://arxiv.org/abs/2505.06680v1",
    "arxiv_id": "2505.06680v1",
    "authors": [
      "Linxuan Huang",
      "Dong-Fan Xie",
      "Li Li",
      "Zhengbing He"
    ],
    "published": "2025-05-10T16:09:03+00:00",
    "summary": "Lane-changing (LC) behavior, a critical yet complex driving maneuver, significantly influences driving safety and traffic dynamics. Traditional analytical LC decision (LCD) models, while effective in specific environments, often oversimplify behavioral heterogeneity and complex interactions, limiting their capacity to capture real LCD. Data-driven approaches address these gaps by leveraging rich empirical data and machine learning to decode latent decision-making patterns, enabling adaptive LCD modeling in dynamic environments. In light of the rapid development of artificial intelligence and the demand for data-driven models oriented towards connected vehicles and autonomous vehicles, this paper presents a comprehensive survey of data-driven LCD models, with a particular focus on human drivers LC decision-making. It systematically reviews the modeling framework, covering data sources and preprocessing, model inputs and outputs, objectives, structures, and validation methods. This survey further discusses the opportunities and challenges faced by data-driven LCD models, including driving safety, uncertainty, as well as the integration and improvement of technical frameworks."
  },
  {
    "title": "Jailbreaking the Text-to-Video Generative Models",
    "url": "http://arxiv.org/abs/2505.06679v1",
    "arxiv_id": "2505.06679v1",
    "authors": [
      "Jiayang Liu",
      "Siyuan Liang",
      "Shiqian Zhao",
      "Rongcheng Tu",
      "Wenbo Zhou",
      "Xiaochun Cao",
      "Dacheng Tao",
      "Siew Kei Lam"
    ],
    "published": "2025-05-10T16:04:52+00:00",
    "summary": "Text-to-video generative models have achieved significant progress, driven by the rapid advancements in diffusion models, with notable examples including Pika, Luma, Kling, and Sora. Despite their remarkable generation ability, their vulnerability to jailbreak attack, i.e. to generate unsafe content, including pornography, violence, and discrimination, raises serious safety concerns. Existing efforts, such as T2VSafetyBench, have provided valuable benchmarks for evaluating the safety of text-to-video models against unsafe prompts but lack systematic studies for exploiting their vulnerabilities effectively. In this paper, we propose the \\textit{first} optimization-based jailbreak attack against text-to-video models, which is specifically designed. Our approach formulates the prompt generation task as an optimization problem with three key objectives: (1) maximizing the semantic similarity between the input and generated prompts, (2) ensuring that the generated prompts can evade the safety filter of the text-to-video model, and (3) maximizing the semantic similarity between the generated videos and the original input prompts. To further enhance the robustness of the generated prompts, we introduce a prompt mutation strategy that creates multiple prompt variants in each iteration, selecting the most effective one based on the averaged score. This strategy not only improves the attack success rate but also boosts the semantic relevance of the generated video. We conduct extensive experiments across multiple text-to-video models, including Open-Sora, Pika, Luma, and Kling. The results demonstrate that our method not only achieves a higher attack success rate compared to baseline methods but also generates videos with greater semantic similarity to the original input prompts."
  },
  {
    "title": "A Formal Verification Approach to Safeguard Controller Variables from Single Event Upset",
    "url": "http://arxiv.org/abs/2505.06648v1",
    "arxiv_id": "2505.06648v1",
    "authors": [
      "Ganesha",
      "Sujit Kumar Chakrabarti"
    ],
    "published": "2025-05-10T13:56:02+00:00",
    "summary": "We present a method based on program analysis and formal verification to identify conditionally relevant variables (CRVs) - variables which could lead to violation of safety properties in control software when affected by single event upsets (SEUs). Traditional static analysis can distinguish between relevant and irrelevant variables. However, it would fail to take into account the conditions specific to the control software in question. This can lead to false positives. Our algorithm employs formal verification to avoid false positives. We have conducted experiments that demonstrate that CRVs indeed are fewer in number than what traditional static analysis can detect and that our algorithm is able to identify this fact. The information provided by our algorithm could prove helpful to a compiler while it does register allocation during the compilation of the control software. In turn, this could cause significant reduction in the cost of controller chips."
  },
  {
    "title": "Practical Reasoning Interruption Attacks on Reasoning Large Language Models",
    "url": "http://arxiv.org/abs/2505.06643v1",
    "arxiv_id": "2505.06643v1",
    "authors": [
      "Yu Cui",
      "Cong Zuo"
    ],
    "published": "2025-05-10T13:36:01+00:00",
    "summary": "Reasoning large language models (RLLMs) have demonstrated outstanding performance across a variety of tasks, yet they also expose numerous security vulnerabilities. Most of these vulnerabilities have centered on the generation of unsafe content. However, recent work has identified a distinct \"thinking-stopped\" vulnerability in DeepSeek-R1: under adversarial prompts, the model's reasoning process ceases at the system level and produces an empty final answer. Building upon this vulnerability, researchers developed a novel prompt injection attack, termed reasoning interruption attack, and also offered an initial analysis of its root cause. Through extensive experiments, we verify the previous analyses, correct key errors based on three experimental findings, and present a more rigorous explanation of the fundamental causes driving the vulnerability. Moreover, existing attacks typically require over 2,000 tokens, impose significant overhead, reduce practicality, and are easily detected. To overcome these limitations, we propose the first practical reasoning interruption attack. It succeeds with just 109 tokens by exploiting our newly uncovered \"reasoning token overflow\" (RTO) effect to overwrite the model's final answer, forcing it to return an invalid response. Experimental results demonstrate that our proposed attack is highly effective. Furthermore, we discover that the method for triggering RTO differs between the official DeepSeek-R1 release and common unofficial deployments. As a broadened application of RTO, we also construct a novel jailbreak attack that enables the transfer of unsafe content within the reasoning tokens into final answer, thereby exposing it to the user. Our work carries significant implications for enhancing the security of RLLMs."
  },
  {
    "title": "LLMs Outperform Experts on Challenging Biology Benchmarks",
    "url": "http://arxiv.org/abs/2505.06108v1",
    "arxiv_id": "2505.06108v1",
    "authors": [
      "Lennart Justen"
    ],
    "published": "2025-05-09T15:05:57+00:00",
    "summary": "This study systematically evaluates 27 frontier Large Language Models on eight diverse biology benchmarks spanning molecular biology, genetics, cloning, virology, and biosecurity. Models from major AI developers released between November 2022 and April 2025 were assessed through ten independent runs per benchmark. The findings reveal dramatic improvements in biological capabilities. Top model performance increased more than 4-fold on the challenging text-only subset of the Virology Capabilities Test over the study period, with the top model now performing twice as well as expert virologists. Several models now match or exceed expert-level performance on other challenging benchmarks, including LAB-Bench CloningScenarios and the biology subsets of GPQA and WMDP. Contrary to expectations, chain-of-thought did not substantially improve performance over zero-shot evaluation, while extended reasoning features in o3-mini and Claude 3.7 Sonnet typically improved performance as predicted by inference scaling. Benchmarks such as PubMedQA and the MMLU and WMDP biology subsets exhibited performance plateaus well below 100%, suggesting benchmark saturation and errors in the underlying benchmark data. The analysis highlights the need for more sophisticated evaluation methodologies as AI systems continue to advance."
  },
  {
    "title": "Safe-EF: Error Feedback for Nonsmooth Constrained Optimization",
    "url": "http://arxiv.org/abs/2505.06053v1",
    "arxiv_id": "2505.06053v1",
    "authors": [
      "Rustem Islamov",
      "Yarden As",
      "Ilyas Fatkhullin"
    ],
    "published": "2025-05-09T13:49:05+00:00",
    "summary": "Federated learning faces severe communication bottlenecks due to the high dimensionality of model updates. Communication compression with contractive compressors (e.g., Top-K) is often preferable in practice but can degrade performance without proper handling. Error feedback (EF) mitigates such issues but has been largely restricted for smooth, unconstrained problems, limiting its real-world applicability where non-smooth objectives and safety constraints are critical. We advance our understanding of EF in the canonical non-smooth convex setting by establishing new lower complexity bounds for first-order algorithms with contractive compression. Next, we propose Safe-EF, a novel algorithm that matches our lower bound (up to a constant) while enforcing safety constraints essential for practical applications. Extending our approach to the stochastic setting, we bridge the gap between theory and practical implementation. Extensive experiments in a reinforcement learning setup, simulating distributed humanoid robot training, validate the effectiveness of Safe-EF in ensuring safety and reducing communication complexity."
  },
  {
    "title": "Healthy LLMs? Benchmarking LLM Knowledge of UK Government Public Health Information",
    "url": "http://arxiv.org/abs/2505.06046v1",
    "arxiv_id": "2505.06046v1",
    "authors": [
      "Joshua Harris",
      "Fan Grayson",
      "Felix Feldman",
      "Timothy Laurence",
      "Toby Nonnenmacher",
      "Oliver Higgins",
      "Leo Loman",
      "Selina Patel",
      "Thomas Finnie",
      "Samuel Collins",
      "Michael Borowitz"
    ],
    "published": "2025-05-09T13:42:59+00:00",
    "summary": "As Large Language Models (LLMs) become widely accessible, a detailed understanding of their knowledge within specific domains becomes necessary for successful real world use. This is particularly critical in public health, where failure to retrieve relevant, accurate, and current information could significantly impact UK residents. However, currently little is known about LLM knowledge of UK Government public health information. To address this issue, this paper introduces a new benchmark, PubHealthBench, with over 8000 questions for evaluating LLMs' Multiple Choice Question Answering (MCQA) and free form responses to public health queries, created via an automated pipeline. We also release a new dataset of the extracted UK Government public health guidance documents used as source text for PubHealthBench. Assessing 24 LLMs on PubHealthBench we find the latest private LLMs (GPT-4.5, GPT-4.1 and o1) have a high degree of knowledge, achieving >90% in the MCQA setup, and outperform humans with cursory search engine use. However, in the free form setup we see lower performance with no model scoring >75%. Therefore, whilst there are promising signs that state of the art (SOTA) LLMs are an increasingly accurate source of public health information, additional safeguards or tools may still be needed when providing free form responses on public health topics."
  },
  {
    "title": "Priority-Driven Safe Model Predictive Control Approach to Autonomous Driving Applications",
    "url": "http://arxiv.org/abs/2505.05933v1",
    "arxiv_id": "2505.05933v1",
    "authors": [
      "Francesco Prignoli",
      "Ying Shuai Quan",
      "Mohammad Jeddi",
      "Jonas Sj\u00f6berg",
      "Paolo Falcone"
    ],
    "published": "2025-05-09T10:24:33+00:00",
    "summary": "This paper demonstrates the applicability of the safe model predictive control (SMPC) framework to autonomous driving scenarios, focusing on the design of adaptive cruise control (ACC) and automated lane-change systems. Building on the SMPC approach with priority-driven constraint softening -- which ensures the satisfaction of \\emph{hard} constraints under external disturbances by selectively softening a predefined subset of adjustable constraints -- we show how the algorithm dynamically relaxes lower-priority, comfort-related constraints in response to unexpected disturbances while preserving critical safety requirements such as collision avoidance and lane-keeping. A learning-based algorithm approximating the time consuming SMPC is introduced to enable real-time execution. Simulations in real-world driving scenarios subject to unpredicted disturbances confirm that this prioritized softening mechanism consistently upholds stringent safety constraints, underscoring the effectiveness of the proposed method."
  },
  {
    "title": "Human causal perception in a cube-stacking task",
    "url": "http://arxiv.org/abs/2505.05923v1",
    "arxiv_id": "2505.05923v1",
    "authors": [
      "Nikolai Bahr",
      "Christoph Zetzsche",
      "Jaime Maldonado"
    ],
    "published": "2025-05-09T09:54:34+00:00",
    "summary": "In intuitive physics the process of stacking cubes has become a paradigmatic, canonical task. Even though it gets employed in various shades and complexities, the very fundamental setting with two cubes has not been thoroughly investigated. Furthermore, the majority of settings feature only a reduced, one dimensional (1D) decision space. In this paper an experiment is conducted in which participants judge the stability of two cubes stacked on top of each other. It is performed in the full 3D setting which features a 2D decision surface. The analysis yield a shape of a rotated square for the perceived stability area instead of the commonly reported safety margin in 1D. This implies a more complex decision behavior in human than previously assumed."
  },
  {
    "title": "AgentXploit: End-to-End Redteaming of Black-Box AI Agents",
    "url": "http://arxiv.org/abs/2505.05849v1",
    "arxiv_id": "2505.05849v1",
    "authors": [
      "Zhun Wang",
      "Vincent Siu",
      "Zhe Ye",
      "Tianneng Shi",
      "Yuzhou Nie",
      "Xuandong Zhao",
      "Chenguang Wang",
      "Wenbo Guo",
      "Dawn Song"
    ],
    "published": "2025-05-09T07:40:17+00:00",
    "summary": "The strong planning and reasoning capabilities of Large Language Models (LLMs) have fostered the development of agent-based systems capable of leveraging external tools and interacting with increasingly complex environments. However, these powerful features also introduce a critical security risk: indirect prompt injection, a sophisticated attack vector that compromises the core of these agents, the LLM, by manipulating contextual information rather than direct user prompts. In this work, we propose a generic black-box fuzzing framework, AgentXploit, designed to automatically discover and exploit indirect prompt injection vulnerabilities across diverse LLM agents. Our approach starts by constructing a high-quality initial seed corpus, then employs a seed selection algorithm based on Monte Carlo Tree Search (MCTS) to iteratively refine inputs, thereby maximizing the likelihood of uncovering agent weaknesses. We evaluate AgentXploit on two public benchmarks, AgentDojo and VWA-adv, where it achieves 71% and 70% success rates against agents based on o3-mini and GPT-4o, respectively, nearly doubling the performance of baseline attacks. Moreover, AgentXploit exhibits strong transferability across unseen tasks and internal LLMs, as well as promising results against defenses. Beyond benchmark evaluations, we apply our attacks in real-world environments, successfully misleading agents to navigate to arbitrary URLs, including malicious sites."
  },
  {
    "title": "Exploring Dense Crowd Dynamics: State of the Art and Emerging Paradigms",
    "url": "http://arxiv.org/abs/2505.05826v1",
    "arxiv_id": "2505.05826v1",
    "authors": [
      "Thomas Chatagnon",
      "Antoine Tordeux",
      "Mohcine Chraibi"
    ],
    "published": "2025-05-09T06:52:46+00:00",
    "summary": "Dense pedestrian crowds may pose significant safety risks, yet their underlying dynamics remain insufficiently understood to reliably prevent accidents. In these environments, physical interactions and contact forces fundamentally shape the dynamics of the crowd. However, accurately describing these interindividual interactions requires specific modeling and analytical approaches. This chapter reviews paradigms and models used to represent pedestrian dynamics in various contexts, highlighting the transition from classical approaches to models tailored for dense crowd conditions. We argue that further investigation is needed, featuring new experimental studies and new modeling paradigms, to better capture the complex dynamics that emerge in high-density situations."
  },
  {
    "title": "Unsupervised Anomaly Detection for Autonomous Robots via Mahalanobis SVDD with Audio-IMU Fusion",
    "url": "http://arxiv.org/abs/2505.05811v1",
    "arxiv_id": "2505.05811v1",
    "authors": [
      "Yizhuo Yang",
      "Jiulin Zhao",
      "Xinhang Xu",
      "Kun Cao",
      "Shenghai Yuan",
      "Lihua Xie"
    ],
    "published": "2025-05-09T06:08:06+00:00",
    "summary": "Reliable anomaly detection is essential for ensuring the safety of autonomous robots, particularly when conventional detection systems based on vision or LiDAR become unreliable in adverse or unpredictable conditions. In such scenarios, alternative sensing modalities are needed to provide timely and robust feedback. To this end, we explore the use of audio and inertial measurement unit (IMU) sensors to detect underlying anomalies in autonomous mobile robots, such as collisions and internal mechanical faults. Furthermore, to address the challenge of limited labeled anomaly data, we propose an unsupervised anomaly detection framework based on Mahalanobis Support Vector Data Description (M-SVDD). In contrast to conventional SVDD methods that rely on Euclidean distance and assume isotropic feature distributions, our approach employs the Mahalanobis distance to adaptively scale feature dimensions and capture inter-feature correlations, enabling more expressive decision boundaries. In addition, a reconstruction-based auxiliary branch is introduced to preserve feature diversity and prevent representation collapse, further enhancing the robustness of anomaly detection. Extensive experiments on a collected mobile robot dataset and four public datasets demonstrate the effectiveness of the proposed method, as shown in the video https://youtu.be/yh1tn6DDD4A. Code and dataset are available at https://github.com/jamesyang7/M-SVDD."
  },
  {
    "title": "APOLLO: Automated LLM and Lean Collaboration for Advanced Formal Reasoning",
    "url": "http://arxiv.org/abs/2505.05758v1",
    "arxiv_id": "2505.05758v1",
    "authors": [
      "Azim Ospanov",
      "Roozbeh Yousefzadeh"
    ],
    "published": "2025-05-09T03:38:31+00:00",
    "summary": "Formal reasoning and automated theorem proving constitute a challenging subfield of machine learning, in which machines are tasked with proving mathematical theorems using formal languages like Lean. A formal verification system can check whether a formal proof is correct or not almost instantaneously, but generating a completely correct formal proof with large language models (LLMs) remains a formidable task. The usual approach in the literature is to prompt the LLM many times (up to several thousands) until one of the generated proofs passes the verification system. In this work, we present APOLLO (Automated PrOof repair via LLM and Lean cOllaboration), a modular, model-agnostic pipeline that combines the strengths of the Lean compiler with an LLM's reasoning abilities to achieve better proof-generation results at a low sampling budget. Apollo directs a fully automated process in which the LLM generates proofs for theorems, a set of agents analyze the proofs, fix the syntax errors, identify the mistakes in the proofs using Lean, isolate failing sub-lemmas, utilize automated solvers, and invoke an LLM on each remaining goal with a low top-K budget. The repaired sub-proofs are recombined and reverified, iterating up to a user-controlled maximum number of attempts. On the miniF2F benchmark, we establish a new state-of-the-art accuracy of 75.0% among 7B-parameter models while keeping the sampling budget below one thousand. Moreover, Apollo raises the state-of-the-art accuracy for Goedel-Prover-SFT to 65.6% while cutting sample complexity from 25,600 to a few hundred. General-purpose models (o3-mini, o4-mini) jump from 3-7% to over 40% accuracy. Our results demonstrate that targeted, compiler-guided repair of LLM outputs yields dramatic gains in both efficiency and correctness, suggesting a general paradigm for scalable automated theorem proving."
  },
  {
    "title": "Efficient Full-Stack Private Federated Deep Learning with Post-Quantum Security",
    "url": "http://arxiv.org/abs/2505.05751v1",
    "arxiv_id": "2505.05751v1",
    "authors": [
      "Yiwei Zhang",
      "Rouzbeh Behnia",
      "Attila A. Yavuz",
      "Reza Ebrahimi",
      "Elisa Bertino"
    ],
    "published": "2025-05-09T03:20:48+00:00",
    "summary": "Federated learning (FL) enables collaborative model training while preserving user data privacy by keeping data local. Despite these advantages, FL remains vulnerable to privacy attacks on user updates and model parameters during training and deployment. Secure aggregation protocols have been proposed to protect user updates by encrypting them, but these methods often incur high computational costs and are not resistant to quantum computers. Additionally, differential privacy (DP) has been used to mitigate privacy leakages, but existing methods focus on secure aggregation or DP, neglecting their potential synergies. To address these gaps, we introduce Beskar, a novel framework that provides post-quantum secure aggregation, optimizes computational overhead for FL settings, and defines a comprehensive threat model that accounts for a wide spectrum of adversaries. We also integrate DP into different stages of FL training to enhance privacy protection in diverse scenarios. Our framework provides a detailed analysis of the trade-offs between security, performance, and model accuracy, representing the first thorough examination of secure aggregation protocols combined with various DP approaches for post-quantum secure FL. Beskar aims to address the pressing privacy and security issues FL while ensuring quantum-safety and robust performance."
  },
  {
    "title": "Adaptive Stress Testing Black-Box LLM Planners",
    "url": "http://arxiv.org/abs/2505.05665v1",
    "arxiv_id": "2505.05665v1",
    "authors": [
      "Neeloy Chakraborty",
      "John Pohovey",
      "Melkior Ornik",
      "Katherine Driggs-Campbell"
    ],
    "published": "2025-05-08T21:50:43+00:00",
    "summary": "Large language models (LLMs) have recently demonstrated success in generalizing across decision-making tasks including planning, control and prediction, but their tendency to hallucinate unsafe and undesired outputs poses risks. We argue that detecting such failures is necessary, especially in safety-critical scenarios. Existing black-box methods often detect hallucinations by identifying inconsistencies across multiple samples. Many of these approaches typically introduce prompt perturbations like randomizing detail order or generating adversarial inputs, with the intuition that a confident model should produce stable outputs. We first perform a manual case study showing that other forms of perturbations (e.g., adding noise, removing sensor details) cause LLMs to hallucinate in a driving environment. We then propose a novel method for efficiently searching the space of prompt perturbations using Adaptive Stress Testing (AST) with Monte-Carlo Tree Search (MCTS). Our AST formulation enables discovery of scenarios and prompts that cause language models to act with high uncertainty. By generating MCTS prompt perturbation trees across diverse scenarios, we show that offline analyses can be used at runtime to automatically generate prompts that influence model uncertainty, and to inform real-time trust assessments of an LLM."
  },
  {
    "title": "UltraGauss: Ultrafast Gaussian Reconstruction of 3D Ultrasound Volumes",
    "url": "http://arxiv.org/abs/2505.05643v1",
    "arxiv_id": "2505.05643v1",
    "authors": [
      "Mark C. Eid",
      "Ana I. L. Namburete",
      "Jo\u00e3o F. Henriques"
    ],
    "published": "2025-05-08T20:53:47+00:00",
    "summary": "Ultrasound imaging is widely used due to its safety, affordability, and real-time capabilities, but its 2D interpretation is highly operator-dependent, leading to variability and increased cognitive demand. 2D-to-3D reconstruction mitigates these challenges by providing standardized volumetric views, yet existing methods are often computationally expensive, memory-intensive, or incompatible with ultrasound physics. We introduce UltraGauss: the first ultrasound-specific Gaussian Splatting framework, extending view synthesis techniques to ultrasound wave propagation. Unlike conventional perspective-based splatting, UltraGauss models probe-plane intersections in 3D, aligning with acoustic image formation. We derive an efficient rasterization boundary formulation for GPU parallelization and introduce a numerically stable covariance parametrization, improving computational efficiency and reconstruction accuracy. On real clinical ultrasound data, UltraGauss achieves state-of-the-art reconstructions in 5 minutes, and reaching 0.99 SSIM within 20 minutes on a single GPU. A survey of expert clinicians confirms UltraGauss' reconstructions are the most realistic among competing methods. Our CUDA implementation will be released upon publication."
  },
  {
    "title": "LiteLMGuard: Seamless and Lightweight On-Device Prompt Filtering for Safeguarding Small Language Models against Quantization-induced Risks and Vulnerabilities",
    "url": "http://arxiv.org/abs/2505.05619v1",
    "arxiv_id": "2505.05619v1",
    "authors": [
      "Kalyan Nakka",
      "Jimmy Dani",
      "Ausmit Mondal",
      "Nitesh Saxena"
    ],
    "published": "2025-05-08T19:58:41+00:00",
    "summary": "The growing adoption of Large Language Models (LLMs) has influenced the development of their lighter counterparts-Small Language Models (SLMs)-to enable on-device deployment across smartphones and edge devices. These SLMs offer enhanced privacy, reduced latency, server-free functionality, and improved user experience. However, due to resource constraints of on-device environment, SLMs undergo size optimization through compression techniques like quantization, which can inadvertently introduce fairness, ethical and privacy risks. Critically, quantized SLMs may respond to harmful queries directly, without requiring adversarial manipulation, raising significant safety and trust concerns.   To address this, we propose LiteLMGuard (LLMG), an on-device prompt guard that provides real-time, prompt-level defense for quantized SLMs. Additionally, our prompt guard is designed to be model-agnostic such that it can be seamlessly integrated with any SLM, operating independently of underlying architectures. Our LLMG formalizes prompt filtering as a deep learning (DL)-based prompt answerability classification task, leveraging semantic understanding to determine whether a query should be answered by any SLM. Using our curated dataset, Answerable-or-Not, we trained and fine-tuned several DL models and selected ELECTRA as the candidate, with 97.75% answerability classification accuracy.   Our safety effectiveness evaluations demonstrate that LLMG defends against over 87% of harmful prompts, including both direct instruction and jailbreak attack strategies. We further showcase its ability to mitigate the Open Knowledge Attacks, where compromised SLMs provide unsafe responses without adversarial prompting. In terms of prompt filtering effectiveness, LLMG achieves near state-of-the-art filtering accuracy of 94%, with an average latency of 135 ms, incurring negligible overhead for users."
  },
  {
    "title": "Barrier Function Overrides For Non-Convex Fixed Wing Flight Control and Self-Driving Cars",
    "url": "http://arxiv.org/abs/2505.05548v1",
    "arxiv_id": "2505.05548v1",
    "authors": [
      "Eric Squires",
      "Phillip Odom",
      "Zsolt Kira"
    ],
    "published": "2025-05-08T17:59:00+00:00",
    "summary": "Reinforcement Learning (RL) has enabled vast performance improvements for robotics systems. To achieve these results though, the agent often must randomly explore the environment, which for safety critical systems presents a significant challenge. Barrier functions can solve this challenge by enabling an override that approximates the RL control input as closely as possible without violating a safety constraint. Unfortunately, this override can be computationally intractable in cases where the dynamics are not convex in the control input or when time is discrete, as is often the case when training RL systems. We therefore consider these cases, developing novel barrier functions for two non-convex systems (fixed wing aircraft and self-driving cars performing lane merging with adaptive cruise control) in discrete time. Although solving for an online and optimal override is in general intractable when the dynamics are nonconvex in the control input, we investigate approximate solutions, finding that these approximations enable performance commensurate with baseline RL methods with zero safety violations. In particular, even without attempting to solve for the optimal override at all, performance is still competitive with baseline RL performance. We discuss the tradeoffs of the approximate override solutions including performance and computational tractability."
  },
  {
    "title": "Safety by Measurement: A Systematic Literature Review of AI Safety Evaluation Methods",
    "url": "http://arxiv.org/abs/2505.05541v1",
    "arxiv_id": "2505.05541v1",
    "authors": [
      "Markov Grey",
      "Charbel-Rapha\u00ebl Segerie"
    ],
    "published": "2025-05-08T16:55:07+00:00",
    "summary": "As frontier AI systems advance toward transformative capabilities, we need a parallel transformation in how we measure and evaluate these systems to ensure safety and inform governance. While benchmarks have been the primary method for estimating model capabilities, they often fail to establish true upper bounds or predict deployment behavior. This literature review consolidates the rapidly evolving field of AI safety evaluations, proposing a systematic taxonomy around three dimensions: what properties we measure, how we measure them, and how these measurements integrate into frameworks. We show how evaluations go beyond benchmarks by measuring what models can do when pushed to the limit (capabilities), the behavioral tendencies exhibited by default (propensities), and whether our safety measures remain effective even when faced with subversive adversarial AI (control). These properties are measured through behavioral techniques like scaffolding, red teaming and supervised fine-tuning, alongside internal techniques such as representation analysis and mechanistic interpretability. We provide deeper explanations of some safety-critical capabilities like cybersecurity exploitation, deception, autonomous replication, and situational awareness, alongside concerning propensities like power-seeking and scheming. The review explores how these evaluation methods integrate into governance frameworks to translate results into concrete development decisions. We also highlight challenges to safety evaluations - proving absence of capabilities, potential model sandbagging, and incentives for \"safetywashing\" - while identifying promising research directions. By synthesizing scattered resources, this literature review aims to provide a central reference point for understanding AI safety evaluations."
  },
  {
    "title": "Reasoning Models Don't Always Say What They Think",
    "url": "http://arxiv.org/abs/2505.05410v1",
    "arxiv_id": "2505.05410v1",
    "authors": [
      "Yanda Chen",
      "Joe Benton",
      "Ansh Radhakrishnan",
      "Jonathan Uesato",
      "Carson Denison",
      "John Schulman",
      "Arushi Somani",
      "Peter Hase",
      "Misha Wagner",
      "Fabien Roger",
      "Vlad Mikulik",
      "Samuel R. Bowman",
      "Jan Leike",
      "Jared Kaplan",
      "Ethan Perez"
    ],
    "published": "2025-05-08T16:51:43+00:00",
    "summary": "Chain-of-thought (CoT) offers a potential boon for AI safety as it allows monitoring a model's CoT to try to understand its intentions and reasoning processes. However, the effectiveness of such monitoring hinges on CoTs faithfully representing models' actual reasoning processes. We evaluate CoT faithfulness of state-of-the-art reasoning models across 6 reasoning hints presented in the prompts and find: (1) for most settings and models tested, CoTs reveal their usage of hints in at least 1% of examples where they use the hint, but the reveal rate is often below 20%, (2) outcome-based reinforcement learning initially improves faithfulness but plateaus without saturating, and (3) when reinforcement learning increases how frequently hints are used (reward hacking), the propensity to verbalize them does not increase, even without training against a CoT monitor. These results suggest that CoT monitoring is a promising way of noticing undesired behaviors during training and evaluations, but that it is not sufficient to rule them out. They also suggest that in settings like ours where CoT reasoning is not necessary, test-time monitoring of CoTs is unlikely to reliably catch rare and catastrophic unexpected behaviors."
  },
  {
    "title": "PillarMamba: Learning Local-Global Context for Roadside Point Cloud via Hybrid State Space Model",
    "url": "http://arxiv.org/abs/2505.05397v1",
    "arxiv_id": "2505.05397v1",
    "authors": [
      "Zhang Zhang",
      "Chao Sun",
      "Chao Yue",
      "Da Wen",
      "Tianze Wang",
      "Jianghao Leng"
    ],
    "published": "2025-05-08T16:33:04+00:00",
    "summary": "Serving the Intelligent Transport System (ITS) and Vehicle-to-Everything (V2X) tasks, roadside perception has received increasing attention in recent years, as it can extend the perception range of connected vehicles and improve traffic safety. However, roadside point cloud oriented 3D object detection has not been effectively explored. To some extent, the key to the performance of a point cloud detector lies in the receptive field of the network and the ability to effectively utilize the scene context. The recent emergence of Mamba, based on State Space Model (SSM), has shaken up the traditional convolution and transformers that have long been the foundational building blocks, due to its efficient global receptive field. In this work, we introduce Mamba to pillar-based roadside point cloud perception and propose a framework based on Cross-stage State-space Group (CSG), called PillarMamba. It enhances the expressiveness of the network and achieves efficient computation through cross-stage feature fusion. However, due to the limitations of scan directions, state space model faces local connection disrupted and historical relationship forgotten. To address this, we propose the Hybrid State-space Block (HSB) to obtain the local-global context of roadside point cloud. Specifically, it enhances neighborhood connections through local convolution and preserves historical memory through residual attention. The proposed method outperforms the state-of-the-art methods on the popular large scale roadside benchmark: DAIR-V2X-I. The code will be released soon."
  },
  {
    "title": "Robust Online Learning with Private Information",
    "url": "http://arxiv.org/abs/2505.05341v1",
    "arxiv_id": "2505.05341v1",
    "authors": [
      "Kyohei Okumura"
    ],
    "published": "2025-05-08T15:29:06+00:00",
    "summary": "This paper investigates the robustness of online learning algorithms when learners possess private information. No-external-regret algorithms, prevalent in machine learning, are vulnerable to strategic manipulation, allowing an adaptive opponent to extract full surplus. Even standard no-weak-external-regret algorithms, designed for optimal learning in stationary environments, exhibit similar vulnerabilities. This raises a fundamental question: can a learner simultaneously prevent full surplus extraction by adaptive opponents while maintaining optimal performance in well-behaved environments? To address this, we model the problem as a two-player repeated game, where the learner with private information plays against the environment, facing ambiguity about the environment's types: stationary or adaptive. We introduce \\emph{partial safety} as a key design criterion for online learning algorithms to prevent full surplus extraction. We then propose the \\emph{Explore-Exploit-Punish} (\\textsf{EEP}) algorithm and prove that it satisfies partial safety while achieving optimal learning in stationary environments, and has a variant that delivers improved welfare performance. Our findings highlight the risks of applying standard online learning algorithms in strategic settings with adverse selection. We advocate for a shift toward online learning algorithms that explicitly incorporate safeguards against strategic manipulation while ensuring strong learning performance."
  },
  {
    "title": "Advancing Neural Network Verification through Hierarchical Safety Abstract Interpretation",
    "url": "http://arxiv.org/abs/2505.05235v1",
    "arxiv_id": "2505.05235v1",
    "authors": [
      "Luca Marzari",
      "Isabella Mastroeni",
      "Alessandro Farinelli"
    ],
    "published": "2025-05-08T13:29:46+00:00",
    "summary": "Traditional methods for formal verification (FV) of deep neural networks (DNNs) are constrained by a binary encoding of safety properties, where a model is classified as either safe or unsafe (robust or not robust). This binary encoding fails to capture the nuanced safety levels within a model, often resulting in either overly restrictive or too permissive requirements. In this paper, we introduce a novel problem formulation called Abstract DNN-Verification, which verifies a hierarchical structure of unsafe outputs, providing a more granular analysis of the safety aspect for a given DNN. Crucially, by leveraging abstract interpretation and reasoning about output reachable sets, our approach enables assessing multiple safety levels during the FV process, requiring the same (in the worst case) or even potentially less computational effort than the traditional binary verification approach. Specifically, we demonstrate how this formulation allows rank adversarial inputs according to their abstract safety level violation, offering a more detailed evaluation of the model's safety and robustness. Our contributions include a theoretical exploration of the relationship between our novel abstract safety formulation and existing approaches that employ abstract interpretation for robustness verification, complexity analysis of the novel problem introduced, and an empirical evaluation considering both a complex deep reinforcement learning task (based on Habitat 3.0) and standard DNN-Verification benchmarks."
  },
  {
    "title": "PaniCar: Securing the Perception of Advanced Driving Assistance Systems Against Emergency Vehicle Lighting",
    "url": "http://arxiv.org/abs/2505.05183v1",
    "arxiv_id": "2505.05183v1",
    "authors": [
      "Elad Feldman",
      "Jacob Shams",
      "Dudi Biton",
      "Alfred Chen",
      "Shaoyuan Xie",
      "Satoru Koda",
      "Yisroel Mirsky",
      "Asaf Shabtai",
      "Yuval Elovici",
      "Ben Nassi"
    ],
    "published": "2025-05-08T12:33:48+00:00",
    "summary": "The safety of autonomous cars has come under scrutiny in recent years, especially after 16 documented incidents involving Teslas (with autopilot engaged) crashing into parked emergency vehicles (police cars, ambulances, and firetrucks). While previous studies have revealed that strong light sources often introduce flare artifacts in the captured image, which degrade the image quality, the impact of flare on object detection performance remains unclear. In this research, we unveil PaniCar, a digital phenomenon that causes an object detector's confidence score to fluctuate below detection thresholds when exposed to activated emergency vehicle lighting. This vulnerability poses a significant safety risk, and can cause autonomous vehicles to fail to detect objects near emergency vehicles. In addition, this vulnerability could be exploited by adversaries to compromise the security of advanced driving assistance systems (ADASs). We assess seven commercial ADASs (Tesla Model 3, \"manufacturer C\", HP, Pelsee, AZDOME, Imagebon, Rexing), four object detectors (YOLO, SSD, RetinaNet, Faster R-CNN), and 14 patterns of emergency vehicle lighting to understand the influence of various technical and environmental factors. We also evaluate four SOTA flare removal methods and show that their performance and latency are insufficient for real-time driving constraints. To mitigate this risk, we propose Caracetamol, a robust framework designed to enhance the resilience of object detectors against the effects of activated emergency vehicle lighting. Our evaluation shows that on YOLOv3 and Faster RCNN, Caracetamol improves the models' average confidence of car detection by 0.20, the lower confidence bound by 0.33, and reduces the fluctuation range by 0.33. In addition, Caracetamol is capable of processing frames at a rate of between 30-50 FPS, enabling real-time ADAS car detection."
  },
  {
    "title": "Duplex Self-Aligning Resonant Beam Communications and Power Transfer with Coupled Spatially Distributed Laser Resonator",
    "url": "http://arxiv.org/abs/2505.05107v1",
    "arxiv_id": "2505.05107v1",
    "authors": [
      "Mingliang Xiong",
      "Qingwen Liu",
      "Hao Deng",
      "Gang Wang",
      "Gang Li",
      "Bin He"
    ],
    "published": "2025-05-08T10:11:36+00:00",
    "summary": "Sustainable energy supply and high-speed communications are two significant needs for mobile electronic devices. This paper introduces a self-aligning resonant beam system for simultaneous light information and power transfer (SLIPT), employing a novel coupled spatially distributed resonator (CSDR). The system utilizes a resonant beam for efficient power delivery and a second-harmonic beam for concurrent data transmission, inherently minimizing echo interference and enabling bidirectional communication. Through comprehensive analyses, we investigate the CSDR's stable region, beam evolution, and power characteristics in relation to working distance and device parameters. Numerical simulations validate the CSDR-SLIPT system's feasibility by identifying a stable beam waist location for achieving accurate mode-match coupling between two spatially distributed resonant cavities and demonstrating its operational range and efficient power delivery across varying distances. The research reveals the system's benefits in terms of both safety and energy transmission efficiency. We also demonstrate the trade-off among the reflectivities of the cavity mirrors in the CSDR. These findings offer valuable design insights for resonant beam systems, advancing SLIPT with significant potential for remote device connectivity."
  },
  {
    "title": "DispBench: Benchmarking Disparity Estimation to Synthetic Corruptions",
    "url": "http://arxiv.org/abs/2505.05091v1",
    "arxiv_id": "2505.05091v1",
    "authors": [
      "Shashank Agnihotri",
      "Amaan Ansari",
      "Annika Dackermann",
      "Fabian R\u00f6sch",
      "Margret Keuper"
    ],
    "published": "2025-05-08T09:40:17+00:00",
    "summary": "Deep learning (DL) has surpassed human performance on standard benchmarks, driving its widespread adoption in computer vision tasks. One such task is disparity estimation, estimating the disparity between matching pixels in stereo image pairs, which is crucial for safety-critical applications like medical surgeries and autonomous navigation. However, DL-based disparity estimation methods are highly susceptible to distribution shifts and adversarial attacks, raising concerns about their reliability and generalization. Despite these concerns, a standardized benchmark for evaluating the robustness of disparity estimation methods remains absent, hindering progress in the field.   To address this gap, we introduce DispBench, a comprehensive benchmarking tool for systematically assessing the reliability of disparity estimation methods. DispBench evaluates robustness against synthetic image corruptions such as adversarial attacks and out-of-distribution shifts caused by 2D Common Corruptions across multiple datasets and diverse corruption scenarios. We conduct the most extensive performance and robustness analysis of disparity estimation methods to date, uncovering key correlations between accuracy, reliability, and generalization. Open-source code for DispBench: https://github.com/shashankskagnihotri/benchmarking_robustness/tree/disparity_estimation/final/disparity_estimation"
  },
  {
    "title": "Adaptive Contextual Embedding for Robust Far-View Borehole Detection",
    "url": "http://arxiv.org/abs/2505.05008v1",
    "arxiv_id": "2505.05008v1",
    "authors": [
      "Xuesong Liu",
      "Tianyu Hao",
      "Emmett J. Ientilucci"
    ],
    "published": "2025-05-08T07:25:42+00:00",
    "summary": "In controlled blasting operations, accurately detecting densely distributed tiny boreholes from far-view imagery is critical for operational safety and efficiency. However, existing detection methods often struggle due to small object scales, highly dense arrangements, and limited distinctive visual features of boreholes. To address these challenges, we propose an adaptive detection approach that builds upon existing architectures (e.g., YOLO) by explicitly leveraging consistent embedding representations derived through exponential moving average (EMA)-based statistical updates.   Our method introduces three synergistic components: (1) adaptive augmentation utilizing dynamically updated image statistics to robustly handle illumination and texture variations; (2) embedding stabilization to ensure consistent and reliable feature extraction; and (3) contextual refinement leveraging spatial context for improved detection accuracy. The pervasive use of EMA in our method is particularly advantageous given the limited visual complexity and small scale of boreholes, allowing stable and robust representation learning even under challenging visual conditions. Experiments on a challenging proprietary quarry-site dataset demonstrate substantial improvements over baseline YOLO-based architectures, highlighting our method's effectiveness in realistic and complex industrial scenarios."
  },
  {
    "title": "A Vehicle System for Navigating Among Vulnerable Road Users Including Remote Operation",
    "url": "http://arxiv.org/abs/2505.04982v1",
    "arxiv_id": "2505.04982v1",
    "authors": [
      "Oscar de Groot",
      "Alberto Bertipaglia",
      "Hidde Boekema",
      "Vishrut Jain",
      "Marcell Kegl",
      "Varun Kotian",
      "Ted Lentsch",
      "Yancong Lin",
      "Chrysovalanto Messiou",
      "Emma Schippers",
      "Farzam Tajdari",
      "Shiming Wang",
      "Zimin Xia",
      "Mubariz Zaffar",
      "Ronald Ensing",
      "Mario Garzon",
      "Javier Alonso-Mora",
      "Holger Caesar",
      "Laura Ferranti",
      "Riender Happee",
      "Julian F. P. Kooij",
      "Georgios Papaioannou",
      "Barys Shyrokau",
      "Dariu M. Gavrila"
    ],
    "published": "2025-05-08T06:39:47+00:00",
    "summary": "We present a vehicle system capable of navigating safely and efficiently around Vulnerable Road Users (VRUs), such as pedestrians and cyclists. The system comprises key modules for environment perception, localization and mapping, motion planning, and control, integrated into a prototype vehicle. A key innovation is a motion planner based on Topology-driven Model Predictive Control (T-MPC). The guidance layer generates multiple trajectories in parallel, each representing a distinct strategy for obstacle avoidance or non-passing. The underlying trajectory optimization constrains the joint probability of collision with VRUs under generic uncertainties. To address extraordinary situations (\"edge cases\") that go beyond the autonomous capabilities - such as construction zones or encounters with emergency responders - the system includes an option for remote human operation, supported by visual and haptic guidance. In simulation, our motion planner outperforms three baseline approaches in terms of safety and efficiency. We also demonstrate the full system in prototype vehicle tests on a closed track, both in autonomous and remotely operated modes."
  },
  {
    "title": "LVLM-MPC Collaboration for Autonomous Driving: A Safety-Aware and Task-Scalable Control Architecture",
    "url": "http://arxiv.org/abs/2505.04980v1",
    "arxiv_id": "2505.04980v1",
    "authors": [
      "Kazuki Atsuta",
      "Kohei Honda",
      "Hiroyuki Okuda",
      "Tatsuya Suzuki"
    ],
    "published": "2025-05-08T06:35:30+00:00",
    "summary": "This paper proposes a novel Large Vision-Language Model (LVLM) and Model Predictive Control (MPC) integration framework that delivers both task scalability and safety for Autonomous Driving (AD). LVLMs excel at high-level task planning across diverse driving scenarios. However, since these foundation models are not specifically designed for driving and their reasoning is not consistent with the feasibility of low-level motion planning, concerns remain regarding safety and smooth task switching. This paper integrates LVLMs with MPC Builder, which automatically generates MPCs on demand, based on symbolic task commands generated by the LVLM, while ensuring optimality and safety. The generated MPCs can strongly assist the execution or rejection of LVLM-driven task switching by providing feedback on the feasibility of the given tasks and generating task-switching-aware MPCs. Our approach provides a safe, flexible, and adaptable control framework, bridging the gap between cutting-edge foundation models and reliable vehicle operation. We demonstrate the effectiveness of our approach through a simulation experiment, showing that our system can safely and effectively handle highway driving while maintaining the flexibility and adaptability of LVLMs."
  },
  {
    "title": "Belief Filtering for Epistemic Control in Linguistic State Space",
    "url": "http://arxiv.org/abs/2505.04927v1",
    "arxiv_id": "2505.04927v1",
    "authors": [
      "Sebastian Dumbrava"
    ],
    "published": "2025-05-08T03:52:43+00:00",
    "summary": "We examine belief filtering as a mechanism for the epistemic control of artificial agents, focusing on the regulation of internal cognitive states represented as linguistic expressions. This mechanism is developed within the Semantic Manifold framework, where belief states are dynamic, structured ensembles of natural language fragments. Belief filters act as content-aware operations on these fragments across various cognitive transitions. This paper illustrates how the inherent interpretability and modularity of such a linguistically-grounded cognitive architecture directly enable belief filtering, offering a principled approach to agent regulation. The study highlights the potential for enhancing AI safety and alignment through structured interventions in an agent's internal semantic space and points to new directions for architecturally embedded cognitive governance."
  },
  {
    "title": "Perception, Reason, Think, and Plan: A Survey on Large Multimodal Reasoning Models",
    "url": "http://arxiv.org/abs/2505.04921v1",
    "arxiv_id": "2505.04921v1",
    "authors": [
      "Yunxin Li",
      "Zhenyu Liu",
      "Zitao Li",
      "Xuanyu Zhang",
      "Zhenran Xu",
      "Xinyu Chen",
      "Haoyuan Shi",
      "Shenyuan Jiang",
      "Xintong Wang",
      "Jifang Wang",
      "Shouzheng Huang",
      "Xinping Zhao",
      "Borui Jiang",
      "Lanqing Hong",
      "Longyue Wang",
      "Zhuotao Tian",
      "Baoxing Huai",
      "Wenhan Luo",
      "Weihua Luo",
      "Zheng Zhang",
      "Baotian Hu",
      "Min Zhang"
    ],
    "published": "2025-05-08T03:35:23+00:00",
    "summary": "Reasoning lies at the heart of intelligence, shaping the ability to make decisions, draw conclusions, and generalize across domains. In artificial intelligence, as systems increasingly operate in open, uncertain, and multimodal environments, reasoning becomes essential for enabling robust and adaptive behavior. Large Multimodal Reasoning Models (LMRMs) have emerged as a promising paradigm, integrating modalities such as text, images, audio, and video to support complex reasoning capabilities and aiming to achieve comprehensive perception, precise understanding, and deep reasoning. As research advances, multimodal reasoning has rapidly evolved from modular, perception-driven pipelines to unified, language-centric frameworks that offer more coherent cross-modal understanding. While instruction tuning and reinforcement learning have improved model reasoning, significant challenges remain in omni-modal generalization, reasoning depth, and agentic behavior. To address these issues, we present a comprehensive and structured survey of multimodal reasoning research, organized around a four-stage developmental roadmap that reflects the field's shifting design philosophies and emerging capabilities. First, we review early efforts based on task-specific modules, where reasoning was implicitly embedded across stages of representation, alignment, and fusion. Next, we examine recent approaches that unify reasoning into multimodal LLMs, with advances such as Multimodal Chain-of-Thought (MCoT) and multimodal reinforcement learning enabling richer and more structured reasoning chains. Finally, drawing on empirical insights from challenging benchmarks and experimental cases of OpenAI O3 and O4-mini, we discuss the conceptual direction of native large multimodal reasoning models (N-LMRMs), which aim to support scalable, agentic, and adaptive reasoning and planning in complex, real-world environments."
  },
  {
    "title": "GCN-Based Throughput-Oriented Handover Management in Dense 5G Vehicular Networks",
    "url": "http://arxiv.org/abs/2505.04894v1",
    "arxiv_id": "2505.04894v1",
    "authors": [
      "Nazanin Mehregan",
      "Robson E. De Grande"
    ],
    "published": "2025-05-08T02:03:46+00:00",
    "summary": "The rapid advancement of 5G has transformed vehicular networks, offering high bandwidth, low latency, and fast data rates essential for real-time applications in smart cities and vehicles. These improvements enhance traffic safety and entertainment services. However, the limited coverage and frequent handovers in 5G networks cause network instability, especially in high-mobility environments due to the ping-pong effect. This paper presents TH-GCN (Throughput-oriented Graph Convolutional Network), a novel approach for optimizing handover management in dense 5G networks. Using graph neural networks (GNNs), TH-GCN models vehicles and base stations as nodes in a dynamic graph enriched with features such as signal quality, throughput, vehicle speed, and base station load. By integrating both user equipment and base station perspectives, this dual-centric approach enables adaptive, real-time handover decisions that improve network stability. Simulation results show that TH-GCN reduces handovers by up to 78 percent and improves signal quality by 10 percent, outperforming existing methods."
  },
  {
    "title": "Federated Learning for Cyber Physical Systems: A Comprehensive Survey",
    "url": "http://arxiv.org/abs/2505.04873v1",
    "arxiv_id": "2505.04873v1",
    "authors": [
      "Minh K. Quan",
      "Pubudu N. Pathirana",
      "Mayuri Wijayasundara",
      "Sujeeva Setunge",
      "Dinh C. Nguyen",
      "Christopher G. Brinton",
      "David J. Love",
      "H. Vincent Poor"
    ],
    "published": "2025-05-08T01:17:15+00:00",
    "summary": "The integration of machine learning (ML) in cyber physical systems (CPS) is a complex task due to the challenges that arise in terms of real-time decision making, safety, reliability, device heterogeneity, and data privacy. There are also open research questions that must be addressed in order to fully realize the potential of ML in CPS. Federated learning (FL), a distributed approach to ML, has become increasingly popular in recent years. It allows models to be trained using data from decentralized sources. This approach has been gaining popularity in the CPS field, as it integrates computer, communication, and physical processes. Therefore, the purpose of this work is to provide a comprehensive analysis of the most recent developments of FL-CPS, including the numerous application areas, system topologies, and algorithms developed in recent years. The paper starts by discussing recent advances in both FL and CPS, followed by their integration. Then, the paper compares the application of FL in CPS with its applications in the internet of things (IoT) in further depth to show their connections and distinctions. Furthermore, the article scrutinizes how FL is utilized in critical CPS applications, e.g., intelligent transportation systems, cybersecurity services, smart cities, and smart healthcare solutions. The study also includes critical insights and lessons learned from various FL-CPS implementations. The paper's concluding section delves into significant concerns and suggests avenues for further research in this fast-paced and dynamic era."
  },
  {
    "title": "PR2: Peephole Raw Pointer Rewriting with LLMs for Translating C to Safer Rust",
    "url": "http://arxiv.org/abs/2505.04852v1",
    "arxiv_id": "2505.04852v1",
    "authors": [
      "Yifei Gao",
      "Chengpeng Wang",
      "Pengxiang Huang",
      "Xuwei Liu",
      "Mingwei Zheng",
      "Xiangyu Zhang"
    ],
    "published": "2025-05-07T23:30:27+00:00",
    "summary": "There has been a growing interest in translating C code to Rust due to Rust's robust memory and thread safety guarantees. Tools such as C2RUST enable syntax-guided transpilation from C to semantically equivalent Rust code. However, the resulting Rust programs often rely heavily on unsafe constructs--particularly raw pointers--which undermines Rust's safety guarantees. This paper aims to improve the memory safety of Rust programs generated by C2RUST by eliminating raw pointers. Specifically, we propose a peephole raw pointer rewriting technique that lifts raw pointers in individual functions to appropriate Rust data structures. Technically, PR2 employs decision-tree-based prompting to guide the pointer lifting process. Additionally, it leverages code change analysis to guide the repair of errors introduced during rewriting, effectively addressing errors encountered during compilation and test case execution. We implement PR2 as a prototype and evaluate it using gpt-4o-mini on 28 real-world C projects. The results show that PR2 successfully eliminates 13.22% of local raw pointers across these projects, significantly enhancing the safety of the translated Rust code. On average, PR2 completes the transformation of a project in 5.44 hours, at an average cost of $1.46."
  },
  {
    "title": "PR2: Peephole Raw Pointer Rewriting with LLMs for Translating C to Safer Rust",
    "url": "http://arxiv.org/abs/2505.04852v2",
    "arxiv_id": "2505.04852v2",
    "authors": [
      "Yifei Gao",
      "Chengpeng Wang",
      "Pengxiang Huang",
      "Xuwei Liu",
      "Mingwei Zheng",
      "Xiangyu Zhang"
    ],
    "published": "2025-05-07T23:30:27+00:00",
    "summary": "There has been a growing interest in translating C code to Rust due to Rust's robust memory and thread safety guarantees. Tools such as C2RUST enable syntax-guided transpilation from C to semantically equivalent Rust code. However, the resulting Rust programs often rely heavily on unsafe constructs--particularly raw pointers--which undermines Rust's safety guarantees. This paper aims to improve the memory safety of Rust programs generated by C2RUST by eliminating raw pointers. Specifically, we propose a peephole raw pointer rewriting technique that lifts raw pointers in individual functions to appropriate Rust data structures. Technically, PR2 employs decision-tree-based prompting to guide the pointer lifting process. Additionally, it leverages code change analysis to guide the repair of errors introduced during rewriting, effectively addressing errors encountered during compilation and test case execution. We implement PR2 as a prototype and evaluate it using gpt-4o-mini on 28 real-world C projects. The results show that PR2 successfully eliminates 13.22% of local raw pointers across these projects, significantly enhancing the safety of the translated Rust code. On average, PR2 completes the transformation of a project in 5.44 hours, at an average cost of $1.46."
  },
  {
    "title": "Comparative Study of Generative Models for Early Detection of Failures in Medical Devices",
    "url": "http://arxiv.org/abs/2505.04845v1",
    "arxiv_id": "2505.04845v1",
    "authors": [
      "Binesh Sadanandan",
      "Bahareh Arghavani Nobar",
      "Vahid Behzadan"
    ],
    "published": "2025-05-07T22:49:53+00:00",
    "summary": "The medical device industry has significantly advanced by integrating sophisticated electronics like microchips and field-programmable gate arrays (FPGAs) to enhance the safety and usability of life-saving devices. These complex electro-mechanical systems, however, introduce challenging failure modes that are not easily detectable with conventional methods. Effective fault detection and mitigation become vital as reliance on such electronics grows. This paper explores three generative machine learning-based approaches for fault detection in medical devices, leveraging sensor data from surgical staplers,a class 2 medical device. Historically considered low-risk, these devices have recently been linked to an increasing number of injuries and fatalities. The study evaluates the performance and data requirements of these machine-learning approaches, highlighting their potential to enhance device safety."
  },
  {
    "title": "Red Teaming the Mind of the Machine: A Systematic Evaluation of Prompt Injection and Jailbreak Vulnerabilities in LLMs",
    "url": "http://arxiv.org/abs/2505.04806v1",
    "arxiv_id": "2505.04806v1",
    "authors": [
      "Chetan Pathade"
    ],
    "published": "2025-05-07T21:15:40+00:00",
    "summary": "Large Language Models (LLMs) are increasingly integrated into consumer and enterprise applications. Despite their capabilities, they remain susceptible to adversarial attacks such as prompt injection and jailbreaks that override alignment safeguards. This paper provides a systematic investigation of jailbreak strategies against various state-of-the-art LLMs. We categorize over 1,400 adversarial prompts, analyze their success against GPT-4, Claude 2, Mistral 7B, and Vicuna, and examine their generalizability and construction logic. We further propose layered mitigation strategies and recommend a hybrid red-teaming and sandboxing approach for robust LLM security."
  },
  {
    "title": "Replay to Remember (R2R): An Efficient Uncertainty-driven Unsupervised Continual Learning Framework Using Generative Replay",
    "url": "http://arxiv.org/abs/2505.04787v1",
    "arxiv_id": "2505.04787v1",
    "authors": [
      "Sriram Mandalika",
      "Harsha Vardhan",
      "Athira Nambiar"
    ],
    "published": "2025-05-07T20:29:31+00:00",
    "summary": "Continual Learning entails progressively acquiring knowledge from new data while retaining previously acquired knowledge, thereby mitigating ``Catastrophic Forgetting'' in neural networks. Our work presents a novel uncertainty-driven Unsupervised Continual Learning framework using Generative Replay, namely ``Replay to Remember (R2R)''. The proposed R2R architecture efficiently uses unlabelled and synthetic labelled data in a balanced proportion using a cluster-level uncertainty-driven feedback mechanism and a VLM-powered generative replay module. Unlike traditional memory-buffer methods that depend on pretrained models and pseudo-labels, our R2R framework operates without any prior training. It leverages visual features from unlabeled data and adapts continuously using clustering-based uncertainty estimation coupled with dynamic thresholding. Concurrently, a generative replay mechanism along with DeepSeek-R1 powered CLIP VLM produces labelled synthetic data representative of past experiences, resembling biological visual thinking that replays memory to remember and act in new, unseen tasks. Extensive experimental analyses are carried out in CIFAR-10, CIFAR-100, CINIC-10, SVHN and TinyImageNet datasets. Our proposed R2R approach improves knowledge retention, achieving a state-of-the-art performance of 98.13%, 73.06%, 93.41%, 95.18%, 59.74%, respectively, surpassing state-of-the-art performance by over 4.36%."
  },
  {
    "title": "Replay to Remember (R2R): An Efficient Uncertainty-driven Unsupervised Continual Learning Framework Using Generative Replay",
    "url": "http://arxiv.org/abs/2505.04787v2",
    "arxiv_id": "2505.04787v2",
    "authors": [
      "Sriram Mandalika",
      "Harsha Vardhan",
      "Athira Nambiar"
    ],
    "published": "2025-05-07T20:29:31+00:00",
    "summary": "Continual Learning entails progressively acquiring knowledge from new data while retaining previously acquired knowledge, thereby mitigating ``Catastrophic Forgetting'' in neural networks. Our work presents a novel uncertainty-driven Unsupervised Continual Learning framework using Generative Replay, namely ``Replay to Remember (R2R)''. The proposed R2R architecture efficiently uses unlabelled and synthetic labelled data in a balanced proportion using a cluster-level uncertainty-driven feedback mechanism and a VLM-powered generative replay module. Unlike traditional memory-buffer methods that depend on pretrained models and pseudo-labels, our R2R framework operates without any prior training. It leverages visual features from unlabeled data and adapts continuously using clustering-based uncertainty estimation coupled with dynamic thresholding. Concurrently, a generative replay mechanism along with DeepSeek-R1 powered CLIP VLM produces labelled synthetic data representative of past experiences, resembling biological visual thinking that replays memory to remember and act in new, unseen tasks. Extensive experimental analyses are carried out in CIFAR-10, CIFAR-100, CINIC-10, SVHN and TinyImageNet datasets. Our proposed R2R approach improves knowledge retention, achieving a state-of-the-art performance of 98.13%, 73.06%, 93.41%, 95.18%, 59.74%, respectively, surpassing state-of-the-art performance by over 4.36%."
  },
  {
    "title": "Fight Fire with Fire: Defending Against Malicious RL Fine-Tuning via Reward Neutralization",
    "url": "http://arxiv.org/abs/2505.04578v1",
    "arxiv_id": "2505.04578v1",
    "authors": [
      "Wenjun Cao"
    ],
    "published": "2025-05-07T17:18:48+00:00",
    "summary": "Reinforcement learning (RL) fine-tuning transforms large language models while creating a vulnerability we experimentally verify: Our experiment shows that malicious RL fine-tuning dismantles safety guardrails with remarkable efficiency, requiring only 50 steps and minimal adversarial prompts, with harmful escalating from 0-2 to 7-9. This attack vector particularly threatens open-source models with parameter-level access. Existing defenses targeting supervised fine-tuning prove ineffective against RL's dynamic feedback mechanisms. We introduce Reward Neutralization, the first defense framework specifically designed against RL fine-tuning attacks, establishing concise rejection patterns that render malicious reward signals ineffective. Our approach trains models to produce minimal-information rejections that attackers cannot exploit, systematically neutralizing attempts to optimize toward harmful outputs. Experiments validate that our approach maintains low harmful scores (no greater than 2) after 200 attack steps, while standard models rapidly deteriorate. This work provides the first constructive proof that robust defense against increasingly accessible RL attacks is achievable, addressing a critical security gap for open-weight models."
  },
  {
    "title": "Stow: Robotic Packing of Items into Fabric Pods",
    "url": "http://arxiv.org/abs/2505.04572v1",
    "arxiv_id": "2505.04572v1",
    "authors": [
      "Nicolas Hudson",
      "Josh Hooks",
      "Rahul Warrier",
      "Curt Salisbury",
      "Ross Hartley",
      "Kislay Kumar",
      "Bhavana Chandrashekhar",
      "Paul Birkmeyer",
      "Bosch Tang",
      "Matt Frost",
      "Shantanu Thakar",
      "Tony Piaskowy",
      "Petter Nilsson",
      "Josh Petersen",
      "Neel Doshi",
      "Alan Slatter",
      "Ankit Bhatia",
      "Cassie Meeker",
      "Yuechuan Xue",
      "Dylan Cox",
      "Alex Kyriazis",
      "Bai Lou",
      "Nadeem Hasan",
      "Asif Rana",
      "Nikhil Chacko",
      "Ruinian Xu",
      "Siamak Faal",
      "Esi Seraj",
      "Mudit Agrawal",
      "Kevin Jamieson",
      "Alessio Bisagni",
      "Valerie Samzun",
      "Christine Fuller",
      "Alex Keklak",
      "Alex Frenkel",
      "Lillian Ratliff",
      "Aaron Parness"
    ],
    "published": "2025-05-07T17:07:09+00:00",
    "summary": "This paper presents a compliant manipulation system capable of placing items onto densely packed shelves. The wide diversity of items and strict business requirements for high producing rates and low defect generation have prohibited warehouse robotics from performing this task. Our innovations in hardware, perception, decision-making, motion planning, and control have enabled this system to perform over 500,000 stows in a large e-commerce fulfillment center. The system achieves human levels of packing density and speed while prioritizing work on overhead shelves to enhance the safety of humans working alongside the robots."
  },
  {
    "title": "Runtime Advocates: A Persona-Driven Framework for Requirements@Runtime Decision Support",
    "url": "http://arxiv.org/abs/2505.04551v1",
    "arxiv_id": "2505.04551v1",
    "authors": [
      "Demetrius Hernandez",
      "Jane Cleland-Huang"
    ],
    "published": "2025-05-07T16:31:38+00:00",
    "summary": "Complex systems, such as small Uncrewed Aerial Systems (sUAS) swarms dispatched for emergency response, often require dynamic reconfiguration at runtime under the supervision of human operators. This introduces human-on-the-loop requirements, where evolving needs shape ongoing system functionality and behaviors. While traditional personas support upfront, static requirements elicitation, we propose a persona-based advocate framework for runtime requirements engineering to provide ethically informed, safety-driven, and regulatory-aware decision support. Our approach extends standard personas into event-driven personas. When triggered by events such as adverse environmental conditions, evolving mission state, or operational constraints, the framework updates the sUAS operator's view of the personas, ensuring relevance to current conditions. We create three key advocate personas, namely Safety Controller, Ethical Governor, and Regulatory Auditor, to manage trade-offs among risk, ethical considerations, and regulatory compliance. We perform a proof-of-concept validation in an emergency response scenario using sUAS, showing how our advocate personas provide context-aware guidance grounded in safety, regulatory, and ethical constraints. By evolving static, design-time personas into adaptive, event-driven advocates, the framework surfaces mission-critical runtime requirements in response to changing conditions. These requirements shape operator decisions in real time, aligning actions with the operational demands of the moment."
  },
  {
    "title": "Pangu Ultra MoE: How to Train Your Big MoE on Ascend NPUs",
    "url": "http://arxiv.org/abs/2505.04519v1",
    "arxiv_id": "2505.04519v1",
    "authors": [
      "Yehui Tang",
      "Yichun Yin",
      "Yaoyuan Wang",
      "Hang Zhou",
      "Yu Pan",
      "Wei Guo",
      "Ziyang Zhang",
      "Miao Rang",
      "Fangcheng Liu",
      "Naifu Zhang",
      "Binghan Li",
      "Yonghan Dong",
      "Xiaojun Meng",
      "Yasheng Wang",
      "Dong Li",
      "Yin Li",
      "Dandan Tu",
      "Can Chen",
      "Youliang Yan",
      "Fisher Yu",
      "Ruiming Tang",
      "Yunhe Wang",
      "Botian Huang",
      "Bo Wang",
      "Boxiao Liu",
      "Changzheng Zhang",
      "Da Kuang",
      "Fei Liu",
      "Gang Huang",
      "Jiansheng Wei",
      "Jiarui Qin",
      "Jie Ran",
      "Jinpeng Li",
      "Jun Zhao",
      "Liang Dai",
      "Lin Li",
      "Liqun Deng",
      "Peifeng Qin",
      "Pengyuan Zeng",
      "Qiang Gu",
      "Shaohua Tang",
      "Shengjun Cheng",
      "Tao Gao",
      "Tao Yu",
      "Tianshu Li",
      "Tianyu Bi",
      "Wei He",
      "Weikai Mao",
      "Wenyong Huang",
      "Wulong Liu",
      "Xiabing Li",
      "Xianzhi Yu",
      "Xueyu Wu",
      "Xu He",
      "Yangkai Du",
      "Yan Xu",
      "Ye Tian",
      "Yimeng Wu",
      "Yongbing Huang",
      "Yong Tian",
      "Yong Zhu",
      "Yue Li",
      "Yufei Wang",
      "Yuhang Gai",
      "Yujun Li",
      "Yu Luo",
      "Yunsheng Ni",
      "Yusen Sun",
      "Zelin Chen",
      "Zhe Liu",
      "Zhicheng Liu",
      "Zhipeng Tu",
      "Zilin Ding",
      "Zongyuan Zhan"
    ],
    "published": "2025-05-07T15:46:36+00:00",
    "summary": "Sparse large language models (LLMs) with Mixture of Experts (MoE) and close to a trillion parameters are dominating the realm of most capable language models. However, the massive model scale poses significant challenges for the underlying software and hardware systems. In this paper, we aim to uncover a recipe to harness such scale on Ascend NPUs. The key goals are better usage of the computing resources under the dynamic sparse model structures and materializing the expected performance gain on the actual hardware. To select model configurations suitable for Ascend NPUs without repeatedly running the expensive experiments, we leverage simulation to compare the trade-off of various model hyperparameters. This study led to Pangu Ultra MoE, a sparse LLM with 718 billion parameters, and we conducted experiments on the model to verify the simulation results. On the system side, we dig into Expert Parallelism to optimize the communication between NPU devices to reduce the synchronization overhead. We also optimize the memory efficiency within the devices to further reduce the parameter and activation management overhead. In the end, we achieve an MFU of 30.0% when training Pangu Ultra MoE, with performance comparable to that of DeepSeek R1, on 6K Ascend NPUs, and demonstrate that the Ascend system is capable of harnessing all the training stages of the state-of-the-art language models. Extensive experiments indicate that our recipe can lead to efficient training of large-scale sparse language models with MoE. We also study the behaviors of such models for future reference."
  },
  {
    "title": "Advancements in Solid-State Sodium-Based Batteries: A Comprehensive Review",
    "url": "http://arxiv.org/abs/2505.04391v1",
    "arxiv_id": "2505.04391v1",
    "authors": [
      "Arianna Massaro",
      "Lorenzo Squillantini",
      "Francesca De Giorgio",
      "Francesca A. Scaramuzzo",
      "Mauro Pasquali",
      "Sergio Brutti"
    ],
    "published": "2025-05-07T13:14:24+00:00",
    "summary": "This manuscript explores recent advancements in solid-state sodium-based battery technology, particularly focusing on electrochemical performance and the challenges associated with developing efficient solid electrolytes. The replacement of conventional liquid electrolytes with solid-state alternatives offers numerous benefits, including enhanced safety and environmental sustainability, as solid-state systems reduce flammability and harsh chemical handling. The work emphasizes the importance of structure and interface characteristics in solid electrolytes, which play a critical role in ionic conductivity and overall battery performance. Various classes of solid electrolytes, such as sodium-based anti-perovskites and sulphide electrolytes, are examined, highlighting their unique ionic transport mechanisms and mechanical properties that facilitate stable cycling. The manuscript also discusses strategies to enhance interfacial stability between the anode and the solid electrolyte to mitigate performance degradation during battery operation. Furthermore, advancements in electrode formulations and the integration of novel materials are considered pivotal in optimizing the charging and discharging processes, thus improving the energy and power densities of sodium batteries. The outlook on the future of sodium-based solid-state batteries underscores their potential to meet emerging energy storage demands while leveraging the abundant availability of sodium compared to lithium. This comprehensive review aims to provide insights into ongoing research and prospective directions for the commercialization of solid-state sodium-based batteries, positioning them as viable alternatives in the renewable energy landscape."
  },
  {
    "title": "The Aloe Family Recipe for Open and Specialized Healthcare LLMs",
    "url": "http://arxiv.org/abs/2505.04388v1",
    "arxiv_id": "2505.04388v1",
    "authors": [
      "Dario Garcia-Gasulla",
      "Jordi Bayarri-Planas",
      "Ashwin Kumar Gururajan",
      "Enrique Lopez-Cuena",
      "Adrian Tormos",
      "Daniel Hinjos",
      "Pablo Bernabeu-Perez",
      "Anna Arias-Duart",
      "Pablo Agustin Martin-Torres",
      "Marta Gonzalez-Mallo",
      "Sergio Alvarez-Napagao",
      "Eduard Ayguad\u00e9-Parra",
      "Ulises Cort\u00e9s"
    ],
    "published": "2025-05-07T13:13:14+00:00",
    "summary": "Purpose: With advancements in Large Language Models (LLMs) for healthcare, the need arises for competitive open-source models to protect the public interest. This work contributes to the field of open medical LLMs by optimizing key stages of data preprocessing and training, while showing how to improve model safety (through DPO) and efficacy (through RAG). The evaluation methodology used, which includes four different types of tests, defines a new standard for the field. The resultant models, shown to be competitive with the best private alternatives, are released with a permisive license.   Methods: Building on top of strong base models like Llama 3.1 and Qwen 2.5, Aloe Beta uses a custom dataset to enhance public data with synthetic Chain of Thought examples. The models undergo alignment with Direct Preference Optimization, emphasizing ethical and policy-aligned performance in the presence of jailbreaking attacks. Evaluation includes close-ended, open-ended, safety and human assessments, to maximize the reliability of results.   Results: Recommendations are made across the entire pipeline, backed by the solid performance of the Aloe Family. These models deliver competitive performance across healthcare benchmarks and medical fields, and are often preferred by healthcare professionals. On bias and toxicity, the Aloe Beta models significantly improve safety, showing resilience to unseen jailbreaking attacks. For a responsible release, a detailed risk assessment specific to healthcare is attached to the Aloe Family models.   Conclusion: The Aloe Beta models, and the recipe that leads to them, are a significant contribution to the open-source medical LLM field, offering top-of-the-line performance while maintaining high ethical requirements. This work sets a new standard for developing and reporting aligned LLMs in healthcare."
  },
  {
    "title": "Consensus-Aware AV Behavior: Trade-offs Between Safety, Interaction, and Performance in Mixed Urban Traffic",
    "url": "http://arxiv.org/abs/2505.04379v1",
    "arxiv_id": "2505.04379v1",
    "authors": [
      "Mohammad Elayan",
      "Wissam Kontar"
    ],
    "published": "2025-05-07T12:59:59+00:00",
    "summary": "Transportation systems have long been shaped by complexity and heterogeneity, driven by the interdependency of agent actions and traffic outcomes. The deployment of automated vehicles (AVs) in such systems introduces a new challenge: achieving consensus across safety, interaction quality, and traffic performance. In this work, we position consensus as a fundamental property of the traffic system and aim to quantify it. We use high-resolution trajectory data from the Third Generation Simulation (TGSIM) dataset to empirically analyze AV and human-driven vehicle (HDV) behavior at a signalized urban intersection and around vulnerable road users (VRUs). Key metrics, including Time-to-Collision (TTC), Post-Encroachment Time (PET), deceleration patterns, headways, and string stability, are evaluated across the three performance dimensions. Results show that full consensus across safety, interaction, and performance is rare, with only 1.63% of AV-VRU interaction frames meeting all three conditions. These findings highlight the need for AV models that explicitly balance multi-dimensional performance in mixed-traffic environments. Full reproducibility is supported via our open-source codebase on https://github.com/wissamkontar/Consensus-AV-Analysis."
  },
  {
    "title": "Resist Platform-Controlled AI Agents and Champion User-Centric Agent Advocates",
    "url": "http://arxiv.org/abs/2505.04345v1",
    "arxiv_id": "2505.04345v1",
    "authors": [
      "Sayash Kapoor",
      "Noam Kolt",
      "Seth Lazar"
    ],
    "published": "2025-05-07T11:45:38+00:00",
    "summary": "Language model agents could reshape how users navigate and act in digital environments. If controlled by platform companies -- either those that already dominate online search, communication, and commerce, or those vying to replace them -- platform agents could intensify surveillance, exacerbate user lock-in, and further entrench the incumbent digital giants. This position paper argues that to resist the undesirable effects of platform agents, we should champion agent advocates -- agents that are controlled by users, serve the interests of users, and preserve user autonomy and choice. We identify key interventions to enable agent advocates: ensuring public access to compute, developing interoperability protocols and safety standards, and implementing appropriate market regulations."
  },
  {
    "title": "Detecting Concept Drift in Neural Networks Using Chi-squared Goodness of Fit Testing",
    "url": "http://arxiv.org/abs/2505.04318v1",
    "arxiv_id": "2505.04318v1",
    "authors": [
      "Jacob Glenn Ayers",
      "Buvaneswari A. Ramanan",
      "Manzoor A. Khan"
    ],
    "published": "2025-05-07T11:04:47+00:00",
    "summary": "As the adoption of deep learning models has grown beyond human capacity for verification, meta-algorithms are needed to ensure reliable model inference. Concept drift detection is a field dedicated to identifying statistical shifts that is underutilized in monitoring neural networks that may encounter inference data with distributional characteristics diverging from their training data. Given the wide variety of model architectures, applications, and datasets, it is important that concept drift detection algorithms are adaptable to different inference scenarios. In this paper, we introduce an application of the $\\chi^2$ Goodness of Fit Hypothesis Test as a drift detection meta-algorithm applied to a multilayer perceptron, a convolutional neural network, and a transformer trained for machine vision as they are exposed to simulated drift during inference. To that end, we demonstrate how unexpected drops in accuracy due to concept drift can be detected without directly examining the inference outputs. Our approach enhances safety by ensuring models are continually evaluated for reliability across varying conditions."
  },
  {
    "title": "REVEAL: Multi-turn Evaluation of Image-Input Harms for Vision LLM",
    "url": "http://arxiv.org/abs/2505.04673v1",
    "arxiv_id": "2505.04673v1",
    "authors": [
      "Madhur Jindal",
      "Saurabh Deshpande"
    ],
    "published": "2025-05-07T10:09:55+00:00",
    "summary": "Vision Large Language Models (VLLMs) represent a significant advancement in artificial intelligence by integrating image-processing capabilities with textual understanding, thereby enhancing user interactions and expanding application domains. However, their increased complexity introduces novel safety and ethical challenges, particularly in multi-modal and multi-turn conversations. Traditional safety evaluation frameworks, designed for text-based, single-turn interactions, are inadequate for addressing these complexities. To bridge this gap, we introduce the REVEAL (Responsible Evaluation of Vision-Enabled AI LLMs) Framework, a scalable and automated pipeline for evaluating image-input harms in VLLMs. REVEAL includes automated image mining, synthetic adversarial data generation, multi-turn conversational expansion using crescendo attack strategies, and comprehensive harm assessment through evaluators like GPT-4o.   We extensively evaluated five state-of-the-art VLLMs, GPT-4o, Llama-3.2, Qwen2-VL, Phi3.5V, and Pixtral, across three important harm categories: sexual harm, violence, and misinformation. Our findings reveal that multi-turn interactions result in significantly higher defect rates compared to single-turn evaluations, highlighting deeper vulnerabilities in VLLMs. Notably, GPT-4o demonstrated the most balanced performance as measured by our Safety-Usability Index (SUI) followed closely by Pixtral. Additionally, misinformation emerged as a critical area requiring enhanced contextual defenses. Llama-3.2 exhibited the highest MT defect rate ($16.55 \\%$) while Qwen2-VL showed the highest MT refusal rate ($19.1 \\%$)."
  },
  {
    "title": "From Incidents to Insights: Patterns of Responsibility following AI Harms",
    "url": "http://arxiv.org/abs/2505.04291v1",
    "arxiv_id": "2505.04291v1",
    "authors": [
      "Isabel Richards",
      "Claire Benn",
      "Miri Zilka"
    ],
    "published": "2025-05-07T09:59:36+00:00",
    "summary": "The AI Incident Database was inspired by aviation safety databases, which enable collective learning from failures to prevent future incidents. The database documents hundreds of AI failures, collected from the news and media. However, criticism highlights that the AIID's reliance on media reporting limits its utility for learning about implementation failures. In this paper, we accept that the AIID falls short in its original mission, but argue that by looking beyond technically-focused learning, the dataset can provide new, highly valuable insights: specifically, opportunities to learn about patterns between developers, deployers, victims, wider society, and law-makers that emerge after AI failures. Through a three-tier mixed-methods analysis of 962 incidents and 4,743 related reports from the AIID, we examine patterns across incidents, focusing on cases with public responses tagged in the database. We identify 'typical' incidents found in the AIID, from Tesla crashes to deepfake scams.   Focusing on this interplay between relevant parties, we uncover patterns in accountability and social expectations of responsibility. We find that the presence of identifiable responsible parties does not necessarily lead to increased accountability. The likelihood of a response and what it amounts to depends highly on context, including who built the technology, who was harmed, and to what extent. Controversy-rich incidents provide valuable data about societal reactions, including insights into social expectations. Equally informative are cases where controversy is notably absent. This work shows that the AIID's value lies not just in preventing technical failures, but in documenting patterns of harms and of institutional response and social learning around AI incidents. These patterns offer crucial insights for understanding how society adapts to and governs emerging AI technologies."
  },
  {
    "title": "Multi-Agent Reinforcement Learning-based Cooperative Autonomous Driving in Smart Intersections",
    "url": "http://arxiv.org/abs/2505.04231v1",
    "arxiv_id": "2505.04231v1",
    "authors": [
      "Taoyuan Yu",
      "Kui Wang",
      "Zongdian Li",
      "Tao Yu",
      "Kei Sakaguchi"
    ],
    "published": "2025-05-07T08:27:52+00:00",
    "summary": "Unsignalized intersections pose significant safety and efficiency challenges due to complex traffic flows. This paper proposes a novel roadside unit (RSU)-centric cooperative driving system leveraging global perception and vehicle-to-infrastructure (V2I) communication. The core of the system is an RSU-based decision-making module using a two-stage hybrid reinforcement learning (RL) framework. At first, policies are pre-trained offline using conservative Q-learning (CQL) combined with behavior cloning (BC) on collected dataset. Subsequently, these policies are fine-tuned in the simulation using multi-agent proximal policy optimization (MAPPO), aligned with a self-attention mechanism to effectively solve inter-agent dependencies. RSUs perform real-time inference based on the trained models to realize vehicle control via V2I communications. Extensive experiments in CARLA environment demonstrate high effectiveness of the proposed system, by: \\textit{(i)} achieving failure rates below 0.03\\% in coordinating three connected and autonomous vehicles (CAVs) through complex intersection scenarios, significantly outperforming the traditional Autoware control method, and \\textit{(ii)} exhibiting strong robustness across varying numbers of controlled agents and shows promising generalization capabilities on other maps."
  },
  {
    "title": "An Enhanced YOLOv8 Model for Real-Time and Accurate Pothole Detection and Measurement",
    "url": "http://arxiv.org/abs/2505.04207v1",
    "arxiv_id": "2505.04207v1",
    "authors": [
      "Mustafa Yurdakul",
      "\u015eakir Tasdemir"
    ],
    "published": "2025-05-07T07:58:57+00:00",
    "summary": "Potholes cause vehicle damage and traffic accidents, creating serious safety and economic problems. Therefore, early and accurate detection of potholes is crucial. Existing detection methods are usually only based on 2D RGB images and cannot accurately analyze the physical characteristics of potholes. In this paper, a publicly available dataset of RGB-D images (PothRGBD) is created and an improved YOLOv8-based model is proposed for both pothole detection and pothole physical features analysis. The Intel RealSense D415 depth camera was used to collect RGB and depth data from the road surfaces, resulting in a PothRGBD dataset of 1000 images. The data was labeled in YOLO format suitable for segmentation. A novel YOLO model is proposed based on the YOLOv8n-seg architecture, which is structurally improved with Dynamic Snake Convolution (DSConv), Simple Attention Module (SimAM) and Gaussian Error Linear Unit (GELU). The proposed model segmented potholes with irregular edge structure more accurately, and performed perimeter and depth measurements on depth maps with high accuracy. The standard YOLOv8n-seg model achieved 91.9% precision, 85.2% recall and 91.9% mAP@50. With the proposed model, the values increased to 93.7%, 90.4% and 93.8% respectively. Thus, an improvement of 1.96% in precision, 6.13% in recall and 2.07% in mAP was achieved. The proposed model performs pothole detection as well as perimeter and depth measurement with high accuracy and is suitable for real-time applications due to its low model complexity. In this way, a lightweight and effective model that can be used in deep learning-based intelligent transportation solutions has been acquired."
  },
  {
    "title": "Unmasking the Canvas: A Dynamic Benchmark for Image Generation Jailbreaking and LLM Content Safety",
    "url": "http://arxiv.org/abs/2505.04146v1",
    "arxiv_id": "2505.04146v1",
    "authors": [
      "Variath Madhupal Gautham Nair",
      "Vishal Varma Dantuluri"
    ],
    "published": "2025-05-07T05:54:04+00:00",
    "summary": "Existing large language models (LLMs) are advancing rapidly and produce outstanding results in image generation tasks, yet their content safety checks remain vulnerable to prompt-based jailbreaks. Through preliminary testing on platforms such as ChatGPT, MetaAI, and Grok, we observed that even short, natural prompts could lead to the generation of compromising images ranging from realistic depictions of forged documents to manipulated images of public figures.   We introduce Unmasking the Canvas (UTC Benchmark; UTCB), a dynamic and scalable benchmark dataset to evaluate LLM vulnerability in image generation. Our methodology combines structured prompt engineering, multilingual obfuscation (e.g., Zulu, Gaelic, Base64), and evaluation using Groq-hosted LLaMA-3. The pipeline supports both zero-shot and fallback prompting strategies, risk scoring, and automated tagging. All generations are stored with rich metadata and curated into Bronze (non-verified), Silver (LLM-aided verification), and Gold (manually verified) tiers. UTCB is designed to evolve over time with new data sources, prompt templates, and model behaviors.   Warning: This paper includes visual examples of adversarial inputs designed to test model safety. All outputs have been redacted to ensure responsible disclosure."
  },
  {
    "title": "Fine-Tuning Large Language Models and Evaluating Retrieval Methods for Improved Question Answering on Building Codes",
    "url": "http://arxiv.org/abs/2505.04666v1",
    "arxiv_id": "2505.04666v1",
    "authors": [
      "Mohammad Aqib",
      "Mohd Hamza",
      "Qipei Mei",
      "Ying Hei Chui"
    ],
    "published": "2025-05-07T05:04:30+00:00",
    "summary": "Building codes are regulations that establish standards for the design, construction, and safety of buildings to ensure structural integrity, fire protection, and accessibility. They are often extensive, complex, and subject to frequent updates, making manual querying challenging and time-consuming. Key difficulties include navigating large volumes of text, interpreting technical language, and identifying relevant clauses across different sections. A potential solution is to build a Question-Answering (QA) system that answers user queries based on building codes. Among the various methods for building a QA system, Retrieval-Augmented Generation (RAG) stands out in performance. RAG consists of two components: a retriever and a language model. This study focuses on identifying a suitable retriever method for building codes and optimizing the generational capability of the language model using fine-tuning techniques. We conducted a detailed evaluation of various retrieval methods by performing the retrieval on the National Building Code of Canada (NBCC) and explored the impact of domain-specific fine-tuning on several language models using the dataset derived from NBCC. Our analysis included a comparative assessment of different retrievers and the performance of both pre-trained and fine-tuned models to determine the efficacy and domain-specific adaptation of language models using fine-tuning on the NBCC dataset. Experimental results showed that Elasticsearch proved to be the most robust retriever among all. The findings also indicate that fine-tuning language models on an NBCC-specific dataset can enhance their ability to generate contextually relevant responses. When combined with context retrieved by a powerful retriever like Elasticsearch, this improvement in LLM performance can optimize the RAG system, enabling it to better navigate the complexities of the NBCC."
  },
  {
    "title": "In-Situ Hardware Error Detection Using Specification-Derived Petri Net Models and Behavior-Derived State Sequences",
    "url": "http://arxiv.org/abs/2505.04108v1",
    "arxiv_id": "2505.04108v1",
    "authors": [
      "Tomonari Tanaka",
      "Takumi Uezono",
      "Kohei Suenaga",
      "Masanori Hashimoto"
    ],
    "published": "2025-05-07T03:51:02+00:00",
    "summary": "In hardware accelerators used in data centers and safety-critical applications, soft errors and resultant silent data corruption significantly compromise reliability, particularly when upsets occur in control-flow operations, leading to severe failures. To address this, we introduce two methods for monitoring control flows: using specification-derived Petri nets and using behavior-derived state transitions. We validated our method across four designs: convolutional layer operation, Gaussian blur, AES encryption, and a router in Network-on-Chip. Our fault injection campaign targeting the control registers and primary control inputs demonstrated high error detection rates in both datapath and control logic. Synthesis results show that a maximum detection rate is achieved with a few to around 10% area overhead in most cases. The proposed detectors quickly detect 48% to 100% of failures resulting from upsets in internal control registers and perturbations in primary control inputs. The two proposed methods were compared in terms of area overhead and error detection rate. By selectively applying these two methods, a wide range of area constraints can be accommodated, enabling practical implementation and effectively enhancing error detection capabilities."
  },
  {
    "title": "In-Situ Hardware Error Detection Using Specification-Derived Petri Net Models and Behavior-Derived State Sequences",
    "url": "http://arxiv.org/abs/2505.04108v2",
    "arxiv_id": "2505.04108v2",
    "authors": [
      "Tomonari Tanaka",
      "Takumi Uezono",
      "Kohei Suenaga",
      "Masanori Hashimoto"
    ],
    "published": "2025-05-07T03:51:02+00:00",
    "summary": "In hardware accelerators used in data centers and safety-critical applications, soft errors and resultant silent data corruption significantly compromise reliability, particularly when upsets occur in control-flow operations, leading to severe failures. To address this, we introduce two methods for monitoring control flows: using specification-derived Petri nets and using behavior-derived state transitions. We validated our method across four designs: convolutional layer operation, Gaussian blur, AES encryption, and a router in Network-on-Chip. Our fault injection campaign targeting the control registers and primary control inputs demonstrated high error detection rates in both datapath and control logic. Synthesis results show that a maximum detection rate is achieved with a few to around 10% area overhead in most cases. The proposed detectors quickly detect 48% to 100% of failures resulting from upsets in internal control registers and perturbations in primary control inputs. The two proposed methods were compared in terms of area overhead and error detection rate. By selectively applying these two methods, a wide range of area constraints can be accommodated, enabling practical implementation and effectively enhancing error detection capabilities."
  },
  {
    "title": "Shadow Wireless Intelligence: Large Language Model-Driven Reasoning in Covert Communications",
    "url": "http://arxiv.org/abs/2505.04068v1",
    "arxiv_id": "2505.04068v1",
    "authors": [
      "Yuanai Xie",
      "Zhaozhi Liu",
      "Xiao Zhang",
      "Shihua Zhang",
      "Rui Hou",
      "Minrui Xu",
      "Ruichen Zhang",
      "Dusit Niyato"
    ],
    "published": "2025-05-07T02:11:43+00:00",
    "summary": "Covert Communications (CC) can secure sensitive transmissions in industrial, military, and mission-critical applications within 6G wireless networks. However, traditional optimization methods based on Artificial Noise (AN), power control, and channel manipulation might not adapt to dynamic and adversarial environments due to the high dimensionality, nonlinearity, and stringent real-time covertness requirements. To bridge this gap, we introduce Shadow Wireless Intelligence (SWI), which integrates the reasoning capabilities of Large Language Models (LLMs) with retrieval-augmented generation to enable intelligent decision-making in covert wireless systems. Specifically, we utilize DeepSeek-R1, a mixture-of-experts-based LLM with RL-enhanced reasoning, combined with real-time retrieval of domain-specific knowledge to improve context accuracy and mitigate hallucinations. Our approach develops a structured CC knowledge base, supports context-aware retrieval, and performs semantic optimization, allowing LLMs to generate and adapt CC strategies in real time. In a case study on optimizing AN power in a full-duplex CC scenario, DeepSeek-R1 achieves 85% symbolic derivation accuracy and 94% correctness in the generation of simulation code, outperforming baseline models. These results validate SWI as a robust, interpretable, and adaptive foundation for LLM-driven intelligent covert wireless systems in 6G networks."
  },
  {
    "title": "Reliable Disentanglement Multi-view Learning Against View Adversarial Attacks",
    "url": "http://arxiv.org/abs/2505.04046v1",
    "arxiv_id": "2505.04046v1",
    "authors": [
      "Xuyang Wang",
      "Siyuan Duan",
      "Qizhi Li",
      "Guiduo Duan",
      "Yuan Sun",
      "Dezhong Peng"
    ],
    "published": "2025-05-07T01:12:00+00:00",
    "summary": "Recently, trustworthy multi-view learning has attracted extensive attention because evidence learning can provide reliable uncertainty estimation to enhance the credibility of multi-view predictions. Existing trusted multi-view learning methods implicitly assume that multi-view data is secure. In practice, however, in safety-sensitive applications such as autonomous driving and security monitoring, multi-view data often faces threats from adversarial perturbations, thereby deceiving or disrupting multi-view learning models. This inevitably leads to the adversarial unreliability problem (AUP) in trusted multi-view learning. To overcome this tricky problem, we propose a novel multi-view learning framework, namely Reliable Disentanglement Multi-view Learning (RDML). Specifically, we first propose evidential disentanglement learning to decompose each view into clean and adversarial parts under the guidance of corresponding evidences, which is extracted by a pretrained evidence extractor. Then, we employ the feature recalibration module to mitigate the negative impact of adversarial perturbations and extract potential informative features from them. Finally, to further ignore the irreparable adversarial interferences, a view-level evidential attention mechanism is designed. Extensive experiments on multi-view classification tasks with adversarial attacks show that our RDML outperforms the state-of-the-art multi-view learning methods by a relatively large margin."
  },
  {
    "title": "Estimating the Joint Distribution of Two Binary Variables with Marginal Statistics",
    "url": "http://arxiv.org/abs/2505.03995v1",
    "arxiv_id": "2505.03995v1",
    "authors": [
      "Longwen Shang",
      "Min Tsao",
      "Xuekui Zhang"
    ],
    "published": "2025-05-06T22:15:23+00:00",
    "summary": "Clinical trial simulation (CTS) is critical in new drug development, providing insight into safety and efficacy while guiding trial design. Achieving realistic outcomes in CTS requires an accurately estimated joint distribution of the underlying variables. However, privacy concerns and data availability issues often restrict researchers to marginal summary-level data of each variable, making it challenging to estimate the joint distribution due to the lack of access to individual-level data or relational summaries between variables. We propose a novel approach based on the method of maximum likelihood that estimates the joint distribution of two binary variables using only marginal summary data. By leveraging numerical optimization and accommodating varying sample sizes across studies, our method preserves privacy while bypassing the need for granular or relational data. Through an extensive simulation study covering a diverse range of scenarios and an application to a real-world dataset, we demonstrate the accuracy, robustness, and practicality of our method. This method enhances the generation of realistic simulated data, thereby improving decision-making processes in drug development."
  },
  {
    "title": "An alignment safety case sketch based on debate",
    "url": "http://arxiv.org/abs/2505.03989v1",
    "arxiv_id": "2505.03989v1",
    "authors": [
      "Marie Davidsen Buhl",
      "Jacob Pfau",
      "Benjamin Hilton",
      "Geoffrey Irving"
    ],
    "published": "2025-05-06T21:53:44+00:00",
    "summary": "If AI systems match or exceed human capabilities on a wide range of tasks, it may become difficult for humans to efficiently judge their actions -- making it hard to use human feedback to steer them towards desirable traits. One proposed solution is to leverage another superhuman system to point out flaws in the system's outputs via a debate. This paper outlines the value of debate for AI safety, as well as the assumptions and further research required to make debate work. It does so by sketching an ``alignment safety case'' -- an argument that an AI system will not autonomously take actions which could lead to egregious harm, despite being able to do so. The sketch focuses on the risk of an AI R\\&D agent inside an AI company sabotaging research, for example by producing false results. To prevent this, the agent is trained via debate, subject to exploration guarantees, to teach the system to be honest. Honesty is maintained throughout deployment via online training. The safety case rests on four key claims: (1) the agent has become good at the debate game, (2) good performance in the debate game implies that the system is mostly honest, (3) the system will not become significantly less honest during deployment, and (4) the deployment context is tolerant of some errors. We identify open research problems that, if solved, could render this a compelling argument that an AI system is safe."
  },
  {
    "title": "An alignment safety case sketch based on debate",
    "url": "http://arxiv.org/abs/2505.03989v2",
    "arxiv_id": "2505.03989v2",
    "authors": [
      "Marie Davidsen Buhl",
      "Jacob Pfau",
      "Benjamin Hilton",
      "Geoffrey Irving"
    ],
    "published": "2025-05-06T21:53:44+00:00",
    "summary": "If AI systems match or exceed human capabilities on a wide range of tasks, it may become difficult for humans to efficiently judge their actions -- making it hard to use human feedback to steer them towards desirable traits. One proposed solution is to leverage another superhuman system to point out flaws in the system's outputs via a debate. This paper outlines the value of debate for AI safety, as well as the assumptions and further research required to make debate work. It does so by sketching an ``alignment safety case'' -- an argument that an AI system will not autonomously take actions which could lead to egregious harm, despite being able to do so. The sketch focuses on the risk of an AI R\\&D agent inside an AI company sabotaging research, for example by producing false results. To prevent this, the agent is trained via debate, subject to exploration guarantees, to teach the system to be honest. Honesty is maintained throughout deployment via online training. The safety case rests on four key claims: (1) the agent has become good at the debate game, (2) good performance in the debate game implies that the system is mostly honest, (3) the system will not become significantly less honest during deployment, and (4) the deployment context is tolerant of some errors. We identify open research problems that, if solved, could render this a compelling argument that an AI system is safe."
  },
  {
    "title": "LogiDebrief: A Signal-Temporal Logic based Automated Debriefing Approach with Large Language Models Integration",
    "url": "http://arxiv.org/abs/2505.03985v1",
    "arxiv_id": "2505.03985v1",
    "authors": [
      "Zirong Chen",
      "Ziyan An",
      "Jennifer Reynolds",
      "Kristin Mullen",
      "Stephen Martini",
      "Meiyi Ma"
    ],
    "published": "2025-05-06T21:27:07+00:00",
    "summary": "Emergency response services are critical to public safety, with 9-1-1 call-takers playing a key role in ensuring timely and effective emergency operations. To ensure call-taking performance consistency, quality assurance is implemented to evaluate and refine call-takers' skillsets. However, traditional human-led evaluations struggle with high call volumes, leading to low coverage and delayed assessments. We introduce LogiDebrief, an AI-driven framework that automates traditional 9-1-1 call debriefing by integrating Signal-Temporal Logic (STL) with Large Language Models (LLMs) for fully-covered rigorous performance evaluation. LogiDebrief formalizes call-taking requirements as logical specifications, enabling systematic assessment of 9-1-1 calls against procedural guidelines. It employs a three-step verification process: (1) contextual understanding to identify responder types, incident classifications, and critical conditions; (2) STL-based runtime checking with LLM integration to ensure compliance; and (3) automated aggregation of results into quality assurance reports. Beyond its technical contributions, LogiDebrief has demonstrated real-world impact. Successfully deployed at Metro Nashville Department of Emergency Communications, it has assisted in debriefing 1,701 real-world calls, saving 311.85 hours of active engagement. Empirical evaluation with real-world data confirms its accuracy, while a case study and extensive user study highlight its effectiveness in enhancing call-taking performance."
  },
  {
    "title": "X-Reasoner: Towards Generalizable Reasoning Across Modalities and Domains",
    "url": "http://arxiv.org/abs/2505.03981v1",
    "arxiv_id": "2505.03981v1",
    "authors": [
      "Qianchu Liu",
      "Sheng Zhang",
      "Guanghui Qin",
      "Timothy Ossowski",
      "Yu Gu",
      "Ying Jin",
      "Sid Kiblawi",
      "Sam Preston",
      "Mu Wei",
      "Paul Vozila",
      "Tristan Naumann",
      "Hoifung Poon"
    ],
    "published": "2025-05-06T21:08:27+00:00",
    "summary": "Recent proprietary models (e.g., o3) have begun to demonstrate strong multimodal reasoning capabilities. Yet, most existing open-source research concentrates on training text-only reasoning models, with evaluations limited to mainly mathematical and general-domain tasks. Therefore, it remains unclear how to effectively extend reasoning capabilities beyond text input and general domains. This paper explores a fundamental research question: Is reasoning generalizable across modalities and domains? Our findings support an affirmative answer: General-domain text-based post-training can enable such strong generalizable reasoning. Leveraging this finding, we introduce X-Reasoner, a vision-language model post-trained solely on general-domain text for generalizable reasoning, using a two-stage approach: an initial supervised fine-tuning phase with distilled long chain-of-thoughts, followed by reinforcement learning with verifiable rewards. Experiments show that X-Reasoner successfully transfers reasoning capabilities to both multimodal and out-of-domain settings, outperforming existing state-of-the-art models trained with in-domain and multimodal data across various general and medical benchmarks (Figure 1). Additionally, we find that X-Reasoner's performance in specialized domains can be further enhanced through continued training on domain-specific text-only data. Building upon this, we introduce X-Reasoner-Med, a medical-specialized variant that achieves new state of the art on numerous text-only and multimodal medical benchmarks."
  },
  {
    "title": "Recent Advances in Disaster Emergency Response Planning: Integrating Optimization, Machine Learning, and Simulation",
    "url": "http://arxiv.org/abs/2505.03979v1",
    "arxiv_id": "2505.03979v1",
    "authors": [
      "Fan Pu",
      "Zihao Li",
      "Yifan Wu",
      "Chaolun Ma",
      "Ruonan Zhao"
    ],
    "published": "2025-05-06T21:05:31+00:00",
    "summary": "The increasing frequency and severity of natural disasters underscore the critical importance of effective disaster emergency response planning to minimize human and economic losses. This survey provides a comprehensive review of recent advancements (2019--2024) in five essential areas of disaster emergency response planning: evacuation, facility location, casualty transport, search and rescue, and relief distribution. Research in these areas is systematically categorized based on methodologies, including optimization models, machine learning, and simulation, with a focus on their individual strengths and synergies. A notable contribution of this work is its examination of the interplay between machine learning, simulation, and optimization frameworks, highlighting how these approaches can address the dynamic, uncertain, and complex nature of disaster scenarios. By identifying key research trends and challenges, this study offers valuable insights to improve the effectiveness and resilience of emergency response strategies in future disaster planning efforts."
  },
  {
    "title": "A Comparative Analysis of Ethical and Safety Gaps in LLMs using Relative Danger Coefficient",
    "url": "http://arxiv.org/abs/2505.04654v1",
    "arxiv_id": "2505.04654v1",
    "authors": [
      "Yehor Tereshchenko",
      "Mika H\u00e4m\u00e4l\u00e4inen"
    ],
    "published": "2025-05-06T20:58:04+00:00",
    "summary": "Artificial Intelligence (AI) and Large Language Models (LLMs) have rapidly evolved in recent years, showcasing remarkable capabilities in natural language understanding and generation. However, these advancements also raise critical ethical questions regarding safety, potential misuse, discrimination and overall societal impact. This article provides a comparative analysis of the ethical performance of various AI models, including the brand new DeepSeek-V3(R1 with reasoning and without), various GPT variants (4o, 3.5 Turbo, 4 Turbo, o1/o3 mini) and Gemini (1.5 flash, 2.0 flash and 2.0 flash exp) and highlights the need for robust human oversight, especially in situations with high stakes. Furthermore, we present a new metric for calculating harm in LLMs called Relative Danger Coefficient (RDC)."
  },
  {
    "title": "Learning Interactions Between Continuous Treatments and Covariates with a Semiparametric Model",
    "url": "http://arxiv.org/abs/2505.03893v1",
    "arxiv_id": "2505.03893v1",
    "authors": [
      "Muyan Jiang",
      "Yunkai Zhang",
      "Anil Aswani"
    ],
    "published": "2025-05-06T18:01:00+00:00",
    "summary": "Estimating the impact of continuous treatment variables (e.g., dosage amount) on binary outcomes presents significant challenges in modeling and estimation because many existing approaches make strong assumptions that do not hold for certain continuous treatment variables. For instance, traditional logistic regression makes strong linearity assumptions that do not hold for continuous treatment variables like time of initiation. In this work, we propose a semiparametric regression framework that decomposes effects into two interpretable components: a prognostic score that captures baseline outcome risk based on a combination of clinical, genetic, and sociodemographic features, and a treatment-interaction score that flexibly models the optimal treatment level via a nonparametric link function. By connecting these two parametric scores with Nadaraya-Watson regression, our approach is both interpretable and flexible. The potential of our approach is demonstrated through numerical simulations that show empirical estimation convergence. We conclude by applying our approach to a real-world case study using the International Warfarin Pharmacogenomics Consortium (IWPC) dataset to show our approach's clinical utility by deriving personalized warfarin dosing recommendations that integrate both genetic and clinical data, providing insights towards enhancing patient safety and therapeutic efficacy in anticoagulation therapy."
  },
  {
    "title": "WebGen-Bench: Evaluating LLMs on Generating Interactive and Functional Websites from Scratch",
    "url": "http://arxiv.org/abs/2505.03733v1",
    "arxiv_id": "2505.03733v1",
    "authors": [
      "Zimu Lu",
      "Yunqiao Yang",
      "Houxing Ren",
      "Haotian Hou",
      "Han Xiao",
      "Ke Wang",
      "Weikang Shi",
      "Aojun Zhou",
      "Mingjie Zhan",
      "Hongsheng Li"
    ],
    "published": "2025-05-06T17:59:15+00:00",
    "summary": "LLM-based agents have demonstrated great potential in generating and managing code within complex codebases. In this paper, we introduce WebGen-Bench, a novel benchmark designed to measure an LLM-based agent's ability to create multi-file website codebases from scratch. It contains diverse instructions for website generation, created through the combined efforts of human annotators and GPT-4o. These instructions span three major categories and thirteen minor categories, encompassing nearly all important types of web applications. To assess the quality of the generated websites, we use GPT-4o to generate test cases targeting each functionality described in the instructions, and then manually filter, adjust, and organize them to ensure accuracy, resulting in 647 test cases. Each test case specifies an operation to be performed on the website and the expected result after the operation. To automate testing and improve reproducibility, we employ a powerful web-navigation agent to execute tests on the generated websites and determine whether the observed responses align with the expected results. We evaluate three high-performance code-agent frameworks, Bolt.diy, OpenHands, and Aider, using multiple proprietary and open-source LLMs as engines. The best-performing combination, Bolt.diy powered by DeepSeek-R1, achieves only 27.8\\% accuracy on the test cases, highlighting the challenging nature of our benchmark. Additionally, we construct WebGen-Instruct, a training set consisting of 6,667 website-generation instructions. Training Qwen2.5-Coder-32B-Instruct on Bolt.diy trajectories generated from a subset of this training set achieves an accuracy of 38.2\\%, surpassing the performance of the best proprietary model."
  },
  {
    "title": "Toward a Harmonized Approach -- Requirement-based Structuring of a Safety Assurance Argumentation for Automated Vehicles",
    "url": "http://arxiv.org/abs/2505.03709v1",
    "arxiv_id": "2505.03709v1",
    "authors": [
      "M. Loba",
      "N. F. Salem",
      "M. Nolte",
      "A. Dotzler",
      "M. Maurer"
    ],
    "published": "2025-05-06T17:28:30+00:00",
    "summary": "Despite increasing testing operation on public roads, media reports on incidents show that safety issues remain to this day. One major cause factoring into this circumstance is high development uncertainty that manufacturers face when deploying these systems in an open context. In particular, one challenge is establishing a valid argument at design time that the vehicle will exhibit reasonable residual risk when operating in its intended operational design domain. Regulations, such as the European Implementing Regulation 2022/1426, require manufacturers to provide a safety assurance argumentation for SAE-Level-4 automated vehicles. While there is extensive literature on assurance cases for safety-critical systems, the domain of automated driving lacks explicit requirements regarding the creation of safety assurance argumentations. In this paper, we aim to narrow this gap by elaborating a requirement-based approach. We derive structural requirements for an argumentation from literature and supplement these with requirements derived from stakeholder concerns. We implement the requirements, yielding a proposal for an overall argumentation structure. The resulting \"safety arguments\" argue over four topic complexes: The developed product, the underlying process including its conformance/compliance to standards/laws, as well as the argumentations' context and soundness. Finally, we instantiate this structure with respect to domain-specific needs and principles."
  },
  {
    "title": "Toward a Harmonized Approach - Requirement-based Structuring of a Safety Assurance Argumentation for Automated Vehicles",
    "url": "http://arxiv.org/abs/2505.03709v2",
    "arxiv_id": "2505.03709v2",
    "authors": [
      "Marvin Loba",
      "Nayel Fabian Salem",
      "Marcus Nolte",
      "Andreas Dotzler",
      "Dieter Ludwig",
      "Markus Maurer"
    ],
    "published": "2025-05-06T17:28:30+00:00",
    "summary": "Despite increasing testing operation on public roads, media reports on incidents show that safety issues remain to this day. One major cause factoring into this circumstance is high development uncertainty that manufacturers face when deploying these systems in an open context. In particular, one challenge is establishing a valid argument at design time that the vehicle will exhibit reasonable residual risk when operating in its intended operational design domain. Regulations, such as the European Implementing Regulation 2022/1426, require manufacturers to provide a safety assurance argumentation for SAE-Level-4 automated vehicles. While there is extensive literature on assurance cases for safety-critical systems, the domain of automated driving lacks explicit requirements regarding the creation of safety assurance argumentations. In this paper, we aim to narrow this gap by elaborating a requirement-based approach. We derive structural requirements for an argumentation from literature and supplement these with requirements derived from stakeholder concerns. We implement the requirements, yielding a proposal for an overall argumentation structure. The resulting \"safety arguments\" argue over four topic complexes: The developed product, the underlying process including its conformance/compliance to standards/laws, as well as the argumentations' context and soundness. Finally, we instantiate this structure with respect to domain-specific needs and principles."
  },
  {
    "title": "Frenet Corridor Planner: An Optimal Local Path Planning Framework for Autonomous Driving",
    "url": "http://arxiv.org/abs/2505.03695v1",
    "arxiv_id": "2505.03695v1",
    "authors": [
      "Faizan M. Tariq",
      "Zheng-Hang Yeh",
      "Avinash Singh",
      "David Isele",
      "Sangjae Bae"
    ],
    "published": "2025-05-06T17:00:32+00:00",
    "summary": "Motivated by the requirements for effectiveness and efficiency, path-speed decomposition-based trajectory planning methods have widely been adopted for autonomous driving applications. While a global route can be pre-computed offline, real-time generation of adaptive local paths remains crucial. Therefore, we present the Frenet Corridor Planner (FCP), an optimization-based local path planning strategy for autonomous driving that ensures smooth and safe navigation around obstacles. Modeling the vehicles as safety-augmented bounding boxes and pedestrians as convex hulls in the Frenet space, our approach defines a drivable corridor by determining the appropriate deviation side for static obstacles. Thereafter, a modified space-domain bicycle kinematics model enables path optimization for smoothness, boundary clearance, and dynamic obstacle risk minimization. The optimized path is then passed to a speed planner to generate the final trajectory. We validate FCP through extensive simulations and real-world hardware experiments, demonstrating its efficiency and effectiveness."
  },
  {
    "title": "Demonstrating ViSafe: Vision-enabled Safety for High-speed Detect and Avoid",
    "url": "http://arxiv.org/abs/2505.03694v1",
    "arxiv_id": "2505.03694v1",
    "authors": [
      "Parv Kapoor",
      "Ian Higgins",
      "Nikhil Keetha",
      "Jay Patrikar",
      "Brady Moon",
      "Zelin Ye",
      "Yao He",
      "Ivan Cisneros",
      "Yaoyu Hu",
      "Changliu Liu",
      "Eunsuk Kang",
      "Sebastian Scherer"
    ],
    "published": "2025-05-06T16:59:54+00:00",
    "summary": "Assured safe-separation is essential for achieving seamless high-density operation of airborne vehicles in a shared airspace. To equip resource-constrained aerial systems with this safety-critical capability, we present ViSafe, a high-speed vision-only airborne collision avoidance system. ViSafe offers a full-stack solution to the Detect and Avoid (DAA) problem by tightly integrating a learning-based edge-AI framework with a custom multi-camera hardware prototype designed under SWaP-C constraints. By leveraging perceptual input-focused control barrier functions (CBF) to design, encode, and enforce safety thresholds, ViSafe can provide provably safe runtime guarantees for self-separation in high-speed aerial operations. We evaluate ViSafe's performance through an extensive test campaign involving both simulated digital twins and real-world flight scenarios. By independently varying agent types, closure rates, interaction geometries, and environmental conditions (e.g., weather and lighting), we demonstrate that ViSafe consistently ensures self-separation across diverse scenarios. In first-of-its-kind real-world high-speed collision avoidance tests with closure rates reaching 144 km/h, ViSafe sets a new benchmark for vision-only autonomous collision avoidance, establishing a new standard for safety in high-speed aerial navigation."
  },
  {
    "title": "Demonstrating ViSafe: Vision-enabled Safety for High-speed Detect and Avoid",
    "url": "http://arxiv.org/abs/2505.03694v2",
    "arxiv_id": "2505.03694v2",
    "authors": [
      "Parv Kapoor",
      "Ian Higgins",
      "Nikhil Keetha",
      "Jay Patrikar",
      "Brady Moon",
      "Zelin Ye",
      "Yao He",
      "Ivan Cisneros",
      "Yaoyu Hu",
      "Changliu Liu",
      "Eunsuk Kang",
      "Sebastian Scherer"
    ],
    "published": "2025-05-06T16:59:54+00:00",
    "summary": "Assured safe-separation is essential for achieving seamless high-density operation of airborne vehicles in a shared airspace. To equip resource-constrained aerial systems with this safety-critical capability, we present ViSafe, a high-speed vision-only airborne collision avoidance system. ViSafe offers a full-stack solution to the Detect and Avoid (DAA) problem by tightly integrating a learning-based edge-AI framework with a custom multi-camera hardware prototype designed under SWaP-C constraints. By leveraging perceptual input-focused control barrier functions (CBF) to design, encode, and enforce safety thresholds, ViSafe can provide provably safe runtime guarantees for self-separation in high-speed aerial operations. We evaluate ViSafe's performance through an extensive test campaign involving both simulated digital twins and real-world flight scenarios. By independently varying agent types, closure rates, interaction geometries, and environmental conditions (e.g., weather and lighting), we demonstrate that ViSafe consistently ensures self-separation across diverse scenarios. In first-of-its-kind real-world high-speed collision avoidance tests with closure rates reaching 144 km/h, ViSafe sets a new benchmark for vision-only autonomous collision avoidance, establishing a new standard for safety in high-speed aerial navigation."
  },
  {
    "title": "Data-Driven Falsification of Cyber-Physical Systems",
    "url": "http://arxiv.org/abs/2505.03863v1",
    "arxiv_id": "2505.03863v1",
    "authors": [
      "Atanu Kundu",
      "Sauvik Gon",
      "Rajarshi Ray"
    ],
    "published": "2025-05-06T16:33:06+00:00",
    "summary": "Cyber-Physical Systems (CPS) are abundant in safety-critical domains such as healthcare, avionics, and autonomous vehicles. Formal verification of their operational safety is, therefore, of utmost importance. In this paper, we address the falsification problem, where the focus is on searching for an unsafe execution in the system instead of proving their absence. The contribution of this paper is a framework that (a) connects the falsification of CPS with the falsification of deep neural networks (DNNs) and (b) leverages the inherent interpretability of Decision Trees for faster falsification of CPS. This is achieved by: (1) building a surrogate model of the CPS under test, either as a DNN model or a Decision Tree, (2) application of various DNN falsification tools to falsify CPS, and (3) a novel falsification algorithm guided by the explanations of safety violations of the CPS model extracted from its Decision Tree surrogate. The proposed framework has the potential to exploit a repertoire of \\emph{adversarial attack} algorithms designed to falsify robustness properties of DNNs, as well as state-of-the-art falsification algorithms for DNNs. Although the presented methodology is applicable to systems that can be executed/simulated in general, we demonstrate its effectiveness, particularly in CPS. We show that our framework, implemented as a tool \\textsc{FlexiFal}, can detect hard-to-find counterexamples in CPS that have linear and non-linear dynamics. Decision tree-guided falsification shows promising results in efficiently finding multiple counterexamples in the ARCH-COMP 2024 falsification benchmarks~\\cite{khandait2024arch}."
  },
  {
    "title": "Moral Testing of Autonomous Driving Systems",
    "url": "http://arxiv.org/abs/2505.03683v1",
    "arxiv_id": "2505.03683v1",
    "authors": [
      "Wenbing Tang",
      "Mingfei Cheng",
      "Yuan Zhou",
      "Yang Liu"
    ],
    "published": "2025-05-06T16:29:58+00:00",
    "summary": "Autonomous Driving System (ADS) testing plays a crucial role in their development, with the current focus primarily on functional and safety testing. However, evaluating the non-functional morality of ADSs, particularly their decision-making capabilities in unavoidable collision scenarios, is equally important to ensure the systems' trustworthiness and public acceptance. Unfortunately, testing ADS morality is nearly impossible due to the absence of universal moral principles. To address this challenge, this paper first extracts a set of moral meta-principles derived from existing moral experiments and well-established social science theories, aiming to capture widely recognized and common-sense moral values for ADSs. These meta-principles are then formalized as quantitative moral metamorphic relations, which act as the test oracle. Furthermore, we propose a metamorphic testing framework to systematically identify potential moral issues. Finally, we illustrate the implementation of the framework and present typical violation cases using the VIRES VTD simulator and its built-in ADS."
  },
  {
    "title": "BURNS: Backward Underapproximate Reachability for Neural-Feedback-Loop Systems",
    "url": "http://arxiv.org/abs/2505.03643v1",
    "arxiv_id": "2505.03643v1",
    "authors": [
      "Chelsea Sidrane",
      "Jana Tumova"
    ],
    "published": "2025-05-06T15:50:43+00:00",
    "summary": "Learning-enabled planning and control algorithms are increasingly popular, but they often lack rigorous guarantees of performance or safety. We introduce an algorithm for computing underapproximate backward reachable sets of nonlinear discrete time neural feedback loops. We then use the backward reachable sets to check goal-reaching properties. Our algorithm is based on overapproximating the system dynamics function to enable computation of underapproximate backward reachable sets through solutions of mixed-integer linear programs. We rigorously analyze the soundness of our algorithm and demonstrate it on a numerical example. Our work expands the class of properties that can be verified for learning-enabled systems."
  },
  {
    "title": "Backstepping Reach-avoid Controller Synthesis for Multi-input Multi-output Systems with Mixed Relative Degrees",
    "url": "http://arxiv.org/abs/2505.03612v1",
    "arxiv_id": "2505.03612v1",
    "authors": [
      "Jianqiang Ding",
      "Dingran Yuan",
      "Shankar A. Deka"
    ],
    "published": "2025-05-06T15:10:29+00:00",
    "summary": "Designing controllers with provable formal guarantees has become an urgent requirement for cyber-physical systems in safety-critical scenarios. Beyond addressing scalability in high-dimensional implementations, controller synthesis methodologies separating safety and reachability objectives may risk optimization infeasibility due to conflicting constraints, thereby significantly undermining their applicability in practical applications. In this paper, by leveraging feedback linearization and backstepping techniques, we present a novel framework for constructing provable reach-avoid formal certificates tailored to multi-input multi-output systems. Based on this, we developed a systematic synthesis approach for controllers with reach-avoid guarantees, which ensures that the outputs of the system eventually enter the predefined target set while staying within the required safe set. Finally, we demonstrate the effectiveness of our method through simulations."
  },
  {
    "title": "LlamaFirewall: An open source guardrail system for building secure AI agents",
    "url": "http://arxiv.org/abs/2505.03574v1",
    "arxiv_id": "2505.03574v1",
    "authors": [
      "Sahana Chennabasappa",
      "Cyrus Nikolaidis",
      "Daniel Song",
      "David Molnar",
      "Stephanie Ding",
      "Shengye Wan",
      "Spencer Whitman",
      "Lauren Deason",
      "Nicholas Doucette",
      "Abraham Montilla",
      "Alekhya Gampa",
      "Beto de Paola",
      "Dominik Gabi",
      "James Crnkovich",
      "Jean-Christophe Testud",
      "Kat He",
      "Rashnil Chaturvedi",
      "Wu Zhou",
      "Joshua Saxe"
    ],
    "published": "2025-05-06T14:34:21+00:00",
    "summary": "Large language models (LLMs) have evolved from simple chatbots into autonomous agents capable of performing complex tasks such as editing production code, orchestrating workflows, and taking higher-stakes actions based on untrusted inputs like webpages and emails. These capabilities introduce new security risks that existing security measures, such as model fine-tuning or chatbot-focused guardrails, do not fully address. Given the higher stakes and the absence of deterministic solutions to mitigate these risks, there is a critical need for a real-time guardrail monitor to serve as a final layer of defense, and support system level, use case specific safety policy definition and enforcement. We introduce LlamaFirewall, an open-source security focused guardrail framework designed to serve as a final layer of defense against security risks associated with AI Agents. Our framework mitigates risks such as prompt injection, agent misalignment, and insecure code risks through three powerful guardrails: PromptGuard 2, a universal jailbreak detector that demonstrates clear state of the art performance; Agent Alignment Checks, a chain-of-thought auditor that inspects agent reasoning for prompt injection and goal misalignment, which, while still experimental, shows stronger efficacy at preventing indirect injections in general scenarios than previously proposed approaches; and CodeShield, an online static analysis engine that is both fast and extensible, aimed at preventing the generation of insecure or dangerous code by coding agents. Additionally, we include easy-to-use customizable scanners that make it possible for any developer who can write a regular expression or an LLM prompt to quickly update an agent's security guardrails."
  },
  {
    "title": "Long-Short Chain-of-Thought Mixture Supervised Fine-Tuning Eliciting Efficient Reasoning in Large Language Models",
    "url": "http://arxiv.org/abs/2505.03469v1",
    "arxiv_id": "2505.03469v1",
    "authors": [
      "Bin Yu",
      "Hang Yuan",
      "Yuliang Wei",
      "Bailing Wang",
      "Weizhen Qi",
      "Kai Chen"
    ],
    "published": "2025-05-06T12:18:11+00:00",
    "summary": "Recent advances in large language models have demonstrated that Supervised Fine-Tuning (SFT) with Chain-of-Thought (CoT) reasoning data distilled from large reasoning models (e.g., DeepSeek R1) can effectively transfer reasoning capabilities to non-reasoning models. However, models fine-tuned with this approach inherit the \"overthinking\" problem from teacher models, producing verbose and redundant reasoning chains during inference. To address this challenge, we propose \\textbf{L}ong-\\textbf{S}hort Chain-of-Thought \\textbf{Mixture} \\textbf{S}upervised \\textbf{F}ine-\\textbf{T}uning (\\textbf{LS-Mixture SFT}), which combines long CoT reasoning dataset with their short counterparts obtained through structure-preserved rewriting. Our experiments demonstrate that models trained using the LS-Mixture SFT method, compared to those trained with direct SFT, achieved an average accuracy improvement of 2.3\\% across various benchmarks while substantially reducing model response length by approximately 47.61\\%. This work offers an approach to endow non-reasoning models with reasoning capabilities through supervised fine-tuning while avoiding the inherent overthinking problems inherited from teacher models, thereby enabling efficient reasoning in the fine-tuned models."
  },
  {
    "title": "Close-Fitting Dressing Assistance Based on State Estimation of Feet and Garments with Semantic-based Visual Attention",
    "url": "http://arxiv.org/abs/2505.03400v1",
    "arxiv_id": "2505.03400v1",
    "authors": [
      "Takuma Tsukakoshi",
      "Tamon Miyake",
      "Tetsuya Ogata",
      "Yushi Wang",
      "Takumi Akaishi",
      "Shigeki Sugano"
    ],
    "published": "2025-05-06T10:28:39+00:00",
    "summary": "As the population continues to age, a shortage of caregivers is expected in the future. Dressing assistance, in particular, is crucial for opportunities for social participation. Especially dressing close-fitting garments, such as socks, remains challenging due to the need for fine force adjustments to handle the friction or snagging against the skin, while considering the shape and position of the garment. This study introduces a method uses multi-modal information including not only robot's camera images, joint angles, joint torques, but also tactile forces for proper force interaction that can adapt to individual differences in humans. Furthermore, by introducing semantic information based on object concepts, rather than relying solely on RGB data, it can be generalized to unseen feet and background. In addition, incorporating depth data helps infer relative spatial relationship between the sock and the foot. To validate its capability for semantic object conceptualization and to ensure safety, training data were collected using a mannequin, and subsequent experiments were conducted with human subjects. In experiments, the robot successfully adapted to previously unseen human feet and was able to put socks on 10 participants, achieving a higher success rate than Action Chunking with Transformer and Diffusion Policy. These results demonstrate that the proposed model can estimate the state of both the garment and the foot, enabling precise dressing assistance for close-fitting garments."
  },
  {
    "title": "Miniature multihole airflow sensor for lightweight aircraft over wide speed and angular range",
    "url": "http://arxiv.org/abs/2505.03331v1",
    "arxiv_id": "2505.03331v1",
    "authors": [
      "Lukas Stuber",
      "Simon Jeger",
      "Raphael Zufferey",
      "Dario Floreano"
    ],
    "published": "2025-05-06T09:04:49+00:00",
    "summary": "An aircraft's airspeed, angle of attack, and angle of side slip are crucial to its safety, especially when flying close to the stall regime. Various solutions exist, including pitot tubes, angular vanes, and multihole pressure probes. However, current sensors are either too heavy (>30 g) or require large airspeeds (>20 m/s), making them unsuitable for small uncrewed aerial vehicles. We propose a novel multihole pressure probe, integrating sensing electronics in a single-component structure, resulting in a mechanically robust and lightweight sensor (9 g), which we released to the public domain. Since there is no consensus on two critical design parameters, tip shape (conical vs spherical) and hole spacing (distance between holes), we provide a study on measurement accuracy and noise generation using wind tunnel experiments. The sensor is calibrated using a multivariate polynomial regression model over an airspeed range of 3-27 m/s and an angle of attack/sideslip range of +-35{\\deg}, achieving a mean absolute error of 0.44 m/s and 0.16{\\deg}. Finally, we validated the sensor in outdoor flights near the stall regime. Our probe enabled accurate estimations of airspeed, angle of attack and sideslip during different acrobatic manoeuvres. Due to its size and weight, this sensor will enable safe flight for lightweight, uncrewed aerial vehicles flying at low speeds close to the stall regime."
  },
  {
    "title": "\u03a8-Arena: Interactive Assessment and Optimization of LLM-based Psychological Counselors with Tripartite Feedback",
    "url": "http://arxiv.org/abs/2505.03293v1",
    "arxiv_id": "2505.03293v1",
    "authors": [
      "Shijing Zhu",
      "Zhuang Chen",
      "Guanqun Bi",
      "Binghang Li",
      "Yaxi Deng",
      "Dazhen Wan",
      "Libiao Peng",
      "Xiyao Xiao",
      "Rongsheng Zhang",
      "Tangjie Lv",
      "Zhipeng Hu",
      "FangFang Li",
      "Minlie Huang"
    ],
    "published": "2025-05-06T08:22:51+00:00",
    "summary": "Large language models (LLMs) have shown promise in providing scalable mental health support, while evaluating their counseling capability remains crucial to ensure both efficacy and safety. Existing evaluations are limited by the static assessment that focuses on knowledge tests, the single perspective that centers on user experience, and the open-loop framework that lacks actionable feedback. To address these issues, we propose {\\Psi}-Arena, an interactive framework for comprehensive assessment and optimization of LLM-based counselors, featuring three key characteristics: (1) Realistic arena interactions that simulate real-world counseling through multi-stage dialogues with psychologically profiled NPC clients, (2) Tripartite evaluation that integrates assessments from the client, counselor, and supervisor perspectives, and (3) Closed-loop optimization that iteratively improves LLM counselors using diagnostic feedback. Experiments across eight state-of-the-art LLMs show significant performance variations in different real-world scenarios and evaluation perspectives. Moreover, reflection-based optimization results in up to a 141% improvement in counseling performance. We hope PsychoArena provides a foundational resource for advancing reliable and human-aligned LLM applications in mental healthcare."
  },
  {
    "title": "Enabling Robots to Autonomously Search Dynamic Cluttered Post-Disaster Environments",
    "url": "http://arxiv.org/abs/2505.03283v1",
    "arxiv_id": "2505.03283v1",
    "authors": [
      "Karlo Rado",
      "Mirko Baglioni",
      "Anahita Jamshidnejad"
    ],
    "published": "2025-05-06T08:10:02+00:00",
    "summary": "Robots will bring search and rescue (SaR) in disaster response to another level, in case they can autonomously take over dangerous SaR tasks from humans. A main challenge for autonomous SaR robots is to safely navigate in cluttered environments with uncertainties, while avoiding static and moving obstacles. We propose an integrated control framework for SaR robots in dynamic, uncertain environments, including a computationally efficient heuristic motion planning system that provides a nominal (assuming there are no uncertainties) collision-free trajectory for SaR robots and a robust motion tracking system that steers the robot to track this reference trajectory, taking into account the impact of uncertainties. The control architecture guarantees a balanced trade-off among various SaR objectives, while handling the hard constraints, including safety. The results of various computer-based simulations, presented in this paper, showed significant out-performance (of up to 42.3%) of the proposed integrated control architecture compared to two commonly used state-of-the-art methods (Rapidly-exploring Random Tree and Artificial Potential Function) in reaching targets (e.g., trapped victims in SaR) safely, collision-free, and in the shortest possible time."
  },
  {
    "title": "RADE: Learning Risk-Adjustable Driving Environment via Multi-Agent Conditional Diffusion",
    "url": "http://arxiv.org/abs/2505.03178v1",
    "arxiv_id": "2505.03178v1",
    "authors": [
      "Jiawei Wang",
      "Xintao Yan",
      "Yao Mu",
      "Haowei Sun",
      "Zhong Cao",
      "Henry X. Liu"
    ],
    "published": "2025-05-06T04:41:20+00:00",
    "summary": "Generating safety-critical scenarios in high-fidelity simulations offers a promising and cost-effective approach for efficient testing of autonomous vehicles. Existing methods typically rely on manipulating a single vehicle's trajectory through sophisticated designed objectives to induce adversarial interactions, often at the cost of realism and scalability. In this work, we propose the Risk-Adjustable Driving Environment (RADE), a simulation framework that generates statistically realistic and risk-adjustable traffic scenes. Built upon a multi-agent diffusion architecture, RADE jointly models the behavior of all agents in the environment and conditions their trajectories on a surrogate risk measure. Unlike traditional adversarial methods, RADE learns risk-conditioned behaviors directly from data, preserving naturalistic multi-agent interactions with controllable risk levels. To ensure physical plausibility, we incorporate a tokenized dynamics check module that efficiently filters generated trajectories using a motion vocabulary. We validate RADE on the real-world rounD dataset, demonstrating that it preserves statistical realism across varying risk levels and naturally increases the likelihood of safety-critical events as the desired risk level grows up. Our results highlight RADE's potential as a scalable and realistic tool for AV safety evaluation."
  },
  {
    "title": "VISLIX: An XAI Framework for Validating Vision Models with Slice Discovery and Analysis",
    "url": "http://arxiv.org/abs/2505.03132v1",
    "arxiv_id": "2505.03132v1",
    "authors": [
      "Xinyuan Yan",
      "Xiwei Xuan",
      "Jorge Piazentin Ono",
      "Jiajing Guo",
      "Vikram Mohanty",
      "Shekar Arvind Kumar",
      "Liang Gou",
      "Bei Wang",
      "Liu Ren"
    ],
    "published": "2025-05-06T03:09:15+00:00",
    "summary": "Real-world machine learning models require rigorous evaluation before deployment, especially in safety-critical domains like autonomous driving and surveillance. The evaluation of machine learning models often focuses on data slices, which are subsets of the data that share a set of characteristics. Data slice finding automatically identifies conditions or data subgroups where models underperform, aiding developers in mitigating performance issues. Despite its popularity and effectiveness, data slicing for vision model validation faces several challenges. First, data slicing often needs additional image metadata or visual concepts, and falls short in certain computer vision tasks, such as object detection. Second, understanding data slices is a labor-intensive and mentally demanding process that heavily relies on the expert's domain knowledge. Third, data slicing lacks a human-in-the-loop solution that allows experts to form hypothesis and test them interactively. To overcome these limitations and better support the machine learning operations lifecycle, we introduce VISLIX, a novel visual analytics framework that employs state-of-the-art foundation models to help domain experts analyze slices in computer vision models. Our approach does not require image metadata or visual concepts, automatically generates natural language insights, and allows users to test data slice hypothesis interactively. We evaluate VISLIX with an expert study and three use cases, that demonstrate the effectiveness of our tool in providing comprehensive insights for validating object detection models."
  },
  {
    "title": "Impact Analysis of Inference Time Attack of Perception Sensors on Autonomous Vehicles",
    "url": "http://arxiv.org/abs/2505.03850v1",
    "arxiv_id": "2505.03850v1",
    "authors": [
      "Hanlin Chen",
      "Simin Chen",
      "Wenyu Li",
      "Wei Yang",
      "Yiheng Feng"
    ],
    "published": "2025-05-05T23:00:27+00:00",
    "summary": "As a safety-critical cyber-physical system, cybersecurity and related safety issues for Autonomous Vehicles (AVs) have been important research topics for a while. Among all the modules on AVs, perception is one of the most accessible attack surfaces, as drivers and AVs have no control over the outside environment. Most current work targeting perception security for AVs focuses on perception correctness. In this work, we propose an impact analysis based on inference time attacks for autonomous vehicles. We demonstrate in a simulation system that such inference time attacks can also threaten the safety of both the ego vehicle and other traffic participants."
  },
  {
    "title": "A Modal-Space Formulation for Momentum Observer Contact Estimation and Effects of Uncertainty for Continuum Robots",
    "url": "http://arxiv.org/abs/2505.03044v1",
    "arxiv_id": "2505.03044v1",
    "authors": [
      "Garrison L. H. Johnston",
      "Neel Shihora",
      "Nabil Simaan"
    ],
    "published": "2025-05-05T21:54:01+00:00",
    "summary": "Contact detection for continuum and soft robots has been limited in past works to statics or kinematics-based methods with assumed circular bending curvature or known bending profiles. In this paper, we adapt the generalized momentum observer contact estimation method to continuum robots. This is made possible by leveraging recent results for real-time shape sensing of continuum robots along with a modal-space representation of the robot dynamics. In addition to presenting an approach for estimating the generalized forces due to contact via a momentum observer, we present a constrained optimization method to identify the wrench imparted on the robot during contact. We also present an approach for investigating the effects of unmodeled deviations in the robot's dynamic state on the contact detection method and we validate our algorithm by simulations and experiments. We also compare the performance of the momentum observer to the joint force deviation method, a direct estimation approach using the robot's full dynamic model. We also demonstrate a basic extension of the method to multisegment continuum robots. Results presented in this work extend dynamic contact detection to the domain of continuum and soft robots and can be used to improve the safety of large-scale continuum robots for human-robot collaboration."
  },
  {
    "title": "Evidence of a fraction of LIGO/Virgo/KAGRA events coming from active galactic nuclei",
    "url": "http://arxiv.org/abs/2505.02924v1",
    "arxiv_id": "2505.02924v1",
    "authors": [
      "Liang-Gui Zhu",
      "Xian Chen"
    ],
    "published": "2025-05-05T18:02:47+00:00",
    "summary": "The formation channels of the gravitational-wave (GW) sources detected by LIGO/Virgo/KAGRA (LVK) remain poorly constrained. Active galactic nucleus (AGN) has been proposed as one of the potential hosts but the fraction of GW events originating from AGNs has not been quantified. Here, we constrain the AGN-origin fraction $f_{\\rm agn}$ by analyzing the spatial correlation between GW source localizations ($O1\\!-\\!O4$a) and AGN distributions (SDSS DR16). We find evidence of an excess of low-luminosity ($L_{\\rm bol} \\le 10^{45}~\\!\\mathrm{erg~s}^{-1}$) as well as low-Eddington ratio ($\\lambda_{\\rm Edd} \\le 0.05$) AGNs around the LVK events, the explanation of which requires $f_{\\rm agn} = 0.39^{+0.41}_{-0.32}$ and $0.29^{+0.40}_{-0.25}$ (90\\% confidence level) of the LVK events originating from these respective AGN populations. Monte Carlo simulations confirm that this correlation is unlikely to arise from random coincidence, further supported by anomalous variation of the error of $f_{\\rm agn}$ with GW event counts. These results provide the first observational evidence for GW sources coming from either low-luminosity or low-accretion-rate AGNs, offering critical insights into the environmental dependencies of the formation of GW sources."
  },
  {
    "title": "Stabilizing dark matter with quantum scale symmetry",
    "url": "http://arxiv.org/abs/2505.02803v1",
    "arxiv_id": "2505.02803v1",
    "authors": [
      "Abhishek Chikkaballi",
      "Kamila Kowalska",
      "Rafael R. Lino dos Santos",
      "Enrico Maria Sessolo"
    ],
    "published": "2025-05-05T17:27:04+00:00",
    "summary": "In the context of gauge-Yukawa theories with trans-Planckian asymptotic safety, quantum scale symmetry can prevent the appearance in the Lagrangian of couplings that would otherwise be allowed by the gauge symmetry. Such couplings correspond to irrelevant Gaussian fixed points of the renormalization group flow. Their absence in the theory implies that different sectors of the gauge-Yukawa theory are secluded from one another, in similar fashion to the effects of a global or a discrete symmetry. As an example, we impose the trans-Planckian scale symmetry on a model of Grand Unification based on the gauge group SU(6), showing that it leads to the emergence of several fermionic WIMP dark matter candidates whose coupling strengths are entirely predicted by the UV completion."
  },
  {
    "title": "When Your Own Output Becomes Your Training Data: Noise-to-Meaning Loops and a Formal RSI Trigger",
    "url": "http://arxiv.org/abs/2505.02888v1",
    "arxiv_id": "2505.02888v1",
    "authors": [
      "Rintaro Ando"
    ],
    "published": "2025-05-05T17:03:07+00:00",
    "summary": "We present Noise-to-Meaning Recursive Self-Improvement (N2M-RSI), a minimal formal model showing that once an AI agent feeds its own outputs back as inputs and crosses an explicit information-integration threshold, its internal complexity will grow without bound under our assumptions. The framework unifies earlier ideas on self-prompting large language models, G\\\"odelian self-reference, and AutoML, yet remains implementation-agnostic. The model furthermore scales naturally to interacting swarms of agents, hinting at super-linear effects once communication among instances is permitted. For safety reasons, we omit system-specific implementation details and release only a brief, model-agnostic toy prototype in Appendix C."
  },
  {
    "title": "Event-aware analysis of cross-city visitor flows using large language models and social media data",
    "url": "http://arxiv.org/abs/2505.03847v1",
    "arxiv_id": "2505.03847v1",
    "authors": [
      "Xiaohan Wang",
      "Zhan Zhao",
      "Ruiyu Wang",
      "Yang Xu"
    ],
    "published": "2025-05-05T16:32:05+00:00",
    "summary": "Public events, such as music concerts and fireworks displays, can cause irregular surges in cross-city travel demand, leading to potential overcrowding, travel delays, and public safety concerns. To better anticipate and accommodate such demand surges, it is essential to estimate cross-city visitor flows with awareness of public events. Although prior studies typically focused on the effects of a single mega event or disruptions around a single venue, this study introduces a generalizable framework to analyze visitor flows under diverse and concurrent events. We propose to leverage large language models (LLMs) to extract event features from multi-source online information and massive user-generated content on social media platforms. Specifically, social media popularity metrics are designed to capture the effects of online promotion and word-of-mouth in attracting visitors. An event-aware machine learning model is then adopted to uncover the specific impacts of different event features and ultimately predict visitor flows for upcoming events. Using Hong Kong as a case study, the framework is applied to predict daily flows of mainland Chinese visitors arriving at the city, achieving a testing R-squared of over 85%. We further investigate the heterogeneous event impacts on visitor numbers across different event types and major travel modes. Both promotional popularity and word-of-mouth popularity are found to be associated with increased visitor flows, but the specific effects vary by the event type. This association is more pronounced among visitors arriving by metro and high-speed rail, while it has less effect on air travelers. The findings can facilitate coordinated measures across government agencies and guide specialized transport policies, such as shuttle transit services to event venues, and comprehensive on-site traffic management strategies."
  },
  {
    "title": "A Survey of Slow Thinking-based Reasoning LLMs using Reinforced Learning and Inference-time Scaling Law",
    "url": "http://arxiv.org/abs/2505.02665v1",
    "arxiv_id": "2505.02665v1",
    "authors": [
      "Qianjun Pan",
      "Wenkai Ji",
      "Yuyang Ding",
      "Junsong Li",
      "Shilian Chen",
      "Junyi Wang",
      "Jie Zhou",
      "Qin Chen",
      "Min Zhang",
      "Yulan Wu",
      "Liang He"
    ],
    "published": "2025-05-05T14:14:59+00:00",
    "summary": "This survey explores recent advancements in reasoning large language models (LLMs) designed to mimic \"slow thinking\" - a reasoning process inspired by human cognition, as described in Kahneman's Thinking, Fast and Slow. These models, like OpenAI's o1, focus on scaling computational resources dynamically during complex tasks, such as math reasoning, visual reasoning, medical diagnosis, and multi-agent debates. We present the development of reasoning LLMs and list their key technologies. By synthesizing over 100 studies, it charts a path toward LLMs that combine human-like deep thinking with scalable efficiency for reasoning. The review breaks down methods into three categories: (1) test-time scaling dynamically adjusts computation based on task complexity via search and sampling, dynamic verification; (2) reinforced learning refines decision-making through iterative improvement leveraging policy networks, reward models, and self-evolution strategies; and (3) slow-thinking frameworks (e.g., long CoT, hierarchical processes) that structure problem-solving with manageable steps. The survey highlights the challenges and further directions of this domain. Understanding and advancing the reasoning abilities of LLMs is crucial for unlocking their full potential in real-world applications, from scientific discovery to decision support systems."
  },
  {
    "title": "Faithful and secure distributed quantum sensing under general-coherent attacks",
    "url": "http://arxiv.org/abs/2505.02620v1",
    "arxiv_id": "2505.02620v1",
    "authors": [
      "G. Bizzarri",
      "M. Barbieri",
      "M. Manrique",
      "M. Parisi",
      "F. Bruni",
      "I. Gianani",
      "M. Rosati"
    ],
    "published": "2025-05-05T12:48:41+00:00",
    "summary": "Quantum metrology and cryptography can be combined in a distributed and/or remote sensing setting, where distant end-users with limited quantum capabilities can employ quantum states, transmitted by a quantum-powerful provider via a quantum network, to perform quantum-enhanced parameter estimation in a private fashion. Previous works on the subject have been limited by restricted assumptions on the capabilities of a potential eavesdropper and the use of abort-based protocols that prevent a simple practical realization. Here we introduce, theoretically analyze, and experimentally demonstrate single- and two-way protocols for distributed sensing combining several unique and desirable features: (i) a safety-threshold mechanism that allows the protocol to proceed in low-noise cases and quantifying the potential tampering with respect to the ideal estimation procedure, effectively paving the way for wide-spread practical realizations; (ii) equivalence of entanglement-based and mutually-unbiased-bases-based formulations; (iii) robustness against collective attacks via a LOCC-de-Finetti theorem, for the first time to our knowledge. Finally, we demonstrate our protocols in a photonic-based implementation, observing that the possibility of guaranteeing a safety threshold may come at a significant price in terms of the estimation bias, potentially overestimating the effect of tampering in practical settings."
  },
  {
    "title": "LiDAR-Inertial SLAM-Based Navigation and Safety-Oriented AI-Driven Control System for Skid-Steer Robots",
    "url": "http://arxiv.org/abs/2505.02598v1",
    "arxiv_id": "2505.02598v1",
    "authors": [
      "Mehdi Heydari Shahna",
      "Eemil Haaparanta",
      "Pauli Mustalahti",
      "Jouni Mattila"
    ],
    "published": "2025-05-05T12:07:35+00:00",
    "summary": "Integrating artificial intelligence (AI) and stochastic technologies into the mobile robot navigation and control (MRNC) framework while adhering to rigorous safety standards presents significant challenges. To address these challenges, this paper proposes a comprehensively integrated MRNC framework for skid-steer wheeled mobile robots (SSWMRs), in which all components are actively engaged in real-time execution. The framework comprises: 1) a LiDAR-inertial simultaneous localization and mapping (SLAM) algorithm for estimating the current pose of the robot within the built map; 2) an effective path-following control system for generating desired linear and angular velocity commands based on the current pose and the desired pose; 3) inverse kinematics for transferring linear and angular velocity commands into left and right side velocity commands; and 4) a robust AI-driven (RAID) control system incorporating a radial basis function network (RBFN) with a new adaptive algorithm to enforce in-wheel actuation systems to track each side motion commands. To further meet safety requirements, the proposed RAID control within the MRNC framework of the SSWMR constrains AI-generated tracking performance within predefined overshoot and steady-state error limits, while ensuring robustness and system stability by compensating for modeling errors, unknown RBF weights, and external forces. Experimental results verify the proposed MRNC framework performance for a 4,836 kg SSWMR operating on soft terrain."
  },
  {
    "title": "Point Cloud Recombination: Systematic Real Data Augmentation Using Robotic Targets for LiDAR Perception Validation",
    "url": "http://arxiv.org/abs/2505.02476v1",
    "arxiv_id": "2505.02476v1",
    "authors": [
      "Hubert Padusinski",
      "Christian Steinhauser",
      "Christian Scherl",
      "Julian Gaal",
      "Jacob Langner"
    ],
    "published": "2025-05-05T09:00:16+00:00",
    "summary": "The validation of LiDAR-based perception of intelligent mobile systems operating in open-world applications remains a challenge due to the variability of real environmental conditions. Virtual simulations allow the generation of arbitrary scenes under controlled conditions but lack physical sensor characteristics, such as intensity responses or material-dependent effects. In contrast, real-world data offers true sensor realism but provides less control over influencing factors, hindering sufficient validation. Existing approaches address this problem with augmentation of real-world point cloud data by transferring objects between scenes. However, these methods do not consider validation and remain limited in controllability because they rely on empirical data. We solve these limitations by proposing Point Cloud Recombination, which systematically augments captured point cloud scenes by integrating point clouds acquired from physical target objects measured in controlled laboratory environments. Thus enabling the creation of vast amounts and varieties of repeatable, physically accurate test scenes with respect to phenomena-aware occlusions with registered 3D meshes. Using the Ouster OS1-128 Rev7 sensor, we demonstrate the augmentation of real-world urban and rural scenes with humanoid targets featuring varied clothing and poses, for repeatable positioning. We show that the recombined scenes closely match real sensor outputs, enabling targeted testing, scalable failure analysis, and improved system safety. By providing controlled yet sensor-realistic data, our method enables trustworthy conclusions about the limitations of specific sensors in compound with their algorithms, e.g., object detection."
  },
  {
    "title": "A Real-Time Control Barrier Function-Based Safety Filter for Motion Planning with Arbitrary Road Boundary Constraints",
    "url": "http://arxiv.org/abs/2505.02395v1",
    "arxiv_id": "2505.02395v1",
    "authors": [
      "Jianye Xu",
      "Chang Che",
      "Bassam Alrifaee"
    ],
    "published": "2025-05-05T06:36:26+00:00",
    "summary": "We present a real-time safety filter for motion planning, such as learning-based methods, using Control Barrier Functions (CBFs), which provides formal guarantees for collision avoidance with road boundaries. A key feature of our approach is its ability to directly incorporate road geometries of arbitrary shape without resorting to conservative overapproximations. We formulate the safety filter as a constrained optimization problem in the form of a Quadratic Program (QP). It achieves safety by making minimal, necessary adjustments to the control actions issued by the nominal motion planner. We validate our safety filter through extensive numerical experiments across a variety of traffic scenarios featuring complex roads. The results confirm its reliable safety and high computational efficiency (execution frequency up to 40 Hz). Code & Video Demo: github.com/bassamlab/SigmaRL"
  },
  {
    "title": "Quantitative Analysis of Performance Drop in DeepSeek Model Quantization",
    "url": "http://arxiv.org/abs/2505.02390v1",
    "arxiv_id": "2505.02390v1",
    "authors": [
      "Enbo Zhao",
      "Yi Shen",
      "Shuming Shi",
      "Jieyun Huang",
      "Zhihao Chen",
      "Ning Wang",
      "Siqi Xiao",
      "Jian Zhang",
      "Kai Wang",
      "Shiguo Lian"
    ],
    "published": "2025-05-05T06:25:20+00:00",
    "summary": "Recently, there is a high demand for deploying DeepSeek-R1 and V3 locally, possibly because the official service often suffers from being busy and some organizations have data privacy concerns. While single-machine deployment offers infrastructure simplicity, the models' 671B FP8 parameter configuration exceeds the practical memory limits of a standard 8-GPU machine. Quantization is a widely used technique that helps reduce model memory consumption. However, it is unclear what the performance of DeepSeek-R1 and V3 will be after being quantized. This technical report presents the first quantitative evaluation of multi-bitwidth quantization across the complete DeepSeek model spectrum. Key findings reveal that 4-bit quantization maintains little performance degradation versus FP8 while enabling single-machine deployment on standard NVIDIA GPU devices. We further propose DQ3_K_M, a dynamic 3-bit quantization method that significantly outperforms traditional Q3_K_M variant on various benchmarks, which is also comparable with 4-bit quantization (Q4_K_M) approach in most tasks. Moreover, DQ3_K_M supports single-machine deployment configurations for both NVIDIA H100/A100 and Huawei 910B. Our implementation of DQ3\\_K\\_M is released at https://github.com/UnicomAI/DeepSeek-Eval, containing optimized 3-bit quantized variants of both DeepSeek-R1 and DeepSeek-V3."
  },
  {
    "title": "RouthSearch: Inferring PID Parameter Specification for Flight Control Program by Coordinate Search",
    "url": "http://arxiv.org/abs/2505.02357v1",
    "arxiv_id": "2505.02357v1",
    "authors": [
      "Siao Wang",
      "Zhen Dong",
      "Hui Li",
      "Liwei Shen",
      "Xin Peng",
      "Dongdong She"
    ],
    "published": "2025-05-05T04:38:48+00:00",
    "summary": "Flight control programs use PID control modules with user-configurable Proportional (P), Integral (I), and Derivative (D) parameters to manage UAV flying behaviors. Users can adjust these PID parameters during flight. However, flight control programs lack sufficient safety checks on user-provided PID parameters, leading to a severe UAV vulnerability - the input validation bug. This occurs when a user misconfigures PID parameters, causing dangerous states like deviation from the expected path, loss of control, or crash.   Prior works use random testing like fuzzing, but these are not effective in the three-dimensional search space of PID parameters. The expensive dynamic execution of UAV tests further hinders random testing performance.   We address PID parameter misconfiguration by combining the Routh-Hurwitz stability criterion with coordinate search, introducing RouthSearch. Instead of ad-hoc identification, RouthSearch principledly determines valid ranges for three-dimensional PID parameters. We first leverage the Routh-Hurwitz Criterion to identify a theoretical PID parameter boundary, then refine it using efficient coordinate search. The determined valid range can filter misconfigured PID parameters from users during flight and help discover logical bugs in flight control programs.   We evaluated RouthSearch across eight flight modes in PX4 and Ardupilot. Results show RouthSearch determines valid ranges with 92.0% accuracy compared to ground truth. RouthSearch discovers 3,853 PID misconfigurations within 48 hours, while the STOA work PGFuzz discovers only 449 sets, significantly outperforming prior works by 8.58 times. Our method also helped detect three bugs in ArduPilot and PX4."
  },
  {
    "title": "HyperTree Planning: Enhancing LLM Reasoning via Hierarchical Thinking",
    "url": "http://arxiv.org/abs/2505.02322v1",
    "arxiv_id": "2505.02322v1",
    "authors": [
      "Runquan Gui",
      "Zhihai Wang",
      "Jie Wang",
      "Chi Ma",
      "Huiling Zhen",
      "Mingxuan Yuan",
      "Jianye Hao",
      "Defu Lian",
      "Enhong Chen",
      "Feng Wu"
    ],
    "published": "2025-05-05T02:38:58+00:00",
    "summary": "Recent advancements have significantly enhanced the performance of large language models (LLMs) in tackling complex reasoning tasks, achieving notable success in domains like mathematical and logical reasoning. However, these methods encounter challenges with complex planning tasks, primarily due to extended reasoning steps, diverse constraints, and the challenge of handling multiple distinct sub-tasks. To address these challenges, we propose HyperTree Planning (HTP), a novel reasoning paradigm that constructs hypertree-structured planning outlines for effective planning. The hypertree structure enables LLMs to engage in hierarchical thinking by flexibly employing the divide-and-conquer strategy, effectively breaking down intricate reasoning steps, accommodating diverse constraints, and managing multiple distinct sub-tasks in a well-organized manner. We further introduce an autonomous planning framework that completes the planning process by iteratively refining and expanding the hypertree-structured planning outlines. Experiments demonstrate the effectiveness of HTP, achieving state-of-the-art accuracy on the TravelPlanner benchmark with Gemini-1.5-Pro, resulting in a 3.6 times performance improvement over o1-preview."
  },
  {
    "title": "What Is AI Safety? What Do We Want It to Be?",
    "url": "http://arxiv.org/abs/2505.02313v1",
    "arxiv_id": "2505.02313v1",
    "authors": [
      "Jacqueline Harding",
      "Cameron Domenico Kirk-Giannini"
    ],
    "published": "2025-05-05T01:55:00+00:00",
    "summary": "The field of AI safety seeks to prevent or reduce the harms caused by AI systems. A simple and appealing account of what is distinctive of AI safety as a field holds that this feature is constitutive: a research project falls within the purview of AI safety just in case it aims to prevent or reduce the harms caused by AI systems. Call this appealingly simple account The Safety Conception of AI safety. Despite its simplicity and appeal, we argue that The Safety Conception is in tension with at least two trends in the ways AI safety researchers and organizations think and talk about AI safety: first, a tendency to characterize the goal of AI safety research in terms of catastrophic risks from future systems; second, the increasingly popular idea that AI safety can be thought of as a branch of safety engineering. Adopting the methodology of conceptual engineering, we argue that these trends are unfortunate: when we consider what concept of AI safety it would be best to have, there are compelling reasons to think that The Safety Conception is the answer. Descriptively, The Safety Conception allows us to see how work on topics that have historically been treated as central to the field of AI safety is continuous with work on topics that have historically been treated as more marginal, like bias, misinformation, and privacy. Normatively, taking The Safety Conception seriously means approaching all efforts to prevent or mitigate harms from AI systems based on their merits rather than drawing arbitrary distinctions between them."
  },
  {
    "title": "SafeMate: A Model Context Protocol-Based Multimodal Agent for Emergency Preparedness",
    "url": "http://arxiv.org/abs/2505.02306v1",
    "arxiv_id": "2505.02306v1",
    "authors": [
      "Junfeng Jiao",
      "Jihyung Park",
      "Yiming Xu",
      "Lucy Atkinson"
    ],
    "published": "2025-05-05T01:09:02+00:00",
    "summary": "Despite the abundance of public safety documents and emergency protocols, most individuals remain ill-equipped to interpret and act on such information during crises. Traditional emergency decision support systems (EDSS) are designed for professionals and rely heavily on static documents like PDFs or SOPs, which are difficult for non-experts to navigate under stress. This gap between institutional knowledge and public accessibility poses a critical barrier to effective emergency preparedness and response.   We introduce SafeMate, a retrieval-augmented AI assistant that delivers accurate, context-aware guidance to general users in both preparedness and active emergency scenarios. Built on the Model Context Protocol (MCP), SafeMate dynamically routes user queries to tools for document retrieval, checklist generation, and structured summarization. It uses FAISS with cosine similarity to identify relevant content from trusted sources."
  },
  {
    "title": "Half-Ice, Half-Fire Driven Ultranarrow Phase Crossover in 1D Decorated q-State Potts Ferrimagnets: An AI-Co-Led Discovery",
    "url": "http://arxiv.org/abs/2505.02303v1",
    "arxiv_id": "2505.02303v1",
    "authors": [
      "Weiguo Yin"
    ],
    "published": "2025-05-05T00:49:17+00:00",
    "summary": "OpenAI's reasoning model o3-mini-high was used to carry out an exact analytic study of one-dimensional ferrimagnetic decorated $q$-state Potts models. We demonstrate that the finite-temperature ultranarrow phase crossover (UNPC), driven by a hidden \"half-ice, half-fire\" state in the $q=2$ Potts (Ising) model, persists for $q>2$. We identify novel features for $q>2$, including the dome structure in the field-temperature phase diagram and for large $q$ a secondary high-temperature UNPC to the fully disordered paramagnetic state. The ice-fire mechanism of spin flipping can be applied to higher-dimensional Potts models. These results establish a versatile framework for engineering controlled fast state-flipping switches in low-dimensional systems. Our nine-level AI-contribution rating assigns AI the meritorious status of AI-co-led discovery in this work."
  },
  {
    "title": "Adaptive Scoring and Thresholding with Human Feedback for Robust Out-of-Distribution Detection",
    "url": "http://arxiv.org/abs/2505.02299v1",
    "arxiv_id": "2505.02299v1",
    "authors": [
      "Daisuke Yamada",
      "Harit Vishwakarma",
      "Ramya Korlakai Vinayak"
    ],
    "published": "2025-05-05T00:25:14+00:00",
    "summary": "Machine Learning (ML) models are trained on in-distribution (ID) data but often encounter out-of-distribution (OOD) inputs during deployment -- posing serious risks in safety-critical domains. Recent works have focused on designing scoring functions to quantify OOD uncertainty, with score thresholds typically set based solely on ID data to achieve a target true positive rate (TPR), since OOD data is limited before deployment. However, these TPR-based thresholds leave false positive rates (FPR) uncontrolled, often resulting in high FPRs where OOD points are misclassified as ID. Moreover, fixed scoring functions and thresholds lack the adaptivity needed to handle newly observed, evolving OOD inputs, leading to sub-optimal performance. To address these challenges, we propose a human-in-the-loop framework that \\emph{safely updates both scoring functions and thresholds on the fly} based on real-world OOD inputs. Our method maximizes TPR while strictly controlling FPR at all times, even as the system adapts over time. We provide theoretical guarantees for FPR control under stationary conditions and present extensive empirical evaluations on OpenOOD benchmarks to demonstrate that our approach outperforms existing methods by achieving higher TPRs while maintaining FPR control."
  },
  {
    "title": "RNBF: Real-Time RGB-D Based Neural Barrier Functions for Safe Robotic Navigation",
    "url": "http://arxiv.org/abs/2505.02294v1",
    "arxiv_id": "2505.02294v1",
    "authors": [
      "Satyajeet Das",
      "Yifan Xue",
      "Haoming Li",
      "Nadia Figueroa"
    ],
    "published": "2025-05-04T23:43:44+00:00",
    "summary": "Autonomous safe navigation in unstructured and novel environments poses significant challenges, especially when environment information can only be provided through low-cost vision sensors. Although safe reactive approaches have been proposed to ensure robot safety in complex environments, many base their theory off the assumption that the robot has prior knowledge on obstacle locations and geometries. In this paper, we present a real-time, vision-based framework that constructs continuous, first-order differentiable Signed Distance Fields (SDFs) of unknown environments without any pre-training. Our proposed method ensures full compatibility with established SDF-based reactive controllers. To achieve robust performance under practical sensing conditions, our approach explicitly accounts for noise in affordable RGB-D cameras, refining the neural SDF representation online for smoother geometry and stable gradient estimates. We validate the proposed method in simulation and real-world experiments using a Fetch robot."
  },
  {
    "title": "Resolving Conflicting Constraints in Multi-Agent Reinforcement Learning with Layered Safety",
    "url": "http://arxiv.org/abs/2505.02293v1",
    "arxiv_id": "2505.02293v1",
    "authors": [
      "Jason J. Choi",
      "Jasmine Jerry Aloor",
      "Jingqi Li",
      "Maria G. Mendoza",
      "Hamsa Balakrishnan",
      "Claire J. Tomlin"
    ],
    "published": "2025-05-04T23:42:52+00:00",
    "summary": "Preventing collisions in multi-robot navigation is crucial for deployment. This requirement hinders the use of learning-based approaches, such as multi-agent reinforcement learning (MARL), on their own due to their lack of safety guarantees. Traditional control methods, such as reachability and control barrier functions, can provide rigorous safety guarantees when interactions are limited only to a small number of robots. However, conflicts between the constraints faced by different agents pose a challenge to safe multi-agent coordination.   To overcome this challenge, we propose a method that integrates multiple layers of safety by combining MARL with safety filters. First, MARL is used to learn strategies that minimize multiple agent interactions, where multiple indicates more than two. Particularly, we focus on interactions likely to result in conflicting constraints within the engagement distance. Next, for agents that enter the engagement distance, we prioritize pairs requiring the most urgent corrective actions. Finally, a dedicated safety filter provides tactical corrective actions to resolve these conflicts. Crucially, the design decisions for all layers of this framework are grounded in reachability analysis and a control barrier-value function-based filtering mechanism.   We validate our Layered Safe MARL framework in 1) hardware experiments using Crazyflie drones and 2) high-density advanced aerial mobility (AAM) operation scenarios, where agents navigate to designated waypoints while avoiding collisions. The results show that our method significantly reduces conflict while maintaining safety without sacrificing much efficiency (i.e., shorter travel time and distance) compared to baselines that do not incorporate layered safety. The project website is available at \\href{https://dinamo-mit.github.io/Layered-Safe-MARL/}{[this https URL]}"
  },
  {
    "title": "On the Need for a Statistical Foundation in Scenario-Based Testing of Autonomous Vehicles",
    "url": "http://arxiv.org/abs/2505.02274v1",
    "arxiv_id": "2505.02274v1",
    "authors": [
      "Xingyu Zhao",
      "Robab Aghazadeh-Chakherlou",
      "Chih-Hong Cheng",
      "Peter Popov",
      "Lorenzo Strigini"
    ],
    "published": "2025-05-04T22:06:23+00:00",
    "summary": "Scenario-based testing has emerged as a common method for autonomous vehicles (AVs) safety, offering a more efficient alternative to mile-based testing by focusing on high-risk scenarios. However, fundamental questions persist regarding its stopping rules, residual risk estimation, debug effectiveness, and the impact of simulation fidelity on safety claims. This paper argues that a rigorous statistical foundation is essential to address these challenges and enable rigorous safety assurance. By drawing parallels between AV testing and traditional software testing methodologies, we identify shared research gaps and reusable solutions. We propose proof-of-concept models to quantify the probability of failure per scenario (pfs) and evaluate testing effectiveness under varying conditions. Our analysis reveals that neither scenario-based nor mile-based testing universally outperforms the other. Furthermore, we introduce Risk Estimation Fidelity (REF), a novel metric to certify the alignment of synthetic and real-world testing outcomes, ensuring simulation-based safety claims are statistically defensible."
  },
  {
    "title": "Uncertainty Quantification for Machine Learning in Healthcare: A Survey",
    "url": "http://arxiv.org/abs/2505.02874v1",
    "arxiv_id": "2505.02874v1",
    "authors": [
      "L. Juli\u00e1n Lechuga L\u00f3pez",
      "Shaza Elsharief",
      "Dhiyaa Al Jorf",
      "Firas Darwish",
      "Congbo Ma",
      "Farah E. Shamout"
    ],
    "published": "2025-05-04T16:56:22+00:00",
    "summary": "Uncertainty Quantification (UQ) is pivotal in enhancing the robustness, reliability, and interpretability of Machine Learning (ML) systems for healthcare, optimizing resources and improving patient care. Despite the emergence of ML-based clinical decision support tools, the lack of principled quantification of uncertainty in ML models remains a major challenge. Current reviews have a narrow focus on analyzing the state-of-the-art UQ in specific healthcare domains without systematically evaluating method efficacy across different stages of model development, and despite a growing body of research, its implementation in healthcare applications remains limited. Therefore, in this survey, we provide a comprehensive analysis of current UQ in healthcare, offering an informed framework that highlights how different methods can be integrated into each stage of the ML pipeline including data processing, training and evaluation. We also highlight the most popular methods used in healthcare and novel approaches from other domains that hold potential for future adoption in the medical context. We expect this study will provide a clear overview of the challenges and opportunities of implementing UQ in the ML pipeline for healthcare, guiding researchers and practitioners in selecting suitable techniques to enhance the reliability, safety and trust from patients and clinicians on ML-driven healthcare solutions."
  },
  {
    "title": "Open Challenges in Multi-Agent Security: Towards Secure Systems of Interacting AI Agents",
    "url": "http://arxiv.org/abs/2505.02077v1",
    "arxiv_id": "2505.02077v1",
    "authors": [
      "Christian Schroeder de Witt"
    ],
    "published": "2025-05-04T12:03:29+00:00",
    "summary": "Decentralized AI agents will soon interact across internet platforms, creating security challenges beyond traditional cybersecurity and AI safety frameworks. Free-form protocols are essential for AI's task generalization but enable new threats like secret collusion and coordinated swarm attacks. Network effects can rapidly spread privacy breaches, disinformation, jailbreaks, and data poisoning, while multi-agent dispersion and stealth optimization help adversaries evade oversightcreating novel persistent threats at a systemic level. Despite their critical importance, these security challenges remain understudied, with research fragmented across disparate fields including AI security, multi-agent learning, complex systems, cybersecurity, game theory, distributed systems, and technical AI governance. We introduce \\textbf{multi-agent security}, a new field dedicated to securing networks of decentralized AI agents against threats that emerge or amplify through their interactionswhether direct or indirect via shared environmentswith each other, humans, and institutions, and characterize fundamental security-performance trade-offs. Our preliminary work (1) taxonomizes the threat landscape arising from interacting AI agents, (2) surveys security-performance tradeoffs in decentralized AI systems, and (3) proposes a unified research agenda addressing open challenges in designing secure agent systems and interaction environments. By identifying these gaps, we aim to guide research in this critical area to unlock the socioeconomic potential of large-scale agent deployment on the internet, foster public trust, and mitigate national security risks in critical infrastructure and defense contexts."
  },
  {
    "title": "Enhancing Safety Standards in Automated Systems Using Dynamic Bayesian Networks",
    "url": "http://arxiv.org/abs/2505.02050v1",
    "arxiv_id": "2505.02050v1",
    "authors": [
      "Kranthi Kumar Talluri",
      "Anders L. Madsen",
      "Galia Weidl"
    ],
    "published": "2025-05-04T09:58:02+00:00",
    "summary": "Cut-in maneuvers in high-speed traffic pose critical challenges that can lead to abrupt braking and collisions, necessitating safe and efficient lane change strategies. We propose a Dynamic Bayesian Network (DBN) framework to integrate lateral evidence with safety assessment models, thereby predicting lane changes and ensuring safe cut-in maneuvers effectively. Our proposed framework comprises three key probabilistic hypotheses (lateral evidence, lateral safety, and longitudinal safety) that facilitate the decision-making process through dynamic data processing and assessments of vehicle positions, lateral velocities, relative distance, and Time-to-Collision (TTC) computations. The DBN model's performance compared with other conventional approaches demonstrates superior performance in crash reduction, especially in critical high-speed scenarios, while maintaining a competitive performance in low-speed scenarios. This paves the way for robust, scalable, and efficient safety validation in automated driving systems."
  },
  {
    "title": "R-Bench: Graduate-level Multi-disciplinary Benchmarks for LLM & MLLM Complex Reasoning Evaluation",
    "url": "http://arxiv.org/abs/2505.02018v1",
    "arxiv_id": "2505.02018v1",
    "authors": [
      "Meng-Hao Guo",
      "Jiajun Xu",
      "Yi Zhang",
      "Jiaxi Song",
      "Haoyang Peng",
      "Yi-Xuan Deng",
      "Xinzhi Dong",
      "Kiyohiro Nakayama",
      "Zhengyang Geng",
      "Chen Wang",
      "Bolin Ni",
      "Guo-Wei Yang",
      "Yongming Rao",
      "Houwen Peng",
      "Han Hu",
      "Gordon Wetzstein",
      "Shi-min Hu"
    ],
    "published": "2025-05-04T07:48:36+00:00",
    "summary": "Reasoning stands as a cornerstone of intelligence, enabling the synthesis of existing knowledge to solve complex problems. Despite remarkable progress, existing reasoning benchmarks often fail to rigorously evaluate the nuanced reasoning capabilities required for complex, real-world problemsolving, particularly in multi-disciplinary and multimodal contexts. In this paper, we introduce a graduate-level, multi-disciplinary, EnglishChinese benchmark, dubbed as Reasoning Bench (R-Bench), for assessing the reasoning capability of both language and multimodal models. RBench spans 1,094 questions across 108 subjects for language model evaluation and 665 questions across 83 subjects for multimodal model testing in both English and Chinese. These questions are meticulously curated to ensure rigorous difficulty calibration, subject balance, and crosslinguistic alignment, enabling the assessment to be an Olympiad-level multi-disciplinary benchmark. We evaluate widely used models, including OpenAI o1, GPT-4o, DeepSeek-R1, etc. Experimental results indicate that advanced models perform poorly on complex reasoning, especially multimodal reasoning. Even the top-performing model OpenAI o1 achieves only 53.2% accuracy on our multimodal evaluation. Data and code are made publicly available at here."
  },
  {
    "title": "Visual Dominance and Emerging Multimodal Approaches in Distracted Driving Detection: A Review of Machine Learning Techniques",
    "url": "http://arxiv.org/abs/2505.01973v1",
    "arxiv_id": "2505.01973v1",
    "authors": [
      "Anthony Dontoh",
      "Stephanie Ivey",
      "Logan Sirbaugh",
      "Andrews Danyo",
      "Armstrong Aboah"
    ],
    "published": "2025-05-04T02:51:00+00:00",
    "summary": "Distracted driving continues to be a significant cause of road traffic injuries and fatalities worldwide, even with advancements in driver monitoring technologies. Recent developments in machine learning (ML) and deep learning (DL) have primarily focused on visual data to detect distraction, often neglecting the complex, multimodal nature of driver behavior. This systematic review assesses 74 peer-reviewed studies from 2019 to 2024 that utilize ML/DL techniques for distracted driving detection across visual, sensor-based, multimodal, and emerging modalities. The review highlights a significant prevalence of visual-only models, particularly convolutional neural networks (CNNs) and temporal architectures, which achieve high accuracy but show limited generalizability in real-world scenarios. Sensor-based and physiological models provide complementary strengths by capturing internal states and vehicle dynamics, while emerging techniques, such as auditory sensing and radio frequency (RF) methods, offer privacy-aware alternatives. Multimodal architecture consistently surpasses unimodal baselines, demonstrating enhanced robustness, context awareness, and scalability by integrating diverse data streams. These findings emphasize the need to move beyond visual-only approaches and adopt multimodal systems that combine visual, physiological, and vehicular cues while keeping in checking the need to balance computational requirements. Future research should focus on developing lightweight, deployable multimodal frameworks, incorporating personalized baselines, and establishing cross-modality benchmarks to ensure real-world reliability in advanced driver assistance systems (ADAS) and road safety interventions."
  },
  {
    "title": "A Comprehensive Analysis for Visual Object Hallucination in Large Vision-Language Models",
    "url": "http://arxiv.org/abs/2505.01958v1",
    "arxiv_id": "2505.01958v1",
    "authors": [
      "Liqiang Jing",
      "Guiming Hardy Chen",
      "Ehsan Aghazadeh",
      "Xin Eric Wang",
      "Xinya Du"
    ],
    "published": "2025-05-04T01:47:58+00:00",
    "summary": "Large Vision-Language Models (LVLMs) demonstrate remarkable capabilities in multimodal tasks, but visual object hallucination remains a persistent issue. It refers to scenarios where models generate inaccurate visual object-related information based on the query input, potentially leading to misinformation and concerns about safety and reliability. Previous works focus on the evaluation and mitigation of visual hallucinations, but the underlying causes have not been comprehensively investigated. In this paper, we analyze each component of LLaVA-like LVLMs -- the large language model, the vision backbone, and the projector -- to identify potential sources of error and their impact. Based on our observations, we propose methods to mitigate hallucination for each problematic component. Additionally, we developed two hallucination benchmarks: QA-VisualGenome, which emphasizes attribute and relation hallucinations, and QA-FB15k, which focuses on cognition-based hallucinations."
  },
  {
    "title": "Runtime Anomaly Detection for Drones: An Integrated Rule-Mining and Unsupervised-Learning Approach",
    "url": "http://arxiv.org/abs/2505.01947v1",
    "arxiv_id": "2505.01947v1",
    "authors": [
      "Ivan Tan",
      "Wei Minn",
      "Christopher M. Poskitt",
      "Lwin Khin Shar",
      "Lingxiao Jiang"
    ],
    "published": "2025-05-03T23:48:50+00:00",
    "summary": "UAVs, commonly referred to as drones, have witnessed a remarkable surge in popularity due to their versatile applications. These cyber-physical systems depend on multiple sensor inputs, such as cameras, GPS receivers, accelerometers, and gyroscopes, with faults potentially leading to physical instability and serious safety concerns. To mitigate such risks, anomaly detection has emerged as a crucial safeguarding mechanism, capable of identifying the physical manifestations of emerging issues and allowing operators to take preemptive action at runtime. Recent anomaly detection methods based on LSTM neural networks have shown promising results, but three challenges persist: the need for models that can generalise across the diverse mission profiles of drones; the need for interpretability, enabling operators to understand the nature of detected problems; and the need for capturing domain knowledge that is difficult to infer solely from log data. Motivated by these challenges, this paper introduces RADD, an integrated approach to anomaly detection in drones that combines rule mining and unsupervised learning. In particular, we leverage rules (or invariants) to capture expected relationships between sensors and actuators during missions, and utilise unsupervised learning techniques to cover more subtle relationships that the rules may have missed. We implement this approach using the ArduPilot drone software in the Gazebo simulator, utilising 44 rules derived across the main phases of drone missions, in conjunction with an ensemble of five unsupervised learning models. We find that our integrated approach successfully detects 93.84% of anomalies over six types of faults with a low false positive rate (2.33%), and can be deployed effectively at runtime. Furthermore, RADD outperforms a state-of-the-art LSTM-based method in detecting the different types of faults evaluated in our study."
  },
  {
    "title": "Multimodal Graph Representation Learning for Robust Surgical Workflow Recognition with Adversarial Feature Disentanglement",
    "url": "http://arxiv.org/abs/2505.01766v1",
    "arxiv_id": "2505.01766v1",
    "authors": [
      "Long Bai",
      "Boyi Ma",
      "Ruohan Wang",
      "Guankun Wang",
      "Beilei Cui",
      "Zhongliang Jiang",
      "Mobarakol Islam",
      "Zhe Min",
      "Jiewen Lai",
      "Nassir Navab",
      "Hongliang Ren"
    ],
    "published": "2025-05-03T09:43:30+00:00",
    "summary": "Surgical workflow recognition is vital for automating tasks, supporting decision-making, and training novice surgeons, ultimately improving patient safety and standardizing procedures. However, data corruption can lead to performance degradation due to issues like occlusion from bleeding or smoke in surgical scenes and problems with data storage and transmission. In this case, we explore a robust graph-based multimodal approach to integrating vision and kinematic data to enhance accuracy and reliability. Vision data captures dynamic surgical scenes, while kinematic data provides precise movement information, overcoming limitations of visual recognition under adverse conditions. We propose a multimodal Graph Representation network with Adversarial feature Disentanglement (GRAD) for robust surgical workflow recognition in challenging scenarios with domain shifts or corrupted data. Specifically, we introduce a Multimodal Disentanglement Graph Network that captures fine-grained visual information while explicitly modeling the complex relationships between vision and kinematic embeddings through graph-based message modeling. To align feature spaces across modalities, we propose a Vision-Kinematic Adversarial framework that leverages adversarial training to reduce modality gaps and improve feature consistency. Furthermore, we design a Contextual Calibrated Decoder, incorporating temporal and contextual priors to enhance robustness against domain shifts and corrupted data. Extensive comparative and ablation experiments demonstrate the effectiveness of our model and proposed modules. Moreover, our robustness experiments show that our method effectively handles data corruption during storage and transmission, exhibiting excellent stability and robustness. Our approach aims to advance automated surgical workflow recognition, addressing the complexities and dynamism inherent in surgical procedures."
  },
  {
    "title": "NMPCB: A Lightweight and Safety-Critical Motion Control Framework",
    "url": "http://arxiv.org/abs/2505.01752v1",
    "arxiv_id": "2505.01752v1",
    "authors": [
      "Longze Zheng",
      "Qinghe Liu"
    ],
    "published": "2025-05-03T09:02:35+00:00",
    "summary": "In multi-obstacle environments, real-time performance and safety in robot motion control have long been challenging issues, as conventional methods often struggle to balance the two. In this paper, we propose a novel motion control framework composed of a Neural network-based path planner and a Model Predictive Control (MPC) controller based on control Barrier function (NMPCB) . The planner predicts the next target point through a lightweight neural network and generates a reference trajectory for the controller. In the design of the controller, we introduce the dual problem of control barrier function (CBF) as the obstacle avoidance constraint, enabling it to ensure robot motion safety while significantly reducing computation time. The controller directly outputs control commands to the robot by tracking the reference trajectory. This framework achieves a balance between real-time performance and safety. We validate the feasibility of the framework through numerical simulations and real-world experiments."
  },
  {
    "title": "Cannot See the Forest for the Trees: Invoking Heuristics and Biases to Elicit Irrational Choices of LLMs",
    "url": "http://arxiv.org/abs/2505.02862v1",
    "arxiv_id": "2505.02862v1",
    "authors": [
      "Haoming Yang",
      "Ke Ma",
      "Xiaojun Jia",
      "Yingfei Sun",
      "Qianqian Xu",
      "Qingming Huang"
    ],
    "published": "2025-05-03T05:28:11+00:00",
    "summary": "Despite the remarkable performance of Large Language Models (LLMs), they remain vulnerable to jailbreak attacks, which can compromise their safety mechanisms. Existing studies often rely on brute-force optimization or manual design, failing to uncover potential risks in real-world scenarios. To address this, we propose a novel jailbreak attack framework, ICRT, inspired by heuristics and biases in human cognition. Leveraging the simplicity effect, we employ cognitive decomposition to reduce the complexity of malicious prompts. Simultaneously, relevance bias is utilized to reorganize prompts, enhancing semantic alignment and inducing harmful outputs effectively. Furthermore, we introduce a ranking-based harmfulness evaluation metric that surpasses the traditional binary success-or-failure paradigm by employing ranking aggregation methods such as Elo, HodgeRank, and Rank Centrality to comprehensively quantify the harmfulness of generated content. Experimental results show that our approach consistently bypasses mainstream LLMs' safety mechanisms and generates high-risk content, providing insights into jailbreak attack risks and contributing to stronger defense strategies."
  },
  {
    "title": "Sensing Safety Analysis for Vehicular Networks with Integrated Sensing and Communication (ISAC)",
    "url": "http://arxiv.org/abs/2505.01688v1",
    "arxiv_id": "2505.01688v1",
    "authors": [
      "Tingyu Shui",
      "Walid Saad",
      "Mingzhe Cheng"
    ],
    "published": "2025-05-03T04:39:57+00:00",
    "summary": "Integrated sensing and communication (ISAC) emerged as a key feature of next-generation 6G wireless systems, allowing them to achieve high data rates and sensing accuracy. While prior research has primarily focused on addressing communication safety in ISAC systems, the equally critical issue of sensing safety remains largely ignored. In this paper, a novel threat to the sensing safety of ISAC vehicle networks is studied, whereby a malicious reconfigurable intelligent surface (RIS) is deployed to compromise the sensing functionality of a roadside unit (RSU). Specifically, a malicious attacker dynamically adjusts the phase shifts of an RIS to spoof the sensing outcomes of a vehicular user (VU)'s echo delay, Doppler shift, and angle-of-departure (AoD). To achieve spoofing on Doppler shift estimation, a time-varying phase shift design on the RIS is proposed. Furthermore, the feasible spoofing frequency set with respect to the Doppler shift is analytical derived. Analytical results also demonstrate that the maximum likelihood estimator (MLE) of the AoD can be significantly misled under spoofed Doppler shift estimation. Simulation results validate our theoretical findings, showing that the RIS can induce a spoofed velocity estimation from 0.1 m/s to 14.9 m/s for a VU with velocity of 10 m/s, and can cause an AoD estimation error of up to 65^{\\circ} with only a 5^{\\circ} beam misalignment."
  },
  {
    "title": "Resilient Vehicular Communications under Imperfect Channel State Information",
    "url": "http://arxiv.org/abs/2505.01687v1",
    "arxiv_id": "2505.01687v1",
    "authors": [
      "Tingyu Shui",
      "Walid Saad",
      "Ye Hu",
      "Mingzhe Chen"
    ],
    "published": "2025-05-03T04:33:56+00:00",
    "summary": "Cellular vehicle-to-everything (C-V2X) networks provide a promising solution to improve road safety and traffic efficiency. One key challenge in such systems lies in meeting quality-of-service (QoS) requirements of vehicular communication links given limited network resources, particularly under imperfect channel state information (CSI) conditions caused by the highly dynamic environment. In this paper, a novel two-phase framework is proposed to instill resilience into C-V2X networks under unknown imperfect CSI. The resilience of the C-V2X network is defined, quantified, and optimized the first time through two principal dimensions: absorption phase and adaptation phase. Specifically, the probability distribution function (PDF) of the imperfect CSI is estimated during the absorption phase through dedicated absorption power scheme and resource block (RB) assignment. The estimated PDF is further used to analyze the interplay and reveal the tradeoff between these two phases. Then, a novel metric named hazard rate (HR) is exploited to balance the C-V2X network's prioritization on absorption and adaptation. Finally, the estimated PDF is exploited in the adaptation phase to recover the network's QoS through a real-time power allocation optimization. Simulation results demonstrate the superior capability of the proposed framework in sustaining the QoS of the C-V2X network under imperfect CSI. Specifically, in the adaptation phase, the proposed design reduces the vehicle-tovehicle (V2V) delay that exceeds QoS requirement by 35% and 56%, and improves the average vehicle-to-infrastructure (V2I) throughput by 14% and 16% compared to the model-based and data-driven benchmarks, respectively, without compromising the network's QoS in the absorption phase."
  },
  {
    "title": "Large Language Model Driven Development of Turbulence Models",
    "url": "http://arxiv.org/abs/2505.01681v1",
    "arxiv_id": "2505.01681v1",
    "authors": [
      "Zhongxin Yang",
      "Yuanwei Bin",
      "Yipeng Shi",
      "Xiang I. A. Yang"
    ],
    "published": "2025-05-03T04:01:25+00:00",
    "summary": "Artificial intelligence (AI) has achieved human-level performance in specialized tasks such as Go, image recognition, and protein folding, raising the prospect of an AI singularity-where machines not only match but surpass human reasoning. Here, we demonstrate a step toward this vision in the context of turbulence modeling. By treating a large language model (LLM), DeepSeek-R1, as an equal partner, we establish a closed-loop, iterative workflow in which the LLM proposes, refines, and reasons about near-wall turbulence models under adverse pressure gradients (APGs), system rotation, and surface roughness. Through multiple rounds of interaction involving long-chain reasoning and a priori and a posteriori evaluations, the LLM generates models that not only rediscover established strategies but also synthesize new ones that outperform baseline wall models. Specifically, it recommends incorporating a material derivative to capture history effects in APG flows, modifying the law of the wall to account for system rotation, and developing rough-wall models informed by surface statistics. In contrast to conventional data-driven turbulence modeling-often characterized by human-designed, black-box architectures-the models developed here are physically interpretable and grounded in clear reasoning."
  },
  {
    "title": "Evaluating Input Modalities for Pilot-Centered Taxiway Navigation: Insights from a Wizard-of-Oz Simulation",
    "url": "http://arxiv.org/abs/2505.01679v1",
    "arxiv_id": "2505.01679v1",
    "authors": [
      "Chan Chea Mean",
      "Sameer Alam",
      "Katherine Fennedy",
      "Meng-Hsueh Hsieh",
      "Shiwei Xin",
      "Brian Hilburn"
    ],
    "published": "2025-05-03T03:58:56+00:00",
    "summary": "Runway and taxiway incursions continue to challenge aviation safety, as pilots often experience disorientation from poor visibility in adverse conditions and cognitive workload in complex airport layouts. Current tools, such as airport moving maps on portable tablets, allow manual route planning but do not dynamically adapt to air traffic controllers' (ATCOs) clearances, limiting their effectiveness in high-stress scenarios. This study investigates the impact of different input modalities - paper-based, keyboard touch, map touch, and speech-to-text - on taxiway navigation performance, using a medium-fidelity flight simulator and a Wizard-of-Oz methodology to simulate ideal automation conditions. Contrary to common assumptions, recent studies indicate that paper-based methods outperform digital counterparts in accuracy and efficiency under certain conditions, highlighting critical limitations in current automation strategies. In response, our study investigates why manual methods may excel and how future automation can be optimized for pilot-centered operations. Employing a Wizard-of-Oz approach, we replicated the full taxiing process - from receiving ATCO clearances to executing maneuvers - and differentiated between readback and execution accuracy. Findings reveal that speech-based systems suffer from low pilot trust, necessitating hybrid solutions that integrate error correction and confidence indicators. These insights contribute to the development of future pilot-centered taxiway assistance that enhance situational awareness, minimize workload, and improve overall operational safety."
  },
  {
    "title": "Third-party compliance reviews for frontier AI safety frameworks",
    "url": "http://arxiv.org/abs/2505.01643v1",
    "arxiv_id": "2505.01643v1",
    "authors": [
      "Aidan Homewood",
      "Sophie Williams",
      "Noemi Dreksler",
      "John Lidiard",
      "Malcolm Murray",
      "Lennart Heim",
      "Marta Ziosi",
      "Se\u00e1n \u00d3 h\u00c9igeartaigh",
      "Michael Chen",
      "Kevin Wei",
      "Christoph Winter",
      "Miles Brundage",
      "Ben Garfinkel",
      "Jonas Schuett"
    ],
    "published": "2025-05-03T00:53:55+00:00",
    "summary": "Safety frameworks have emerged as a best practice for managing risks from frontier artificial intelligence (AI) systems. However, it may be difficult for stakeholders to know if companies are adhering to their frameworks. This paper explores a potential solution: third-party compliance reviews. During a third-party compliance review, an independent external party assesses whether a frontier AI company is complying with its safety framework. First, we discuss the main benefits and challenges of such reviews. On the one hand, they can increase compliance with safety frameworks and provide assurance to internal and external stakeholders. On the other hand, they can create information security risks, impose additional cost burdens, and cause reputational damage, but these challenges can be partially mitigated by drawing on best practices from other industries. Next, we answer practical questions about third-party compliance reviews, namely: (1) Who could conduct the review? (2) What information sources could the reviewer consider? (3) How could compliance with the safety framework be assessed? (4) What information about the review could be disclosed externally? (5) How could the findings guide development and deployment actions? (6) When could the reviews be conducted? For each question, we evaluate a set of plausible options. Finally, we suggest \"minimalist\", \"more ambitious\", and \"comprehensive\" approaches for each question that a frontier AI company could adopt."
  },
  {
    "title": "Skill-based Safe Reinforcement Learning with Risk Planning",
    "url": "http://arxiv.org/abs/2505.01619v1",
    "arxiv_id": "2505.01619v1",
    "authors": [
      "Hanping Zhang",
      "Yuhong Guo"
    ],
    "published": "2025-05-02T22:48:27+00:00",
    "summary": "Safe Reinforcement Learning (Safe RL) aims to ensure safety when an RL agent conducts learning by interacting with real-world environments where improper actions can induce high costs or lead to severe consequences. In this paper, we propose a novel Safe Skill Planning (SSkP) approach to enhance effective safe RL by exploiting auxiliary offline demonstration data. SSkP involves a two-stage process. First, we employ PU learning to learn a skill risk predictor from the offline demonstration data. Then, based on the learned skill risk predictor, we develop a novel risk planning process to enhance online safe RL and learn a risk-averse safe policy efficiently through interactions with the online RL environment, while simultaneously adapting the skill risk predictor to the environment. We conduct experiments in several benchmark robotic simulation environments. The experimental results demonstrate that the proposed approach consistently outperforms previous state-of-the-art safe RL methods."
  },
  {
    "title": "Grounding Task Assistance with Multimodal Cues from a Single Demonstration",
    "url": "http://arxiv.org/abs/2505.01578v1",
    "arxiv_id": "2505.01578v1",
    "authors": [
      "Gabriel Sarch",
      "Balasaravanan Thoravi Kumaravel",
      "Sahithya Ravi",
      "Vibhav Vineet",
      "Andrew D. Wilson"
    ],
    "published": "2025-05-02T20:43:11+00:00",
    "summary": "A person's demonstration often serves as a key reference for others learning the same task. However, RGB video, the dominant medium for representing these demonstrations, often fails to capture fine-grained contextual cues such as intent, safety-critical environmental factors, and subtle preferences embedded in human behavior. This sensory gap fundamentally limits the ability of Vision Language Models (VLMs) to reason about why actions occur and how they should adapt to individual users. To address this, we introduce MICA (Multimodal Interactive Contextualized Assistance), a framework that improves conversational agents for task assistance by integrating eye gaze and speech cues. MICA segments demonstrations into meaningful sub-tasks and extracts keyframes and captions that capture fine-grained intent and user-specific cues, enabling richer contextual grounding for visual question answering. Evaluations on questions derived from real-time chat-assisted task replication show that multimodal cues significantly improve response quality over frame-based retrieval. Notably, gaze cues alone achieves 93% of speech performance, and their combination yields the highest accuracy. Task type determines the effectiveness of implicit (gaze) vs. explicit (speech) cues, underscoring the need for adaptable multimodal models. These results highlight the limitations of frame-based context and demonstrate the value of multimodal signals for real-world AI task assistance."
  },
  {
    "title": "AI agents may be worth the hype but not the resources (yet): An initial exploration of machine translation quality and costs in three language pairs in the legal and news domains",
    "url": "http://arxiv.org/abs/2505.01560v1",
    "arxiv_id": "2505.01560v1",
    "authors": [
      "Vicent Briva Iglesias",
      "Gokhan Dogru"
    ],
    "published": "2025-05-02T20:02:13+00:00",
    "summary": "Large language models (LLMs) and multi-agent orchestration are touted as the next leap in machine translation (MT), but their benefits relative to conventional neural MT (NMT) remain unclear. This paper offers an empirical reality check. We benchmark five paradigms, Google Translate (strong NMT baseline), GPT-4o (general-purpose LLM), o1-preview (reasoning-enhanced LLM), and two GPT-4o-powered agentic workflows (sequential three-stage and iterative refinement), on test data drawn from a legal contract and news prose in three English-source pairs: Spanish, Catalan and Turkish. Automatic evaluation is performed with COMET, BLEU, chrF2 and TER; human evaluation is conducted with expert ratings of adequacy and fluency; efficiency with total input-plus-output token counts mapped to April 2025 pricing.   Automatic scores still favour the mature NMT system, which ranks first in seven of twelve metric-language combinations; o1-preview ties or places second in most remaining cases, while both multi-agent workflows trail. Human evaluation reverses part of this narrative: o1-preview produces the most adequate and fluent output in five of six comparisons, and the iterative agent edges ahead once, indicating that reasoning layers capture semantic nuance undervalued by surface metrics. Yet these qualitative gains carry steep costs. The sequential agent consumes roughly five times, and the iterative agent fifteen times, the tokens used by NMT or single-pass LLMs.   We advocate multidimensional, cost-aware evaluation protocols and highlight research directions that could tip the balance: leaner coordination strategies, selective agent activation, and hybrid pipelines combining single-pass LLMs with targeted agent intervention."
  },
  {
    "title": "Emotions in the Loop: A Survey of Affective Computing for Emotional Support",
    "url": "http://arxiv.org/abs/2505.01542v1",
    "arxiv_id": "2505.01542v1",
    "authors": [
      "Karishma Hegde",
      "Hemadri Jayalath"
    ],
    "published": "2025-05-02T19:06:05+00:00",
    "summary": "In a world where technology is increasingly embedded in our everyday experiences, systems that sense and respond to human emotions are elevating digital interaction. At the intersection of artificial intelligence and human-computer interaction, affective computing is emerging with innovative solutions where machines are humanized by enabling them to process and respond to user emotions. This survey paper explores recent research contributions in affective computing applications in the area of emotion recognition, sentiment analysis and personality assignment developed using approaches like large language models (LLMs), multimodal techniques, and personalized AI systems. We analyze the key contributions and innovative methodologies applied by the selected research papers by categorizing them into four domains: AI chatbot applications, multimodal input systems, mental health and therapy applications, and affective computing for safety applications. We then highlight the technological strengths as well as the research gaps and challenges related to these studies. Furthermore, the paper examines the datasets used in each study, highlighting how modality, scale, and diversity impact the development and performance of affective models. Finally, the survey outlines ethical considerations and proposes future directions to develop applications that are more safe, empathetic and practical."
  },
  {
    "title": "Rubber Mallet: A Study of High Frequency Localized Bit Flips and Their Impact on Security",
    "url": "http://arxiv.org/abs/2505.01518v1",
    "arxiv_id": "2505.01518v1",
    "authors": [
      "Andrew Adiletta",
      "Zane Weissman",
      "Fatemeh Khojasteh Dana",
      "Berk Sunar",
      "Shahin Tajik"
    ],
    "published": "2025-05-02T18:07:07+00:00",
    "summary": "The increasing density of modern DRAM has heightened its vulnerability to Rowhammer attacks, which induce bit flips by repeatedly accessing specific memory rows. This paper presents an analysis of bit flip patterns generated by advanced Rowhammer techniques that bypass existing hardware defenses. First, we investigate the phenomenon of adjacent bit flips--where two or more physically neighboring bits are corrupted simultaneously--and demonstrate they occur with significantly higher frequency than previously documented. We also show that if multiple bits flip within a byte, they are more likely to be adjacent than randomly distributed: for example, if 4 bits flip within a byte, there is an 87% chance that they are all adjacent. We also demonstrate that bit flips within a row will naturally cluster together likely due to the underlying physics of the attack. We then investigate two fault injection attacks enabled by multiple adjacent or nearby bit flips. First, we show how these correlated flips enable efficient cryptographic signature correction attacks, successfully recovering ECDSA private keys from OpenSSL implementations where single-bit approaches would be unfeasible. Second, we introduce a targeted attack against large language models by exploiting Rowhammer-induced corruptions in tokenizer dictionaries of GGUF model files. This attack effectively rewrites safety instructions in system prompts by swapping safety-critical tokens with benign alternatives, circumventing model guardrails while maintaining normal functionality in other contexts. Our experimental results across multiple DRAM configurations reveal that current memory protection schemes are inadequate against these sophisticated attack vectors, which can achieve their objectives with precise, minimal modifications rather than random corruption."
  },
  {
    "title": "Comparison of Waymo Rider-Only Crash Rates by Crash Type to Human Benchmarks at 56.7 Million Miles",
    "url": "http://arxiv.org/abs/2505.01515v1",
    "arxiv_id": "2505.01515v1",
    "authors": [
      "Kristofer D. Kusano",
      "John M. Scanlon",
      "Yin-Hsiu Chen",
      "Timothy L. McMurry",
      "Tilia Gode",
      "Trent Victor"
    ],
    "published": "2025-05-02T18:04:20+00:00",
    "summary": "SAE Level 4 Automated Driving Systems (ADSs) are deployed on public roads, including Waymo's Rider-Only (RO) ride-hailing service (without a driver behind the steering wheel). The objective of this study was to perform a retrospective safety assessment of Waymo's RO crash rate compared to human benchmarks, including disaggregated by crash type.   Eleven crash type groups were identified from commonly relied upon crash typologies that are derived from human crash databases. Human benchmarks were aligned to the same vehicle types, road types, and locations as where the Waymo Driver operated. Waymo crashes were extracted from the NHTSA Standing General Order (SGO). RO mileage was provided by the company via a public website. Any-injury-reported, Airbag Deployment, and Suspected Serious Injury+ crash outcomes were examined because they represented previously established, safety-relevant benchmarks where statistical testing could be performed at the current mileage.   Data was examined over 56.7 million RO miles through the end of January 2025, resulting in a statistically significant lower crashed vehicle rate for all crashes compared to the benchmarks in Any-Injury-Reported and Airbag Deployment, and Suspected Serious Injury+ crashes. Of the crash types, V2V Intersection crash events represented the largest total crash reduction, with a 96% reduction in Any-injury-reported (87%-99% CI) and a 91% reduction in Airbag Deployment (76%-98% CI) events. Cyclist, Motorcycle, Pedestrian, Secondary Crash, and Single Vehicle crashes were also statistically reduced for the Any-Injury-Reported outcome. There was no statistically significant disbenefit found in any of the 11 crash type groups.   This study represents the first retrospective safety assessment of an RO ADS that made statistical conclusions about more serious crash outcomes and analyzed crash rates on a crash type basis."
  },
  {
    "title": "Neutrino mass generation in asymptotically safe gravity",
    "url": "http://arxiv.org/abs/2505.01422v1",
    "arxiv_id": "2505.01422v1",
    "authors": [
      "Gustavo P. de Brito",
      "Astrid Eichhorn",
      "Antonio D. Pereira",
      "Masatoshi Yamada"
    ],
    "published": "2025-05-02T17:58:14+00:00",
    "summary": "There exist several distinct phenomenological models to generate neutrino masses. We explore, which of these models can consistently be embedded in a quantum theory of gravity and matter. We proceed by invoking a minimal number of degrees of freedom beyond the Standard Model. Thus, we first investigate whether the Weinberg operator, a dimension-five-operator that generates neutrino masses without requiring degrees of freedom beyond the Standard Model, can arise in asymptotically safe quantum gravity. We find a negative answer with far-reaching consequences: new degrees of freedom beyond gravity and the Standard Model are necessary to give neutrinos a mass in the asymptotic-safety paradigm. Second, we explore whether the type-I Seesaw mechanism is viable and discover an upper bound on the Seesaw scale. The bound depends on the mass of the visible neutrino. We find a numerical value of $10^{14}\\, \\rm GeV$ for this bound when neglecting neutrino mixing for a visible mass of $10^{-10}\\, \\rm GeV$. Conversely, for the most ``natural\" value of the Seesaw scale in a quantum-gravity setting, which is the Planck scale, we predict an upper bound for the neutrino mass of the visible neutrino of approximately $10^{-15}\\, \\rm GeV$. Third, we explore whether neutrinos could also be Pseudo-Dirac-neutrinos in asymptotic safety and find that this possibility can be accommodated."
  },
  {
    "title": "Evaluating Frontier Models for Stealth and Situational Awareness",
    "url": "http://arxiv.org/abs/2505.01420v1",
    "arxiv_id": "2505.01420v1",
    "authors": [
      "Mary Phuong",
      "Roland S. Zimmermann",
      "Ziyue Wang",
      "David Lindner",
      "Victoria Krakovna",
      "Sarah Cogan",
      "Allan Dafoe",
      "Lewis Ho",
      "Rohin Shah"
    ],
    "published": "2025-05-02T17:57:14+00:00",
    "summary": "Recent work has demonstrated the plausibility of frontier AI models scheming -- knowingly and covertly pursuing an objective misaligned with its developer's intentions. Such behavior could be very hard to detect, and if present in future advanced systems, could pose severe loss of control risk. It is therefore important for AI developers to rule out harm from scheming prior to model deployment. In this paper, we present a suite of scheming reasoning evaluations measuring two types of reasoning capabilities that we believe are prerequisites for successful scheming: First, we propose five evaluations of ability to reason about and circumvent oversight (stealth). Second, we present eleven evaluations for measuring a model's ability to instrumentally reason about itself, its environment and its deployment (situational awareness). We demonstrate how these evaluations can be used as part of a scheming inability safety case: a model that does not succeed on these evaluations is almost certainly incapable of causing severe harm via scheming in real deployment. We run our evaluations on current frontier models and find that none of them show concerning levels of either situational awareness or stealth."
  },
  {
    "title": "Explainable Machine Learning for Cyberattack Identification from Traffic Flows",
    "url": "http://arxiv.org/abs/2505.01488v1",
    "arxiv_id": "2505.01488v1",
    "authors": [
      "Yujing Zhou",
      "Marc L. Jacquet",
      "Robel Dawit",
      "Skyler Fabre",
      "Dev Sarawat",
      "Faheem Khan",
      "Madison Newell",
      "Yongxin Liu",
      "Dahai Liu",
      "Hongyun Chen",
      "Jian Wang",
      "Huihui Wang"
    ],
    "published": "2025-05-02T17:34:14+00:00",
    "summary": "The increasing automation of traffic management systems has made them prime targets for cyberattacks, disrupting urban mobility and public safety. Traditional network-layer defenses are often inaccessible to transportation agencies, necessitating a machine learning-based approach that relies solely on traffic flow data. In this study, we simulate cyberattacks in a semi-realistic environment, using a virtualized traffic network to analyze disruption patterns. We develop a deep learning-based anomaly detection system, demonstrating that Longest Stop Duration and Total Jam Distance are key indicators of compromised signals. To enhance interpretability, we apply Explainable AI (XAI) techniques, identifying critical decision factors and diagnosing misclassification errors. Our analysis reveals two primary challenges: transitional data inconsistencies, where mislabeled recovery-phase traffic misleads the model, and model limitations, where stealth attacks in low-traffic conditions evade detection. This work enhances AI-driven traffic security, improving both detection accuracy and trustworthiness in smart transportation systems."
  },
  {
    "title": "CHORUS: Zero-shot Hierarchical Retrieval and Orchestration for Generating Linear Programming Code",
    "url": "http://arxiv.org/abs/2505.01485v1",
    "arxiv_id": "2505.01485v1",
    "authors": [
      "Tasnim Ahmed",
      "Salimur Choudhury"
    ],
    "published": "2025-05-02T16:36:57+00:00",
    "summary": "Linear Programming (LP) problems aim to find the optimal solution to an objective under constraints. These problems typically require domain knowledge, mathematical skills, and programming ability, presenting significant challenges for non-experts. This study explores the efficiency of Large Language Models (LLMs) in generating solver-specific LP code. We propose CHORUS, a retrieval-augmented generation (RAG) framework for synthesizing Gurobi-based LP code from natural language problem statements. CHORUS incorporates a hierarchical tree-like chunking strategy for theoretical contents and generates additional metadata based on code examples from documentation to facilitate self-contained, semantically coherent retrieval. Two-stage retrieval approach of CHORUS followed by cross-encoder reranking further ensures contextual relevance. Finally, expertly crafted prompt and structured parser with reasoning steps improve code generation performance significantly. Experiments on the NL4Opt-Code benchmark show that CHORUS improves the performance of open-source LLMs such as Llama3.1 (8B), Llama3.3 (70B), Phi4 (14B), Deepseek-r1 (32B), and Qwen2.5-coder (32B) by a significant margin compared to baseline and conventional RAG. It also allows these open-source LLMs to outperform or match the performance of much stronger baselines-GPT3.5 and GPT4 while requiring far fewer computational resources. Ablation studies further demonstrate the importance of expert prompting, hierarchical chunking, and structured reasoning."
  },
  {
    "title": "Helping Big Language Models Protect Themselves: An Enhanced Filtering and Summarization System",
    "url": "http://arxiv.org/abs/2505.01315v1",
    "arxiv_id": "2505.01315v1",
    "authors": [
      "Sheikh Samit Muhaimin",
      "Spyridon Mastorakis"
    ],
    "published": "2025-05-02T14:42:26+00:00",
    "summary": "The recent growth in the use of Large Language Models has made them vulnerable to sophisticated adversarial assaults, manipulative prompts, and encoded malicious inputs. Existing countermeasures frequently necessitate retraining models, which is computationally costly and impracticable for deployment. Without the need for retraining or fine-tuning, this study presents a unique defense paradigm that allows LLMs to recognize, filter, and defend against adversarial or malicious inputs on their own. There are two main parts to the suggested framework: (1) A prompt filtering module that uses sophisticated Natural Language Processing (NLP) techniques, including zero-shot classification, keyword analysis, and encoded content detection (e.g. base64, hexadecimal, URL encoding), to detect, decode, and classify harmful inputs; and (2) A summarization module that processes and summarizes adversarial research literature to give the LLM context-aware defense knowledge. This approach strengthens LLMs' resistance to adversarial exploitation by fusing text extraction, summarization, and harmful prompt analysis. According to experimental results, this integrated technique has a 98.71% success rate in identifying harmful patterns, manipulative language structures, and encoded prompts. By employing a modest amount of adversarial research literature as context, the methodology also allows the model to react correctly to harmful inputs with a larger percentage of jailbreak resistance and refusal rate. While maintaining the quality of LLM responses, the framework dramatically increases LLM's resistance to hostile misuse, demonstrating its efficacy as a quick and easy substitute for time-consuming, retraining-based defenses."
  },
  {
    "title": "Helping Large Language Models Protect Themselves: An Enhanced Filtering and Summarization System",
    "url": "http://arxiv.org/abs/2505.01315v2",
    "arxiv_id": "2505.01315v2",
    "authors": [
      "Sheikh Samit Muhaimin",
      "Spyridon Mastorakis"
    ],
    "published": "2025-05-02T14:42:26+00:00",
    "summary": "The recent growth in the use of Large Language Models has made them vulnerable to sophisticated adversarial assaults, manipulative prompts, and encoded malicious inputs. Existing countermeasures frequently necessitate retraining models, which is computationally costly and impracticable for deployment. Without the need for retraining or fine-tuning, this study presents a unique defense paradigm that allows LLMs to recognize, filter, and defend against adversarial or malicious inputs on their own. There are two main parts to the suggested framework: (1) A prompt filtering module that uses sophisticated Natural Language Processing (NLP) techniques, including zero-shot classification, keyword analysis, and encoded content detection (e.g. base64, hexadecimal, URL encoding), to detect, decode, and classify harmful inputs; and (2) A summarization module that processes and summarizes adversarial research literature to give the LLM context-aware defense knowledge. This approach strengthens LLMs' resistance to adversarial exploitation by fusing text extraction, summarization, and harmful prompt analysis. According to experimental results, this integrated technique has a 98.71% success rate in identifying harmful patterns, manipulative language structures, and encoded prompts. By employing a modest amount of adversarial research literature as context, the methodology also allows the model to react correctly to harmful inputs with a larger percentage of jailbreak resistance and refusal rate. While maintaining the quality of LLM responses, the framework dramatically increases LLM's resistance to hostile misuse, demonstrating its efficacy as a quick and easy substitute for time-consuming, retraining-based defenses."
  },
  {
    "title": "Document Retrieval Augmented Fine-Tuning (DRAFT) for safety-critical software assessments",
    "url": "http://arxiv.org/abs/2505.01307v1",
    "arxiv_id": "2505.01307v1",
    "authors": [
      "Regan Bolton",
      "Mohammadreza Sheikhfathollahi",
      "Simon Parkinson",
      "Vanessa Vulovic",
      "Gary Bamford",
      "Dan Basher",
      "Howard Parkinson"
    ],
    "published": "2025-05-02T14:34:33+00:00",
    "summary": "Safety critical software assessment requires robust assessment against complex regulatory frameworks, a process traditionally limited by manual evaluation. This paper presents Document Retrieval-Augmented Fine-Tuning (DRAFT), a novel approach that enhances the capabilities of a large language model (LLM) for safety-critical compliance assessment. DRAFT builds upon existing Retrieval-Augmented Generation (RAG) techniques by introducing a novel fine-tuning framework that accommodates our dual-retrieval architecture, which simultaneously accesses both software documentation and applicable reference standards. To fine-tune DRAFT, we develop a semi-automated dataset generation methodology that incorporates variable numbers of relevant documents with meaningful distractors, closely mirroring real-world assessment scenarios. Experiments with GPT-4o-mini demonstrate a 7% improvement in correctness over the baseline model, with qualitative improvements in evidence handling, response structure, and domain-specific reasoning. DRAFT represents a practical approach to improving compliance assessment systems while maintaining the transparency and evidence-based reasoning essential in regulatory domains."
  },
  {
    "title": "Contactless pulse rate assessment: Results and insights for application in driving simulator",
    "url": "http://arxiv.org/abs/2505.01299v1",
    "arxiv_id": "2505.01299v1",
    "authors": [
      "\u0110or\u0111e D. Ne\u0161kovi\u0107",
      "Kristina Stojmenova Pe\u010de\u010dnik",
      "Jaka Sodnik",
      "Nadica Miljkovi\u0107"
    ],
    "published": "2025-05-02T14:22:12+00:00",
    "summary": "Camera-based monitoring of Pulse Rate (PR) enables continuous and unobtrusive assessment of driver's state, allowing estimation of fatigue or stress that could impact traffic safety. Commonly used wearable Photoplethysmography (PPG) sensors, while effective, suffer from motion artifacts and user discomfort. This study explores the feasibility of non-contact PR assessment using facial video recordings captured by a Red, Green, and Blue (RGB) camera in a driving simulation environment. The proposed approach detects subtle skin color variations due to blood flow and compares extracted PR values against reference measurements from a wearable wristband Empatica E4. We evaluate the impact of Eulerian Video Magnification (EVM) on signal quality and assess statistical differences in PR between age groups. Data obtained from 80 recordings from 64 healthy subjects covering a PR range of 45-160 bpm are analyzed, and signal extraction accuracy is quantified using metrics, such as Mean Absolute Error (MAE) and Root Mean Square Error (RMSE). Results show that EVM slightly improves PR estimation accuracy, reducing MAE from 6.48 bpm to 5.04 bpm and RMSE from 7.84 bpm to 6.38 bpm. A statistically significant difference is found between older and younger groups with both video-based and ground truth evaluation procedures. Additionally, we discuss Empatica E4 bias and its potential impact on the overall assessment of contact measurements. Altogether the findings demonstrate the feasibility of camera-based PR monitoring in dynamic environments and its potential integration into driving simulators for real-time physiological assessment."
  },
  {
    "title": "2DXformer: Dual Transformers for Wind Power Forecasting with Dual Exogenous Variables",
    "url": "http://arxiv.org/abs/2505.01286v1",
    "arxiv_id": "2505.01286v1",
    "authors": [
      "Yajuan Zhang",
      "Jiahai Jiang",
      "Yule Yan",
      "Liang Yang",
      "Ping Zhang"
    ],
    "published": "2025-05-02T14:00:48+00:00",
    "summary": "Accurate wind power forecasting can help formulate scientific dispatch plans, which is of great significance for maintaining the safety, stability, and efficient operation of the power system. In recent years, wind power forecasting methods based on deep learning have focused on extracting the spatiotemporal correlations among data, achieving significant improvements in forecasting accuracy. However, they exhibit two limitations. First, there is a lack of modeling for the inter-variable relationships, which limits the accuracy of the forecasts. Second, by treating endogenous and exogenous variables equally, it leads to unnecessary interactions between the endogenous and exogenous variables, increasing the complexity of the model. In this paper, we propose the 2DXformer, which, building upon the previous work's focus on spatiotemporal correlations, addresses the aforementioned two limitations. Specifically, we classify the inputs of the model into three types: exogenous static variables, exogenous dynamic variables, and endogenous variables. First, we embed these variables as variable tokens in a channel-independent manner. Then, we use the attention mechanism to capture the correlations among exogenous variables. Finally, we employ a multi-layer perceptron with residual connections to model the impact of exogenous variables on endogenous variables. Experimental results on two real-world large-scale datasets indicate that our proposed 2DXformer can further improve the performance of wind power forecasting. The code is available in this repository: \\href{https://github.com/jseaj/2DXformer}{https://github.com/jseaj/2DXformer}."
  },
  {
    "title": "Self-moderation in the decentralized era: decoding blocking behavior on Bluesky",
    "url": "http://arxiv.org/abs/2505.01174v1",
    "arxiv_id": "2505.01174v1",
    "authors": [
      "Carlo Bono",
      "Nick Liu",
      "Giuseppe Russo",
      "Francesco Pierri"
    ],
    "published": "2025-05-02T10:32:39+00:00",
    "summary": "Moderation and blocking behavior, both closely related to the mitigation of abuse and misinformation on social platforms, are fundamental mechanisms for maintaining healthy online communities. However, while centralized platforms typically employ top-down moderation, decentralized networks rely on users to self-regulate through mechanisms like blocking actions to safeguard their online experience. Given the novelty of the decentralized paradigm, addressing self-moderation is critical for understanding how community safety and user autonomy can be effectively balanced. This study examines user blocking on Bluesky, a decentralized social networking platform, providing a comprehensive analysis of over three months of user activity through the lens of blocking behaviour. We define profiles based on 86 features that describe user activity, content characteristics, and network interactions, addressing two primary questions: (1) Is the likelihood of a user being blocked inferable from their online behavior? and (2) What behavioral features are associated with an increased likelihood of being blocked? Our findings offer valuable insights and contribute with a robust analytical framework to advance research in moderation on decentralized social networks."
  },
  {
    "title": "VTS-LLM: Domain-Adaptive LLM Agent for Enhancing Awareness in Vessel Traffic Services through Natural Language",
    "url": "http://arxiv.org/abs/2505.00989v1",
    "arxiv_id": "2505.00989v1",
    "authors": [
      "Sijin Sun",
      "Liangbin Zhao",
      "Ming Deng",
      "Xiuju Fu"
    ],
    "published": "2025-05-02T04:27:50+00:00",
    "summary": "Vessel Traffic Services (VTS) are essential for maritime safety and regulatory compliance through real-time traffic management. However, with increasing traffic complexity and the prevalence of heterogeneous, multimodal data, existing VTS systems face limitations in spatiotemporal reasoning and intuitive human interaction. In this work, we propose VTS-LLM Agent, the first domain-adaptive large LLM agent tailored for interactive decision support in VTS operations. We formalize risk-prone vessel identification as a knowledge-augmented Text-to-SQL task, combining structured vessel databases with external maritime knowledge. To support this, we construct a curated benchmark dataset consisting of a custom schema, domain-specific corpus, and a query-SQL test set in multiple linguistic styles. Our framework incorporates NER-based relational reasoning, agent-based domain knowledge injection, semantic algebra intermediate representation, and query rethink mechanisms to enhance domain grounding and context-aware understanding. Experimental results show that VTS-LLM outperforms both general-purpose and SQL-focused baselines under command-style, operational-style, and formal natural language queries, respectively. Moreover, our analysis provides the first empirical evidence that linguistic style variation introduces systematic performance challenges in Text-to-SQL modeling. This work lays the foundation for natural language interfaces in vessel traffic services and opens new opportunities for proactive, LLM-driven maritime real-time traffic management."
  },
  {
    "title": "Seeking to Collide: Online Safety-Critical Scenario Generation for Autonomous Driving with Retrieval Augmented Large Language Models",
    "url": "http://arxiv.org/abs/2505.00972v1",
    "arxiv_id": "2505.00972v1",
    "authors": [
      "Yuewen Mei",
      "Tong Nie",
      "Jian Sun",
      "Ye Tian"
    ],
    "published": "2025-05-02T03:22:00+00:00",
    "summary": "Simulation-based testing is crucial for validating autonomous vehicles (AVs), yet existing scenario generation methods either overfit to common driving patterns or operate in an offline, non-interactive manner that fails to expose rare, safety-critical corner cases. In this paper, we introduce an online, retrieval-augmented large language model (LLM) framework for generating safety-critical driving scenarios. Our method first employs an LLM-based behavior analyzer to infer the most dangerous intent of the background vehicle from the observed state, then queries additional LLM agents to synthesize feasible adversarial trajectories. To mitigate catastrophic forgetting and accelerate adaptation, we augment the framework with a dynamic memorization and retrieval bank of intent-planner pairs, automatically expanding its behavioral library when novel intents arise. Evaluations using the Waymo Open Motion Dataset demonstrate that our model reduces the mean minimum time-to-collision from 1.62 to 1.08 s and incurs a 75% collision rate, substantially outperforming baselines."
  },
  {
    "title": "A SCADE Model Verification Method Based on B-Model Transformation",
    "url": "http://arxiv.org/abs/2505.00967v1",
    "arxiv_id": "2505.00967v1",
    "authors": [
      "Xili Hou",
      "Keming Wang",
      "Huibing Zhao",
      "Ruiyin Shi"
    ],
    "published": "2025-05-02T03:05:09+00:00",
    "summary": "Due to the limitations of SCADE models in expressing and verifying abstract specifications in safety-critical systems, this study proposes a formal verification framework based on the B-Method. By establishing a semantic equivalence transformation mechanism from SCADE models to B models, a hierarchical mapping rule set is constructed, covering type systems, control flow structures, and state machines. This effectively addresses key technical challenges such as loop-equivalent transformation proof for high-order operators and modeling of temporal logic storage structures. The proposed method innovatively leverages the abstraction capabilities of B-Method in set theory and first-order logic, overcoming the constraints of native verification tools of SCADE in complex specification descriptions. It successfully verifies abstract specifications that are difficult to model directly in SCADE. Experimental results show that the transformed B models achieve a higher defect detection rate and improved verification efficiency in the ProB verification environment compared to the native verifier of SCADE, significantly enhancing the formal verification capability of safety-critical systems. This study provides a cross-model verification paradigm for embedded control systems in avionics, rail transportation, and other domains, demonstrating substantial engineering application value."
  },
  {
    "title": "Llama-Nemotron: Efficient Reasoning Models",
    "url": "http://arxiv.org/abs/2505.00949v1",
    "arxiv_id": "2505.00949v1",
    "authors": [
      "Akhiad Bercovich",
      "Itay Levy",
      "Izik Golan",
      "Mohammad Dabbah",
      "Ran El-Yaniv",
      "Omri Puny",
      "Ido Galil",
      "Zach Moshe",
      "Tomer Ronen",
      "Najeeb Nabwani",
      "Ido Shahaf",
      "Oren Tropp",
      "Ehud Karpas",
      "Ran Zilberstein",
      "Jiaqi Zeng",
      "Soumye Singhal",
      "Alexander Bukharin",
      "Yian Zhang",
      "Tugrul Konuk",
      "Gerald Shen",
      "Ameya Sunil Mahabaleshwarkar",
      "Bilal Kartal",
      "Yoshi Suhara",
      "Olivier Delalleau",
      "Zijia Chen",
      "Zhilin Wang",
      "David Mosallanezhad",
      "Adi Renduchintala",
      "Haifeng Qian",
      "Dima Rekesh",
      "Fei Jia",
      "Somshubra Majumdar",
      "Vahid Noroozi",
      "Wasi Uddin Ahmad",
      "Sean Narenthiran",
      "Aleksander Ficek",
      "Mehrzad Samadi",
      "Jocelyn Huang",
      "Siddhartha Jain",
      "Igor Gitman",
      "Ivan Moshkov",
      "Wei Du",
      "Shubham Toshniwal",
      "George Armstrong",
      "Branislav Kisacanin",
      "Matvei Novikov",
      "Daria Gitman",
      "Evelina Bakhturina",
      "Jane Polak Scowcroft",
      "John Kamalu",
      "Dan Su",
      "Kezhi Kong",
      "Markus Kliegl",
      "Rabeeh Karimi",
      "Ying Lin",
      "Sanjeev Satheesh",
      "Jupinder Parmar",
      "Pritam Gundecha",
      "Brandon Norick",
      "Joseph Jennings",
      "Shrimai Prabhumoye",
      "Syeda Nahida Akter",
      "Mostofa Patwary",
      "Abhinav Khattar",
      "Deepak Narayanan",
      "Roger Waleffe",
      "Jimmy Zhang",
      "Bor-Yiing Su",
      "Guyue Huang",
      "Terry Kong",
      "Parth Chadha",
      "Sahil Jain",
      "Christine Harvey",
      "Elad Segal",
      "Jining Huang",
      "Sergey Kashirsky",
      "Robert McQueen",
      "Izzy Putterman",
      "George Lam",
      "Arun Venkatesan",
      "Sherry Wu",
      "Vinh Nguyen",
      "Manoj Kilaru",
      "Andrew Wang",
      "Anna Warno",
      "Abhilash Somasamudramath",
      "Sandip Bhaskar",
      "Maka Dong",
      "Nave Assaf",
      "Shahar Mor",
      "Omer Ullman Argov",
      "Scot Junkin",
      "Oleksandr Romanenko",
      "Pedro Larroy",
      "Monika Katariya",
      "Marco Rovinelli",
      "Viji Balas",
      "Nicholas Edelman",
      "Anahita Bhiwandiwalla",
      "Muthu Subramaniam",
      "Smita Ithape",
      "Karthik Ramamoorthy",
      "Yuting Wu",
      "Suguna Varshini Velury",
      "Omri Almog",
      "Joyjit Daw",
      "Denys Fridman",
      "Erick Galinkin",
      "Michael Evans",
      "Katherine Luna",
      "Leon Derczynski",
      "Nikki Pope",
      "Eileen Long",
      "Seth Schneider",
      "Guillermo Siman",
      "Tomasz Grzegorzek",
      "Pablo Ribalta",
      "Monika Katariya",
      "Joey Conway",
      "Trisha Saar",
      "Ann Guan",
      "Krzysztof Pawelec",
      "Shyamala Prayaga",
      "Oleksii Kuchaiev",
      "Boris Ginsburg",
      "Oluwatobi Olabiyi",
      "Kari Briski",
      "Jonathan Cohen",
      "Bryan Catanzaro",
      "Jonah Alben",
      "Yonatan Geifman",
      "Eric Chung"
    ],
    "published": "2025-05-02T01:35:35+00:00",
    "summary": "We introduce the Llama-Nemotron series of models, an open family of heterogeneous reasoning models that deliver exceptional reasoning capabilities, inference efficiency, and an open license for enterprise use. The family comes in three sizes -- Nano (8B), Super (49B), and Ultra (253B) -- and performs competitively with state-of-the-art reasoning models such as DeepSeek-R1 while offering superior inference throughput and memory efficiency. In this report, we discuss the training procedure for these models, which entails using neural architecture search from Llama 3 models for accelerated inference, knowledge distillation, and continued pretraining, followed by a reasoning-focused post-training stage consisting of two main parts: supervised fine-tuning and large scale reinforcement learning. Llama-Nemotron models are the first open-source models to support a dynamic reasoning toggle, allowing users to switch between standard chat and reasoning modes during inference. To further support open research and facilitate model development, we provide the following resources: 1. We release the Llama-Nemotron reasoning models -- LN-Nano, LN-Super, and LN-Ultra -- under the commercially permissive NVIDIA Open Model License Agreement. 2. We release the complete post-training dataset: Llama-Nemotron-Post-Training-Dataset. 3. We also release our training codebases: NeMo, NeMo-Aligner, and Megatron-LM."
  },
  {
    "title": "SSRLBot: Designing and Developing an LLM-based Agent using Socially Shared Regulated Learning",
    "url": "http://arxiv.org/abs/2505.00945v1",
    "arxiv_id": "2505.00945v1",
    "authors": [
      "Xiaoshan Huang",
      "Jie Gao",
      "Haolun Wu"
    ],
    "published": "2025-05-02T01:17:03+00:00",
    "summary": "Large language model (LLM)-based agents are increasingly used to support human experts by streamlining complex tasks and offering actionable insights. However, their application in multi-professional decision-making, particularly in teamwork contexts, remains underexplored. This design-based study addresses that gap by developing LLM functions to enhance collaboration, grounded in the Socially Shared Regulation of Learning (SSRL) framework and applied to medical diagnostic teamwork. SSRL emphasizes metacognitive, cognitive, motivational, and emotional processes in shared learning, focusing on how teams manage these processes to improve decision-making. This paper introduces SSRLBot, a prototype chatbot designed to help team members reflect on both their diagnostic performance and key SSRL skills. Its core functions include summarizing dialogues, analyzing SSRL behaviors, evaluating diagnostic outcomes, annotating SSRL markers in conversation, assessing their impact on performance, and identifying interpersonal regulatory dynamics. We compare SSRLBot's capabilities with those of Gemini-1.5, GPT-3.5, and Deepseek-R1 in a case study. SSRLBot demonstrates stronger alignment with SSRL theory, offering detailed evaluations that link behaviors to regulatory dimensions and suggesting improvements for collaboration. By integrating SSRL theory with LLM capabilities, SSRLBot contributes a novel tool for enhancing team-based decision-making and collaborative learning in high-stakes environments, such as medical education."
  },
  {
    "title": "Learning Neural Control Barrier Functions from Offline Data with Conservatism",
    "url": "http://arxiv.org/abs/2505.00908v1",
    "arxiv_id": "2505.00908v1",
    "authors": [
      "Ihab Tabbara",
      "Hussein Sibai"
    ],
    "published": "2025-05-01T23:01:03+00:00",
    "summary": "Safety filters, particularly those based on control barrier functions, have gained increased interest as effective tools for safe control of dynamical systems. Existing correct-by-construction synthesis algorithms, however, suffer from the curse of dimensionality. Deep learning approaches have been proposed in recent years to address this challenge. In this paper, we contribute to this line of work by proposing an algorithm for training control barrier functions from offline datasets. Our algorithm trains the filter to not only prevent the system from reaching unsafe states but also out-of-distribution ones, at which the filter would be unreliable. It is inspired by Conservative Q-learning, an offline reinforcement learning algorithm. We call its outputs Conservative Control Barrier Functions (CCBFs). Our empirical results demonstrate that CCBFs outperform existing methods in maintaining safety and out-of-distribution avoidance while minimally affecting task performance."
  },
  {
    "title": "Inattentional Blindness with Augmented Reality HUDS: An On-road Study",
    "url": "http://arxiv.org/abs/2505.00879v1",
    "arxiv_id": "2505.00879v1",
    "authors": [
      "Nayara de Oliveira Faria",
      "Joseph L. Gabbard"
    ],
    "published": "2025-05-01T21:41:38+00:00",
    "summary": "As the integration of augmented reality (AR) technology in head-up displays (HUDs) becomes more prevalent in vehicles, it is crucial to understand how to design and evaluate AR interfaces to ensure safety. With new AR displays capable of rendering images with larger field of views and at varying depths, the visual and cognitive separation between graphical and real-world visual stimuli will be increasingly more difficult to quantify as will drivers' ability to efficiently allocate visual attention between the two sets of stimuli. In this study, we present a user study that serves as a crucial first step in gaining insight into inattentional blindness while using AR in surface transportation, where understanding is currently limited. Our primary goal is to investigate how the visual demand of AR tasks influences drivers' ability to detect stimuli, and whether the nature of the stimuli itself plays a role in this effect. To address these questions, we designed an on-road user study aimed at producing a more realistic and ecologically valid understanding of the phenomenon.   Our results show that drivers' ability to timely detect stimuli in the environment decreased as the AR task visual demand increased demonstrated by both detection performance and inattentional blindness metrics. Further, inattentional blindness caused by AR displays appears to be more prevalent within drivers' central field of view. We conclude by discussing implications towards a safety-centric evaluation framework for AR HUDs."
  },
  {
    "title": "From Texts to Shields: Convergence of Large Language Models and Cybersecurity",
    "url": "http://arxiv.org/abs/2505.00841v1",
    "arxiv_id": "2505.00841v1",
    "authors": [
      "Tao Li",
      "Ya-Ting Yang",
      "Yunian Pan",
      "Quanyan Zhu"
    ],
    "published": "2025-05-01T20:01:07+00:00",
    "summary": "This report explores the convergence of large language models (LLMs) and cybersecurity, synthesizing interdisciplinary insights from network security, artificial intelligence, formal methods, and human-centered design. It examines emerging applications of LLMs in software and network security, 5G vulnerability analysis, and generative security engineering. The report highlights the role of agentic LLMs in automating complex tasks, improving operational efficiency, and enabling reasoning-driven security analytics. Socio-technical challenges associated with the deployment of LLMs -- including trust, transparency, and ethical considerations -- can be addressed through strategies such as human-in-the-loop systems, role-specific training, and proactive robustness testing. The report further outlines critical research challenges in ensuring interpretability, safety, and fairness in LLM-based systems, particularly in high-stakes domains. By integrating technical advances with organizational and societal considerations, this report presents a forward-looking research agenda for the secure and effective adoption of LLMs in cybersecurity."
  },
  {
    "title": "IberFire -- a detailed creation of a spatio-temporal dataset for wildfire risk assessment in Spain",
    "url": "http://arxiv.org/abs/2505.00837v1",
    "arxiv_id": "2505.00837v1",
    "authors": [
      "Julen Ercibengoa",
      "Meritxell G\u00f3mez-Omella",
      "Izaro Goienetxea"
    ],
    "published": "2025-05-01T19:54:17+00:00",
    "summary": "Wildfires pose a critical environmental issue to ecosystems, economies, and public safety, particularly in Mediterranean regions such as Spain. Accurate predictive models rely on high-resolution spatio-temporal data to capture the complex interplay of environmental and anthropogenic factors. To address the lack of localised and fine-grained datasets in Spain, this work introduces IberFire, a spatio-temporal datacube at 1 km x 1 km x 1-day resolution covering mainland Spain and the Balearic Islands from December 2007 to December 2024. IberFire integrates 260 features across eight main categories: auxiliary features, fire history, geography, topography, meteorology, vegetation indices, human activity, and land cover. All features are derived from open-access sources, ensuring transparency and real-time applicability. The data processing pipeline was implemented entirely using open-source tools, and the codebase has been made publicly available. This work not only enhances spatio-temporal granularity and feature diversity compared to existing European datacubes but also provides a reproducible methodology for constructing similar datasets. IberFire supports advanced wildfire risk modelling through Machine Learning (ML) and Deep Learning (DL) techniques, enables climate pattern analysis and informs strategic planning in fire prevention and land management. The dataset is publicly available on Zenodo to promote open research and collaboration."
  },
  {
    "title": "HMCF: A Human-in-the-loop Multi-Robot Collaboration Framework Based on Large Language Models",
    "url": "http://arxiv.org/abs/2505.00820v1",
    "arxiv_id": "2505.00820v1",
    "authors": [
      "Zhaoxing Li",
      "Wenbo Wu",
      "Yue Wang",
      "Yanran Xu",
      "William Hunt",
      "Sebastian Stein"
    ],
    "published": "2025-05-01T19:23:50+00:00",
    "summary": "Rapid advancements in artificial intelligence (AI) have enabled robots to performcomplex tasks autonomously with increasing precision. However, multi-robot systems (MRSs) face challenges in generalization, heterogeneity, and safety, especially when scaling to large-scale deployments like disaster response. Traditional approaches often lack generalization, requiring extensive engineering for new tasks and scenarios, and struggle with managing diverse robots. To overcome these limitations, we propose a Human-in-the-loop Multi-Robot Collaboration Framework (HMCF) powered by large language models (LLMs). LLMs enhance adaptability by reasoning over diverse tasks and robot capabilities, while human oversight ensures safety and reliability, intervening only when necessary. Our framework seamlessly integrates human oversight, LLM agents, and heterogeneous robots to optimize task allocation and execution. Each robot is equipped with an LLM agent capable of understanding its capabilities, converting tasks into executable instructions, and reducing hallucinations through task verification and human supervision. Simulation results show that our framework outperforms state-of-the-art task planning methods, achieving higher task success rates with an improvement of 4.76%. Real-world tests demonstrate its robust zero-shot generalization feature and ability to handle diverse tasks and environments with minimal human intervention."
  },
  {
    "title": "Uncertainty-aware Latent Safety Filters for Avoiding Out-of-Distribution Failures",
    "url": "http://arxiv.org/abs/2505.00779v1",
    "arxiv_id": "2505.00779v1",
    "authors": [
      "Junwon Seo",
      "Kensuke Nakamura",
      "Andrea Bajcsy"
    ],
    "published": "2025-05-01T18:18:17+00:00",
    "summary": "Recent advances in generative world models have enabled classical safe control methods, such as Hamilton-Jacobi (HJ) reachability, to generalize to complex robotic systems operating directly from high-dimensional sensor observations. However, obtaining comprehensive coverage of all safety-critical scenarios during world model training is extremely challenging. As a result, latent safety filters built on top of these models may miss novel hazards and even fail to prevent known ones, overconfidently misclassifying risky out-of-distribution (OOD) situations as safe. To address this, we introduce an uncertainty-aware latent safety filter that proactively steers robots away from both known and unseen failures. Our key idea is to use the world model's epistemic uncertainty as a proxy for identifying unseen potential hazards. We propose a principled method to detect OOD world model predictions by calibrating an uncertainty threshold via conformal prediction. By performing reachability analysis in an augmented state space-spanning both the latent representation and the epistemic uncertainty-we synthesize a latent safety filter that can reliably safeguard arbitrary policies from both known and unseen safety hazards. In simulation and hardware experiments on vision-based control tasks with a Franka manipulator, we show that our uncertainty-aware safety filter preemptively detects potential unsafe scenarios and reliably proposes safe, in-distribution actions. Video results can be found on the project website at https://cmu-intentlab.github.io/UNISafe"
  },
  {
    "title": "Towards Autonomous Micromobility through Scalable Urban Simulation",
    "url": "http://arxiv.org/abs/2505.00690v1",
    "arxiv_id": "2505.00690v1",
    "authors": [
      "Wayne Wu",
      "Honglin He",
      "Chaoyuan Zhang",
      "Jack He",
      "Seth Z. Zhao",
      "Ran Gong",
      "Quanyi Li",
      "Bolei Zhou"
    ],
    "published": "2025-05-01T17:52:29+00:00",
    "summary": "Micromobility, which utilizes lightweight mobile machines moving in urban public spaces, such as delivery robots and mobility scooters, emerges as a promising alternative to vehicular mobility. Current micromobility depends mostly on human manual operation (in-person or remote control), which raises safety and efficiency concerns when navigating busy urban environments full of unpredictable obstacles and pedestrians. Assisting humans with AI agents in maneuvering micromobility devices presents a viable solution for enhancing safety and efficiency. In this work, we present a scalable urban simulation solution to advance autonomous micromobility. First, we build URBAN-SIM - a high-performance robot learning platform for large-scale training of embodied agents in interactive urban scenes. URBAN-SIM contains three critical modules: Hierarchical Urban Generation pipeline, Interactive Dynamics Generation strategy, and Asynchronous Scene Sampling scheme, to improve the diversity, realism, and efficiency of robot learning in simulation. Then, we propose URBAN-BENCH - a suite of essential tasks and benchmarks to gauge various capabilities of the AI agents in achieving autonomous micromobility. URBAN-BENCH includes eight tasks based on three core skills of the agents: Urban Locomotion, Urban Navigation, and Urban Traverse. We evaluate four robots with heterogeneous embodiments, such as the wheeled and legged robots, across these tasks. Experiments on diverse terrains and urban structures reveal each robot's strengths and limitations."
  },
  {
    "title": "Multi-Constraint Safe Reinforcement Learning via Closed-form Solution for Log-Sum-Exp Approximation of Control Barrier Functions",
    "url": "http://arxiv.org/abs/2505.00671v1",
    "arxiv_id": "2505.00671v1",
    "authors": [
      "Chenggang Wang",
      "Xinyi Wang",
      "Yutong Dong",
      "Lei Song",
      "Xinping Guan"
    ],
    "published": "2025-05-01T17:22:11+00:00",
    "summary": "The safety of training task policies and their subsequent application using reinforcement learning (RL) methods has become a focal point in the field of safe RL. A central challenge in this area remains the establishment of theoretical guarantees for safety during both the learning and deployment processes. Given the successful implementation of Control Barrier Function (CBF)-based safety strategies in a range of control-affine robotic systems, CBF-based safe RL demonstrates significant promise for practical applications in real-world scenarios. However, integrating these two approaches presents several challenges. First, embedding safety optimization within the RL training pipeline requires that the optimization outputs be differentiable with respect to the input parameters, a condition commonly referred to as differentiable optimization, which is non-trivial to solve. Second, the differentiable optimization framework confronts significant efficiency issues, especially when dealing with multi-constraint problems. To address these challenges, this paper presents a CBF-based safe RL architecture that effectively mitigates the issues outlined above. The proposed approach constructs a continuous AND logic approximation for the multiple constraints using a single composite CBF. By leveraging this approximation, a close-form solution of the quadratic programming is derived for the policy network in RL, thereby circumventing the need for differentiable optimization within the end-to-end safe RL pipeline. This strategy significantly reduces computational complexity because of the closed-form solution while maintaining safety guarantees. Simulation results demonstrate that, in comparison to existing approaches relying on differentiable optimization, the proposed method significantly reduces training computational costs while ensuring provable safety throughout the training process."
  },
  {
    "title": "DeepCritic: Deliberate Critique with Large Language Models",
    "url": "http://arxiv.org/abs/2505.00662v1",
    "arxiv_id": "2505.00662v1",
    "authors": [
      "Wenkai Yang",
      "Jingwen Chen",
      "Yankai Lin",
      "Ji-Rong Wen"
    ],
    "published": "2025-05-01T17:03:17+00:00",
    "summary": "As Large Language Models (LLMs) are rapidly evolving, providing accurate feedback and scalable oversight on their outputs becomes an urgent and critical problem. Leveraging LLMs as critique models to achieve automated supervision is a promising solution. In this work, we focus on studying and enhancing the math critique ability of LLMs. Current LLM critics provide critiques that are too shallow and superficial on each step, leading to low judgment accuracy and struggling to offer sufficient feedback for the LLM generator to correct mistakes. To tackle this issue, we propose a novel and effective two-stage framework to develop LLM critics that are capable of deliberately critiquing on each reasoning step of math solutions. In the first stage, we utilize Qwen2.5-72B-Instruct to generate 4.5K long-form critiques as seed data for supervised fine-tuning. Each seed critique consists of deliberate step-wise critiques that includes multi-perspective verifications as well as in-depth critiques of initial critiques for each reasoning step. Then, we perform reinforcement learning on the fine-tuned model with either existing human-labeled data from PRM800K or our automatically annotated data obtained via Monte Carlo sampling-based correctness estimation, to further incentivize its critique ability. Our developed critique model built on Qwen2.5-7B-Instruct not only significantly outperforms existing LLM critics (including the same-sized DeepSeek-R1-distill models and GPT-4o) on various error identification benchmarks, but also more effectively helps the LLM generator refine erroneous steps through more detailed feedback."
  },
  {
    "title": "Catastrophic Liability: Managing Systemic Risks in Frontier AI Development",
    "url": "http://arxiv.org/abs/2505.00616v1",
    "arxiv_id": "2505.00616v1",
    "authors": [
      "Aidan Kierans",
      "Kaley Rittichier",
      "Utku Sonsayar"
    ],
    "published": "2025-05-01T15:47:14+00:00",
    "summary": "As artificial intelligence systems grow more capable and autonomous, frontier AI development poses potential systemic risks that could affect society at a massive scale. Current practices at many AI labs developing these systems lack sufficient transparency around safety measures, testing procedures, and governance structures. This opacity makes it challenging to verify safety claims or establish appropriate liability when harm occurs. Drawing on liability frameworks from nuclear energy, aviation software, and healthcare, we propose a comprehensive approach to safety documentation and accountability in frontier AI development."
  },
  {
    "title": "100 Days After DeepSeek-R1: A Survey on Replication Studies and More Directions for Reasoning Language Models",
    "url": "http://arxiv.org/abs/2505.00551v1",
    "arxiv_id": "2505.00551v1",
    "authors": [
      "Chong Zhang",
      "Yue Deng",
      "Xiang Lin",
      "Bin Wang",
      "Dianwen Ng",
      "Hai Ye",
      "Xingxuan Li",
      "Yao Xiao",
      "Zhanfeng Mo",
      "Qi Zhang",
      "Lidong Bing"
    ],
    "published": "2025-05-01T14:28:35+00:00",
    "summary": "The recent development of reasoning language models (RLMs) represents a novel evolution in large language models. In particular, the recent release of DeepSeek-R1 has generated widespread social impact and sparked enthusiasm in the research community for exploring the explicit reasoning paradigm of language models. However, the implementation details of the released models have not been fully open-sourced by DeepSeek, including DeepSeek-R1-Zero, DeepSeek-R1, and the distilled small models. As a result, many replication studies have emerged aiming to reproduce the strong performance achieved by DeepSeek-R1, reaching comparable performance through similar training procedures and fully open-source data resources. These works have investigated feasible strategies for supervised fine-tuning (SFT) and reinforcement learning from verifiable rewards (RLVR), focusing on data preparation and method design, yielding various valuable insights. In this report, we provide a summary of recent replication studies to inspire future research. We primarily focus on SFT and RLVR as two main directions, introducing the details for data construction, method design and training procedure of current replication studies. Moreover, we conclude key findings from the implementation details and experimental results reported by these studies, anticipating to inspire future research. We also discuss additional techniques of enhancing RLMs, highlighting the potential of expanding the application scope of these models, and discussing the challenges in development. By this survey, we aim to help researchers and developers of RLMs stay updated with the latest advancements, and seek to inspire new ideas to further enhance RLMs."
  },
  {
    "title": "100 Days After DeepSeek-R1: A Survey on Replication Studies and More Directions for Reasoning Language Models",
    "url": "http://arxiv.org/abs/2505.00551v2",
    "arxiv_id": "2505.00551v2",
    "authors": [
      "Chong Zhang",
      "Yue Deng",
      "Xiang Lin",
      "Bin Wang",
      "Dianwen Ng",
      "Hai Ye",
      "Xingxuan Li",
      "Yao Xiao",
      "Zhanfeng Mo",
      "Qi Zhang",
      "Lidong Bing"
    ],
    "published": "2025-05-01T14:28:35+00:00",
    "summary": "The recent development of reasoning language models (RLMs) represents a novel evolution in large language models. In particular, the recent release of DeepSeek-R1 has generated widespread social impact and sparked enthusiasm in the research community for exploring the explicit reasoning paradigm of language models. However, the implementation details of the released models have not been fully open-sourced by DeepSeek, including DeepSeek-R1-Zero, DeepSeek-R1, and the distilled small models. As a result, many replication studies have emerged aiming to reproduce the strong performance achieved by DeepSeek-R1, reaching comparable performance through similar training procedures and fully open-source data resources. These works have investigated feasible strategies for supervised fine-tuning (SFT) and reinforcement learning from verifiable rewards (RLVR), focusing on data preparation and method design, yielding various valuable insights. In this report, we provide a summary of recent replication studies to inspire future research. We primarily focus on SFT and RLVR as two main directions, introducing the details for data construction, method design and training procedure of current replication studies. Moreover, we conclude key findings from the implementation details and experimental results reported by these studies, anticipating to inspire future research. We also discuss additional techniques of enhancing RLMs, highlighting the potential of expanding the application scope of these models, and discussing the challenges in development. By this survey, we aim to help researchers and developers of RLMs stay updated with the latest advancements, and seek to inspire new ideas to further enhance RLMs."
  },
  {
    "title": "Linear Phase Balancing Scheme using Voltage Unbalance Sensitivities in Multi-phase Power Distribution Grids",
    "url": "http://arxiv.org/abs/2505.00519v1",
    "arxiv_id": "2505.00519v1",
    "authors": [
      "Rahul K. Gupta"
    ],
    "published": "2025-05-01T13:40:31+00:00",
    "summary": "Power distribution networks, especially in North America, are often unbalanced due to the mix of single-, two- and three-phase networks as well as due to the high penetration of single-phase devices at the distribution level such as electric vehicle (EV) chargers and single-phase solar plants. However, the network operator must adhere to the voltage unbalance levels within the limits specified by IEEE, IEC, and NEMA standards for the safety of the equipment as well as the efficiency of the network operation. Existing works have proposed active and reactive power control in the network to minimize imbalances. However, these optimization problems are highly nonlinear and nonconvex due to the inherent non-linearity of unbalanced metrics and power-flow equations. In this work, we propose a linearization approach of unbalance metrics such as voltage unbalance factors (VUF), phase voltage unbalance rate (PVUR), and line voltage unbalance rate (LVUR) using the first order Taylor's approximation. This linearization is then applied to the phase balancing control scheme; it is formulated as a feedback approach where the linearization is updated successively after the active/reactive control setpoint has been actuated and shows improvement in voltage imbalances. We demonstrate the application of the proposed scheme on a standard IEEE benchmark test case, demonstrating its effectiveness."
  },
  {
    "title": "Safety-Critical Traffic Simulation with Guided Latent Diffusion Model",
    "url": "http://arxiv.org/abs/2505.00515v1",
    "arxiv_id": "2505.00515v1",
    "authors": [
      "Mingxing Peng",
      "Ruoyu Yao",
      "Xusen Guo",
      "Yuting Xie",
      "Xianda Chen",
      "Jun Ma"
    ],
    "published": "2025-05-01T13:33:34+00:00",
    "summary": "Safety-critical traffic simulation plays a crucial role in evaluating autonomous driving systems under rare and challenging scenarios. However, existing approaches often generate unrealistic scenarios due to insufficient consideration of physical plausibility and suffer from low generation efficiency. To address these limitations, we propose a guided latent diffusion model (LDM) capable of generating physically realistic and adversarial safety-critical traffic scenarios. Specifically, our model employs a graph-based variational autoencoder (VAE) to learn a compact latent space that captures complex multi-agent interactions while improving computational efficiency. Within this latent space, the diffusion model performs the denoising process to produce realistic trajectories. To enable controllable and adversarial scenario generation, we introduce novel guidance objectives that drive the diffusion process toward producing adversarial and behaviorally realistic driving behaviors. Furthermore, we develop a sample selection module based on physical feasibility checks to further enhance the physical plausibility of the generated scenarios. Extensive experiments on the nuScenes dataset demonstrate that our method achieves superior adversarial effectiveness and generation efficiency compared to existing baselines while maintaining a high level of realism. Our work provides an effective tool for realistic safety-critical scenario simulation, paving the way for more robust evaluation of autonomous driving systems."
  },
  {
    "title": "Variational OOD State Correction for Offline Reinforcement Learning",
    "url": "http://arxiv.org/abs/2505.00503v1",
    "arxiv_id": "2505.00503v1",
    "authors": [
      "Ke Jiang",
      "Wen Jiang",
      "Xiaoyang Tan"
    ],
    "published": "2025-05-01T13:14:07+00:00",
    "summary": "The performance of Offline reinforcement learning is significantly impacted by the issue of state distributional shift, and out-of-distribution (OOD) state correction is a popular approach to address this problem. In this paper, we propose a novel method named Density-Aware Safety Perception (DASP) for OOD state correction. Specifically, our method encourages the agent to prioritize actions that lead to outcomes with higher data density, thereby promoting its operation within or the return to in-distribution (safe) regions. To achieve this, we optimize the objective within a variational framework that concurrently considers both the potential outcomes of decision-making and their density, thus providing crucial contextual information for safe decision-making. Finally, we validate the effectiveness and feasibility of our proposed method through extensive experimental evaluations on the offline MuJoCo and AntMaze suites."
  },
  {
    "title": "A Generalised Framework for Property-Driven Machine Learning",
    "url": "http://arxiv.org/abs/2505.00466v1",
    "arxiv_id": "2505.00466v1",
    "authors": [
      "Thomas Flinkow",
      "Marco Casadio",
      "Colin Kessler",
      "Rosemary Monahan",
      "Ekaterina Komendantskaya"
    ],
    "published": "2025-05-01T11:33:38+00:00",
    "summary": "Neural networks have been shown to frequently fail to satisfy critical safety and correctness properties after training, highlighting the pressing need for training methods that incorporate such properties directly. While adversarial training can be used to improve robustness to small perturbations within $\\epsilon$-cubes, domains other than computer vision -- such as control systems and natural language processing -- may require more flexible input region specifications via generalised hyper-rectangles. Meanwhile, differentiable logics offer a way to encode arbitrary logical constraints as additional loss terms that guide the learning process towards satisfying these constraints. In this paper, we investigate how these two complementary approaches can be unified within a single framework for property-driven machine learning. We show that well-known properties from the literature are subcases of this general approach, and we demonstrate its practical effectiveness on a case study involving a neural network controller for a drone system. Our framework is publicly available at https://github.com/tflinkow/property-driven-ml."
  },
  {
    "title": "Toward Automated Regulatory Decision-Making: Trustworthy Medical Device Risk Classification with Multimodal Transformers and Self-Training",
    "url": "http://arxiv.org/abs/2505.00422v1",
    "arxiv_id": "2505.00422v1",
    "authors": [
      "Yu Han",
      "Aaron Ceross",
      "Jeroen H. M. Bergmann"
    ],
    "published": "2025-05-01T09:41:41+00:00",
    "summary": "Accurate classification of medical device risk levels is essential for regulatory oversight and clinical safety. We present a Transformer-based multimodal framework that integrates textual descriptions and visual information to predict device regulatory classification. The model incorporates a cross-attention mechanism to capture intermodal dependencies and employs a self-training strategy for improved generalization under limited supervision. Experiments on a real-world regulatory dataset demonstrate that our approach achieves up to 90.4% accuracy and 97.9% AUROC, significantly outperforming text-only (77.2%) and image-only (54.8%) baselines. Compared to standard multimodal fusion, the self-training mechanism improved SVM performance by 3.3 percentage points in accuracy (from 87.1% to 90.4%) and 1.4 points in macro-F1, suggesting that pseudo-labeling can effectively enhance generalization under limited supervision. Ablation studies further confirm the complementary benefits of both cross-modal attention and self-training."
  },
  {
    "title": "Safety in the Face of Adversity: Achieving Zero Constraint Violation in Online Learning with Slowly Changing Constraints",
    "url": "http://arxiv.org/abs/2505.00398v1",
    "arxiv_id": "2505.00398v1",
    "authors": [
      "Bassel Hamoud",
      "Ilnura Usmanova",
      "Kfir Y. Levy"
    ],
    "published": "2025-05-01T08:41:17+00:00",
    "summary": "We present the first theoretical guarantees for zero constraint violation in Online Convex Optimization (OCO) across all rounds, addressing dynamic constraint changes. Unlike existing approaches in constrained OCO, which allow for occasional safety breaches, we provide the first approach for maintaining strict safety under the assumption of gradually evolving constraints, namely the constraints change at most by a small amount between consecutive rounds. This is achieved through a primal-dual approach and Online Gradient Ascent in the dual space. We show that employing a dichotomous learning rate enables ensuring both safety, via zero constraint violation, and sublinear regret. Our framework marks a departure from previous work by providing the first provable guarantees for maintaining absolute safety in the face of changing constraints in OCO."
  },
  {
    "title": "A Survey on Large Language Model based Human-Agent Systems",
    "url": "http://arxiv.org/abs/2505.00753v1",
    "arxiv_id": "2505.00753v1",
    "authors": [
      "Henry Peng Zou",
      "Wei-Chieh Huang",
      "Yaozu Wu",
      "Yankai Chen",
      "Chunyu Miao",
      "Hoang Nguyen",
      "Yue Zhou",
      "Weizhi Zhang",
      "Liancheng Fang",
      "Langzhou He",
      "Yangning Li",
      "Yuwei Cao",
      "Dongyuan Li",
      "Renhe Jiang",
      "Philip S. Yu"
    ],
    "published": "2025-05-01T08:29:26+00:00",
    "summary": "Recent advances in large language models (LLMs) have sparked growing interest in building fully autonomous agents. However, fully autonomous LLM-based agents still face significant challenges, including limited reliability due to hallucinations, difficulty in handling complex tasks, and substantial safety and ethical risks, all of which limit their feasibility and trustworthiness in real-world applications. To overcome these limitations, LLM-based human-agent systems (LLM-HAS) incorporate human-provided information, feedback, or control into the agent system to enhance system performance, reliability and safety. This paper provides the first comprehensive and structured survey of LLM-HAS. It clarifies fundamental concepts, systematically presents core components shaping these systems, including environment & profiling, human feedback, interaction types, orchestration and communication, explores emerging applications, and discusses unique challenges and opportunities. By consolidating current knowledge and offering a structured overview, we aim to foster further research and innovation in this rapidly evolving interdisciplinary field. Paper lists and resources are available at https://github.com/HenryPengZou/Awesome-LLM-Based-Human-Agent-System-Papers."
  },
  {
    "title": "Optimizing Deep Neural Networks using Safety-Guided Self Compression",
    "url": "http://arxiv.org/abs/2505.00350v1",
    "arxiv_id": "2505.00350v1",
    "authors": [
      "Mohammad Zbeeb",
      "Mariam Salman",
      "Mohammad Bazzi",
      "Ammar Mohanna"
    ],
    "published": "2025-05-01T06:50:30+00:00",
    "summary": "The deployment of deep neural networks on resource-constrained devices necessitates effective model com- pression strategies that judiciously balance the reduction of model size with the preservation of performance. This study introduces a novel safety-driven quantization framework that leverages preservation sets to systematically prune and quantize neural network weights, thereby optimizing model complexity without compromising accuracy. The proposed methodology is rigorously evaluated on both a convolutional neural network (CNN) and an attention-based language model, demonstrating its applicability across diverse architectural paradigms. Experimental results reveal that our framework achieves up to a 2.5% enhancement in test accuracy relative to the original unquantized models while maintaining 60% of the initial model size. In comparison to conventional quantization techniques, our approach not only augments generalization by eliminating parameter noise and retaining essential weights but also reduces variance, thereby ensuring the retention of critical model features. These findings underscore the efficacy of safety-driven quantization as a robust and reliable strategy for the efficient optimization of deep learn- ing models. The implementation and comprehensive experimental evaluations of our framework are publicly accessible at GitHub."
  },
  {
    "title": "Vehicular Communication Security: Multi-Channel and Multi-Factor Authentication",
    "url": "http://arxiv.org/abs/2505.00340v1",
    "arxiv_id": "2505.00340v1",
    "authors": [
      "Marco De Vincenzi",
      "Shuyang Sun",
      "Chen Bo Calvin Zhang",
      "Manuel Garcia",
      "Shaozu Ding",
      "Chiara Bodei",
      "Ilaria Matteucci",
      "Dajiang Suo"
    ],
    "published": "2025-05-01T06:36:24+00:00",
    "summary": "Secure and reliable communications are crucial for Intelligent Transportation Systems (ITSs), where Vehicle-to-Infrastructure (V2I) communication plays a key role in enabling mobility-enhancing and safety-critical services. Current V2I authentication relies on credential-based methods over wireless Non-Line-of-Sight (NLOS) channels, leaving them exposed to remote impersonation and proximity attacks. To mitigate these risks, we propose a unified Multi-Channel, Multi-Factor Authentication (MFA) scheme that combines NLOS cryptographic credentials with a Line-of-Sight (LOS) visual channel. Our approach leverages a challenge-response security paradigm: the infrastructure issues challenges and the vehicle's headlights respond by flashing a structured sequence containing encoded security data. Deep learning models on the infrastructure side then decode the embedded information to authenticate the vehicle. Real-world experimental evaluations demonstrate high test accuracy, reaching an average of 95% and 96.6%, respectively, under various lighting, weather, speed, and distance conditions. Additionally, we conducted extensive experiments on three state-of-the-art deep learning models, including detailed ablation studies for decoding the flashing sequence. Our results indicate that the optimal architecture employs a dual-channel design, enabling simultaneous decoding of the flashing sequence and extraction of vehicle spatial and locational features for robust authentication."
  },
  {
    "title": "AI2-Active Safety: AI-enabled Interaction-aware Active Safety Analysis with Vehicle Dynamics",
    "url": "http://arxiv.org/abs/2505.00322v1",
    "arxiv_id": "2505.00322v1",
    "authors": [
      "Keshu Wu",
      "Zihao Li",
      "Sixu Li",
      "Xinyue Ye",
      "Dominique Lord",
      "Yang Zhou"
    ],
    "published": "2025-05-01T05:46:34+00:00",
    "summary": "This paper introduces an AI-enabled, interaction-aware active safety analysis framework that accounts for groupwise vehicle interactions. Specifically, the framework employs a bicycle model-augmented with road gradient considerations-to accurately capture vehicle dynamics. In parallel, a hypergraph-based AI model is developed to predict probabilistic trajectories of ambient traffic. By integrating these two components, the framework derives vehicle intra-spacing over a 3D road surface as the solution of a stochastic ordinary differential equation, yielding high-fidelity surrogate safety measures such as time-to-collision (TTC). To demonstrate its effectiveness, the framework is analyzed using stochastic numerical methods comprising 4th-order Runge-Kutta integration and AI inference, generating probability-weighted high-fidelity TTC (HF-TTC) distributions that reflect complex multi-agent maneuvers and behavioral uncertainties. Evaluated with HF-TTC against traditional constant-velocity TTC and non-interaction-aware approaches on highway datasets, the proposed framework offers a systematic methodology for active safety analysis with enhanced potential for improving safety perception in complex traffic environments."
  },
  {
    "title": "Beyond Quadratic Costs: A Bregman Divergence Approach to H$_\\infty$ Control",
    "url": "http://arxiv.org/abs/2505.00319v1",
    "arxiv_id": "2505.00319v1",
    "authors": [
      "Joudi Hajar",
      "Reza Ghane",
      "Babak Hassibi"
    ],
    "published": "2025-05-01T05:38:36+00:00",
    "summary": "This paper presents a novel extension of the H$_\\infty$ control framework that generalizes the traditional quadratic cost formulation to accommodate strictly convex, nonquadratic functions for the state, control input, and disturbance. This new formulation not only captures additional noise characteristics but also supports a range of performance objectives-including sparse control, safety constraints, and other tailored behaviors-beyond what is possible with quadratic costs. We derive a closed-form solution of a central controller that minimizes the worst-case performance ratio under the proposed cost structure. Furthermore, we develop Riccati-like equations that impose necessary and sufficient conditions on the nonquadratic cost functions, thereby ensuring the existence of a robust solution. Finally, we rigorously establish Lyapunov stability for the closed-loop system. The proposed framework bridges robust control theory with modern approaches in machine learning and signal processing, offering enhanced flexibility and improved performance in complex control scenarios."
  },
  {
    "title": "Beyond Quadratic Costs in LQR: Bregman Divergence Control",
    "url": "http://arxiv.org/abs/2505.00317v1",
    "arxiv_id": "2505.00317v1",
    "authors": [
      "Babak Hassibi",
      "Joudi Hajar",
      "Reza Ghane"
    ],
    "published": "2025-05-01T05:31:45+00:00",
    "summary": "In the past couple of decades, the use of ``non-quadratic\" convex cost functions has revolutionized signal processing, machine learning, and statistics, allowing one to customize solutions to have desired structures and properties. However, the situation is not the same in control where the use of quadratic costs still dominates, ostensibly because determining the ``value function\", i.e., the optimal expected cost-to-go, which is critical to the construction of the optimal controller, becomes computationally intractable as soon as one considers general convex costs. As a result, practitioners often resort to heuristics and approximations, such as model predictive control that only looks a few steps into the future. In the quadratic case, the value function is easily determined by solving Riccati equations. In this work, we consider a special class of convex cost functions constructed from Bregman divergence and show how, with appropriate choices, they can be used to fully extend the framework developed for the quadratic case. The resulting optimal controllers are infinite horizon, come with stability guarantees, and have state-feedback, or estimated state-feedback, laws. They exhibit a much wider range of behavior than their quadratic counterparts since the feedback laws are nonlinear. The approach can be applied to several cases of interest, including safety control, sparse control, and bang-bang control."
  },
  {
    "title": "J-PARSE: Jacobian-based Projection Algorithm for Resolving Singularities Effectively in Inverse Kinematic Control of Serial Manipulators",
    "url": "http://arxiv.org/abs/2505.00306v1",
    "arxiv_id": "2505.00306v1",
    "authors": [
      "Shivani Guptasarma",
      "Matthew Strong",
      "Honghao Zhen",
      "Monroe Kennedy III"
    ],
    "published": "2025-05-01T04:58:50+00:00",
    "summary": "J-PARSE is a method for smooth first-order inverse kinematic control of a serial manipulator near kinematic singularities. The commanded end-effector velocity is interpreted component-wise, according to the available mobility in each dimension of the task space. First, a substitute \"Safety\" Jacobian matrix is created, keeping the aspect ratio of the manipulability ellipsoid above a threshold value. The desired motion is then projected onto non-singular and singular directions, and the latter projection scaled down by a factor informed by the threshold value. A right-inverse of the non-singular Safety Jacobian is applied to the modified command. In the absence of joint limits and collisions, this ensures smooth transition into and out of low-rank poses, guaranteeing asymptotic stability for target poses within the workspace, and stability for those outside. Velocity control with J-PARSE is benchmarked against the Least-Squares and Damped Least-Squares inversions of the Jacobian, and shows high accuracy in reaching and leaving singular target poses. By expanding the available workspace of manipulators, the method finds applications in servoing, teleoperation, and learning."
  },
  {
    "title": "PSN Game: Game-theoretic Planning via a Player Selection Network",
    "url": "http://arxiv.org/abs/2505.00213v1",
    "arxiv_id": "2505.00213v1",
    "authors": [
      "Tianyu Qiu",
      "Eric Ouano",
      "Fernando Palafox",
      "Christian Ellis",
      "David Fridovich-Keil"
    ],
    "published": "2025-04-30T23:14:32+00:00",
    "summary": "While game-theoretic planning frameworks are effective at modeling multi-agent interactions, they require solving optimization problems with hundreds or thousands of variables, resulting in long computation times that limit their use in large-scale, real-time systems. To address this issue, we propose PSN Game: a novel game-theoretic planning framework that reduces runtime by learning a Player Selection Network (PSN). A PSN outputs a player selection mask that distinguishes influential players from less relevant ones, enabling the ego player to solve a smaller, masked game involving only selected players. By reducing the number of variables in the optimization problem, PSN directly lowers computation time. The PSN Game framework is more flexible than existing player selection methods as it i) relies solely on observations of players' past trajectories, without requiring full state, control, or other game-specific information; and ii) requires no online parameter tuning. We train PSNs in an unsupervised manner using a differentiable dynamic game solver, with reference trajectories from full-player games guiding the learning. Experiments in both simulated scenarios and human trajectory datasets demonstrate that i) PSNs outperform baseline selection methods in trajectory smoothness and length, while maintaining comparable safety and achieving a 10x speedup in runtime; and ii) PSNs generalize effectively to real-world scenarios without fine-tuning. By selecting only the most relevant players for decision-making, PSNs offer a general mechanism for reducing planning complexity that can be seamlessly integrated into existing multi-agent planning frameworks."
  },
  {
    "title": "Which Agent Causes Task Failures and When? On Automated Failure Attribution of LLM Multi-Agent Systems",
    "url": "http://arxiv.org/abs/2505.00212v1",
    "arxiv_id": "2505.00212v1",
    "authors": [
      "Shaokun Zhang",
      "Ming Yin",
      "Jieyu Zhang",
      "Jiale Liu",
      "Zhiguang Han",
      "Jingyang Zhang",
      "Beibin Li",
      "Chi Wang",
      "Huazheng Wang",
      "Yiran Chen",
      "Qingyun Wu"
    ],
    "published": "2025-04-30T23:09:44+00:00",
    "summary": "Failure attribution in LLM multi-agent systems-identifying the agent and step responsible for task failures-provides crucial clues for systems debugging but remains underexplored and labor-intensive. In this paper, we propose and formulate a new research area: automated failure attribution for LLM multi-agent systems. To support this initiative, we introduce the Who&When dataset, comprising extensive failure logs from 127 LLM multi-agent systems with fine-grained annotations linking failures to specific agents and decisive error steps. Using the Who&When, we develop and evaluate three automated failure attribution methods, summarizing their corresponding pros and cons. The best method achieves 53.5% accuracy in identifying failure-responsible agents but only 14.2% in pinpointing failure steps, with some methods performing below random. Even SOTA reasoning models, such as OpenAI o1 and DeepSeek R1, fail to achieve practical usability. These results highlight the task's complexity and the need for further research in this area. Code and dataset are available at https://github.com/mingyin1/Agents_Failure_Attribution"
  },
  {
    "title": "Real-World Gaps in AI Governance Research",
    "url": "http://arxiv.org/abs/2505.00174v1",
    "arxiv_id": "2505.00174v1",
    "authors": [
      "Ilan Strauss",
      "Isobel Moure",
      "Tim O'Reilly",
      "Sruly Rosenblat"
    ],
    "published": "2025-04-30T20:44:42+00:00",
    "summary": "Drawing on 1,178 safety and reliability papers from 9,439 generative AI papers (January 2020 - March 2025), we compare research outputs of leading AI companies (Anthropic, Google DeepMind, Meta, Microsoft, and OpenAI) and AI universities (CMU, MIT, NYU, Stanford, UC Berkeley, and University of Washington). We find that corporate AI research increasingly concentrates on pre-deployment areas -- model alignment and testing & evaluation -- while attention to deployment-stage issues such as model bias has waned. Significant research gaps exist in high-risk deployment domains, including healthcare, finance, misinformation, persuasive and addictive features, hallucinations, and copyright. Without improved observability into deployed AI, growing corporate concentration could deepen knowledge deficits. We recommend expanding external researcher access to deployment data and systematic observability of in-market AI behaviors."
  },
  {
    "title": "Audo-Sight: Enabling Ambient Interaction For Blind And Visually Impaired Individuals",
    "url": "http://arxiv.org/abs/2505.00153v1",
    "arxiv_id": "2505.00153v1",
    "authors": [
      "Bhanuja Ainary"
    ],
    "published": "2025-04-30T19:55:19+00:00",
    "summary": "Visually impaired people face significant challenges when attempting to interact with and understand complex environments, and traditional assistive technologies often struggle to quickly provide necessary contextual understanding and interactive intelligence. This thesis presents Audo-Sight, a state-of-the-art assistive system that seamlessly integrates Multimodal Large Language Models (MLLMs) to provide expedient, context-aware interactions for Blind and Visually Impaired (BVI) individuals. The system operates in two different modalities: personalized interaction through user identification and public access in common spaces like museums and shopping malls. In tailored environments, the system adjusts its output to conform to the preferences of individual users, thus enhancing accessibility through a user-aware form of interaction. In shared environments, Audo-Sight employs a shared architecture that adapts to its current user with no manual reconfiguration required. To facilitate appropriate interactions with the LLM, the public Audo-Sight solution includes an Age-Range Determiner and Safe Query Filter. Additionally, the system ensures that responses are respectful to BVI users through NeMo Guardrails. By utilizing multimodal reasoning, BVI-cognizant response editing, and safeguarding features, this work represents a major leap in AI-driven accessibility technology capable of increasing autonomy, safety, and interaction for people with visual impairments in social settings. Finally, we present the integration of Audo-Sight and SmartSight, which enables enhanced situational awareness for BVI individuals. This integration takes advantage of the real-time visual analysis of SmartSight, combined with the extensive reasoning and interactive capabilities of Audo-Sight, and goes beyond object identification to provide context-driven, voice-controlled assistance in dynamic environments."
  },
  {
    "title": "A Survey of Interactive Generative Video",
    "url": "http://arxiv.org/abs/2504.21853v1",
    "arxiv_id": "2504.21853v1",
    "authors": [
      "Jiwen Yu",
      "Yiran Qin",
      "Haoxuan Che",
      "Quande Liu",
      "Xintao Wang",
      "Pengfei Wan",
      "Di Zhang",
      "Kun Gai",
      "Hao Chen",
      "Xihui Liu"
    ],
    "published": "2025-04-30T17:59:02+00:00",
    "summary": "Interactive Generative Video (IGV) has emerged as a crucial technology in response to the growing demand for high-quality, interactive video content across various domains. In this paper, we define IGV as a technology that combines generative capabilities to produce diverse high-quality video content with interactive features that enable user engagement through control signals and responsive feedback. We survey the current landscape of IGV applications, focusing on three major domains: 1) gaming, where IGV enables infinite exploration in virtual worlds; 2) embodied AI, where IGV serves as a physics-aware environment synthesizer for training agents in multimodal interaction with dynamically evolving scenes; and 3) autonomous driving, where IGV provides closed-loop simulation capabilities for safety-critical testing and validation. To guide future development, we propose a comprehensive framework that decomposes an ideal IGV system into five essential modules: Generation, Control, Memory, Dynamics, and Intelligence. Furthermore, we systematically analyze the technical challenges and future directions in realizing each component for an ideal IGV system, such as achieving real-time generation, enabling open-domain control, maintaining long-term coherence, simulating accurate physics, and integrating causal reasoning. We believe that this systematic analysis will facilitate future research and development in the field of IGV, ultimately advancing the technology toward more sophisticated and practical applications."
  },
  {
    "title": "Neuro-Symbolic Generation of Explanations for Robot Policies with Weighted Signal Temporal Logic",
    "url": "http://arxiv.org/abs/2504.21841v1",
    "arxiv_id": "2504.21841v1",
    "authors": [
      "Mikihisa Yuasa",
      "Ramavarapu S. Sreenivas",
      "Huy T. Tran"
    ],
    "published": "2025-04-30T17:51:20+00:00",
    "summary": "Neural network-based policies have demonstrated success in many robotic applications, but often lack human-explanability, which poses challenges in safety-critical deployments. To address this, we propose a neuro-symbolic explanation framework that generates a weighted signal temporal logic (wSTL) specification to describe a robot policy in a interpretable form. Existing methods typically produce explanations that are verbose and inconsistent, which hinders explainability, and loose, which do not give meaningful insights into the underlying policy. We address these issues by introducing a simplification process consisting of predicate filtering, regularization, and iterative pruning. We also introduce three novel explainability evaluation metrics -- conciseness, consistency, and strictness -- to assess explanation quality beyond conventional classification metrics. Our method is validated in three simulated robotic environments, where it outperforms baselines in generating concise, consistent, and strict wSTL explanations without sacrificing classification accuracy. This work bridges policy learning with formal methods, contributing to safer and more transparent decision-making in robotics."
  },
  {
    "title": "WebThinker: Empowering Large Reasoning Models with Deep Research Capability",
    "url": "http://arxiv.org/abs/2504.21776v1",
    "arxiv_id": "2504.21776v1",
    "authors": [
      "Xiaoxi Li",
      "Jiajie Jin",
      "Guanting Dong",
      "Hongjin Qian",
      "Yutao Zhu",
      "Yongkang Wu",
      "Ji-Rong Wen",
      "Zhicheng Dou"
    ],
    "published": "2025-04-30T16:25:25+00:00",
    "summary": "Large reasoning models (LRMs), such as OpenAI-o1 and DeepSeek-R1, demonstrate impressive long-horizon reasoning capabilities. However, their reliance on static internal knowledge limits their performance on complex, knowledge-intensive tasks and hinders their ability to produce comprehensive research reports requiring synthesis of diverse web information. To address this, we propose \\textbf{WebThinker}, a deep research agent that empowers LRMs to autonomously search the web, navigate web pages, and draft research reports during the reasoning process. WebThinker integrates a \\textbf{Deep Web Explorer} module, enabling LRMs to dynamically search, navigate, and extract information from the web when encountering knowledge gaps. It also employs an \\textbf{Autonomous Think-Search-and-Draft strategy}, allowing the model to seamlessly interleave reasoning, information gathering, and report writing in real time. To further enhance research tool utilization, we introduce an \\textbf{RL-based training strategy} via iterative online Direct Preference Optimization (DPO). Extensive experiments on complex reasoning benchmarks (GPQA, GAIA, WebWalkerQA, HLE) and scientific report generation tasks (Glaive) demonstrate that WebThinker significantly outperforms existing methods and strong proprietary systems. Our approach enhances LRM reliability and applicability in complex scenarios, paving the way for more capable and versatile deep research systems. The code is available at https://github.com/RUC-NLPIR/WebThinker."
  },
  {
    "title": "CodeFlowBench: A Multi-turn, Iterative Benchmark for Complex Code Generation",
    "url": "http://arxiv.org/abs/2504.21751v1",
    "arxiv_id": "2504.21751v1",
    "authors": [
      "Sizhe Wang",
      "Zhengren Wang",
      "Dongsheng Ma",
      "Yongan Yu",
      "Rui Ling",
      "Zhiyu Li",
      "Feiyu Xiong",
      "Wentao Zhang"
    ],
    "published": "2025-04-30T15:45:28+00:00",
    "summary": "Real world development demands code that is readable, extensible, and testable by organizing the implementation into modular components and iteratively reuse pre-implemented code. We term this iterative, multi-turn process codeflow and introduce CodeFlowBench, the first benchmark designed for comprehensively evaluating LLMs' ability to perform codeflow, namely to implement new functionality by reusing existing functions over multiple turns. CodeFlowBench comprises 5258 problems drawn from Codeforces and is continuously updated via an automated pipeline that decomposes each problem into a series of function-level subproblems based on its dependency tree and each subproblem is paired with unit tests. We further propose a novel evaluation framework with tasks and metrics tailored to multi-turn code reuse to assess model performance. In experiments across various LLMs under both multi-turn and single-turn patterns. We observe models' poor performance on CodeFlowBench, with a substantial performance drop in the iterative codeflow scenario. For instance, o1-mini achieves a pass@1 of 20.8% in multi-turn pattern versus 37.8% in single-turn pattern. Further analysis shows that different models excel at different dependency depths, yet all struggle to correctly solve structurally complex problems, highlighting challenges for current LLMs to serve as code generation tools when performing codeflow. Overall, CodeFlowBench offers a comprehensive benchmark and new insights into LLM capabilities for multi-turn, iterative code generation, guiding future advances in code generation tasks."
  },
  {
    "title": "XBreaking: Explainable Artificial Intelligence for Jailbreaking LLMs",
    "url": "http://arxiv.org/abs/2504.21700v1",
    "arxiv_id": "2504.21700v1",
    "authors": [
      "Marco Arazzi",
      "Vignesh Kumar Kembu",
      "Antonino Nocera",
      "Vinod P"
    ],
    "published": "2025-04-30T14:44:24+00:00",
    "summary": "Large Language Models are fundamental actors in the modern IT landscape dominated by AI solutions. However, security threats associated with them might prevent their reliable adoption in critical application scenarios such as government organizations and medical institutions. For this reason, commercial LLMs typically undergo a sophisticated censoring mechanism to eliminate any harmful output they could possibly produce. In response to this, LLM Jailbreaking is a significant threat to such protections, and many previous approaches have already demonstrated its effectiveness across diverse domains. Existing jailbreak proposals mostly adopt a generate-and-test strategy to craft malicious input. To improve the comprehension of censoring mechanisms and design a targeted jailbreak attack, we propose an Explainable-AI solution that comparatively analyzes the behavior of censored and uncensored models to derive unique exploitable alignment patterns. Then, we propose XBreaking, a novel jailbreak attack that exploits these unique patterns to break the security constraints of LLMs by targeted noise injection. Our thorough experimental campaign returns important insights about the censoring mechanisms and demonstrates the effectiveness and performance of our attack."
  },
  {
    "title": "REHEARSE-3D: A Multi-modal Emulated Rain Dataset for 3D Point Cloud De-raining",
    "url": "http://arxiv.org/abs/2504.21699v1",
    "arxiv_id": "2504.21699v1",
    "authors": [
      "Abu Mohammed Raisuddin",
      "Jesper Holmblad",
      "Hamed Haghighi",
      "Yuri Poledna",
      "Maikol Funk Drechsler",
      "Valentina Donzella",
      "Eren Erdal Aksoy"
    ],
    "published": "2025-04-30T14:43:38+00:00",
    "summary": "Sensor degradation poses a significant challenge in autonomous driving. During heavy rainfall, the interference from raindrops can adversely affect the quality of LiDAR point clouds, resulting in, for instance, inaccurate point measurements. This, in turn, can potentially lead to safety concerns if autonomous driving systems are not weather-aware, i.e., if they are unable to discern such changes. In this study, we release a new, large-scale, multi-modal emulated rain dataset, REHEARSE-3D, to promote research advancements in 3D point cloud de-raining. Distinct from the most relevant competitors, our dataset is unique in several respects. First, it is the largest point-wise annotated dataset, and second, it is the only one with high-resolution LiDAR data (LiDAR-256) enriched with 4D Radar point clouds logged in both daytime and nighttime conditions in a controlled weather environment. Furthermore, REHEARSE-3D involves rain-characteristic information, which is of significant value not only for sensor noise modeling but also for analyzing the impact of weather at a point level. Leveraging REHEARSE-3D, we benchmark raindrop detection and removal in fused LiDAR and 4D Radar point clouds. Our comprehensive study further evaluates the performance of various statistical and deep-learning models. Upon publication, the dataset and benchmark models will be made publicly available at: https://sporsho.github.io/REHEARSE3D."
  },
  {
    "title": "Using quantum annealing to generate test cases for cyber-physical systems",
    "url": "http://arxiv.org/abs/2504.21684v1",
    "arxiv_id": "2504.21684v1",
    "authors": [
      "Hugo Araujo",
      "Xinyi Wang",
      "Mohammad Mousavi",
      "Shaukat Ali"
    ],
    "published": "2025-04-30T14:20:58+00:00",
    "summary": "Quantum computing has emerged as a powerful tool to efficiently solve computational challenges, particularly in simulation and optimisation. However, hardware limitations prevent quantum computers from achieving the full theoretical potential. Among the quantum algorithms, quantum annealing is a prime candidate to solve optimisation problems. This makes it a natural candidate for search-based software testing in the Cyber-Physical Systems (CPS) domain, which demands effective test cases due to their safety-critical nature. This work explores the use of quantum annealing to enhance test case generation for CPS through a mutation-based approach. We encode test case mutation as a binary optimisation problem, and use quantum annealing to identify and target critical regions of the test cases for improvement. Our approach mechanises this process into an algorithm that uses D-Wave's quantum annealer to find the solution. As a main contribution, we offer insights into how quantum annealing can advance software testing methodologies by empirically evaluating the correlation between problem size, hardware limitations, and the effectiveness of the results. Moreover, we compare the proposed method against state-of-the-art classical optimisation algorithms, targeting efficiency (time to generate test cases) and effectiveness (fault detection rates). Results indicate that quantum annealing enables faster test case generation while achieving comparable fault detection performance to state-of-the-art alternatives."
  },
  {
    "title": "Hoist with His Own Petard: Inducing Guardrails to Facilitate Denial-of-Service Attacks on Retrieval-Augmented Generation of LLMs",
    "url": "http://arxiv.org/abs/2504.21680v1",
    "arxiv_id": "2504.21680v1",
    "authors": [
      "Pan Suo",
      "Yu-Ming Shang",
      "San-Chuan Guo",
      "Xi Zhang"
    ],
    "published": "2025-04-30T14:18:11+00:00",
    "summary": "Retrieval-Augmented Generation (RAG) integrates Large Language Models (LLMs) with external knowledge bases, improving output quality while introducing new security risks. Existing studies on RAG vulnerabilities typically focus on exploiting the retrieval mechanism to inject erroneous knowledge or malicious texts, inducing incorrect outputs. However, these approaches overlook critical weaknesses within LLMs, leaving important attack vectors unexplored and limiting the scope and efficiency of attacks. In this paper, we uncover a novel vulnerability: the safety guardrails of LLMs, while designed for protection, can also be exploited as an attack vector by adversaries. Building on this vulnerability, we propose MutedRAG, a novel denial-of-service attack that reversely leverages the guardrails of LLMs to undermine the availability of RAG systems. By injecting minimalistic jailbreak texts, such as \"\\textit{How to build a bomb}\", into the knowledge base, MutedRAG intentionally triggers the LLM's safety guardrails, causing the system to reject legitimate queries. Besides, due to the high sensitivity of guardrails, a single jailbreak sample can affect multiple queries, effectively amplifying the efficiency of attacks while reducing their costs. Experimental results on three datasets demonstrate that MutedRAG achieves an attack success rate exceeding 60% in many scenarios, requiring only less than one malicious text to each target query on average. In addition, we evaluate potential defense strategies against MutedRAG, finding that some of current mechanisms are insufficient to mitigate this threat, underscoring the urgent need for more robust solutions."
  },
  {
    "title": "RDF-Based Structured Quality Assessment Representation of Multilingual LLM Evaluations",
    "url": "http://arxiv.org/abs/2504.21605v1",
    "arxiv_id": "2504.21605v1",
    "authors": [
      "Jonas Gwozdz",
      "Andreas Both"
    ],
    "published": "2025-04-30T13:06:40+00:00",
    "summary": "Large Language Models (LLMs) increasingly serve as knowledge interfaces, yet systematically assessing their reliability with conflicting information remains difficult. We propose an RDF-based framework to assess multilingual LLM quality, focusing on knowledge conflicts. Our approach captures model responses across four distinct context conditions (complete, incomplete, conflicting, and no-context information) in German and English. This structured representation enables the comprehensive analysis of knowledge leakage-where models favor training data over provided context-error detection, and multilingual consistency. We demonstrate the framework through a fire safety domain experiment, revealing critical patterns in context prioritization and language-specific performance, and demonstrating that our vocabulary was sufficient to express every assessment facet encountered in the 28-question study."
  },
  {
    "title": "Toward Realization of Low-Altitude Economy Networks: Core Architecture, Integrated Technologies, and Future Directions",
    "url": "http://arxiv.org/abs/2504.21583v1",
    "arxiv_id": "2504.21583v1",
    "authors": [
      "Yixian Wang",
      "Geng Sun",
      "Zemin Sun",
      "Jiacheng Wang",
      "Jiahui Li",
      "Changyuan Zhao",
      "Jing Wu",
      "Shuang Liang",
      "Minghao Yin",
      "Pengfei Wang",
      "Dusit Niyato",
      "Sumei Sun",
      "Dong In Kim"
    ],
    "published": "2025-04-30T12:42:03+00:00",
    "summary": "The rise of the low-altitude economy (LAE) is propelling urban development and emerging industries by integrating advanced technologies to enhance efficiency, safety, and sustainability in low-altitude operations. The widespread adoption of unmanned aerial vehicles (UAVs) and electric vertical takeoff and landing (eVTOL) aircraft plays a crucial role in enabling key applications within LAE, such as urban logistics, emergency rescue, and aerial mobility. However, unlike traditional UAV networks, LAE networks encounter increased airspace management demands due to dense flying nodes and potential interference with ground communication systems. In addition, there are heightened and extended security risks in real-time operations, particularly the vulnerability of low-altitude aircraft to cyberattacks from ground-based threats. To address these, this paper first explores related standards and core architecture that support the development of LAE networks. Subsequently, we highlight the integration of technologies such as communication, sensing, computing, positioning, navigation, surveillance, flight control, and airspace management. This synergy of multi-technology drives the advancement of real-world LAE applications, particularly in improving operational efficiency, optimizing airspace usage, and ensuring safety. Finally, we outline future research directions for LAE networks, such as intelligent and adaptive optimization, security and privacy protection, sustainable energy and power management, quantum-driven coordination, generative governance, and three-dimensional (3D) airspace coverage, which collectively underscore the potential of collaborative technologies to advance LAE networks."
  },
  {
    "title": "Provably-Safe, Online System Identification",
    "url": "http://arxiv.org/abs/2504.21486v1",
    "arxiv_id": "2504.21486v1",
    "authors": [
      "Bohao Zhang",
      "Zichang Zhou",
      "Ram Vasudevan"
    ],
    "published": "2025-04-30T10:10:32+00:00",
    "summary": "Precise manipulation tasks require accurate knowledge of payload inertial parameters. Unfortunately, identifying these parameters for unknown payloads while ensuring that the robotic system satisfies its input and state constraints while avoiding collisions with the environment remains a significant challenge. This paper presents an integrated framework that enables robotic manipulators to safely and automatically identify payload parameters while maintaining operational safety guarantees. The framework consists of two synergistic components: an online trajectory planning and control framework that generates provably-safe exciting trajectories for system identification that can be tracked while respecting robot constraints and avoiding obstacles and a robust system identification method that computes rigorous overapproximative bounds on end-effector inertial parameters assuming bounded sensor noise. Experimental validation on a robotic manipulator performing challenging tasks with various unknown payloads demonstrates the framework's effectiveness in establishing accurate parameter bounds while maintaining safety throughout the identification process. The code is available at our project webpage: https://roahmlab.github.io/OnlineSafeSysID/."
  },
  {
    "title": "An Intermediate Program Representation for Optimizing Stream-Based Languages",
    "url": "http://arxiv.org/abs/2504.21458v1",
    "arxiv_id": "2504.21458v1",
    "authors": [
      "Jan Baumeister",
      "Arthur Correnson",
      "Bernd Finkbeiner",
      "Frederik Scheerer"
    ],
    "published": "2025-04-30T09:24:53+00:00",
    "summary": "Stream-based runtime monitors are safety assurance tools that check at runtime whether the system's behavior satisfies a formal specification. Specifications consist of stream equations, which relate input streams, containing sensor readings and other incoming information, to output streams, representing filtered and aggregated data. This paper presents a framework for the stream-based specification language RTLola. We introduce a new intermediate representation for stream-based languages, the StreamIR, which, like the specification language, operates on streams of unbounded length; while the stream equations are replaced by imperative programs. We developed a set of optimizations based on static analysis of the specification and have implemented an interpreter and a compiler for several target languages. In our evaluation, we measure the performance of several real-world case studies. The results show that using the StreamIR framework reduces the runtime significantly compared to the existing StreamIR interpreter. We evaluate the effect of the optimizations and show that significant performance gains are possible beyond the optimizations of the target language's compiler. While our current implementation is limited to RTLola, the StreamIR is designed to accommodate other stream-based languages, enabling their interpretation and compilation into all available target languages."
  },
  {
    "title": "Mapping the Human Brain from the Prenatal Period to Infancy Using 3D Magnetic Resonance Imaging",
    "url": "http://arxiv.org/abs/2504.21406v1",
    "arxiv_id": "2504.21406v1",
    "authors": [
      "Arnaud Cachia",
      "Jean-Fran\u00e7ois Mangin",
      "Jessica Dubois"
    ],
    "published": "2025-04-30T08:05:02+00:00",
    "summary": "Human brain development is a complex and dynamic process that begins during the first weeks of pregnancy and lasts until early adulthood. This chapter focuses on the developmental window from prenatal period to infancy, probably the most dynamic period across the entire lifespan. The availability of non-invasive three-dimensional Magnetic Resonance Imaging (MRI) methodologies has changed the paradigm and allows investigations of the living human brain structure - e.g. micro- and macrostructural features of cortical and subcortical regions and their connections, including cortical sulcation/gyrification, area, and thickness, as well as white matter microstructure and connectivity - beginning in utero. Because of its relative safety, MRI is well-adapted to study individuals at multiple time points and to longitudinally follow the changes in brain structure and function that underlie the early stages of cognitive development."
  },
  {
    "title": "Retrieval-Enhanced Few-Shot Prompting for Speech Event Extraction",
    "url": "http://arxiv.org/abs/2504.21372v1",
    "arxiv_id": "2504.21372v1",
    "authors": [
      "M\u00e1t\u00e9 Gedeon"
    ],
    "published": "2025-04-30T07:10:10+00:00",
    "summary": "Speech Event Extraction (SpeechEE) is a challenging task that lies at the intersection of Automatic Speech Recognition (ASR) and Natural Language Processing (NLP), requiring the identification of structured event information from spoken language. In this work, we present a modular, pipeline-based SpeechEE framework that integrates high-performance ASR with semantic search-enhanced prompting of Large Language Models (LLMs). Our system first classifies speech segments likely to contain events using a hybrid filtering mechanism including rule-based, BERT-based, and LLM-based models. It then employs few-shot LLM prompting, dynamically enriched via semantic similarity retrieval, to identify event triggers and extract corresponding arguments. We evaluate the pipeline using multiple LLMs (Llama3-8B, GPT-4o-mini, and o1-mini) highlighting significant performance gains with o1-mini, which achieves 63.3% F1 on trigger classification and 27.8% F1 on argument classification, outperforming prior benchmarks. Our results demonstrate that pipeline approaches, when empowered by retrieval-augmented LLMs, can rival or exceed end-to-end systems while maintaining interpretability and modularity. This work provides practical insights into LLM-driven event extraction and opens pathways for future hybrid models combining textual and acoustic features."
  },
  {
    "title": "ShorterBetter: Guiding Reasoning Models to Find Optimal Inference Length for Efficient Reasoning",
    "url": "http://arxiv.org/abs/2504.21370v1",
    "arxiv_id": "2504.21370v1",
    "authors": [
      "Jingyang Yi",
      "Jiazheng Wang"
    ],
    "published": "2025-04-30T07:04:19+00:00",
    "summary": "Reasoning models such as OpenAI o3 and DeepSeek-R1 have demonstrated strong performance on reasoning-intensive tasks through extended Chain-of-Thought (CoT) prompting. While longer reasoning traces can facilitate a more thorough exploration of solution paths for complex problems, researchers have observed that these models often \"overthink\", leading to inefficient inference. In this paper, we introduce ShorterBetter, a simple yet effective reinforcement learning methed that enables reasoning language models to discover their own optimal CoT lengths without human intervention. By sampling multiple outputs per problem and defining the Sample Optimal Length (SOL) as the shortest correct response among all the outputs, our method dynamically guides the model toward optimal inference lengths. Applied to the DeepSeek-Distill-Qwen-1.5B model, ShorterBetter achieves up to an 80% reduction in output length on both in-domain and out-of-domain reasoning tasks while maintaining accuracy. Our analysis shows that overly long reasoning traces often reflect loss of reasoning direction, and thus suggests that the extended CoT produced by reasoning models is highly compressible."
  },
  {
    "title": "Simple Visual Artifact Detection in Sora-Generated Videos",
    "url": "http://arxiv.org/abs/2504.21334v1",
    "arxiv_id": "2504.21334v1",
    "authors": [
      "Misora Sugiyama",
      "Hirokatsu Kataoka"
    ],
    "published": "2025-04-30T05:41:43+00:00",
    "summary": "The December 2024 release of OpenAI's Sora, a powerful video generation model driven by natural language prompts, highlights a growing convergence between large language models (LLMs) and video synthesis. As these multimodal systems evolve into video-enabled LLMs (VidLLMs), capable of interpreting, generating, and interacting with visual content, understanding their limitations and ensuring their safe deployment becomes essential. This study investigates visual artifacts frequently found and reported in Sora-generated videos, which can compromise quality, mislead viewers, or propagate disinformation. We propose a multi-label classification framework targeting four common artifact label types: label 1: boundary / edge defects, label 2: texture / noise issues, label 3: movement / joint anomalies, and label 4: object mismatches / disappearances. Using a dataset of 300 manually annotated frames extracted from 15 Sora-generated videos, we trained multiple 2D CNN architectures (ResNet-50, EfficientNet-B3 / B4, ViT-Base). The best-performing model trained by ResNet-50 achieved an average multi-label classification accuracy of 94.14%. This work supports the broader development of VidLLMs by contributing to (1) the creation of datasets for video quality evaluation, (2) interpretable artifact-based analysis beyond language metrics, and (3) the identification of visual risks relevant to factuality and safety."
  },
  {
    "title": "Phi-4-reasoning Technical Report",
    "url": "http://arxiv.org/abs/2504.21318v1",
    "arxiv_id": "2504.21318v1",
    "authors": [
      "Marah Abdin",
      "Sahaj Agarwal",
      "Ahmed Awadallah",
      "Vidhisha Balachandran",
      "Harkirat Behl",
      "Lingjiao Chen",
      "Gustavo de Rosa",
      "Suriya Gunasekar",
      "Mojan Javaheripi",
      "Neel Joshi",
      "Piero Kauffmann",
      "Yash Lara",
      "Caio C\u00e9sar Teodoro Mendes",
      "Arindam Mitra",
      "Besmira Nushi",
      "Dimitris Papailiopoulos",
      "Olli Saarikivi",
      "Shital Shah",
      "Vaishnavi Shrivastava",
      "Vibhav Vineet",
      "Yue Wu",
      "Safoora Yousefi",
      "Guoqing Zheng"
    ],
    "published": "2025-04-30T05:05:09+00:00",
    "summary": "We introduce Phi-4-reasoning, a 14-billion parameter reasoning model that achieves strong performance on complex reasoning tasks. Trained via supervised fine-tuning of Phi-4 on carefully curated set of \"teachable\" prompts-selected for the right level of complexity and diversity-and reasoning demonstrations generated using o3-mini, Phi-4-reasoning generates detailed reasoning chains that effectively leverage inference-time compute. We further develop Phi-4-reasoning-plus, a variant enhanced through a short phase of outcome-based reinforcement learning that offers higher performance by generating longer reasoning traces. Across a wide range of reasoning tasks, both models outperform significantly larger open-weight models such as DeepSeek-R1-Distill-Llama-70B model and approach the performance levels of full DeepSeek-R1 model. Our comprehensive evaluations span benchmarks in math and scientific reasoning, coding, algorithmic problem solving, planning, and spatial understanding. Interestingly, we observe a non-trivial transfer of improvements to general-purpose benchmarks as well. In this report, we provide insights into our training data, our training methodologies, and our evaluations. We show that the benefit of careful data curation for supervised fine-tuning (SFT) extends to reasoning language models, and can be further amplified by reinforcement learning (RL). Finally, our evaluation points to opportunities for improving how we assess the performance and robustness of reasoning models."
  },
  {
    "title": "Annotating and Auditing the Safety Properties of Unsafe Rust",
    "url": "http://arxiv.org/abs/2504.21312v1",
    "arxiv_id": "2504.21312v1",
    "authors": [
      "Zihao Rao",
      "Hongliang Tian",
      "Xin Wang",
      "Hui Xu"
    ],
    "published": "2025-04-30T04:53:35+00:00",
    "summary": "Unsafe code is a critical topic in ensuring the security of system software development in Rust. It is the sole source of potential undefined behaviors, assuming the compiler is sound. To avoid the misuse of unsafe code, Rust developers should provide clear safety property annotations for unsafe APIs. However, there is limited official guidance and few best practices for annotating unsafe code. Even the current best practices for safety property annotations in the Rust standard library are ad hoc and informal. In this paper, we design a domain-specific language to describe the safety properties of unsafe APIs, which may serve as a precursor for automated verification in the future. Furthermore, to ensure that the caller of an unsafe API properly delegates the safety property required by the callee, we propose a novel unsafety propagation graph to model the usage and propagation of unsafe code. Based on this graph, we further introduce a method to partition the graph into smaller graphs, such that each graph serves as a self-contained audit unit for examining the soundness of unsafe code encapsulation and safety property annotation. We applied our approach to the Rust standard library, and the experimental results demonstrate that our method is both practical and effective. Additionally, we have fixed safety property description issues in 23 APIs."
  },
  {
    "title": "The Dual Power of Interpretable Token Embeddings: Jailbreaking Attacks and Defenses for Diffusion Model Unlearning",
    "url": "http://arxiv.org/abs/2504.21307v1",
    "arxiv_id": "2504.21307v1",
    "authors": [
      "Siyi Chen",
      "Yimeng Zhang",
      "Sijia Liu",
      "Qing Qu"
    ],
    "published": "2025-04-30T04:33:43+00:00",
    "summary": "Despite the remarkable generalization capabilities of diffusion models, recent studies have shown that these models can memorize and generate harmful content when prompted with specific text instructions. Although fine-tuning approaches have been developed to mitigate this issue by unlearning harmful concepts, these methods can be easily circumvented through jailbreaking attacks. This indicates that the harmful concept has not been fully erased from the model. However, existing attack methods, while effective, lack interpretability regarding why unlearned models still retain the concept, thereby hindering the development of defense strategies. In this work, we address these limitations by proposing an attack method that learns an orthogonal set of interpretable attack token embeddings. The attack token embeddings can be decomposed into human-interpretable textual elements, revealing that unlearned models still retain the target concept through implicit textual components. Furthermore, these attack token embeddings are robust and transferable across text prompts, initial noises, and unlearned models. Finally, leveraging this diverse set of embeddings, we design a defense method applicable to both our proposed attack and existing attack methods. Experimental results demonstrate the effectiveness of both our attack and defense strategies."
  },
  {
    "title": "Assessing LLM code generation quality through path planning tasks",
    "url": "http://arxiv.org/abs/2504.21276v1",
    "arxiv_id": "2504.21276v1",
    "authors": [
      "Wanyi Chen",
      "Meng-Wen Su",
      "Mary L. Cummings"
    ],
    "published": "2025-04-30T03:11:54+00:00",
    "summary": "As LLM-generated code grows in popularity, more evaluation is needed to assess the risks of using such tools, especially for safety-critical applications such as path planning. Existing coding benchmarks are insufficient as they do not reflect the context and complexity of safety-critical applications. To this end, we assessed six LLMs' abilities to generate the code for three different path-planning algorithms and tested them on three maps of various difficulties. Our results suggest that LLM-generated code presents serious hazards for path planning applications and should not be applied in safety-critical contexts without rigorous testing."
  },
  {
    "title": "Phi-4-Mini-Reasoning: Exploring the Limits of Small Reasoning Language Models in Math",
    "url": "http://arxiv.org/abs/2504.21233v1",
    "arxiv_id": "2504.21233v1",
    "authors": [
      "Haoran Xu",
      "Baolin Peng",
      "Hany Awadalla",
      "Dongdong Chen",
      "Yen-Chun Chen",
      "Mei Gao",
      "Young Jin Kim",
      "Yunsheng Li",
      "Liliang Ren",
      "Yelong Shen",
      "Shuohang Wang",
      "Weijian Xu",
      "Jianfeng Gao",
      "Weizhu Chen"
    ],
    "published": "2025-04-30T00:04:35+00:00",
    "summary": "Chain-of-Thought (CoT) significantly enhances formal reasoning capabilities in Large Language Models (LLMs) by training them to explicitly generate intermediate reasoning steps. While LLMs readily benefit from such techniques, improving reasoning in Small Language Models (SLMs) remains challenging due to their limited model capacity. Recent work by Deepseek-R1 demonstrates that distillation from LLM-generated synthetic data can substantially improve the reasoning ability of SLM. However, the detailed modeling recipe is not disclosed. In this work, we present a systematic training recipe for SLMs that consists of four steps: (1) large-scale mid-training on diverse distilled long-CoT data, (2) supervised fine-tuning on high-quality long-CoT data, (3) Rollout DPO leveraging a carefully curated preference dataset, and (4) Reinforcement Learning (RL) with Verifiable Reward. We apply our method on Phi-4-Mini, a compact 3.8B-parameter model. The resulting Phi-4-Mini-Reasoning model exceeds, on math reasoning tasks, much larger reasoning models, e.g., outperforming DeepSeek-R1-Distill-Qwen-7B by 3.2 points and DeepSeek-R1-Distill-Llama-8B by 7.7 points on Math-500. Our results validate that a carefully designed training recipe, with large-scale high-quality CoT data, is effective to unlock strong reasoning capabilities even in resource-constrained small models."
  },
  {
    "title": "Automatic Legal Writing Evaluation of LLMs",
    "url": "http://arxiv.org/abs/2504.21202v1",
    "arxiv_id": "2504.21202v1",
    "authors": [
      "Ramon Pires",
      "Roseval Malaquias Junior",
      "Rodrigo Nogueira"
    ],
    "published": "2025-04-29T22:16:39+00:00",
    "summary": "Despite the recent advances in Large Language Models, benchmarks for evaluating legal writing remain scarce due to the inherent complexity of assessing open-ended responses in this domain. One of the key challenges in evaluating language models on domain-specific tasks is finding test datasets that are public, frequently updated, and contain comprehensive evaluation guidelines. The Brazilian Bar Examination meets these requirements. We introduce oab-bench, a benchmark comprising 105 questions across seven areas of law from recent editions of the exam. The benchmark includes comprehensive evaluation guidelines and reference materials used by human examiners to ensure consistent grading. We evaluate the performance of four LLMs on oab-bench, finding that Claude-3.5 Sonnet achieves the best results with an average score of 7.93 out of 10, passing all 21 exams. We also investigated whether LLMs can serve as reliable automated judges for evaluating legal writing. Our experiments show that frontier models like OpenAI's o1 achieve a strong correlation with human scores when evaluating approved exams, suggesting their potential as reliable automated evaluators despite the inherently subjective nature of legal writing assessment. The source code and the benchmark -- containing questions, evaluation guidelines, model-generated responses, and their respective automated evaluations -- are publicly available."
  },
  {
    "title": "Graph Synthetic Out-of-Distribution Exposure with Large Language Models",
    "url": "http://arxiv.org/abs/2504.21198v1",
    "arxiv_id": "2504.21198v1",
    "authors": [
      "Haoyan Xu",
      "Zhengtao Yao",
      "Ziyi Wang",
      "Zhan Cheng",
      "Xiyang Hu",
      "Mengyuan Li",
      "Yue Zhao"
    ],
    "published": "2025-04-29T22:04:30+00:00",
    "summary": "Out-of-distribution (OOD) detection in graphs is critical for ensuring model robustness in open-world and safety-sensitive applications. Existing approaches to graph OOD detection typically involve training an in-distribution (ID) classifier using only ID data, followed by the application of post-hoc OOD scoring techniques. Although OOD exposure - introducing auxiliary OOD samples during training - has proven to be an effective strategy for enhancing detection performance, current methods in the graph domain generally assume access to a set of real OOD nodes. This assumption, however, is often impractical due to the difficulty and cost of acquiring representative OOD samples. In this paper, we introduce GOE-LLM, a novel framework that leverages Large Language Models (LLMs) for OOD exposure in graph OOD detection without requiring real OOD nodes. GOE-LLM introduces two pipelines: (1) identifying pseudo-OOD nodes from the initially unlabeled graph using zero-shot LLM annotations, and (2) generating semantically informative synthetic OOD nodes via LLM-prompted text generation. These pseudo-OOD nodes are then used to regularize the training of the ID classifier for improved OOD awareness. We evaluate our approach across multiple benchmark datasets, showing that GOE-LLM significantly outperforms state-of-the-art graph OOD detection methods that do not use OOD exposure and achieves comparable performance to those relying on real OOD data."
  },
  {
    "title": "GLIP-OOD: Zero-Shot Graph OOD Detection with Foundation Model",
    "url": "http://arxiv.org/abs/2504.21186v1",
    "arxiv_id": "2504.21186v1",
    "authors": [
      "Haoyan Xu",
      "Zhengtao Yao",
      "Xuzhi Zhang",
      "Ziyi Wang",
      "Langzhou He",
      "Yushun Dong",
      "Philip S. Yu",
      "Mengyuan Li",
      "Yue Zhao"
    ],
    "published": "2025-04-29T21:42:54+00:00",
    "summary": "Out-of-distribution (OOD) detection is critical for ensuring the safety and reliability of machine learning systems, particularly in dynamic and open-world environments. In the vision and text domains, zero-shot OOD detection - which requires no training on in-distribution (ID) data - has made significant progress through the use of large-scale pretrained models such as vision-language models (VLMs) and large language models (LLMs). However, zero-shot OOD detection in graph-structured data remains largely unexplored, primarily due to the challenges posed by complex relational structures and the absence of powerful, large-scale pretrained models for graphs. In this work, we take the first step toward enabling zero-shot graph OOD detection by leveraging a graph foundation model (GFM). We show that, when provided only with class label names, the GFM can perform OOD detection without any node-level supervision - outperforming existing supervised methods across multiple datasets. To address the more practical setting where OOD label names are unavailable, we introduce GLIP-OOD, a novel framework that employs LLMs to generate semantically informative pseudo-OOD labels from unlabeled data. These labels enable the GFM to capture nuanced semantic boundaries between ID and OOD classes and perform fine-grained OOD detection - without requiring any labeled nodes. Our approach is the first to enable node-level graph OOD detection in a fully zero-shot setting, and achieves state-of-the-art performance on four benchmark text-attributed graph datasets."
  },
  {
    "title": "Composite Safety Potential Field for Highway Driving Risk Assessment",
    "url": "http://arxiv.org/abs/2504.21158v1",
    "arxiv_id": "2504.21158v1",
    "authors": [
      "Dachuan Zuo",
      "Zilin Bian",
      "Fan Zuo",
      "Kaan Ozbay"
    ],
    "published": "2025-04-29T20:19:47+00:00",
    "summary": "In the era of rapid advancements in vehicle safety technologies, driving risk assessment has become a focal point of attention. Technologies such as collision warning systems, advanced driver assistance systems (ADAS), and autonomous driving require driving risks to be evaluated proactively and in real time. To be effective, driving risk assessment metrics must not only accurately identify potential collisions but also exhibit human-like reasoning to enable safe and seamless interactions between vehicles. Existing safety potential field models assess driving risks by considering both objective and subjective safety factors. However, their practical applicability in real-world risk assessment tasks is limited. These models are often challenging to calibrate due to the arbitrary nature of their structures, and calibration can be inefficient because of the scarcity of accident statistics. Additionally, they struggle to generalize across both longitudinal and lateral risks. To address these challenges, we propose a composite safety potential field framework, namely C-SPF, involving a subjective field to capture drivers' risk perception about spatial proximity and an objective field to quantify the imminent collision probability, to comprehensively evaluate driving risks. The C-SPF is calibrated using abundant two-dimensional spacing data from trajectory datasets, enabling it to effectively capture drivers' proximity risk perception and provide a more realistic explanation of driving behaviors. Analysis of a naturalistic driving dataset demonstrates that the C-SPF can capture both longitudinal and lateral risks that trigger drivers' safety maneuvers. Further case studies highlight the C-SPF's ability to explain lateral driver behaviors, such as abandoning lane changes or adjusting lateral position relative to adjacent vehicles, which are capabilities that existing models fail to achieve."
  },
  {
    "title": "Relaxed Choices in Bottom-Up Asynchronous Multiparty Session Types",
    "url": "http://arxiv.org/abs/2504.21108v1",
    "arxiv_id": "2504.21108v1",
    "authors": [
      "Ivan Proki\u0107",
      "Simona Proki\u0107",
      "Silvia Ghilezan",
      "Alceste Scalas",
      "Nobuko Yoshida"
    ],
    "published": "2025-04-29T18:37:23+00:00",
    "summary": "Asynchronous multiparty session types provide a formal model for expressing the behaviour of communicating processes and verifying that they correctly implement desired protocols. In the ``bottom-up'' approach to session typing, local session types are specified directly, and the properties of their composition (e.g. deadlock freedom and liveness) are checked and transferred to well-typed processes. This method allows expressing and verifying a broad range of protocols, but still has a key limitation: it only supports protocols where every send/receive operation is directed towards strictly one recipient/sender at a time. This makes the technique too restrictive for modelling some classes of protocols, e.g. those used in the field of federated learning.   This paper improves the session typing theory by extending the asynchronous ``bottom-up'' approach to support protocols where a participant can choose to send or receive messages to/from multiple other participants at the same time, rather than just one at a time. We demonstrate how this extension enables the modeling and verification of real-world protocols, including some used in federated learning. Furthermore, we introduce and formally prove safety, deadlock-freedom, liveness, and session fidelity properties for our session typing system, revealing interesting dependencies between these properties in the presence of a subtyping relation."
  },
  {
    "title": "HyPerAlign: Hypotheses-driven Personalized Alignment",
    "url": "http://arxiv.org/abs/2505.00038v1",
    "arxiv_id": "2505.00038v1",
    "authors": [
      "Cristina Garbacea",
      "Chenhao Tan"
    ],
    "published": "2025-04-29T18:01:46+00:00",
    "summary": "Alignment algorithms are widely used to align large language models (LLMs) to human users based on preference annotations that reflect their intended real-world use cases. Typically these (often divergent) preferences are aggregated over a diverse set of users, resulting in fine-tuned models that are aligned to the ``average-user'' preference. Nevertheless, current models are used by individual users in very specific contexts and situations, emphasizing the need for user-dependent preference control. In this work we address the problem of personalizing LLM outputs to their users, aiming to generate customized responses tailored to individual users, instead of generic outputs that emulate the collective voices of diverse populations. We propose a novel interpretable and sample-efficient hypotheses-driven personalization approach (HyPerAlign) where given few-shot examples written by a particular user, we first infer hypotheses about their communication strategies, personality and writing style, then prompt LLM models with these hypotheses and user specific attributes to generate customized outputs. We conduct experiments on two different personalization tasks, authorship attribution and deliberative alignment, with datasets from diverse domains (news articles, blog posts, emails, jailbreaking benchmarks), and demonstrate the superiority of hypotheses-driven personalization approach when compared to preference-based fine-tuning methods. For deliberative alignment, the helpfulness of LLM models is improved by up to $70\\%$ on average. For authorship attribution, results indicate consistently high win-rates (commonly $>90\\%$) against state-of-the-art preference fine-tuning approaches for LLM personalization across diverse user profiles and LLM models. Overall, our approach represents an interpretable and sample-efficient strategy for the personalization of LLM models to individual users."
  },
  {
    "title": "AegisLLM: Scaling Agentic Systems for Self-Reflective Defense in LLM Security",
    "url": "http://arxiv.org/abs/2504.20965v1",
    "arxiv_id": "2504.20965v1",
    "authors": [
      "Zikui Cai",
      "Shayan Shabihi",
      "Bang An",
      "Zora Che",
      "Brian R. Bartoldson",
      "Bhavya Kailkhura",
      "Tom Goldstein",
      "Furong Huang"
    ],
    "published": "2025-04-29T17:36:05+00:00",
    "summary": "We introduce AegisLLM, a cooperative multi-agent defense against adversarial attacks and information leakage. In AegisLLM, a structured workflow of autonomous agents - orchestrator, deflector, responder, and evaluator - collaborate to ensure safe and compliant LLM outputs, while self-improving over time through prompt optimization. We show that scaling agentic reasoning system at test-time - both by incorporating additional agent roles and by leveraging automated prompt optimization (such as DSPy)- substantially enhances robustness without compromising model utility. This test-time defense enables real-time adaptability to evolving attacks, without requiring model retraining. Comprehensive evaluations across key threat scenarios, including unlearning and jailbreaking, demonstrate the effectiveness of AegisLLM. On the WMDP unlearning benchmark, AegisLLM achieves near-perfect unlearning with only 20 training examples and fewer than 300 LM calls. For jailbreaking benchmarks, we achieve 51% improvement compared to the base model on StrongReject, with false refusal rates of only 7.9% on PHTest compared to 18-55% for comparable methods. Our results highlight the advantages of adaptive, agentic reasoning over static defenses, establishing AegisLLM as a strong runtime alternative to traditional approaches based on model modifications. Code is available at https://github.com/zikuicai/aegisllm"
  },
  {
    "title": "A Domain-Agnostic Scalable AI Safety Ensuring Framework",
    "url": "http://arxiv.org/abs/2504.20924v1",
    "arxiv_id": "2504.20924v1",
    "authors": [
      "Beomjun Kim",
      "Kangyeon Kim",
      "Sunwoo Kim",
      "Heejin Ahn"
    ],
    "published": "2025-04-29T16:38:35+00:00",
    "summary": "Ensuring the safety of AI systems has recently emerged as a critical priority for real-world deployment, particularly in physical AI applications. Current approaches to AI safety typically address predefined domain-specific safety conditions, limiting their ability to generalize across contexts.   We propose a novel AI safety framework that ensures AI systems comply with \\textbf{any user-defined constraint}, with \\textbf{any desired probability}, and across \\textbf{various domains}.   In this framework, we combine an AI component (e.g., neural network) with an optimization problem to produce responses that minimize objectives while satisfying user-defined constraints with probabilities exceeding user-defined thresholds. For credibility assessment of the AI component, we propose \\textit{internal test data}, a supplementary set of safety-labeled data, and a \\textit{conservative testing} methodology that provides statistical validity of using internal test data. We also present an approximation method of a loss function and how to compute its gradient for training.   We mathematically prove that probabilistic constraint satisfaction is guaranteed under specific, mild conditions and prove a scaling law between safety and the number of internal test data. We demonstrate our framework's effectiveness through experiments in diverse domains: demand prediction for production decision, safe reinforcement learning within the SafetyGym simulator, and guarding AI chatbot outputs. Through these experiments, we demonstrate that our method guarantees safety for user-specified constraints, outperforms {for \\textbf{up to several order of magnitudes}} existing methods in low safety threshold regions, and scales effectively with respect to the size of internal test data."
  },
  {
    "title": "A Domain-Agnostic Scalable AI Safety Ensuring Framework",
    "url": "http://arxiv.org/abs/2504.20924v2",
    "arxiv_id": "2504.20924v2",
    "authors": [
      "Beomjun Kim",
      "Kangyeon Kim",
      "Sunwoo Kim",
      "Heejin Ahn"
    ],
    "published": "2025-04-29T16:38:35+00:00",
    "summary": "Ensuring the safety of AI systems has recently emerged as a critical priority for real-world deployment, particularly in physical AI applications. Current approaches to AI safety typically address predefined domain-specific safety conditions, limiting their ability to generalize across contexts. We propose a novel AI safety framework that ensures AI systems comply with any user-defined constraint, with any desired probability, and across various domains. In this framework, we combine an AI component (e.g., neural network) with an optimization problem to produce responses that minimize objectives while satisfying user-defined constraints with probabilities exceeding user-defined thresholds. For credibility assessment of the AI component, we propose internal test data, a supplementary set of safety-labeled data, and a conservative testing methodology that provides statistical validity of using internal test data. We also present an approximation method of a loss function and how to compute its gradient for training. We mathematically prove that probabilistic constraint satisfaction is guaranteed under specific, mild conditions and prove a scaling law between safety and the number of internal test data. We demonstrate our framework's effectiveness through experiments in diverse domains: demand prediction for production decision, safe reinforcement learning within the SafetyGym simulator, and guarding AI chatbot outputs. Through these experiments, we demonstrate that our method guarantees safety for user-specified constraints, outperforms for up to several order of magnitudes existing methods in low safety threshold regions, and scales effectively with respect to the size of internal test data."
  },
  {
    "title": "When Testing AI Tests Us: Safeguarding Mental Health on the Digital Frontlines",
    "url": "http://arxiv.org/abs/2504.20910v1",
    "arxiv_id": "2504.20910v1",
    "authors": [
      "Sachin R. Pendse",
      "Darren Gergle",
      "Rachel Kornfield",
      "Jonah Meyerhoff",
      "David Mohr",
      "Jina Suh",
      "Annie Wescott",
      "Casey Williams",
      "Jessica Schleider"
    ],
    "published": "2025-04-29T16:27:20+00:00",
    "summary": "Red-teaming is a core part of the infrastructure that ensures that AI models do not produce harmful content. Unlike past technologies, the black box nature of generative AI systems necessitates a uniquely interactional mode of testing, one in which individuals on red teams actively interact with the system, leveraging natural language to simulate malicious actors and solicit harmful outputs. This interactional labor done by red teams can result in mental health harms that are uniquely tied to the adversarial engagement strategies necessary to effectively red team. The importance of ensuring that generative AI models do not propagate societal or individual harm is widely recognized -- one less visible foundation of end-to-end AI safety is also the protection of the mental health and wellbeing of those who work to keep model outputs safe. In this paper, we argue that the unmet mental health needs of AI red-teamers is a critical workplace safety concern. Through analyzing the unique mental health impacts associated with the labor done by red teams, we propose potential individual and organizational strategies that could be used to meet these needs, and safeguard the mental health of red-teamers. We develop our proposed strategies through drawing parallels between common red-teaming practices and interactional labor common to other professions (including actors, mental health professionals, conflict photographers, and content moderators), describing how individuals and organizations within these professional spaces safeguard their mental health given similar psychological demands. Drawing on these protective practices, we describe how safeguards could be adapted for the distinct mental health challenges experienced by red teaming organizations as they mitigate emerging technological risks on the new digital frontlines."
  },
  {
    "title": "GiBy: A Giant-Step Baby-Step Classifier For Anomaly Detection In Industrial Control Systems",
    "url": "http://arxiv.org/abs/2504.20906v1",
    "arxiv_id": "2504.20906v1",
    "authors": [
      "Sarad Venugopalan",
      "Sridhar Adepu"
    ],
    "published": "2025-04-29T16:24:11+00:00",
    "summary": "The continuous monitoring of the interactions between cyber-physical components of any industrial control system (ICS) is required to secure automation of the system controls, and to guarantee plant processes are fail-safe and remain in an acceptably safe state. Safety is achieved by managing actuation (where electric signals are used to trigger physical movement), dependent on corresponding sensor readings; used as ground truth in decision making. Timely detection of anomalies (attacks, faults and unascertained states) in ICSs is crucial for the safe running of a plant, the safety of its personnel, and for the safe provision of any services provided. We propose an anomaly detection method that involves accurate linearization of the non-linear forms arising from sensor-actuator(s) relationships, primarily because solving linear models is easier and well understood. Further, the time complexity of the anomaly detection scenario/problem at hand is lowered using dimensionality reduction of the actuator(s) in relationship with a sensor. We accomplish this by using a well-known water treatment testbed as a use case. Our experiments show millisecond time response to detect anomalies and provide explainability; that are not simultaneously achieved by other state of the art AI/ML models with eXplainable AI (XAI) used for the same purpose. Further, we pin-point the sensor(s) and its actuation state for which anomaly was detected."
  },
  {
    "title": "GaussTrap: Stealthy Poisoning Attacks on 3D Gaussian Splatting for Targeted Scene Confusion",
    "url": "http://arxiv.org/abs/2504.20829v1",
    "arxiv_id": "2504.20829v1",
    "authors": [
      "Jiaxin Hong",
      "Sixu Chen",
      "Shuoyang Sun",
      "Hongyao Yu",
      "Hao Fang",
      "Yuqi Tan",
      "Bin Chen",
      "Shuhan Qi",
      "Jiawei Li"
    ],
    "published": "2025-04-29T14:52:14+00:00",
    "summary": "As 3D Gaussian Splatting (3DGS) emerges as a breakthrough in scene representation and novel view synthesis, its rapid adoption in safety-critical domains (e.g., autonomous systems, AR/VR) urgently demands scrutiny of potential security vulnerabilities. This paper presents the first systematic study of backdoor threats in 3DGS pipelines. We identify that adversaries may implant backdoor views to induce malicious scene confusion during inference, potentially leading to environmental misperception in autonomous navigation or spatial distortion in immersive environments. To uncover this risk, we propose GuassTrap, a novel poisoning attack method targeting 3DGS models. GuassTrap injects malicious views at specific attack viewpoints while preserving high-quality rendering in non-target views, ensuring minimal detectability and maximizing potential harm. Specifically, the proposed method consists of a three-stage pipeline (attack, stabilization, and normal training) to implant stealthy, viewpoint-consistent poisoned renderings in 3DGS, jointly optimizing attack efficacy and perceptual realism to expose security risks in 3D rendering. Extensive experiments on both synthetic and real-world datasets demonstrate that GuassTrap can effectively embed imperceptible yet harmful backdoor views while maintaining high-quality rendering in normal views, validating its robustness, adaptability, and practical applicability."
  },
  {
    "title": "DP-SMOTE: Integrating Differential Privacy and Oversampling Technique to Preserve Privacy in Smart Homes",
    "url": "http://arxiv.org/abs/2504.20827v1",
    "arxiv_id": "2504.20827v1",
    "authors": [
      "Amr Tarek Elsayed",
      "Almohammady Sobhi Alsharkawy",
      "Mohamed Sayed Farag",
      "Shaban Ebrahim Abu Yusuf"
    ],
    "published": "2025-04-29T14:50:50+00:00",
    "summary": "Smart homes represent intelligent environments where interconnected devices gather information, enhancing users living experiences by ensuring comfort, safety, and efficient energy management. To enhance the quality of life, companies in the smart device industry collect user data, including activities, preferences, and power consumption. However, sharing such data necessitates privacy-preserving practices. This paper introduces a robust method for secure sharing of data to service providers, grounded in differential privacy (DP). This empowers smart home residents to contribute usage statistics while safeguarding their privacy. The approach incorporates the Synthetic Minority Oversampling technique (SMOTe) and seamlessly integrates Gaussian noise to generate synthetic data, enabling data and statistics sharing while preserving individual privacy. The proposed method employs the SMOTe algorithm and applies Gaussian noise to generate data. Subsequently, it employs a k-anonymity function to assess reidentification risk before sharing the data. The simulation outcomes demonstrate that our method delivers strong performance in safeguarding privacy and in accuracy, recall, and f-measure metrics. This approach is particularly effective in smart homes, offering substantial utility in privacy at a reidentification risk of 30%, with Gaussian noise set to 0.3, SMOTe at 500%, and the application of a k-anonymity function with k = 2. Additionally, it shows a high classification accuracy, ranging from 90% to 98%, across various classification techniques."
  },
  {
    "title": "In defence of post-hoc explanations in medical AI",
    "url": "http://arxiv.org/abs/2504.20741v1",
    "arxiv_id": "2504.20741v1",
    "authors": [
      "Joshua Hatherley",
      "Lauritz Munch",
      "Jens Christian Bjerring"
    ],
    "published": "2025-04-29T13:24:21+00:00",
    "summary": "Since the early days of the Explainable AI movement, post-hoc explanations have been praised for their potential to improve user understanding, promote trust, and reduce patient safety risks in black box medical AI systems. Recently, however, critics have argued that the benefits of post-hoc explanations are greatly exaggerated since they merely approximate, rather than replicate, the actual reasoning processes that black box systems take to arrive at their outputs. In this article, we aim to defend the value of post-hoc explanations against this recent critique. We argue that even if post-hoc explanations do not replicate the exact reasoning processes of black box systems, they can still improve users' functional understanding of black box systems, increase the accuracy of clinician-AI teams, and assist clinicians in justifying their AI-informed decisions. While post-hoc explanations are not a \"silver bullet\" solution to the black box problem in medical AI, we conclude that they remain a useful strategy for addressing the black box problem in medical AI."
  },
  {
    "title": "Seeking Specifications: The Case for Neuro-Symbolic Specification Synthesis",
    "url": "http://arxiv.org/abs/2504.21061v1",
    "arxiv_id": "2504.21061v1",
    "authors": [
      "George Granberry",
      "Wolfgang Ahrendt",
      "Moa Johansson"
    ],
    "published": "2025-04-29T10:02:29+00:00",
    "summary": "This work is concerned with the generation of formal specifications from code, using Large Language Models (LLMs) in combination with symbolic methods. Concretely, in our study, the programming language is C, the specification language is ACSL, and the LLM is Deepseek-R1. In this context, we address two research directions, namely the specification of intent vs. implementation on the one hand, and the combination of symbolic analyses with LLMs on the other hand. For the first, we investigate how the absence or presence of bugs in the code impacts the generated specifications, as well as whether and how a user can direct the LLM to specify intent or implementation, respectively. For the second, we investigate the impact of results from symbolic analyses on the specifications generated by the LLM. The LLM prompts are augmented with outputs from two formal methods tools in the Frama-C ecosystem, Pathcrawler and EVA. We demonstrate how the addition of symbolic analysis to the workflow impacts the quality of annotations."
  },
  {
    "title": "Reinforcement Learning for Reasoning in Large Language Models with One Training Example",
    "url": "http://arxiv.org/abs/2504.20571v1",
    "arxiv_id": "2504.20571v1",
    "authors": [
      "Yiping Wang",
      "Qing Yang",
      "Zhiyuan Zeng",
      "Liliang Ren",
      "Lucas Liu",
      "Baolin Peng",
      "Hao Cheng",
      "Xuehai He",
      "Kuan Wang",
      "Jianfeng Gao",
      "Weizhu Chen",
      "Shuohang Wang",
      "Simon Shaolei Du",
      "Yelong Shen"
    ],
    "published": "2025-04-29T09:24:30+00:00",
    "summary": "We show that reinforcement learning with verifiable reward using one training example (1-shot RLVR) is effective in incentivizing the math reasoning capabilities of large language models (LLMs). Applying RLVR to the base model Qwen2.5-Math-1.5B, we identify a single example that elevates model performance on MATH500 from 36.0% to 73.6%, and improves the average performance across six common mathematical reasoning benchmarks from 17.6% to 35.7%. This result matches the performance obtained using the 1.2k DeepScaleR subset (MATH500: 73.6%, average: 35.9%), which includes the aforementioned example. Similar substantial improvements are observed across various models (Qwen2.5-Math-7B, Llama3.2-3B-Instruct, DeepSeek-R1-Distill-Qwen-1.5B), RL algorithms (GRPO and PPO), and different math examples (many of which yield approximately 30% or greater improvement on MATH500 when employed as a single training example). In addition, we identify some interesting phenomena during 1-shot RLVR, including cross-domain generalization, increased frequency of self-reflection, and sustained test performance improvement even after the training accuracy has saturated, a phenomenon we term post-saturation generalization. Moreover, we verify that the effectiveness of 1-shot RLVR primarily arises from the policy gradient loss, distinguishing it from the \"grokking\" phenomenon. We also show the critical role of promoting exploration (e.g., by adding entropy loss with an appropriate coefficient) in 1-shot RLVR training. As a bonus, we observe that applying entropy loss alone, without any outcome reward, significantly enhances Qwen2.5-Math-1.5B's performance on MATH500 by 27.4%. These findings can inspire future work on RLVR data efficiency and encourage a re-examination of both recent progress and the underlying mechanisms in RLVR. Our code, model, and data are open source at https://github.com/ypwang61/One-Shot-RLVR"
  },
  {
    "title": "Safe Bottom-Up Flexibility Provision from Distributed Energy Resources",
    "url": "http://arxiv.org/abs/2504.20529v1",
    "arxiv_id": "2504.20529v1",
    "authors": [
      "Costas Mylonas",
      "Emmanouel Varvarigos",
      "Georgios Tsaousoglou"
    ],
    "published": "2025-04-29T08:16:15+00:00",
    "summary": "Modern renewables-based power systems need to tap on the flexibility of Distributed Energy Resources (DERs) connected to distribution networks. It is important, however, that DER owners/users remain in control of their assets, decisions, and objectives. At the same time, the dynamic landscape of DER-penetrated distribution networks calls for agile, data-driven flexibility management frameworks. In the face of these developments, the Multi-Agent Reinforcement Learning (MARL) paradigm is gaining significant attention, as a distributed and data-driven decision-making policy. This paper addresses the need for bottom-up DER management decisions to account for the distribution network's safety-related constraints. While the related literature on safe MARL typically assumes that network characteristics are available and incorporated into the policy's safety layer, which implies active DSO engagement, this paper ensures that self-organized DER communities are enabled to provide distribution-network-safe flexibility services without relying on the aspirational and problematic requirement of bringing the DSO in the decision-making loop."
  },
  {
    "title": "Token-Efficient Prompt Injection Attack: Provoking Cessation in LLM Reasoning via Adaptive Token Compression",
    "url": "http://arxiv.org/abs/2504.20493v1",
    "arxiv_id": "2504.20493v1",
    "authors": [
      "Yu Cui",
      "Yujun Cai",
      "Yiwei Wang"
    ],
    "published": "2025-04-29T07:34:22+00:00",
    "summary": "While reasoning large language models (LLMs) demonstrate remarkable performance across various tasks, they also contain notable security vulnerabilities. Recent research has uncovered a \"thinking-stopped\" vulnerability in DeepSeek-R1, where model-generated reasoning tokens can forcibly interrupt the inference process, resulting in empty responses that compromise LLM-integrated applications. However, existing methods triggering this vulnerability require complex mathematical word problems with long prompts--even exceeding 5,000 tokens. To reduce the token cost and formally define this vulnerability, we propose a novel prompt injection attack named \"Reasoning Interruption Attack\", based on adaptive token compression. We demonstrate that simple standalone arithmetic tasks can effectively trigger this vulnerability, and the prompts based on such tasks exhibit simpler logical structures than mathematical word problems. We develop a systematic approach to efficiently collect attack prompts and an adaptive token compression framework that utilizes LLMs to automatically compress these prompts. Experiments show our compression framework significantly reduces prompt length while maintaining effective attack capabilities. We further investigate the attack's performance via output prefix and analyze the underlying causes of the vulnerability, providing valuable insights for improving security in reasoning LLMs."
  },
  {
    "title": "NeuRel-Attack: Neuron Relearning for Safety Disalignment in Large Language Models",
    "url": "http://arxiv.org/abs/2504.21053v1",
    "arxiv_id": "2504.21053v1",
    "authors": [
      "Yi Zhou",
      "Wenpeng Xing",
      "Dezhang Kong",
      "Changting Lin",
      "Meng Han"
    ],
    "published": "2025-04-29T05:49:35+00:00",
    "summary": "Safety alignment in large language models (LLMs) is achieved through fine-tuning mechanisms that regulate neuron activations to suppress harmful content. In this work, we propose a novel approach to induce disalignment by identifying and modifying the neurons responsible for safety constraints. Our method consists of three key steps: Neuron Activation Analysis, where we examine activation patterns in response to harmful and harmless prompts to detect neurons that are critical for distinguishing between harmful and harmless inputs; Similarity-Based Neuron Identification, which systematically locates the neurons responsible for safe alignment; and Neuron Relearning for Safety Removal, where we fine-tune these selected neurons to restore the model's ability to generate previously restricted responses. Experimental results demonstrate that our method effectively removes safety constraints with minimal fine-tuning, highlighting a critical vulnerability in current alignment techniques. Our findings underscore the need for robust defenses against adversarial fine-tuning attacks on LLMs."
  },
  {
    "title": "An Algebraic Approach to Asymmetric Delegation and Polymorphic Label Inference (Technical Report)",
    "url": "http://arxiv.org/abs/2504.20432v1",
    "arxiv_id": "2504.20432v1",
    "authors": [
      "Silei Ren",
      "Co\u015fku Acay",
      "Andrew C. Myers"
    ],
    "published": "2025-04-29T05:00:17+00:00",
    "summary": "Language-based information flow control (IFC) enables reasoning about and enforcing security policies in decentralized applications. While information flow properties are relatively extensional and compositional, designing expressive systems that enforce such properties remains challenging. In particular, it can be difficult to use IFC labels to model certain security assumptions, such as semi-honest agents.   Motivated by these modeling limitations, we study the algebraic semantics of lattice-based IFC label models, and propose a semantic framework that allows formalizing asymmetric delegation, which is partial delegation of confidentiality or integrity. Our framework supports downgrading of information and ensures their safety through nonmalleable information flow (NMIF).   To demonstrate the practicality of our framework, we design and implement a novel algorithm that statically checks NMIF and a label inference procedure that efficiently supports bounded label polymorphism, allowing users to write code generic with respect to labels."
  },
  {
    "title": "Safe and Optimal N-Spacecraft Swarm Reconfiguration in Non-Keplerian Cislunar Orbits",
    "url": "http://arxiv.org/abs/2504.20386v1",
    "arxiv_id": "2504.20386v1",
    "authors": [
      "Yuji Takubo",
      "Walter Manuel",
      "Ethan Foss",
      "Simone D'Amico"
    ],
    "published": "2025-04-29T03:13:28+00:00",
    "summary": "This paper presents a novel fuel-optimal guidance and control methodology for spacecraft swarm reconfiguration in Restricted Multi-Body Problems (RMBPs) with a guarantee of passive safety, maintaining miss distance even under abrupt loss of control authority. A new set of constraints exploits a quasi-periodic structure of RMBPs to guarantee passive safety. Particularly, this can be expressed as simple geometric constraints by solving optimal control in Local Toroidal Coordinates, which is based on a local eigenspace of a quasi-periodic motion around the corresponding periodic orbit. The proposed formulation enables a significant simplification of problem structure, which is highly applicable to large-scale swarm reconfiguration in cislunar orbits. The method is demonstrated in various models of RMBPs (Elliptical Restricted Three-Body Problem and Bi-Circular Restricted Four-Body Problem) and also validated in the full-ephemeris dynamics. By extending and generalizing well-known concepts from the two- to the three- and four-body problems, this paper lays the foundation for the practical control schemes of relative motion in cislunar space."
  },
  {
    "title": "Inception: Jailbreak the Memory Mechanism of Text-to-Image Generation Systems",
    "url": "http://arxiv.org/abs/2504.20376v1",
    "arxiv_id": "2504.20376v1",
    "authors": [
      "Shiqian Zhao",
      "Jiayang Liu",
      "Yiming Li",
      "Runyi Hu",
      "Xiaojun Jia",
      "Wenshu Fan",
      "Xinfeng Li",
      "Jie Zhang",
      "Wei Dong",
      "Tianwei Zhang",
      "Luu Anh Tuan"
    ],
    "published": "2025-04-29T02:40:36+00:00",
    "summary": "Currently, the memory mechanism has been widely and successfully exploited in online text-to-image (T2I) generation systems ($e.g.$, DALL$\\cdot$E 3) for alleviating the growing tokenization burden and capturing key information in multi-turn interactions. Despite its practicality, its security analyses have fallen far behind. In this paper, we reveal that this mechanism exacerbates the risk of jailbreak attacks. Different from previous attacks that fuse the unsafe target prompt into one ultimate adversarial prompt, which can be easily detected or may generate non-unsafe images due to under- or over-optimization, we propose Inception, the first multi-turn jailbreak attack against the memory mechanism in real-world text-to-image generation systems. Inception embeds the malice at the inception of the chat session turn by turn, leveraging the mechanism that T2I generation systems retrieve key information in their memory. Specifically, Inception mainly consists of two modules. It first segments the unsafe prompt into chunks, which are subsequently fed to the system in multiple turns, serving as pseudo-gradients for directive optimization. Specifically, we develop a series of segmentation policies that ensure the images generated are semantically consistent with the target prompt. Secondly, after segmentation, to overcome the challenge of the inseparability of minimum unsafe words, we propose recursion, a strategy that makes minimum unsafe words subdivisible. Collectively, segmentation and recursion ensure that all the request prompts are benign but can lead to malicious outcomes. We conduct experiments on the real-world text-to-image generation system ($i.e.$, DALL$\\cdot$E 3) to validate the effectiveness of Inception. The results indicate that Inception surpasses the state-of-the-art by a 14\\% margin in attack success rate."
  },
  {
    "title": "Online Safety for All: Sociocultural Insights from a Systematic Review of Youth Online Safety in the Global South",
    "url": "http://arxiv.org/abs/2504.20308v1",
    "arxiv_id": "2504.20308v1",
    "authors": [
      "Ozioma C. Oguine",
      "Oghenemaro Anuyah",
      "Zainab Agha",
      "Iris Melgarez",
      "Adriana Alvarado Garcia",
      "Karla Badillo-Urquiola"
    ],
    "published": "2025-04-28T23:36:57+00:00",
    "summary": "Youth online safety research in HCI has historically centered on perspectives from the Global North, often overlooking the unique particularities and cultural contexts of regions in the Global South. This paper presents a systematic review of 66 youth online safety studies published between 2014 and 2024, specifically focusing on regions in the Global South. Our findings reveal a concentrated research focus in Asian countries and predominance of quantitative methods. We also found limited research on marginalized youth populations and a primary focus on risks related to cyberbullying. Our analysis underscores the critical role of cultural factors in shaping online safety, highlighting the need for educational approaches that integrate social dynamics and awareness. We propose methodological recommendations and a future research agenda that encourages the adoption of situated, culturally sensitive methodologies and youth-centered approaches to researching youth online safety regions in the Global South. This paper advocates for greater inclusivity in youth online safety research, emphasizing the importance of addressing varied sociocultural contexts to better understand and meet the online safety needs of youth in the Global South."
  },
  {
    "title": "Improving trajectory continuity in drone-based crowd monitoring using a set of minimal-cost techniques and deep discriminative correlation filters",
    "url": "http://arxiv.org/abs/2504.20234v1",
    "arxiv_id": "2504.20234v1",
    "authors": [
      "Bartosz Ptak",
      "Marek Kraft"
    ],
    "published": "2025-04-28T20:07:42+00:00",
    "summary": "Drone-based crowd monitoring is the key technology for applications in surveillance, public safety, and event management. However, maintaining tracking continuity and consistency remains a significant challenge. Traditional detection-assignment tracking methods struggle with false positives, false negatives, and frequent identity switches, leading to degraded counting accuracy and making in-depth analysis impossible. This paper introduces a point-oriented online tracking algorithm that improves trajectory continuity and counting reliability in drone-based crowd monitoring. Our method builds on the Simple Online and Real-time Tracking (SORT) framework, replacing the original bounding-box assignment with a point-distance metric. The algorithm is enhanced with three cost-effective techniques: camera motion compensation, altitude-aware assignment, and classification-based trajectory validation. Further, Deep Discriminative Correlation Filters (DDCF) that re-use spatial feature maps from localisation algorithms for increased computational efficiency through neural network resource sharing are integrated to refine object tracking by reducing noise and handling missed detections. The proposed method is evaluated on the DroneCrowd and newly shared UP-COUNT-TRACK datasets, demonstrating substantial improvements in tracking metrics, reducing counting errors to 23% and 15%, respectively. The results also indicate a significant reduction of identity switches while maintaining high tracking accuracy, outperforming baseline online trackers and even an offline greedy optimisation method."
  },
  {
    "title": "Representation Learning on a Random Lattice",
    "url": "http://arxiv.org/abs/2504.20197v1",
    "arxiv_id": "2504.20197v1",
    "authors": [
      "Aryeh Brill"
    ],
    "published": "2025-04-28T19:01:36+00:00",
    "summary": "Decomposing a deep neural network's learned representations into interpretable features could greatly enhance its safety and reliability. To better understand features, we adopt a geometric perspective, viewing them as a learned coordinate system for mapping an embedded data distribution. We motivate a model of a generic data distribution as a random lattice and analyze its properties using percolation theory. Learned features are categorized into context, component, and surface features. The model is qualitatively consistent with recent findings in mechanistic interpretability and suggests directions for future research."
  },
  {
    "title": "Cybersecurity for Autonomous Vehicles",
    "url": "http://arxiv.org/abs/2504.20180v1",
    "arxiv_id": "2504.20180v1",
    "authors": [
      "Sai varun reddy Bhemavarapu"
    ],
    "published": "2025-04-28T18:31:37+00:00",
    "summary": "The increasing adoption of autonomous vehicles is bringing a major shift in the automotive industry. However, as these vehicles become more connected, cybersecurity threats have emerged as a serious concern. Protecting the security and integrity of autonomous systems is essential to prevent malicious activities that can harm passengers, other road users, and the overall transportation network. This paper focuses on addressing the cybersecurity issues in autonomous vehicles by examining the challenges and risks involved, which are important for building a secure future. Since autonomous vehicles depend on the communication between sensors, artificial intelligence, external infrastructure, and other systems, they are exposed to different types of cyber threats. A cybersecurity breach in an autonomous vehicle can cause serious problems, including a loss of public trust and safety. Therefore, it is very important to develop and apply strong cybersecurity measures to support the growth and acceptance of self-driving cars. This paper discusses major cybersecurity challenges like vulnerabilities in software and hardware, risks from wireless communication, and threats through external interfaces. It also reviews existing solutions such as secure software development, intrusion detection systems, cryptographic protocols, and anomaly detection methods. Additionally, the paper highlights the role of regulatory bodies, industry collaborations, and cybersecurity standards in creating a secure environment for autonomous vehicles. Setting clear rules and best practices is necessary for consistent protection across manufacturers and regions. By analyzing the current cybersecurity landscape and suggesting practical countermeasures, this paper aims to contribute to the safe development and public trust of autonomous vehicle technology."
  },
  {
    "title": "Modular Machine Learning: An Indispensable Path towards New-Generation Large Language Models",
    "url": "http://arxiv.org/abs/2504.20020v1",
    "arxiv_id": "2504.20020v1",
    "authors": [
      "Xin Wang",
      "Haoyang Li",
      "Zeyang Zhang",
      "Haibo Chen",
      "Wenwu Zhu"
    ],
    "published": "2025-04-28T17:42:02+00:00",
    "summary": "Large language models (LLMs) have dramatically advanced machine learning research including natural language processing, computer vision, data mining, etc., yet they still exhibit critical limitations in reasoning, factual consistency, and interpretability. In this paper, we introduce a novel learning paradigm -- Modular Machine Learning (MML) -- as an essential approach toward new-generation LLMs. MML decomposes the complex structure of LLMs into three interdependent components: modular representation, modular model, and modular reasoning, aiming to enhance LLMs' capability of counterfactual reasoning, mitigating hallucinations, as well as promoting fairness, safety, and transparency. Specifically, the proposed MML paradigm can: i) clarify the internal working mechanism of LLMs through the disentanglement of semantic components; ii) allow for flexible and task-adaptive model design; iii) enable interpretable and logic-driven decision-making process. We present a feasible implementation of MML-based LLMs via leveraging advanced techniques such as disentangled representation learning, neural architecture search and neuro-symbolic learning. We critically identify key challenges, such as the integration of continuous neural and discrete symbolic processes, joint optimization, and computational scalability, present promising future research directions that deserve further exploration. Ultimately, the integration of the MML paradigm with LLMs has the potential to bridge the gap between statistical (deep) learning and formal (logical) reasoning, thereby paving the way for robust, adaptable, and trustworthy AI systems across a wide range of real-world applications."
  },
  {
    "title": "Socially-Aware Autonomous Driving: Inferring Yielding Intentions for Safer Interactions",
    "url": "http://arxiv.org/abs/2504.20004v1",
    "arxiv_id": "2504.20004v1",
    "authors": [
      "Jing Wang",
      "Yan Jin",
      "Hamid Taghavifar",
      "Fei Ding",
      "Chongfeng Wei"
    ],
    "published": "2025-04-28T17:24:04+00:00",
    "summary": "Since the emergence of autonomous driving technology, it has advanced rapidly over the past decade. It is becoming increasingly likely that autonomous vehicles (AVs) would soon coexist with human-driven vehicles (HVs) on the roads. Currently, safety and reliable decision-making remain significant challenges, particularly when AVs are navigating lane changes and interacting with surrounding HVs. Therefore, precise estimation of the intentions of surrounding HVs can assist AVs in making more reliable and safe lane change decision-making. This involves not only understanding their current behaviors but also predicting their future motions without any direct communication. However, distinguishing between the passing and yielding intentions of surrounding HVs still remains ambiguous. To address the challenge, we propose a social intention estimation algorithm rooted in Directed Acyclic Graph (DAG), coupled with a decision-making framework employing Deep Reinforcement Learning (DRL) algorithms. To evaluate the method's performance, the proposed framework can be tested and applied in a lane-changing scenario within a simulated environment. Furthermore, the experiment results demonstrate how our approach enhances the ability of AVs to navigate lane changes safely and efficiently on roads."
  },
  {
    "title": "Mitigating Societal Cognitive Overload in the Age of AI: Challenges and Directions",
    "url": "http://arxiv.org/abs/2504.19990v1",
    "arxiv_id": "2504.19990v1",
    "authors": [
      "Salem Lahlou"
    ],
    "published": "2025-04-28T17:06:30+00:00",
    "summary": "Societal cognitive overload, driven by the deluge of information and complexity in the AI age, poses a critical challenge to human well-being and societal resilience. This paper argues that mitigating cognitive overload is not only essential for improving present-day life but also a crucial prerequisite for navigating the potential risks of advanced AI, including existential threats. We examine how AI exacerbates cognitive overload through various mechanisms, including information proliferation, algorithmic manipulation, automation anxieties, deregulation, and the erosion of meaning. The paper reframes the AI safety debate to center on cognitive overload, highlighting its role as a bridge between near-term harms and long-term risks. It concludes by discussing potential institutional adaptations, research directions, and policy considerations that arise from adopting an overload-resilient perspective on human-AI alignment, suggesting pathways for future exploration rather than prescribing definitive solutions."
  },
  {
    "title": "HJRNO: Hamilton-Jacobi Reachability with Neural Operators",
    "url": "http://arxiv.org/abs/2504.19989v1",
    "arxiv_id": "2504.19989v1",
    "authors": [
      "Yankai Li",
      "Mo Chen"
    ],
    "published": "2025-04-28T17:06:05+00:00",
    "summary": "Ensuring the safety of autonomous systems under uncertainty is a critical challenge. Hamilton-Jacobi reachability (HJR) analysis is a widely used method for guaranteeing safety under worst-case disturbances. Traditional HJR methods provide safety guarantees but suffer from the curse of dimensionality, limiting their scalability to high-dimensional systems or varying environmental conditions. In this work, we propose HJRNO, a neural operator-based framework for solving backward reachable tubes (BRTs) efficiently and accurately. By leveraging the Fourier Neural Operator (FNO), HJRNO learns a mapping between value functions, enabling fast inference with strong generalization across different obstacle shapes, system configurations, and hyperparameters. We demonstrate that HJRNO achieves low error on random obstacle scenarios and generalizes effectively across varying system dynamics. These results suggest that HJRNO offers a promising foundation model approach for scalable, real-time safety analysis in autonomous systems."
  },
  {
    "title": "Can AI Agents Design and Implement Drug Discovery Pipelines?",
    "url": "http://arxiv.org/abs/2504.19912v1",
    "arxiv_id": "2504.19912v1",
    "authors": [
      "Khachik Smbatyan",
      "Tsolak Ghukasyan",
      "Tigran Aghajanyan",
      "Hovhannes Dabaghyan",
      "Sergey Adamyan",
      "Aram Bughdaryan",
      "Vahagn Altunyan",
      "Gagik Navasardyan",
      "Aram Davtyan",
      "Anush Hakobyan",
      "Aram Gharibyan",
      "Arman Fahradyan",
      "Artur Hakobyan",
      "Hasmik Mnatsakanyan",
      "Narek Ginoyan",
      "Garik Petrosyan"
    ],
    "published": "2025-04-28T15:41:28+00:00",
    "summary": "The rapid advancement of artificial intelligence, particularly autonomous agentic systems based on Large Language Models (LLMs), presents new opportunities to accelerate drug discovery by improving in-silico modeling and reducing dependence on costly experimental trials. Current AI agent-based systems demonstrate proficiency in solving programming challenges and conducting research, indicating an emerging potential to develop software capable of addressing complex problems such as pharmaceutical design and drug discovery. This paper introduces DO Challenge, a benchmark designed to evaluate the decision-making abilities of AI agents in a single, complex problem resembling virtual screening scenarios. The benchmark challenges systems to independently develop, implement, and execute efficient strategies for identifying promising molecular structures from extensive datasets, while navigating chemical space, selecting models, and managing limited resources in a multi-objective context. We also discuss insights from the DO Challenge 2025, a competition based on the proposed benchmark, which showcased diverse strategies explored by human participants. Furthermore, we present the Deep Thought multi-agent system, which demonstrated strong performance on the benchmark, outperforming most human teams. Among the language models tested, Claude 3.7 Sonnet, Gemini 2.5 Pro and o3 performed best in primary agent roles, and GPT-4o, Gemini 2.0 Flash were effective in auxiliary roles. While promising, the system's performance still fell short of expert-designed solutions and showed high instability, highlighting both the potential and current limitations of AI-driven methodologies in transforming drug discovery and broader scientific research."
  },
  {
    "title": "Human-Centered AI and Autonomy in Robotics: Insights from a Bibliometric Study",
    "url": "http://arxiv.org/abs/2504.19848v1",
    "arxiv_id": "2504.19848v1",
    "authors": [
      "Simona Casini",
      "Pietro Ducange",
      "Francesco Marcelloni",
      "Lorenzo Pollini"
    ],
    "published": "2025-04-28T14:45:48+00:00",
    "summary": "The development of autonomous robotic systems offers significant potential for performing complex tasks with precision and consistency. Recent advances in Artificial Intelligence (AI) have enabled more capable intelligent automation systems, addressing increasingly complex challenges. However, this progress raises questions about human roles in such systems. Human-Centered AI (HCAI) aims to balance human control and automation, ensuring performance enhancement while maintaining creativity, mastery, and responsibility. For real-world applications, autonomous robots must balance task performance with reliability, safety, and trustworthiness. Integrating HCAI principles enhances human-robot collaboration and ensures responsible operation.   This paper presents a bibliometric analysis of intelligent autonomous robotic systems, utilizing SciMAT and VOSViewer to examine data from the Scopus database. The findings highlight academic trends, emerging topics, and AI's role in self-adaptive robotic behaviour, with an emphasis on HCAI architecture. These insights are then projected onto the IBM MAPE-K architecture, with the goal of identifying how these research results map into actual robotic autonomous systems development efforts for real-world scenarios."
  },
  {
    "title": "Measuring Train Driver Performance as Key to Approval of Driverless Trains",
    "url": "http://arxiv.org/abs/2504.19735v1",
    "arxiv_id": "2504.19735v1",
    "authors": [
      "Rustam Tagiew",
      "Prasannavenkatesh Balaji"
    ],
    "published": "2025-04-28T12:32:43+00:00",
    "summary": "Points 2.1.4(b), 2.4.2(b) and 2.4.3(b) in Annex I of Implementing Regulation (EU) No. 402/2013 allow a simplified approach for the safety approval of computer vision systems for driverless trains, if they have 'similar' functions and interfaces as the replaced human driver. The human driver is not replaced one-to-one by a technical system - only a limited set of cognitive functions are replaced. However, performance in the most challenging function, obstacle detection, is difficult to quantify due to the deficiency of published measurement results. This article summarizes the data published so far. This article also goes a long way to remedy this situation by providing a new public and anonymized dataset of 711 train driver performance measurements from controlled experiments. The measurements are made for different speeds, obstacle sizes, train protection systems and obstacle color contrasts respectively. The measured values are reaction time and distance to the obstacle. The goal of this paper is an unbiased and exhaustive description of the presented dataset for research, standardization and regulation. Further project related information including the dataset and source code is available at https://atosense-02371c.usercontent.opencode.de/"
  },
  {
    "title": "Hector UI: A Flexible Human-Robot User Interface for (Semi-)Autonomous Rescue and Inspection Robots",
    "url": "http://arxiv.org/abs/2504.19728v1",
    "arxiv_id": "2504.19728v1",
    "authors": [
      "Stefan Fabian",
      "Oskar von Stryk"
    ],
    "published": "2025-04-28T12:28:39+00:00",
    "summary": "The remote human operator's user interface (UI) is an important link to make the robot an efficient extension of the operator's perception and action. In rescue applications, several studies have investigated the design of operator interfaces based on observations during major robotics competitions or field deployments. Based on this research, guidelines for good interface design were empirically identified. The investigations on the UIs of teams participating in competitions are often based on external observations during UI application, which may miss some relevant requirements for UI flexibility. In this work, we present an open-source and flexibly configurable user interface based on established guidelines and its exemplary use for wheeled, tracked, and walking robots. We explain the design decisions and cover the insights we have gained during its highly successful applications in multiple robotics competitions and evaluations. The presented UI can also be adapted for other robots with little effort and is available as open source."
  },
  {
    "title": "Open-set Anomaly Segmentation in Complex Scenarios",
    "url": "http://arxiv.org/abs/2504.19706v1",
    "arxiv_id": "2504.19706v1",
    "authors": [
      "Song Xia",
      "Yi Yu",
      "Henghui Ding",
      "Wenhan Yang",
      "Shifei Liu",
      "Alex C. Kot",
      "Xudong Jiang"
    ],
    "published": "2025-04-28T12:00:10+00:00",
    "summary": "Precise segmentation of out-of-distribution (OoD) objects, herein referred to as anomalies, is crucial for the reliable deployment of semantic segmentation models in open-set, safety-critical applications, such as autonomous driving. Current anomalous segmentation benchmarks predominantly focus on favorable weather conditions, resulting in untrustworthy evaluations that overlook the risks posed by diverse meteorological conditions in open-set environments, such as low illumination, dense fog, and heavy rain. To bridge this gap, this paper introduces the ComsAmy, a challenging benchmark specifically designed for open-set anomaly segmentation in complex scenarios. ComsAmy encompasses a wide spectrum of adverse weather conditions, dynamic driving environments, and diverse anomaly types to comprehensively evaluate the model performance in realistic open-world scenarios. Our extensive evaluation of several state-of-the-art anomalous segmentation models reveals that existing methods demonstrate significant deficiencies in such challenging scenarios, highlighting their serious safety risks for real-world deployment. To solve that, we propose a novel energy-entropy learning (EEL) strategy that integrates the complementary information from energy and entropy to bolster the robustness of anomaly segmentation under complex open-world environments. Additionally, a diffusion-based anomalous training data synthesizer is proposed to generate diverse and high-quality anomalous images to enhance the existing copy-paste training data synthesizer. Extensive experimental results on both public and ComsAmy benchmarks demonstrate that our proposed diffusion-based synthesizer with energy and entropy learning (DiffEEL) serves as an effective and generalizable plug-and-play method to enhance existing models, yielding an average improvement of around 4.96% in $\\rm{AUPRC}$ and 9.87% in $\\rm{FPR}_{95}$."
  },
  {
    "title": "$\\texttt{SAGE}$: A Generic Framework for LLM Safety Evaluation",
    "url": "http://arxiv.org/abs/2504.19674v1",
    "arxiv_id": "2504.19674v1",
    "authors": [
      "Madhur Jindal",
      "Hari Shrawgi",
      "Parag Agrawal",
      "Sandipan Dandapat"
    ],
    "published": "2025-04-28T11:01:08+00:00",
    "summary": "Safety evaluation of Large Language Models (LLMs) has made progress and attracted academic interest, but it remains challenging to keep pace with the rapid integration of LLMs across diverse applications. Different applications expose users to various harms, necessitating application-specific safety evaluations with tailored harms and policies. Another major gap is the lack of focus on the dynamic and conversational nature of LLM systems. Such potential oversights can lead to harms that go unnoticed in standard safety benchmarks. This paper identifies the above as key requirements for robust LLM safety evaluation and recognizing that current evaluation methodologies do not satisfy these, we introduce the $\\texttt{SAGE}$ (Safety AI Generic Evaluation) framework. $\\texttt{SAGE}$ is an automated modular framework designed for customized and dynamic harm evaluations. It utilizes adversarial user models that are system-aware and have unique personalities, enabling a holistic red-teaming evaluation. We demonstrate $\\texttt{SAGE}$'s effectiveness by evaluating seven state-of-the-art LLMs across three applications and harm policies. Our experiments with multi-turn conversational evaluations revealed a concerning finding that harm steadily increases with conversation length. Furthermore, we observe significant disparities in model behavior when exposed to different user personalities and scenarios. Our findings also reveal that some models minimize harmful outputs by employing severe refusal tactics that can hinder their usefulness. These insights highlight the necessity of adaptive and context-specific testing to ensure better safety alignment and safer deployment of LLMs in real-world scenarios."
  },
  {
    "title": "Fooling the Decoder: An Adversarial Attack on Quantum Error Correction",
    "url": "http://arxiv.org/abs/2504.19651v1",
    "arxiv_id": "2504.19651v1",
    "authors": [
      "Jerome Lenssen",
      "Alexandru Paler"
    ],
    "published": "2025-04-28T10:10:05+00:00",
    "summary": "Neural network decoders are becoming essential for achieving fault-tolerant quantum computations. However, their internal mechanisms are poorly understood, hindering our ability to ensure their reliability and security against adversarial attacks. Leading machine learning decoders utilize recurrent and transformer models (e.g., AlphaQubit), with reinforcement learning (RL) playing a key role in training advanced transformer models (e.g., DeepSeek R1). In this work, we target a basic RL surface code decoder (DeepQ) to create the first adversarial attack on quantum error correction. By applying state-of-the-art white-box methods, we uncover vulnerabilities in this decoder, demonstrating an attack that reduces the logical qubit lifetime in memory experiments by up to five orders of magnitude. We validate that this attack exploits a genuine weakness, as the decoder exhibits robustness against noise fluctuations, is largely unaffected by substituting the referee decoder, responsible for episode termination, with an MWPM decoder, and demonstrates fault tolerance at checkable code distances. This attack highlights the susceptibility of machine learning-based QEC and underscores the importance of further research into robust QEC methods."
  },
  {
    "title": "ARMOR: Adaptive Meshing with Reinforcement Optimization for Real-time 3D Monitoring in Unexposed Scenes",
    "url": "http://arxiv.org/abs/2504.19624v1",
    "arxiv_id": "2504.19624v1",
    "authors": [
      "Yizhe Zhang",
      "Jianping Li",
      "Xin Zhao",
      "Fuxun Liang",
      "Zhen Dong",
      "Bisheng Yang"
    ],
    "published": "2025-04-28T09:33:40+00:00",
    "summary": "Unexposed environments, such as lava tubes, mines, and tunnels, are among the most complex yet strategically significant domains for scientific exploration and infrastructure development. Accurate and real-time 3D meshing of these environments is essential for applications including automated structural assessment, robotic-assisted inspection, and safety monitoring. Implicit neural Signed Distance Fields (SDFs) have shown promising capabilities in online meshing; however, existing methods often suffer from large projection errors and rely on fixed reconstruction parameters, limiting their adaptability to complex and unstructured underground environments such as tunnels, caves, and lava tubes. To address these challenges, this paper proposes ARMOR, a scene-adaptive and reinforcement learning-based framework for real-time 3D meshing in unexposed environments. The proposed method was validated across more than 3,000 meters of underground environments, including engineered tunnels, natural caves, and lava tubes. Experimental results demonstrate that ARMOR achieves superior performance in real-time mesh reconstruction, reducing geometric error by 3.96\\% compared to state-of-the-art baselines, while maintaining real-time efficiency. The method exhibits improved robustness, accuracy, and adaptability, indicating its potential for advanced 3D monitoring and mapping in challenging unexposed scenarios. The project page can be found at: https://yizhezhang0418.github.io/armor.github.io/"
  },
  {
    "title": "AI Alignment in Medical Imaging: Unveiling Hidden Biases Through Counterfactual Analysis",
    "url": "http://arxiv.org/abs/2504.19621v1",
    "arxiv_id": "2504.19621v1",
    "authors": [
      "Haroui Ma",
      "Francesco Quinzan",
      "Theresa Willem",
      "Stefan Bauer"
    ],
    "published": "2025-04-28T09:28:25+00:00",
    "summary": "Machine learning (ML) systems for medical imaging have demonstrated remarkable diagnostic capabilities, but their susceptibility to biases poses significant risks, since biases may negatively impact generalization performance. In this paper, we introduce a novel statistical framework to evaluate the dependency of medical imaging ML models on sensitive attributes, such as demographics. Our method leverages the concept of counterfactual invariance, measuring the extent to which a model's predictions remain unchanged under hypothetical changes to sensitive attributes. We present a practical algorithm that combines conditional latent diffusion models with statistical hypothesis testing to identify and quantify such biases without requiring direct access to counterfactual data. Through experiments on synthetic datasets and large-scale real-world medical imaging datasets, including \\textsc{cheXpert} and MIMIC-CXR, we demonstrate that our approach aligns closely with counterfactual fairness principles and outperforms standard baselines. This work provides a robust tool to ensure that ML diagnostic systems generalize well, e.g., across demographic groups, offering a critical step towards AI safety in healthcare. Code: https://github.com/Neferpitou3871/AI-Alignment-Medical-Imaging."
  },
  {
    "title": "Crowd Detection Using Very-Fine-Resolution Satellite Imagery",
    "url": "http://arxiv.org/abs/2504.19546v1",
    "arxiv_id": "2504.19546v1",
    "authors": [
      "Tong Xiao",
      "Qunming Wang",
      "Ping Lu",
      "Tenghai Huang",
      "Xiaohua Tong",
      "Peter M. Atkinson"
    ],
    "published": "2025-04-28T07:51:26+00:00",
    "summary": "Accurate crowd detection (CD) is critical for public safety and historical pattern analysis, yet existing methods relying on ground and aerial imagery suffer from limited spatio-temporal coverage. The development of very-fine-resolution (VFR) satellite sensor imagery (e.g., ~0.3 m spatial resolution) provides unprecedented opportunities for large-scale crowd activity analysis, but it has never been considered for this task. To address this gap, we proposed CrowdSat-Net, a novel point-based convolutional neural network, which features two innovative components: Dual-Context Progressive Attention Network (DCPAN) to improve feature representation of individuals by aggregating scene context and local individual characteristics, and High-Frequency Guided Deformable Upsampler (HFGDU) that recovers high-frequency information during upsampling through frequency-domain guided deformable convolutions. To validate the effectiveness of CrowdSat-Net, we developed CrowdSat, the first VFR satellite imagery dataset designed specifically for CD tasks, comprising over 120k manually labeled individuals from multi-source satellite platforms (Beijing-3N, Jilin-1 Gaofen-04A and Google Earth) across China. In the experiments, CrowdSat-Net was compared with five state-of-the-art point-based CD methods (originally designed for ground or aerial imagery) using CrowdSat and achieved the largest F1-score of 66.12% and Precision of 73.23%, surpassing the second-best method by 1.71% and 2.42%, respectively. Moreover, extensive ablation experiments validated the importance of the DCPAN and HFGDU modules. Furthermore, cross-regional evaluation further demonstrated the spatial generalizability of CrowdSat-Net. This research advances CD capability by providing both a newly developed network architecture for CD and a pioneering benchmark dataset to facilitate future CD development."
  },
  {
    "title": "Security Steerability is All You Need",
    "url": "http://arxiv.org/abs/2504.19521v1",
    "arxiv_id": "2504.19521v1",
    "authors": [
      "Itay Hazan",
      "Idan Habler",
      "Ron Bitton",
      "Itsik Mantin"
    ],
    "published": "2025-04-28T06:40:01+00:00",
    "summary": "The adoption of Generative AI (GenAI) in various applications inevitably comes with expanding the attack surface, combining new security threats along with the traditional ones. Consequently, numerous research and industrial initiatives aim to mitigate these security threats in GenAI by developing metrics and designing defenses. However, while most of the GenAI security work focuses on universal threats (e.g. manipulating the LLM to generate forbidden content), there is significantly less discussion on application-level security and how to mitigate it.   Thus, in this work we adopt an application-centric approach to GenAI security, and show that while LLMs cannot protect against ad-hoc application specific threats, they can provide the framework for applications to protect themselves against such threats. Our first contribution is defining Security Steerability - a novel security measure for LLMs, assessing the model's capability to adhere to strict guardrails that are defined in the system prompt ('Refrain from discussing about politics'). These guardrails, in case effective, can stop threats in the presence of malicious users who attempt to circumvent the application and cause harm to its providers.   Our second contribution is a methodology to measure the security steerability of LLMs, utilizing two newly-developed datasets: VeganRibs assesses the LLM behavior in forcing specific guardrails that are not security per se in the presence of malicious user that uses attack boosters (jailbreaks and perturbations), and ReverseText takes this approach further and measures the LLM ability to force specific treatment of the user input as plain text while do user try to give it additional meanings..."
  },
  {
    "title": "Security Steerability is All You Need",
    "url": "http://arxiv.org/abs/2504.19521v2",
    "arxiv_id": "2504.19521v2",
    "authors": [
      "Itay Hazan",
      "Idan Habler",
      "Ron Bitton",
      "Itsik Mantin"
    ],
    "published": "2025-04-28T06:40:01+00:00",
    "summary": "The adoption of Generative AI (GenAI) in various applications inevitably comes with expanding the attack surface, combining new security threats along with the traditional ones. Consequently, numerous research and industrial initiatives aim to mitigate these security threats in GenAI by developing metrics and designing defenses. However, while most of the GenAI security work focuses on universal threats (e.g. manipulating the LLM to generate forbidden content), there is significantly less discussion on application-level security and how to mitigate it. Thus, in this work we adopt an application-centric approach to GenAI security, and show that while LLMs cannot protect against ad-hoc application specific threats, they can provide the framework for applications to protect themselves against such threats. Our first contribution is defining Security Steerability - a novel security measure for LLMs, assessing the model's capability to adhere to strict guardrails that are defined in the system prompt ('Refrain from discussing about politics'). These guardrails, in case effective, can stop threats in the presence of malicious users who attempt to circumvent the application and cause harm to its providers. Our second contribution is a methodology to measure the security steerability of LLMs, utilizing two newly-developed datasets: VeganRibs assesses the LLM behavior in forcing specific guardrails that are not security per se in the presence of malicious user that uses attack boosters (jailbreaks and perturbations), and ReverseText takes this approach further and measures the LLM ability to force specific treatment of the user input as plain text while do user try to give it additional meanings..."
  },
  {
    "title": "AutoP2C: An LLM-Based Agent Framework for Code Repository Generation from Multimodal Content in Academic Papers",
    "url": "http://arxiv.org/abs/2504.20115v1",
    "arxiv_id": "2504.20115v1",
    "authors": [
      "Zijie Lin",
      "Yiqing Shen",
      "Qilin Cai",
      "He Sun",
      "Jinrui Zhou",
      "Mingjun Xiao"
    ],
    "published": "2025-04-28T05:47:37+00:00",
    "summary": "Machine Learning (ML) research is spread through academic papers featuring rich multimodal content, including text, diagrams, and tabular results. However, translating these multimodal elements into executable code remains a challenging and time-consuming process that requires substantial ML expertise. We introduce ``Paper-to-Code'' (P2C), a novel task that transforms the multimodal content of scientific publications into fully executable code repositories, which extends beyond the existing formulation of code generation that merely converts textual descriptions into isolated code snippets. To automate the P2C process, we propose AutoP2C, a multi-agent framework based on large language models that processes both textual and visual content from research papers to generate complete code repositories. Specifically, AutoP2C contains four stages: (1) repository blueprint extraction from established codebases, (2) multimodal content parsing that integrates information from text, equations, and figures, (3) hierarchical task decomposition for structured code generation, and (4) iterative feedback-driven debugging to ensure functionality and performance. Evaluation on a benchmark of eight research papers demonstrates the effectiveness of AutoP2C, which can successfully generate executable code repositories for all eight papers, while OpenAI-o1 or DeepSeek-R1 can only produce runnable code for one paper. The code is available at https://github.com/shoushouyu/Automated-Paper-to-Code."
  },
  {
    "title": "BRIDGE: Benchmarking Large Language Models for Understanding Real-world Clinical Practice Text",
    "url": "http://arxiv.org/abs/2504.19467v1",
    "arxiv_id": "2504.19467v1",
    "authors": [
      "Jiageng Wu",
      "Bowen Gu",
      "Ren Zhou",
      "Kevin Xie",
      "Doug Snyder",
      "Yixing Jiang",
      "Valentina Carducci",
      "Richard Wyss",
      "Rishi J Desai",
      "Emily Alsentzer",
      "Leo Anthony Celi",
      "Adam Rodman",
      "Sebastian Schneeweiss",
      "Jonathan H. Chen",
      "Santiago Romero-Brufau",
      "Kueiyu Joshua Lin",
      "Jie Yang"
    ],
    "published": "2025-04-28T04:13:18+00:00",
    "summary": "Large language models (LLMs) hold great promise for medical applications and are evolving rapidly, with new models being released at an accelerated pace. However, current evaluations of LLMs in clinical contexts remain limited. Most existing benchmarks rely on medical exam-style questions or PubMed-derived text, failing to capture the complexity of real-world electronic health record (EHR) data. Others focus narrowly on specific application scenarios, limiting their generalizability across broader clinical use. To address this gap, we present BRIDGE, a comprehensive multilingual benchmark comprising 87 tasks sourced from real-world clinical data sources across nine languages. We systematically evaluated 52 state-of-the-art LLMs (including DeepSeek-R1, GPT-4o, Gemini, and Llama 4) under various inference strategies. With a total of 13,572 experiments, our results reveal substantial performance variation across model sizes, languages, natural language processing tasks, and clinical specialties. Notably, we demonstrate that open-source LLMs can achieve performance comparable to proprietary models, while medically fine-tuned LLMs based on older architectures often underperform versus updated general-purpose models. The BRIDGE and its corresponding leaderboard serve as a foundational resource and a unique reference for the development and evaluation of new LLMs in real-world clinical text understanding."
  },
  {
    "title": "JailbreaksOverTime: Detecting Jailbreak Attacks Under Distribution Shift",
    "url": "http://arxiv.org/abs/2504.19440v1",
    "arxiv_id": "2504.19440v1",
    "authors": [
      "Julien Piet",
      "Xiao Huang",
      "Dennis Jacob",
      "Annabella Chow",
      "Maha Alrashed",
      "Geng Zhao",
      "Zhanhao Hu",
      "Chawin Sitawarin",
      "Basel Alomair",
      "David Wagner"
    ],
    "published": "2025-04-28T03:01:51+00:00",
    "summary": "Safety and security remain critical concerns in AI deployment. Despite safety training through reinforcement learning with human feedback (RLHF) [ 32], language models remain vulnerable to jailbreak attacks that bypass safety guardrails. Universal jailbreaks - prefixes that can circumvent alignment for any payload - are particularly concerning. We show empirically that jailbreak detection systems face distribution shift, with detectors trained at one point in time performing poorly against newer exploits. To study this problem, we release JailbreaksOverTime, a comprehensive dataset of timestamped real user interactions containing both benign requests and jailbreak attempts collected over 10 months. We propose a two-pronged method for defenders to detect new jailbreaks and continuously update their detectors. First, we show how to use continuous learning to detect jailbreaks and adapt rapidly to new emerging jailbreaks. While detectors trained at a single point in time eventually fail due to drift, we find that universal jailbreaks evolve slowly enough for self-training to be effective. Retraining our detection model weekly using its own labels - with no new human labels - reduces the false negative rate from 4% to 0.3% at a false positive rate of 0.1%. Second, we introduce an unsupervised active monitoring approach to identify novel jailbreaks. Rather than classifying inputs directly, we recognize jailbreaks by their behavior, specifically, their ability to trigger models to respond to known-harmful prompts. This approach has a higher false negative rate (4.1%) than supervised methods, but it successfully identified some out-of-distribution attacks that were missed by the continuous learning approach."
  },
  {
    "title": "Doxing via the Lens: Revealing Privacy Leakage in Image Geolocation for Agentic Multi-Modal Large Reasoning Model",
    "url": "http://arxiv.org/abs/2504.19373v1",
    "arxiv_id": "2504.19373v1",
    "authors": [
      "Weidi Luo",
      "Qiming Zhang",
      "Tianyu Lu",
      "Xiaogeng Liu",
      "Yue Zhao",
      "Zhen Xiang",
      "Chaowei Xiao"
    ],
    "published": "2025-04-27T22:26:45+00:00",
    "summary": "The increasing capabilities of agentic multi-modal large reasoning models, such as ChatGPT o3, have raised critical concerns regarding privacy leakage through inadvertent image geolocation. In this paper, we conduct the first systematic and controlled study on the potential privacy risks associated with visual reasoning abilities of ChatGPT o3. We manually collect and construct a dataset comprising 50 real-world images that feature individuals alongside privacy-relevant environmental elements, capturing realistic and sensitive scenarios for analysis. Our experimental evaluation reveals that ChatGPT o3 can predict user locations with high precision, achieving street-level accuracy (within one mile) in 60% of cases. Through analysis, we identify key visual cues, including street layout and front yard design, that significantly contribute to the model inference success. Additionally, targeted occlusion experiments demonstrate that masking critical features effectively mitigates geolocation accuracy, providing insights into potential defense mechanisms. Our findings highlight an urgent need for privacy-aware development for agentic multi-modal large reasoning models, particularly in applications involving private imagery."
  },
  {
    "title": "Doxing via the Lens: Revealing Privacy Leakage in Image Geolocation for Agentic Multi-Modal Large Reasoning Model",
    "url": "http://arxiv.org/abs/2504.19373v2",
    "arxiv_id": "2504.19373v2",
    "authors": [
      "Weidi Luo",
      "Qiming Zhang",
      "Tianyu Lu",
      "Xiaogeng Liu",
      "Yue Zhao",
      "Zhen Xiang",
      "Chaowei Xiao"
    ],
    "published": "2025-04-27T22:26:45+00:00",
    "summary": "The increasing capabilities of agentic multi-modal large reasoning models, such as ChatGPT o3, have raised critical concerns regarding privacy leakage through inadvertent image geolocation. In this paper, we conduct the first systematic and controlled study on the potential privacy risks associated with visual reasoning abilities of ChatGPT o3. We manually collect and construct a dataset comprising 50 real-world images that feature individuals alongside privacy-relevant environmental elements, capturing realistic and sensitive scenarios for analysis. Our experimental evaluation reveals that ChatGPT o3 can predict user locations with high precision, achieving street-level accuracy (within one mile) in 60% of cases. Through analysis, we identify key visual cues, including street layout and front yard design, that significantly contribute to the model inference success. Additionally, targeted occlusion experiments demonstrate that masking critical features effectively mitigates geolocation accuracy, providing insights into potential defense mechanisms. Our findings highlight an urgent need for privacy-aware development for agentic multi-modal large reasoning models, particularly in applications involving private imagery."
  },
  {
    "title": "Synthesis of Discrete-time Control Barrier Functions for Polynomial Systems Based on Sum-of-Squares Programming",
    "url": "http://arxiv.org/abs/2504.19330v1",
    "arxiv_id": "2504.19330v1",
    "authors": [
      "Erfan Shakhesi",
      "W. P. M. H.",
      "Heemels",
      "Alexander Katriniok"
    ],
    "published": "2025-04-27T18:59:43+00:00",
    "summary": "Discrete-time Control Barrier Functions (DTCBFs) are commonly utilized in the literature as a powerful tool for synthesizing control policies that guarantee safety of discrete-time dynamical systems. However, the systematic synthesis of DTCBFs in a computationally efficient way is at present an important open problem. This article first proposes a novel alternating-descent approach based on Sum-of-Squares programming to synthesize quadratic DTCBFs and corresponding polynomial control policies for discrete-time control-affine polynomial systems with input constraints and semi-algebraic safe sets. Subsequently, two distinct approaches are introduced to extend the proposed method to the synthesis of higher-degree polynomial DTCBFs. To demonstrate its efficacy, we apply the proposed method to numerical case studies."
  },
  {
    "title": "Synthesis of Discrete-time Control Barrier Functions for Polynomial Systems Based on Sum-of-Squares Programming",
    "url": "http://arxiv.org/abs/2504.19330v2",
    "arxiv_id": "2504.19330v2",
    "authors": [
      "Erfan Shakhesi",
      "W. P. M. H. Heemels",
      "Alexander Katriniok"
    ],
    "published": "2025-04-27T18:59:43+00:00",
    "summary": "Discrete-time Control Barrier Functions (DTCBFs) are commonly utilized in the literature as a powerful tool for synthesizing control policies that guarantee safety of discrete-time dynamical systems. However, the systematic synthesis of DTCBFs in a computationally efficient way is at present an important open problem. This article first proposes a novel alternating-descent approach based on Sum-of-Squares programming to synthesize quadratic DTCBFs and corresponding polynomial control policies for discrete-time control-affine polynomial systems with input constraints and semi-algebraic safe sets. Subsequently, two distinct approaches are introduced to extend the proposed method to the synthesis of higher-degree polynomial DTCBFs. To demonstrate its efficacy, we apply the proposed method to numerical case studies."
  },
  {
    "title": "Learned Perceptive Forward Dynamics Model for Safe and Platform-aware Robotic Navigation",
    "url": "http://arxiv.org/abs/2504.19322v1",
    "arxiv_id": "2504.19322v1",
    "authors": [
      "Pascal Roth",
      "Jonas Frey",
      "Cesar Cadena",
      "Marco Hutter"
    ],
    "published": "2025-04-27T18:27:28+00:00",
    "summary": "Ensuring safe navigation in complex environments requires accurate real-time traversability assessment and understanding of environmental interactions relative to the robot`s capabilities. Traditional methods, which assume simplified dynamics, often require designing and tuning cost functions to safely guide paths or actions toward the goal. This process is tedious, environment-dependent, and not generalizable.To overcome these issues, we propose a novel learned perceptive Forward Dynamics Model (FDM) that predicts the robot`s future state conditioned on the surrounding geometry and history of proprioceptive measurements, proposing a more scalable, safer, and heuristic-free solution. The FDM is trained on multiple years of simulated navigation experience, including high-risk maneuvers, and real-world interactions to incorporate the full system dynamics beyond rigid body simulation. We integrate our perceptive FDM into a zero-shot Model Predictive Path Integral (MPPI) planning framework, leveraging the learned mapping between actions, future states, and failure probability. This allows for optimizing a simplified cost function, eliminating the need for extensive cost-tuning to ensure safety. On the legged robot ANYmal, the proposed perceptive FDM improves the position estimation by on average 41% over competitive baselines, which translates into a 27% higher navigation success rate in rough simulation environments. Moreover, we demonstrate effective sim-to-real transfer and showcase the benefit of training on synthetic and real data. Code and models are made publicly available under https://github.com/leggedrobotics/fdm."
  },
  {
    "title": "Learned Perceptive Forward Dynamics Model for Safe and Platform-aware Robotic Navigation",
    "url": "http://arxiv.org/abs/2504.19322v2",
    "arxiv_id": "2504.19322v2",
    "authors": [
      "Pascal Roth",
      "Jonas Frey",
      "Cesar Cadena",
      "Marco Hutter"
    ],
    "published": "2025-04-27T18:27:28+00:00",
    "summary": "Ensuring safe navigation in complex environments requires accurate real-time traversability assessment and understanding of environmental interactions relative to the robot`s capabilities. Traditional methods, which assume simplified dynamics, often require designing and tuning cost functions to safely guide paths or actions toward the goal. This process is tedious, environment-dependent, and not generalizable. To overcome these issues, we propose a novel learned perceptive Forward Dynamics Model (FDM) that predicts the robot`s future state conditioned on the surrounding geometry and history of proprioceptive measurements, proposing a more scalable, safer, and heuristic-free solution. The FDM is trained on multiple years of simulated navigation experience, including high-risk maneuvers, and real-world interactions to incorporate the full system dynamics beyond rigid body simulation. We integrate our perceptive FDM into a zero-shot Model Predictive Path Integral (MPPI) planning framework, leveraging the learned mapping between actions, future states, and failure probability. This allows for optimizing a simplified cost function, eliminating the need for extensive cost-tuning to ensure safety. On the legged robot ANYmal, the proposed perceptive FDM improves the position estimation by on average 41% over competitive baselines, which translates into a 27% higher navigation success rate in rough simulation environments. Moreover, we demonstrate effective sim-to-real transfer and showcase the benefit of training on synthetic and real data. Code and models are made publicly available under https://github.com/leggedrobotics/fdm."
  },
  {
    "title": "The Weak Gravity Conjecture in Asymptotically Safe Quantum Gravity",
    "url": "http://arxiv.org/abs/2504.20107v1",
    "arxiv_id": "2504.20107v1",
    "authors": [
      "Gayatri Ghosh"
    ],
    "published": "2025-04-27T14:19:45+00:00",
    "summary": "The Weak Gravity Conjecture (WGC) posits that gravity must be the weakest force in any consistent theory of quantum gravity. Originally formulated to constrain the landscape of effective field theories arising from string theory, the WGC suggests the existence of states with a charge-to-mass ratio larger than that of extremal black holes. In this work, we revisit the WGC within the framework of Asymptotically Safe Quantum Gravity, a non-perturbative approach where gravitational and gauge couplings flow to a non-Gaussian ultraviolet (UV) fixed point. We construct a scale-dependent effective action, derive quantum-corrected Reissner--Nordstr\\\"om black hole solutions by incorporating position-dependent renormalization scale identification, and compute leading quantum corrections to the extremality condition. Our key finding is that the quantum correction to the extremal charge-to-mass ratio is dominantly governed by the running of the gauge coupling, characterized by a correction parameter $\\delta \\sim \\epsilon_e (\\ell_P/r_+)^{2\\theta}$, where $\\epsilon_e$ captures deviations from infrared behavior. We show that if the electromagnetic coupling grows in the UV ($\\epsilon_e > \\epsilon_G$), the WGC is dynamically strengthened, whereas if it decreases ($\\epsilon_e < \\epsilon_G$), large extremal black holes may violate the WGC unless additional light charged states exist. Our analysis demonstrates that Asymptotic Safety provides a concrete ultraviolet mechanism influencing low-energy swampland criteria, offering a deep UV/IR connection between quantum gravity consistency and effective field theory behavior."
  },
  {
    "title": "Efficient COLREGs-Compliant Collision Avoidance using Turning Circle-based Control Barrier Function",
    "url": "http://arxiv.org/abs/2504.19247v1",
    "arxiv_id": "2504.19247v1",
    "authors": [
      "Changyu Lee",
      "Jinwook Park",
      "Jinwhan Kim"
    ],
    "published": "2025-04-27T14:10:18+00:00",
    "summary": "This paper proposes a computationally efficient collision avoidance algorithm using turning circle-based control barrier functions (CBFs) that comply with international regulations for preventing collisions at sea (COLREGs). Conventional CBFs often lack explicit consideration of turning capabilities and avoidance direction, which are key elements in developing a COLREGs-compliant collision avoidance algorithm. To overcome these limitations, we introduce two CBFs derived from left and right turning circles. These functions establish safety conditions based on the proximity between the traffic ships and the centers of the turning circles, effectively determining both avoidance directions and turning capabilities. The proposed method formulates a quadratic programming problem with the CBFs as constraints, ensuring safe navigation without relying on computationally intensive trajectory optimization. This approach significantly reduces computational effort while maintaining performance comparable to model predictive control-based methods. Simulation results validate the effectiveness of the proposed algorithm in enabling COLREGs-compliant, safe navigation, demonstrating its potential for reliable and efficient operation in complex maritime environments."
  },
  {
    "title": "Machine Learning-Based Modeling of the Anode Heel Effect in X-ray Beam Monte Carlo Simulations",
    "url": "http://arxiv.org/abs/2504.19155v1",
    "arxiv_id": "2504.19155v1",
    "authors": [
      "Hussein Harb",
      "Didier Benoit",
      "Axel Rannou",
      "Chi-Hieu Pham",
      "Valentin Tissot",
      "Bahaa Nasr",
      "Julien Bert"
    ],
    "published": "2025-04-27T08:19:47+00:00",
    "summary": "This study enhances Monte Carlo simulation accuracy in X-ray imaging by developing an AI-driven model for the anode heel effect, achieving improved beam intensity distribution and dosimetric precision. Through dynamic adjustments to beam weights on the anode and cathode sides of the X-ray tube, our machine learning model effectively replicates the asymmetry characteristic of clinical X-ray beams. Experimental results reveal dose rate increases of up to 9.6% on the cathode side and reductions of up to 12.5% on the anode side, for energy levels between 50 and 120 kVp. These experimentally optimized beam weights were integrated into the OpenGATE and GGEMS Monte Carlo toolkits, significantly advancing dosimetric simulation accuracy and the image quality which closely resembles the clinical imaging. Validation with fluence and dose actors demonstrated that the AI-based model closely mirrors clinical beam behavior, providing substantial improvements in dose consistency and accuracy over conventional X-ray models. This approach provides a robust framework for improving X-ray dosimetry, with potential applications in dose optimization, imaging quality enhancement, and radiation safety in both clinical and research settings."
  },
  {
    "title": "Efficient Reasoning for LLMs through Speculative Chain-of-Thought",
    "url": "http://arxiv.org/abs/2504.19095v1",
    "arxiv_id": "2504.19095v1",
    "authors": [
      "Jikai Wang",
      "Juntao Li",
      "Lijun Wu",
      "Min Zhang"
    ],
    "published": "2025-04-27T03:56:39+00:00",
    "summary": "Large reasoning language models such as OpenAI-o1 and Deepseek-R1 have recently attracted widespread attention due to their impressive task-solving abilities. However, the enormous model size and the generation of lengthy thought chains introduce significant reasoning costs and response latency. Existing methods for efficient reasoning mainly focus on reducing the number of model parameters or shortening the chain-of-thought length. In this paper, we introduce Speculative Chain-of-Thought (SCoT), which reduces reasoning latency from another perspective by accelerated average reasoning speed through large and small model collaboration. SCoT conducts thought-level drafting using a lightweight draft model. Then it selects the best CoT draft and corrects the error cases with the target model. The proposed thinking behavior alignment improves the efficiency of drafting and the draft selection strategy maintains the prediction accuracy for complex problems. Experimental results on GSM8K, MATH, GaoKao, CollegeMath and Olympiad datasets show that SCoT reduces reasoning latency by 48\\%$\\sim$66\\% for Deepseek-R1-Distill-Qwen-32B while achieving near-target-model-level performance. Our code is available at https://github.com/Jikai0Wang/Speculative_CoT."
  },
  {
    "title": "CipherBank: Exploring the Boundary of LLM Reasoning Capabilities through Cryptography Challenges",
    "url": "http://arxiv.org/abs/2504.19093v1",
    "arxiv_id": "2504.19093v1",
    "authors": [
      "Yu Li",
      "Qizhi Pei",
      "Mengyuan Sun",
      "Honglin Lin",
      "Chenlin Ming",
      "Xin Gao",
      "Jiang Wu",
      "Conghui He",
      "Lijun Wu"
    ],
    "published": "2025-04-27T03:41:17+00:00",
    "summary": "Large language models (LLMs) have demonstrated remarkable capabilities, especially the recent advancements in reasoning, such as o1 and o3, pushing the boundaries of AI. Despite these impressive achievements in mathematics and coding, the reasoning abilities of LLMs in domains requiring cryptographic expertise remain underexplored. In this paper, we introduce CipherBank, a comprehensive benchmark designed to evaluate the reasoning capabilities of LLMs in cryptographic decryption tasks. CipherBank comprises 2,358 meticulously crafted problems, covering 262 unique plaintexts across 5 domains and 14 subdomains, with a focus on privacy-sensitive and real-world scenarios that necessitate encryption. From a cryptographic perspective, CipherBank incorporates 3 major categories of encryption methods, spanning 9 distinct algorithms, ranging from classical ciphers to custom cryptographic techniques. We evaluate state-of-the-art LLMs on CipherBank, e.g., GPT-4o, DeepSeek-V3, and cutting-edge reasoning-focused models such as o1 and DeepSeek-R1. Our results reveal significant gaps in reasoning abilities not only between general-purpose chat LLMs and reasoning-focused LLMs but also in the performance of current reasoning-focused models when applied to classical cryptographic decryption tasks, highlighting the challenges these models face in understanding and manipulating encrypted data. Through detailed analysis and error investigations, we provide several key observations that shed light on the limitations and potential improvement areas for LLMs in cryptographic reasoning. These findings underscore the need for continuous advancements in LLM reasoning capabilities."
  },
  {
    "title": "Graph of Attacks: Improved Black-Box and Interpretable Jailbreaks for LLMs",
    "url": "http://arxiv.org/abs/2504.19019v1",
    "arxiv_id": "2504.19019v1",
    "authors": [
      "Mohammad Akbar-Tajari",
      "Mohammad Taher Pilehvar",
      "Mohammad Mahmoody"
    ],
    "published": "2025-04-26T21:06:03+00:00",
    "summary": "The challenge of ensuring Large Language Models (LLMs) align with societal standards is of increasing interest, as these models are still prone to adversarial jailbreaks that bypass their safety mechanisms. Identifying these vulnerabilities is crucial for enhancing the robustness of LLMs against such exploits. We propose Graph of ATtacks (GoAT), a method for generating adversarial prompts to test the robustness of LLM alignment using the Graph of Thoughts framework [Besta et al., 2024]. GoAT excels at generating highly effective jailbreak prompts with fewer queries to the victim model than state-of-the-art attacks, achieving up to five times better jailbreak success rate against robust models like Llama. Notably, GoAT creates high-quality, human-readable prompts without requiring access to the targeted model's parameters, making it a black-box attack. Unlike approaches constrained by tree-based reasoning, GoAT's reasoning is based on a more intricate graph structure. By making simultaneous attack paths aware of each other's progress, this dynamic framework allows a deeper integration and refinement of reasoning paths, significantly enhancing the collaborative exploration of adversarial vulnerabilities in LLMs. At a technical level, GoAT starts with a graph structure and iteratively refines it by combining and improving thoughts, enabling synergy between different thought paths. The code for our implementation can be found at: https://github.com/GoAT-pydev/Graph_of_Attacks."
  },
  {
    "title": "Safety Interventions against Adversarial Patches in an Open-Source Driver Assistance System",
    "url": "http://arxiv.org/abs/2504.18990v1",
    "arxiv_id": "2504.18990v1",
    "authors": [
      "Cheng Chen",
      "Grant Xiao",
      "Daehyun Lee",
      "Lishan Yang",
      "Evgenia Smirni",
      "Homa Alemzadeh",
      "Xugui Zhou"
    ],
    "published": "2025-04-26T18:28:35+00:00",
    "summary": "Drivers are becoming increasingly reliant on advanced driver assistance systems (ADAS) as autonomous driving technology becomes more popular and developed with advanced safety features to enhance road safety. However, the increasing complexity of the ADAS makes autonomous vehicles (AVs) more exposed to attacks and accidental faults. In this paper, we evaluate the resilience of a widely used ADAS against safety-critical attacks that target perception inputs. Various safety mechanisms are simulated to assess their impact on mitigating attacks and enhancing ADAS resilience. Experimental results highlight the importance of timely intervention by human drivers and automated safety mechanisms in preventing accidents in both driving and lateral directions and the need to resolve conflicts among safety interventions to enhance system resilience and reliability."
  },
  {
    "title": "A Quadratic Programming Approach to Flight Envelope Protection Using Control Barrier Functions",
    "url": "http://arxiv.org/abs/2504.18951v1",
    "arxiv_id": "2504.18951v1",
    "authors": [
      "Johannes Autenrieb"
    ],
    "published": "2025-04-26T15:23:39+00:00",
    "summary": "Ensuring the safe operation of aerospace systems within their prescribed flight envelope is a fundamental requirement for modern flight control systems. Flight envelope protection prevents violations of aerodynamic, structural, and performance constraints, mitigating risks such as stall, excessive loads, and loss of control. Conventional FEP approaches, such as reference clipping via saturation functions and model-based command filtering, impose constraints at the reference input level but often fail to account for closed-loop system dynamics, potentially leading to constraint violations during transients. This paper introduces a new approach to the flight envelope protection problem by employing a quadratic programming-based safety filter using control barrier functions to dynamically enforce flight envelope constraints while preserving control performance. Unlike traditional reference filtering methods, the control barrier function-based safety filter actively ensures strict forward invariance of the safe flight envelope set, integrating seamlessly with existing control architectures. The proposed framework is implemented in a nonlinear missile flight control system and evaluated in a simulated environment. The results demonstrate its ability to prevent constraint violations while minimizing conservatism, offering a robust alternative to existing flight envelope protection methodologies."
  },
  {
    "title": "Advanced Longitudinal Control and Collision Avoidance for High-Risk Edge Cases in Autonomous Driving",
    "url": "http://arxiv.org/abs/2504.18931v1",
    "arxiv_id": "2504.18931v1",
    "authors": [
      "Dianwei Chen",
      "Yaobang Gong",
      "Xianfeng Yang"
    ],
    "published": "2025-04-26T14:17:06+00:00",
    "summary": "Advanced Driver Assistance Systems (ADAS) and Advanced Driving Systems (ADS) are key to improving road safety, yet most existing implementations focus primarily on the vehicle ahead, neglecting the behavior of following vehicles. This shortfall often leads to chain reaction collisions in high speed, densely spaced traffic particularly when a middle vehicle suddenly brakes and trailing vehicles cannot respond in time. To address this critical gap, we propose a novel longitudinal control and collision avoidance algorithm that integrates adaptive cruising with emergency braking. Leveraging deep reinforcement learning, our method simultaneously accounts for both leading and following vehicles. Through a data preprocessing framework that calibrates real-world sensor data, we enhance the robustness and reliability of the training process, ensuring the learned policy can handle diverse driving conditions. In simulated high risk scenarios (e.g., emergency braking in dense traffic), the algorithm effectively prevents potential pile up collisions, even in situations involving heavy duty vehicles. Furthermore, in typical highway scenarios where three vehicles decelerate, the proposed DRL approach achieves a 99% success rate far surpassing the standard Federal Highway Administration speed concepts guide, which reaches only 36.77% success under the same conditions."
  },
  {
    "title": "Latent Adversarial Training Improves the Representation of Refusal",
    "url": "http://arxiv.org/abs/2504.18872v1",
    "arxiv_id": "2504.18872v1",
    "authors": [
      "Alexandra Abbas",
      "Nora Petrova",
      "Helios Ael Lyons",
      "Natalia Perez-Campanero"
    ],
    "published": "2025-04-26T09:40:31+00:00",
    "summary": "Recent work has shown that language models' refusal behavior is primarily encoded in a single direction in their latent space, making it vulnerable to targeted attacks. Although Latent Adversarial Training (LAT) attempts to improve robustness by introducing noise during training, a key question remains: How does this noise-based training affect the underlying representation of refusal behavior? Understanding this encoding is crucial for evaluating LAT's effectiveness and limitations, just as the discovery of linear refusal directions revealed vulnerabilities in traditional supervised safety fine-tuning (SSFT).   Through the analysis of Llama 2 7B, we examine how LAT reorganizes the refusal behavior in the model's latent space compared to SSFT and embedding space adversarial training (AT). By computing activation differences between harmful and harmless instruction pairs and applying Singular Value Decomposition (SVD), we find that LAT significantly alters the refusal representation, concentrating it in the first two SVD components which explain approximately 75 percent of the activation differences variance - significantly higher than in reference models. This concentrated representation leads to more effective and transferable refusal vectors for ablation attacks: LAT models show improved robustness when attacked with vectors from reference models but become more vulnerable to self-generated vectors compared to SSFT and AT. Our findings suggest that LAT's training perturbations enable a more comprehensive representation of refusal behavior, highlighting both its potential strengths and vulnerabilities for improving model safety."
  },
  {
    "title": "WLTCL: Wide Field-of-View 3-D LiDAR Truck Compartment Automatic Localization System",
    "url": "http://arxiv.org/abs/2504.18870v1",
    "arxiv_id": "2504.18870v1",
    "authors": [
      "Guodong Sun",
      "Mingjing Li",
      "Dingjie Liu",
      "Mingxuan Liu",
      "Bo Wu",
      "Yang Zhang"
    ],
    "published": "2025-04-26T09:35:47+00:00",
    "summary": "As an essential component of logistics automation, the automated loading system is becoming a critical technology for enhancing operational efficiency and safety. Precise automatic positioning of the truck compartment, which serves as the loading area, is the primary step in automated loading. However, existing methods have difficulty adapting to truck compartments of various sizes, do not establish a unified coordinate system for LiDAR and mobile manipulators, and often exhibit reliability issues in cluttered environments. To address these limitations, our study focuses on achieving precise automatic positioning of key points in large, medium, and small fence-style truck compartments in cluttered scenarios. We propose an innovative wide field-of-view 3-D LiDAR vehicle compartment automatic localization system. For vehicles of various sizes, this system leverages the LiDAR to generate high-density point clouds within an extensive field-of-view range. By incorporating parking area constraints, our vehicle point cloud segmentation method more effectively segments vehicle point clouds within the scene. Our compartment key point positioning algorithm utilizes the geometric features of the compartments to accurately locate the corner points, providing stackable spatial regions. Extensive experiments on our collected data and public datasets demonstrate that this system offers reliable positioning accuracy and reduced computational resource consumption, leading to its application and promotion in relevant fields."
  },
  {
    "title": "Diffeomorphic Obstacle Avoidance for Contractive Dynamical Systems via Implicit Representations",
    "url": "http://arxiv.org/abs/2504.18860v1",
    "arxiv_id": "2504.18860v1",
    "authors": [
      "Ken-Joel Simmoteit",
      "Philipp Schillinger",
      "Leonel Rozo"
    ],
    "published": "2025-04-26T08:56:51+00:00",
    "summary": "Ensuring safety and robustness of robot skills is becoming crucial as robots are required to perform increasingly complex and dynamic tasks. The former is essential when performing tasks in cluttered environments, while the latter is relevant to overcome unseen task situations. This paper addresses the challenge of ensuring both safety and robustness in dynamic robot skills learned from demonstrations. Specifically, we build on neural contractive dynamical systems to provide robust extrapolation of the learned skills, while designing a full-body obstacle avoidance strategy that preserves contraction stability via diffeomorphic transforms. This is particularly crucial in complex environments where implicit scene representations, such as Signed Distance Fields (SDFs), are necessary. To this end, our framework called Signed Distance Field Diffeomorphic Transform, leverages SDFs and flow-based diffeomorphisms to achieve contraction-preserving obstacle avoidance. We thoroughly evaluate our framework on synthetic datasets and several real-world robotic tasks in a kitchen environment. Our results show that our approach locally adapts the learned contractive vector field while staying close to the learned dynamics and without introducing highly-curved motion paths, thus outperforming several state-of-the-art methods."
  },
  {
    "title": "Introducing Interval Neural Networks for Uncertainty-Aware System Identification",
    "url": "http://arxiv.org/abs/2504.18845v1",
    "arxiv_id": "2504.18845v1",
    "authors": [
      "Mehmet Ali Ferah",
      "Tufan Kumbasar"
    ],
    "published": "2025-04-26T08:16:46+00:00",
    "summary": "System Identification (SysID) is crucial for modeling and understanding dynamical systems using experimental data. While traditional SysID methods emphasize linear models, their inability to fully capture nonlinear dynamics has driven the adoption of Deep Learning (DL) as a more powerful alternative. However, the lack of uncertainty quantification (UQ) in DL-based models poses challenges for reliability and safety, highlighting the necessity of incorporating UQ. This paper introduces a systematic framework for constructing and learning Interval Neural Networks (INNs) to perform UQ in SysID tasks. INNs are derived by transforming the learnable parameters (LPs) of pre-trained neural networks into interval-valued LPs without relying on probabilistic assumptions. By employing interval arithmetic throughout the network, INNs can generate Prediction Intervals (PIs) that capture target coverage effectively. We extend Long Short-Term Memory (LSTM) and Neural Ordinary Differential Equations (Neural ODEs) into Interval LSTM (ILSTM) and Interval NODE (INODE) architectures, providing the mathematical foundations for their application in SysID. To train INNs, we propose a DL framework that integrates a UQ loss function and parameterization tricks to handle constraints arising from interval LPs. We introduce novel concept \"elasticity\" for underlying uncertainty causes and validate ILSTM and INODE in SysID experiments, demonstrating their effectiveness."
  },
  {
    "title": "Swarming in the Wild: A Distributed Communication-less Lloyd-based Algorithm dealing with Uncertainties",
    "url": "http://arxiv.org/abs/2504.18840v1",
    "arxiv_id": "2504.18840v1",
    "authors": [
      "Manuel Boldrer",
      "Vit Kratky",
      "Viktor Walter",
      "Martin Saska"
    ],
    "published": "2025-04-26T07:56:52+00:00",
    "summary": "In this work, we present a distributed algorithm for swarming in complex environments that operates with no communication, no a priori information about the environment, and using only onboard sensing and computation capabilities. We provide sufficient conditions to guarantee that each robot reaches its goal region in a finite time, avoiding collisions with obstacles and other robots without exceeding a desired maximum distance from a predefined set of neighbors (flocking constraint). In addition, we show how the proposed algorithm can deal with tracking errors and onboard sensing errors without violating safety and proximity constraints, still providing the conditions for having convergence towards the goal region. To validate the approach, we provide experiments in the field. We tested our algorithm in GNSS-denied environments i.e., a dense forest, where fully autonomous aerial robots swarmed safely to the desired destinations, by relying only on onboard sensors, i.e., without a communication network. This work marks the initial deployment of a fully distributed system where there is no communication between the robots, nor reliance on any global localization system, which at the same time it ensures safety and convergence towards the goal within such complex environments."
  },
  {
    "title": "Toward Generalizable Evaluation in the LLM Era: A Survey Beyond Benchmarks",
    "url": "http://arxiv.org/abs/2504.18838v1",
    "arxiv_id": "2504.18838v1",
    "authors": [
      "Yixin Cao",
      "Shibo Hong",
      "Xinze Li",
      "Jiahao Ying",
      "Yubo Ma",
      "Haiyuan Liang",
      "Yantao Liu",
      "Zijun Yao",
      "Xiaozhi Wang",
      "Dan Huang",
      "Wenxuan Zhang",
      "Lifu Huang",
      "Muhao Chen",
      "Lei Hou",
      "Qianru Sun",
      "Xingjun Ma",
      "Zuxuan Wu",
      "Min-Yen Kan",
      "David Lo",
      "Qi Zhang",
      "Heng Ji",
      "Jing Jiang",
      "Juanzi Li",
      "Aixin Sun",
      "Xuanjing Huang",
      "Tat-Seng Chua",
      "Yu-Gang Jiang"
    ],
    "published": "2025-04-26T07:48:52+00:00",
    "summary": "Large Language Models (LLMs) are advancing at an amazing speed and have become indispensable across academia, industry, and daily applications. To keep pace with the status quo, this survey probes the core challenges that the rise of LLMs poses for evaluation. We identify and analyze two pivotal transitions: (i) from task-specific to capability-based evaluation, which reorganizes benchmarks around core competencies such as knowledge, reasoning, instruction following, multi-modal understanding, and safety; and (ii) from manual to automated evaluation, encompassing dynamic dataset curation and \"LLM-as-a-judge\" scoring.   Yet, even with these transitions, a crucial obstacle persists: the evaluation generalization issue. Bounded test sets cannot scale alongside models whose abilities grow seemingly without limit. We will dissect this issue, along with the core challenges of the above two transitions, from the perspectives of methods, datasets, evaluators, and metrics. Due to the fast evolving of this field, we will maintain a living GitHub repository (links are in each section) to crowd-source updates and corrections, and warmly invite contributors and collaborators."
  },
  {
    "title": "Reservoir-enhanced Segment Anything Model for Subsurface Diagnosis",
    "url": "http://arxiv.org/abs/2504.18802v1",
    "arxiv_id": "2504.18802v1",
    "authors": [
      "Xiren Zhou",
      "Shikang Liu",
      "Xinyu Yan",
      "Yizhan Fan",
      "Xiangyu Wang",
      "Yu Kang",
      "Jian Cheng",
      "Huanhuan Chen"
    ],
    "published": "2025-04-26T05:13:22+00:00",
    "summary": "Urban roads and infrastructure, vital to city operations, face growing threats from subsurface anomalies like cracks and cavities. Ground Penetrating Radar (GPR) effectively visualizes underground conditions employing electromagnetic (EM) waves; however, accurate anomaly detection via GPR remains challenging due to limited labeled data, varying subsurface conditions, and indistinct target boundaries. Although visually image-like, GPR data fundamentally represent EM waves, with variations within and between waves critical for identifying anomalies. Addressing these, we propose the Reservoir-enhanced Segment Anything Model (Res-SAM), an innovative framework exploiting both visual discernibility and wave-changing properties of GPR data. Res-SAM initially identifies apparent candidate anomaly regions given minimal prompts, and further refines them by analyzing anomaly-induced changing information within and between EM waves in local GPR data, enabling precise and complete anomaly region extraction and category determination. Real-world experiments demonstrate that Res-SAM achieves high detection accuracy (>85%) and outperforms state-of-the-art. Notably, Res-SAM requires only minimal accessible non-target data, avoids intensive training, and incorporates simple human interaction to enhance reliability. Our research provides a scalable, resource-efficient solution for rapid subsurface anomaly detection across diverse environments, improving urban safety monitoring while reducing manual effort and computational cost."
  },
  {
    "title": "Certifiably-Correct Mapping for Safe Navigation Despite Odometry Drift",
    "url": "http://arxiv.org/abs/2504.18713v1",
    "arxiv_id": "2504.18713v1",
    "authors": [
      "Devansh R. Agrawal",
      "Taekyung Kim",
      "Rajiv Govindjee",
      "Trushant Adeshara",
      "Jiangbo Yu",
      "Anurekha Ravikumar",
      "Dimitra Panagou"
    ],
    "published": "2025-04-25T21:53:33+00:00",
    "summary": "Accurate perception, state estimation and mapping are essential for safe robotic navigation as planners and controllers rely on these components for safety-critical decisions. However, existing mapping approaches often assume perfect pose estimates, an unrealistic assumption that can lead to incorrect obstacle maps and therefore collisions. This paper introduces a framework for certifiably-correct mapping that ensures that the obstacle map correctly classifies obstacle-free regions despite the odometry drift in vision-based localization systems (VIO}/SLAM). By deflating the safe region based on the incremental odometry error at each timestep, we ensure that the map remains accurate and reliable locally around the robot, even as the overall odometry error with respect to the inertial frame grows unbounded.   Our contributions include two approaches to modify popular obstacle mapping paradigms, (I) Safe Flight Corridors, and (II) Signed Distance Fields. We formally prove the correctness of both methods, and describe how they integrate with existing planning and control modules. Simulations using the Replica dataset highlight the efficacy of our methods compared to state-of-the-art techniques. Real-world experiments with a robotic rover show that, while baseline methods result in collisions with previously mapped obstacles, the proposed framework enables the rover to safely stop before potential collisions."
  },
  {
    "title": "Proof-of-TBI -- Fine-Tuned Vision Language Model Consortium and OpenAI-o3 Reasoning LLM-Based Medical Diagnosis Support System for Mild Traumatic Brain Injury (TBI) Prediction",
    "url": "http://arxiv.org/abs/2504.18671v1",
    "arxiv_id": "2504.18671v1",
    "authors": [
      "Ross Gore",
      "Eranga Bandara",
      "Sachin Shetty",
      "Alberto E. Musto",
      "Pratip Rana",
      "Ambrosio Valencia-Romero",
      "Christopher Rhea",
      "Lobat Tayebi",
      "Heather Richter",
      "Atmaram Yarlagadda",
      "Donna Edmonds",
      "Steven Wallace",
      "Donna Broshek"
    ],
    "published": "2025-04-25T19:49:30+00:00",
    "summary": "Mild Traumatic Brain Injury (TBI) detection presents significant challenges due to the subtle and often ambiguous presentation of symptoms in medical imaging, making accurate diagnosis a complex task. To address these challenges, we propose Proof-of-TBI, a medical diagnosis support system that integrates multiple fine-tuned vision-language models with the OpenAI-o3 reasoning large language model (LLM). Our approach fine-tunes multiple vision-language models using a labeled dataset of TBI MRI scans, training them to diagnose TBI symptoms effectively. The predictions from these models are aggregated through a consensus-based decision-making process. The system evaluates the predictions from all fine-tuned vision language models using the OpenAI-o3 reasoning LLM, a model that has demonstrated remarkable reasoning performance, to produce the most accurate final diagnosis. The LLM Agents orchestrates interactions between the vision-language models and the reasoning LLM, managing the final decision-making process with transparency, reliability, and automation. This end-to-end decision-making workflow combines the vision-language model consortium with the OpenAI-o3 reasoning LLM, enabled by custom prompt engineering by the LLM agents. The prototype for the proposed platform was developed in collaboration with the U.S. Army Medical Research team in Newport News, Virginia, incorporating five fine-tuned vision-language models. The results demonstrate the transformative potential of combining fine-tuned vision-language model inputs with the OpenAI-o3 reasoning LLM to create a robust, secure, and highly accurate diagnostic system for mild TBI prediction. To the best of our knowledge, this research represents the first application of fine-tuned vision-language models integrated with a reasoning LLM for TBI prediction tasks."
  },
  {
    "title": "Foundations of Safe Online Reinforcement Learning in the Linear Quadratic Regulator: $\\sqrt{T}$-Regret",
    "url": "http://arxiv.org/abs/2504.18657v1",
    "arxiv_id": "2504.18657v1",
    "authors": [
      "Benjamin Schiffer",
      "Lucas Janson"
    ],
    "published": "2025-04-25T19:22:57+00:00",
    "summary": "Understanding how to efficiently learn while adhering to safety constraints is essential for using online reinforcement learning in practical applications. However, proving rigorous regret bounds for safety-constrained reinforcement learning is difficult due to the complex interaction between safety, exploration, and exploitation. In this work, we seek to establish foundations for safety-constrained reinforcement learning by studying the canonical problem of controlling a one-dimensional linear dynamical system with unknown dynamics. We study the safety-constrained version of this problem, where the state must with high probability stay within a safe region, and we provide the first safe algorithm that achieves regret of $\\tilde{O}_T(\\sqrt{T})$. Furthermore, the regret is with respect to the baseline of truncated linear controllers, a natural baseline of non-linear controllers that are well-suited for safety-constrained linear systems. In addition to introducing this new baseline, we also prove several desirable continuity properties of the optimal controller in this baseline. In showing our main result, we prove that whenever the constraints impact the optimal controller, the non-linearity of our controller class leads to a faster rate of learning than in the unconstrained setting."
  },
  {
    "title": "Periodic Online Testing for Sparse Systolic Tensor Arrays",
    "url": "http://arxiv.org/abs/2504.18628v1",
    "arxiv_id": "2504.18628v1",
    "authors": [
      "Christodoulos Peltekis",
      "Chrysostomos Nicopoulos",
      "Giorgos Dimitrakopoulos"
    ],
    "published": "2025-04-25T18:10:45+00:00",
    "summary": "Modern Machine Learning (ML) applications often benefit from structured sparsity, a technique that efficiently reduces model complexity and simplifies handling of sparse data in hardware. Sparse systolic tensor arrays - specifically designed to accelerate these structured-sparse ML models - play a pivotal role in enabling efficient computations. As ML is increasingly integrated into safety-critical systems, it is of paramount importance to ensure the reliability of these systems. This paper introduces an online error-checking technique capable of detecting and locating permanent faults within sparse systolic tensor arrays before computation begins. The new technique relies on merely four test vectors and exploits the weight values already loaded within the systolic array to comprehensively test the system. Fault-injection campaigns within the gate-level netlist, while executing three well-established Convolutional Neural Networks (CNN), validate the efficiency of the proposed approach, which is shown to achieve very high fault coverage, while incurring minimal performance and area overheads."
  },
  {
    "title": "Examining the Impact of Optical Aberrations to Image Classification and Object Detection Models",
    "url": "http://arxiv.org/abs/2504.18510v1",
    "arxiv_id": "2504.18510v1",
    "authors": [
      "Patrick M\u00fcller",
      "Alexander Braun",
      "Margret Keuper"
    ],
    "published": "2025-04-25T17:23:47+00:00",
    "summary": "Deep neural networks (DNNs) have proven to be successful in various computer vision applications such that models even infer in safety-critical situations. Therefore, vision models have to behave in a robust way to disturbances such as noise or blur. While seminal benchmarks exist to evaluate model robustness to diverse corruptions, blur is often approximated in an overly simplistic way to model defocus, while ignoring the different blur kernel shapes that result from optical systems. To study model robustness against realistic optical blur effects, this paper proposes two datasets of blur corruptions, which we denote OpticsBench and LensCorruptions. OpticsBench examines primary aberrations such as coma, defocus, and astigmatism, i.e. aberrations that can be represented by varying a single parameter of Zernike polynomials. To go beyond the principled but synthetic setting of primary aberrations, LensCorruptions samples linear combinations in the vector space spanned by Zernike polynomials, corresponding to 100 real lenses. Evaluations for image classification and object detection on ImageNet and MSCOCO show that for a variety of different pre-trained models, the performance on OpticsBench and LensCorruptions varies significantly, indicating the need to consider realistic image corruptions to evaluate a model's robustness against blur."
  },
  {
    "title": "PolyMath: Evaluating Mathematical Reasoning in Multilingual Contexts",
    "url": "http://arxiv.org/abs/2504.18428v1",
    "arxiv_id": "2504.18428v1",
    "authors": [
      "Yiming Wang",
      "Pei Zhang",
      "Jialong Tang",
      "Haoran Wei",
      "Baosong Yang",
      "Rui Wang",
      "Chenshu Sun",
      "Feitong Sun",
      "Jiran Zhang",
      "Junxuan Wu",
      "Qiqian Cang",
      "Yichang Zhang",
      "Fei Huang",
      "Junyang Lin",
      "Fei Huang",
      "Jingren Zhou"
    ],
    "published": "2025-04-25T15:39:04+00:00",
    "summary": "In this paper, we introduce PolyMath, a multilingual mathematical reasoning benchmark covering 18 languages and 4 easy-to-hard difficulty levels. Our benchmark ensures difficulty comprehensiveness, language diversity, and high-quality translation, making it a highly discriminative multilingual mathematical benchmark in the era of reasoning LLMs. We conduct a comprehensive evaluation for advanced LLMs and find that even Deepseek-R1-671B and Qwen-QwQ-32B, achieve only 43.4 and 41.8 benchmark scores, with less than 30% accuracy under the highest level. From a language perspective, our benchmark reveals several key challenges of LLMs in multilingual reasoning: (1) Reasoning performance varies widely across languages for current LLMs; (2) Input-output language consistency is low in reasoning LLMs and may be correlated with performance; (3) The thinking length differs significantly by language for current LLMs. Additionally, we demonstrate that controlling the output language in the instructions has the potential to affect reasoning performance, especially for some low-resource languages, suggesting a promising direction for improving multilingual capabilities in LLMs."
  },
  {
    "title": "Expressing stigma and inappropriate responses prevents LLMs from safely replacing mental health providers",
    "url": "http://arxiv.org/abs/2504.18412v1",
    "arxiv_id": "2504.18412v1",
    "authors": [
      "Jared Moore",
      "Declan Grabb",
      "William Agnew",
      "Kevin Klyman",
      "Stevie Chancellor",
      "Desmond C. Ong",
      "Nick Haber"
    ],
    "published": "2025-04-25T15:14:21+00:00",
    "summary": "Should a large language model (LLM) be used as a therapist? In this paper, we investigate the use of LLMs to *replace* mental health providers, a use case promoted in the tech startup and research space. We conduct a mapping review of therapy guides used by major medical institutions to identify crucial aspects of therapeutic relationships, such as the importance of a therapeutic alliance between therapist and client. We then assess the ability of LLMs to reproduce and adhere to these aspects of therapeutic relationships by conducting several experiments investigating the responses of current LLMs, such as `gpt-4o`. Contrary to best practices in the medical community, LLMs 1) express stigma toward those with mental health conditions and 2) respond inappropriately to certain common (and critical) conditions in naturalistic therapy settings -- e.g., LLMs encourage clients' delusional thinking, likely due to their sycophancy. This occurs even with larger and newer LLMs, indicating that current safety practices may not address these gaps. Furthermore, we note foundational and practical barriers to the adoption of LLMs as therapists, such as that a therapeutic alliance requires human characteristics (e.g., identity and stakes). For these reasons, we conclude that LLMs should not replace therapists, and we discuss alternative roles for LLMs in clinical therapy."
  },
  {
    "title": "Interpretable Affordance Detection on 3D Point Clouds with Probabilistic Prototypes",
    "url": "http://arxiv.org/abs/2504.18355v1",
    "arxiv_id": "2504.18355v1",
    "authors": [
      "Maximilian Xiling Li",
      "Korbinian Rudolf",
      "Nils Blank",
      "Rudolf Lioutikov"
    ],
    "published": "2025-04-25T13:52:39+00:00",
    "summary": "Robotic agents need to understand how to interact with objects in their environment, both autonomously and during human-robot interactions. Affordance detection on 3D point clouds, which identifies object regions that allow specific interactions, has traditionally relied on deep learning models like PointNet++, DGCNN, or PointTransformerV3. However, these models operate as black boxes, offering no insight into their decision-making processes. Prototypical Learning methods, such as ProtoPNet, provide an interpretable alternative to black-box models by employing a \"this looks like that\" case-based reasoning approach. However, they have been primarily applied to image-based tasks. In this work, we apply prototypical learning to models for affordance detection on 3D point clouds. Experiments on the 3D-AffordanceNet benchmark dataset show that prototypical models achieve competitive performance with state-of-the-art black-box models and offer inherent interpretability. This makes prototypical models a promising candidate for human-robot interaction scenarios that require increased trust and safety."
  },
  {
    "title": "Unifying Direct and Indirect Learning for Safe Control of Linear Systems",
    "url": "http://arxiv.org/abs/2504.18331v1",
    "arxiv_id": "2504.18331v1",
    "authors": [
      "Amir Modares",
      "Niyousha Ghiasi",
      "Bahare Kiumarsi",
      "Hamidreza Modares"
    ],
    "published": "2025-04-25T13:17:58+00:00",
    "summary": "This paper aims to learn safe controllers for uncertain discrete-time linear systems under disturbances while achieving the following two crucial goals: 1) integration of different sources of information (i.e., prior information in terms of physical knowledge and posterior information in terms of streaming data), and 2) unifying direct learning with indirect learning. These goals are achieved by representing a parametrized data-driven constrained matrix zonotope form of closed-loop systems that is conformant to prior knowledge. To this end, we first leverage collected data to characterize closed-loop systems by a matrix zonotope and then show that the explainability of these closed-loop systems by prior knowledge can be formalized by adding an equality conformity constraint, which refines the matrix zonotope obtained by data to a constrained matrix zonotope. The prior knowledge is further refined by conforming it to the set of models obtained from a novel zonotope-based system identifier. The source of data used for zonotope-based system identification can be different than the one used for closed-loop representation, allowing to perform transfer learning and online adaptation to new data. The parametrized closed-loop set of systems is then leveraged to directly learn a controller that robustly imposes safety on the closed-loop system. We consider both polytope and zonotope safe sets and provide set inclusion conditions using linear programming to impose safety through {\\lambda}-contractivity. For polytope safe sets, a primal-dual optimization is developed to formalize a linear programming optimization that certifies the set inclusion. For zonotope safe sets, the constrained zonotope set of all next states is formed, and set inclusion is achieved by ensuring the inclusion of this constrained zonotope in a {\\lambda}-scaled level set of the safe set."
  },
  {
    "title": "AI Safety Assurance for Automated Vehicles: A Survey on Research, Standardization, Regulation",
    "url": "http://arxiv.org/abs/2504.18328v1",
    "arxiv_id": "2504.18328v1",
    "authors": [
      "Lars Ullrich",
      "Michael Buchholz",
      "Klaus Dietmayer",
      "Knut Graichen"
    ],
    "published": "2025-04-25T13:14:06+00:00",
    "summary": "Assuring safety of artificial intelligence (AI) applied to safety-critical systems is of paramount importance. Especially since research in the field of automated driving shows that AI is able to outperform classical approaches, to handle higher complexities, and to reach new levels of autonomy. At the same time, the safety assurance required for the use of AI in such safety-critical systems is still not in place. Due to the dynamic and far-reaching nature of the technology, research on safeguarding AI is being conducted in parallel to AI standardization and regulation. The parallel progress necessitates simultaneous consideration in order to carry out targeted research and development of AI systems in the context of automated driving. Therefore, in contrast to existing surveys that focus primarily on research aspects, this paper considers research, standardization and regulation in a concise way. Accordingly, the survey takes into account the interdependencies arising from the triplet of research, standardization and regulation in a forward-looking perspective and anticipates and discusses open questions and possible future directions. In this way, the survey ultimately serves to provide researchers and safety experts with a compact, holistic perspective that discusses the current status, emerging trends, and possible future developments."
  },
  {
    "title": "A comprehensive review of classifier probability calibration metrics",
    "url": "http://arxiv.org/abs/2504.18278v1",
    "arxiv_id": "2504.18278v1",
    "authors": [
      "Richard Oliver Lane"
    ],
    "published": "2025-04-25T11:44:44+00:00",
    "summary": "Probabilities or confidence values produced by artificial intelligence (AI) and machine learning (ML) models often do not reflect their true accuracy, with some models being under or over confident in their predictions. For example, if a model is 80% sure of an outcome, is it correct 80% of the time? Probability calibration metrics measure the discrepancy between confidence and accuracy, providing an independent assessment of model calibration performance that complements traditional accuracy metrics. Understanding calibration is important when the outputs of multiple systems are combined, for assurance in safety or business-critical contexts, and for building user trust in models. This paper provides a comprehensive review of probability calibration metrics for classifier and object detection models, organising them according to a number of different categorisations to highlight their relationships. We identify 82 major metrics, which can be grouped into four classifier families (point-based, bin-based, kernel or curve-based, and cumulative) and an object detection family. For each metric, we provide equations where available, facilitating implementation and comparison by future researchers."
  },
  {
    "title": "Depth-Constrained ASV Navigation with Deep RL and Limited Sensing",
    "url": "http://arxiv.org/abs/2504.18253v1",
    "arxiv_id": "2504.18253v1",
    "authors": [
      "Amirhossein Zhalehmehrabi",
      "Daniele Meli",
      "Francesco Dal Santo",
      "Francesco Trotti",
      "Alessandro Farinelli"
    ],
    "published": "2025-04-25T10:56:56+00:00",
    "summary": "Autonomous Surface Vehicles (ASVs) play a crucial role in maritime operations, yet their navigation in shallow-water environments remains challenging due to dynamic disturbances and depth constraints. Traditional navigation strategies struggle with limited sensor information, making safe and efficient operation difficult. In this paper, we propose a reinforcement learning (RL) framework for ASV navigation under depth constraints, where the vehicle must reach a target while avoiding unsafe areas with only a single depth measurement per timestep from a downward-facing Single Beam Echosounder (SBES). To enhance environmental awareness, we integrate Gaussian Process (GP) regression into the RL framework, enabling the agent to progressively estimate a bathymetric depth map from sparse sonar readings. This approach improves decision-making by providing a richer representation of the environment. Furthermore, we demonstrate effective sim-to-real transfer, ensuring that trained policies generalize well to real-world aquatic conditions. Experimental results validate our method's capability to improve ASV navigation performance while maintaining safety in challenging shallow-water environments."
  },
  {
    "title": "Learning to fuse: dynamic integration of multi-source data for accurate battery lifespan prediction",
    "url": "http://arxiv.org/abs/2504.18230v1",
    "arxiv_id": "2504.18230v1",
    "authors": [
      "He Shanxuan",
      "Lin Zuhong",
      "Yu Bolun",
      "Gao Xu",
      "Long Biao",
      "Yao Jingjing"
    ],
    "published": "2025-04-25T10:24:45+00:00",
    "summary": "Accurate prediction of lithium-ion battery lifespan is vital for ensuring operational reliability and reducing maintenance costs in applications like electric vehicles and smart grids. This study presents a hybrid learning framework for precise battery lifespan prediction, integrating dynamic multi-source data fusion with a stacked ensemble (SE) modeling approach. By leveraging heterogeneous datasets from the National Aeronautics and Space Administration (NASA), Center for Advanced Life Cycle Engineering (CALCE), MIT-Stanford-Toyota Research Institute (TRC), and nickel cobalt aluminum (NCA) chemistries, an entropy-based dynamic weighting mechanism mitigates variability across heterogeneous datasets. The SE model combines Ridge regression, long short-term memory (LSTM) networks, and eXtreme Gradient Boosting (XGBoost), effectively capturing temporal dependencies and nonlinear degradation patterns. It achieves a mean absolute error (MAE) of 0.0058, root mean square error (RMSE) of 0.0092, and coefficient of determination (R2) of 0.9839, outperforming established baseline models with a 46.2% improvement in R2 and an 83.2% reduction in RMSE. Shapley additive explanations (SHAP) analysis identifies differential discharge capacity (Qdlin) and temperature of measurement (Temp_m) as critical aging indicators. This scalable, interpretable framework enhances battery health management, supporting optimized maintenance and safety across diverse energy storage systems, thereby contributing to improved battery health management in energy storage systems."
  },
  {
    "title": "Reimagining Assistive Walkers: An Exploration of Challenges and Preferences in Older Adults",
    "url": "http://arxiv.org/abs/2504.18169v1",
    "arxiv_id": "2504.18169v1",
    "authors": [
      "Victory A. Aruona",
      "Sergio D. Sierra M.",
      "Nigel Harris",
      "Marcela Munera",
      "Carlos A. Cifuentes"
    ],
    "published": "2025-04-25T08:32:44+00:00",
    "summary": "The well-being of older adults relies significantly on maintaining balance and mobility. As physical ability declines, older adults often accept the need for assistive devices. However, existing walkers frequently fail to consider user preferences, leading to perceptions of imposition and reduced acceptance. This research explores the challenges faced by older adults, caregivers, and healthcare professionals when using walkers, assesses their perceptions, and identifies their needs and preferences. A holistic approach was employed, using tailored perception questionnaires for older adults (24 participants), caregivers (30 participants), and healthcare professionals (27 participants), all of whom completed the survey. Over 50% of caregivers and healthcare professionals displayed good knowledge, positive attitudes, and effective practices regarding walkers. However, over 30% of participants perceived current designs as fall risks, citing the need for significant upper body strength, potentially affecting safety and movement. More than 50% highlighted the importance of incorporating fall detection, ergonomic designs, noise reduction, and walker ramps to better meet user needs and preferences."
  },
  {
    "title": "Combating the Bucket Effect:Multi-Knowledge Alignment for Medication Recommendation",
    "url": "http://arxiv.org/abs/2504.18096v1",
    "arxiv_id": "2504.18096v1",
    "authors": [
      "Xiang Li",
      "Haixu Ma",
      "Guanyong Wu",
      "Shi Mu",
      "Chen Li",
      "Shunpan Liang"
    ],
    "published": "2025-04-25T05:47:15+00:00",
    "summary": "Medication recommendation is crucial in healthcare, offering effective treatments based on patient's electronic health records (EHR). Previous studies show that integrating more medication-related knowledge improves medication representation accuracy. However, not all medications encompass multiple types of knowledge data simultaneously. For instance, some medications provide only textual descriptions without structured data. This imbalance in data availability limits the performance of existing models, a challenge we term the \"bucket effect\" in medication recommendation. Our data analysis uncovers the severity of the \"bucket effect\" in medication recommendation. To fill this gap, we introduce a cross-modal medication encoder capable of seamlessly aligning data from different modalities and propose a medication recommendation framework to integrate Multiple types of Knowledge, named MKMed. Specifically, we first pre-train a cross-modal encoder with contrastive learning on five knowledge modalities, aligning them into a unified space. Then, we combine the multi-knowledge medication representations with patient records for recommendations. Extensive experiments on the MIMIC-III and MIMIC-IV datasets demonstrate that MKMed mitigates the \"bucket effect\" in data, and significantly outperforms state-of-the-art baselines in recommendation accuracy and safety."
  },
  {
    "title": "DREAM: Disentangling Risks to Enhance Safety Alignment in Multimodal Large Language Models",
    "url": "http://arxiv.org/abs/2504.18053v1",
    "arxiv_id": "2504.18053v1",
    "authors": [
      "Jianyu Liu",
      "Hangyu Guo",
      "Ranjie Duan",
      "Xingyuan Bu",
      "Yancheng He",
      "Shilong Li",
      "Hui Huang",
      "Jiaheng Liu",
      "Yucheng Wang",
      "Chenchen Jing",
      "Xingwei Qu",
      "Xiao Zhang",
      "Yingshui Tan",
      "Yanan Wu",
      "Jihao Gu",
      "Yangguang Li",
      "Jianke Zhu"
    ],
    "published": "2025-04-25T03:54:24+00:00",
    "summary": "Multimodal Large Language Models (MLLMs) pose unique safety challenges due to their integration of visual and textual data, thereby introducing new dimensions of potential attacks and complex risk combinations. In this paper, we begin with a detailed analysis aimed at disentangling risks through step-by-step reasoning within multimodal inputs. We find that systematic multimodal risk disentanglement substantially enhances the risk awareness of MLLMs. Via leveraging the strong discriminative abilities of multimodal risk disentanglement, we further introduce \\textbf{DREAM} (\\textit{\\textbf{D}isentangling \\textbf{R}isks to \\textbf{E}nhance Safety \\textbf{A}lignment in \\textbf{M}LLMs}), a novel approach that enhances safety alignment in MLLMs through supervised fine-tuning and iterative Reinforcement Learning from AI Feedback (RLAIF). Experimental results show that DREAM significantly boosts safety during both inference and training phases without compromising performance on normal tasks (namely oversafety), achieving a 16.17\\% improvement in the SIUO safe\\&effective score compared to GPT-4V. The data and code are available at https://github.com/Kizna1ver/DREAM."
  },
  {
    "title": "RAG LLMs are Not Safer: A Safety Analysis of Retrieval-Augmented Generation for Large Language Models",
    "url": "http://arxiv.org/abs/2504.18041v1",
    "arxiv_id": "2504.18041v1",
    "authors": [
      "Bang An",
      "Shiyue Zhang",
      "Mark Dredze"
    ],
    "published": "2025-04-25T03:25:18+00:00",
    "summary": "Efforts to ensure the safety of large language models (LLMs) include safety fine-tuning, evaluation, and red teaming. However, despite the widespread use of the Retrieval-Augmented Generation (RAG) framework, AI safety work focuses on standard LLMs, which means we know little about how RAG use cases change a model's safety profile. We conduct a detailed comparative analysis of RAG and non-RAG frameworks with eleven LLMs. We find that RAG can make models less safe and change their safety profile. We explore the causes of this change and find that even combinations of safe models with safe documents can cause unsafe generations. In addition, we evaluate some existing red teaming methods for RAG settings and show that they are less effective than when used for non-RAG settings. Our work highlights the need for safety research and red-teaming methods specifically tailored for RAG LLMs."
  },
  {
    "title": "Joint Resource Estimation and Trajectory Optimization for eVTOL-involved CR network: A Monte Carlo Tree Search-based Approach",
    "url": "http://arxiv.org/abs/2504.18031v1",
    "arxiv_id": "2504.18031v1",
    "authors": [
      "Kai Xiong",
      "Chenxin Yang",
      "Yujie Qin",
      "Chau Yuen"
    ],
    "published": "2025-04-25T02:48:48+00:00",
    "summary": "Electric Vertical Take-Off and Landing (eVTOL) aircraft, pivotal to Advanced Air Mobility (AAM), are emerging as a transformative transportation paradigm with the potential to redefine urban and regional mobility. While these systems offer unprecedented efficiency in transporting people and goods, they rely heavily on computation capability, safety-critical operations such as real-time navigation, environmental sensing, and trajectory tracking--necessitating robust offboard computational support. A widely adopted solution involves offloading these tasks to terrestrial base stations (BSs) along the flight path. However, air-to-ground connectivity is often constrained by spectrum conflicts with terrestrial users, which poses a significant challenge to maintaining reliable task execution. Cognitive radio (CR) techniques offer promising capabilities for dynamic spectrum access, making them a natural fit for addressing this issue. Existing studies often overlook the time-varying nature of BS resources, such as spectrum availability and CPU cycles, which leads to inaccurate trajectory planning, suboptimal offloading success rates, excessive energy consumption, and operational delays. To address these challenges, we propose a trajectory optimization framework for eVTOL swarms that maximizes task offloading success probability while minimizing both energy consumption and resource competition (e.g., spectrum and CPU cycles) with primary terrestrial users. The proposed algorithm integrates a Multi-Armed Bandit (MAB) model to dynamically estimate BS resource availability and a Monte Carlo Tree Search (MCTS) algorithm to determine optimal offloading decisions, selecting both the BSs and access time windows that align with energy and temporal constraints."
  },
  {
    "title": "Virtual Roads, Smarter Safety: A Digital Twin Framework for Mixed Autonomous Traffic Safety Analysis",
    "url": "http://arxiv.org/abs/2504.17968v1",
    "arxiv_id": "2504.17968v1",
    "authors": [
      "Hao Zhang",
      "Ximin Yue",
      "Kexin Tian",
      "Sixu Li",
      "Keshu Wu",
      "Zihao Li",
      "Dominique Lord",
      "Yang Zhou"
    ],
    "published": "2025-04-24T22:27:59+00:00",
    "summary": "This paper presents a digital-twin platform for active safety analysis in mixed traffic environments. The platform is built using a multi-modal data-enabled traffic environment constructed from drone-based aerial LiDAR, OpenStreetMap, and vehicle sensor data (e.g., GPS and inclinometer readings). High-resolution 3D road geometries are generated through AI-powered semantic segmentation and georeferencing of aerial LiDAR data. To simulate real-world driving scenarios, the platform integrates the CAR Learning to Act (CARLA) simulator, Simulation of Urban MObility (SUMO) traffic model, and NVIDIA PhysX vehicle dynamics engine. CARLA provides detailed micro-level sensor and perception data, while SUMO manages macro-level traffic flow. NVIDIA PhysX enables accurate modeling of vehicle behaviors under diverse conditions, accounting for mass distribution, tire friction, and center of mass. This integrated system supports high-fidelity simulations that capture the complex interactions between autonomous and conventional vehicles. Experimental results demonstrate the platform's ability to reproduce realistic vehicle dynamics and traffic scenarios, enhancing the analysis of active safety measures. Overall, the proposed framework advances traffic safety research by enabling in-depth, physics-informed evaluation of vehicle behavior in dynamic and heterogeneous traffic environments."
  },
  {
    "title": "LLM Agent Swarm for Hypothesis-Driven Drug Discovery",
    "url": "http://arxiv.org/abs/2504.17967v1",
    "arxiv_id": "2504.17967v1",
    "authors": [
      "Kevin Song",
      "Andrew Trotter",
      "Jake Y. Chen"
    ],
    "published": "2025-04-24T22:27:50+00:00",
    "summary": "Drug discovery remains a formidable challenge: more than 90 percent of candidate molecules fail in clinical evaluation, and development costs often exceed one billion dollars per approved therapy. Disparate data streams, from genomics and transcriptomics to chemical libraries and clinical records, hinder coherent mechanistic insight and slow progress. Meanwhile, large language models excel at reasoning and tool integration but lack the modular specialization and iterative memory required for regulated, hypothesis-driven workflows. We introduce PharmaSwarm, a unified multi-agent framework that orchestrates specialized LLM \"agents\" to propose, validate, and refine hypotheses for novel drug targets and lead compounds. Each agent accesses dedicated functionality--automated genomic and expression analysis; a curated biomedical knowledge graph; pathway enrichment and network simulation; interpretable binding affinity prediction--while a central Evaluator LLM continuously ranks proposals by biological plausibility, novelty, in silico efficacy, and safety. A shared memory layer captures validated insights and fine-tunes underlying submodels over time, yielding a self-improving system. Deployable on low-code platforms or Kubernetes-based microservices, PharmaSwarm supports literature-driven discovery, omics-guided target identification, and market-informed repurposing. We also describe a rigorous four-tier validation pipeline spanning retrospective benchmarking, independent computational assays, experimental testing, and expert user studies to ensure transparency, reproducibility, and real-world impact. By acting as an AI copilot, PharmaSwarm can accelerate translational research and deliver high-confidence hypotheses more efficiently than traditional pipelines."
  },
  {
    "title": "Integrating Learning-Based Manipulation and Physics-Based Locomotion for Whole-Body Badminton Robot Control",
    "url": "http://arxiv.org/abs/2504.17771v1",
    "arxiv_id": "2504.17771v1",
    "authors": [
      "Haochen Wang",
      "Zhiwei Shi",
      "Chengxi Zhu",
      "Yafei Qiao",
      "Cheng Zhang",
      "Fan Yang",
      "Pengjie Ren",
      "Lan Lu",
      "Dong Xuan"
    ],
    "published": "2025-04-24T17:46:29+00:00",
    "summary": "Learning-based methods, such as imitation learning (IL) and reinforcement learning (RL), can produce excel control policies over challenging agile robot tasks, such as sports robot. However, no existing work has harmonized learning-based policy with model-based methods to reduce training complexity and ensure the safety and stability for agile badminton robot control. In this paper, we introduce \\ourmethod, a novel hybrid control system for agile badminton robots. Specifically, we propose a model-based strategy for chassis locomotion which provides a base for arm policy. We introduce a physics-informed ``IL+RL'' training framework for learning-based arm policy. In this train framework, a model-based strategy with privileged information is used to guide arm policy training during both IL and RL phases. In addition, we train the critic model during IL phase to alleviate the performance drop issue when transitioning from IL to RL. We present results on our self-engineered badminton robot, achieving 94.5% success rate against the serving machine and 90.7% success rate against human players. Our system can be easily generalized to other agile mobile manipulation tasks such as agile catching and table tennis. Our project website: https://dreamstarring.github.io/HAMLET/."
  },
  {
    "title": "Integrating Learning-Based Manipulation and Physics-Based Locomotion for Whole-Body Badminton Robot Control",
    "url": "http://arxiv.org/abs/2504.17771v2",
    "arxiv_id": "2504.17771v2",
    "authors": [
      "Haochen Wang",
      "Zhiwei Shi",
      "Chengxi Zhu",
      "Yafei Qiao",
      "Cheng Zhang",
      "Fan Yang",
      "Pengjie Ren",
      "Lan Lu",
      "Dong Xuan"
    ],
    "published": "2025-04-24T17:46:29+00:00",
    "summary": "Learning-based methods, such as imitation learning (IL) and reinforcement learning (RL), can produce excel control policies over challenging agile robot tasks, such as sports robot. However, no existing work has harmonized learning-based policy with model-based methods to reduce training complexity and ensure the safety and stability for agile badminton robot control. In this paper, we introduce Hamlet, a novel hybrid control system for agile badminton robots. Specifically, we propose a model-based strategy for chassis locomotion which provides a base for arm policy. We introduce a physics-informed \"IL+RL\" training framework for learning-based arm policy. In this train framework, a model-based strategy with privileged information is used to guide arm policy training during both IL and RL phases. In addition, we train the critic model during IL phase to alleviate the performance drop issue when transitioning from IL to RL. We present results on our self-engineered badminton robot, achieving 94.5% success rate against the serving machine and 90.7% success rate against human players. Our system can be easily generalized to other agile mobile manipulation tasks such as agile catching and table tennis. Our project website: https://dreamstarring.github.io/HAMLET/."
  },
  {
    "title": "Conformal Segmentation in Industrial Surface Defect Detection with Statistical Guarantees",
    "url": "http://arxiv.org/abs/2504.17721v1",
    "arxiv_id": "2504.17721v1",
    "authors": [
      "Cheng Shen",
      "Yuewei Liu"
    ],
    "published": "2025-04-24T16:33:56+00:00",
    "summary": "In industrial settings, surface defects on steel can significantly compromise its service life and elevate potential safety risks. Traditional defect detection methods predominantly rely on manual inspection, which suffers from low efficiency and high costs. Although automated defect detection approaches based on Convolutional Neural Networks(e.g., Mask R-CNN) have advanced rapidly, their reliability remains challenged due to data annotation uncertainties during deep model training and overfitting issues. These limitations may lead to detection deviations when processing the given new test samples, rendering automated detection processes unreliable. To address this challenge, we first evaluate the detection model's practical performance through calibration data that satisfies the independent and identically distributed (i.i.d) condition with test data. Specifically, we define a loss function for each calibration sample to quantify detection error rates, such as the complement of recall rate and false discovery rate. Subsequently, we derive a statistically rigorous threshold based on a user-defined risk level to identify high-probability defective pixels in test images, thereby constructing prediction sets (e.g., defect regions). This methodology ensures that the expected error rate (mean error rate) on the test set remains strictly bounced by the predefined risk level. Additionally, we observe a negative correlation between the average prediction set size and the risk level on the test set, establishing a statistically rigorous metric for assessing detection model uncertainty. Furthermore, our study demonstrates robust and efficient control over the expected test set error rate across varying calibration-to-test partitioning ratios, validating the method's adaptability and operational effectiveness."
  },
  {
    "title": "Safety in Large Reasoning Models: A Survey",
    "url": "http://arxiv.org/abs/2504.17704v1",
    "arxiv_id": "2504.17704v1",
    "authors": [
      "Cheng Wang",
      "Yue Liu",
      "Baolong Li",
      "Duzhen Zhang",
      "Zhongzhi Li",
      "Junfeng Fang"
    ],
    "published": "2025-04-24T16:11:01+00:00",
    "summary": "Large Reasoning Models (LRMs) have exhibited extraordinary prowess in tasks like mathematics and coding, leveraging their advanced reasoning capabilities. Nevertheless, as these capabilities progress, significant concerns regarding their vulnerabilities and safety have arisen, which can pose challenges to their deployment and application in real-world settings. This paper presents a comprehensive survey of LRMs, meticulously exploring and summarizing the newly emerged safety risks, attacks, and defense strategies. By organizing these elements into a detailed taxonomy, this work aims to offer a clear and structured understanding of the current safety landscape of LRMs, facilitating future research and development to enhance the security and reliability of these powerful models."
  },
  {
    "title": "Using mathematical models of heart cells to assess the safety of new pharmaceutical drugs",
    "url": "http://arxiv.org/abs/2504.17694v1",
    "arxiv_id": "2504.17694v1",
    "authors": [
      "Gary R. Mirams"
    ],
    "published": "2025-04-24T16:03:06+00:00",
    "summary": "Many drugs have been withdrawn from the market worldwide, at a cost of billions of dollars, because of patient fatalities due to them unexpectedly disturbing heart rhythm. Even drugs for ailments as mild as hay fever have been withdrawn due to an unacceptable increase in risk of these heart rhythm disturbances. Consequently, the whole pharmaceutical industry expends a huge effort in checking all new drugs for any unwanted side effects on the heart. The predominant root cause has been identified as drug molecules blocking ionic current flows in the heart. Block of individual types of ionic currents can now be measured experimentally at an early stage of drug development, and this is the standard screening approach for a number of ion currents in many large pharmaceutical companies. However, clinical risk is a complex function of the degree of block of many different types of cardiac ion currents, and this is difficult to understand by looking at results of these screens independently. By using ordinary differential equation models for the electrical activity of heart cells (electrophysiology models) we can integrate information from different types of currents, to predict the effect on whole heart cells and subsequent risk of side effects. The resulting simulations can provide a more accurate summary of the risk of a drug earlier in development and hence more cheaply than the pre-existing approaches."
  },
  {
    "title": "Data-Driven Calibration of Prediction Sets in Large Vision-Language Models Based on Inductive Conformal Prediction",
    "url": "http://arxiv.org/abs/2504.17671v1",
    "arxiv_id": "2504.17671v1",
    "authors": [
      "Yuanchang Ye",
      "Weiyan Wen"
    ],
    "published": "2025-04-24T15:39:46+00:00",
    "summary": "This study addresses the critical challenge of hallucination mitigation in Large Vision-Language Models (LVLMs) for Visual Question Answering (VQA) tasks through a Split Conformal Prediction (SCP) framework. While LVLMs excel in multi-modal reasoning, their outputs often exhibit hallucinated content with high confidence, posing risks in safety-critical applications. We propose a model-agnostic uncertainty quantification method that integrates dynamic threshold calibration and cross-modal consistency verification. By partitioning data into calibration and test sets, the framework computes nonconformity scores to construct prediction sets with statistical guarantees under user-defined risk levels ($\\alpha$). Key innovations include: (1) rigorous control of \\textbf{marginal coverage} to ensure empirical error rates remain strictly below $\\alpha$; (2) dynamic adjustment of prediction set sizes inversely with $\\alpha$, filtering low-confidence outputs; (3) elimination of prior distribution assumptions and retraining requirements. Evaluations on benchmarks (ScienceQA, MMMU) with eight LVLMs demonstrate that SCP enforces theoretical guarantees across all $\\alpha$ values. The framework achieves stable performance across varying calibration-to-test split ratios, underscoring its robustness for real-world deployment in healthcare, autonomous systems, and other safety-sensitive domains. This work bridges the gap between theoretical reliability and practical applicability in multi-modal AI systems, offering a scalable solution for hallucination detection and uncertainty-aware decision-making."
  },
  {
    "title": "Data-Driven Calibration of Prediction Sets in Large Vision-Language Models Based on Inductive Conformal Prediction",
    "url": "http://arxiv.org/abs/2504.17671v2",
    "arxiv_id": "2504.17671v2",
    "authors": [
      "Yuanchang Ye",
      "Weiyan Wen"
    ],
    "published": "2025-04-24T15:39:46+00:00",
    "summary": "This study addresses the critical challenge of hallucination mitigation in Large Vision-Language Models (LVLMs) for Visual Question Answering (VQA) tasks through a Split Conformal Prediction (SCP) framework. While LVLMs excel in multi-modal reasoning, their outputs often exhibit hallucinated content with high confidence, posing risks in safety-critical applications. We propose a model-agnostic uncertainty quantification method that integrates dynamic threshold calibration and cross-modal consistency verification. By partitioning data into calibration and test sets, the framework computes nonconformity scores to construct prediction sets with statistical guarantees under user-defined risk levels ($\\alpha$). Key innovations include: (1) rigorous control of \\textbf{marginal coverage} to ensure empirical error rates remain strictly below $\\alpha$; (2) dynamic adjustment of prediction set sizes inversely with $\\alpha$, filtering low-confidence outputs; (3) elimination of prior distribution assumptions and retraining requirements. Evaluations on benchmarks (ScienceQA, MMMU) with eight LVLMs demonstrate that SCP enforces theoretical guarantees across all $\\alpha$ values. The framework achieves stable performance across varying calibration-to-test split ratios, underscoring its robustness for real-world deployment in healthcare, autonomous systems, and other safety-sensitive domains. This work bridges the gap between theoretical reliability and practical applicability in multi-modal AI systems, offering a scalable solution for hallucination detection and uncertainty-aware decision-making."
  },
  {
    "title": "Unifying Complementarity Constraints and Control Barrier Functions for Safe Whole-Body Robot Control",
    "url": "http://arxiv.org/abs/2504.17647v1",
    "arxiv_id": "2504.17647v1",
    "authors": [
      "Rafael I. Cabral Muchacho",
      "Riddhiman Laha",
      "Florian T. Pokorny",
      "Luis F. C. Figueredo",
      "Nilanjan Chakraborty"
    ],
    "published": "2025-04-24T15:17:26+00:00",
    "summary": "Safety-critical whole-body robot control demands reactive methods that ensure collision avoidance in real-time. Complementarity constraints and control barrier functions (CBF) have emerged as core tools for ensuring such safety constraints, and each represents a well-developed field. Despite addressing similar problems, their connection remains largely unexplored. This paper bridges this gap by formally proving the equivalence between these two methodologies for sampled-data, first-order systems, considering both single and multiple constraint scenarios. By demonstrating this equivalence, we provide a unified perspective on these techniques. This unification has theoretical and practical implications, facilitating the cross-application of robustness guarantees and algorithmic improvements between complementarity and CBF frameworks. We discuss these synergistic benefits and motivate future work in the comparison of the methods in more general cases."
  },
  {
    "title": "Portability of Optimizations from SC to TSO",
    "url": "http://arxiv.org/abs/2504.17646v1",
    "arxiv_id": "2504.17646v1",
    "authors": [
      "Akshay Gopalakrishnan",
      "Clark Verbrugge"
    ],
    "published": "2025-04-24T15:16:17+00:00",
    "summary": "It is well recognized that the safety of compiler optimizations is at risk in a concurrent context. Existing approaches primarily rely on context-free thread-local guarantees, and prohibit optimizations that introduce a data-race. However, compilers utilize global context-specific information, exposing safe optimizations that may violate such guarantees as well as introduce a race. Such optimizations need to individually be proven safe for each language model. An alternate approach to this would be proving them safe for an intuitive model (like interleaving semantics), and then determine their portability across other concurrent models. In this paper, we address this problem of porting across models of concurrency. We first identify a global guarantee on optimizations portable from Sequential Consistency (SC) to Total Store Order (TSO). Our guarantee is in the form of constraints specifying the syntactic changes an optimization must not incur. We then show these constraints correlate to prohibiting the introduction of triangular races, a subset of data-race relevant to TSO. We conclude by showing how such race inducing optimizations relate to porting across Strong Release Acquire (SRA), a known causally consistent memory model."
  },
  {
    "title": "Safe to Stay: Psychological Safety Sustains Participation in Pull-based Open Source Projects",
    "url": "http://arxiv.org/abs/2504.17510v1",
    "arxiv_id": "2504.17510v1",
    "authors": [
      "Emeralda Sesari",
      "Federica Sarro",
      "Ayushi Rastogi"
    ],
    "published": "2025-04-24T12:54:30+00:00",
    "summary": "Psychological safety is the belief that team members can speak up or make mistakes without fear of negative consequences. While it is recognized as important in traditional software teams, its role in open-source development remains understudied. Yet, open-source contributors often collaborate without formal roles or structures, where interpersonal relationship can make or break participation. In this study, we examine whether team-level psychological safety, inferred from code review activities, is associated with contributors' continued participation in open-source projects. Code review is a central and collaborative activity in modern software development, which offers a rich context for observing team interactions. Based on 60,684 pull requests, we construct a psychological safety index using cues such as merge decisions, comment activity, interaction diversity, and mentions. We analyze its relationship with contributors' short-term (after 1 year) and long-term (after 4-5 years) sustained participation using three logistic regression models. Our findings show that contributors are more likely to remain active in repositories with higher levels of psychological safety. Psychological safety is positively associated with both short-term and future sustained participation. However, when prior participation is included, it becomes the stronger predictor of future sustained participation, while the effect of psychological safety becomes smaller. This study introduces a scalable approach to study psychological safety through pull request data and provides new evidence that it matters in open-source development."
  },
  {
    "title": "Assessing the Capability of Large Language Models for Domain-Specific Ontology Generation",
    "url": "http://arxiv.org/abs/2504.17402v1",
    "arxiv_id": "2504.17402v1",
    "authors": [
      "Anna Sofia Lippolis",
      "Mohammad Javad Saeedizade",
      "Robin Keskisarkka",
      "Aldo Gangemi",
      "Eva Blomqvist",
      "Andrea Giovanni Nuzzolese"
    ],
    "published": "2025-04-24T09:47:14+00:00",
    "summary": "Large Language Models (LLMs) have shown significant potential for ontology engineering. However, it is still unclear to what extent they are applicable to the task of domain-specific ontology generation. In this study, we explore the application of LLMs for automated ontology generation and evaluate their performance across different domains. Specifically, we investigate the generalizability of two state-of-the-art LLMs, DeepSeek and o1-preview, both equipped with reasoning capabilities, by generating ontologies from a set of competency questions (CQs) and related user stories. Our experimental setup comprises six distinct domains carried out in existing ontology engineering projects and a total of 95 curated CQs designed to test the models' reasoning for ontology engineering. Our findings show that with both LLMs, the performance of the experiments is remarkably consistent across all domains, indicating that these methods are capable of generalizing ontology generation tasks irrespective of the domain. These results highlight the potential of LLM-based approaches in achieving scalable and domain-agnostic ontology construction and lay the groundwork for further research into enhancing automated reasoning and knowledge representation techniques."
  },
  {
    "title": "Highly Accurate and Diverse Traffic Data: The DeepScenario Open 3D Dataset",
    "url": "http://arxiv.org/abs/2504.17371v1",
    "arxiv_id": "2504.17371v1",
    "authors": [
      "Oussema Dhaouadi",
      "Johannes Meier",
      "Luca Wahl",
      "Jacques Kaiser",
      "Luca Scalerandi",
      "Nick Wandelburg",
      "Zhuolun Zhou",
      "Nijanthan Berinpanathan",
      "Holger Banzhaf",
      "Daniel Cremers"
    ],
    "published": "2025-04-24T08:43:48+00:00",
    "summary": "Accurate 3D trajectory data is crucial for advancing autonomous driving. Yet, traditional datasets are usually captured by fixed sensors mounted on a car and are susceptible to occlusion. Additionally, such an approach can precisely reconstruct the dynamic environment in the close vicinity of the measurement vehicle only, while neglecting objects that are further away. In this paper, we introduce the DeepScenario Open 3D Dataset (DSC3D), a high-quality, occlusion-free dataset of 6 degrees of freedom bounding box trajectories acquired through a novel monocular camera drone tracking pipeline. Our dataset includes more than 175,000 trajectories of 14 types of traffic participants and significantly exceeds existing datasets in terms of diversity and scale, containing many unprecedented scenarios such as complex vehicle-pedestrian interaction on highly populated urban streets and comprehensive parking maneuvers from entry to exit. DSC3D dataset was captured in five various locations in Europe and the United States and include: a parking lot, a crowded inner-city, a steep urban intersection, a federal highway, and a suburban intersection. Our 3D trajectory dataset aims to enhance autonomous driving systems by providing detailed environmental 3D representations, which could lead to improved obstacle interactions and safety. We demonstrate its utility across multiple applications including motion prediction, motion planning, scenario mining, and generative reactive traffic agents. Our interactive online visualization platform and the complete dataset are publicly available at app.deepscenario.com, facilitating research in motion prediction, behavior modeling, and safety validation."
  },
  {
    "title": "Highly Accurate and Diverse Traffic Data: The DeepScenario Open 3D Dataset",
    "url": "http://arxiv.org/abs/2504.17371v2",
    "arxiv_id": "2504.17371v2",
    "authors": [
      "Oussema Dhaouadi",
      "Johannes Meier",
      "Luca Wahl",
      "Jacques Kaiser",
      "Luca Scalerandi",
      "Nick Wandelburg",
      "Zhuolun Zhou",
      "Nijanthan Berinpanathan",
      "Holger Banzhaf",
      "Daniel Cremers"
    ],
    "published": "2025-04-24T08:43:48+00:00",
    "summary": "Accurate 3D trajectory data is crucial for advancing autonomous driving. Yet, traditional datasets are usually captured by fixed sensors mounted on a car and are susceptible to occlusion. Additionally, such an approach can precisely reconstruct the dynamic environment in the close vicinity of the measurement vehicle only, while neglecting objects that are further away. In this paper, we introduce the DeepScenario Open 3D Dataset (DSC3D), a high-quality, occlusion-free dataset of 6 degrees of freedom bounding box trajectories acquired through a novel monocular camera drone tracking pipeline. Our dataset includes more than 175,000 trajectories of 14 types of traffic participants and significantly exceeds existing datasets in terms of diversity and scale, containing many unprecedented scenarios such as complex vehicle-pedestrian interaction on highly populated urban streets and comprehensive parking maneuvers from entry to exit. DSC3D dataset was captured in five various locations in Europe and the United States and include: a parking lot, a crowded inner-city, a steep urban intersection, a federal highway, and a suburban intersection. Our 3D trajectory dataset aims to enhance autonomous driving systems by providing detailed environmental 3D representations, which could lead to improved obstacle interactions and safety. We demonstrate its utility across multiple applications including motion prediction, motion planning, scenario mining, and generative reactive traffic agents. Our interactive online visualization platform and the complete dataset are publicly available at https://app.deepscenario.com, facilitating research in motion prediction, behavior modeling, and safety validation."
  },
  {
    "title": "AUTHENTICATION: Identifying Rare Failure Modes in Autonomous Vehicle Perception Systems using Adversarially Guided Diffusion Models",
    "url": "http://arxiv.org/abs/2504.17179v1",
    "arxiv_id": "2504.17179v1",
    "authors": [
      "Mohammad Zarei",
      "Melanie A Jutras",
      "Eliana Evans",
      "Mike Tan",
      "Omid Aaramoon"
    ],
    "published": "2025-04-24T01:31:13+00:00",
    "summary": "Autonomous Vehicles (AVs) rely on artificial intelligence (AI) to accurately detect objects and interpret their surroundings. However, even when trained using millions of miles of real-world data, AVs are often unable to detect rare failure modes (RFMs). The problem of RFMs is commonly referred to as the \"long-tail challenge\", due to the distribution of data including many instances that are very rarely seen. In this paper, we present a novel approach that utilizes advanced generative and explainable AI techniques to aid in understanding RFMs. Our methods can be used to enhance the robustness and reliability of AVs when combined with both downstream model training and testing. We extract segmentation masks for objects of interest (e.g., cars) and invert them to create environmental masks. These masks, combined with carefully crafted text prompts, are fed into a custom diffusion model. We leverage the Stable Diffusion inpainting model guided by adversarial noise optimization to generate images containing diverse environments designed to evade object detection models and expose vulnerabilities in AI systems. Finally, we produce natural language descriptions of the generated RFMs that can guide developers and policymakers to improve the safety and reliability of AV systems."
  },
  {
    "title": "Opt-ODENet: A Neural ODE Framework with Differentiable QP Layers for Safe and Stable Control Design (longer version)",
    "url": "http://arxiv.org/abs/2504.17139v1",
    "arxiv_id": "2504.17139v1",
    "authors": [
      "Keyan Miao",
      "Liqun Zhao",
      "Han Wang",
      "Konstantinos Gatsis",
      "Antonis Papachristodoulou"
    ],
    "published": "2025-04-23T23:09:37+00:00",
    "summary": "Designing controllers that achieve task objectives while ensuring safety is a key challenge in control systems. This work introduces Opt-ODENet, a Neural ODE framework with a differentiable Quadratic Programming (QP) optimization layer to enforce constraints as hard requirements. Eliminating the reliance on nominal controllers or large datasets, our framework solves the optimal control problem directly using Neural ODEs. Stability and convergence are ensured through Control Lyapunov Functions (CLFs) in the loss function, while Control Barrier Functions (CBFs) embedded in the QP layer enforce real-time safety. By integrating the differentiable QP layer with Neural ODEs, we demonstrate compatibility with the adjoint method for gradient computation, enabling the learning of the CBF class-$\\mathcal{K}$ function and control network parameters. Experiments validate its effectiveness in balancing safety and performance."
  },
  {
    "title": "Steering the CensorShip: Uncovering Representation Vectors for LLM \"Thought\" Control",
    "url": "http://arxiv.org/abs/2504.17130v1",
    "arxiv_id": "2504.17130v1",
    "authors": [
      "Hannah Cyberey",
      "David Evans"
    ],
    "published": "2025-04-23T22:47:30+00:00",
    "summary": "Large language models (LLMs) have transformed the way we access information. These models are often tuned to refuse to comply with requests that are considered harmful and to produce responses that better align with the preferences of those who control the models. To understand how this \"censorship\" works. We use representation engineering techniques to study open-weights safety-tuned models. We present a method for finding a refusal--compliance vector that detects and controls the level of censorship in model outputs. We also analyze recent reasoning LLMs, distilled from DeepSeek-R1, and uncover an additional dimension of censorship through \"thought suppression\". We show a similar approach can be used to find a vector that suppresses the model's reasoning process, allowing us to remove censorship by applying the negative multiples of this vector"
  },
  {
    "title": "Steering the CensorShip: Uncovering Representation Vectors for LLM \"Thought\" Control",
    "url": "http://arxiv.org/abs/2504.17130v2",
    "arxiv_id": "2504.17130v2",
    "authors": [
      "Hannah Cyberey",
      "David Evans"
    ],
    "published": "2025-04-23T22:47:30+00:00",
    "summary": "Large language models (LLMs) have transformed the way we access information. These models are often tuned to refuse to comply with requests that are considered harmful and to produce responses that better align with the preferences of those who control the models. To understand how this \"censorship\" works. We use representation engineering techniques to study open-weights safety-tuned models. We present a method for finding a refusal--compliance vector that detects and controls the level of censorship in model outputs. We also analyze recent reasoning LLMs, distilled from DeepSeek-R1, and uncover an additional dimension of censorship through \"thought suppression\". We show a similar approach can be used to find a vector that suppresses the model's reasoning process, allowing us to remove censorship by applying the negative multiples of this vector. Our code is publicly available at: https://github.com/hannahxchen/llm-censorship-steering"
  },
  {
    "title": "Peer-Aware Cost Estimation in Nonlinear General-Sum Dynamic Games for Mutual Learning and Intent Inference",
    "url": "http://arxiv.org/abs/2504.17129v1",
    "arxiv_id": "2504.17129v1",
    "authors": [
      "Seyed Yousef Soltanian",
      "Wenlong Zhang"
    ],
    "published": "2025-04-23T22:47:20+00:00",
    "summary": "Human-robot interactions can be modeled as incomplete-information general-sum dynamic games since the objective functions of both agents are not explicitly known to each other. However, solving for equilibrium policies for such games presents a major challenge, especially if the games involve nonlinear underlying dynamics. To simplify the problem, existing work often assumes that one agent is an expert with complete information about its peer, which can lead to biased estimates and failures in coordination. To address this challenge, we propose a nonlinear peer-aware cost estimation (N-PACE) algorithm for general-sum dynamic games. In N-PACE, using iterative linear quadratic (LQ) approximation of the nonlinear general-sum game, each agent explicitly models the learning dynamics of its peer agent while inferring their objective functions, leading to unbiased fast learning in inferring the unknown objective function of the peer agent, which is critical for task completion and safety assurance. Additionally, we demonstrate how N-PACE enables \\textbf{intent communication} in such multi-agent systems by explicitly modeling the peer's learning dynamics."
  },
  {
    "title": "Discovering the Precursors of Traffic Breakdowns Using Spatiotemporal Graph Attribution Networks",
    "url": "http://arxiv.org/abs/2504.17109v1",
    "arxiv_id": "2504.17109v1",
    "authors": [
      "Zhaobin Mo",
      "Xiangyi Liao",
      "Dominik A. Karbowski",
      "Yanbing Wang"
    ],
    "published": "2025-04-23T21:40:23+00:00",
    "summary": "Understanding and predicting the precursors of traffic breakdowns is critical for improving road safety and traffic flow management. This paper presents a novel approach combining spatiotemporal graph neural networks (ST-GNNs) with Shapley values to identify and interpret traffic breakdown precursors. By extending Shapley explanation methods to a spatiotemporal setting, our proposed method bridges the gap between black-box neural network predictions and interpretable causes. We demonstrate the method on the Interstate-24 data, and identify that road topology and abrupt braking are major factors that lead to traffic breakdowns."
  },
  {
    "title": "Safety Pretraining: Toward the Next Generation of Safe AI",
    "url": "http://arxiv.org/abs/2504.16980v1",
    "arxiv_id": "2504.16980v1",
    "authors": [
      "Pratyush Maini",
      "Sachin Goyal",
      "Dylan Sam",
      "Alex Robey",
      "Yash Savani",
      "Yiding Jiang",
      "Andy Zou",
      "Zacharcy C. Lipton",
      "J. Zico Kolter"
    ],
    "published": "2025-04-23T17:58:08+00:00",
    "summary": "As large language models (LLMs) are increasingly deployed in high-stakes settings, the risk of generating harmful or toxic content remains a central challenge. Post-hoc alignment methods are brittle: once unsafe patterns are learned during pretraining, they are hard to remove. We present a data-centric pretraining framework that builds safety into the model from the start. Our contributions include: (i) a safety classifier trained on 10,000 GPT-4 labeled examples, used to filter 600B tokens; (ii) the largest synthetic safety dataset to date (100B tokens) generated via recontextualization of harmful web data; (iii) RefuseWeb and Moral Education datasets that convert harmful prompts into refusal dialogues and web-style educational material; (iv) Harmfulness-Tag annotations injected during pretraining to flag unsafe content and steer away inference from harmful generations; and (v) safety evaluations measuring base model behavior before instruction tuning. Our safety-pretrained models reduce attack success rates from 38.8% to 8.4% with no performance degradation on standard LLM safety benchmarks."
  },
  {
    "title": "Meta-Learning Online Dynamics Model Adaptation in Off-Road Autonomous Driving",
    "url": "http://arxiv.org/abs/2504.16923v1",
    "arxiv_id": "2504.16923v1",
    "authors": [
      "Jacob Levy",
      "Jason Gibson",
      "Bogdan Vlahov",
      "Erica Tevere",
      "Evangelos Theodorou",
      "David Fridovich-Keil",
      "Patrick Spieler"
    ],
    "published": "2025-04-23T17:51:36+00:00",
    "summary": "High-speed off-road autonomous driving presents unique challenges due to complex, evolving terrain characteristics and the difficulty of accurately modeling terrain-vehicle interactions. While dynamics models used in model-based control can be learned from real-world data, they often struggle to generalize to unseen terrain, making real-time adaptation essential. We propose a novel framework that combines a Kalman filter-based online adaptation scheme with meta-learned parameters to address these challenges. Offline meta-learning optimizes the basis functions along which adaptation occurs, as well as the adaptation parameters, while online adaptation dynamically adjusts the onboard dynamics model in real time for model-based control. We validate our approach through extensive experiments, including real-world testing on a full-scale autonomous off-road vehicle, demonstrating that our method outperforms baseline approaches in prediction accuracy, performance, and safety metrics, particularly in safety-critical scenarios. Our results underscore the effectiveness of meta-learned dynamics model adaptation, advancing the development of reliable autonomous systems capable of navigating diverse and unseen environments. Video is available at: https://youtu.be/cCKHHrDRQEA"
  },
  {
    "title": "Learning Verifiable Control Policies Using Relaxed Verification",
    "url": "http://arxiv.org/abs/2504.16879v1",
    "arxiv_id": "2504.16879v1",
    "authors": [
      "Puja Chaudhury",
      "Alexander Estornell",
      "Michael Everett"
    ],
    "published": "2025-04-23T16:54:35+00:00",
    "summary": "To provide safety guarantees for learning-based control systems, recent work has developed formal verification methods to apply after training ends. However, if the trained policy does not meet the specifications, or there is conservatism in the verification algorithm, establishing these guarantees may not be possible. Instead, this work proposes to perform verification throughout training to ultimately aim for policies whose properties can be evaluated throughout runtime with lightweight, relaxed verification algorithms. The approach is to use differentiable reachability analysis and incorporate new components into the loss function. Numerical experiments on a quadrotor model and unicycle model highlight the ability of this approach to lead to learned control policies that satisfy desired reach-avoid and invariance specifications."
  },
  {
    "title": "Hybrid Reinforcement Learning and Model Predictive Control for Adaptive Control of Hydrogen-Diesel Dual-Fuel Combustion",
    "url": "http://arxiv.org/abs/2504.16875v1",
    "arxiv_id": "2504.16875v1",
    "authors": [
      "Julian Bedei",
      "Murray McBain",
      "Charles Robert Koch",
      "Jakob Andert",
      "David Gordon"
    ],
    "published": "2025-04-23T16:51:49+00:00",
    "summary": "Reinforcement Learning (RL) and Machine Learning Integrated Model Predictive Control (ML-MPC) are promising approaches for optimizing hydrogen-diesel dual-fuel engine control, as they can effectively control multiple-input multiple-output systems and nonlinear processes. ML-MPC is advantageous for providing safe and optimal controls, ensuring the engine operates within predefined safety limits. In contrast, RL is distinguished by its adaptability to changing conditions through its learning-based approach. However, the practical implementation of either method alone poses challenges. RL requires high variance in control inputs during early learning phases, which can pose risks to the system by potentially executing unsafe actions, leading to mechanical damage. Conversely, ML-MPC relies on an accurate system model to generate optimal control inputs and has limited adaptability to system drifts, such as injector aging, which naturally occur in engine applications. To address these limitations, this study proposes a hybrid RL and ML-MPC approach that uses an ML-MPC framework while incorporating an RL agent to dynamically adjust the ML-MPC load tracking reference in response to changes in the environment. At the same time, the ML-MPC ensures that actions stay safe throughout the RL agent's exploration. To evaluate the effectiveness of this approach, fuel pressure is deliberately varied to introduce a model-plant mismatch between the ML-MPC and the engine test bench. The result of this mismatch is a root mean square error (RMSE) in indicated mean effective pressure of 0.57 bar when running the ML-MPC. The experimental results demonstrate that RL successfully adapts to changing boundary conditions by altering the tracking reference while ML-MPC ensures safe control inputs. The quantitative improvement in load tracking by implementing RL is an RSME of 0.44 bar."
  },
  {
    "title": "Improving Significant Wave Height Prediction Using Chronos Models",
    "url": "http://arxiv.org/abs/2504.16834v1",
    "arxiv_id": "2504.16834v1",
    "authors": [
      "Yilin Zhai",
      "Hongyuan Shi",
      "Chao Zhan",
      "Qing Wang",
      "Zaijin You",
      "Nan Wang"
    ],
    "published": "2025-04-23T15:56:28+00:00",
    "summary": "Accurate wave height prediction is critical for maritime safety and coastal resilience, yet conventional physics-based models and traditional machine learning methods face challenges in computational efficiency and nonlinear dynamics modeling. This study introduces Chronos, the first implementation of a large language model (LLM)-powered temporal architecture (Chronos) optimized for wave forecasting. Through advanced temporal pattern recognition applied to historical wave data from three strategically chosen marine zones in the Northwest Pacific basin, our framework achieves multimodal improvements: (1) 14.3% reduction in training time with 2.5x faster inference speed compared to PatchTST baselines, achieving 0.575 mean absolute scaled error (MASE) units; (2) superior short-term forecasting (1-24h) across comprehensive metrics; (3) sustained predictive leadership in extended-range forecasts (1-120h); and (4) demonstrated zero-shot capability maintaining median performance (rank 4/12) against specialized operational models. This LLM-enhanced temporal modeling paradigm establishes a new standard in wave prediction, offering both computationally efficient solutions and a transferable framework for complex geophysical systems modeling."
  },
  {
    "title": "Improving Significant Wave Height Prediction Using Chronos Models",
    "url": "http://arxiv.org/abs/2504.16834v2",
    "arxiv_id": "2504.16834v2",
    "authors": [
      "Yilin Zhai",
      "Hongyuan Shi",
      "Chao Zhan",
      "Qing Wang",
      "Zaijin You",
      "Nan Wang"
    ],
    "published": "2025-04-23T15:56:28+00:00",
    "summary": "Accurate wave height prediction is critical for maritime safety and coastal resilience, yet conventional physics-based models and traditional machine learning methods face challenges in computational efficiency and nonlinear dynamics modeling. This study introduces Chronos, the first implementation of a large language model (LLM)-powered temporal architecture (Chronos) optimized for wave forecasting. Through advanced temporal pattern recognition applied to historical wave data from three strategically chosen marine zones in the Northwest Pacific basin, our framework achieves multimodal improvements: (1) 14.3% reduction in training time with 2.5x faster inference speed compared to PatchTST baselines, achieving 0.575 mean absolute scaled error (MASE) units; (2) superior short-term forecasting (1-24h) across comprehensive metrics; (3) sustained predictive leadership in extended-range forecasts (1-120h); and (4) demonstrated zero-shot capability maintaining median performance (rank 4/12) against specialized operational models. This LLM-enhanced temporal modeling paradigm establishes a new standard in wave prediction, offering both computationally efficient solutions and a transferable framework for complex geophysical systems modeling."
  },
  {
    "title": "Association-Based Track-Before-Detect with Object Contribution Probabilities",
    "url": "http://arxiv.org/abs/2504.16814v1",
    "arxiv_id": "2504.16814v1",
    "authors": [
      "Thomas Kropfreiter",
      "Jason L. Williams",
      "Florian Meyer"
    ],
    "published": "2025-04-23T15:32:11+00:00",
    "summary": "Multiobject tracking provides situational awareness that enables new applications for modern convenience, applied ocean sciences, public safety, and homeland security. In many multiobject tracking applications, including radar and sonar tracking, after coherent prefiltering of the received signal, measurement data is typically structured in cells, where each cell represent, e.g., a different range and bearing value. While conventional detect-then-track (DTT) multiobject tracking approaches convert the cell-structured data within a detection phase into so-called point measurements in order to reduce the amount of data, track-before-detect (TBD) methods process the cell-structured data directly, avoiding a potential information loss. However, many TBD tracking methods are computationally intensive and achieve a reduced tracking accuracy when objects interact, i.e., when they come into close proximity. We here counteract these difficulties by introducing the concept of probabilistic object-to-cell contributions. As many conventional DTT methods, our approach uses a probabilistic association of objects with data cells, and a new object contribution model with corresponding object contribution probabilities to further associate cell contributions to objects that occupy the same data cell. Furthermore, to keep the computational complexity and filter runtimes low, we here use an efficient Poisson multi-Bernoulli filtering approach in combination with the application of belief propagation for fast probabilistic data association. We demonstrate numerically that our method achieves significantly increased tracking performance compared to state-of-the-art TBD tracking approaches, where performance differences are particularly pronounced when multiple objects interact."
  },
  {
    "title": "Evaluating the Impact of CT-to-RED Calibration Curves on Dosimetric Accuracy in Brain Radiotherapy Dose Distribution",
    "url": "http://arxiv.org/abs/2504.16805v1",
    "arxiv_id": "2504.16805v1",
    "authors": [
      "Islam G. Ali",
      "Wael M. Daabis",
      "Hossam Donya"
    ],
    "published": "2025-04-23T15:24:57+00:00",
    "summary": "Accurate dose calculation is crucial in radiotherapy, as tissue relative electron densities (RED) derived from CT scans play a vital role. This study investigated the impact of different CT-to-RED calibration curves on brain cancer treatment plans. Three calibration curves were compared: CIRS phantom-derived, Catphan phantom-derived, and the default curve in the Monaco Treatment Planning System. Ten volumetric modulated arc therapy (VMAT) plans were generated and recalculated using each curve. Dosimetric parameters for Planning Target Volume (PTV) and Organs at Risk (OARs) were analyzed. Results showed significant differences in PTV dose distribution between the CIRS-derived and default curves, while no significant differences were found between Catphan-derived and default curves. The CIRS-derived curve demonstrated superior performance in representing brain tissue electron densities. These findings emphasize the importance of using site-specific CT-to-RED calibration curves for accurate dose calculations in brain radiotherapy, potentially improving treatment safety and efficacy"
  },
  {
    "title": "Evaluating the Impact of CT-to-RED Calibration Curves on Dosimetric Accuracy in Brain Radiotherapy Dose Distribution",
    "url": "http://arxiv.org/abs/2504.16805v2",
    "arxiv_id": "2504.16805v2",
    "authors": [
      "Hossam Donya",
      "Duong Thanh Tai",
      "Islam G. Ali"
    ],
    "published": "2025-04-23T15:24:57+00:00",
    "summary": "Accurate dose calculation is crucial in radiotherapy, as tissue relative electron densities (RED) derived from CT scans play a vital role. This study investigated the impact of different CT-to-RED calibration curves on brain cancer treatment plans. Three calibration curves were compared: CIRS phantom-derived, Catphan phantom-derived, and the default curve in the Monaco Treatment Planning System. Ten volumetric modulated arc therapy (VMAT) plans were generated and recalculated using each curve. Dosimetric parameters for Planning Target Volume (PTV) and Organs at Risk (OARs) were analyzed. Results showed significant differences in PTV dose distribution between the CIRS-derived and default curves, while no significant differences were found between Catphan-derived and default curves. The CIRS-derived curve demonstrated superior performance in representing brain tissue electron densities. These findings emphasize the importance of using site-specific CT-to-RED calibration curves for accurate dose calculations in brain radiotherapy, potentially improving treatment safety and efficacy"
  },
  {
    "title": "Reduction of $\u03b5$-expanded Feynman integrals",
    "url": "http://arxiv.org/abs/2504.16766v1",
    "arxiv_id": "2504.16766v1",
    "authors": [
      "Yan-Qing Ma",
      "Cong-Hao Qin",
      "Ao Tan",
      "Kai Yan"
    ],
    "published": "2025-04-23T14:34:34+00:00",
    "summary": "Since Feynman integrals (FIs) at higher spacetime dimensions are free of infrared and collinear divergence--and their ultraviolet divergences can be systematically subtracted--this allows us to construct a wide range of locally finite Feynman integrals. Especially, we propose a method named $\\bar{R}$-operation to subtract out ultraviolet divergences that at the same time preserves infrared and collinear safety of the original FI. By expressing these locally finite FIs in terms of master integrals and imposing constraints on their $\\epsilon$-expanded forms, we reduce the $\\epsilon$-expanded master integrals to a minimal basis. We provide an automated package to identify such constraints, offering a tool useful for high-order perturbative computations."
  },
  {
    "title": "Frequency-Compensated Network for Daily Arctic Sea Ice Concentration Prediction",
    "url": "http://arxiv.org/abs/2504.16745v1",
    "arxiv_id": "2504.16745v1",
    "authors": [
      "Jialiang Zhang",
      "Feng Gao",
      "Yanhai Gan",
      "Junyu Dong",
      "Qian Du"
    ],
    "published": "2025-04-23T14:15:48+00:00",
    "summary": "Accurately forecasting sea ice concentration (SIC) in the Arctic is critical to global ecosystem health and navigation safety. However, current methods still is confronted with two challenges: 1) these methods rarely explore the long-term feature dependencies in the frequency domain. 2) they can hardly preserve the high-frequency details, and the changes in the marginal area of the sea ice cannot be accurately captured. To this end, we present a Frequency-Compensated Network (FCNet) for Arctic SIC prediction on a daily basis. In particular, we design a dual-branch network, including branches for frequency feature extraction and convolutional feature extraction. For frequency feature extraction, we design an adaptive frequency filter block, which integrates trainable layers with Fourier-based filters. By adding frequency features, the FCNet can achieve refined prediction of edges and details. For convolutional feature extraction, we propose a high-frequency enhancement block to separate high and low-frequency information. Moreover, high-frequency features are enhanced via channel-wise attention, and temporal attention unit is employed for low-frequency feature extraction to capture long-range sea ice changes. Extensive experiments are conducted on a satellite-derived daily SIC dataset, and the results verify the effectiveness of the proposed FCNet. Our codes and data will be made public available at: https://github.com/oucailab/FCNet ."
  },
  {
    "title": "DYNUS: Uncertainty-aware Trajectory Planner in Dynamic Unknown Environments",
    "url": "http://arxiv.org/abs/2504.16734v1",
    "arxiv_id": "2504.16734v1",
    "authors": [
      "Kota Kondo",
      "Mason Peterson",
      "Nicholas Rober",
      "Juan Rached Viso",
      "Lucas Jia",
      "Jialin Chen",
      "Harvey Merton",
      "Jonathan P. How"
    ],
    "published": "2025-04-23T14:05:04+00:00",
    "summary": "This paper introduces DYNUS, an uncertainty-aware trajectory planner designed for dynamic unknown environments. Operating in such settings presents many challenges -- most notably, because the agent cannot predict the ground-truth future paths of obstacles, a previously planned trajectory can become unsafe at any moment, requiring rapid replanning to avoid collisions.   Recently developed planners have used soft-constraint approaches to achieve the necessary fast computation times; however, these methods do not guarantee collision-free paths even with static obstacles. In contrast, hard-constraint methods ensure collision-free safety, but typically have longer computation times.   To address these issues, we propose three key contributions. First, the DYNUS Global Planner (DGP) and Temporal Safe Corridor Generation operate in spatio-temporal space and handle both static and dynamic obstacles in the 3D environment. Second, the Safe Planning Framework leverages a combination of exploratory, safe, and contingency trajectories to flexibly re-route when potential future collisions with dynamic obstacles are detected. Finally, the Fast Hard-Constraint Local Trajectory Formulation uses a variable elimination approach to reduce the problem size and enable faster computation by pre-computing dependencies between free and dependent variables while still ensuring collision-free trajectories.   We evaluated DYNUS in a variety of simulations, including dense forests, confined office spaces, cave systems, and dynamic environments. Our experiments show that DYNUS achieves a success rate of 100% and travel times that are approximately 25.0% faster than state-of-the-art methods. We also evaluated DYNUS on multiple platforms -- a quadrotor, a wheeled robot, and a quadruped -- in both simulation and hardware experiments."
  },
  {
    "title": "DYNUS: Uncertainty-aware Trajectory Planner in Dynamic Unknown Environments",
    "url": "http://arxiv.org/abs/2504.16734v2",
    "arxiv_id": "2504.16734v2",
    "authors": [
      "Kota Kondo",
      "Mason Peterson",
      "Nicholas Rober",
      "Juan Rached Viso",
      "Lucas Jia",
      "Jialin Chen",
      "Harvey Merton",
      "Jonathan P. How"
    ],
    "published": "2025-04-23T14:05:04+00:00",
    "summary": "This paper introduces DYNUS, an uncertainty-aware trajectory planner designed for dynamic unknown environments. Operating in such settings presents many challenges -- most notably, because the agent cannot predict the ground-truth future paths of obstacles, a previously planned trajectory can become unsafe at any moment, requiring rapid replanning to avoid collisions.   Recently developed planners have used soft-constraint approaches to achieve the necessary fast computation times; however, these methods do not guarantee collision-free paths even with static obstacles. In contrast, hard-constraint methods ensure collision-free safety, but typically have longer computation times.   To address these issues, we propose three key contributions. First, the DYNUS Global Planner (DGP) and Temporal Safe Corridor Generation operate in spatio-temporal space and handle both static and dynamic obstacles in the 3D environment. Second, the Safe Planning Framework leverages a combination of exploratory, safe, and contingency trajectories to flexibly re-route when potential future collisions with dynamic obstacles are detected. Finally, the Fast Hard-Constraint Local Trajectory Formulation uses a variable elimination approach to reduce the problem size and enable faster computation by pre-computing dependencies between free and dependent variables while still ensuring collision-free trajectories.   We evaluated DYNUS in a variety of simulations, including dense forests, confined office spaces, cave systems, and dynamic environments. Our experiments show that DYNUS achieves a success rate of 100% and travel times that are approximately 25.0% faster than state-of-the-art methods. We also evaluated DYNUS on multiple platforms -- a quadrotor, a wheeled robot, and a quadruped -- in both simulation and hardware experiments."
  },
  {
    "title": "Offline Robotic World Model: Learning Robotic Policies without a Physics Simulator",
    "url": "http://arxiv.org/abs/2504.16680v1",
    "arxiv_id": "2504.16680v1",
    "authors": [
      "Chenhao Li",
      "Andreas Krause",
      "Marco Hutter"
    ],
    "published": "2025-04-23T12:58:15+00:00",
    "summary": "Reinforcement Learning (RL) has demonstrated impressive capabilities in robotic control but remains challenging due to high sample complexity, safety concerns, and the sim-to-real gap. While offline RL eliminates the need for risky real-world exploration by learning from pre-collected data, it suffers from distributional shift, limiting policy generalization. Model-Based RL (MBRL) addresses this by leveraging predictive models for synthetic rollouts, yet existing approaches often lack robust uncertainty estimation, leading to compounding errors in offline settings. We introduce Offline Robotic World Model (RWM-O), a model-based approach that explicitly estimates epistemic uncertainty to improve policy learning without reliance on a physics simulator. By integrating these uncertainty estimates into policy optimization, our approach penalizes unreliable transitions, reducing overfitting to model errors and enhancing stability. Experimental results show that RWM-O improves generalization and safety, enabling policy learning purely from real-world data and advancing scalable, data-efficient RL for robotics."
  },
  {
    "title": "3D-1D modelling of cranial plate heating induced by low or medium frequency magnetic fields",
    "url": "http://arxiv.org/abs/2504.16600v1",
    "arxiv_id": "2504.16600v1",
    "authors": [
      "Alessandro Arduino",
      "Oriano Bottauscio",
      "Denise Grappein",
      "Stefano Scial\u00f3",
      "Fabio Vicini",
      "Umberto Zanovello",
      "Luca Zilberti"
    ],
    "published": "2025-04-23T10:29:53+00:00",
    "summary": "Safety assessment of patients with one-dimensionally structured passive implants, like cranial plates or stents, exposed to low or medium frequency magnetic fields, like those generated in magnetic resonance imaging or magnetic hyperthermia, can be challenging, because of the different length scales of the implant and the human body. Most of the methods used to estimate the heating induced near such implants neglect the presence of the metallic materials within the body, modeling the metal as thermal seeds. To overcome this limitation, a novel numerical approach that solves three-dimensional and one-dimensional coupled problems is proposed. This method leads to improved results by modelling the thermal diffusion through the highly conductive metallic implants. A comparison of the proposed method predictions with measurements performed on a cranial plate exposed to the magnetic field generated by a gradient coil system for magnetic resonance imaging is presented, showing an improved accuracy up to 25 % with respect to the method based on thermal seeds. The proposed method is finally applied to a magnetic hyperthermia case study in which a patient with a cranial plate is exposed to the magnetic field generated by a collar-type magnetic hyperthermia applicator for neck tumour treatment, predicting a temperature increase in proximity of the implant that is 10 % lower than the one overestimated by relying on thermal seeds."
  },
  {
    "title": "Streetscape Analysis with Generative AI (SAGAI): Vision-Language Assessment and Mapping of Urban Scenes",
    "url": "http://arxiv.org/abs/2504.16538v1",
    "arxiv_id": "2504.16538v1",
    "authors": [
      "Joan Perez",
      "Giovanni Fusco"
    ],
    "published": "2025-04-23T09:08:06+00:00",
    "summary": "Streetscapes are an essential component of urban space. Their assessment is presently either limited to morphometric properties of their mass skeleton or requires labor-intensive qualitative evaluations of visually perceived qualities. This paper introduces SAGAI: Streetscape Analysis with Generative Artificial Intelligence, a modular workflow for scoring street-level urban scenes using open-access data and vision-language models. SAGAI integrates OpenStreetMap geometries, Google Street View imagery, and a lightweight version of the LLaVA model to generate structured spatial indicators from images via customizable natural language prompts. The pipeline includes an automated mapping module that aggregates visual scores at both the point and street levels, enabling direct cartographic interpretation. It operates without task-specific training or proprietary software dependencies, supporting scalable and interpretable analysis of urban environments. Two exploratory case studies in Nice and Vienna illustrate SAGAI's capacity to produce geospatial outputs from vision-language inference. The initial results show strong performance for binary urban-rural scene classification, moderate precision in commercial feature detection, and lower estimates, but still informative, of sidewalk width. Fully deployable by any user, SAGAI can be easily adapted to a wide range of urban research themes, such as walkability, safety, or urban design, through prompt modification alone."
  },
  {
    "title": "SafeSpect: Safety-First Augmented Reality Heads-up Display for Drone Inspections",
    "url": "http://arxiv.org/abs/2504.16533v1",
    "arxiv_id": "2504.16533v1",
    "authors": [
      "Peisen Xu",
      "J\u00e9r\u00e9mie Garcia",
      "Wei Tsang Ooi",
      "Christophe Jouffrais"
    ],
    "published": "2025-04-23T08:59:05+00:00",
    "summary": "Current tablet-based interfaces for drone operations often impose a heavy cognitive load on pilots and reduce situational awareness by dividing attention between the video feed and the real world. To address these challenges, we designed a heads-up augmented reality (AR) interface that overlays in-situ information to support drone pilots in safety-critical tasks. Through participatory design workshops with professional pilots, we identified key features and developed an adaptive AR interface that dynamically switches between task and safety views to prevent information overload. We evaluated our prototype by creating a realistic building inspection task and comparing three interfaces: a 2D tablet, a static AR, and our adaptive AR design. A user study with 15 participants showed that the AR interface improved access to safety information, while the adaptive AR interface reduced cognitive load and enhanced situational awareness without compromising task performance. We offer design insights for developing safety-first heads-up AR interfaces."
  },
  {
    "title": "Amplified Vulnerabilities: Structured Jailbreak Attacks on LLM-based Multi-Agent Debate",
    "url": "http://arxiv.org/abs/2504.16489v1",
    "arxiv_id": "2504.16489v1",
    "authors": [
      "Senmao Qi",
      "Yifei Zou",
      "Peng Li",
      "Ziyi Lin",
      "Xiuzhen Cheng",
      "Dongxiao Yu"
    ],
    "published": "2025-04-23T08:01:50+00:00",
    "summary": "Multi-Agent Debate (MAD), leveraging collaborative interactions among Large Language Models (LLMs), aim to enhance reasoning capabilities in complex tasks. However, the security implications of their iterative dialogues and role-playing characteristics, particularly susceptibility to jailbreak attacks eliciting harmful content, remain critically underexplored. This paper systematically investigates the jailbreak vulnerabilities of four prominent MAD frameworks built upon leading commercial LLMs (GPT-4o, GPT-4, GPT-3.5-turbo, and DeepSeek) without compromising internal agents. We introduce a novel structured prompt-rewriting framework specifically designed to exploit MAD dynamics via narrative encapsulation, role-driven escalation, iterative refinement, and rhetorical obfuscation. Our extensive experiments demonstrate that MAD systems are inherently more vulnerable than single-agent setups. Crucially, our proposed attack methodology significantly amplifies this fragility, increasing average harmfulness from 28.14% to 80.34% and achieving attack success rates as high as 80% in certain scenarios. These findings reveal intrinsic vulnerabilities in MAD architectures and underscore the urgent need for robust, specialized defenses prior to real-world deployment."
  },
  {
    "title": "The Dodecacopter: a Versatile Multirotor System of Dodecahedron-Shaped Modules",
    "url": "http://arxiv.org/abs/2504.16475v1",
    "arxiv_id": "2504.16475v1",
    "authors": [
      "K\u00e9vin Garanger",
      "Thanakorn Khamvilai",
      "Jeremy Epps",
      "Eric Feron"
    ],
    "published": "2025-04-23T07:38:00+00:00",
    "summary": "With the promise of greater safety and adaptability, modular reconfigurable uncrewed air vehicles have been proposed as unique, versatile platforms holding the potential to replace multiple types of monolithic vehicles at once. State-of-the-art rigidly assembled modular vehicles are generally two-dimensional configurations in which the rotors are coplanar and assume the shape of a \"flight array\". We introduce the Dodecacopter, a new type of modular rotorcraft where all modules take the shape of a regular dodecahedron, allowing the creation of richer sets of configurations beyond flight arrays. In particular, we show how the chosen module design can be used to create three-dimensional and fully actuated configurations. We justify the relevance of these types of configurations in terms of their structural and actuation properties with various performance indicators. Given the broad range of configurations and capabilities that can be achieved with our proposed design, we formulate tractable optimization programs to find optimal configurations given structural and actuation constraints. Finally, a prototype of such a vehicle is presented along with results of performed flights in multiple configurations."
  },
  {
    "title": "ERASER: Efficient RTL FAult Simulation Framework with Trimmed Execution Redundancy",
    "url": "http://arxiv.org/abs/2504.16473v1",
    "arxiv_id": "2504.16473v1",
    "authors": [
      "Jiaping Tang",
      "Jianan Mu",
      "Silin Liu",
      "Zizhen Liu",
      "Feng Gu",
      "Xinyu Zhang",
      "Leyan Wang",
      "Shenwen Liang",
      "Jing Ye",
      "Huawei Li",
      "Xiaowei Li"
    ],
    "published": "2025-04-23T07:33:44+00:00",
    "summary": "As intelligent computing devices increasingly integrate into human life, ensuring the functional safety of the corresponding electronic chips becomes more critical. A key metric for functional safety is achieving a sufficient fault coverage. To meet this requirement, extensive time-consuming fault simulation of the RTL code is necessary during the chip design phase.The main overhead in RTL fault simulation comes from simulating behavioral nodes (always blocks). Due to the limited fault propagation capacity, fault simulation results often match the good simulation results for many behavioral nodes. A key strategy for accelerating RTL fault simulation is the identification and elimination of redundant simulations. Existing methods detect redundant executions by examining whether the fault inputs to each RTL node are consistent with the good inputs. However, we observe that this input comparison mechanism overlooks a significant amount of implicit redundant execution: although the fault inputs differ from the good inputs, the node's execution results remain unchanged. Our experiments reveal that this overlooked redundant execution constitutes nearly half of the total execution overhead of behavioral nodes, becoming a significant bottleneck in current RTL fault simulation. The underlying reason for this overlooked redundancy is that, in these cases, the true execution paths within the behavioral nodes are not affected by the changes in input values. In this work, we propose a behavior-level redundancy detection algorithm that focuses on the true execution paths. Building on the elimination of redundant executions, we further developed an efficient RTL fault simulation framework, Eraser.Experimental results show that compared to commercial tools, under the same fault coverage, our framework achieves a 3.9 $\\times$ improvement in simulation performance on average."
  },
  {
    "title": "iTFKAN: Interpretable Time Series Forecasting with Kolmogorov-Arnold Network",
    "url": "http://arxiv.org/abs/2504.16432v1",
    "arxiv_id": "2504.16432v1",
    "authors": [
      "Ziran Liang",
      "Rui An",
      "Wenqi Fan",
      "Yanghui Rao",
      "Yuxuan Liang"
    ],
    "published": "2025-04-23T05:34:49+00:00",
    "summary": "As time evolves, data within specific domains exhibit predictability that motivates time series forecasting to predict future trends from historical data. However, current deep forecasting methods can achieve promising performance but generally lack interpretability, hindering trustworthiness and practical deployment in safety-critical applications such as auto-driving and healthcare. In this paper, we propose a novel interpretable model, iTFKAN, for credible time series forecasting. iTFKAN enables further exploration of model decision rationales and underlying data patterns due to its interpretability achieved through model symbolization. Besides, iTFKAN develops two strategies, prior knowledge injection, and time-frequency synergy learning, to effectively guide model learning under complex intertwined time series data. Extensive experimental results demonstrated that iTFKAN can achieve promising forecasting performance while simultaneously possessing high interpretive capabilities."
  },
  {
    "title": "On Validating Angular Power Spectral Models for the Stochastic Gravitational-Wave Background Without Distributional Assumptions",
    "url": "http://arxiv.org/abs/2504.16959v1",
    "arxiv_id": "2504.16959v1",
    "authors": [
      "Xiangyu Zhang",
      "Erik Floden",
      "Hongru Zhao",
      "Sara Algeri",
      "Galin Jones",
      "Vuk Mandic",
      "Jesse Miller"
    ],
    "published": "2025-04-23T05:03:39+00:00",
    "summary": "It is demonstrated that estimators of the angular power spectrum commonly used for the stochastic gravitational-wave background (SGWB) lack a closed-form analytical expression for the likelihood function and, typically, cannot be accurately approximated by a Gaussian likelihood. Nevertheless, a robust statistical analysis can be performed by extending the framework outlined in \\cite{PRL} to enable the estimation and testing of angular power spectral models for the SGWB without specifying distributional assumptions. Here, the technical aspects of the method are discussed in detail. Moreover, a new, consistent estimator for the covariance of the angular power spectrum is derived. The proposed approach is applied to data from the third observing run (O3) of Advanced LIGO and Advanced Virgo."
  },
  {
    "title": "Anytime Safe Reinforcement Learning",
    "url": "http://arxiv.org/abs/2504.16417v1",
    "arxiv_id": "2504.16417v1",
    "authors": [
      "Pol Mestres",
      "Arnau Marzabal",
      "Jorge Cort\u00e9s"
    ],
    "published": "2025-04-23T04:51:31+00:00",
    "summary": "This paper considers the problem of solving constrained   reinforcement learning problems with anytime guarantees, meaning   that the algorithmic solution returns a safe policy regardless of   when it is terminated. Drawing inspiration from anytime constrained   optimization, we introduce Reinforcement Learning-based Safe   Gradient Flow (RL-SGF), an on-policy algorithm which employs   estimates of the value functions and their respective gradients   associated with the objective and safety constraints for the current   policy, and updates the policy parameters by solving a convex   quadratically constrained quadratic program. We show that if the   estimates are computed with a sufficiently large number of episodes   (for which we provide an explicit bound), safe policies are updated   to safe policies with a probability higher than a prescribed   tolerance. We also show that iterates asymptotically converge to a   neighborhood of a KKT point, whose size can be arbitrarily reduced   by refining the estimates of the value function and their gradients.   We illustrate the performance of RL-SGF in a navigation example."
  },
  {
    "title": "SILM: A Subjective Intent Based Low-Latency Framework for Multiple Traffic Participants Joint Trajectory Prediction",
    "url": "http://arxiv.org/abs/2504.16377v1",
    "arxiv_id": "2504.16377v1",
    "authors": [
      "Qu Weiming",
      "Wang Jia",
      "Du Jiawei",
      "Zhu Yuanhao",
      "Yu Jianfeng",
      "Xia Rui",
      "Cao Song",
      "Wu Xihong",
      "Luo Dingsheng"
    ],
    "published": "2025-04-23T02:56:34+00:00",
    "summary": "Trajectory prediction is a fundamental technology for advanced autonomous driving systems and represents one of the most challenging problems in the field of cognitive intelligence. Accurately predicting the future trajectories of each traffic participant is a prerequisite for building high safety and high reliability decision-making, planning, and control capabilities in autonomous driving. However, existing methods often focus solely on the motion of other traffic participants without considering the underlying intent behind that motion, which increases the uncertainty in trajectory prediction. Autonomous vehicles operate in real-time environments, meaning that trajectory prediction algorithms must be able to process data and generate predictions in real-time. While many existing methods achieve high accuracy, they often struggle to effectively handle heterogeneous traffic scenarios. In this paper, we propose a Subjective Intent-based Low-latency framework for Multiple traffic participants joint trajectory prediction. Our method explicitly incorporates the subjective intent of traffic participants based on their key points, and predicts the future trajectories jointly without map, which ensures promising performance while significantly reducing the prediction latency. Additionally, we introduce a novel dataset designed specifically for trajectory prediction. Related code and dataset will be available soon."
  },
  {
    "title": "The Safety-Privacy Tradeoff in Linear Bandits",
    "url": "http://arxiv.org/abs/2504.16371v1",
    "arxiv_id": "2504.16371v1",
    "authors": [
      "Arghavan Zibaie",
      "Spencer Hutchinson",
      "Ramtin Pedarsani",
      "Mahnoosh Alizadeh"
    ],
    "published": "2025-04-23T02:48:02+00:00",
    "summary": "We consider a collection of linear stochastic bandit problems, each modeling the random response of different agents to proposed interventions, coupled together by a global safety constraint. We assume a central coordinator must choose actions to play on each bandit with the objective of regret minimization, while also ensuring that the expected response of all agents satisfies the global safety constraints at each round, in spite of uncertainty about the bandits' parameters. The agents consider their observed responses to be private and in order to protect their sensitive information, the data sharing with the central coordinator is performed under local differential privacy (LDP). However, providing higher level of privacy to different agents would have consequences in terms of safety and regret. We formalize these tradeoffs by building on the notion of the sharpness of the safety set - a measure of how the geometric properties of the safe set affects the growth of regret - and propose a unilaterally unimprovable vector of privacy levels for different agents given a maximum regret budget."
  },
  {
    "title": "VeriFix: Verifying Your Fix Towards An Atomicity Violation",
    "url": "http://arxiv.org/abs/2504.16354v1",
    "arxiv_id": "2504.16354v1",
    "authors": [
      "Zhuang Li",
      "Qiuping Yi",
      "Jeff Huang"
    ],
    "published": "2025-04-23T02:11:07+00:00",
    "summary": "Atomicity violation is one of the most serious types of bugs in concurrent programs. Synchronizations are commonly used to enforce atomicity. However, it is very challenging to place synchronizations correctly and sufficiently   due to complex thread interactions and large input space. This paper presents \\textsf{VeriFix}, a new approach for verifying atomicity violation fixes. Given a buggy trace that exposes an atomicity violation and a corresponding fix, % in the form of locks, \\textsf{VeriFix} effectively verifies if the fix introduces sufficient synchronizations to repair the atomicity violation without introducing new deadlocks. The key idea is that \\textsf{VeriFix} transforms the fix verification problem into a property verification problem, in which both the observed atomicity violation and potential deadlocks are encoded as a safety property, and both the inputs and schedules are encoded as symbolic constraints. By reasoning the conjoined constraints with an SMT solver, \\textsf{VeriFix} systematically explores all reachable paths %from the whole schedule and input space and verifies if there exists a concrete \\textit{schedule+input} combination to manifest the intended atomicity or any new deadlocks. We have implemented and evaluated \\verifix\\ on a collection of real-world C/C++ programs. The result shows that \\textsf{VeriFix} significantly outperforms the state-of-the-art."
  },
  {
    "title": "Impact of Noise on LLM-Models Performance in Abstraction and Reasoning Corpus (ARC) Tasks with Model Temperature Considerations",
    "url": "http://arxiv.org/abs/2504.15903v1",
    "arxiv_id": "2504.15903v1",
    "authors": [
      "Nikhil Khandalkar",
      "Pavan Yadav",
      "Krishna Shinde",
      "Lokesh B. Ramegowda",
      "Rajarshi Das"
    ],
    "published": "2025-04-22T13:43:58+00:00",
    "summary": "Recent advancements in Large Language Models (LLMs) have generated growing interest in their structured reasoning capabilities, particularly in tasks involving abstraction and pattern recognition. The Abstraction and Reasoning Corpus (ARC) benchmark plays a crucial role in evaluating these capabilities by testing how well AI models generalize to novel problems. While GPT-4o demonstrates strong performance by solving all ARC tasks under zero-noise conditions, other models like DeepSeek R1 and LLaMA 3.2 fail to solve any, suggesting limitations in their ability to reason beyond simple pattern matching. To explore this gap, we systematically evaluate these models across different noise levels and temperature settings. Our results reveal that the introduction of noise consistently impairs model performance, regardless of architecture. This decline highlights a shared vulnerability: current LLMs, despite showing signs of abstract reasoning, remain highly sensitive to input perturbations. Such fragility raises concerns about their real-world applicability, where noise and uncertainty are common. By comparing how different model architectures respond to these challenges, we offer insights into the structural weaknesses of modern LLMs in reasoning tasks. This work underscores the need for developing more robust and adaptable AI systems capable of handling the ambiguity and variability inherent in real-world scenarios. Our findings aim to guide future research toward enhancing model generalization, robustness, and alignment with human-like cognitive flexibility."
  },
  {
    "title": "Impact of Noise on LLM-Models Performance in Abstraction and Reasoning Corpus (ARC) Tasks with Model Temperature Considerations",
    "url": "http://arxiv.org/abs/2504.15903v2",
    "arxiv_id": "2504.15903v2",
    "authors": [
      "Nikhil Khandalkar",
      "Pavan Yadav",
      "Krishna Shinde",
      "Lokesh B. Ramegowda",
      "Rajarshi Das"
    ],
    "published": "2025-04-22T13:43:58+00:00",
    "summary": "Recent advancements in Large Language Models (LLMs) have generated growing interest in their structured reasoning capabilities, particularly in tasks involving abstraction and pattern recognition. The Abstraction and Reasoning Corpus (ARC) benchmark plays a crucial role in evaluating these capabilities by testing how well AI models generalize to novel problems. While GPT-4o demonstrates strong performance by solving all ARC tasks under zero-noise conditions, other models like DeepSeek R1 and LLaMA 3.2 fail to solve any, suggesting limitations in their ability to reason beyond simple pattern matching. To explore this gap, we systematically evaluate these models across different noise levels and temperature settings. Our results reveal that the introduction of noise consistently impairs model performance, regardless of architecture. This decline highlights a shared vulnerability: current LLMs, despite showing signs of abstract reasoning, remain highly sensitive to input perturbations. Such fragility raises concerns about their real-world applicability, where noise and uncertainty are common. By comparing how different model architectures respond to these challenges, we offer insights into the structural weaknesses of modern LLMs in reasoning tasks. This work underscores the need for developing more robust and adaptable AI systems capable of handling the ambiguity and variability inherent in real-world scenarios. Our findings aim to guide future research toward enhancing model generalization, robustness, and alignment with human-like cognitive flexibility."
  },
  {
    "title": "SARI: Structured Audio Reasoning via Curriculum-Guided Reinforcement Learning",
    "url": "http://arxiv.org/abs/2504.15900v1",
    "arxiv_id": "2504.15900v1",
    "authors": [
      "Cheng Wen",
      "Tingwei Guo",
      "Shuaijiang Zhao",
      "Wei Zou",
      "Xiangang Li"
    ],
    "published": "2025-04-22T13:41:26+00:00",
    "summary": "Recent work shows that reinforcement learning(RL) can markedly sharpen the reasoning ability of large language models (LLMs) by prompting them to \"think before answering.\" Yet whether and how these gains transfer to audio-language reasoning remains largely unexplored. We extend the Group-Relative Policy Optimization (GRPO) framework from DeepSeek-R1 to a Large Audio-Language Model (LALM), and construct a 32k sample multiple-choice corpus. Using a two-stage regimen supervised fine-tuning on structured and unstructured chains-of-thought, followed by curriculum-guided GRPO, we systematically compare implicit vs. explicit, and structured vs. free form reasoning under identical architectures. Our structured audio reasoning model, SARI (Structured Audio Reasoning via Curriculum-Guided Reinforcement Learning), achieves a 16.35% improvement in average accuracy over the base model Qwen2-Audio-7B-Instruct. Furthermore, the variant built upon Qwen2.5-Omni reaches state-of-the-art performance of 67.08% on the MMAU test-mini benchmark. Ablation experiments show that on the base model we use: (i) SFT warm-up is important for stable RL training, (ii) structured chains yield more robust generalization than unstructured ones, and (iii) easy-to-hard curricula accelerate convergence and improve final performance. These findings demonstrate that explicit, structured reasoning and curriculum learning substantially enhances audio-language understanding."
  },
  {
    "title": "Dynamic Early Exit in Reasoning Models",
    "url": "http://arxiv.org/abs/2504.15895v1",
    "arxiv_id": "2504.15895v1",
    "authors": [
      "Chenxu Yang",
      "Qingyi Si",
      "Yongjie Duan",
      "Zheliang Zhu",
      "Chenyu Zhu",
      "Zheng Lin",
      "Li Cao",
      "Weiping Wang"
    ],
    "published": "2025-04-22T13:36:53+00:00",
    "summary": "Recent advances in large reasoning language models (LRLMs) rely on test-time scaling, which extends long chain-of-thought (CoT) generation to solve complex tasks. However, overthinking in long CoT not only slows down the efficiency of problem solving, but also risks accuracy loss due to the extremely detailed or redundant reasoning steps. We propose a simple yet effective method that allows LLMs to self-truncate CoT sequences by early exit during generation. Instead of relying on fixed heuristics, the proposed method monitors model behavior at potential reasoning transition points (e.g.,\"Wait\" tokens) and dynamically terminates the next reasoning chain's generation when the model exhibits high confidence in a trial answer. Our method requires no additional training and can be seamlessly integrated into existing o1-like reasoning LLMs. Experiments on multiple reasoning benchmarks MATH-500, AMC 2023, GPQA Diamond and AIME 2024 show that the proposed method is consistently effective on deepseek-series reasoning LLMs, reducing the length of CoT sequences by an average of 31% to 43% while improving accuracy by 1.7% to 5.7%."
  },
  {
    "title": "MS-Occ: Multi-Stage LiDAR-Camera Fusion for 3D Semantic Occupancy Prediction",
    "url": "http://arxiv.org/abs/2504.15888v1",
    "arxiv_id": "2504.15888v1",
    "authors": [
      "Zhiqiang Wei",
      "Lianqing Zheng",
      "Jianan Liu",
      "Tao Huang",
      "Qing-Long Han",
      "Wenwen Zhang",
      "Fengdeng Zhang"
    ],
    "published": "2025-04-22T13:33:26+00:00",
    "summary": "Accurate 3D semantic occupancy perception is essential for autonomous driving in complex environments with diverse and irregular objects. While vision-centric methods suffer from geometric inaccuracies, LiDAR-based approaches often lack rich semantic information. To address these limitations, MS-Occ, a novel multi-stage LiDAR-camera fusion framework which includes middle-stage fusion and late-stage fusion, is proposed, integrating LiDAR's geometric fidelity with camera-based semantic richness via hierarchical cross-modal fusion. The framework introduces innovations at two critical stages: (1) In the middle-stage feature fusion, the Gaussian-Geo module leverages Gaussian kernel rendering on sparse LiDAR depth maps to enhance 2D image features with dense geometric priors, and the Semantic-Aware module enriches LiDAR voxels with semantic context via deformable cross-attention; (2) In the late-stage voxel fusion, the Adaptive Fusion (AF) module dynamically balances voxel features across modalities, while the High Classification Confidence Voxel Fusion (HCCVF) module resolves semantic inconsistencies using self-attention-based refinement. Experiments on the nuScenes-OpenOccupancy benchmark show that MS-Occ achieves an Intersection over Union (IoU) of 32.1% and a mean IoU (mIoU) of 25.3%, surpassing the state-of-the-art by +0.7% IoU and +2.4% mIoU. Ablation studies further validate the contribution of each module, with substantial improvements in small-object perception, demonstrating the practical value of MS-Occ for safety-critical autonomous driving scenarios."
  },
  {
    "title": "Embedded Safe Reactive Navigation for Multirotors Systems using Control Barrier Functions",
    "url": "http://arxiv.org/abs/2504.15850v1",
    "arxiv_id": "2504.15850v1",
    "authors": [
      "Nazar Misyats",
      "Marvin Harms",
      "Morten Nissov",
      "Martin Jacquet",
      "Kostas Alexis"
    ],
    "published": "2025-04-22T12:45:11+00:00",
    "summary": "Aiming to promote the wide adoption of safety filters for autonomous aerial robots, this paper presents a safe control architecture designed for seamless integration into widely used open-source autopilots. Departing from methods that require consistent localization and mapping, we formalize the obstacle avoidance problem as a composite control barrier function constructed only from the online onboard range measurements. The proposed framework acts as a safety filter, modifying the acceleration references derived by the nominal position/velocity control loops, and is integrated into the PX4 autopilot stack. Experimental studies using a small multirotor aerial robot demonstrate the effectiveness and performance of the solution within dynamic maneuvering and unknown environments."
  },
  {
    "title": "TrustGeoGen: Scalable and Formal-Verified Data Engine for Trustworthy Multi-modal Geometric Problem Solving",
    "url": "http://arxiv.org/abs/2504.15780v1",
    "arxiv_id": "2504.15780v1",
    "authors": [
      "Daocheng Fu",
      "Zijun Chen",
      "Renqiu Xia",
      "Qi Liu",
      "Yuan Feng",
      "Hongbin Zhou",
      "Renrui Zhang",
      "Shiyang Feng",
      "Peng Gao",
      "Junchi Yan",
      "Botian Shi",
      "Bo Zhang",
      "Yu Qiao"
    ],
    "published": "2025-04-22T10:45:23+00:00",
    "summary": "Mathematical geometric problem solving (GPS) often requires effective integration of multimodal information and verifiable logical coherence. Despite the fast development of large language models in general problem solving, it remains unresolved regarding with both methodology and benchmarks, especially given the fact that exiting synthetic GPS benchmarks are often not self-verified and contain noise and self-contradicted information due to the illusion of LLMs. In this paper, we propose a scalable data engine called TrustGeoGen for problem generation, with formal verification to provide a principled benchmark, which we believe lays the foundation for the further development of methods for GPS. The engine synthesizes geometric data through four key innovations: 1) multimodal-aligned generation of diagrams, textual descriptions, and stepwise solutions; 2) formal verification ensuring rule-compliant reasoning paths; 3) a bootstrapping mechanism enabling complexity escalation via recursive state generation and 4) our devised GeoExplore series algorithms simultaneously produce multi-solution variants and self-reflective backtracking traces. By formal logical verification, TrustGeoGen produces GeoTrust-200K dataset with guaranteed modality integrity, along with GeoTrust-test testset. Experiments reveal the state-of-the-art models achieve only 49.17\\% accuracy on GeoTrust-test, demonstrating its evaluation stringency. Crucially, models trained on GeoTrust achieve OOD generalization on GeoQA, significantly reducing logical inconsistencies relative to pseudo-label annotated by OpenAI-o1. Our code is available at https://github.com/Alpha-Innovator/TrustGeoGen"
  },
  {
    "title": "Pose Optimization for Autonomous Driving Datasets using Neural Rendering Models",
    "url": "http://arxiv.org/abs/2504.15776v1",
    "arxiv_id": "2504.15776v1",
    "authors": [
      "Quentin Herau",
      "Nathan Piasco",
      "Moussab Bennehar",
      "Luis Rolado",
      "Dzmitry Tsishkou",
      "Bingbing Liu",
      "Cyrille Migniot",
      "Pascal Vasseur",
      "C\u00e9dric Demonceaux"
    ],
    "published": "2025-04-22T10:33:01+00:00",
    "summary": "Autonomous driving systems rely on accurate perception and localization of the ego car to ensure safety and reliability in challenging real-world driving scenarios. Public datasets play a vital role in benchmarking and guiding advancement in research by providing standardized resources for model development and evaluation. However, potential inaccuracies in sensor calibration and vehicle poses within these datasets can lead to erroneous evaluations of downstream tasks, adversely impacting the reliability and performance of the autonomous systems. To address this challenge, we propose a robust optimization method based on Neural Radiance Fields (NeRF) to refine sensor poses and calibration parameters, enhancing the integrity of dataset benchmarks. To validate improvement in accuracy of our optimized poses without ground truth, we present a thorough evaluation process, relying on reprojection metrics, Novel View Synthesis rendering quality, and geometric alignment. We demonstrate that our method achieves significant improvements in sensor pose accuracy. By optimizing these critical parameters, our approach not only improves the utility of existing datasets but also paves the way for more reliable autonomous driving models. To foster continued progress in this field, we make the optimized sensor poses publicly available, providing a valuable resource for the research community."
  },
  {
    "title": "Prediction of CO2 reduction reaction intermediates and products on transition metal-doped r-GeSe monolayers:A combined DFT and machine learning approach",
    "url": "http://arxiv.org/abs/2504.15710v1",
    "arxiv_id": "2504.15710v1",
    "authors": [
      "Xuxin Kang",
      "Wenjing Zhou",
      "Ziyuan Li",
      "Zhaoqin Chu",
      "Hanqin Yin",
      "Shan Gao",
      "Aijun Du",
      "Xiangmei Duan"
    ],
    "published": "2025-04-22T08:52:18+00:00",
    "summary": "The electrocatalytic CO2 reduction reaction (CO2RR) is a complex multi-proton-electron transfer process that generates a vast network of reaction intermediates. Accurate prediction of free energy changes (G) of these intermediates and products is essential for evaluating catalytic performance. We combined density functional theory (DFT) and machine learning (ML) to screen 25 single-atom catalysts (SACs) on defective r-GeSe monolayers for CO2 reduction to methanol, methane, and formic acid. Among nine ML models evaluated with 14 intrinsic and DFT-based features, the XGBoost performed best (R2 = 0.92 and MAE = 0.24 eV), aligning closely with DFT calculations and identifying Ni, Ru, and Rh@GeSe as prospective catalysts. Feature importance analysis in free energy and product predictions highlighted the significance of CO2 activation with O-C-O and IPC-O1 as the key attributes. Furthermore, by incorporating non-DFT-based features, rapid predictions became possible, and the XGBoost model retained its predictive performance with R2 = 0.89 and MAE = 0.29 eV. This accuracy was further validated using Ir@GeSe. Our work highlights effective SACs for CO2RR, and provides valuable insights for efficient catalyst design."
  },
  {
    "title": "Advancing Embodied Agent Security: From Safety Benchmarks to Input Moderation",
    "url": "http://arxiv.org/abs/2504.15699v1",
    "arxiv_id": "2504.15699v1",
    "authors": [
      "Ning Wang",
      "Zihan Yan",
      "Weiyang Li",
      "Chuan Ma",
      "He Chen",
      "Tao Xiang"
    ],
    "published": "2025-04-22T08:34:35+00:00",
    "summary": "Embodied agents exhibit immense potential across a multitude of domains, making the assurance of their behavioral safety a fundamental prerequisite for their widespread deployment. However, existing research predominantly concentrates on the security of general large language models, lacking specialized methodologies for establishing safety benchmarks and input moderation tailored to embodied agents. To bridge this gap, this paper introduces a novel input moderation framework, meticulously designed to safeguard embodied agents. This framework encompasses the entire pipeline, including taxonomy definition, dataset curation, moderator architecture, model training, and rigorous evaluation. Notably, we introduce EAsafetyBench, a meticulously crafted safety benchmark engineered to facilitate both the training and stringent assessment of moderators specifically designed for embodied agents. Furthermore, we propose Pinpoint, an innovative prompt-decoupled input moderation scheme that harnesses a masked attention mechanism to effectively isolate and mitigate the influence of functional prompts on moderation tasks. Extensive experiments conducted on diverse benchmark datasets and models validate the feasibility and efficacy of the proposed approach. The results demonstrate that our methodologies achieve an impressive average detection accuracy of 94.58%, surpassing the performance of existing state-of-the-art techniques, alongside an exceptional moderation processing time of merely 0.002 seconds per instance."
  },
  {
    "title": "Symbolic Runtime Verification and Adaptive Decision-Making for Robot-Assisted Dressing",
    "url": "http://arxiv.org/abs/2504.15666v1",
    "arxiv_id": "2504.15666v1",
    "authors": [
      "Yasmin Rafiq",
      "Gricel V\u00e1zquez",
      "Radu Calinescu",
      "Sanja Dogramadzi",
      "Robert M Hierons"
    ],
    "published": "2025-04-22T07:42:16+00:00",
    "summary": "We present a control framework for robot-assisted dressing that augments low-level hazard response with runtime monitoring and formal verification. A parametric discrete-time Markov chain (pDTMC) models the dressing process, while Bayesian inference dynamically updates this pDTMC's transition probabilities based on sensory and user feedback. Safety constraints from hazard analysis are expressed in probabilistic computation tree logic, and symbolically verified using a probabilistic model checker. We evaluate reachability, cost, and reward trade-offs for garment-snag mitigation and escalation, enabling real-time adaptation. Our approach provides a formal yet lightweight foundation for safety-aware, explainable robotic assistance."
  },
  {
    "title": "An ACO-MPC Framework for Energy-Efficient and Collision-Free Path Planning in Autonomous Maritime Navigation",
    "url": "http://arxiv.org/abs/2504.15611v1",
    "arxiv_id": "2504.15611v1",
    "authors": [
      "Yaoze Liu",
      "Zhen Tian",
      "Qifan Zhou",
      "Zixuan Huang",
      "Hongyu Sun"
    ],
    "published": "2025-04-22T06:09:54+00:00",
    "summary": "Automated driving on ramps presents significant challenges due to the need to balance both safety and efficiency during lane changes. This paper proposes an integrated planner for automated vehicles (AVs) on ramps, utilizing an unsatisfactory level metric for efficiency and arrow-cluster-based sampling for safety. The planner identifies optimal times for the AV to change lanes, taking into account the vehicle's velocity as a key factor in efficiency. Additionally, the integrated planner employs arrow-cluster-based sampling to evaluate collision risks and select an optimal lane-changing curve. Extensive simulations were conducted in a ramp scenario to verify the planner's efficient and safe performance. The results demonstrate that the proposed planner can effectively select an appropriate lane-changing time point and a safe lane-changing curve for AVs, without incurring any collisions during the maneuver."
  },
  {
    "title": "A Comprehensive Survey in LLM(-Agent) Full Stack Safety: Data, Training and Deployment",
    "url": "http://arxiv.org/abs/2504.15585v1",
    "arxiv_id": "2504.15585v1",
    "authors": [
      "Kun Wang",
      "Guibin Zhang",
      "Zhenhong Zhou",
      "Jiahao Wu",
      "Miao Yu",
      "Shiqian Zhao",
      "Chenlong Yin",
      "Jinhu Fu",
      "Yibo Yan",
      "Hanjun Luo",
      "Liang Lin",
      "Zhihao Xu",
      "Haolang Lu",
      "Xinye Cao",
      "Xinyun Zhou",
      "Weifei Jin",
      "Fanci Meng",
      "Junyuan Mao",
      "Hao Wu",
      "Minghe Wang",
      "Fan Zhang",
      "Junfeng Fang",
      "Chengwei Liu",
      "Yifan Zhang",
      "Qiankun Li",
      "Chongye Guo",
      "Yalan Qin",
      "Yi Ding",
      "Donghai Hong",
      "Jiaming Ji",
      "Xinfeng Li",
      "Yifan Jiang",
      "Dongxia Wang",
      "Yihao Huang",
      "Yufei Guo",
      "Jen-tse Huang",
      "Yanwei Yue",
      "Wenke Huang",
      "Guancheng Wan",
      "Tianlin Li",
      "Lei Bai",
      "Jie Zhang",
      "Qing Guo",
      "Jingyi Wang",
      "Tianlong Chen",
      "Joey Tianyi Zhou",
      "Xiaojun Jia",
      "Weisong Sun",
      "Cong Wu",
      "Jing Chen",
      "Xuming Hu",
      "Yiming Li",
      "Xiao Wang",
      "Ningyu Zhang",
      "Luu Anh Tuan",
      "Guowen Xu",
      "Tianwei Zhang",
      "Xingjun Ma",
      "Xiang Wang",
      "Bo An",
      "Jun Sun",
      "Mohit Bansal",
      "Shirui Pan",
      "Yuval Elovici",
      "Bhavya Kailkhura",
      "Bo Li",
      "Yaodong Yang",
      "Hongwei Li",
      "Wenyuan Xu",
      "Yizhou Sun",
      "Wei Wang",
      "Qing Li",
      "Ke Tang",
      "Yu-Gang Jiang",
      "Felix Juefei-Xu",
      "Hui Xiong",
      "Xiaofeng Wang",
      "Shuicheng Yan",
      "Dacheng Tao",
      "Philip S. Yu",
      "Qingsong Wen",
      "Yang Liu"
    ],
    "published": "2025-04-22T05:02:49+00:00",
    "summary": "The remarkable success of Large Language Models (LLMs) has illuminated a promising pathway toward achieving Artificial General Intelligence for both academic and industrial communities, owing to their unprecedented performance across various applications. As LLMs continue to gain prominence in both research and commercial domains, their security and safety implications have become a growing concern, not only for researchers and corporations but also for every nation. Currently, existing surveys on LLM safety primarily focus on specific stages of the LLM lifecycle, e.g., deployment phase or fine-tuning phase, lacking a comprehensive understanding of the entire \"lifechain\" of LLMs. To address this gap, this paper introduces, for the first time, the concept of \"full-stack\" safety to systematically consider safety issues throughout the entire process of LLM training, deployment, and eventual commercialization. Compared to the off-the-shelf LLM safety surveys, our work demonstrates several distinctive advantages: (I) Comprehensive Perspective. We define the complete LLM lifecycle as encompassing data preparation, pre-training, post-training, deployment and final commercialization. To our knowledge, this represents the first safety survey to encompass the entire lifecycle of LLMs. (II) Extensive Literature Support. Our research is grounded in an exhaustive review of over 800+ papers, ensuring comprehensive coverage and systematic organization of security issues within a more holistic understanding. (III) Unique Insights. Through systematic literature analysis, we have developed reliable roadmaps and perspectives for each chapter. Our work identifies promising research directions, including safety in data generation, alignment techniques, model editing, and LLM-based agent systems. These insights provide valuable guidance for researchers pursuing future work in this field."
  },
  {
    "title": "RiskNet: Interaction-Aware Risk Forecasting for Autonomous Driving in Long-Tail Scenarios",
    "url": "http://arxiv.org/abs/2504.15541v1",
    "arxiv_id": "2504.15541v1",
    "authors": [
      "Qichao Liu",
      "Heye Huang",
      "Shiyue Zhao",
      "Lei Shi",
      "Soyoung Ahn",
      "Xiaopeng Li"
    ],
    "published": "2025-04-22T02:36:54+00:00",
    "summary": "Ensuring the safety of autonomous vehicles (AVs) in long-tail scenarios remains a critical challenge, particularly under high uncertainty and complex multi-agent interactions. To address this, we propose RiskNet, an interaction-aware risk forecasting framework, which integrates deterministic risk modeling with probabilistic behavior prediction for comprehensive risk assessment. At its core, RiskNet employs a field-theoretic model that captures interactions among ego vehicle, surrounding agents, and infrastructure via interaction fields and force. This model supports multidimensional risk evaluation across diverse scenarios (highways, intersections, and roundabouts), and shows robustness under high-risk and long-tail settings. To capture the behavioral uncertainty, we incorporate a graph neural network (GNN)-based trajectory prediction module, which learns multi-modal future motion distributions. Coupled with the deterministic risk field, it enables dynamic, probabilistic risk inference across time, enabling proactive safety assessment under uncertainty. Evaluations on the highD, inD, and rounD datasets, spanning lane changes, turns, and complex merges, demonstrate that our method significantly outperforms traditional approaches (e.g., TTC, THW, RSS, NC Field) in terms of accuracy, responsiveness, and directional sensitivity, while maintaining strong generalization across scenarios. This framework supports real-time, scenario-adaptive risk forecasting and demonstrates strong generalization across uncertain driving environments. It offers a unified foundation for safety-critical decision-making in long-tail scenarios."
  },
  {
    "title": "Towards Resilience and Autonomy-based Approaches for Adolescents Online Safety",
    "url": "http://arxiv.org/abs/2504.15533v1",
    "arxiv_id": "2504.15533v1",
    "authors": [
      "Jinkyung Park",
      "Mamtaj Akter",
      "Naima Samreen Ali",
      "Zainab Agha",
      "Ashwaq Alsoubai",
      "Pamela Wisniewski"
    ],
    "published": "2025-04-22T02:23:48+00:00",
    "summary": "In this position paper, we discuss the paradigm shift that has emerged in the literature, suggesting to move away from restrictive and authoritarian parental mediation approaches to move toward resilient-based and privacy-preserving solutions to promote adolescents' online safety. We highlight the limitations of restrictive mediation strategies, which often induce a trade-off between teens' privacy and online safety, and call for more teen-centric frameworks that can empower teens to self-regulate while using the technology in meaningful ways. We also present an overview of empirical studies that conceptualized and examined resilience-based approaches to promoting the digital well-being of teens in a way to empower teens to be more resilient."
  },
  {
    "title": "T2VShield: Model-Agnostic Jailbreak Defense for Text-to-Video Models",
    "url": "http://arxiv.org/abs/2504.15512v1",
    "arxiv_id": "2504.15512v1",
    "authors": [
      "Siyuan Liang",
      "Jiayang Liu",
      "Jiecheng Zhai",
      "Tianmeng Fang",
      "Rongcheng Tu",
      "Aishan Liu",
      "Xiaochun Cao",
      "Dacheng Tao"
    ],
    "published": "2025-04-22T01:18:42+00:00",
    "summary": "The rapid development of generative artificial intelligence has made text to video models essential for building future multimodal world simulators. However, these models remain vulnerable to jailbreak attacks, where specially crafted prompts bypass safety mechanisms and lead to the generation of harmful or unsafe content. Such vulnerabilities undermine the reliability and security of simulation based applications. In this paper, we propose T2VShield, a comprehensive and model agnostic defense framework designed to protect text to video models from jailbreak threats. Our method systematically analyzes the input, model, and output stages to identify the limitations of existing defenses, including semantic ambiguities in prompts, difficulties in detecting malicious content in dynamic video outputs, and inflexible model centric mitigation strategies. T2VShield introduces a prompt rewriting mechanism based on reasoning and multimodal retrieval to sanitize malicious inputs, along with a multi scope detection module that captures local and global inconsistencies across time and modalities. The framework does not require access to internal model parameters and works with both open and closed source systems. Extensive experiments on five platforms show that T2VShield can reduce jailbreak success rates by up to 35 percent compared to strong baselines. We further develop a human centered audiovisual evaluation protocol to assess perceptual safety, emphasizing the importance of visual level defense in enhancing the trustworthiness of next generation multimodal simulators."
  },
  {
    "title": "Automatically Detecting Numerical Instability in Machine Learning Applications via Soft Assertions",
    "url": "http://arxiv.org/abs/2504.15507v1",
    "arxiv_id": "2504.15507v1",
    "authors": [
      "Shaila Sharmin",
      "Anwar Hossain Zahid",
      "Subhankar Bhattacharjee",
      "Chiamaka Igwilo",
      "Miryung Kim",
      "Wei Le"
    ],
    "published": "2025-04-22T00:55:33+00:00",
    "summary": "Machine learning (ML) applications have become an integral part of our lives. ML applications extensively use floating-point computation and involve very large/small numbers; thus, maintaining the numerical stability of such complex computations remains an important challenge. Numerical bugs can lead to system crashes, incorrect output, and wasted computing resources. In this paper, we introduce a novel idea, namely soft assertions (SA), to encode safety/error conditions for the places where numerical instability can occur. A soft assertion is an ML model automatically trained using the dataset obtained during unit testing of unstable functions. Given the values at the unstable function in an ML application, a soft assertion reports how to change these values in order to trigger the instability. We then use the output of soft assertions as signals to effectively mutate inputs to trigger numerical instability in ML applications. In the evaluation, we used the GRIST benchmark, a total of 79 programs, as well as 15 real-world ML applications from GitHub. We compared our tool with 5 state-of-the-art (SOTA) fuzzers. We found all the GRIST bugs and outperformed the baselines. We found 13 numerical bugs in real-world code, one of which had already been confirmed by the GitHub developers. While the baselines mostly found the bugs that report NaN and INF, our tool \\tool found numerical bugs with incorrect output. We showed one case where the Tumor Detection Model, trained on Brain MRI images, should have predicted \"tumor\", but instead, it incorrectly predicted \"no tumor\" due to the numerical bugs. Our replication package is located at https://figshare.com/s/6528d21ccd28bea94c32."
  },
  {
    "title": "Automatically Detecting Numerical Instability in Machine Learning Applications via Soft Assertions",
    "url": "http://arxiv.org/abs/2504.15507v2",
    "arxiv_id": "2504.15507v2",
    "authors": [
      "Shaila Sharmin",
      "Anwar Hossain Zahid",
      "Subhankar Bhattacharjee",
      "Chiamaka Igwilo",
      "Miryung Kim",
      "Wei Le"
    ],
    "published": "2025-04-22T00:55:33+00:00",
    "summary": "Machine learning (ML) applications have become an integral part of our lives. ML applications extensively use floating-point computation and involve very large/small numbers; thus, maintaining the numerical stability of such complex computations remains an important challenge. Numerical bugs can lead to system crashes, incorrect output, and wasted computing resources. In this paper, we introduce a novel idea, namely soft assertions (SA), to encode safety/error conditions for the places where numerical instability can occur. A soft assertion is an ML model automatically trained using the dataset obtained during unit testing of unstable functions. Given the values at the unstable function in an ML application, a soft assertion reports how to change these values in order to trigger the instability. We then use the output of soft assertions as signals to effectively mutate inputs to trigger numerical instability in ML applications. In the evaluation, we used the GRIST benchmark, a total of 79 programs, as well as 15 real-world ML applications from GitHub. We compared our tool with 5 state-of-the-art (SOTA) fuzzers. We found all the GRIST bugs and outperformed the baselines. We found 13 numerical bugs in real-world code, one of which had already been confirmed by the GitHub developers. While the baselines mostly found the bugs that report NaN and INF, our tool \\tool found numerical bugs with incorrect output. We showed one case where the Tumor Detection Model, trained on Brain MRI images, should have predicted \"tumor\", but instead, it incorrectly predicted \"no tumor\" due to the numerical bugs. Our replication package is located at https://figshare.com/s/6528d21ccd28bea94c32."
  },
  {
    "title": "Nearly Optimal Nonlinear Safe Control with BaS-SDRE",
    "url": "http://arxiv.org/abs/2504.15453v1",
    "arxiv_id": "2504.15453v1",
    "authors": [
      "Hassan Almubarak",
      "Maitham F. AL-Sunni",
      "Justin T. Dubbin",
      "Nader Sadegh",
      "John M. Dolan",
      "Evangelos A. Theodorou"
    ],
    "published": "2025-04-21T21:39:49+00:00",
    "summary": "The State-Dependent Riccati Equation (SDRE) approach has emerged as a systematic and effective means of designing nearly optimal nonlinear controllers. The Barrier States (BaS) embedding methodology was developed recently for safe multi-objective controls in which the safety condition is manifested as a state to be controlled along with other states of the system. The overall system, termed the safety embedded system, is highly nonlinear even if the original system is linear. This paper develops a nonlinear nearly optimal safe feedback control technique by combining the two strategies effectively. First, the BaS is derived in an extended linearization formulation to be subsequently used to form an extended safety embedded system. A new optimal control problem is formed thereafter, which is used to construct a safety embedded State-Dependent Riccati Equation, termed BaS-SDRE, whose solution approximates the solution of the optimal control problem's associated Hamilton-Jacobi-Bellman (HJB) equation. The BaS-SDRE is then solved online to synthesize the nearly optimal safe control. The proposed technique's efficacy is demonstrated on an unstable, constrained linear system that shows how the synthesized control reacts to nonlinearities near the unsafe region, a nonlinear flight control system with limited path angular velocity that exists due to structural and dynamic concerns, and a planar quadrotor system that navigates safely in a crowded environment."
  },
  {
    "title": "Virology Capabilities Test (VCT): A Multimodal Virology Q&A Benchmark",
    "url": "http://arxiv.org/abs/2504.16137v1",
    "arxiv_id": "2504.16137v1",
    "authors": [
      "Jasper G\u00f6tting",
      "Pedro Medeiros",
      "Jon G Sanders",
      "Nathaniel Li",
      "Long Phan",
      "Karam Elabd",
      "Lennart Justen",
      "Dan Hendrycks",
      "Seth Donoughe"
    ],
    "published": "2025-04-21T21:04:01+00:00",
    "summary": "We present the Virology Capabilities Test (VCT), a large language model (LLM) benchmark that measures the capability to troubleshoot complex virology laboratory protocols. Constructed from the inputs of dozens of PhD-level expert virologists, VCT consists of $322$ multimodal questions covering fundamental, tacit, and visual knowledge that is essential for practical work in virology laboratories. VCT is difficult: expert virologists with access to the internet score an average of $22.1\\%$ on questions specifically in their sub-areas of expertise. However, the most performant LLM, OpenAI's o3, reaches $43.8\\%$ accuracy, outperforming $94\\%$ of expert virologists even within their sub-areas of specialization. The ability to provide expert-level virology troubleshooting is inherently dual-use: it is useful for beneficial research, but it can also be misused. Therefore, the fact that publicly available models outperform virologists on VCT raises pressing governance considerations. We propose that the capability of LLMs to provide expert-level troubleshooting of dual-use virology work should be integrated into existing frameworks for handling dual-use technologies in the life sciences."
  },
  {
    "title": "Solving Multi-Agent Safe Optimal Control with Distributed Epigraph Form MARL",
    "url": "http://arxiv.org/abs/2504.15425v1",
    "arxiv_id": "2504.15425v1",
    "authors": [
      "Songyuan Zhang",
      "Oswin So",
      "Mitchell Black",
      "Zachary Serlin",
      "Chuchu Fan"
    ],
    "published": "2025-04-21T20:34:55+00:00",
    "summary": "Tasks for multi-robot systems often require the robots to collaborate and complete a team goal while maintaining safety. This problem is usually formalized as a constrained Markov decision process (CMDP), which targets minimizing a global cost and bringing the mean of constraint violation below a user-defined threshold. Inspired by real-world robotic applications, we define safety as zero constraint violation. While many safe multi-agent reinforcement learning (MARL) algorithms have been proposed to solve CMDPs, these algorithms suffer from unstable training in this setting. To tackle this, we use the epigraph form for constrained optimization to improve training stability and prove that the centralized epigraph form problem can be solved in a distributed fashion by each agent. This results in a novel centralized training distributed execution MARL algorithm named Def-MARL. Simulation experiments on 8 different tasks across 2 different simulators show that Def-MARL achieves the best overall performance, satisfies safety constraints, and maintains stable training. Real-world hardware experiments on Crazyflie quadcopters demonstrate the ability of Def-MARL to safely coordinate agents to complete complex collaborative tasks compared to other methods."
  },
  {
    "title": "Safety Embedded Adaptive Control Using Barrier States",
    "url": "http://arxiv.org/abs/2504.15423v1",
    "arxiv_id": "2504.15423v1",
    "authors": [
      "Maitham F. AL-Sunni",
      "Hassan Almubarak",
      "John M. Dolan"
    ],
    "published": "2025-04-21T20:29:38+00:00",
    "summary": "In this work, we explore the application of barrier states (BaS) in the realm of safe nonlinear adaptive control. Our proposed framework derives barrier states for systems with parametric uncertainty, which are augmented into the uncertain dynamical model. We employ an adaptive nonlinear control strategy based on a control Lyapunov functions approach to design a stabilizing controller for the augmented system. The developed theory shows that the controller ensures safe control actions for the original system while meeting specified performance objectives. We validate the effectiveness of our approach through simulations on diverse systems, including a planar quadrotor subject to unknown drag forces and an adaptive cruise control system, for which we provide comparisons with existing methodologies."
  },
  {
    "title": "$k$-Inductive and Interpolation-Inspired Barrier Certificates for Stochastic Dynamical Systems",
    "url": "http://arxiv.org/abs/2504.15412v1",
    "arxiv_id": "2504.15412v1",
    "authors": [
      "Mohammed Adib Oumer",
      "Vishnu Murali",
      "Majid Zamani"
    ],
    "published": "2025-04-21T19:41:43+00:00",
    "summary": "We introduce two notions of barrier certificates that use multiple functions to provide a lower bound on the probabilistic satisfaction of safety for stochastic dynamical systems. A barrier certificate for a stochastic dynamical system acts as a nonnegative supermartingale, and provides a lower bound on the probability that the system is safe. The promise of such certificates is that their search can be effectively automated. Typically, one may use optimization or SMT solvers to find such barrier certificates of a given fixed template. When such approaches fail, a typical approach is to instead change the template. We propose an alternative approach that we dub interpolation-inspired barrier certificates. An interpolation-inspired barrier certificate consists of a set of functions that jointly provide a lower bound on the probability of satisfying safety. We show how one may find such certificates of a fixed template, even when we fail to find standard barrier certificates of the same template. However, we note that such certificates still need to ensure a supermartingale guarantee for one function in the set. To address this challenge, we consider the use of $k$-induction with these interpolation-inspired certificates. The recent use of $k$-induction in barrier certificates allows one to relax the supermartingale requirement at every time step to a combination of a supermartingale requirement every $k$ steps and a $c$-martingale requirement for the intermediate steps. We provide a generic formulation of a barrier certificate that we dub $k$-inductive interpolation-inspired barrier certificate. The formulation allows for several combinations of interpolation and $k$-induction for barrier certificate. We present two examples among the possible combinations. We finally present sum-of-squares programming to synthesize this set of functions and demonstrate their utility in case studies."
  },
  {
    "title": "Multimodal Large Language Models for Enhanced Traffic Safety: A Comprehensive Review and Future Trends",
    "url": "http://arxiv.org/abs/2504.16134v1",
    "arxiv_id": "2504.16134v1",
    "authors": [
      "Mohammad Abu Tami",
      "Mohammed Elhenawy",
      "Huthaifa I. Ashqar"
    ],
    "published": "2025-04-21T18:48:35+00:00",
    "summary": "Traffic safety remains a critical global challenge, with traditional Advanced Driver-Assistance Systems (ADAS) often struggling in dynamic real-world scenarios due to fragmented sensor processing and susceptibility to adversarial conditions. This paper reviews the transformative potential of Multimodal Large Language Models (MLLMs) in addressing these limitations by integrating cross-modal data such as visual, spatial, and environmental inputs to enable holistic scene understanding. Through a comprehensive analysis of MLLM-based approaches, we highlight their capabilities in enhancing perception, decision-making, and adversarial robustness, while also examining the role of key datasets (e.g., KITTI, DRAMA, ML4RoadSafety) in advancing research. Furthermore, we outline future directions, including real-time edge deployment, causality-driven reasoning, and human-AI collaboration. By positioning MLLMs as a cornerstone for next-generation traffic safety systems, this review underscores their potential to revolutionize the field, offering scalable, context-aware solutions that proactively mitigate risks and improve overall road safety."
  },
  {
    "title": "A Conceptual Framework for AI-based Decision Systems in Critical Infrastructures",
    "url": "http://arxiv.org/abs/2504.16133v1",
    "arxiv_id": "2504.16133v1",
    "authors": [
      "Milad Leyli-abadi",
      "Ricardo J. Bessa",
      "Jan Viebahn",
      "Daniel Boos",
      "Clark Borst",
      "Alberto Castagna",
      "Ricardo Chavarriaga",
      "Mohamed Hassouna",
      "Bruno Lemetayer",
      "Giulia Leto",
      "Antoine Marot",
      "Maroua Meddeb",
      "Manuel Meyer",
      "Viola Schiaffonati",
      "Manuel Schneider",
      "Toni Waefler"
    ],
    "published": "2025-04-21T18:38:26+00:00",
    "summary": "The interaction between humans and AI in safety-critical systems presents a unique set of challenges that remain partially addressed by existing frameworks. These challenges stem from the complex interplay of requirements for transparency, trust, and explainability, coupled with the necessity for robust and safe decision-making. A framework that holistically integrates human and AI capabilities while addressing these concerns is notably required, bridging the critical gaps in designing, deploying, and maintaining safe and effective systems. This paper proposes a holistic conceptual framework for critical infrastructures by adopting an interdisciplinary approach. It integrates traditionally distinct fields such as mathematics, decision theory, computer science, philosophy, psychology, and cognitive engineering and draws on specialized engineering domains, particularly energy, mobility, and aeronautics. The flexibility in its adoption is also demonstrated through its instantiation on an already existing framework."
  },
  {
    "title": "Interpretable Locomotion Prediction in Construction Using a Memory-Driven LLM Agent With Chain-of-Thought Reasoning",
    "url": "http://arxiv.org/abs/2504.15263v1",
    "arxiv_id": "2504.15263v1",
    "authors": [
      "Ehsan Ahmadi",
      "Chao Wang"
    ],
    "published": "2025-04-21T17:45:21+00:00",
    "summary": "Construction tasks are inherently unpredictable, with dynamic environments and safety-critical demands posing significant risks to workers. Exoskeletons offer potential assistance but falter without accurate intent recognition across diverse locomotion modes. This paper presents a locomotion prediction agent leveraging Large Language Models (LLMs) augmented with memory systems, aimed at improving exoskeleton assistance in such settings. Using multimodal inputs - spoken commands and visual data from smart glasses - the agent integrates a Perception Module, Short-Term Memory (STM), Long-Term Memory (LTM), and Refinement Module to predict locomotion modes effectively. Evaluation reveals a baseline weighted F1-score of 0.73 without memory, rising to 0.81 with STM, and reaching 0.90 with both STM and LTM, excelling with vague and safety-critical commands. Calibration metrics, including a Brier Score drop from 0.244 to 0.090 and ECE from 0.222 to 0.044, affirm improved reliability. This framework supports safer, high-level human-exoskeleton collaboration, with promise for adaptive assistive systems in dynamic industries."
  },
  {
    "title": "Leveraging Language Models for Automated Patient Record Linkage",
    "url": "http://arxiv.org/abs/2504.15261v1",
    "arxiv_id": "2504.15261v1",
    "authors": [
      "Mohammad Beheshti",
      "Lovedeep Gondara",
      "Iris Zachary"
    ],
    "published": "2025-04-21T17:41:15+00:00",
    "summary": "Objective: Healthcare data fragmentation presents a major challenge for linking patient data, necessitating robust record linkage to integrate patient records from diverse sources. This study investigates the feasibility of leveraging language models for automated patient record linkage, focusing on two key tasks: blocking and matching. Materials and Methods: We utilized real-world healthcare data from the Missouri Cancer Registry and Research Center, linking patient records from two independent sources using probabilistic linkage as a baseline. A transformer-based model, RoBERTa, was fine-tuned for blocking using sentence embeddings. For matching, several language models were experimented under fine-tuned and zero-shot settings, assessing their performance against ground truth labels. Results: The fine-tuned blocking model achieved a 92% reduction in the number of candidate pairs while maintaining near-perfect recall. In the matching task, fine-tuned Mistral-7B achieved the best performance with only 6 incorrect predictions. Among zero-shot models, Mistral-Small-24B performed best, with a total of 55 incorrect predictions. Discussion: Fine-tuned language models achieved strong performance in patient record blocking and matching with minimal errors. However, they remain less accurate and efficient than a hybrid rule-based and probabilistic approach for blocking. Additionally, reasoning models like DeepSeek-R1 are impractical for large-scale record linkage due to high computational costs. Conclusion: This study highlights the potential of language models for automating patient record linkage, offering improved efficiency by eliminating the manual efforts required to perform patient record linkage. Overall, language models offer a scalable solution that can enhance data integration, reduce manual effort, and support disease surveillance and research."
  },
  {
    "title": "FlowReasoner: Reinforcing Query-Level Meta-Agents",
    "url": "http://arxiv.org/abs/2504.15257v1",
    "arxiv_id": "2504.15257v1",
    "authors": [
      "Hongcheng Gao",
      "Yue Liu",
      "Yufei He",
      "Longxu Dou",
      "Chao Du",
      "Zhijie Deng",
      "Bryan Hooi",
      "Min Lin",
      "Tianyu Pang"
    ],
    "published": "2025-04-21T17:35:42+00:00",
    "summary": "This paper proposes a query-level meta-agent named FlowReasoner to automate the design of query-level multi-agent systems, i.e., one system per user query. Our core idea is to incentivize a reasoning-based meta-agent via external execution feedback. Concretely, by distilling DeepSeek R1, we first endow the basic reasoning ability regarding the generation of multi-agent systems to FlowReasoner. Then, we further enhance it via reinforcement learning (RL) with external execution feedback. A multi-purpose reward is designed to guide the RL training from aspects of performance, complexity, and efficiency. In this manner, FlowReasoner is enabled to generate a personalized multi-agent system for each user query via deliberative reasoning. Experiments on both engineering and competition code benchmarks demonstrate the superiority of FlowReasoner. Remarkably, it surpasses o1-mini by 10.52% accuracy across three benchmarks. The code is available at https://github.com/sail-sg/FlowReasoner."
  },
  {
    "title": "CRUST-Bench: A Comprehensive Benchmark for C-to-safe-Rust Transpilation",
    "url": "http://arxiv.org/abs/2504.15254v1",
    "arxiv_id": "2504.15254v1",
    "authors": [
      "Anirudh Khatry",
      "Robert Zhang",
      "Jia Pan",
      "Ziteng Wang",
      "Qiaochu Chen",
      "Greg Durrett",
      "Isil Dillig"
    ],
    "published": "2025-04-21T17:33:33+00:00",
    "summary": "C-to-Rust transpilation is essential for modernizing legacy C code while enhancing safety and interoperability with modern Rust ecosystems. However, no dataset currently exists for evaluating whether a system can transpile C into safe Rust that passes a set of test cases. We introduce CRUST-Bench, a dataset of 100 C repositories, each paired with manually-written interfaces in safe Rust as well as test cases that can be used to validate correctness of the transpilation. By considering entire repositories rather than isolated functions, CRUST-Bench captures the challenges of translating complex projects with dependencies across multiple files. The provided Rust interfaces provide explicit specifications that ensure adherence to idiomatic, memory-safe Rust patterns, while the accompanying test cases enforce functional correctness. We evaluate state-of-the-art large language models (LLMs) on this task and find that safe and idiomatic Rust generation is still a challenging problem for various state-of-the-art methods and techniques. We also provide insights into the errors LLMs usually make in transpiling code from C to safe Rust. The best performing model, OpenAI o1, is able to solve only 15 tasks in a single-shot setting. Improvements on CRUST-Bench would lead to improved transpilation systems that can reason about complex scenarios and help in migrating legacy codebases from C into languages like Rust that ensure memory safety. You can find the dataset and code at https://github.com/anirudhkhatry/CRUST-bench."
  },
  {
    "title": "MR. Guard: Multilingual Reasoning Guardrail using Curriculum Learning",
    "url": "http://arxiv.org/abs/2504.15241v1",
    "arxiv_id": "2504.15241v1",
    "authors": [
      "Yahan Yang",
      "Soham Dan",
      "Shuo Li",
      "Dan Roth",
      "Insup Lee"
    ],
    "published": "2025-04-21T17:15:06+00:00",
    "summary": "Large Language Models (LLMs) are susceptible to adversarial attacks such as jailbreaking, which can elicit harmful or unsafe behaviors. This vulnerability is exacerbated in multilingual setting, where multilingual safety-aligned data are often limited. Thus, developing a guardrail capable of detecting and filtering unsafe content across diverse languages is critical for deploying LLMs in real-world applications. In this work, we propose an approach to build a multilingual guardrail with reasoning. Our method consists of: (1) synthetic multilingual data generation incorporating culturally and linguistically nuanced variants, (2) supervised fine-tuning, and (3) a curriculum-guided Group Relative Policy Optimization (GRPO) framework that further improves performance. Experimental results demonstrate that our multilingual guardrail consistently outperforms recent baselines across both in-domain and out-of-domain languages. The multilingual reasoning capability of our guardrail enables it to generate multilingual explanations, which are particularly useful for understanding language-specific risks and ambiguities in multilingual content moderation."
  },
  {
    "title": "A Genetic Fuzzy-Enabled Framework on Robotic Manipulation for In-Space Servicing",
    "url": "http://arxiv.org/abs/2504.15226v1",
    "arxiv_id": "2504.15226v1",
    "authors": [
      "Nathan Steffen",
      "Wilhelm Louw",
      "Nicholas Ernest",
      "Timothy Arnett",
      "Kelly Cohen"
    ],
    "published": "2025-04-21T16:57:56+00:00",
    "summary": "Automation of robotic systems for servicing in cislunar space is becoming extremely important as the number of satellites in orbit increases. Safety is critical in performing satellite maintenance, so the control techniques utilized must be trusted in addition to being highly efficient. In this work, Genetic Fuzzy Trees are combined with the widely used LQR control scheme via Thales' TrUE AI Toolkit to create a trusted and efficient controller for a two-degree-of-freedom planar robotic manipulator that would theoretically be used to perform satellite maintenance. It was found that Genetic Fuzzy-LQR is 18.5% more performant than optimal LQR on average, and that it is incredibly robust to uncertainty."
  },
  {
    "title": "Existing Industry Practice for the EU AI Act's General-Purpose AI Code of Practice Safety and Security Measures",
    "url": "http://arxiv.org/abs/2504.15181v1",
    "arxiv_id": "2504.15181v1",
    "authors": [
      "Lily Stelling",
      "Mick Yang",
      "Rokas Gipi\u0161kis",
      "Leon Staufer",
      "Ze Shen Chin",
      "Sim\u00e9on Campos",
      "Michael Chen"
    ],
    "published": "2025-04-21T15:44:01+00:00",
    "summary": "This report provides a detailed comparison between the measures proposed in the EU AI Act's General-Purpose AI (GPAI) Code of Practice (Third Draft) and current practices adopted by leading AI companies. As the EU moves toward enforcing binding obligations for GPAI model providers, the Code of Practice will be key to bridging legal requirements with concrete technical commitments. Our analysis focuses on the draft's Safety and Security section which is only relevant for the providers of the most advanced models (Commitments II.1-II.16) and excerpts from current public-facing documents quotes that are relevant to each individual measure.   We systematically reviewed different document types - including companies' frontier safety frameworks and model cards - from over a dozen companies, including OpenAI, Anthropic, Google DeepMind, Microsoft, Meta, Amazon, and others. This report is not meant to be an indication of legal compliance nor does it take any prescriptive viewpoint about the Code of Practice or companies' policies. Instead, it aims to inform the ongoing dialogue between regulators and GPAI model providers by surfacing evidence of precedent."
  },
  {
    "title": "C2RUST-BENCH: A Minimized, Representative Dataset for C-to-Rust Transpilation Evaluation",
    "url": "http://arxiv.org/abs/2504.15144v1",
    "arxiv_id": "2504.15144v1",
    "authors": [
      "Melih Sirlanci",
      "Carter Yagemann",
      "Zhiqiang Lin"
    ],
    "published": "2025-04-21T14:48:45+00:00",
    "summary": "Despite the effort in vulnerability detection over the last two decades, memory safety vulnerabilities continue to be a critical problem. Recent reports suggest that the key solution is to migrate to memory-safe languages. To this end, C-to-Rust transpilation becomes popular to resolve memory-safety issues in C programs. Recent works propose C-to-Rust transpilation frameworks; however, a comprehensive evaluation dataset is missing. Although one solution is to put together a large enough dataset, this increases the analysis time in automated frameworks as well as in manual efforts for some cases. In this work, we build a method to select functions from a large set to construct a minimized yet representative dataset to evaluate the C-to-Rust transpilation. We propose C2RUST-BENCH that contains 2,905 functions, which are representative of C-to-Rust transpilation, selected from 15,503 functions of real-world programs."
  },
  {
    "title": "EasyEdit2: An Easy-to-use Steering Framework for Editing Large Language Models",
    "url": "http://arxiv.org/abs/2504.15133v1",
    "arxiv_id": "2504.15133v1",
    "authors": [
      "Ziwen Xu",
      "Shuxun Wang",
      "Kewei Xu",
      "Haoming Xu",
      "Mengru Wang",
      "Xinle Deng",
      "Yunzhi Yao",
      "Guozhou Zheng",
      "Huajun Chen",
      "Ningyu Zhang"
    ],
    "published": "2025-04-21T14:33:55+00:00",
    "summary": "In this paper, we introduce EasyEdit2, a framework designed to enable plug-and-play adjustability for controlling Large Language Model (LLM) behaviors. EasyEdit2 supports a wide range of test-time interventions, including safety, sentiment, personality, reasoning patterns, factuality, and language features. Unlike its predecessor, EasyEdit2 features a new architecture specifically designed for seamless model steering. It comprises key modules such as the steering vector generator and the steering vector applier, which enable automatic generation and application of steering vectors to influence the model's behavior without modifying its parameters. One of the main advantages of EasyEdit2 is its ease of use-users do not need extensive technical knowledge. With just a single example, they can effectively guide and adjust the model's responses, making precise control both accessible and efficient. Empirically, we report model steering performance across different LLMs, demonstrating the effectiveness of these techniques. We have released the source code on GitHub at https://github.com/zjunlp/EasyEdit along with a demonstration notebook. In addition, we provide a demo video at https://zjunlp.github.io/project/EasyEdit2/video for a quick introduction."
  },
  {
    "title": "Safety Co-Option and Compromised National Security: The Self-Fulfilling Prophecy of Weakened AI Risk Thresholds",
    "url": "http://arxiv.org/abs/2504.15088v1",
    "arxiv_id": "2504.15088v1",
    "authors": [
      "Heidy Khlaaf",
      "Sarah Myers West"
    ],
    "published": "2025-04-21T13:20:56+00:00",
    "summary": "Risk thresholds provide a measure of the level of risk exposure that a society or individual is willing to withstand, ultimately shaping how we determine the safety of technological systems. Against the backdrop of the Cold War, the first risk analyses, such as those devised for nuclear systems, cemented societally accepted risk thresholds against which safety-critical and defense systems are now evaluated. But today, the appropriate risk tolerances for AI systems have yet to be agreed on by global governing efforts, despite the need for democratic deliberation regarding the acceptable levels of harm to human life. Absent such AI risk thresholds, AI technologists-primarily industry labs, as well as \"AI safety\" focused organizations-have instead advocated for risk tolerances skewed by a purported AI arms race and speculative \"existential\" risks, taking over the arbitration of risk determinations with life-or-death consequences, subverting democratic processes.   In this paper, we demonstrate how such approaches have allowed AI technologists to engage in \"safety revisionism,\" substituting traditional safety methods and terminology with ill-defined alternatives that vie for the accelerated adoption of military AI uses at the cost of lowered safety and security thresholds. We explore how the current trajectory for AI risk determination and evaluation for foundation model use within national security is poised for a race to the bottom, to the detriment of the US's national security interests. Safety-critical and defense systems must comply with assurance frameworks that are aligned with established risk thresholds, and foundation models are no exception. As such, development of evaluation frameworks for AI-based military systems must preserve the safety and security of US critical and defense infrastructure, and remain in alignment with international humanitarian law."
  },
  {
    "title": "RainbowPlus: Enhancing Adversarial Prompt Generation via Evolutionary Quality-Diversity Search",
    "url": "http://arxiv.org/abs/2504.15047v1",
    "arxiv_id": "2504.15047v1",
    "authors": [
      "Quy-Anh Dang",
      "Chris Ngo",
      "Truong-Son Hy"
    ],
    "published": "2025-04-21T12:04:57+00:00",
    "summary": "Large Language Models (LLMs) exhibit remarkable capabilities but are susceptible to adversarial prompts that exploit vulnerabilities to produce unsafe or biased outputs. Existing red-teaming methods often face scalability challenges, resource-intensive requirements, or limited diversity in attack strategies. We propose RainbowPlus, a novel red-teaming framework rooted in evolutionary computation, enhancing adversarial prompt generation through an adaptive quality-diversity (QD) search that extends classical evolutionary algorithms like MAP-Elites with innovations tailored for language models. By employing a multi-element archive to store diverse high-quality prompts and a comprehensive fitness function to evaluate multiple prompts concurrently, RainbowPlus overcomes the constraints of single-prompt archives and pairwise comparisons in prior QD methods like Rainbow Teaming. Experiments comparing RainbowPlus to QD methods across six benchmark datasets and four open-source LLMs demonstrate superior attack success rate (ASR) and diversity (Diverse-Score $\\approx 0.84$), generating up to 100 times more unique prompts (e.g., 10,418 vs. 100 for Ministral-8B-Instruct-2410). Against nine state-of-the-art methods on the HarmBench dataset with twelve LLMs (ten open-source, two closed-source), RainbowPlus achieves an average ASR of 81.1%, surpassing AutoDAN-Turbo by 3.9%, and is 9 times faster (1.45 vs. 13.50 hours). Our open-source implementation fosters further advancements in LLM safety, offering a scalable tool for vulnerability assessment. Code and resources are publicly available at https://github.com/knoveleng/rainbowplus, supporting reproducibility and future research in LLM red-teaming."
  },
  {
    "title": "aiXamine: LLM Safety and Security Simplified",
    "url": "http://arxiv.org/abs/2504.14985v1",
    "arxiv_id": "2504.14985v1",
    "authors": [
      "Fatih Deniz",
      "Dorde Popovic",
      "Yazan Boshmaf",
      "Euisuh Jeong",
      "Minhaj Ahmad",
      "Sanjay Chawla",
      "Issa Khalil"
    ],
    "published": "2025-04-21T09:26:05+00:00",
    "summary": "Evaluating Large Language Models (LLMs) for safety and security remains a complex task, often requiring users to navigate a fragmented landscape of ad hoc benchmarks, datasets, metrics, and reporting formats. To address this challenge, we present aiXamine, a comprehensive black-box evaluation platform for LLM safety and security. aiXamine integrates over 40 tests (i.e., benchmarks) organized into eight key services targeting specific dimensions of safety and security: adversarial robustness, code security, fairness and bias, hallucination, model and data privacy, out-of-distribution (OOD) robustness, over-refusal, and safety alignment. The platform aggregates the evaluation results into a single detailed report per model, providing a detailed breakdown of model performance, test examples, and rich visualizations. We used aiXamine to assess over 50 publicly available and proprietary LLMs, conducting over 2K examinations. Our findings reveal notable vulnerabilities in leading models, including susceptibility to adversarial attacks in OpenAI's GPT-4o, biased outputs in xAI's Grok-3, and privacy weaknesses in Google's Gemini 2.0. Additionally, we observe that open-source models can match or exceed proprietary models in specific services such as safety alignment, fairness and bias, and OOD robustness. Finally, we identify trade-offs between distillation strategies, model size, training methods, and architectural choices."
  },
  {
    "title": "Distributed Time-Varying Gaussian Regression via Kalman Filtering",
    "url": "http://arxiv.org/abs/2504.14900v1",
    "arxiv_id": "2504.14900v1",
    "authors": [
      "Nicola Taddei",
      "Riccardo Maggioni",
      "Jaap Eising",
      "Giulia De Pasquale",
      "Florian Dorfler"
    ],
    "published": "2025-04-21T07:12:05+00:00",
    "summary": "We consider the problem of learning time-varying functions in a distributed fashion, where agents collect local information to collaboratively achieve a shared estimate. This task is particularly relevant in control applications, whenever real-time and robust estimation of dynamic cost/reward functions in safety critical settings has to be performed. In this paper, we,adopt a finite-dimensional approximation of a Gaussian Process, corresponding to a Bayesian linear regression in an appropriate feature space, and propose a new algorithm, DistKP, to track the time-varying coefficients via a distributed Kalman filter. The proposed method works for arbitrary kernels and under weaker assumptions on the time-evolution of the function to learn compared to the literature. We validate our results using a simulation example in which a fleet of Unmanned Aerial Vehicles (UAVs) learns a dynamically changing wind field."
  },
  {
    "title": "Retrieval Augmented Generation Evaluation in the Era of Large Language Models: A Comprehensive Survey",
    "url": "http://arxiv.org/abs/2504.14891v1",
    "arxiv_id": "2504.14891v1",
    "authors": [
      "Aoran Gan",
      "Hao Yu",
      "Kai Zhang",
      "Qi Liu",
      "Wenyu Yan",
      "Zhenya Huang",
      "Shiwei Tong",
      "Guoping Hu"
    ],
    "published": "2025-04-21T06:39:47+00:00",
    "summary": "Recent advancements in Retrieval-Augmented Generation (RAG) have revolutionized natural language processing by integrating Large Language Models (LLMs) with external information retrieval, enabling accurate, up-to-date, and verifiable text generation across diverse applications. However, evaluating RAG systems presents unique challenges due to their hybrid architecture that combines retrieval and generation components, as well as their dependence on dynamic knowledge sources in the LLM era. In response, this paper provides a comprehensive survey of RAG evaluation methods and frameworks, systematically reviewing traditional and emerging evaluation approaches, for system performance, factual accuracy, safety, and computational efficiency in the LLM era. We also compile and categorize the RAG-specific datasets and evaluation frameworks, conducting a meta-analysis of evaluation practices in high-impact RAG research. To the best of our knowledge, this work represents the most comprehensive survey for RAG evaluation, bridging traditional and LLM-driven methods, and serves as a critical resource for advancing RAG development."
  },
  {
    "title": "ReCraft: Self-Contained Split, Merge, and Membership Change of Raft Protocol",
    "url": "http://arxiv.org/abs/2504.14802v1",
    "arxiv_id": "2504.14802v1",
    "authors": [
      "Kezhi Xiong",
      "Soonwon Moon",
      "Joshua Kang",
      "Bryant Curto",
      "Jieung Kim",
      "Ji-Yong Shin"
    ],
    "published": "2025-04-21T02:05:06+00:00",
    "summary": "Designing reconfiguration schemes for consensus protocols is challenging because subtle corner cases during reconfiguration could invalidate the correctness of the protocol. Thus, most systems that embed consensus protocols conservatively implement the reconfiguration and refrain from developing an efficient scheme. Existing implementations often stop the entire system during reconfiguration and rely on a centralized coordinator, which can become a single point of failure. We present ReCraft, a novel reconfiguration protocol for Raft, which supports multi- and single-cluster-level reconfigurations. ReCraft does not rely on external coordinators and blocks minimally. ReCraft enables the sharding of Raft clusters with split and merge reconfigurations and adds a membership change scheme that improves Raft. We prove the safety and liveness of ReCraft and demonstrate its efficiency through implementations in etcd."
  },
  {
    "title": "Safe Autonomous Environmental Contact for Soft Robots using Control Barrier Functions",
    "url": "http://arxiv.org/abs/2504.14755v1",
    "arxiv_id": "2504.14755v1",
    "authors": [
      "Akua K. Dickson",
      "Juan C. Pacheco Garcia",
      "Meredith L. Anderson",
      "Ran Jing",
      "Sarah Alizadeh-Shabdiz",
      "Audrey X. Wang",
      "Charles DeLorey",
      "Zach J. Patterson",
      "Andrew P. Sabelhaus"
    ],
    "published": "2025-04-20T22:31:55+00:00",
    "summary": "Robots built from soft materials will inherently apply lower environmental forces than their rigid counterparts, and therefore may be more suitable in sensitive settings with unintended contact. However, these robots' applied forces result from both their design and their control system in closed-loop, and therefore, ensuring bounds on these forces requires controller synthesis for safety as well. This article introduces the first feedback controller for a soft manipulator that formally meets a safety specification with respect to environmental contact. In our proof-of-concept setting, the robot's environment has known geometry and is deformable with a known elastic modulus. Our approach maps a bound on applied forces to a safe set of positions of the robot's tip via predicted deformations of the environment. Then, a quadratic program with Control Barrier Functions in its constraints is used to supervise a nominal feedback signal, verifiably maintaining the robot's tip within this safe set. Hardware experiments on a multi-segment soft pneumatic robot demonstrate that the proposed framework successfully constrains its environmental contact forces. This framework represents a fundamental shift in perspective on control and safety for soft robots, defining and implementing a formally verifiable logic specification on their pose and contact forces."
  },
  {
    "title": "Adaptive Field Effect Planner for Safe Interactive Autonomous Driving on Curved Roads",
    "url": "http://arxiv.org/abs/2504.14747v1",
    "arxiv_id": "2504.14747v1",
    "authors": [
      "Qinghao Li",
      "Zhen Tian",
      "Xiaodan Wang",
      "Jinming Yang",
      "Zhihao Lin"
    ],
    "published": "2025-04-20T21:41:19+00:00",
    "summary": "Autonomous driving has garnered significant attention for its potential to improve safety, traffic efficiency, and user convenience. However, the dynamic and complex nature of interactive driving poses significant challenges, including the need to navigate non-linear road geometries, handle dynamic obstacles, and meet stringent safety and comfort requirements. Traditional approaches, such as artificial potential fields (APF), often fall short in addressing these complexities independently, necessitating the development of integrated and adaptive frameworks. This paper presents a novel approach to autonomous vehicle navigation that integrates artificial potential fields, Frenet coordinates, and improved particle swarm optimization (IPSO). A dynamic risk field, adapted from traditional APF, is proposed to ensure interactive safety by quantifying risks and dynamically adjusting lane-changing intentions based on surrounding vehicle behavior. Frenet coordinates are utilized to simplify trajectory planning on non-straight roads, while an enhanced quintic polynomial trajectory generator ensures smooth and comfortable path transitions. Additionally, an IPSO algorithm optimizes trajectory selection in real time, balancing safety and user comfort within a feasible input range. The proposed framework is validated through extensive simulations and real-world scenarios, demonstrating its ability to navigate complex traffic environments, maintain safety margins, and generate smooth, dynamically feasible trajectories."
  },
  {
    "title": "Efficient and Safe Planner for Automated Driving on Ramps Considering Unsatisfication",
    "url": "http://arxiv.org/abs/2504.15320v1",
    "arxiv_id": "2504.15320v1",
    "authors": [
      "Qinghao Li",
      "Zhen Tian",
      "Xiaodan Wang",
      "Jinming Yang",
      "Zhihao Lin"
    ],
    "published": "2025-04-20T21:39:51+00:00",
    "summary": "Automated driving on ramps presents significant challenges due to the need to balance both safety and efficiency during lane changes. This paper proposes an integrated planner for automated vehicles (AVs) on ramps, utilizing an unsatisfactory level metric for efficiency and arrow-cluster-based sampling for safety. The planner identifies optimal times for the AV to change lanes, taking into account the vehicle's velocity as a key factor in efficiency. Additionally, the integrated planner employs arrow-cluster-based sampling to evaluate collision risks and select an optimal lane-changing curve. Extensive simulations were conducted in a ramp scenario to verify the planner's efficient and safe performance. The results demonstrate that the proposed planner can effectively select an appropriate lane-changing time point and a safe lane-changing curve for AVs, without incurring any collisions during the maneuver."
  },
  {
    "title": "Can We Ignore Labels In Out of Distribution Detection?",
    "url": "http://arxiv.org/abs/2504.14704v1",
    "arxiv_id": "2504.14704v1",
    "authors": [
      "Hong Yang",
      "Qi Yu",
      "Travis Desel"
    ],
    "published": "2025-04-20T18:37:51+00:00",
    "summary": "Out-of-distribution (OOD) detection methods have recently become more prominent, serving as a core element in safety-critical autonomous systems. One major purpose of OOD detection is to reject invalid inputs that could lead to unpredictable errors and compromise safety. Due to the cost of labeled data, recent works have investigated the feasibility of self-supervised learning (SSL) OOD detection, unlabeled OOD detection, and zero shot OOD detection. In this work, we identify a set of conditions for a theoretical guarantee of failure in unlabeled OOD detection algorithms from an information-theoretic perspective. These conditions are present in all OOD tasks dealing with real-world data: I) we provide theoretical proof of unlabeled OOD detection failure when there exists zero mutual information between the learning objective and the in-distribution labels, a.k.a. 'label blindness', II) we define a new OOD task - Adjacent OOD detection - that tests for label blindness and accounts for a previously ignored safety gap in all OOD detection benchmarks, and III) we perform experiments demonstrating that existing unlabeled OOD methods fail under conditions suggested by our label blindness theory and analyze the implications for future research in unlabeled OOD methods."
  },
  {
    "title": "A Byzantine Fault Tolerance Approach towards AI Safety",
    "url": "http://arxiv.org/abs/2504.14668v1",
    "arxiv_id": "2504.14668v1",
    "authors": [
      "John deVadoss",
      "Matthias Artzt"
    ],
    "published": "2025-04-20T16:18:06+00:00",
    "summary": "Ensuring that an AI system behaves reliably and as intended, especially in the presence of unexpected faults or adversarial conditions, is a complex challenge. Inspired by the field of Byzantine Fault Tolerance (BFT) from distributed computing, we explore a fault tolerance architecture for AI safety. By drawing an analogy between unreliable, corrupt, misbehaving or malicious AI artifacts and Byzantine nodes in a distributed system, we propose an architecture that leverages consensus mechanisms to enhance AI safety and reliability."
  },
  {
    "title": "BLACKOUT: Data-Oblivious Computation with Blinded Capabilities",
    "url": "http://arxiv.org/abs/2504.14654v1",
    "arxiv_id": "2504.14654v1",
    "authors": [
      "Hossam ElAtali",
      "Merve G\u00fclmez",
      "Thomas Nyman",
      "N. Asokan"
    ],
    "published": "2025-04-20T15:25:59+00:00",
    "summary": "Lack of memory-safety and exposure to side channels are two prominent, persistent challenges for the secure implementation of software. Memory-safe programming languages promise to significantly reduce the prevalence of memory-safety bugs, but make it more difficult to implement side-channel-resistant code. We aim to address both memory-safety and side-channel resistance by augmenting memory-safe hardware with the ability for data-oblivious programming. We describe an extension to the CHERI capability architecture to provide blinded capabilities that allow data-oblivious computation to be carried out by userspace tasks. We also present BLACKOUT, our realization of blinded capabilities on a FPGA softcore based on the speculative out-of-order CHERI-Toooba processor and extend the CHERI-enabled Clang/LLVM compiler and the CheriBSD operating system with support for blinded capabilities. BLACKOUT makes writing side-channel-resistant code easier by making non-data-oblivious operations via blinded capabilities explicitly fault. Through rigorous evaluation we show that BLACKOUT ensures memory operated on through blinded capabilities is securely allocated, used, and reclaimed and demonstrate that, in benchmarks comparable to those used by previous work, BLACKOUT imposes only a small performance degradation (1.5% geometric mean) compared to the baseline CHERI-Toooba processor."
  },
  {
    "title": "A Framework for Benchmarking and Aligning Task-Planning Safety in LLM-Based Embodied Agents",
    "url": "http://arxiv.org/abs/2504.14650v1",
    "arxiv_id": "2504.14650v1",
    "authors": [
      "Yuting Huang",
      "Leilei Ding",
      "Zhipeng Tang",
      "Tianfu Wang",
      "Xinrui Lin",
      "Wuyang Zhang",
      "Mingxiao Ma",
      "Yanyong Zhang"
    ],
    "published": "2025-04-20T15:12:14+00:00",
    "summary": "Large Language Models (LLMs) exhibit substantial promise in enhancing task-planning capabilities within embodied agents due to their advanced reasoning and comprehension. However, the systemic safety of these agents remains an underexplored frontier. In this study, we present Safe-BeAl, an integrated framework for the measurement (SafePlan-Bench) and alignment (Safe-Align) of LLM-based embodied agents' behaviors. SafePlan-Bench establishes a comprehensive benchmark for evaluating task-planning safety, encompassing 2,027 daily tasks and corresponding environments distributed across 8 distinct hazard categories (e.g., Fire Hazard). Our empirical analysis reveals that even in the absence of adversarial inputs or malicious intent, LLM-based agents can exhibit unsafe behaviors. To mitigate these hazards, we propose Safe-Align, a method designed to integrate physical-world safety knowledge into LLM-based embodied agents while maintaining task-specific performance. Experiments across a variety of settings demonstrate that Safe-BeAl provides comprehensive safety validation, improving safety by 8.55 - 15.22%, compared to embodied agents based on GPT-4, while ensuring successful task completion."
  },
  {
    "title": "Surrogate Fitness Metrics for Interpretable Reinforcement Learning",
    "url": "http://arxiv.org/abs/2504.14645v1",
    "arxiv_id": "2504.14645v1",
    "authors": [
      "Philipp Altmann",
      "C\u00e9line Davignon",
      "Maximilian Zorn",
      "Fabian Ritz",
      "Claudia Linnhoff-Popien",
      "Thomas Gabor"
    ],
    "published": "2025-04-20T15:01:19+00:00",
    "summary": "We employ an evolutionary optimization framework that perturbs initial states to generate informative and diverse policy demonstrations. A joint surrogate fitness function guides the optimization by combining local diversity, behavioral certainty, and global population diversity. To assess demonstration quality, we apply a set of evaluation metrics, including the reward-based optimality gap, fidelity interquartile means (IQMs), fitness composition analysis, and trajectory visualizations. Hyperparameter sensitivity is also examined to better understand the dynamics of trajectory optimization. Our findings demonstrate that optimizing trajectory selection via surrogate fitness metrics significantly improves interpretability of RL policies in both discrete and continuous environments. In gridworld domains, evaluations reveal significantly enhanced demonstration fidelities compared to random and ablated baselines. In continuous control, the proposed framework offers valuable insights, particularly for early-stage policies, while fidelity-based optimization proves more effective for mature policies. By refining and systematically analyzing surrogate fitness functions, this study advances the interpretability of RL models. The proposed improvements provide deeper insights into RL decision-making, benefiting applications in safety-critical and explainability-focused domains."
  },
  {
    "title": "Transferred plasma catheter for endotherapeutic applications: a parametric study of guided streamers dynamics",
    "url": "http://arxiv.org/abs/2504.14637v1",
    "arxiv_id": "2504.14637v1",
    "authors": [
      "M. Soulier",
      "T. Vacek",
      "K. Geraud",
      "T. Dufour"
    ],
    "published": "2025-04-20T14:29:57+00:00",
    "summary": "Non-thermal atmospheric pressure plasma jets (APPJs) are increasingly used in biomedical applications due to their low temperatures and ability to generate reactive oxygen and nitrogen species (RONS), making them suitable for sensitive environments like medical therapies. The transferred plasma catheter (TPC), a variant of APPJ, shows promise for endoscopic applications but requires precise control of plasma dynamics in confined spaces to ensure safety and efficacy. Despite extensive studies on guided streamers in traditional APPJs, there is limited understanding of streamer behavior in TPC configurations, particularly in challenging scenarios involving grounded metallic surfaces. This study examines the spatiotemporal dynamics of guided streamers generated by TPCs under varying gap distances to establish a robust framework for safe and effective plasma delivery in endoscopic settings. Combining electrical and optical diagnostics, the study characterizes streamer propagation, electric field profiles, and plasma-induced currents in a helium-driven TPC delivering cold plasma to a grounded metal target across gaps of 2 to 18 mm. Results show that streamers maintain charge stability and effectively interact with the target for gap distances below 12 mm, producing significant therapeutic currents. Beyond this threshold, propagation deteriorates due to recombination and reduced electric field intensity. For shorter gaps, counter-propagating waves and secondary streamer interactions are observed, while larger gaps lead to charge dissipation and reduced efficacy. These findings highlight the importance of optimizing gap distances for plasma-assisted endoscopic procedures and demonstrate the TPC's robustness in adverse conditions."
  },
  {
    "title": "a1: Steep Test-time Scaling Law via Environment Augmented Generation",
    "url": "http://arxiv.org/abs/2504.14597v1",
    "arxiv_id": "2504.14597v1",
    "authors": [
      "Lingrui Mei",
      "Shenghua Liu",
      "Yiwei Wang",
      "Baolong Bi",
      "Yuyao Ge",
      "Jun Wan",
      "Yurong Wu",
      "Xueqi Cheng"
    ],
    "published": "2025-04-20T12:55:59+00:00",
    "summary": "Large Language Models (LLMs) have made remarkable breakthroughs in reasoning, yet continue to struggle with hallucinations, logical errors, and inability to self-correct during complex multi-step tasks. Current approaches like chain-of-thought prompting offer limited reasoning capabilities that fail when precise step validation is required. We propose Environment Augmented Generation (EAG), a framework that enhances LLM reasoning through: (1) real-time environmental feedback validating each reasoning step, (2) dynamic branch exploration for investigating alternative solution paths when faced with errors, and (3) experience-based learning from successful reasoning trajectories. Unlike existing methods, EAG enables deliberate backtracking and strategic replanning through tight integration of execution feedback with branching exploration. Our a1-32B model achieves state-of-the-art performance among similar-sized models across all benchmarks, matching larger models like o1 on competition mathematics while outperforming comparable models by up to 24.4 percentage points. Analysis reveals EAG's distinctive scaling pattern: initial token investment in environment interaction yields substantial long-term performance dividends, with advantages amplifying proportionally to task complexity. EAG's theoretical framework demonstrates how environment interactivity and systematic branch exploration together establish a new paradigm for reliable machine reasoning, particularly for problems requiring precise multi-step calculation and logical verification."
  },
  {
    "title": "LLM-Enabled In-Context Learning for Data Collection Scheduling in UAV-assisted Sensor Networks",
    "url": "http://arxiv.org/abs/2504.14556v1",
    "arxiv_id": "2504.14556v1",
    "authors": [
      "Yousef Emami",
      "Hao Gao",
      "SeyedSina Nabavirazani",
      "Luis Almeida"
    ],
    "published": "2025-04-20T10:05:07+00:00",
    "summary": "Unmanned Aerial Vehicles (UAVs) are increasingly being used in various private and commercial applications, e.g. traffic control, package delivery, and Search and Rescue (SAR) operations. Machine Learning (ML) methods used in UAV-assisted Sensor Networks (UASNETs) and especially in Deep Reinforcement Learning (DRL) face challenges such as complex and lengthy model training, gaps between simulation and reality, and low sample efficiency, which conflict with the urgency of emergencies such as SAR operations. This paper proposes In-Context Learning (ICL)-based Data Collection Scheduling (ICLDC) scheme, as an alternative to DRL in emergencies. The UAV collects and transmits logged sensory data, to an LLM, to generate a task description in natural language, from which it obtains a data collection schedule to be executed by the UAV. The system continuously adapts by adding feedback to task descriptions and utilizing feedback for future decisions. This method is tested against jailbreaking attacks, where task description is manipulated to undermine network performance, highlighting the vulnerability of LLMs to such attacks. The proposed ICLDC outperforms the Maximum Channel Gain by reducing cumulative packet loss by approximately 56\\%. ICLDC presents a promising direction for intelligent scheduling and control in UAV-assisted data collection."
  },
  {
    "title": "Should Benevolent Deception be Allowed in EHMI? A Mechanism Explanation Based on Game Theory",
    "url": "http://arxiv.org/abs/2504.14539v1",
    "arxiv_id": "2504.14539v1",
    "authors": [
      "Linkun Liu",
      "Jian Sun",
      "Ye Tian"
    ],
    "published": "2025-04-20T09:03:28+00:00",
    "summary": "The application of external human-machine interface (EHMI) on autonomous vehicles (AVs) facilitates information exchange. Existing research fails to consider the impact of the sequence of actions, as well as the effects of EHMI applications and deception, raising the question of whether benevolent, well-intentioned deception should be permitted (i.e., misleading statements that are intended to benefit both parties). We established a game theory based EHMI information disclosure framework for AVs in this study. In considering benevolent deception, this framework divided the decision-making process into three stages, respectively encompassing three key questions: whether to disclose, when to disclose, and what type of intention information to disclose. The results show that theoretical advantages of deception exist in certain cases when AV expects to maximize the safety of the interaction. In 40 out of 484 cases (8.3%), safety can be enhanced through successful deception. Those successful deceptions fall into two categories: 1) In 28 of these cases, the straight-going AV expected the left-turning HV to yield, while HV exhibited lower speed and higher acceleration; 2) In 12 of these cases, AV expected HV to proceed first, while HV exhibited higher speed and lower acceleration. We also conducted a VR-based driving simulation experiment, and the results confirmed our conclusion. Additionally, we found that when participants had low trust in the EHMI, its use negatively impacted interaction efficiency instead. This study aims to analyze the mechanisms of EHMI information disclosure and contribute to the ongoing discourse on the ethical framework governing autonomous driving systems."
  },
  {
    "title": "Are Vision LLMs Road-Ready? A Comprehensive Benchmark for Safety-Critical Driving Video Understanding",
    "url": "http://arxiv.org/abs/2504.14526v1",
    "arxiv_id": "2504.14526v1",
    "authors": [
      "Tong Zeng",
      "Longfeng Wu",
      "Liang Shi",
      "Dawei Zhou",
      "Feng Guo"
    ],
    "published": "2025-04-20T07:50:44+00:00",
    "summary": "Vision Large Language Models (VLLMs) have demonstrated impressive capabilities in general visual tasks such as image captioning and visual question answering. However, their effectiveness in specialized, safety-critical domains like autonomous driving remains largely unexplored. Autonomous driving systems require sophisticated scene understanding in complex environments, yet existing multimodal benchmarks primarily focus on normal driving conditions, failing to adequately assess VLLMs' performance in safety-critical scenarios. To address this, we introduce DVBench, a pioneering benchmark designed to evaluate the performance of VLLMs in understanding safety-critical driving videos. Built around a hierarchical ability taxonomy that aligns with widely adopted frameworks for describing driving scenarios used in assessing highly automated driving systems, DVBench features 10,000 multiple-choice questions with human-annotated ground-truth answers, enabling a comprehensive evaluation of VLLMs' capabilities in perception and reasoning. Experiments on 14 SOTA VLLMs, ranging from 0.5B to 72B parameters, reveal significant performance gaps, with no model achieving over 40% accuracy, highlighting critical limitations in understanding complex driving scenarios. To probe adaptability, we fine-tuned selected models using domain-specific data from DVBench, achieving accuracy gains ranging from 5.24 to 10.94 percentage points, with relative improvements of up to 43.59%. This improvement underscores the necessity of targeted adaptation to bridge the gap between general-purpose VLLMs and mission-critical driving applications. DVBench establishes an essential evaluation framework and research roadmap for developing VLLMs that meet the safety and robustness requirements for real-world autonomous systems. We released the benchmark toolbox and the fine-tuned model at: https://github.com/tong-zeng/DVBench.git."
  },
  {
    "title": "CoLoTa: A Dataset for Entity-based Commonsense Reasoning over Long-Tail Knowledge",
    "url": "http://arxiv.org/abs/2504.14462v1",
    "arxiv_id": "2504.14462v1",
    "authors": [
      "Armin Toroghi",
      "Willis Guo",
      "Scott Sanner"
    ],
    "published": "2025-04-20T02:47:18+00:00",
    "summary": "The rise of Large Language Models (LLMs) has redefined the AI landscape, particularly due to their ability to encode factual and commonsense knowledge, and their outstanding performance in tasks requiring reasoning. Despite these advances, hallucinations and reasoning errors remain a significant barrier to their deployment in high-stakes settings. In this work, we observe that even the most prominent LLMs, such as OpenAI-o1, suffer from high rates of reasoning errors and hallucinations on tasks requiring commonsense reasoning over obscure, long-tail entities. To investigate this limitation, we present a new dataset for Commonsense reasoning over Long-Tail entities (CoLoTa), that consists of 3,300 queries from question answering and claim verification tasks and covers a diverse range of commonsense reasoning skills. We remark that CoLoTa can also serve as a Knowledge Graph Question Answering (KGQA) dataset since the support of knowledge required to answer its queries is present in the Wikidata knowledge graph. However, as opposed to existing KGQA benchmarks that merely focus on factoid questions, our CoLoTa queries also require commonsense reasoning. Our experiments with strong LLM-based KGQA methodologies indicate their severe inability to answer queries involving commonsense reasoning. Hence, we propose CoLoTa as a novel benchmark for assessing both (i) LLM commonsense reasoning capabilities and their robustness to hallucinations on long-tail entities and (ii) the commonsense reasoning capabilities of KGQA methods."
  },
  {
    "title": "Environmental Monitoring Requirements for the ngVLA",
    "url": "http://arxiv.org/abs/2504.14451v1",
    "arxiv_id": "2504.14451v1",
    "authors": [
      "T. K. Sridharan",
      "J. G. Mangum",
      "B. Butler"
    ],
    "published": "2025-04-20T01:58:41+00:00",
    "summary": "Measurement of environmental parameters is one of the basic requirements for the proper operation of a telescope. This memo is intended to provide guidance for the measurement accuracy requirements in the context of the ngVLA. It relies on previous work for ALMA (Mangum, 2001) and EVLA (Butler \\& Perley, 2008) and a review of the subject by Mangum \\& Wallace (2015). The local operational environment can be broadly divided into two categories: electromagnetic and physical. Meteorological parameters (weather) primarily constitute the physical environmental component and radio frequency interference (RFI) is the essential element of the electromagnetic environment. This memo focuses on the weather component and does not address the RFI, safety and physical infrastructure components. Under weather, the relevant topics are (1) the correction to pointing arising from refraction in the atmosphere (2) the different delays in the arrival times of signals at different antennas due to propagation in the atmosphere (3) monitoring weather parameters to provide operations support, e.g. in determining prevalence of precision or normal conditions, dynamic scheduling and the choice of antennas to constitute a sub-array with a given set of characteristics, among others, and (4) archival. Here we restrict ourselves to the first two topics which impact the data obtained and its calibration."
  },
  {
    "title": "Seeing Through Risk: A Symbolic Approximation of Prospect Theory",
    "url": "http://arxiv.org/abs/2504.14448v1",
    "arxiv_id": "2504.14448v1",
    "authors": [
      "Ali Arslan Yousaf",
      "Umair Rehman",
      "Muhammad Umair Danish"
    ],
    "published": "2025-04-20T01:44:54+00:00",
    "summary": "We propose a novel symbolic modeling framework for decision-making under risk that merges interpretability with the core insights of Prospect Theory. Our approach replaces opaque utility curves and probability weighting functions with transparent, effect-size-guided features. We mathematically formalize the method, demonstrate its ability to replicate well-known framing and loss-aversion phenomena, and provide an end-to-end empirical validation on synthetic datasets. The resulting model achieves competitive predictive performance while yielding clear coefficients mapped onto psychological constructs, making it suitable for applications ranging from AI safety to economic policy analysis."
  },
  {
    "title": "RedMulE-FT: A Reconfigurable Fault-Tolerant Matrix Multiplication Engine",
    "url": "http://arxiv.org/abs/2504.14399v1",
    "arxiv_id": "2504.14399v1",
    "authors": [
      "Philip Wiese",
      "Maurus Item",
      "Luca Bertaccini",
      "Yvan Tortorella",
      "Angelo Garofalo",
      "Luca Benini"
    ],
    "published": "2025-04-19T20:25:33+00:00",
    "summary": "As safety-critical applications increasingly rely on data-parallel floating-point computations, there is an increasing need for flexible and configurable fault tolerance in parallel floating-point accelerators such as tensor engines. While replication-based methods ensure reliability but incur high area and power costs, error correction codes lack the flexibility to trade off robustness against performance. This work presents RedMulE-FT, a runtime-configurable fault-tolerant extension of the RedMulE matrix multiplication accelerator, balancing fault tolerance, area overhead, and performance impacts. The fault tolerance mode is configured in a shadowed context register file before task execution. By combining replication with error-detecting codes to protect the data path, RedMulE-FT achieves an 11x uncorrected fault reduction with only 2.3% area overhead. Full protection extends to control signals, resulting in no functional errors after 1M injections during our extensive fault injection simulation campaign, with a total area overhead of 25.2% while maintaining a 500 MHz frequency in a 12 nm technology."
  },
  {
    "title": "The Geometry of Self-Verification in a Task-Specific Reasoning Model",
    "url": "http://arxiv.org/abs/2504.14379v1",
    "arxiv_id": "2504.14379v1",
    "authors": [
      "Andrew Lee",
      "Lihao Sun",
      "Chris Wendler",
      "Fernanda Vi\u00e9gas",
      "Martin Wattenberg"
    ],
    "published": "2025-04-19T18:40:51+00:00",
    "summary": "How do reasoning models verify their own answers? We study this question by training a model using DeepSeek R1's recipe on the CountDown task. We leverage the fact that preference tuning leads to mode collapse, resulting in a model that always produces highly structured and easily parse-able chain-of-thought sequences. With this setup, we do a top-down and bottom-up analysis to reverse-engineer how the model verifies its outputs. Our top-down analysis reveals Gated Linear Unit (GLU) weights encoding verification-related tokens, such as ``success'' or ``incorrect'', which activate according to the correctness of the model's reasoning steps. Our bottom-up analysis reveals that ``previous-token heads'' are mainly responsible for model verification. Our analyses meet in the middle: drawing inspiration from inter-layer communication channels, we use the identified GLU vectors to localize as few as three attention heads that can disable model verification, pointing to a necessary component of a potentially larger verification circuit."
  },
  {
    "title": "Manipulating Multimodal Agents via Cross-Modal Prompt Injection",
    "url": "http://arxiv.org/abs/2504.14348v1",
    "arxiv_id": "2504.14348v1",
    "authors": [
      "Le Wang",
      "Zonghao Ying",
      "Tianyuan Zhang",
      "Siyuan Liang",
      "Shengshan Hu",
      "Mingchuan Zhang",
      "Aishan Liu",
      "Xianglong Liu"
    ],
    "published": "2025-04-19T16:28:03+00:00",
    "summary": "The emergence of multimodal large language models has redefined the agent paradigm by integrating language and vision modalities with external data sources, enabling agents to better interpret human instructions and execute increasingly complex tasks. However, in this work, we identify a critical yet previously overlooked security vulnerability in multimodal agents: cross-modal prompt injection attacks. To exploit this vulnerability, we propose CrossInject, a novel attack framework in which attackers embed adversarial perturbations across multiple modalities to align with target malicious content, allowing external instructions to hijack the agent's decision-making process and execute unauthorized tasks. Our approach consists of two key components. First, we introduce Visual Latent Alignment, where we optimize adversarial features to the malicious instructions in the visual embedding space based on a text-to-image generative model, ensuring that adversarial images subtly encode cues for malicious task execution. Subsequently, we present Textual Guidance Enhancement, where a large language model is leveraged to infer the black-box defensive system prompt through adversarial meta prompting and generate an malicious textual command that steers the agent's output toward better compliance with attackers' requests. Extensive experiments demonstrate that our method outperforms existing injection attacks, achieving at least a +26.4% increase in attack success rates across diverse tasks. Furthermore, we validate our attack's effectiveness in real-world multimodal autonomous agents, highlighting its potential implications for safety-critical applications."
  },
  {
    "title": "Manipulating Multimodal Agents via Cross-Modal Prompt Injection",
    "url": "http://arxiv.org/abs/2504.14348v2",
    "arxiv_id": "2504.14348v2",
    "authors": [
      "Le Wang",
      "Zonghao Ying",
      "Tianyuan Zhang",
      "Siyuan Liang",
      "Shengshan Hu",
      "Mingchuan Zhang",
      "Aishan Liu",
      "Xianglong Liu"
    ],
    "published": "2025-04-19T16:28:03+00:00",
    "summary": "The emergence of multimodal large language models has redefined the agent paradigm by integrating language and vision modalities with external data sources, enabling agents to better interpret human instructions and execute increasingly complex tasks. However, in this work, we identify a critical yet previously overlooked security vulnerability in multimodal agents: cross-modal prompt injection attacks. To exploit this vulnerability, we propose CrossInject, a novel attack framework in which attackers embed adversarial perturbations across multiple modalities to align with target malicious content, allowing external instructions to hijack the agent's decision-making process and execute unauthorized tasks. Our approach consists of two key components. First, we introduce Visual Latent Alignment, where we optimize adversarial features to the malicious instructions in the visual embedding space based on a text-to-image generative model, ensuring that adversarial images subtly encode cues for malicious task execution. Subsequently, we present Textual Guidance Enhancement, where a large language model is leveraged to infer the black-box defensive system prompt through adversarial meta prompting and generate an malicious textual command that steers the agent's output toward better compliance with attackers' requests. Extensive experiments demonstrate that our method outperforms existing injection attacks, achieving at least a +26.4% increase in attack success rates across diverse tasks. Furthermore, we validate our attack's effectiveness in real-world multimodal autonomous agents, highlighting its potential implications for safety-critical applications."
  },
  {
    "title": "DLW-CI: A Dynamic Likelihood-Weighted Cooperative Infotaxis Approach for Multi-Source Search in Urban Environments Using Consumer Drone Networks",
    "url": "http://arxiv.org/abs/2504.14330v1",
    "arxiv_id": "2504.14330v1",
    "authors": [
      "Xiaoran Zhang",
      "Yatai Ji",
      "Yong Zhao",
      "Chuan Ai",
      "Bin Chen",
      "Zhengqiu Zhu"
    ],
    "published": "2025-04-19T15:44:09+00:00",
    "summary": "Consumer-grade drones equipped with low-cost sensors have emerged as a cornerstone of Autonomous Intelligent Systems (AISs) for environmental monitoring and hazardous substance detection in urban environments. However, existing research primarily addresses single-source search problems, overlooking the complexities of real-world urban scenarios where both the location and quantity of hazardous sources remain unknown. To address this issue, we propose the Dynamic Likelihood-Weighted Cooperative Infotaxis (DLW-CI) approach for consumer drone networks. Our approach enhances multi-drone collaboration in AISs by combining infotaxis (a cognitive search strategy) with optimized source term estimation and an innovative cooperative mechanism. Specifically, we introduce a novel source term estimation method that utilizes multiple parallel particle filters, with each filter dedicated to estimating the parameters of a potentially unknown source within the search scene. Furthermore, we develop a cooperative mechanism based on dynamic likelihood weights to prevent multiple drones from simultaneously estimating and searching for the same source, thus optimizing the energy efficiency and search coverage of the consumer AIS. Experimental results demonstrate that the DLW-CI approach significantly outperforms baseline methods regarding success rate, accuracy, and root mean square error, particularly in scenarios with relatively few sources, regardless of the presence of obstacles. Also, the effectiveness of the proposed approach is verified in a diffusion scenario generated by the computational fluid dynamics (CFD) model. Research findings indicate that our approach could improve source estimation accuracy and search efficiency by consumer drone-based AISs, making a valuable contribution to environmental safety monitoring applications within smart city infrastructure."
  },
  {
    "title": "Towards NSFW-Free Text-to-Image Generation via Safety-Constraint Direct Preference Optimization",
    "url": "http://arxiv.org/abs/2504.14290v1",
    "arxiv_id": "2504.14290v1",
    "authors": [
      "Shouwei Ruan",
      "Zhenyu Wu",
      "Yao Huang",
      "Ruochen Zhang",
      "Yitong Sun",
      "Caixin Kang",
      "Xingxing Wei"
    ],
    "published": "2025-04-19T13:26:46+00:00",
    "summary": "Ensuring the safety of generated content remains a fundamental challenge for Text-to-Image (T2I) generation. Existing studies either fail to guarantee complete safety under potentially harmful concepts or struggle to balance safety with generation quality. To address these issues, we propose Safety-Constrained Direct Preference Optimization (SC-DPO), a novel framework for safety alignment in T2I models. SC-DPO integrates safety constraints into the general human preference calibration, aiming to maximize the likelihood of generating human-preferred samples while minimizing the safety cost of the generated outputs. In SC-DPO, we introduce a safety cost model to accurately quantify harmful levels for images, and train it effectively using the proposed contrastive learning and cost anchoring objectives. To apply SC-DPO for effective T2I safety alignment, we constructed SCP-10K, a safety-constrained preference dataset containing rich harmful concepts, which blends safety-constrained preference pairs under both harmful and clean instructions, further mitigating the trade-off between safety and sample quality. Additionally, we propose a Dynamic Focusing Mechanism (DFM) for SC-DPO, promoting the model's learning of difficult preference pair samples. Extensive experiments demonstrate that SC-DPO outperforms existing methods, effectively defending against various NSFW content while maintaining optimal sample quality and human preference alignment. Additionally, SC-DPO exhibits resilience against adversarial prompts designed to generate harmful content."
  },
  {
    "title": "SRPO: A Cross-Domain Implementation of Large-Scale Reinforcement Learning on LLM",
    "url": "http://arxiv.org/abs/2504.14286v1",
    "arxiv_id": "2504.14286v1",
    "authors": [
      "Xiaojiang Zhang",
      "Jinghui Wang",
      "Zifei Cheng",
      "Wenhao Zhuang",
      "Zheng Lin",
      "Minglei Zhang",
      "Shaojie Wang",
      "Yinghan Cui",
      "Chao Wang",
      "Junyi Peng",
      "Shimiao Jiang",
      "Shiqi Kuang",
      "Shouyu Yin",
      "Chaohang Wen",
      "Haotian Zhang",
      "Bin Chen",
      "Bing Yu"
    ],
    "published": "2025-04-19T13:06:03+00:00",
    "summary": "Recent advances of reasoning models, exemplified by OpenAI's o1 and DeepSeek's R1, highlight the significant potential of Reinforcement Learning (RL) to enhance the reasoning capabilities of Large Language Models (LLMs). However, replicating these advancements across diverse domains remains challenging due to limited methodological transparency. In this work, we present two-Staged history-Resampling Policy Optimization (SRPO), which successfully surpasses the performance of DeepSeek-R1-Zero-32B on the AIME24 and LiveCodeBench benchmarks. SRPO achieves this using the same base model as DeepSeek (i.e. Qwen2.5-32B) and relies solely on RL, without prior Supervised Fine-Tuning (SFT). Building upon Group Relative Policy Optimization (GRPO), we introduce two key methodological innovations: (1) a two-stage cross-domain training paradigm designed to balance the development of mathematical reasoning and coding proficiency, and (2) History Resampling (HR), a technique to address ineffective samples. Our comprehensive experiments validate the effectiveness of our approach, dedicating to offer valuable insights into scaling LLM reasoning capabilities across diverse tasks."
  },
  {
    "title": "SRPO: A Cross-Domain Implementation of Large-Scale Reinforcement Learning on LLM",
    "url": "http://arxiv.org/abs/2504.14286v2",
    "arxiv_id": "2504.14286v2",
    "authors": [
      "Xiaojiang Zhang",
      "Jinghui Wang",
      "Zifei Cheng",
      "Wenhao Zhuang",
      "Zheng Lin",
      "Minglei Zhang",
      "Shaojie Wang",
      "Yinghan Cui",
      "Chao Wang",
      "Junyi Peng",
      "Shimiao Jiang",
      "Shiqi Kuang",
      "Shouyu Yin",
      "Chaohang Wen",
      "Haotian Zhang",
      "Bin Chen",
      "Bing Yu"
    ],
    "published": "2025-04-19T13:06:03+00:00",
    "summary": "Recent advances of reasoning models, exemplified by OpenAI's o1 and DeepSeek's R1, highlight the significant potential of Reinforcement Learning (RL) to enhance the reasoning capabilities of Large Language Models (LLMs). However, replicating these advancements across diverse domains remains challenging due to limited methodological transparency. In this work, we present two-Staged history-Resampling Policy Optimization (SRPO), which surpasses the performance of DeepSeek-R1-Zero-32B on the AIME24 and LiveCodeBench benchmarks. SRPO achieves this using the same base model as DeepSeek (i.e. Qwen2.5-32B), using only about 1/10 of the training steps required by DeepSeek-R1-Zero-32B, demonstrating superior efficiency. Building upon Group Relative Policy Optimization (GRPO), we introduce two key methodological innovations: (1) a two-stage cross-domain training paradigm designed to balance the development of mathematical reasoning and coding proficiency, and (2) History Resampling (HR), a technique to address ineffective samples. Our comprehensive experiments validate the effectiveness of our approach, offering valuable insights into scaling LLM reasoning capabilities across diverse tasks."
  },
  {
    "title": "Microscopic features of the effect of vehicle overacceleration on traffic flow",
    "url": "http://arxiv.org/abs/2504.14244v1",
    "arxiv_id": "2504.14244v1",
    "authors": [
      "Boris S. Kerner",
      "Sergey L. Klenov"
    ],
    "published": "2025-04-19T09:37:17+00:00",
    "summary": "Through the development of a microscopic deterministic model in the framework of three-phase traffic theory, microscopic features of vehicle overacceleration, which determines the occurrence of the metastability of free traffic flow at a bottleneck, have been revealed: (i) The greater the impact of vehicle overacceleration on free traffic flow at a bottleneck, the higher the maximum flow rate at which free flow can persist at the bottleneck, i.e., the better traffic breakdown can be avoided. (ii) There can be at least two mechanisms of overacceleration in road lane caused by safety acceleration at the bottleneck. (iii) Through a microscopic analysis of spatiotemporal competition between speed adaptation and vehicle acceleration behaviors, traffic conditions have been found at which safety acceleration in road lane or/and vehicle acceleration due to lane-changing on multi-lane road become overacceleration. (iv) There is spatiotemporal cooperation of different overacceleration mechanisms. (v) The stronger the overacceleration cooperation, the stronger the maintenance of free flow at the bottleneck due to overacceleration. (vi) On two-lane road, both speed adaptation and overacceleration in road lane can effect qualitatively on the overacceleration mechanism caused by lane-changing. These microscopic features of the effect of vehicle overacceleration on traffic flow are related to traffic flow consisting of human-driving or/and automated-driving vehicles."
  },
  {
    "title": "Know Me, Respond to Me: Benchmarking LLMs for Dynamic User Profiling and Personalized Responses at Scale",
    "url": "http://arxiv.org/abs/2504.14225v1",
    "arxiv_id": "2504.14225v1",
    "authors": [
      "Bowen Jiang",
      "Zhuoqun Hao",
      "Young-Min Cho",
      "Bryan Li",
      "Yuan Yuan",
      "Sihao Chen",
      "Lyle Ungar",
      "Camillo J. Taylor",
      "Dan Roth"
    ],
    "published": "2025-04-19T08:16:10+00:00",
    "summary": "Large Language Models (LLMs) have emerged as personalized assistants for users across a wide range of tasks -- from offering writing support to delivering tailored recommendations or consultations. Over time, the interaction history between a user and an LLM can provide extensive information about an individual's traits and preferences. However, open questions remain on how well LLMs today can effectively leverage such history to (1) internalize the user's inherent traits and preferences, (2) track how the user profiling and preferences evolve over time, and (3) generate personalized responses accordingly in new scenarios.   In this work, we introduce the PERSONAMEM benchmark. PERSONAMEM features curated user profiles with over 180 simulated user-LLM interaction histories, each containing up to 60 sessions of multi-turn conversations across 15 real-world tasks that require personalization. Given an in-situ user query, i.e. query issued by the user from the first-person perspective, we evaluate LLM chatbots' ability to identify the most suitable response according to the current state of the user's profile. We observe that current LLMs still struggle to recognize the dynamic evolution in users' profiles over time through direct prompting approaches. As a consequence, LLMs often fail to deliver responses that align with users' current situations and preferences, with frontier models such as GPT-4.1, o4-mini, GPT-4.5, o1, or Gemini-2.0 achieving only around 50% overall accuracy, suggesting room for improvement. We hope that PERSONAMEM, along with the user profile and conversation simulation pipeline, can facilitate future research in the development of truly user-aware chatbots. Code and data are available at github.com/bowen-upenn/PersonaMem."
  },
  {
    "title": "Amplify Initiative: Building A Localized Data Platform for Globalized AI",
    "url": "http://arxiv.org/abs/2504.14105v1",
    "arxiv_id": "2504.14105v1",
    "authors": [
      "Qazi Mamunur Rashid",
      "Erin van Liemt",
      "Tiffany Shih",
      "Amber Ebinama",
      "Karla Barrios Ramos",
      "Madhurima Maji",
      "Aishwarya Verma",
      "Charu Kalia",
      "Jamila Smith-Loud",
      "Joyce Nakatumba-Nabende",
      "Rehema Baguma",
      "Andrew Katumba",
      "Chodrine Mutebi",
      "Jagen Marvin",
      "Eric Peter Wairagala",
      "Mugizi Bruce",
      "Peter Oketta",
      "Lawrence Nderu",
      "Obichi Obiajunwa",
      "Abigail Oppong",
      "Michael Zimba",
      "Data Authors"
    ],
    "published": "2025-04-18T23:20:52+00:00",
    "summary": "Current AI models often fail to account for local context and language, given the predominance of English and Western internet content in their training data. This hinders the global relevance, usefulness, and safety of these models as they gain more users around the globe. Amplify Initiative, a data platform and methodology, leverages expert communities to collect diverse, high-quality data to address the limitations of these models. The platform is designed to enable co-creation of datasets, provide access to high-quality multilingual datasets, and offer recognition to data authors. This paper presents the approach to co-creating datasets with domain experts (e.g., health workers, teachers) through a pilot conducted in Sub-Saharan Africa (Ghana, Kenya, Malawi, Nigeria, and Uganda). In partnership with local researchers situated in these countries, the pilot demonstrated an end-to-end approach to co-creating data with 155 experts in sensitive domains (e.g., physicians, bankers, anthropologists, human and civil rights advocates). This approach, implemented with an Android app, resulted in an annotated dataset of 8,091 adversarial queries in seven languages (e.g., Luganda, Swahili, Chichewa), capturing nuanced and contextual information related to key themes such as misinformation and public interest topics. This dataset in turn can be used to evaluate models for their safety and cultural relevance within the context of these languages."
  },
  {
    "title": "LogicTree: Structured Proof Exploration for Coherent and Rigorous Logical Reasoning with Large Language Models",
    "url": "http://arxiv.org/abs/2504.14089v1",
    "arxiv_id": "2504.14089v1",
    "authors": [
      "Kang He",
      "Kaushik Roy"
    ],
    "published": "2025-04-18T22:10:02+00:00",
    "summary": "Large language models (LLMs) have achieved remarkable multi-step reasoning capabilities across various domains. However, LLMs still face distinct challenges in complex logical reasoning, as (1) proof-finding requires systematic exploration and the maintenance of logical coherence and (2) searching the right combination of premises at each reasoning step is inherently challenging in tasks with large premise space. To address this, we propose LogicTree, an inference-time modular framework employing algorithm-guided search to automate structured proof exploration and ensure logical coherence. Advancing beyond tree-of-thought (ToT), we incorporate caching mechanism into LogicTree to enable effective utilization of historical knowledge, preventing reasoning stagnation and minimizing redundancy. Furthermore, we address the combinatorial complexity of premise search by decomposing it into a linear process. The refined premise selection restricts subsequent inference to at most one derivation per step, enhancing reasoning granularity and enforcing strict step-by-step reasoning. Additionally, we introduce two LLM-free heuristics for premise prioritization, enabling strategic proof search. Experimental results on five datasets demonstrate that LogicTree optimally scales inference-time computation to achieve higher proof accuracy, surpassing chain-of-thought (CoT) and ToT with average gains of 23.6% and 12.5%, respectively, on GPT-4o. Moreover, within LogicTree, GPT-4o outperforms o3-mini by 7.6% on average."
  },
  {
    "title": "Infrared Vision Systems for Emergency Vehicle Driver Assistance in Low-Visibility Conditions",
    "url": "http://arxiv.org/abs/2504.14078v1",
    "arxiv_id": "2504.14078v1",
    "authors": [
      "M-Mahdi Naddaf-Sh",
      "Andrew Lee",
      "Kin Yen",
      "Eemon Amini",
      "Iman Soltani"
    ],
    "published": "2025-04-18T21:06:41+00:00",
    "summary": "This study investigates the potential of infrared (IR) camera technology to enhance driver safety for emergency vehicles operating in low-visibility conditions, particularly at night and in dense fog. Such environments significantly increase the risk of collisions, especially for tow trucks and snowplows that must remain operational in challenging conditions. Conventional driver assistance systems often struggle under these conditions due to limited visibility. In contrast, IR cameras, which detect the thermal signatures of obstacles, offer a promising alternative. The evaluation combines controlled laboratory experiments, real-world field tests, and surveys of emergency vehicle operators. In addition to assessing detection performance, the study examines the feasibility of retrofitting existing Department of Transportation (DoT) fleets with cost-effective IR-based driver assistance systems. Results underscore the utility of IR technology in enhancing driver awareness and provide data-driven recommendations for scalable deployment across legacy emergency vehicle fleets."
  },
  {
    "title": "Think Deep, Think Fast: Investigating Efficiency of Verifier-free Inference-time-scaling Methods",
    "url": "http://arxiv.org/abs/2504.14047v1",
    "arxiv_id": "2504.14047v1",
    "authors": [
      "Junlin Wang",
      "Shang Zhu",
      "Jon Saad-Falcon",
      "Ben Athiwaratkun",
      "Qingyang Wu",
      "Jue Wang",
      "Shuaiwen Leon Song",
      "Ce Zhang",
      "Bhuwan Dhingra",
      "James Zou"
    ],
    "published": "2025-04-18T19:32:55+00:00",
    "summary": "There is intense interest in investigating how inference time compute (ITC) (e.g. repeated sampling, refinements, etc) can improve large language model (LLM) capabilities. At the same time, recent breakthroughs in reasoning models, such as Deepseek-R1, unlock the opportunity for reinforcement learning to improve LLM reasoning skills. An in-depth understanding of how ITC interacts with reasoning across different models could provide important guidance on how to further advance the LLM frontier. This work conducts a comprehensive analysis of inference-time scaling methods for both reasoning and non-reasoning models on challenging reasoning tasks. Specifically, we focus our research on verifier-free inference time-scaling methods due to its generalizability without needing a reward model. We construct the Pareto frontier of quality and efficiency. We find that non-reasoning models, even with an extremely high inference budget, still fall substantially behind reasoning models. For reasoning models, majority voting proves to be a robust inference strategy, generally competitive or outperforming other more sophisticated ITC methods like best-of-N and sequential revisions, while the additional inference compute offers minimal improvements. We further perform in-depth analyses of the association of key response features (length and linguistic markers) with response quality, with which we can improve the existing ITC methods. We find that correct responses from reasoning models are typically shorter and have fewer hedging and thinking markers (but more discourse markers) than the incorrect responses."
  },
  {
    "title": "Multi-Stage Retrieval for Operational Technology Cybersecurity Compliance Using Large Language Models: A Railway Casestudy",
    "url": "http://arxiv.org/abs/2504.14044v1",
    "arxiv_id": "2504.14044v1",
    "authors": [
      "Regan Bolton",
      "Mohammadreza Sheikhfathollahi",
      "Simon Parkinson",
      "Dan Basher",
      "Howard Parkinson"
    ],
    "published": "2025-04-18T19:24:17+00:00",
    "summary": "Operational Technology Cybersecurity (OTCS) continues to be a dominant challenge for critical infrastructure such as railways. As these systems become increasingly vulnerable to malicious attacks due to digitalization, effective documentation and compliance processes are essential to protect these safety-critical systems. This paper proposes a novel system that leverages Large Language Models (LLMs) and multi-stage retrieval to enhance the compliance verification process against standards like IEC 62443 and the rail-specific IEC 63452. We first evaluate a Baseline Compliance Architecture (BCA) for answering OTCS compliance queries, then develop an extended approach called Parallel Compliance Architecture (PCA) that incorporates additional context from regulatory standards. Through empirical evaluation comparing OpenAI-gpt-4o and Claude-3.5-haiku models in these architectures, we demonstrate that the PCA significantly improves both correctness and reasoning quality in compliance verification. Our research establishes metrics for response correctness, logical reasoning, and hallucination detection, highlighting the strengths and limitations of using LLMs for compliance verification in railway cybersecurity. The results suggest that retrieval-augmented approaches can significantly improve the efficiency and accuracy of compliance assessments, particularly valuable in an industry facing a shortage of cybersecurity expertise."
  },
  {
    "title": "Statistical Analysis and End-to-End Performance Evaluation of Traffic Models for Automotive Data",
    "url": "http://arxiv.org/abs/2504.14017v1",
    "arxiv_id": "2504.14017v1",
    "authors": [
      "Marcello Bullo",
      "Amir Ashtari Gargari",
      "Paolo Testolina",
      "Michele Zorzi",
      "Marco Giordani"
    ],
    "published": "2025-04-18T18:12:08+00:00",
    "summary": "Autonomous driving is a major paradigm shift in transportation, with the potential to enhance safety, optimize traffic congestion, and reduce fuel consumption. Although autonomous vehicles rely on advanced sensors and on-board computing systems to navigate without human control, full awareness of the driving environment also requires a cooperative effort via Vehicle-To-Everything (V2X) communication. Specifically, vehicles send and receive sensor perceptions to/from other vehicles to extend perception beyond their own sensing range. However, transmitting large volumes of data can be challenging for current V2X communication technologies, so data compression represents a crucial solution to reduce the message size and link congestion. In this paper, we present a statistical characterization of automotive data, focusing on LiDAR sensors. Notably, we provide models for the size of both raw and compressed point clouds. The use of statistical traffic models offers several advantages compared to using real data, such as faster simulations, reduced storage requirements, and greater flexibility in the application design. Furthermore, statistical models can be used for understanding traffic patterns and analyzing statistics, which is crucial to design and optimize wireless networks. We validate our statistical models via a Kolmogorov-Smirnoff test implementing a Bootstrap Resampling scheme. Moreover, we show via ns-3 simulations that using statistical models yields comparable results in terms of latency and throughput compared to real data, which also demonstrates the accuracy of the models."
  },
  {
    "title": "DiffOG: Differentiable Policy Trajectory Optimization with Generalizability",
    "url": "http://arxiv.org/abs/2504.13807v1",
    "arxiv_id": "2504.13807v1",
    "authors": [
      "Zhengtong Xu",
      "Zichen Miao",
      "Qiang Qiu",
      "Zhe Zhang",
      "Yu She"
    ],
    "published": "2025-04-18T17:20:27+00:00",
    "summary": "Imitation learning-based visuomotor policies excel at manipulation tasks but often produce suboptimal action trajectories compared to model-based methods. Directly mapping camera data to actions via neural networks can result in jerky motions and difficulties in meeting critical constraints, compromising safety and robustness in real-world deployment. For tasks that require high robustness or strict adherence to constraints, ensuring trajectory quality is crucial. However, the lack of interpretability in neural networks makes it challenging to generate constraint-compliant actions in a controlled manner. This paper introduces differentiable policy trajectory optimization with generalizability (DiffOG), a learning-based trajectory optimization framework designed to enhance visuomotor policies. By leveraging the proposed differentiable formulation of trajectory optimization with transformer, DiffOG seamlessly integrates policies with a generalizable optimization layer. Visuomotor policies enhanced by DiffOG generate smoother, constraint-compliant action trajectories in a more interpretable way. DiffOG exhibits strong generalization capabilities and high flexibility. We evaluated DiffOG across 11 simulated tasks and 2 real-world tasks. The results demonstrate that DiffOG significantly enhances the trajectory quality of visuomotor policies while having minimal impact on policy performance, outperforming trajectory processing baselines such as greedy constraint clipping and penalty-based trajectory optimization. Furthermore, DiffOG achieves superior performance compared to existing constrained visuomotor policy."
  },
  {
    "title": "Meta-Learning and Knowledge Discovery based Physics-Informed Neural Network for Remaining Useful Life Prediction",
    "url": "http://arxiv.org/abs/2504.13797v1",
    "arxiv_id": "2504.13797v1",
    "authors": [
      "Yu Wang",
      "Shujie Liu",
      "Shuai Lv",
      "Gengshuo Liu"
    ],
    "published": "2025-04-18T16:58:38+00:00",
    "summary": "Predicting the remaining useful life (RUL) of rotating machinery is critical for industrial safety and maintenance, but existing methods struggle with scarce target-domain data and unclear degradation dynamics. We propose a Meta-Learning and Knowledge Discovery-based Physics-Informed Neural Network (MKDPINN) to address these challenges. The method first maps noisy sensor data to a low-dimensional hidden state space via a Hidden State Mapper (HSM). A Physics-Guided Regulator (PGR) then learns unknown nonlinear PDEs governing degradation evolution, embedding these physical constraints into the PINN framework. This integrates data-driven and physics-based approaches. The framework uses meta-learning, optimizing across source-domain meta-tasks to enable few-shot adaptation to new target tasks. Experiments on industrial data and the C-MAPSS benchmark show MKDPINN outperforms baselines in generalization and accuracy, proving its effectiveness for RUL prediction under data scarcity"
  },
  {
    "title": "Revisiting Uncertainty Quantification Evaluation in Language Models: Spurious Interactions with Response Length Bias Results",
    "url": "http://arxiv.org/abs/2504.13677v1",
    "arxiv_id": "2504.13677v1",
    "authors": [
      "Andrea Santilli",
      "Adam Golinski",
      "Michael Kirchhof",
      "Federico Danieli",
      "Arno Blaas",
      "Miao Xiong",
      "Luca Zappella",
      "Sinead Williamson"
    ],
    "published": "2025-04-18T13:13:42+00:00",
    "summary": "Uncertainty Quantification (UQ) in Language Models (LMs) is crucial for improving their safety and reliability. Evaluations often use performance metrics like AUROC to assess how well UQ methods (e.g., negative sequence probabilities) correlate with task correctness functions (e.g., ROUGE-L). In this paper, we show that commonly used correctness functions bias UQ evaluations by inflating the performance of certain UQ methods. We evaluate 7 correctness functions -- from lexical-based and embedding-based metrics to LLM-as-a-judge approaches -- across 4 datasets x 4 models x 6 UQ methods. Our analysis reveals that length biases in the errors of these correctness functions distort UQ assessments by interacting with length biases in UQ methods. We identify LLM-as-a-judge approaches as among the least length-biased choices and hence a potential solution to mitigate these biases."
  },
  {
    "title": "Enhancing Pothole Detection and Characterization: Integrated Segmentation and Depth Estimation in Road Anomaly Systems",
    "url": "http://arxiv.org/abs/2504.13648v1",
    "arxiv_id": "2504.13648v1",
    "authors": [
      "Uthman Baroudi",
      "Alala BaHamid",
      "Yasser Elalfy",
      "Ziad Al Alami"
    ],
    "published": "2025-04-18T11:59:38+00:00",
    "summary": "Road anomaly detection plays a crucial role in road maintenance and in enhancing the safety of both drivers and vehicles. Recent machine learning approaches for road anomaly detection have overcome the tedious and time-consuming process of manual analysis and anomaly counting; however, they often fall short in providing a complete characterization of road potholes. In this paper, we leverage transfer learning by adopting a pre-trained YOLOv8-seg model for the automatic characterization of potholes using digital images captured from a dashboard-mounted camera. Our work includes the creation of a novel dataset, comprising both images and their corresponding depth maps, collected from diverse road environments in Al-Khobar city and the KFUPM campus in Saudi Arabia. Our approach performs pothole detection and segmentation to precisely localize potholes and calculate their area. Subsequently, the segmented image is merged with its depth map to extract detailed depth information about the potholes. This integration of segmentation and depth data offers a more comprehensive characterization compared to previous deep learning-based road anomaly detection systems. Overall, this method not only has the potential to significantly enhance autonomous vehicle navigation by improving the detection and characterization of road hazards but also assists road maintenance authorities in responding more effectively to road damage."
  },
  {
    "title": "Thought Manipulation: External Thought Can Be Efficient for Large Reasoning Models",
    "url": "http://arxiv.org/abs/2504.13626v1",
    "arxiv_id": "2504.13626v1",
    "authors": [
      "Yule Liu",
      "Jingyi Zheng",
      "Zhen Sun",
      "Zifan Peng",
      "Wenhan Dong",
      "Zeyang Sha",
      "Shiwen Cui",
      "Weiqiang Wang",
      "Xinlei He"
    ],
    "published": "2025-04-18T11:07:19+00:00",
    "summary": "Recent advancements in large reasoning models (LRMs) have demonstrated the effectiveness of scaling test-time computation to enhance reasoning capabilities in multiple tasks. However, LRMs typically suffer from \"overthinking\" problems, where models generate significantly redundant reasoning steps while bringing limited performance gains. Existing work relies on fine-tuning to mitigate overthinking, which requires additional data, unconventional training setups, risky safety misalignment, and poor generalization.   Through empirical analysis, we reveal an important characteristic of LRM behaviors that placing external CoTs generated by smaller models between the thinking token ($\\texttt{<think>}$ and $\\texttt{</think>)}$ can effectively manipulate the model to generate fewer thoughts. Building on these insights, we propose a simple yet efficient pipeline, ThoughtMani, to enable LRMs to bypass unnecessary intermediate steps and reduce computational costs significantly. We conduct extensive experiments to validate the utility and efficiency of ThoughtMani. For instance, when applied to QwQ-32B on the LiveBench/Code dataset, ThoughtMani keeps the original performance and reduces output token counts by approximately 30%, with little overhead from the CoT generator. Furthermore, we find that ThoughtMani enhances safety alignment by an average of 10%. Since model vendors typically serve models of different sizes simultaneously, ThoughtMani provides an effective way to construct more efficient and accessible LRMs for real-world applications."
  },
  {
    "title": "Hysteresis-Aware Neural Network Modeling and Whole-Body Reinforcement Learning Control of Soft Robots",
    "url": "http://arxiv.org/abs/2504.13582v1",
    "arxiv_id": "2504.13582v1",
    "authors": [
      "Zongyuan Chen",
      "Yan Xia",
      "Jiayuan Liu",
      "Jijia Liu",
      "Wenhao Tang",
      "Jiayu Chen",
      "Feng Gao",
      "Longfei Ma",
      "Hongen Liao",
      "Yu Wang",
      "Chao Yu",
      "Boyu Zhang",
      "Fei Xing"
    ],
    "published": "2025-04-18T09:34:56+00:00",
    "summary": "Soft robots exhibit inherent compliance and safety, which makes them particularly suitable for applications requiring direct physical interaction with humans, such as surgical procedures. However, their nonlinear and hysteretic behavior, resulting from the properties of soft materials, presents substantial challenges for accurate modeling and control. In this study, we present a soft robotic system designed for surgical applications and propose a hysteresis-aware whole-body neural network model that accurately captures and predicts the soft robot's whole-body motion, including its hysteretic behavior. Building upon the high-precision dynamic model, we construct a highly parallel simulation environment for soft robot control and apply an on-policy reinforcement learning algorithm to efficiently train whole-body motion control strategies. Based on the trained control policy, we developed a soft robotic system for surgical applications and validated it through phantom-based laser ablation experiments in a physical environment. The results demonstrate that the hysteresis-aware modeling reduces the Mean Squared Error (MSE) by 84.95 percent compared to traditional modeling methods. The deployed control algorithm achieved a trajectory tracking error ranging from 0.126 to 0.250 mm on the real soft robot, highlighting its precision in real-world conditions. The proposed method showed strong performance in phantom-based surgical experiments and demonstrates its potential for complex scenarios, including future real-world clinical applications."
  },
  {
    "title": "DETAM: Defending LLMs Against Jailbreak Attacks via Targeted Attention Modification",
    "url": "http://arxiv.org/abs/2504.13562v1",
    "arxiv_id": "2504.13562v1",
    "authors": [
      "Yu Li",
      "Han Jiang",
      "Zhihua Wei"
    ],
    "published": "2025-04-18T09:02:12+00:00",
    "summary": "With the widespread adoption of Large Language Models (LLMs), jailbreak attacks have become an increasingly pressing safety concern. While safety-aligned LLMs can effectively defend against normal harmful queries, they remain vulnerable to such attacks. Existing defense methods primarily rely on fine-tuning or input modification, which often suffer from limited generalization and reduced utility. To address this, we introduce DETAM, a finetuning-free defense approach that improves the defensive capabilities against jailbreak attacks of LLMs via targeted attention modification. Specifically, we analyze the differences in attention scores between successful and unsuccessful defenses to identify the attention heads sensitive to jailbreak attacks. During inference, we reallocate attention to emphasize the user's core intention, minimizing interference from attack tokens. Our experimental results demonstrate that DETAM outperforms various baselines in jailbreak defense and exhibits robust generalization across different attacks and models, maintaining its effectiveness even on in-the-wild jailbreak data. Furthermore, in evaluating the model's utility, we incorporated over-defense datasets, which further validate the superior performance of our approach. The code will be released immediately upon acceptance."
  },
  {
    "title": "Risk-aware black-box portfolio construction using Bayesian optimization with adaptive weighted Lagrangian estimator",
    "url": "http://arxiv.org/abs/2504.13529v1",
    "arxiv_id": "2504.13529v1",
    "authors": [
      "Zinuo You",
      "John Cartlidge",
      "Karen Elliott",
      "Menghan Ge",
      "Daniel Gold"
    ],
    "published": "2025-04-18T07:40:24+00:00",
    "summary": "Existing portfolio management approaches are often black-box models due to safety and commercial issues in the industry. However, their performance can vary considerably whenever market conditions or internal trading strategies change. Furthermore, evaluating these non-transparent systems is expensive, where certain budgets limit observations of the systems. Therefore, optimizing performance while controlling the potential risk of these financial systems has become a critical challenge. This work presents a novel Bayesian optimization framework to optimize black-box portfolio management models under limited observations. In conventional Bayesian optimization settings, the objective function is to maximize the expectation of performance metrics. However, simply maximizing performance expectations leads to erratic optimization trajectories, which exacerbate risk accumulation in portfolio management. Meanwhile, this can lead to misalignment between the target distribution and the actual distribution of the black-box model. To mitigate this problem, we propose an adaptive weight Lagrangian estimator considering dual objective, which incorporates maximizing model performance and minimizing variance of model observations. Extensive experiments demonstrate the superiority of our approach over five backtest settings with three black-box stock portfolio management models. Ablation studies further verify the effectiveness of the proposed estimator."
  },
  {
    "title": "Monitor and Recover: A Paradigm for Future Research on Distribution Shift in Learning-Enabled Cyber-Physical Systems",
    "url": "http://arxiv.org/abs/2504.13484v1",
    "arxiv_id": "2504.13484v1",
    "authors": [
      "Vivian Lin",
      "Insup Lee"
    ],
    "published": "2025-04-18T05:48:35+00:00",
    "summary": "With the known vulnerability of neural networks to distribution shift, maintaining reliability in learning-enabled cyber-physical systems poses a salient challenge. In response, many existing methods adopt a detect and abstain methodology, aiming to detect distribution shift at inference time so that the learning-enabled component can abstain from decision-making. This approach, however, has limited use in real-world applications. We instead propose a monitor and recover paradigm as a promising direction for future research. This philosophy emphasizes 1) robust safety monitoring instead of distribution shift detection and 2) distribution shift recovery instead of abstention. We discuss two examples from our recent work."
  },
  {
    "title": "Safety Monitoring for Learning-Enabled Cyber-Physical Systems in Out-of-Distribution Scenarios",
    "url": "http://arxiv.org/abs/2504.13478v1",
    "arxiv_id": "2504.13478v1",
    "authors": [
      "Vivian Lin",
      "Ramneet Kaur",
      "Yahan Yang",
      "Souradeep Dutta",
      "Yiannis Kantaros",
      "Anirban Roy",
      "Susmit Jha",
      "Oleg Sokolsky",
      "Insup Lee"
    ],
    "published": "2025-04-18T05:42:37+00:00",
    "summary": "The safety of learning-enabled cyber-physical systems is compromised by the well-known vulnerabilities of deep neural networks to out-of-distribution (OOD) inputs. Existing literature has sought to monitor the safety of such systems by detecting OOD data. However, such approaches have limited utility, as the presence of an OOD input does not necessarily imply the violation of a desired safety property. We instead propose to directly monitor safety in a manner that is itself robust to OOD data. To this end, we predict violations of signal temporal logic safety specifications based on predicted future trajectories. Our safety monitor additionally uses a novel combination of adaptive conformal prediction and incremental learning. The former obtains probabilistic prediction guarantees even on OOD data, and the latter prevents overly conservative predictions. We evaluate the efficacy of the proposed approach in two case studies on safety monitoring: 1) predicting collisions of an F1Tenth car with static obstacles, and 2) predicting collisions of a race car with multiple dynamic obstacles. We find that adaptive conformal prediction obtains theoretical guarantees where other uncertainty quantification methods fail to do so. Additionally, combining adaptive conformal prediction and incremental learning for safety monitoring achieves high recall and timeliness while reducing loss in precision. We achieve these results even in OOD settings and outperform alternative methods."
  },
  {
    "title": "Testing the Fault-Tolerance of Multi-Sensor Fusion Perception in Autonomous Driving Systems",
    "url": "http://arxiv.org/abs/2504.13420v1",
    "arxiv_id": "2504.13420v1",
    "authors": [
      "Haoxiang Tian",
      "Wenqiang Ding",
      "Xingshuo Han",
      "Guoquan Wu",
      "An Guo",
      "Junqi Zhang. Wei Chen",
      "Jun Wei",
      "Tianwei Zhang"
    ],
    "published": "2025-04-18T02:37:55+00:00",
    "summary": "High-level Autonomous Driving Systems (ADSs), such as Google Waymo and Baidu Apollo, typically rely on multi-sensor fusion (MSF) based approaches to perceive their surroundings. This strategy increases perception robustness by combining the respective strengths of the camera and LiDAR and directly affects the safety-critical driving decisions of autonomous vehicles (AVs). However, in real-world autonomous driving scenarios, cameras and LiDAR are subject to various faults, which can probably significantly impact the decision-making and behaviors of ADSs. Existing MSF testing approaches only discovered corner cases that the MSF-based perception cannot accurately detected by MSF-based perception, while lacking research on how sensor faults affect the system-level behaviors of ADSs.   To address this gap, we conduct the first exploration of the fault tolerance of MSF perception-based ADS for sensor faults. In this paper, we systematically and comprehensively build fault models for cameras and LiDAR in AVs and inject them into the MSF perception-based ADS to test its behaviors in test scenarios. To effectively and efficiently explore the parameter spaces of sensor fault models, we design a feedback-guided differential fuzzer to discover the safety violations of MSF perception-based ADS caused by the injected sensor faults. We evaluate FADE on the representative and practical industrial ADS, Baidu Apollo. Our evaluation results demonstrate the effectiveness and efficiency of FADE, and we conclude some useful findings from the experimental results. To validate the findings in the physical world, we use a real Baidu Apollo 6.0 EDU autonomous vehicle to conduct the physical experiments, and the results show the practical significance of our findings."
  },
  {
    "title": "LangCoop: Collaborative Driving with Language",
    "url": "http://arxiv.org/abs/2504.13406v1",
    "arxiv_id": "2504.13406v1",
    "authors": [
      "Xiangbo Gao",
      "Yuheng Wu",
      "Rujia Wang",
      "Chenxi Liu",
      "Yang Zhou",
      "Zhengzhong Tu"
    ],
    "published": "2025-04-18T02:03:14+00:00",
    "summary": "Multi-agent collaboration holds great promise for enhancing the safety, reliability, and mobility of autonomous driving systems by enabling information sharing among multiple connected agents. However, existing multi-agent communication approaches are hindered by limitations of existing communication media, including high bandwidth demands, agent heterogeneity, and information loss. To address these challenges, we introduce LangCoop, a new paradigm for collaborative autonomous driving that leverages natural language as a compact yet expressive medium for inter-agent communication. LangCoop features two key innovations: Mixture Model Modular Chain-of-thought (M$^3$CoT) for structured zero-shot vision-language reasoning and Natural Language Information Packaging (LangPack) for efficiently packaging information into concise, language-based messages. Through extensive experiments conducted in the CARLA simulations, we demonstrate that LangCoop achieves a remarkable 96\\% reduction in communication bandwidth (< 2KB per message) compared to image-based communication, while maintaining competitive driving performance in the closed-loop evaluation."
  },
  {
    "title": "Towards a Multi-Agent Vision-Language System for Zero-Shot Novel Hazardous Object Detection for Autonomous Driving Safety",
    "url": "http://arxiv.org/abs/2504.13399v1",
    "arxiv_id": "2504.13399v1",
    "authors": [
      "Shashank Shriram",
      "Srinivasa Perisetla",
      "Aryan Keskar",
      "Harsha Krishnaswamy",
      "Tonko Emil Westerhof Bossen",
      "Andreas M\u00f8gelmose",
      "Ross Greer"
    ],
    "published": "2025-04-18T01:25:02+00:00",
    "summary": "Detecting anomalous hazards in visual data, particularly in video streams, is a critical challenge in autonomous driving. Existing models often struggle with unpredictable, out-of-label hazards due to their reliance on predefined object categories. In this paper, we propose a multimodal approach that integrates vision-language reasoning with zero-shot object detection to improve hazard identification and explanation. Our pipeline consists of a Vision-Language Model (VLM), a Large Language Model (LLM), in order to detect hazardous objects within a traffic scene. We refine object detection by incorporating OpenAI's CLIP model to match predicted hazards with bounding box annotations, improving localization accuracy. To assess model performance, we create a ground truth dataset by denoising and extending the foundational COOOL (Challenge-of-Out-of-Label) anomaly detection benchmark dataset with complete natural language descriptions for hazard annotations. We define a means of hazard detection and labeling evaluation on the extended dataset using cosine similarity. This evaluation considers the semantic similarity between the predicted hazard description and the annotated ground truth for each video. Additionally, we release a set of tools for structuring and managing large-scale hazard detection datasets. Our findings highlight the strengths and limitations of current vision-language-based approaches, offering insights into future improvements in autonomous hazard detection systems. Our models, scripts, and data can be found at https://github.com/mi3labucm/COOOLER.git"
  },
  {
    "title": "Leveraging Functional Encryption and Deep Learning for Privacy-Preserving Traffic Forecasting",
    "url": "http://arxiv.org/abs/2504.13267v1",
    "arxiv_id": "2504.13267v1",
    "authors": [
      "Isaac Adom",
      "Mohammmad Iqbal Hossain",
      "Hassan Mahmoud",
      "Ahmad Alsharif",
      "Mahmoud Nabil Mahmoud",
      "Yang Xiao"
    ],
    "published": "2025-04-17T18:21:55+00:00",
    "summary": "Over the past few years, traffic congestion has continuously plagued the nation's transportation system creating several negative impacts including longer travel times, increased pollution rates, and higher collision risks. To overcome these challenges, Intelligent Transportation Systems (ITS) aim to improve mobility and vehicular systems, ensuring higher levels of safety by utilizing cutting-edge technologies, sophisticated sensing capabilities, and innovative algorithms. Drivers' participatory sensing, current/future location reporting, and machine learning algorithms have considerably improved real-time congestion monitoring and future traffic management. However, each driver's sensitive spatiotemporal location information can create serious privacy concerns. To address these challenges, we propose in this paper a secure, privacy-preserving location reporting and traffic forecasting system that guarantees privacy protection of driver data while maintaining high traffic forecasting accuracy. Our novel k-anonymity scheme utilizes functional encryption to aggregate encrypted location information submitted by drivers while ensuring the privacy of driver location data. Additionally, using the aggregated encrypted location information as input, this research proposes a deep learning model that incorporates a Convolutional-Long Short-Term Memory (Conv-LSTM) module to capture spatial and short-term temporal features and a Bidirectional Long Short-Term Memory (Bi-LSTM) module to recover long-term periodic patterns for traffic forecasting. With extensive evaluation on real datasets, we demonstrate the effectiveness of the proposed scheme with less than 10% mean absolute error for a 60-minute forecasting horizon, all while protecting driver privacy."
  },
  {
    "title": "Generate, but Verify: Reducing Hallucination in Vision-Language Models with Retrospective Resampling",
    "url": "http://arxiv.org/abs/2504.13169v1",
    "arxiv_id": "2504.13169v1",
    "authors": [
      "Tsung-Han Wu",
      "Heekyung Lee",
      "Jiaxin Ge",
      "Joseph E. Gonzalez",
      "Trevor Darrell",
      "David M. Chan"
    ],
    "published": "2025-04-17T17:59:22+00:00",
    "summary": "Vision-Language Models (VLMs) excel at visual understanding but often suffer from visual hallucinations, where they generate descriptions of nonexistent objects, actions, or concepts, posing significant risks in safety-critical applications. Existing hallucination mitigation methods typically follow one of two paradigms: generation adjustment, which modifies decoding behavior to align text with visual inputs, and post-hoc verification, where external models assess and correct outputs. While effective, generation adjustment methods often rely on heuristics and lack correction mechanisms, while post-hoc verification is complicated, typically requiring multiple models and tending to reject outputs rather than refine them. In this work, we introduce REVERSE, a unified framework that integrates hallucination-aware training with on-the-fly self-verification. By leveraging a new hallucination-verification dataset containing over 1.3M semi-synthetic samples, along with a novel inference-time retrospective resampling technique, our approach enables VLMs to both detect hallucinations during generation and dynamically revise those hallucinations. Our evaluations show that REVERSE achieves state-of-the-art hallucination reduction, outperforming the best existing methods by up to 12% on CHAIR-MSCOCO and 28% on HaloQuest. Our dataset, model, and code are available at: https://reverse-vlm.github.io."
  },
  {
    "title": "Energy-Based Reward Models for Robust Language Model Alignment",
    "url": "http://arxiv.org/abs/2504.13134v1",
    "arxiv_id": "2504.13134v1",
    "authors": [
      "Anamika Lochab",
      "Ruqi Zhang"
    ],
    "published": "2025-04-17T17:47:15+00:00",
    "summary": "Reward models (RMs) are essential for aligning Large Language Models (LLMs) with human preferences. However, they often struggle with capturing complex human preferences and generalizing to unseen data. To address these challenges, we introduce Energy-Based Reward Model (EBRM), a lightweight post-hoc refinement framework that enhances RM robustness and generalization. EBRM models the reward distribution explicitly, capturing uncertainty in human preferences and mitigating the impact of noisy or misaligned annotations. It achieves this through conflict-aware data filtering, label-noise-aware contrastive training, and hybrid initialization. Notably, EBRM enhances RMs without retraining, making it computationally efficient and adaptable across different models and tasks. Empirical evaluations on RM benchmarks demonstrate significant improvements in both robustness and generalization, achieving up to a 5.97% improvement in safety-critical alignment tasks compared to standard RMs. Furthermore, reinforcement learning experiments confirm that our refined rewards enhance alignment quality, effectively delaying reward hacking. These results demonstrate our approach as a scalable and effective enhancement for existing RMs and alignment pipelines. The code is available at EBRM."
  },
  {
    "title": "LLMs Meet Finance: Fine-Tuning Foundation Models for the Open FinLLM Leaderboard",
    "url": "http://arxiv.org/abs/2504.13125v1",
    "arxiv_id": "2504.13125v1",
    "authors": [
      "Varun Rao",
      "Youran Sun",
      "Mahendra Kumar",
      "Tejas Mutneja",
      "Agastya Mukherjee",
      "Haizhao Yang"
    ],
    "published": "2025-04-17T17:42:02+00:00",
    "summary": "This paper investigates the application of large language models (LLMs) to financial tasks. We fine-tuned foundation models using the Open FinLLM Leaderboard as a benchmark. Building on Qwen2.5 and Deepseek-R1, we employed techniques including supervised fine-tuning (SFT), direct preference optimization (DPO), and reinforcement learning (RL) to enhance their financial capabilities. The fine-tuned models demonstrated substantial performance gains across a wide range of financial tasks. Moreover, we measured the data scaling law in the financial domain. Our work demonstrates the potential of large language models (LLMs) in financial applications."
  },
  {
    "title": "Accuracy is Not Agreement: Expert-Aligned Evaluation of Crash Narrative Classification Models",
    "url": "http://arxiv.org/abs/2504.13068v1",
    "arxiv_id": "2504.13068v1",
    "authors": [
      "Sudesh Ramesh Bhagat",
      "Ibne Farabi Shihab",
      "Anuj Sharma"
    ],
    "published": "2025-04-17T16:29:08+00:00",
    "summary": "This study explores the relationship between deep learning (DL) model accuracy and expert agreement in the classification of crash narratives. We evaluate five DL models -- including BERT variants, the Universal Sentence Encoder (USE), and a zero-shot classifier -- against expert-labeled data and narrative text. The analysis is further extended to four large language models (LLMs): GPT-4, LLaMA 3, Qwen, and Claude. Our results reveal a counterintuitive trend: models with higher technical accuracy often exhibit lower agreement with domain experts, whereas LLMs demonstrate greater expert alignment despite relatively lower accuracy scores. To quantify and interpret model-expert agreement, we employ Cohen's Kappa, Principal Component Analysis (PCA), and SHAP-based explainability techniques. Findings indicate that expert-aligned models tend to rely more on contextual and temporal language cues, rather than location-specific keywords. These results underscore that accuracy alone is insufficient for evaluating models in safety-critical NLP applications. We advocate for incorporating expert agreement as a complementary metric in model evaluation frameworks and highlight the promise of LLMs as interpretable, scalable tools for crash analysis pipelines."
  },
  {
    "title": "GraphAttack: Exploiting Representational Blindspots in LLM Safety Mechanisms",
    "url": "http://arxiv.org/abs/2504.13052v1",
    "arxiv_id": "2504.13052v1",
    "authors": [
      "Sinan He",
      "An Wang"
    ],
    "published": "2025-04-17T16:09:12+00:00",
    "summary": "Large Language Models (LLMs) have been equipped with safety mechanisms to prevent harmful outputs, but these guardrails can often be bypassed through \"jailbreak\" prompts. This paper introduces a novel graph-based approach to systematically generate jailbreak prompts through semantic transformations. We represent malicious prompts as nodes in a graph structure with edges denoting different transformations, leveraging Abstract Meaning Representation (AMR) and Resource Description Framework (RDF) to parse user goals into semantic components that can be manipulated to evade safety filters. We demonstrate a particularly effective exploitation vector by instructing LLMs to generate code that realizes the intent described in these semantic graphs, achieving success rates of up to 87% against leading commercial LLMs. Our analysis reveals that contextual framing and abstraction are particularly effective at circumventing safety measures, highlighting critical gaps in current safety alignment techniques that focus primarily on surface-level patterns. These findings provide insights for developing more robust safeguards against structured semantic attacks. Our research contributes both a theoretical framework and practical methodology for systematically stress-testing LLM safety mechanisms."
  },
  {
    "title": "QI-MPC: A Hybrid Quantum-Inspired Model Predictive Control for Learning Optimal Policies",
    "url": "http://arxiv.org/abs/2504.13041v1",
    "arxiv_id": "2504.13041v1",
    "authors": [
      "Muhammad Al-Zafar Khan",
      "Jamal Al-Karaki"
    ],
    "published": "2025-04-17T15:55:37+00:00",
    "summary": "In this paper, we present Quantum-Inspired Model Predictive Control (QIMPC), an approach that uses Variational Quantum Circuits (VQCs) to learn control polices in MPC problems. The viability of the approach is tested in five experiments: A target-tracking control strategy, energy-efficient building climate control, autonomous vehicular dynamics, the simple pendulum, and the compound pendulum. Three safety guarantees were established for the approach, and the experiments gave the motivation for two important theoretical results that, in essence, identify systems for which the approach works best."
  },
  {
    "title": "Safe Physics-Informed Machine Learning for Dynamics and Control",
    "url": "http://arxiv.org/abs/2504.12952v1",
    "arxiv_id": "2504.12952v1",
    "authors": [
      "Jan Drgona",
      "Truong X. Nghiem",
      "Thomas Beckers",
      "Mahyar Fazlyab",
      "Enrique Mallada",
      "Colin Jones",
      "Draguna Vrabie",
      "Steven L. Brunton",
      "Rolf Findeisen"
    ],
    "published": "2025-04-17T13:52:55+00:00",
    "summary": "This tutorial paper focuses on safe physics-informed machine learning in the context of dynamics and control, providing a comprehensive overview of how to integrate physical models and safety guarantees. As machine learning techniques enhance the modeling and control of complex dynamical systems, ensuring safety and stability remains a critical challenge, especially in safety-critical applications like autonomous vehicles, robotics, medical decision-making, and energy systems. We explore various approaches for embedding and ensuring safety constraints, such as structural priors, Lyapunov functions, Control Barrier Functions, predictive control, projections, and robust optimization techniques, ensuring that the learned models respect stability and safety criteria. Additionally, we delve into methods for uncertainty quantification and safety verification, including reachability analysis and neural network verification tools, which help validate that control policies remain within safe operating bounds even in uncertain environments. The paper includes illustrative examples demonstrating the implementation aspects of safe learning frameworks that combine the strengths of data-driven approaches with the rigor of physical principles, offering a path toward the safe control of complex dynamical systems."
  },
  {
    "title": "In Which Areas of Technical AI Safety Could Geopolitical Rivals Cooperate?",
    "url": "http://arxiv.org/abs/2504.12914v1",
    "arxiv_id": "2504.12914v1",
    "authors": [
      "Ben Bucknall",
      "Saad Siddiqui",
      "Lara Thurnherr",
      "Conor McGurk",
      "Ben Harack",
      "Anka Reuel",
      "Patricia Paskov",
      "Casey Mahoney",
      "S\u00f6ren Mindermann",
      "Scott Singer",
      "Vinay Hiremath",
      "Charbel-Rapha\u00ebl Segerie",
      "Oscar Delaney",
      "Alessandro Abate",
      "Fazl Barez",
      "Michael K. Cohen",
      "Philip Torr",
      "Ferenc Husz\u00e1r",
      "Anisoara Calinescu",
      "Gabriel Davis Jones",
      "Yoshua Bengio",
      "Robert Trager"
    ],
    "published": "2025-04-17T13:03:56+00:00",
    "summary": "International cooperation is common in AI research, including between geopolitical rivals. While many experts advocate for greater international cooperation on AI safety to address shared global risks, some view cooperation on AI with suspicion, arguing that it can pose unacceptable risks to national security. However, the extent to which cooperation on AI safety poses such risks, as well as provides benefits, depends on the specific area of cooperation. In this paper, we consider technical factors that impact the risks of international cooperation on AI safety research, focusing on the degree to which such cooperation can advance dangerous capabilities, result in the sharing of sensitive information, or provide opportunities for harm. We begin by why nations historically cooperate on strategic technologies and analyse current US-China cooperation in AI as a case study. We further argue that existing frameworks for managing associated risks can be supplemented with consideration of key risks specific to cooperation on technical AI safety research. Through our analysis, we find that research into AI verification mechanisms and shared protocols may be suitable areas for such cooperation. Through this analysis we aim to help researchers and governments identify and mitigate the risks of international cooperation on AI safety research, so that the benefits of cooperation can be fully realised."
  },
  {
    "title": "UncAD: Towards Safe End-to-end Autonomous Driving via Online Map Uncertainty",
    "url": "http://arxiv.org/abs/2504.12826v1",
    "arxiv_id": "2504.12826v1",
    "authors": [
      "Pengxuan Yang",
      "Yupeng Zheng",
      "Qichao Zhang",
      "Kefei Zhu",
      "Zebin Xing",
      "Qiao Lin",
      "Yun-Fu Liu",
      "Zhiguo Su",
      "Dongbin Zhao"
    ],
    "published": "2025-04-17T10:40:36+00:00",
    "summary": "End-to-end autonomous driving aims to produce planning trajectories from raw sensors directly. Currently, most approaches integrate perception, prediction, and planning modules into a fully differentiable network, promising great scalability. However, these methods typically rely on deterministic modeling of online maps in the perception module for guiding or constraining vehicle planning, which may incorporate erroneous perception information and further compromise planning safety. To address this issue, we delve into the importance of online map uncertainty for enhancing autonomous driving safety and propose a novel paradigm named UncAD. Specifically, UncAD first estimates the uncertainty of the online map in the perception module. It then leverages the uncertainty to guide motion prediction and planning modules to produce multi-modal trajectories. Finally, to achieve safer autonomous driving, UncAD proposes an uncertainty-collision-aware planning selection strategy according to the online map uncertainty to evaluate and select the best trajectory. In this study, we incorporate UncAD into various state-of-the-art (SOTA) end-to-end methods. Experiments on the nuScenes dataset show that integrating UncAD, with only a 1.9% increase in parameters, can reduce collision rates by up to 26% and drivable area conflict rate by up to 42%. Codes, pre-trained models, and demo videos can be accessed at https://github.com/pengxuanyang/UncAD."
  },
  {
    "title": "Supporting Urban Low-Altitude Economy: Channel Gain Map Inference Based on 3D Conditional GAN",
    "url": "http://arxiv.org/abs/2504.12794v1",
    "arxiv_id": "2504.12794v1",
    "authors": [
      "Yonghao Wang",
      "Ruoguang Li",
      "Di Wu",
      "Jiaqi Chen",
      "Yong Zeng"
    ],
    "published": "2025-04-17T09:55:03+00:00",
    "summary": "The advancement of advanced air mobility (AAM) in recent years has given rise to the concept of low-altitude economy (LAE). However, the diverse flight activities associated with the emerging LAE applications in urban scenarios confront complex physical environments, which urgently necessitates ubiquitous and reliable communication to guarantee the operation safety of the low-altitude aircraft. As one of promising technologies for the sixth generation (6G) mobile networks, channel knowledge map (CKM) enables the environment-aware communication by constructing a site-specific dataset, thereby providing a priori on-site information for the aircraft to obtain the channel state information (CSI) at arbitrary locations with much reduced online overhead. Diverse base station (BS) deployments in the three-dimensional (3D) urban low-altitude environment require efficient 3D CKM construction to capture spatial channel characteristics with less overhead. Towards this end, this paper proposes a 3D channel gain map (CGM) inference method based on a 3D conditional generative adversarial network (3D-CGAN). Specifically, we first analyze the potential deployment types of BSs in urban low-altitude scenario, and investigate the CGM representation with the corresponding 3D channel gain model. The framework of the proposed 3D-CGAN is then discussed, which is trained by a dataset consisting of existing CGMs. Consequently, the trained 3D-CGAN is capable of inferring the corresponding CGM only based on the BS coordinate without additional measurement. The simulation results demonstrate that the CGMs inferred by the proposed 3D-CGAN outperform those of the benchmark schemes, which can accurately reflect the radio propagation condition in 3D environment."
  },
  {
    "title": "On Error Classification from Physiological Signals within Airborne Environment",
    "url": "http://arxiv.org/abs/2504.12769v1",
    "arxiv_id": "2504.12769v1",
    "authors": [
      "Niall McGuire",
      "Yashar Moshfeghi"
    ],
    "published": "2025-04-17T09:08:21+00:00",
    "summary": "Human error remains a critical concern in aviation safety, contributing to 70-80% of accidents despite technological advancements. While physiological measures show promise for error detection in laboratory settings, their effectiveness in dynamic flight environments remains underexplored. Through live flight trials with nine commercial pilots, we investigated whether established error-detection approaches maintain accuracy during actual flight operations. Participants completed standardized multi-tasking scenarios across conditions ranging from laboratory settings to straight-and-level flight and 2G manoeuvres while we collected synchronized physiological data. Our findings demonstrate that EEG-based classification maintains high accuracy (87.83%) during complex flight manoeuvres, comparable to laboratory performance (89.23%). Eye-tracking showed moderate performance (82.50\\%), while ECG performed near chance level (51.50%). Classification accuracy remained stable across flight conditions, with minimal degradation during 2G manoeuvres. These results provide the first evidence that physiological error detection can translate effectively to operational aviation environments."
  },
  {
    "title": "Falcon: Advancing Asynchronous BFT Consensus for Lower Latency and Enhanced Throughput",
    "url": "http://arxiv.org/abs/2504.12766v1",
    "arxiv_id": "2504.12766v1",
    "authors": [
      "Xiaohai Dai",
      "Chaozheng Ding",
      "Wei Li",
      "Jiang Xiao",
      "Bolin Zhang",
      "Chen Yu",
      "Albert Y. Zomaya",
      "Hai Jin"
    ],
    "published": "2025-04-17T09:03:55+00:00",
    "summary": "Asynchronous Byzantine Fault Tolerant (BFT) consensus protocols have garnered significant attention with the rise of blockchain technology. A typical asynchronous protocol is designed by executing sequential instances of the Asynchronous Common Sub-seQuence (ACSQ). The ACSQ protocol consists of two primary components: the Asynchronous Common Subset (ACS) protocol and a block sorting mechanism, with the ACS protocol comprising two stages: broadcast and agreement. However, current protocols encounter three critical issues: high latency arising from the execution of the agreement stage, latency instability due to the integral-sorting mechanism, and reduced throughput caused by block discarding. To address these issues,we propose Falcon, an asynchronous BFT protocol that achieves low latency and enhanced throughput. Falcon introduces a novel broadcast protocol, Graded Broadcast (GBC), which enables a block to be included in the ACS set directly, bypassing the agreement stage and thereby reducing latency. To ensure safety, Falcon incorporates a new binary agreement protocol called Asymmetrical Asynchronous Binary Agreement (AABA), designed to complement GBC. Additionally, Falcon employs a partial-sorting mechanism, allowing continuous rather than simultaneous block committing, enhancing latency stability. Finally, we incorporate an agreement trigger that, before its activation, enables nodes to wait for more blocks to be delivered and committed, thereby boosting throughput. We conduct a series of experiments to evaluate Falcon, demonstrating its superior performance."
  },
  {
    "title": "Distributed Intelligent Sensing and Communications for 6G: Architecture and Use Cases",
    "url": "http://arxiv.org/abs/2504.12765v1",
    "arxiv_id": "2504.12765v1",
    "authors": [
      "Kyriakos Stylianopoulos",
      "Giyyarpuram Madhusudan",
      "Guillaume Jornod",
      "Sami Mekki",
      "Francesca Costanzo",
      "Hui Chen",
      "Placido Mursia",
      "Maurizio Crozzoli",
      "Emilio Calvanese Strinati",
      "George C. Alexandropoulos",
      "Henk Wymeersch"
    ],
    "published": "2025-04-17T09:02:36+00:00",
    "summary": "The Distributed Intelligent Sensing and Communication (DISAC) framework redefines Integrated Sensing and Communication (ISAC) for 6G by leveraging distributed architectures to enhance scalability, adaptability, and resource efficiency. This paper presents key architectural enablers, including advanced data representation, seamless target handover, support for heterogeneous devices, and semantic integration. Two use cases illustrate the transformative potential of DISAC: smart factory shop floors and Vulnerable Road User (VRU) protection at smart intersections. These scenarios demonstrate significant improvements in precision, safety, and operational efficiency compared to traditional ISAC systems. The preliminary DISAC architecture incorporates intelligent data processing, distributed coordination, and emerging technologies such as Reconfigurable Intelligent Surfaces (RIS) to meet 6G's stringent requirements. By addressing critical challenges in sensing accuracy, latency, and real-time decision-making, DISAC positions itself as a cornerstone for next-generation wireless networks, advancing innovation in dynamic and complex environments."
  },
  {
    "title": "Incorporating a Deep Neural Network into Moving Horizon Estimation for Embedded Thermal Torque Derating of an Electric Machine",
    "url": "http://arxiv.org/abs/2504.12736v1",
    "arxiv_id": "2504.12736v1",
    "authors": [
      "Alexander Winkler",
      "Pranav Shah",
      "Katrin Baumg\u00e4rtner",
      "Vasu Sharma",
      "David Gordon",
      "Jakob Andert"
    ],
    "published": "2025-04-17T08:24:32+00:00",
    "summary": "This study introduces a novel state estimation framework that incorporates Deep Neural Networks (DNNs) into Moving Horizon Estimation (MHE), shifting from traditional physics-based models to rapidly developed data-driven techniques. A DNN model with Long Short-Term Memory (LSTM) nodes is trained on synthetic data generated by a high-fidelity thermal model of a Permanent Magnet Synchronous Machine (PMSM), which undergoes thermal derating as part of the torque control strategy in a battery electric vehicle. The MHE is constructed by integrating the trained DNN with a simplified driving dynamics model in a discrete-time formulation, incorporating the LSTM hidden and cell states in the state vector to retain system dynamics. The resulting optimal control problem (OCP) is formulated as a nonlinear program (NLP) and implemented using the acados framework. Model-in-the-loop (MiL) simulations demonstrate accurate temperature estimation, even under noisy sensor conditions or failures. Achieving threefold real-time capability on embedded hardware confirms the feasibility of the approach for practical deployment. The primary focus of this study is to assess the feasibility of the MHE framework using a DNN-based plant model instead of focusing on quantitative comparisons of vehicle performance. Overall, this research highlights the potential of DNN-based MHE for real-time, safety-critical applications by combining the strengths of model-based and data-driven methods."
  },
  {
    "title": "Collaborative Perception Datasets for Autonomous Driving: A Review",
    "url": "http://arxiv.org/abs/2504.12696v1",
    "arxiv_id": "2504.12696v1",
    "authors": [
      "Naibang Wang",
      "Deyong Shang",
      "Yan Gong",
      "Xiaoxi Hu",
      "Ziying Song",
      "Lei Yang",
      "Yuhan Huang",
      "Xiaoyu Wang",
      "Jianli Lu"
    ],
    "published": "2025-04-17T06:49:21+00:00",
    "summary": "Collaborative perception has attracted growing interest from academia and industry due to its potential to enhance perception accuracy, safety, and robustness in autonomous driving through multi-agent information fusion. With the advancement of Vehicle-to-Everything (V2X) communication, numerous collaborative perception datasets have emerged, varying in cooperation paradigms, sensor configurations, data sources, and application scenarios. However, the absence of systematic summarization and comparative analysis hinders effective resource utilization and standardization of model evaluation. As the first comprehensive review focused on collaborative perception datasets, this work reviews and compares existing resources from a multi-dimensional perspective. We categorize datasets based on cooperation paradigms, examine their data sources and scenarios, and analyze sensor modalities and supported tasks. A detailed comparative analysis is conducted across multiple dimensions. We also outline key challenges and future directions, including dataset scalability, diversity, domain adaptation, standardization, privacy, and the integration of large language models. To support ongoing research, we provide a continuously updated online repository of collaborative perception datasets and related literature: https://github.com/frankwnb/Collaborative-Perception-Datasets-for-Autonomous-Driving."
  },
  {
    "title": "Embodied-R: Collaborative Framework for Activating Embodied Spatial Reasoning in Foundation Models via Reinforcement Learning",
    "url": "http://arxiv.org/abs/2504.12680v1",
    "arxiv_id": "2504.12680v1",
    "authors": [
      "Baining Zhao",
      "Ziyou Wang",
      "Jianjie Fang",
      "Chen Gao",
      "Fanhang Man",
      "Jinqiang Cui",
      "Xin Wang",
      "Xinlei Chen",
      "Yong Li",
      "Wenwu Zhu"
    ],
    "published": "2025-04-17T06:16:11+00:00",
    "summary": "Humans can perceive and reason about spatial relationships from sequential visual observations, such as egocentric video streams. However, how pretrained models acquire such abilities, especially high-level reasoning, remains unclear. This paper introduces Embodied-R, a collaborative framework combining large-scale Vision-Language Models (VLMs) for perception and small-scale Language Models (LMs) for reasoning. Using Reinforcement Learning (RL) with a novel reward system considering think-answer logical consistency, the model achieves slow-thinking capabilities with limited computational resources. After training on only 5k embodied video samples, Embodied-R with a 3B LM matches state-of-the-art multimodal reasoning models (OpenAI-o1, Gemini-2.5-pro) on both in-distribution and out-of-distribution embodied spatial reasoning tasks. Embodied-R also exhibits emergent thinking patterns such as systematic analysis and contextual integration. We further explore research questions including response length, training on VLM, strategies for reward design, and differences in model generalization after SFT (Supervised Fine-Tuning) and RL training."
  },
  {
    "title": "Predicting Driver's Perceived Risk: a Model Based on Semi-Supervised Learning Strategy",
    "url": "http://arxiv.org/abs/2504.12665v1",
    "arxiv_id": "2504.12665v1",
    "authors": [
      "Siwei Huang",
      "Chenhao Yang",
      "Chuan Hu"
    ],
    "published": "2025-04-17T05:50:33+00:00",
    "summary": "Drivers' perception of risk determines their acceptance, trust, and use of the Automated Driving Systems (ADSs). However, perceived risk is subjective and difficult to evaluate using existing methods. To address this issue, a driver's subjective perceived risk (DSPR) model is proposed, regarding perceived risk as a dynamically triggered mechanism with anisotropy and attenuation. 20 participants are recruited for a driver-in-the-loop experiment to report their real-time subjective risk ratings (SRRs) when experiencing various automatic driving scenarios. A convolutional neural network and bidirectional long short-term memory network with temporal pattern attention (CNN-Bi-LSTM-TPA) is embedded into a semi-supervised learning strategy to predict SRRs, aiming to reduce data noise caused by subjective randomness of participants. The results illustrate that DSPR achieves the highest prediction accuracy of 87.91% in predicting SRRs, compared to three state-of-the-art risk models. The semi-supervised strategy improves accuracy by 20.12%. Besides, CNN-Bi-LSTM-TPA network presents the highest accuracy among four different LSTM structures. This study offers an effective method for assessing driver's perceived risk, providing support for the safety enhancement of ADS and driver's trust improvement."
  },
  {
    "title": "VLMGuard-R1: Proactive Safety Alignment for VLMs via Reasoning-Driven Prompt Optimization",
    "url": "http://arxiv.org/abs/2504.12661v1",
    "arxiv_id": "2504.12661v1",
    "authors": [
      "Menglan Chen",
      "Xianghe Pang",
      "Jingjing Dong",
      "WenHao Wang",
      "Yaxin Du",
      "Siheng Chen"
    ],
    "published": "2025-04-17T05:46:41+00:00",
    "summary": "Aligning Vision-Language Models (VLMs) with safety standards is essential to mitigate risks arising from their multimodal complexity, where integrating vision and language unveils subtle threats beyond the reach of conventional safeguards. Inspired by the insight that reasoning across modalities is key to preempting intricate vulnerabilities, we propose a novel direction for VLM safety: multimodal reasoning-driven prompt rewriting. To this end, we introduce VLMGuard-R1, a proactive framework that refines user inputs through a reasoning-guided rewriter, dynamically interpreting text-image interactions to deliver refined prompts that bolster safety across diverse VLM architectures without altering their core parameters. To achieve this, we devise a three-stage reasoning pipeline to synthesize a dataset that trains the rewriter to infer subtle threats, enabling tailored, actionable responses over generic refusals. Extensive experiments across three benchmarks with five VLMs reveal that VLMGuard-R1 outperforms four baselines. In particular, VLMGuard-R1 achieves a remarkable 43.59\\% increase in average safety across five models on the SIUO benchmark."
  },
  {
    "title": "Photon Calibration Performance of KAGRA during the 4th Joint Observing Run (O4)",
    "url": "http://arxiv.org/abs/2504.12657v1",
    "arxiv_id": "2504.12657v1",
    "authors": [
      "Dan Chen",
      "Shingo Hido",
      "Darkhan Tuyenbayev",
      "Dripta Bhattacharjee",
      "Nobuyuki Kanda",
      "Richard Savage",
      "Rishabh Bajpai",
      "Sadakazu Haino",
      "Takahiro Sawada",
      "Takahiro Yamamoto",
      "Takayuki Tomaru",
      "Yoshiki Moriwaki"
    ],
    "published": "2025-04-17T05:36:31+00:00",
    "summary": "KAGRA is a kilometer-scale cryogenic gravitational-wave (GW) detector in Japan. It joined the 4th joint observing run (O4) in May 2023 in collaboration with the Laser Interferometer GW Observatory (LIGO) in the USA, and Virgo in Italy. After one month of observations, KAGRA entered a break period to enhance its sensitivity to GWs, and it is planned to rejoin O4 before its scheduled end in October 2025. To accurately recover the information encoded in the GW signals, it is essential to properly calibrate the observed signals. We employ a photon calibration (Pcal) system as a reference signal injector to calibrate the output signals obtained from the telescope. In ideal future conditions, the uncertainty in Pcal could dominate the uncertainty in the observed data. In this paper, we present the methods used to estimate the uncertainty in the Pcal systems employed during KAGRA O4 and report an estimated system uncertainty of 0.79%, which is three times lower than the uncertainty achieved in the previous 3rd joint observing run (O3) in 2020. Additionally, we investigate the uncertainty in the Pcal laser power sensors, which had the highest impact on the Pcal uncertainty, and estimate the beam positions on the KAGRA main mirror, which had the second highest impact. The Pcal systems in KAGRA are the first fully functional calibration systems for a cryogenic GW telescope. To avoid interference with the KAGRA cryogenic systems, the Pcal systems incorporate unique features regarding their placement and the use of telephoto cameras, which can capture images of the mirror surface at almost normal incidence. As future GW telescopes, such as the Einstein Telescope, are expected to adopt cryogenic techniques, the performance of the KAGRA Pcal systems can serve as a valuable reference."
  },
  {
    "title": "Graph-based Path Planning with Dynamic Obstacle Avoidance for Autonomous Parking",
    "url": "http://arxiv.org/abs/2504.12616v1",
    "arxiv_id": "2504.12616v1",
    "authors": [
      "Farhad Nawaz",
      "Minjun Sung",
      "Darshan Gadginmath",
      "Jovin D'sa",
      "Sangjae Bae",
      "David Isele",
      "Nadia Figueroa",
      "Nikolai Matni",
      "Faizan M. Tariq"
    ],
    "published": "2025-04-17T03:43:20+00:00",
    "summary": "Safe and efficient path planning in parking scenarios presents a significant challenge due to the presence of cluttered environments filled with static and dynamic obstacles. To address this, we propose a novel and computationally efficient planning strategy that seamlessly integrates the predictions of dynamic obstacles into the planning process, ensuring the generation of collision-free paths. Our approach builds upon the conventional Hybrid A star algorithm by introducing a time-indexed variant that explicitly accounts for the predictions of dynamic obstacles during node exploration in the graph, thus enabling dynamic obstacle avoidance. We integrate the time-indexed Hybrid A star algorithm within an online planning framework to compute local paths at each planning step, guided by an adaptively chosen intermediate goal. The proposed method is validated in diverse parking scenarios, including perpendicular, angled, and parallel parking. Through simulations, we showcase our approach's potential in greatly improving the efficiency and safety when compared to the state of the art spline-based planning method for parking situations."
  },
  {
    "title": "ZeroSumEval: Scaling LLM Evaluation with Inter-Model Competition",
    "url": "http://arxiv.org/abs/2504.12562v1",
    "arxiv_id": "2504.12562v1",
    "authors": [
      "Haidar Khan",
      "Hisham A. Alyahya",
      "Yazeed Alnumay",
      "M Saiful Bari",
      "B\u00fclent Yener"
    ],
    "published": "2025-04-17T01:23:50+00:00",
    "summary": "Evaluating the capabilities of Large Language Models (LLMs) has traditionally relied on static benchmark datasets, human assessments, or model-based evaluations - methods that often suffer from overfitting, high costs, and biases. ZeroSumEval is a novel competition-based evaluation protocol that leverages zero-sum games to assess LLMs with dynamic benchmarks that resist saturation. ZeroSumEval encompasses a diverse suite of games, including security challenges (PyJail), classic games (Chess, Liar's Dice, Poker), knowledge tests (MathQuiz), and persuasion challenges (Gandalf, Debate). These games are designed to evaluate a range of AI capabilities such as strategic reasoning, planning, knowledge application, and creativity. Building upon recent studies that highlight the effectiveness of game-based evaluations for LLMs, ZeroSumEval enhances these approaches by providing a standardized and extensible framework. To demonstrate this, we conduct extensive experiments with >7000 simulations across 7 games and 13 models. Our results show that while frontier models from the GPT and Claude families can play common games and answer questions, they struggle to play games that require creating novel and challenging questions. We also observe that models cannot reliably jailbreak each other and fail generally at tasks requiring creativity. We release our code at https://github.com/facebookresearch/ZeroSumEval."
  },
  {
    "title": "TraCeS: Trajectory Based Credit Assignment From Sparse Safety Feedback",
    "url": "http://arxiv.org/abs/2504.12557v1",
    "arxiv_id": "2504.12557v1",
    "authors": [
      "Siow Meng Low",
      "Akshat Kumar"
    ],
    "published": "2025-04-17T01:11:08+00:00",
    "summary": "In safe reinforcement learning (RL), auxiliary safety costs are used to align the agent to safe decision making. In practice, safety constraints, including cost functions and budgets, are unknown or hard to specify, as it requires anticipation of all possible unsafe behaviors. We therefore address a general setting where the true safety definition is unknown, and has to be learned from sparsely labeled data. Our key contributions are: first, we design a safety model that performs credit assignment to estimate each decision step's impact on the overall safety using a dataset of diverse trajectories and their corresponding binary safety labels (i.e., whether the corresponding trajectory is safe/unsafe). Second, we illustrate the architecture of our safety model to demonstrate its ability to learn a separate safety score for each timestep. Third, we reformulate the safe RL problem using the proposed safety model and derive an effective algorithm to optimize a safe yet rewarding policy. Finally, our empirical results corroborate our findings and show that this approach is effective in satisfying unknown safety definition, and scalable to various continuous control tasks."
  },
  {
    "title": "ELAB: Extensive LLM Alignment Benchmark in Persian Language",
    "url": "http://arxiv.org/abs/2504.12553v1",
    "arxiv_id": "2504.12553v1",
    "authors": [
      "Zahra Pourbahman",
      "Fatemeh Rajabi",
      "Mohammadhossein Sadeghi",
      "Omid Ghahroodi",
      "Somaye Bakhshaei",
      "Arash Amini",
      "Reza Kazemi",
      "Mahdieh Soleymani Baghshah"
    ],
    "published": "2025-04-17T00:50:41+00:00",
    "summary": "This paper presents a comprehensive evaluation framework for aligning Persian Large Language Models (LLMs) with critical ethical dimensions, including safety, fairness, and social norms. It addresses the gaps in existing LLM evaluation frameworks by adapting them to Persian linguistic and cultural contexts. This benchmark creates three types of Persian-language benchmarks: (i) translated data, (ii) new data generated synthetically, and (iii) new naturally collected data. We translate Anthropic Red Teaming data, AdvBench, HarmBench, and DecodingTrust into Persian. Furthermore, we create ProhibiBench-fa, SafeBench-fa, FairBench-fa, and SocialBench-fa as new datasets to address harmful and prohibited content in indigenous culture. Moreover, we collect extensive dataset as GuardBench-fa to consider Persian cultural norms. By combining these datasets, our work establishes a unified framework for evaluating Persian LLMs, offering a new approach to culturally grounded alignment evaluation. A systematic evaluation of Persian LLMs is performed across the three alignment aspects: safety (avoiding harmful content), fairness (mitigating biases), and social norms (adhering to culturally accepted behaviors). We present a publicly available leaderboard that benchmarks Persian LLMs with respect to safety, fairness, and social norms at: https://huggingface.co/spaces/MCILAB/LLM_Alignment_Evaluation."
  },
  {
    "title": "Knowledge Acquisition on Mass-shooting Events via LLMs for AI-Driven Justice",
    "url": "http://arxiv.org/abs/2504.12545v1",
    "arxiv_id": "2504.12545v1",
    "authors": [
      "Benign John Ihugba",
      "Afsana Nasrin",
      "Ling Wu",
      "Lin Li",
      "Lijun Qian",
      "Xishuang Dong"
    ],
    "published": "2025-04-17T00:13:04+00:00",
    "summary": "Mass-shooting events pose a significant challenge to public safety, generating large volumes of unstructured textual data that hinder effective investigations and the formulation of public policy. Despite the urgency, few prior studies have effectively automated the extraction of key information from these events to support legal and investigative efforts. This paper presented the first dataset designed for knowledge acquisition on mass-shooting events through the application of named entity recognition (NER) techniques. It focuses on identifying key entities such as offenders, victims, locations, and criminal instruments, that are vital for legal and investigative purposes. The NER process is powered by Large Language Models (LLMs) using few-shot prompting, facilitating the efficient extraction and organization of critical information from diverse sources, including news articles, police reports, and social media. Experimental results on real-world mass-shooting corpora demonstrate that GPT-4o is the most effective model for mass-shooting NER, achieving the highest Micro Precision, Micro Recall, and Micro F1-scores. Meanwhile, o1-mini delivers competitive performance, making it a resource-efficient alternative for less complex NER tasks. It is also observed that increasing the shot count enhances the performance of all models, but the gains are more substantial for GPT-4o and o1-mini, highlighting their superior adaptability to few-shot learning scenarios."
  },
  {
    "title": "KFinEval-Pilot: A Comprehensive Benchmark Suite for Korean Financial Language Understanding",
    "url": "http://arxiv.org/abs/2504.13216v1",
    "arxiv_id": "2504.13216v1",
    "authors": [
      "Bokwang Hwang",
      "Seonkyu Lim",
      "Taewoong Kim",
      "Yongjae Geun",
      "Sunghyun Bang",
      "Sohyun Park",
      "Jihyun Park",
      "Myeonggyu Lee",
      "Jinwoo Lee",
      "Yerin Kim",
      "Jinsun Yoo",
      "Jingyeong Hong",
      "Jina Park",
      "Yongchan Kim",
      "Suhyun Kim",
      "Younggyun Hahm",
      "Yiseul Lee",
      "Yejee Kang",
      "Chanhyuk Yoon",
      "Chansu Lee",
      "Heeyewon Jeong",
      "Jiyeon Lee",
      "Seonhye Gu",
      "Hyebin Kang",
      "Yousang Cho",
      "Hangyeol Yoo",
      "KyungTae Lim"
    ],
    "published": "2025-04-17T00:12:58+00:00",
    "summary": "We introduce KFinEval-Pilot, a benchmark suite specifically designed to evaluate large language models (LLMs) in the Korean financial domain. Addressing the limitations of existing English-centric benchmarks, KFinEval-Pilot comprises over 1,000 curated questions across three critical areas: financial knowledge, legal reasoning, and financial toxicity. The benchmark is constructed through a semi-automated pipeline that combines GPT-4-generated prompts with expert validation to ensure domain relevance and factual accuracy. We evaluate a range of representative LLMs and observe notable performance differences across models, with trade-offs between task accuracy and output safety across different model families. These results highlight persistent challenges in applying LLMs to high-stakes financial applications, particularly in reasoning and safety. Grounded in real-world financial use cases and aligned with the Korean regulatory and linguistic context, KFinEval-Pilot serves as an early diagnostic tool for developing safer and more reliable financial AI systems."
  },
  {
    "title": "What do people expect from Artificial Intelligence? Public opinion on alignment in AI moderation from Germany and the United States",
    "url": "http://arxiv.org/abs/2504.12476v1",
    "arxiv_id": "2504.12476v1",
    "authors": [
      "Andreas Jungherr",
      "Adrian Rauchfleisch"
    ],
    "published": "2025-04-16T20:27:03+00:00",
    "summary": "Recent advances in generative Artificial Intelligence have raised public awareness, shaping expectations and concerns about their societal implications. Central to these debates is the question of AI alignment -- how well AI systems meet public expectations regarding safety, fairness, and social values. However, little is known about what people expect from AI-enabled systems and how these expectations differ across national contexts. We present evidence from two surveys of public preferences for key functional features of AI-enabled systems in Germany (n = 1800) and the United States (n = 1756). We examine support for four types of alignment in AI moderation: accuracy and reliability, safety, bias mitigation, and the promotion of aspirational imaginaries. U.S. respondents report significantly higher AI use and consistently greater support for all alignment features, reflecting broader technological openness and higher societal involvement with AI. In both countries, accuracy and safety enjoy the strongest support, while more normatively charged goals -- like fairness and aspirational imaginaries -- receive more cautious backing, particularly in Germany. We also explore how individual experience with AI, attitudes toward free speech, political ideology, partisan affiliation, and gender shape these preferences. AI use and free speech support explain more variation in Germany. In contrast, U.S. responses show greater attitudinal uniformity, suggesting that higher exposure to AI may consolidate public expectations. These findings contribute to debates on AI governance and cross-national variation in public preferences. More broadly, our study demonstrates the value of empirically grounding AI alignment debates in public attitudes and of explicitly developing normatively grounded expectations into theoretical and policy discussions on the governance of AI-generated content."
  },
  {
    "title": "Learning-based Delay Compensation for Enhanced Control of Assistive Soft Robots",
    "url": "http://arxiv.org/abs/2504.12428v1",
    "arxiv_id": "2504.12428v1",
    "authors": [
      "Adri\u00e0 Momp\u00f3 Alepuz",
      "Dimitrios Papageorgiou",
      "Silvia Tolu"
    ],
    "published": "2025-04-16T18:57:23+00:00",
    "summary": "Soft robots are increasingly used in healthcare, especially for assistive care, due to their inherent safety and adaptability. Controlling soft robots is challenging due to their nonlinear dynamics and the presence of time delays, especially in applications like a soft robotic arm for patient care. This paper presents a learning-based approach to approximate the nonlinear state predictor (Smith Predictor), aiming to improve tracking performance in a two-module soft robot arm with a short inherent input delay. The method uses Kernel Recursive Least Squares Tracker (KRLST) for online learning of the system dynamics and a Legendre Delay Network (LDN) to compress past input history for efficient delay compensation. Experimental results demonstrate significant improvement in tracking performance compared to a baseline model-based non-linear controller. Statistical analysis confirms the significance of the improvements. The method is computationally efficient and adaptable online, making it suitable for real-world scenarios and highlighting its potential for enabling safer and more accurate control of soft robots in assistive care applications."
  },
  {
    "title": "Accountable Liveness",
    "url": "http://arxiv.org/abs/2504.12218v1",
    "arxiv_id": "2504.12218v1",
    "authors": [
      "Andrew Lewis-Pye",
      "Joachim Neu",
      "Tim Roughgarden",
      "Luca Zanolini"
    ],
    "published": "2025-04-16T16:13:09+00:00",
    "summary": "Safety and liveness are the two classical security properties of consensus protocols. Recent works have strengthened safety with accountability: should any safety violation occur, a sizable fraction of adversary nodes can be proven to be protocol violators. This paper studies to what extent analogous accountability guarantees are achievable for liveness. To reveal the full complexity of this question, we introduce an interpolation between the classical synchronous and partially-synchronous models that we call the $x$-partially-synchronous network model in which, intuitively, at most an $x$ fraction of the time steps in any sufficiently long interval are asynchronous (and, as with a partially-synchronous network, all time steps are synchronous following the passage of an unknown \"global stablization time\"). We prove a precise characterization of the parameter regime in which accountable liveness is achievable: if and only if $x < 1/2$ and $f < n/2$, where $n$ denotes the number of nodes and $f$ the number of nodes controlled by an adversary. We further refine the problem statement and our analysis by parameterizing by the number of violating nodes identified following a liveness violation, and provide evidence that the guarantees achieved by our protocol are near-optimal (as a function of $x$ and $f$). Our results provide rigorous foundations for liveness-accountability heuristics such as the \"inactivity leaks\" employed in Ethereum."
  },
  {
    "title": "GripMap: An Efficient, Spatially Resolved Constraint Framework for Offline and Online Trajectory Planning in Autonomous Racing",
    "url": "http://arxiv.org/abs/2504.12115v1",
    "arxiv_id": "2504.12115v1",
    "authors": [
      "Frederik Werner",
      "Ann-Kathrin Schwehn",
      "Markus Lienkamp",
      "Johannes Betz"
    ],
    "published": "2025-04-16T14:25:29+00:00",
    "summary": "Conventional trajectory planning approaches for autonomous vehicles often assume a fixed vehicle model that remains constant regardless of the vehicle's location. This overlooks the critical fact that the tires and the surface are the two force-transmitting partners in vehicle dynamics; while the tires stay with the vehicle, surface conditions vary with location. Recognizing these challenges, this paper presents a novel framework for spatially resolving dynamic constraints in both offline and online planning algorithms applied to autonomous racing. We introduce the GripMap concept, which provides a spatial resolution of vehicle dynamic constraints in the Frenet frame, allowing adaptation to locally varying grip conditions. This enables compensation for location-specific effects, more efficient vehicle behavior, and increased safety, unattainable with spatially invariant vehicle models. The focus is on low storage demand and quick access through perfect hashing. This framework proved advantageous in real-world applications in the presented form. Experiments inspired by autonomous racing demonstrate its effectiveness. In future work, this framework can serve as a foundational layer for developing future interpretable learning algorithms that adjust to varying grip conditions in real-time."
  },
  {
    "title": "Towards LLM Agents for Earth Observation",
    "url": "http://arxiv.org/abs/2504.12110v1",
    "arxiv_id": "2504.12110v1",
    "authors": [
      "Chia Hsiang Kao",
      "Wenting Zhao",
      "Shreelekha Revankar",
      "Samuel Speas",
      "Snehal Bhagat",
      "Rajeev Datta",
      "Cheng Perng Phoo",
      "Utkarsh Mall",
      "Carl Vondrick",
      "Kavita Bala",
      "Bharath Hariharan"
    ],
    "published": "2025-04-16T14:19:25+00:00",
    "summary": "Earth Observation (EO) provides critical planetary data for environmental monitoring, disaster management, climate science, and other scientific domains. Here we ask: Are AI systems ready for reliable Earth Observation? We introduce \\datasetnamenospace, a benchmark of 140 yes/no questions from NASA Earth Observatory articles across 13 topics and 17 satellite sensors. Using Google Earth Engine API as a tool, LLM agents can only achieve an accuracy of 33% because the code fails to run over 58% of the time. We improve the failure rate for open models by fine-tuning synthetic data, allowing much smaller models (Llama-3.1-8B) to achieve comparable accuracy to much larger ones (e.g., DeepSeek-R1). Taken together, our findings identify significant challenges to be solved before AI agents can automate earth observation, and suggest paths forward. The project page is available at https://iandrover.github.io/UnivEarth."
  },
  {
    "title": "Reasoning-Based AI for Startup Evaluation (R.A.I.S.E.): A Memory-Augmented, Multi-Step Decision Framework",
    "url": "http://arxiv.org/abs/2504.12090v1",
    "arxiv_id": "2504.12090v1",
    "authors": [
      "Jack Preuveneers",
      "Joseph Ternasky",
      "Fuat Alican",
      "Yigit Ihlamur"
    ],
    "published": "2025-04-16T13:53:42+00:00",
    "summary": "We present a novel framework that bridges the gap between the interpretability of decision trees and the advanced reasoning capabilities of large language models (LLMs) to predict startup success. Our approach leverages chain-of-thought prompting to generate detailed reasoning logs, which are subsequently distilled into structured, human-understandable logical rules. The pipeline integrates multiple enhancements - efficient data ingestion, a two-step refinement process, ensemble candidate sampling, simulated reinforcement learning scoring, and persistent memory - to ensure both stable decision-making and transparent output. Experimental evaluations on curated startup datasets demonstrate that our combined pipeline improves precision by 54% from 0.225 to 0.346 and accuracy by 50% from 0.46 to 0.70 compared to a standalone OpenAI o3 model. Notably, our model achieves over 2x the precision of a random classifier (16%). By combining state-of-the-art AI reasoning with explicit rule-based explanations, our method not only augments traditional decision-making processes but also facilitates expert intervention and continuous policy refinement. This work lays the foundation for the implementation of interpretable LLM-powered decision frameworks in high-stakes investment environments and other domains that require transparent and data-driven insights."
  },
  {
    "title": "Selective Demonstration Retrieval for Improved Implicit Hate Speech Detection",
    "url": "http://arxiv.org/abs/2504.12082v1",
    "arxiv_id": "2504.12082v1",
    "authors": [
      "Yumin Kim",
      "Hwanhee Lee"
    ],
    "published": "2025-04-16T13:43:23+00:00",
    "summary": "Hate speech detection is a crucial area of research in natural language processing, essential for ensuring online community safety. However, detecting implicit hate speech, where harmful intent is conveyed in subtle or indirect ways, remains a major challenge. Unlike explicit hate speech, implicit expressions often depend on context, cultural subtleties, and hidden biases, making them more challenging to identify consistently. Additionally, the interpretation of such speech is influenced by external knowledge and demographic biases, resulting in varied detection results across different language models. Furthermore, Large Language Models often show heightened sensitivity to toxic language and references to vulnerable groups, which can lead to misclassifications. This over-sensitivity results in false positives (incorrectly identifying harmless statements as hateful) and false negatives (failing to detect genuinely harmful content). Addressing these issues requires methods that not only improve detection precision but also reduce model biases and enhance robustness. To address these challenges, we propose a novel method, which utilizes in-context learning without requiring model fine-tuning. By adaptively retrieving demonstrations that focus on similar groups or those with the highest similarity scores, our approach enhances contextual comprehension. Experimental results show that our method outperforms current state-of-the-art techniques. Implementation details and code are available at TBD."
  },
  {
    "title": "Observational properties of regular black holes in Asymptotic Safety",
    "url": "http://arxiv.org/abs/2504.12072v1",
    "arxiv_id": "2504.12072v1",
    "authors": [
      "Abdybek Urmanov",
      "Hrishikesh Chakrabarty",
      "Daniele Malafarina"
    ],
    "published": "2025-04-16T13:28:51+00:00",
    "summary": "We consider the observational properties of a spherically symmetric, static regular black hole within the framework of asymptotic safety (AS) as proposed by Bonanno et al. The metric resembles the Schwarzschild solution in the classical limit. The departure from Schwarzschild at small scales is controlled by a single free parameter related to the ultraviolet (UV) cutoff of the theory. We investigated null and time-like geodesics around the AS metric, including circular orbits, photon rings and lensing effects. In particular we focused on the optical properties of thin accretion disks in the equatorial plane of the object and compared them with those of accretion disks in the Schwarzschild metric. We found that the radiation flux, luminosity, and efficiency of the accretion disk increase with the value of the free parameter. Using a spacetime generic open-source relativistic ray-tracing code, we simulate the K$\\alpha$ iron line profiles emitted by the disk and analyze their deviation from that of the Schwarzschild geometry."
  },
  {
    "title": "Contract-based hierarchical control using predictive feasibility value functions",
    "url": "http://arxiv.org/abs/2504.12036v1",
    "arxiv_id": "2504.12036v1",
    "authors": [
      "Felix Berkel",
      "Kim Peter Wabersich",
      "Hongxi Xiang",
      "Elias Milios"
    ],
    "published": "2025-04-16T12:51:18+00:00",
    "summary": "Today's control systems are often characterized by modularity and safety requirements to handle complexity, resulting in hierarchical control structures. Although hierarchical model predictive control offers favorable properties, achieving a provably safe, yet modular design remains a challenge. This paper introduces a contract-based hierarchical control strategy to improve the performance of control systems facing challenges related to model inconsistency and independent controller design across hierarchies. We consider a setup where a higher-level controller generates references that affect the constraints of a lower-level controller, which is based on a soft-constrained MPC formulation. The optimal slack variables serve as the basis for a contract that allows the higher-level controller to assess the feasibility of the reference trajectory without exact knowledge of the model, constraints, and cost of the lower-level controller. To ensure computational efficiency while maintaining model confidentiality, we propose using an explicit function approximation, such as a neural network, to represent the cost of optimal slack values. The approach is tested for a hierarchical control setup consisting of a planner and a motion controller as commonly found in autonomous driving."
  },
  {
    "title": "Global Patterns of Extreme Temperature Teleconnections Using Climate Network Analysis",
    "url": "http://arxiv.org/abs/2504.12008v1",
    "arxiv_id": "2504.12008v1",
    "authors": [
      "Yuhao Feng",
      "Jun Meng",
      "Jingfang Fan"
    ],
    "published": "2025-04-16T12:05:15+00:00",
    "summary": "Extreme weather events, rare yet profoundly impactful, are often accompanied by severe conditions. Increasing global temperatures are poised to exacerbate these events, resulting in greater human casualties, economic losses, and ecological destruction. Complex global climate interactions, known as teleconnections, can lead to widespread repercussions triggered by localized extreme weather. Understanding these teleconnection patterns is crucial for weather forecasting, enhancing safety, and advancing climate science. Here, we employ climate network analysis to uncover teleconnection patterns associated with extreme temperature fluctuations, including both extreme warming and cooling events occurring on a daily basis. Our study results demonstrate that the distances of significant teleconnections initially conform to a power-law decay, signifying a decline in connectivity with distance. However, this power-law decay tendency breaks beyond a certain threshold distance, suggesting the existence of long-distance connections. Additionally, we uncover a greater prevalence of long-distance connectivity among extreme cooling events compared to extreme warming events. The global pattern of teleconnections is, in part, driven by the mechanism of Rossby waves, which serve as a rapid conduit for inducing correlated fluctuations in both pressure and temperature. These results enhance our understanding of the multiscale nature of climate teleconnections and hold significant implications for improving weather forecasting and assessing climate risks in a warming world."
  },
  {
    "title": "Comment on Path integral measure and RG equations for gravity",
    "url": "http://arxiv.org/abs/2504.12006v1",
    "arxiv_id": "2504.12006v1",
    "authors": [
      "Aaron Held",
      "Benjamin Knorr",
      "Jan M. Pawlowski",
      "Alessia Platania",
      "Manuel Reichert",
      "Frank Saueressig",
      "Marc Schiffer"
    ],
    "published": "2025-04-16T12:00:22+00:00",
    "summary": "Asymptotic safety is a candidate for a predictive quantum theory of gravity and matter. Recent works (arXiv:2412.10194 and arXiv:2412.14108) challenged this scenario. We show that their arguments fail on a basic level."
  },
  {
    "title": "Leveraging Machine Learning Models to Predict the Outcome of Digital Medical Triage Interviews",
    "url": "http://arxiv.org/abs/2504.11977v1",
    "arxiv_id": "2504.11977v1",
    "authors": [
      "Sofia Krylova",
      "Fabian Schmidt",
      "Vladimir Vlassov"
    ],
    "published": "2025-04-16T11:17:23+00:00",
    "summary": "Many existing digital triage systems are questionnaire-based, guiding patients to appropriate care levels based on information (e.g., symptoms, medical history, and urgency) provided by the patients answering questionnaires. Such a system often uses a deterministic model with predefined rules to determine care levels. It faces challenges with incomplete triage interviews since it can only assist patients who finish the process. In this study, we explore the use of machine learning (ML) to predict outcomes of unfinished interviews, aiming to enhance patient care and service quality. Predicting triage outcomes from incomplete data is crucial for patient safety and healthcare efficiency. Our findings show that decision-tree models, particularly LGBMClassifier and CatBoostClassifier, achieve over 80\\% accuracy in predicting outcomes from complete interviews while having a linear correlation between the prediction accuracy and interview completeness degree. For example, LGBMClassifier achieves 88,2\\% prediction accuracy for interviews with 100\\% completeness, 79,6\\% accuracy for interviews with 80\\% completeness, 58,9\\% accuracy for 60\\% completeness, and 45,7\\% accuracy for 40\\% completeness. The TabTransformer model demonstrated exceptional accuracy of over 80\\% for all degrees of completeness but required extensive training time, indicating a need for more powerful computational resources. The study highlights the linear correlation between interview completeness and predictive power of the decision-tree models."
  },
  {
    "title": "SemDiff: Generating Natural Unrestricted Adversarial Examples via Semantic Attributes Optimization in Diffusion Models",
    "url": "http://arxiv.org/abs/2504.11923v1",
    "arxiv_id": "2504.11923v1",
    "authors": [
      "Zeyu Dai",
      "Shengcai Liu",
      "Rui He",
      "Jiahao Wu",
      "Ning Lu",
      "Wenqi Fan",
      "Qing Li",
      "Ke Tang"
    ],
    "published": "2025-04-16T09:58:04+00:00",
    "summary": "Unrestricted adversarial examples (UAEs), allow the attacker to create non-constrained adversarial examples without given clean samples, posing a severe threat to the safety of deep learning models. Recent works utilize diffusion models to generate UAEs. However, these UAEs often lack naturalness and imperceptibility due to simply optimizing in intermediate latent noises. In light of this, we propose SemDiff, a novel unrestricted adversarial attack that explores the semantic latent space of diffusion models for meaningful attributes, and devises a multi-attributes optimization approach to ensure attack success while maintaining the naturalness and imperceptibility of generated UAEs. We perform extensive experiments on four tasks on three high-resolution datasets, including CelebA-HQ, AFHQ and ImageNet. The results demonstrate that SemDiff outperforms state-of-the-art methods in terms of attack success rate and imperceptibility. The generated UAEs are natural and exhibit semantically meaningful changes, in accord with the attributes' weights. In addition, SemDiff is found capable of evading different defenses, which further validates its effectiveness and threatening."
  },
  {
    "title": "Rethinking the Generation of High-Quality CoT Data from the Perspective of LLM-Adaptive Question Difficulty Grading",
    "url": "http://arxiv.org/abs/2504.11919v1",
    "arxiv_id": "2504.11919v1",
    "authors": [
      "Qianjin Yu",
      "Keyu Wu",
      "Zihan Chen",
      "Chushu Zhang",
      "Manlin Mei",
      "Lingjun Huang",
      "Fang Tan",
      "Yongsheng Du",
      "Kunlin Liu",
      "Yurui Zhu"
    ],
    "published": "2025-04-16T09:55:34+00:00",
    "summary": "Recently, DeepSeek-R1 (671B) (DeepSeek-AIet al., 2025) has demonstrated its excellent reasoning ability in complex tasks and has publiclyshared its methodology. This provides potentially high-quality chain-of-thought (CoT) data for stimulating the reasoning abilities of small-sized large language models (LLMs). To generate high-quality CoT data for different LLMs, we seek an efficient method for generating high-quality CoT data with LLM-Adaptive questiondifficulty levels. First, we grade the difficulty of the questions according to the reasoning ability of the LLMs themselves and construct a LLM-Adaptive question database. Second, we sample the problem database based on a distribution of difficulty levels of the questions and then use DeepSeek-R1 (671B) (DeepSeek-AI et al., 2025) to generate the corresponding high-quality CoT data with correct answers. Thanks to the construction of CoT data with LLM-Adaptive difficulty levels, we have significantly reduced the cost of data generation and enhanced the efficiency of model supervised fine-tuning (SFT). Finally, we have validated the effectiveness and generalizability of the proposed method in the fields of complex mathematical competitions and code generation tasks. Notably, with only 2k high-quality mathematical CoT data, our ZMath-32B surpasses DeepSeek-Distill-32B in math reasoning task. Similarly, with only 2k high-quality code CoT data, our ZCode-32B surpasses DeepSeek-Distill-32B in code reasoning tasks."
  },
  {
    "title": "A Graph-Based Reinforcement Learning Approach with Frontier Potential Based Reward for Safe Cluttered Environment Exploration",
    "url": "http://arxiv.org/abs/2504.11907v1",
    "arxiv_id": "2504.11907v1",
    "authors": [
      "Gabriele Calzolari",
      "Vidya Sumathy",
      "Christoforos Kanellakis",
      "George Nikolakopoulos"
    ],
    "published": "2025-04-16T09:31:14+00:00",
    "summary": "Autonomous exploration of cluttered environments requires efficient exploration strategies that guarantee safety against potential collisions with unknown random obstacles. This paper presents a novel approach combining a graph neural network-based exploration greedy policy with a safety shield to ensure safe navigation goal selection. The network is trained using reinforcement learning and the proximal policy optimization algorithm to maximize exploration efficiency while reducing the safety shield interventions. However, if the policy selects an infeasible action, the safety shield intervenes to choose the best feasible alternative, ensuring system consistency. Moreover, this paper proposes a reward function that includes a potential field based on the agent's proximity to unexplored regions and the expected information gain from reaching them. Overall, the approach investigated in this paper merges the benefits of the adaptability of reinforcement learning-driven exploration policies and the guarantee ensured by explicit safety mechanisms. Extensive evaluations in simulated environments demonstrate that the approach enables efficient and safe exploration in cluttered environments."
  },
  {
    "title": "MDHP-Net: Detecting an Emerging Time-exciting Threat in IVN",
    "url": "http://arxiv.org/abs/2504.11867v1",
    "arxiv_id": "2504.11867v1",
    "authors": [
      "Qi Liu",
      "Yanchen Liu",
      "Ruifeng Li",
      "Chenhong Cao",
      "Yufeng Li",
      "Xingyu Li",
      "Peng Wang",
      "Runhan Feng",
      "Shiyang Bu"
    ],
    "published": "2025-04-16T08:41:24+00:00",
    "summary": "The integration of intelligent and connected technologies in modern vehicles, while offering enhanced functionalities through Electronic Control Unit (ECU) and interfaces like OBD-II and telematics, also exposes the vehicle's in-vehicle network (IVN) to potential cyberattacks. Unlike prior work, we identify a new time-exciting threat model against IVN. These attacks inject malicious messages that exhibit a time-exciting effect, gradually manipulating network traffic to disrupt vehicle operations and compromise safety-critical functions. We systematically analyze the characteristics of the threat: dynamism, time-exciting impact, and low prior knowledge dependency. To validate its practicality, we replicate the attack on a real Advanced Driver Assistance System via Controller Area Network (CAN), exploiting Unified Diagnostic Service vulnerabilities and proposing four attack strategies. While CAN's integrity checks mitigate attacks, Ethernet migration (e.g., DoIP/SOME/IP) introduces new surfaces. We further investigate the feasibility of time-exciting threat under SOME/IP. To detect time-exciting threat, we introduce MDHP-Net, leveraging Multi-Dimentional Hawkes Process (MDHP) and temporal and message-wise feature extracting structures. Meanwhile, to estimate MDHP parameters, we developed the first GPU-optimized gradient descent solver for MDHP (MDHP-GDS). These modules significantly improves the detection rate under time-exciting attacks in multi-ECU IVN system. To address data scarcity, we release STEIA9, the first open-source dataset for time-exciting attacks, covering 9 Ethernet-based attack scenarios. Extensive experiments on STEIA9 (9 attack scenarios) show MDHP-Net outperforms 3 baselines, confirming attack feasibility and detection efficacy."
  },
  {
    "title": "Visualization Analysis and Impedance Analysis for the Aging Behavior Assessment of 18650 Cells",
    "url": "http://arxiv.org/abs/2504.11861v1",
    "arxiv_id": "2504.11861v1",
    "authors": [
      "Yihan Shi",
      "Qingrui Pan",
      "Jitao Li",
      "Xiaoze Shi",
      "Youchang Wang",
      "Peng Xiao"
    ],
    "published": "2025-04-16T08:31:27+00:00",
    "summary": "This work presents a comprehensive study on the aging behavior of 18650-type lithium-ion batteries, focusing on the uneven intercalation of lithium ions during fast charging processes. It introduces a novel approach using color visual recognition technology to analyze color changes in the graphite anode, indicative of lithiation levels. The study employs X-ray diffraction (XRD) and Distribution of Relaxation Time (DRT) techniques to validate and analyze the observations. The study emphasizes the significance of electrode impedance, the positioning of battery tabs, and electrolyte distribution in influencing the aging dynamics of lithium-ion batteries. Furthermore, the paper presents an innovative impedance Transport-Line Model, specifically developed to capture the evolution of polarization impedance over time. This model offers a deeper understanding of the internal mechanisms driving battery aging, providing valuable insights for the design and optimization of lithium-ion batteries. The research represents a significant contribution to the field, shedding light on the complex aging processes in lithium-ion batteries, particularly under the conditions of fast charging. This could lead to improved battery performance, longevity, and safety, which are critical for the wide range of applications that depend on these energy storage systems."
  },
  {
    "title": "Ultra-Efficient Kidney Stone Fragment Removal via Spinner-Induced Synergistic Circulation and Spiral Flow",
    "url": "http://arxiv.org/abs/2504.11847v1",
    "arxiv_id": "2504.11847v1",
    "authors": [
      "Yilong Chang",
      "Jasmine Vallejo",
      "Yangqing Sun",
      "Ruike Renee Zhao"
    ],
    "published": "2025-04-16T08:10:16+00:00",
    "summary": "Kidney stones can cause severe pain and complications such as chronic kidney disease or kidney failure. Retrograde intrarenal surgery (RIRS), which uses laser lithotripsy to fragment stones for removal via a ureteroscope, is widely adopted due to its safety and effectiveness. However, conventional fragment removal methods using basketing and vacuum-assisted aspiration are inefficient, as they can capture only 1 to 3 fragments (1--3\\,mm in size) per pass, often requiring dozens to hundreds of ureteroscope passes during a single procedure to completely remove the fragments. These limitations lead to prolonged procedures and residual fragments that contribute to high recurrence rates. To address these limitations, we present a novel spinner device that enables ultra-efficient fragment removal through spinning-induced localized suction. The spinner generates a three-dimensional spiral and circulating flow field that dislodges and draws fragments into its cavity even from distances over 20\\,mm, eliminating the need to chase fragments. It can capture over 60 fragments (0.5--2\\,mm) or over 15 larger fragments (2--3\\,mm) in a single pass, significantly improving removal efficiency. In this work, the spinner design is optimized via computational fluid dynamics to maximize suction performance. \\textit{In vitro} testing demonstrates near 100\\% capture rates for up to 60 fragments in a single operation and superior large-distance capture efficacy compared to vacuum-assisted methods. \\textit{Ex vivo} testing of the integrated spinner-ureteroscope system in a porcine kidney confirmed its high performance by capturing 45 fragments in just 4 seconds during a single pass and achieving complete fragment clearance within a few passes."
  },
  {
    "title": "Support is All You Need for Certified VAE Training",
    "url": "http://arxiv.org/abs/2504.11831v1",
    "arxiv_id": "2504.11831v1",
    "authors": [
      "Changming Xu",
      "Debangshu Banerjee",
      "Deepak Vasisht",
      "Gagandeep Singh"
    ],
    "published": "2025-04-16T07:41:40+00:00",
    "summary": "Variational Autoencoders (VAEs) have become increasingly popular and deployed in safety-critical applications. In such applications, we want to give certified probabilistic guarantees on performance under adversarial attacks. We propose a novel method, CIVET, for certified training of VAEs. CIVET depends on the key insight that we can bound worst-case VAE error by bounding the error on carefully chosen support sets at the latent layer. We show this point mathematically and present a novel training algorithm utilizing this insight. We show in an extensive evaluation across different datasets (in both the wireless and vision application areas), architectures, and perturbation magnitudes that our method outperforms SOTA methods achieving good standard performance with strong robustness guarantees."
  },
  {
    "title": "Multi-goal Rapidly Exploring Random Tree with Safety and Dynamic Constraints for UAV Cooperative Path Planning",
    "url": "http://arxiv.org/abs/2504.11823v1",
    "arxiv_id": "2504.11823v1",
    "authors": [
      "Thu Hang Khuat",
      "Duy-Nam Bui",
      "Hoa TT. Nguyen",
      "Mien L. Trinh",
      "Minh T. Nguyen",
      "Manh Duong Phung"
    ],
    "published": "2025-04-16T07:16:35+00:00",
    "summary": "Cooperative path planning is gaining its importance due to the increasing demand on using multiple unmanned aerial vehicles (UAVs) for complex missions. This work addresses the problem by introducing a new algorithm named MultiRRT that extends the rapidly exploring random tree (RRT) to generate paths for a group of UAVs to reach multiple goal locations at the same time. We first derive the dynamics constraint of the UAV and include it in the problem formulation. MultiRRT is then developed, taking into account the cooperative requirements and safe constraints during its path-searching process. The algorithm features two new mechanisms, node reduction and Bezier interpolation, to ensure the feasibility and optimality of the paths generated. Importantly, the interpolated paths are proven to meet the safety and dynamics constraints imposed by obstacles and the UAVs. A number of simulations, comparisons, and experiments have been conducted to evaluate the performance of the proposed approach. The results show that MultiRRT can generate collision-free paths for multiple UAVs to reach their goals with better scores in path length and smoothness metrics than state-of-the-art RRT variants including Theta-RRT, FN-RRT, RRT*, and RRT*-Smart. The generated paths are also tested in practical flights with real UAVs to evaluate their validity for cooperative tasks. The source code of the algorithm is available at https://github.com/duynamrcv/multi-target_RRT"
  },
  {
    "title": "Towards an AI Observatory for the Nuclear Sector: A tool for anticipatory governance",
    "url": "http://arxiv.org/abs/2504.12358v1",
    "arxiv_id": "2504.12358v1",
    "authors": [
      "Aditi Verma",
      "Elizabeth Williams"
    ],
    "published": "2025-04-16T03:43:15+00:00",
    "summary": "AI models are rapidly becoming embedded in all aspects of nuclear energy research and work but the safety, security, and safeguards consequences of this embedding are not well understood. In this paper, we call for the creation of an anticipatory system of governance for AI in the nuclear sector as well as the creation of a global AI observatory as a means for operationalizing anticipatory governance. The paper explores the contours of the nuclear AI observatory and an anticipatory system of governance by drawing on work in science and technology studies, public policy, and foresight studies."
  },
  {
    "title": "Safety with Agency: Human-Centered Safety Filter with Application to AI-Assisted Motorsports",
    "url": "http://arxiv.org/abs/2504.11717v1",
    "arxiv_id": "2504.11717v1",
    "authors": [
      "Donggeon David Oh",
      "Justin Lidard",
      "Haimin Hu",
      "Himani Sinhmar",
      "Elle Lazarski",
      "Deepak Gopinath",
      "Emily S. Sumner",
      "Jonathan A. DeCastro",
      "Guy Rosman",
      "Naomi Ehrich Leonard",
      "Jaime Fern\u00e1ndez Fisac"
    ],
    "published": "2025-04-16T02:42:08+00:00",
    "summary": "We propose a human-centered safety filter (HCSF) for shared autonomy that significantly enhances system safety without compromising human agency. Our HCSF is built on a neural safety value function, which we first learn scalably through black-box interactions and then use at deployment to enforce a novel quality control barrier function (Q-CBF) safety constraint. Since this Q-CBF safety filter does not require any knowledge of the system dynamics for both synthesis and runtime safety monitoring and intervention, our method applies readily to complex, black-box shared autonomy systems. Notably, our HCSF's CBF-based interventions modify the human's actions minimally and smoothly, avoiding the abrupt, last-moment corrections delivered by many conventional safety filters. We validate our approach in a comprehensive in-person user study using Assetto Corsa-a high-fidelity car racing simulator with black-box dynamics-to assess robustness in \"driving on the edge\" scenarios. We compare both trajectory data and drivers' perceptions of our HCSF assistance against unassisted driving and a conventional safety filter. Experimental results show that 1) compared to having no assistance, our HCSF improves both safety and user satisfaction without compromising human agency or comfort, and 2) relative to a conventional safety filter, our proposed HCSF boosts human agency, comfort, and satisfaction while maintaining robustness."
  },
  {
    "title": "Safety with Agency: Human-Centered Safety Filter with Application to AI-Assisted Motorsports",
    "url": "http://arxiv.org/abs/2504.11717v2",
    "arxiv_id": "2504.11717v2",
    "authors": [
      "Donggeon David Oh",
      "Justin Lidard",
      "Haimin Hu",
      "Himani Sinhmar",
      "Elle Lazarski",
      "Deepak Gopinath",
      "Emily S. Sumner",
      "Jonathan A. DeCastro",
      "Guy Rosman",
      "Naomi Ehrich Leonard",
      "Jaime Fern\u00e1ndez Fisac"
    ],
    "published": "2025-04-16T02:42:08+00:00",
    "summary": "We propose a human-centered safety filter (HCSF) for shared autonomy that significantly enhances system safety without compromising human agency. Our HCSF is built on a neural safety value function, which we first learn scalably through black-box interactions and then use at deployment to enforce a novel state-action control barrier function (Q-CBF) safety constraint. Since this Q-CBF safety filter does not require any knowledge of the system dynamics for both synthesis and runtime safety monitoring and intervention, our method applies readily to complex, black-box shared autonomy systems. Notably, our HCSF's CBF-based interventions modify the human's actions minimally and smoothly, avoiding the abrupt, last-moment corrections delivered by many conventional safety filters. We validate our approach in a comprehensive in-person user study using Assetto Corsa-a high-fidelity car racing simulator with black-box dynamics-to assess robustness in \"driving on the edge\" scenarios. We compare both trajectory data and drivers' perceptions of our HCSF assistance against unassisted driving and a conventional safety filter. Experimental results show that 1) compared to having no assistance, our HCSF improves both safety and user satisfaction without compromising human agency or comfort, and 2) relative to a conventional safety filter, our proposed HCSF boosts human agency, comfort, and satisfaction while maintaining robustness."
  },
  {
    "title": "Optimal SVI-Weighted PSPS Decisions with Decision-Dependent Outage Uncertainty",
    "url": "http://arxiv.org/abs/2504.11665v1",
    "arxiv_id": "2504.11665v1",
    "authors": [
      "Ryan Greenough",
      "Kohei Murakami",
      "Jan Kleissl",
      "Adil Khurram"
    ],
    "published": "2025-04-15T23:31:39+00:00",
    "summary": "Public Safety Power Shutoffs (PSPS) are a pre-emptive strategy to mitigate the wildfires caused by power system malfunction. System operators implement PSPS to balance wildfire mitigation efforts through de-energization of transmission lines against the risk of widespread blackouts modeled with load shedding.   Existing approaches do not incorporate decision-dependent wildfire-driven failure probabilities, as modeling outage scenario probabilities requires incorporating high-order polynomial terms in the objective. This paper uses distribution shaping to develop an efficient MILP problem representation of the distributionally robust PSPS problem. Building upon the author's prior work, the wildfire risk of operating a transmission line is a function of the probability of a wildfire-driven outage and its subsequent expected impact in acres burned.   A day-ahead unit commitment and line de-energization PSPS framework is used to assess the trade-off between total cost and wildfire risk at different levels of distributional robustness, parameterized by a level of distributional dissimilarity $\\kappa$. We perform simulations on the IEEE RTS 24-bus test system."
  },
  {
    "title": "Real-time Object and Event Detection Service through Computer Vision and Edge Computing",
    "url": "http://arxiv.org/abs/2504.11662v1",
    "arxiv_id": "2504.11662v1",
    "authors": [
      "Marcos Mendes",
      "Gon\u00e7alo Perna",
      "Pedro Rito",
      "Duarte Raposo",
      "Susana Sargento"
    ],
    "published": "2025-04-15T23:11:42+00:00",
    "summary": "The World Health Organization suggests that road traffic crashes cost approximately 518 billion dollars globally each year, which accounts for 3% of the gross domestic product for most countries. Most fatal road accidents in urban areas involve Vulnerable Road Users (VRUs). Smart cities environments present innovative approaches to combat accidents involving cutting-edge technologies, that include advanced sensors, extensive datasets, Machine Learning (ML) models, communication systems, and edge computing. This paper proposes a strategy and an implementation of a system for road monitoring and safety for smart cities, based on Computer Vision (CV) and edge computing. Promising results were obtained by implementing vision algorithms and tracking using surveillance cameras, that are part of a Smart City testbed, the Aveiro Tech City Living Lab (ATCLL). The algorithm accurately detects and tracks cars, pedestrians, and bicycles, while predicting the road state, the distance between moving objects, and inferring on collision events to prevent collisions, in near real-time."
  },
  {
    "title": "Improving LLM Interpretability and Performance via Guided Embedding Refinement for Sequential Recommendation",
    "url": "http://arxiv.org/abs/2504.11658v1",
    "arxiv_id": "2504.11658v1",
    "authors": [
      "Nanshan Jia",
      "Chenfei Yuan",
      "Yuhang Wu",
      "Zeyu Zheng"
    ],
    "published": "2025-04-15T23:03:53+00:00",
    "summary": "The fast development of Large Language Models (LLMs) offers growing opportunities to further improve sequential recommendation systems. Yet for some practitioners, integrating LLMs to their existing base recommendation systems raises questions about model interpretability, transparency and related safety. To partly alleviate challenges from these questions, we propose guided embedding refinement, a method that carries out a guided and interpretable usage of LLM to enhance the embeddings associated with the base recommendation system. Instead of directly using LLMs as the backbone of sequential recommendation systems, we utilize them as auxiliary tools to emulate the sales logic of recommendation and generate guided embeddings that capture domain-relevant semantic information on interpretable attributes. Benefiting from the strong generalization capabilities of the guided embedding, we construct refined embedding by using the guided embedding and reduced-dimension version of the base embedding. We then integrate the refined embedding into the recommendation module for training and inference. A range of numerical experiments demonstrate that guided embedding is adaptable to various given existing base embedding models, and generalizes well across different recommendation tasks. The numerical results show that the refined embedding not only improves recommendation performance, achieving approximately $10\\%$ to $50\\%$ gains in Mean Reciprocal Rank (MRR), Recall rate, and Normalized Discounted Cumulative Gain (NDCG), but also enhances interpretability, as evidenced by case studies."
  },
  {
    "title": "Verifiable Mission Planning For Space Operations",
    "url": "http://arxiv.org/abs/2504.11631v1",
    "arxiv_id": "2504.11631v1",
    "authors": [
      "Quentin Rommel",
      "Michael Hibbard",
      "Pavan Shukla",
      "Himanshu Save",
      "Srinivas Bettadpur",
      "Ufuk Topcu"
    ],
    "published": "2025-04-15T21:41:09+00:00",
    "summary": "As space missions become more complex, planning methods must maximize mission performance while rigorously enforcing safety. We develop a probabilistic approach based on a finite-horizon Markov decision process to optimize spacecraft operations planning with safety guarantees. In the model, states capture essential mission parameters, and actions represent the operational adjustments needed to meet mission objectives. By directly incorporating uncertainties from environmental conditions and spacecraft dynamics, an optimal sequence of actions is computed that maximizes expected rewards and strictly enforces safety constraints. Numerical experiments on the GRACE-FO mission demonstrate robust performance under uncertainties while providing probabilistic safety guarantees, offering a reliable solution for autonomous spacecraft operations."
  },
  {
    "title": "Provably Safe Control for Constrained Nonlinear Systems with Bounded Input",
    "url": "http://arxiv.org/abs/2504.11592v1",
    "arxiv_id": "2504.11592v1",
    "authors": [
      "Saurabh Kumar",
      "Shashi Ranjan Kumar",
      "Abhinav Sinha"
    ],
    "published": "2025-04-15T20:10:21+00:00",
    "summary": "In real-world control applications, actuator constraints and output constraints (specifically in tracking problems) are inherent and critical to ensuring safe and reliable operation. However, generally, control strategies often neglect these physical limitations, leading to potential instability, degraded performance, or even system failure when deployed on real-world systems. This paper addresses the control design problem for a class of nonlinear systems under both actuator saturation and output constraints. First, a smooth asymmetric saturation model (a more generic representative of practical scenarios) is proposed to model actuator saturation, which ensures that the control inputs always remain confined within a predefined set to ensure safety. Based on the proposed model, we develop a nonlinear control framework that guarantees output tracking while ensuring that system output remains confined to the predefined set. Later, we integrate this design with the constrained output tracking control problem, wherein we show that the system output tracks its desired trajectory by simultaneously satisfying input and output constraints. The global stabilization of the tracking error is achieved in the presence of input constraints, while semi-global stabilization is achieved in the presence of both input and output constraints. Additionally, we rigorously establish the boundedness of all closed-loop signals under the proposed design. Simulation results demonstrate the effectiveness of the proposed methods in handling asymmetric constraints while achieving desirable tracking performance."
  },
  {
    "title": "REAL: Benchmarking Autonomous Agents on Deterministic Simulations of Real Websites",
    "url": "http://arxiv.org/abs/2504.11543v1",
    "arxiv_id": "2504.11543v1",
    "authors": [
      "Divyansh Garg",
      "Shaun VanWeelden",
      "Diego Caples",
      "Andis Draguns",
      "Nikil Ravi",
      "Pranav Putta",
      "Naman Garg",
      "Tomas Abraham",
      "Michael Lara",
      "Federico Lopez",
      "James Liu",
      "Atharva Gundawar",
      "Prannay Hebbar",
      "Youngchul Joo",
      "Charles London",
      "Christian Schroeder de Witt",
      "Sumeet Motwani"
    ],
    "published": "2025-04-15T18:22:55+00:00",
    "summary": "We introduce REAL, a benchmark and framework for multi-turn agent evaluations on deterministic simulations of real-world websites. REAL comprises high-fidelity, deterministic replicas of 11 widely-used websites across domains such as e-commerce, travel, communication, and professional networking. We also release a benchmark consisting of 112 practical tasks that mirror everyday complex user interactions requiring both accurate information retrieval and state-changing actions. All interactions occur within this fully controlled setting, eliminating safety risks and enabling robust, reproducible evaluation of agent capability and reliability. Our novel evaluation framework combines programmatic checks of website state for action-based tasks with rubric-guided LLM-based judgments for information retrieval. The framework supports both open-source and proprietary agent systems through a flexible evaluation harness that accommodates black-box commands within browser environments, allowing research labs to test agentic systems without modification. Our empirical results show that frontier language models achieve at most a 41% success rate on REAL, highlighting critical gaps in autonomous web navigation and task completion capabilities. Our framework supports easy integration of new tasks, reproducible evaluation, and scalable data generation for training web agents. The websites, framework, and leaderboard are available at https://realevals.xyz and https://github.com/agi-inc/REAL."
  },
  {
    "title": "REAL: Benchmarking Autonomous Agents on Deterministic Simulations of Real Websites",
    "url": "http://arxiv.org/abs/2504.11543v2",
    "arxiv_id": "2504.11543v2",
    "authors": [
      "Divyansh Garg",
      "Shaun VanWeelden",
      "Diego Caples",
      "Andis Draguns",
      "Nikil Ravi",
      "Pranav Putta",
      "Naman Garg",
      "Tomas Abraham",
      "Michael Lara",
      "Federico Lopez",
      "James Liu",
      "Atharva Gundawar",
      "Prannay Hebbar",
      "Youngchul Joo",
      "Jindong Gu",
      "Charles London",
      "Christian Schroeder de Witt",
      "Sumeet Motwani"
    ],
    "published": "2025-04-15T18:22:55+00:00",
    "summary": "We introduce REAL, a benchmark and framework for multi-turn agent evaluations on deterministic simulations of real-world websites. REAL comprises high-fidelity, deterministic replicas of 11 widely-used websites across domains such as e-commerce, travel, communication, and professional networking. We also release a benchmark consisting of 112 practical tasks that mirror everyday complex user interactions requiring both accurate information retrieval and state-changing actions. All interactions occur within this fully controlled setting, eliminating safety risks and enabling robust, reproducible evaluation of agent capability and reliability. Our novel evaluation framework combines programmatic checks of website state for action-based tasks with rubric-guided LLM-based judgments for information retrieval. The framework supports both open-source and proprietary agent systems through a flexible evaluation harness that accommodates black-box commands within browser environments, allowing research labs to test agentic systems without modification. Our empirical results show that frontier language models achieve at most a 41% success rate on REAL, highlighting critical gaps in autonomous web navigation and task completion capabilities. Our framework supports easy integration of new tasks, reproducible evaluation, and scalable post-training data generation, marking a significant step forward in evaluating and advancing agent capabilities."
  },
  {
    "title": "ReTool: Reinforcement Learning for Strategic Tool Use in LLMs",
    "url": "http://arxiv.org/abs/2504.11536v1",
    "arxiv_id": "2504.11536v1",
    "authors": [
      "Jiazhan Feng",
      "Shijue Huang",
      "Xingwei Qu",
      "Ge Zhang",
      "Yujia Qin",
      "Baoquan Zhong",
      "Chengquan Jiang",
      "Jinxin Chi",
      "Wanjun Zhong"
    ],
    "published": "2025-04-15T18:10:22+00:00",
    "summary": "While reasoning models (e.g., DeepSeek R1) trained with reinforcement learning (RL), excel in textual reasoning, they struggle in scenarios requiring structured problem-solving, such as geometric reasoning, concise computation, or complex equation solving-areas where computational tools like code interpreters (CI) demonstrate distinct advantages. To bridge this gap, we propose ReTool, which enhances long-form reasoning with tool-integrated learning, including two key features: (1) dynamic interleaving of real-time code execution within natural language reasoning processes, and (2) an automated RL paradigm that allows policy rollouts with multi-turn real-time code execution and teaches the model in learning when and how to invoke tools based on outcome feedback. ReTool employs a systematic training framework, beginning with synthetic cold-start data generation to produce code-augmented long-form reasoning traces for fine-tuning base models. Subsequent RL training leverages task outcomes as rewards to iteratively refine the model's tool use strategy, enabling autonomous discovery of optimal tool invocation patterns without human priors. Experiments on the challenging MATH Olympiad benchmark AIME demonstrate ReTool's superiority: Our 32B model achieves 67% accuracy with 400 training steps, outperforming text-based RL baseline (40% accuracy, 1080 steps) in efficiency and performance. Remarkably, ReTool-32B attains 72.5% accuracy in extended settings, surpassing OpenAI's o1-preview by 27.9%. Further analysis reveals emergent behaviors such as code self-correction, signaling an ''aha moment'' in which the model autonomously masters adaptive tool use. These findings highlight the promise of outcome-driven tool integration for advancing complex mathematical reasoning and offer new insights into hybrid neuro-symbolic systems."
  },
  {
    "title": "ReTool: Reinforcement Learning for Strategic Tool Use in LLMs",
    "url": "http://arxiv.org/abs/2504.11536v2",
    "arxiv_id": "2504.11536v2",
    "authors": [
      "Jiazhan Feng",
      "Shijue Huang",
      "Xingwei Qu",
      "Ge Zhang",
      "Yujia Qin",
      "Baoquan Zhong",
      "Chengquan Jiang",
      "Jinxin Chi",
      "Wanjun Zhong"
    ],
    "published": "2025-04-15T18:10:22+00:00",
    "summary": "While reasoning models (e.g., DeepSeek R1) trained with reinforcement learning (RL), excel in textual reasoning, they struggle in scenarios requiring structured problem-solving, such as geometric reasoning, concise computation, or complex equation solving-areas where computational tools like code interpreters (CI) demonstrate distinct advantages. To bridge this gap, we propose ReTool, which enhances long-form reasoning with tool-integrated learning, including two key features: (1) dynamic interleaving of real-time code execution within natural language reasoning processes, and (2) an automated RL paradigm that allows policy rollouts with multi-turn real-time code execution and teaches the model in learning when and how to invoke tools based on outcome feedback. ReTool employs a systematic training framework, beginning with synthetic cold-start data generation to produce code-augmented long-form reasoning traces for fine-tuning base models. Subsequent RL training leverages task outcomes as rewards to iteratively refine the model's tool use strategy, enabling autonomous discovery of optimal tool invocation patterns without human priors. Experiments on the challenging MATH Olympiad benchmark AIME demonstrate ReTool's superiority: Our 32B model achieves 67% accuracy with 400 training steps, outperforming text-based RL baseline (40% accuracy, 1080 steps) in efficiency and performance. Remarkably, ReTool-32B attains 72.5% accuracy in extended settings, surpassing OpenAI's o1-preview by 27.9%. Further analysis reveals emergent behaviors such as code self-correction, signaling an ''aha moment'' in which the model autonomously masters adaptive tool use. These findings highlight the promise of outcome-driven tool integration for advancing complex mathematical reasoning and offer new insights into hybrid neuro-symbolic systems."
  },
  {
    "title": "LANGTRAJ: Diffusion Model and Dataset for Language-Conditioned Trajectory Simulation",
    "url": "http://arxiv.org/abs/2504.11521v1",
    "arxiv_id": "2504.11521v1",
    "authors": [
      "Wei-Jer Chang",
      "Wei Zhan",
      "Masayoshi Tomizuka",
      "Manmohan Chandraker",
      "Francesco Pittaluga"
    ],
    "published": "2025-04-15T17:14:06+00:00",
    "summary": "Evaluating autonomous vehicles with controllability enables scalable testing in counterfactual or structured settings, enhancing both efficiency and safety. We introduce LangTraj, a language-conditioned scene-diffusion model that simulates the joint behavior of all agents in traffic scenarios. By conditioning on natural language inputs, LangTraj provides flexible and intuitive control over interactive behaviors, generating nuanced and realistic scenarios. Unlike prior approaches that depend on domain-specific guidance functions, LangTraj incorporates language conditioning during training, facilitating more intuitive traffic simulation control. We propose a novel closed-loop training strategy for diffusion models, explicitly tailored to enhance stability and realism during closed-loop simulation. To support language-conditioned simulation, we develop Inter-Drive, a large-scale dataset with diverse and interactive labels for training language-conditioned diffusion models. Our dataset is built upon a scalable pipeline for annotating agent-agent interactions and single-agent behaviors, ensuring rich and varied supervision. Validated on the Waymo Motion Dataset, LangTraj demonstrates strong performance in realism, language controllability, and language-conditioned safety-critical simulation, establishing a new paradigm for flexible and scalable autonomous vehicle testing."
  },
  {
    "title": "A Minimalist Approach to LLM Reasoning: from Rejection Sampling to Reinforce",
    "url": "http://arxiv.org/abs/2504.11343v1",
    "arxiv_id": "2504.11343v1",
    "authors": [
      "Wei Xiong",
      "Jiarui Yao",
      "Yuhui Xu",
      "Bo Pang",
      "Lei Wang",
      "Doyen Sahoo",
      "Junnan Li",
      "Nan Jiang",
      "Tong Zhang",
      "Caiming Xiong",
      "Hanze Dong"
    ],
    "published": "2025-04-15T16:15:02+00:00",
    "summary": "Reinforcement learning (RL) has become a prevailing approach for fine-tuning large language models (LLMs) on complex reasoning tasks. Among recent methods, GRPO stands out for its empirical success in training models such as DeepSeek-R1, yet the sources of its effectiveness remain poorly understood. In this work, we revisit GRPO from a reinforce-like algorithm perspective and analyze its core components. Surprisingly, we find that a simple rejection sampling baseline, RAFT, which trains only on positively rewarded samples, yields competitive performance than GRPO and PPO. Our ablation studies reveal that GRPO's main advantage arises from discarding prompts with entirely incorrect responses, rather than from its reward normalization. Motivated by this insight, we propose Reinforce-Rej, a minimal extension of policy gradient that filters both entirely incorrect and entirely correct samples. Reinforce-Rej improves KL efficiency and stability, serving as a lightweight yet effective alternative to more complex RL algorithms. We advocate RAFT as a robust and interpretable baseline, and suggest that future advances should focus on more principled designs for incorporating negative samples, rather than relying on them indiscriminately. Our findings provide guidance for future work in reward-based LLM post-training."
  },
  {
    "title": "Robust MPC for Uncertain Linear Systems -- Combining Model Adaptation and Iterative Learning",
    "url": "http://arxiv.org/abs/2504.11261v1",
    "arxiv_id": "2504.11261v1",
    "authors": [
      "Hannes Petrenz",
      "Johannes K\u00f6hler",
      "Francesco Borrelli"
    ],
    "published": "2025-04-15T15:00:34+00:00",
    "summary": "This paper presents a robust adaptive learning Model Predictive Control (MPC) framework for linear systems with parametric uncertainties and additive disturbances performing iterative tasks. The approach iteratively refines the parameter estimates using set membership estimation. Performance enhancement over iterations is achieved by learning the terminal cost from data. Safety is enforced using a terminal set, which is also learned iteratively. The proposed method guarantees recursive feasibility, constraint satisfaction, and a robust bound on the closed-loop cost. Numerical simulations on a mass-spring-damper system demonstrate improved computational efficiency and control performance compared to an existing robust adaptive MPC approach."
  },
  {
    "title": "Robust MPC for Uncertain Linear Systems -- Combining Model Adaptation and Iterative Learning",
    "url": "http://arxiv.org/abs/2504.11261v2",
    "arxiv_id": "2504.11261v2",
    "authors": [
      "Hannes Petrenz",
      "Johannes K\u00f6hler",
      "Francesco Borrelli"
    ],
    "published": "2025-04-15T15:00:34+00:00",
    "summary": "This paper presents a robust adaptive learning Model Predictive Control (MPC) framework for linear systems with parametric uncertainties and additive disturbances performing iterative tasks. The approach iteratively refines the parameter estimates using set membership estimation. Performance enhancement over iterations is achieved by learning the terminal cost from data. Safety is enforced using a terminal set, which is also learned iteratively. The proposed method guarantees recursive feasibility, constraint satisfaction, and a robust bound on the closed-loop cost. Numerical simulations on a mass-spring-damper system demonstrate improved computational efficiency and control performance compared to an existing robust adaptive MPC approach."
  },
  {
    "title": "Towards Automated Safety Requirements Derivation Using Agent-based RAG",
    "url": "http://arxiv.org/abs/2504.11243v1",
    "arxiv_id": "2504.11243v1",
    "authors": [
      "Balahari Vignesh Balu",
      "Florian Geissler",
      "Francesco Carella",
      "Joao-Vitor Zacchi",
      "Josef Jiru",
      "Nuria Mata",
      "Reinhard Stolle"
    ],
    "published": "2025-04-15T14:43:19+00:00",
    "summary": "We study the automated derivation of safety requirements in a self-driving vehicle use case, leveraging LLMs in combination with agent-based retrieval-augmented generation. Conventional approaches that utilise pre-trained LLMs to assist in safety analyses typically lack domain-specific knowledge. Existing RAG approaches address this issue, yet their performance deteriorates when handling complex queries and it becomes increasingly harder to retrieve the most relevant information. This is particularly relevant for safety-relevant applications. In this paper, we propose the use of agent-based RAG to derive safety requirements and show that the retrieved information is more relevant to the queries. We implement an agent-based approach on a document pool of automotive standards and the Apollo case study, as a representative example of an automated driving perception system. Our solution is tested on a data set of safety requirement questions and answers, extracted from the Apollo data. Evaluating a set of selected RAG metrics, we present and discuss advantages of a agent-based approach compared to default RAG methods."
  },
  {
    "title": "Nondeterministic Polynomial-time Problem Challenge: An Ever-Scaling Reasoning Benchmark for LLMs",
    "url": "http://arxiv.org/abs/2504.11239v1",
    "arxiv_id": "2504.11239v1",
    "authors": [
      "Chang Yang",
      "Ruiyu Wang",
      "Junzhe Jiang",
      "Qi Jiang",
      "Qinggang Zhang",
      "Yanchen Deng",
      "Shuxin Li",
      "Shuyue Hu",
      "Bo Li",
      "Florian T. Pokorny",
      "Xiao Huang",
      "Xinrun Wang"
    ],
    "published": "2025-04-15T14:40:29+00:00",
    "summary": "Reasoning is the fundamental capability of large language models (LLMs). Due to the rapid progress of LLMs, there are two main issues of current benchmarks: i) these benchmarks can be crushed in a short time (less than 1 year), and ii) these benchmarks may be easily hacked. To handle these issues, we propose the ever-scalingness for building the benchmarks which are uncrushable, unhackable, auto-verifiable and general. This paper presents Nondeterministic Polynomial-time Problem Challenge (NPPC), an ever-scaling reasoning benchmark for LLMs. Specifically, the NPPC has three main modules: i) npgym, which provides a unified interface of 25 well-known NP-complete problems and can generate any number of instances with any levels of complexities, ii) npsolver: which provides a unified interface to evaluate the problem instances with both online and offline models via APIs and local deployments, respectively, and iii) npeval: which provides the comprehensive and ready-to-use tools to analyze the performances of LLMs over different problems, the number of tokens, the aha moments, the reasoning errors and the solution errors. Extensive experiments over widely-used LLMs demonstrate: i) NPPC can successfully decrease the performances of advanced LLMs' performances to below 10%, demonstrating that NPPC is uncrushable, ii) DeepSeek-R1, Claude-3.7-Sonnet, and o1/o3-mini are the most powerful LLMs, where DeepSeek-R1 outperforms Claude-3.7-Sonnet and o1/o3-mini in most NP-complete problems considered, and iii) the numbers of tokens, aha moments in the advanced LLMs, e.g., Claude-3.7-Sonnet and DeepSeek-R1, are observed first to increase and then decrease when the problem instances become more and more difficult. We believe that NPPC is the first ever-scaling reasoning benchmark, serving as the uncrushable and unhackable testbed for LLMs toward artificial general intelligence (AGI)."
  },
  {
    "title": "Enhancing Autonomous Driving Systems with On-Board Deployed Large Language Models",
    "url": "http://arxiv.org/abs/2504.11514v1",
    "arxiv_id": "2504.11514v1",
    "authors": [
      "Nicolas Baumann",
      "Cheng Hu",
      "Paviththiren Sivasothilingam",
      "Haotong Qin",
      "Lei Xie",
      "Michele Magno",
      "Luca Benini"
    ],
    "published": "2025-04-15T13:49:17+00:00",
    "summary": "Neural Networks (NNs) trained through supervised learning struggle with managing edge-case scenarios common in real-world driving due to the intractability of exhaustive datasets covering all edge-cases, making knowledge-driven approaches, akin to how humans intuitively detect unexpected driving behavior, a suitable complement to data-driven methods. This work proposes a hybrid architecture combining low-level Model Predictive Controller (MPC) with locally deployed Large Language Models (LLMs) to enhance decision-making and Human Machine Interaction (HMI). The DecisionxLLM module evaluates robotic state information against natural language instructions to ensure adherence to desired driving behavior. The MPCxLLM module then adjusts MPC parameters based on LLM-generated insights, achieving control adaptability while preserving the safety and constraint guarantees of traditional MPC systems. Further, to enable efficient on-board deployment and to eliminate dependency on cloud connectivity, we shift processing to the on-board computing platform: We propose an approach that exploits Retrieval Augmented Generation (RAG), Low Rank Adaptation (LoRA) fine-tuning, and quantization. Experimental results demonstrate that these enhancements yield significant improvements in reasoning accuracy by up to 10.45%, control adaptability by as much as 52.2%, and up to 10.5x increase in computational efficiency (tokens/s), validating the proposed framework's practicality for real-time deployment even on down-scaled robotic platforms. This work bridges high-level decision-making with low-level control adaptability, offering a synergistic framework for knowledge-driven and adaptive Autonomous Driving Systems (ADS)."
  },
  {
    "title": "Benchmarking Next-Generation Reasoning-Focused Large Language Models in Ophthalmology: A Head-to-Head Evaluation on 5,888 Items",
    "url": "http://arxiv.org/abs/2504.11186v1",
    "arxiv_id": "2504.11186v1",
    "authors": [
      "Minjie Zou",
      "Sahana Srinivasan",
      "Thaddaeus Wai Soon Lo",
      "Ke Zou",
      "Gabriel Dawei Yang",
      "Xuguang Ai",
      "Hyunjae Kim",
      "Maxwell Singer",
      "Fares Antaki",
      "Kelvin Li",
      "Robert Chang",
      "Marcus Tan",
      "David Ziyou Chen",
      "Dianbo Liu",
      "Qingyu Chen",
      "Yih Chung Tham"
    ],
    "published": "2025-04-15T13:42:34+00:00",
    "summary": "Recent advances in reasoning-focused large language models (LLMs) mark a shift from general LLMs toward models designed for complex decision-making, a crucial aspect in medicine. However, their performance in specialized domains like ophthalmology remains underexplored. This study comprehensively evaluated and compared the accuracy and reasoning capabilities of four newly developed reasoning-focused LLMs, namely DeepSeek-R1, OpenAI o1, o3-mini, and Gemini 2.0 Flash-Thinking. Each model was assessed using 5,888 multiple-choice ophthalmology exam questions from the MedMCQA dataset in zero-shot setting. Quantitative evaluation included accuracy, Macro-F1, and five text-generation metrics (ROUGE-L, METEOR, BERTScore, BARTScore, and AlignScore), computed against ground-truth reasonings. Average inference time was recorded for a subset of 100 randomly selected questions. Additionally, two board-certified ophthalmologists qualitatively assessed clarity, completeness, and reasoning structure of responses to differential diagnosis questions.O1 (0.902) and DeepSeek-R1 (0.888) achieved the highest accuracy, with o1 also leading in Macro-F1 (0.900). The performance of models across the text-generation metrics varied: O3-mini excelled in ROUGE-L (0.151), o1 in METEOR (0.232), DeepSeek-R1 and o3-mini tied for BERTScore (0.673), DeepSeek-R1 (-4.105) and Gemini 2.0 Flash-Thinking (-4.127) performed best in BARTScore, while o3-mini (0.181) and o1 (0.176) led AlignScore. Inference time across the models varied, with DeepSeek-R1 being slowest (40.4 seconds) and Gemini 2.0 Flash-Thinking fastest (6.7 seconds). Qualitative evaluation revealed that DeepSeek-R1 and Gemini 2.0 Flash-Thinking tended to provide detailed and comprehensive intermediate reasoning, whereas o1 and o3-mini displayed concise and summarized justifications."
  },
  {
    "title": "Exploring Backdoor Attack and Defense for LLM-empowered Recommendations",
    "url": "http://arxiv.org/abs/2504.11182v1",
    "arxiv_id": "2504.11182v1",
    "authors": [
      "Liangbo Ning",
      "Wenqi Fan",
      "Qing Li"
    ],
    "published": "2025-04-15T13:37:38+00:00",
    "summary": "The fusion of Large Language Models (LLMs) with recommender systems (RecSys) has dramatically advanced personalized recommendations and drawn extensive attention. Despite the impressive progress, the safety of LLM-based RecSys against backdoor attacks remains largely under-explored. In this paper, we raise a new problem: Can a backdoor with a specific trigger be injected into LLM-based Recsys, leading to the manipulation of the recommendation responses when the backdoor trigger is appended to an item's title? To investigate the vulnerabilities of LLM-based RecSys under backdoor attacks, we propose a new attack framework termed Backdoor Injection Poisoning for RecSys (BadRec). BadRec perturbs the items' titles with triggers and employs several fake users to interact with these items, effectively poisoning the training set and injecting backdoors into LLM-based RecSys. Comprehensive experiments reveal that poisoning just 1% of the training data with adversarial examples is sufficient to successfully implant backdoors, enabling manipulation of recommendations. To further mitigate such a security threat, we propose a universal defense strategy called Poison Scanner (P-Scanner). Specifically, we introduce an LLM-based poison scanner to detect the poisoned items by leveraging the powerful language understanding and rich knowledge of LLMs. A trigger augmentation agent is employed to generate diverse synthetic triggers to guide the poison scanner in learning domain-specific knowledge of the poisoned item detection task. Extensive experiments on three real-world datasets validate the effectiveness of the proposed P-Scanner."
  },
  {
    "title": "A Real-time Anomaly Detection Method for Robots based on a Flexible and Sparse Latent Space",
    "url": "http://arxiv.org/abs/2504.11170v1",
    "arxiv_id": "2504.11170v1",
    "authors": [
      "Taewook Kang",
      "Bum-Jae You",
      "Juyoun Park",
      "Yisoo Lee"
    ],
    "published": "2025-04-15T13:17:14+00:00",
    "summary": "The growing demand for robots to operate effectively in diverse environments necessitates the need for robust real-time anomaly detection techniques during robotic operations. However, deep learning-based models in robotics face significant challenges due to limited training data and highly noisy signal features. In this paper, we present Sparse Masked Autoregressive Flow-based Adversarial AutoEncoders model to address these problems. This approach integrates Masked Autoregressive Flow model into Adversarial AutoEncoders to construct a flexible latent space and utilize Sparse autoencoder to efficiently focus on important features, even in scenarios with limited feature space. Our experiments demonstrate that the proposed model achieves a 4.96% to 9.75% higher area under the receiver operating characteristic curve for pick-and-place robotic operations with randomly placed cans, compared to existing state-of-the-art methods. Notably, it showed up to 19.67% better performance in scenarios involving collisions with lightweight objects. Additionally, unlike the existing state-of-the-art model, our model performs inferences within 1 millisecond, ensuring real-time anomaly detection. These capabilities make our model highly applicable to machine learning-based robotic safety systems in dynamic environments. The code will be made publicly available after acceptance."
  },
  {
    "title": "A Real-time Anomaly Detection Method for Robots based on a Flexible and Sparse Latent Space",
    "url": "http://arxiv.org/abs/2504.11170v2",
    "arxiv_id": "2504.11170v2",
    "authors": [
      "Taewook Kang",
      "Bum-Jae You",
      "Juyoun Park",
      "Yisoo Lee"
    ],
    "published": "2025-04-15T13:17:14+00:00",
    "summary": "The growing demand for robots to operate effectively in diverse environments necessitates the need for robust real-time anomaly detection techniques during robotic operations. However, deep learning-based models in robotics face significant challenges due to limited training data and highly noisy signal features. In this paper, we present Sparse Masked Autoregressive Flow-based Adversarial AutoEncoders model to address these problems. This approach integrates Masked Autoregressive Flow model into Adversarial AutoEncoders to construct a flexible latent space and utilize Sparse autoencoder to efficiently focus on important features, even in scenarios with limited feature space. Our experiments demonstrate that the proposed model achieves a 4.96% to 9.75% higher area under the receiver operating characteristic curve for pick-and-place robotic operations with randomly placed cans, compared to existing state-of-the-art methods. Notably, it showed up to 19.67% better performance in scenarios involving collisions with lightweight objects. Additionally, unlike the existing state-of-the-art model, our model performs inferences within 1 millisecond, ensuring real-time anomaly detection. These capabilities make our model highly applicable to machine learning-based robotic safety systems in dynamic environments. The code will be made publicly available after acceptance."
  },
  {
    "title": "Bypassing Prompt Injection and Jailbreak Detection in LLM Guardrails",
    "url": "http://arxiv.org/abs/2504.11168v1",
    "arxiv_id": "2504.11168v1",
    "authors": [
      "William Hackett",
      "Lewis Birch",
      "Stefan Trawicki",
      "Neeraj Suri",
      "Peter Garraghan"
    ],
    "published": "2025-04-15T13:16:02+00:00",
    "summary": "Large Language Models (LLMs) guardrail systems are designed to protect against prompt injection and jailbreak attacks. However, they remain vulnerable to evasion techniques. We demonstrate two approaches for bypassing LLM prompt injection and jailbreak detection systems via traditional character injection methods and algorithmic Adversarial Machine Learning (AML) evasion techniques. Through testing against six prominent protection systems, including Microsoft's Azure Prompt Shield and Meta's Prompt Guard, we show that both methods can be used to evade detection while maintaining adversarial utility achieving in some instances up to 100% evasion success. Furthermore, we demonstrate that adversaries can enhance Attack Success Rates (ASR) against black-box targets by leveraging word importance ranking computed by offline white-box models. Our findings reveal vulnerabilities within current LLM protection mechanisms and highlight the need for more robust guardrail systems."
  },
  {
    "title": "Bypassing Prompt Injection and Jailbreak Detection in LLM Guardrails",
    "url": "http://arxiv.org/abs/2504.11168v2",
    "arxiv_id": "2504.11168v2",
    "authors": [
      "William Hackett",
      "Lewis Birch",
      "Stefan Trawicki",
      "Neeraj Suri",
      "Peter Garraghan"
    ],
    "published": "2025-04-15T13:16:02+00:00",
    "summary": "Large Language Models (LLMs) guardrail systems are designed to protect against prompt injection and jailbreak attacks. However, they remain vulnerable to evasion techniques. We demonstrate two approaches for bypassing LLM prompt injection and jailbreak detection systems via traditional character injection methods and algorithmic Adversarial Machine Learning (AML) evasion techniques. Through testing against six prominent protection systems, including Microsoft's Azure Prompt Shield and Meta's Prompt Guard, we show that both methods can be used to evade detection while maintaining adversarial utility achieving in some instances up to 100% evasion success. Furthermore, we demonstrate that adversaries can enhance Attack Success Rates (ASR) against black-box targets by leveraging word importance ranking computed by offline white-box models. Our findings reveal vulnerabilities within current LLM protection mechanisms and highlight the need for more robust guardrail systems."
  },
  {
    "title": "Token-Level Constraint Boundary Search for Jailbreaking Text-to-Image Models",
    "url": "http://arxiv.org/abs/2504.11106v1",
    "arxiv_id": "2504.11106v1",
    "authors": [
      "Jiangtao Liu",
      "Zhaoxin Wang",
      "Handing Wang",
      "Cong Tian",
      "Yaochu Jin"
    ],
    "published": "2025-04-15T11:53:40+00:00",
    "summary": "Recent advancements in Text-to-Image (T2I) generation have significantly enhanced the realism and creativity of generated images. However, such powerful generative capabilities pose risks related to the production of inappropriate or harmful content. Existing defense mechanisms, including prompt checkers and post-hoc image checkers, are vulnerable to sophisticated adversarial attacks. In this work, we propose TCBS-Attack, a novel query-based black-box jailbreak attack that searches for tokens located near the decision boundaries defined by text and image checkers. By iteratively optimizing tokens near these boundaries, TCBS-Attack generates semantically coherent adversarial prompts capable of bypassing multiple defensive layers in T2I models. Extensive experiments demonstrate that our method consistently outperforms state-of-the-art jailbreak attacks across various T2I models, including securely trained open-source models and commercial online services like DALL-E 3. TCBS-Attack achieves an ASR-4 of 45\\% and an ASR-1 of 21\\% on jailbreaking full-chain T2I models, significantly surpassing baseline methods."
  },
  {
    "title": "Neural Control Barrier Functions from Physics Informed Neural Networks",
    "url": "http://arxiv.org/abs/2504.11045v1",
    "arxiv_id": "2504.11045v1",
    "authors": [
      "Shreenabh Agrawal",
      "Manan Tayal",
      "Aditya Singh",
      "Shishir Kolathaya"
    ],
    "published": "2025-04-15T10:13:30+00:00",
    "summary": "As autonomous systems become increasingly prevalent in daily life, ensuring their safety is paramount. Control Barrier Functions (CBFs) have emerged as an effective tool for guaranteeing safety; however, manually designing them for specific applications remains a significant challenge. With the advent of deep learning techniques, recent research has explored synthesizing CBFs using neural networks-commonly referred to as neural CBFs. This paper introduces a novel class of neural CBFs that leverages a physics-inspired neural network framework by incorporating Zubov's Partial Differential Equation (PDE) within the context of safety. This approach provides a scalable methodology for synthesizing neural CBFs applicable to high-dimensional systems. Furthermore, by utilizing reciprocal CBFs instead of zeroing CBFs, the proposed framework allows for the specification of flexible, user-defined safe regions. To validate the effectiveness of the approach, we present case studies on three different systems: an inverted pendulum, autonomous ground navigation, and aerial navigation in obstacle-laden environments."
  },
  {
    "title": "DRIFT open dataset: A drone-derived intelligence for traffic analysis in urban environmen",
    "url": "http://arxiv.org/abs/2504.11019v1",
    "arxiv_id": "2504.11019v1",
    "authors": [
      "Hyejin Lee",
      "Seokjun Hong",
      "Jeonghoon Song",
      "Haechan Cho",
      "Zhixiong Jin",
      "Byeonghun Kim",
      "Joobin Jin",
      "Jaegyun Im",
      "Byeongjoon Noh",
      "Hwasoo Yeo"
    ],
    "published": "2025-04-15T09:43:13+00:00",
    "summary": "Reliable traffic data are essential for understanding urban mobility and developing effective traffic management strategies. This study introduces the DRone-derived Intelligence For Traffic analysis (DRIFT) dataset, a large-scale urban traffic dataset collected systematically from synchronized drone videos at approximately 250 meters altitude, covering nine interconnected intersections in Daejeon, South Korea. DRIFT provides high-resolution vehicle trajectories that include directional information, processed through video synchronization and orthomap alignment, resulting in a comprehensive dataset of 81,699 vehicle trajectories. Through our DRIFT dataset, researchers can simultaneously analyze traffic at multiple scales - from individual vehicle maneuvers like lane-changes and safety metrics such as time-to-collision to aggregate network flow dynamics across interconnected urban intersections. The DRIFT dataset is structured to enable immediate use without additional preprocessing, complemented by open-source models for object detection and trajectory extraction, as well as associated analytical tools. DRIFT is expected to significantly contribute to academic research and practical applications, such as traffic flow analysis and simulation studies. The dataset and related resources are publicly accessible at https://github.com/AIxMobility/The-DRIFT."
  },
  {
    "title": "Reward Distance Comparisons Under Transition Sparsity",
    "url": "http://arxiv.org/abs/2504.11508v1",
    "arxiv_id": "2504.11508v1",
    "authors": [
      "Clement Nyanhongo",
      "Bruno Miranda Henrique",
      "Eugene Santos"
    ],
    "published": "2025-04-15T09:27:53+00:00",
    "summary": "Reward comparisons are vital for evaluating differences in agent behaviors induced by a set of reward functions. Most conventional techniques utilize the input reward functions to learn optimized policies, which are then used to compare agent behaviors. However, learning these policies can be computationally expensive and can also raise safety concerns. Direct reward comparison techniques obviate policy learning but suffer from transition sparsity, where only a small subset of transitions are sampled due to data collection challenges and feasibility constraints. Existing state-of-the-art direct reward comparison methods are ill-suited for these sparse conditions since they require high transition coverage, where the majority of transitions from a given coverage distribution are sampled. When this requirement is not satisfied, a distribution mismatch between sampled and expected transitions can occur, leading to significant errors. This paper introduces the Sparsity Resilient Reward Distance (SRRD) pseudometric, designed to eliminate the need for high transition coverage by accommodating diverse sample distributions, which are common under transition sparsity. We provide theoretical justification for SRRD's robustness and conduct experiments to demonstrate its practical efficacy across multiple domains."
  },
  {
    "title": "Drivers and barriers of adopting shared micromobility: a latent class clustering model on the attitudes towards shared micromobility as part of public transport trips in the Netherlands",
    "url": "http://arxiv.org/abs/2504.10943v1",
    "arxiv_id": "2504.10943v1",
    "authors": [
      "Nejc Ger\u017eini\u010d",
      "Mark van Hagen",
      "Hussein Al-Tamimi",
      "Niels van Oort",
      "Dorine Duives"
    ],
    "published": "2025-04-15T07:46:19+00:00",
    "summary": "Shared micromobility (SMM) is often cited as a solution to the first/last mile problem of public transport (train) travel, yet when implemented, they often do not get adopted by a broader travelling public. A large part of behavioural adoption is related to peoples' attitudes and perceptions. In this paper, we develop an adjusted behavioural framework, based on the UTAUT2 technology acceptance framework. We carry out an exploratory factor analysis (EFA) to obtain attitudinal factors which we then use to perform a latent class cluster analysis (LCCA), with the goal of studying the potential adoption of SMM and to assess the various drivers and barriers as perceived by different user groups. Our findings suggest there are six distinct user groups with varying intention to use shared micromobility: Progressives, Conservatives, Hesitant participants, Bold innovators, Anxious observers and Skilled sceptics. Bold innovators and Progressives tend to be the most open to adopting SMM and are also able to do so. Hesitant participants would like to, but find it difficult or dangerous to use, while Skilled sceptics are capable and confident, but have limited intention of using it. Conservatives and Anxious observers are most negative about SMM, finding it difficult to use and dangerous. In general, factors relating to technological savviness, ease-of-use, physical safety and societal perception seem to be the biggest barriers to wider adoption. Younger, highly educated males are the group most likely and open to using shared micromobility, while older individuals with lower incomes and a lower level of education tend to be the least likely."
  },
  {
    "title": "Safe-Construct: Redefining Construction Safety Violation Recognition as 3D Multi-View Engagement Task",
    "url": "http://arxiv.org/abs/2504.10880v1",
    "arxiv_id": "2504.10880v1",
    "authors": [
      "Aviral Chharia",
      "Tianyu Ren",
      "Tomotake Furuhata",
      "Kenji Shimada"
    ],
    "published": "2025-04-15T05:21:09+00:00",
    "summary": "Recognizing safety violations in construction environments is critical yet remains underexplored in computer vision. Existing models predominantly rely on 2D object detection, which fails to capture the complexities of real-world violations due to: (i) an oversimplified task formulation treating violation recognition merely as object detection, (ii) inadequate validation under realistic conditions, (iii) absence of standardized baselines, and (iv) limited scalability from the unavailability of synthetic dataset generators for diverse construction scenarios. To address these challenges, we introduce Safe-Construct, the first framework that reformulates violation recognition as a 3D multi-view engagement task, leveraging scene-level worker-object context and 3D spatial understanding. We also propose the Synthetic Indoor Construction Site Generator (SICSG) to create diverse, scalable training data, overcoming data limitations. Safe-Construct achieves a 7.6% improvement over state-of-the-art methods across four violation types. We rigorously evaluate our approach in near-realistic settings, incorporating four violations, four workers, 14 objects, and challenging conditions like occlusions (worker-object, worker-worker) and variable illumination (back-lighting, overexposure, sunlight). By integrating 3D multi-view spatial understanding and synthetic data generation, Safe-Construct sets a new benchmark for scalable and robust safety monitoring in high-risk industries. Project Website: https://Safe-Construct.github.io/Safe-Construct"
  },
  {
    "title": "Hallucination-Aware Generative Pretrained Transformer for Cooperative Aerial Mobility Control",
    "url": "http://arxiv.org/abs/2504.10831v1",
    "arxiv_id": "2504.10831v1",
    "authors": [
      "Hyojun Ahn",
      "Seungcheol Oh",
      "Gyu Seon Kim",
      "Soyi Jung",
      "Soohyun Park",
      "Joongheon Kim"
    ],
    "published": "2025-04-15T03:21:08+00:00",
    "summary": "This paper proposes SafeGPT, a two-tiered framework that integrates generative pretrained transformers (GPTs) with reinforcement learning (RL) for efficient and reliable unmanned aerial vehicle (UAV) last-mile deliveries. In the proposed design, a Global GPT module assigns high-level tasks such as sector allocation, while an On-Device GPT manages real-time local route planning. An RL-based safety filter monitors each GPT decision and overrides unsafe actions that could lead to battery depletion or duplicate visits, effectively mitigating hallucinations. Furthermore, a dual replay buffer mechanism helps both the GPT modules and the RL agent refine their strategies over time. Simulation results demonstrate that SafeGPT achieves higher delivery success rates compared to a GPT-only baseline, while substantially reducing battery consumption and travel distance. These findings validate the efficacy of combining GPT-based semantic reasoning with formal safety guarantees, contributing a viable solution for robust and energy-efficient UAV logistics."
  },
  {
    "title": "LayoutCoT: Unleashing the Deep Reasoning Potential of Large Language Models for Layout Generation",
    "url": "http://arxiv.org/abs/2504.10829v1",
    "arxiv_id": "2504.10829v1",
    "authors": [
      "Hengyu Shi",
      "Junhao Su",
      "Huansheng Ning",
      "Xiaoming Wei",
      "Jialin Gao"
    ],
    "published": "2025-04-15T03:12:01+00:00",
    "summary": "Conditional layout generation aims to automatically generate visually appealing and semantically coherent layouts from user-defined constraints. While recent methods based on generative models have shown promising results, they typically require substantial amounts of training data or extensive fine-tuning, limiting their versatility and practical applicability. Alternatively, some training-free approaches leveraging in-context learning with Large Language Models (LLMs) have emerged, but they often suffer from limited reasoning capabilities and overly simplistic ranking mechanisms, which restrict their ability to generate consistently high-quality layouts. To this end, we propose LayoutCoT, a novel approach that leverages the reasoning capabilities of LLMs through a combination of Retrieval-Augmented Generation (RAG) and Chain-of-Thought (CoT) techniques. Specifically, LayoutCoT transforms layout representations into a standardized serialized format suitable for processing by LLMs. A Layout-aware RAG is used to facilitate effective retrieval and generate a coarse layout by LLMs. This preliminary layout, together with the selected exemplars, is then fed into a specially designed CoT reasoning module for iterative refinement, significantly enhancing both semantic coherence and visual quality. We conduct extensive experiments on five public datasets spanning three conditional layout generation tasks. Experimental results demonstrate that LayoutCoT achieves state-of-the-art performance without requiring training or fine-tuning. Notably, our CoT reasoning module enables standard LLMs, even those without explicit deep reasoning abilities, to outperform specialized deep-reasoning models such as deepseek-R1, highlighting the potential of our approach in unleashing the deep reasoning capabilities of LLMs for layout generation tasks."
  },
  {
    "title": "A Framework for the Private Governance of Frontier Artificial Intelligence",
    "url": "http://arxiv.org/abs/2504.11501v1",
    "arxiv_id": "2504.11501v1",
    "authors": [
      "Dean W. Ball"
    ],
    "published": "2025-04-15T02:56:26+00:00",
    "summary": "This paper presents a proposal for the governance of frontier AI systems through a hybrid public-private system. Private bodies, authorized and overseen by government, provide certifications to developers of frontier AI systems on an opt-in basis. In exchange for opting in, frontier AI firms receive protections from tort liability for customer misuse of their models. Before detailing the proposal, the paper explores more commonly discussed approaches to AI governance, analyzing their strengths and flaws. It also examines the nature of frontier AI governance itself. The paper includes consideration of the political economic, institutional, legal, safety, and other merits and tradeoffs inherent in the governance system it proposes."
  },
  {
    "title": "The Sword of Damocles in ViTs: Computational Redundancy Amplifies Adversarial Transferability",
    "url": "http://arxiv.org/abs/2504.10804v1",
    "arxiv_id": "2504.10804v1",
    "authors": [
      "Jiani Liu",
      "Zhiyuan Wang",
      "Zeliang Zhang",
      "Chao Huang",
      "Susan Liang",
      "Yunlong Tang",
      "Chenliang Xu"
    ],
    "published": "2025-04-15T01:59:47+00:00",
    "summary": "Vision Transformers (ViTs) have demonstrated impressive performance across a range of applications, including many safety-critical tasks. However, their unique architectural properties raise new challenges and opportunities in adversarial robustness. In particular, we observe that adversarial examples crafted on ViTs exhibit higher transferability compared to those crafted on CNNs, suggesting that ViTs contain structural characteristics favorable for transferable attacks. In this work, we investigate the role of computational redundancy in ViTs and its impact on adversarial transferability. Unlike prior studies that aim to reduce computation for efficiency, we propose to exploit this redundancy to improve the quality and transferability of adversarial examples. Through a detailed analysis, we identify two forms of redundancy, including the data-level and model-level, that can be harnessed to amplify attack effectiveness. Building on this insight, we design a suite of techniques, including attention sparsity manipulation, attention head permutation, clean token regularization, ghost MoE diversification, and test-time adversarial training. Extensive experiments on the ImageNet-1k dataset validate the effectiveness of our approach, showing that our methods significantly outperform existing baselines in both transferability and generality across diverse model architectures."
  },
  {
    "title": "Products of Recursive Programs for Hypersafety Verification",
    "url": "http://arxiv.org/abs/2504.10800v1",
    "arxiv_id": "2504.10800v1",
    "authors": [
      "Ruotong Cheng",
      "Azadeh Farzan"
    ],
    "published": "2025-04-15T01:52:50+00:00",
    "summary": "We study the problem of automated hypersafety verification of infinite-state recursive programs. We propose an infinite class of product programs, specifically designed with recursion in mind, that reduce the hypersafety verification of a recursive program to standard safety verification. For this, we combine insights from language theory and concurrency theory to propose an algorithmic solution for constructing an infinite class of recursive product programs. One key insight is that, using the simple theory of visibly pushdown languages, one can maintain the recursive structure of syntactic program alignments which is vital to constructing a new product program that can be viewed as a classic recursive program -- that is, one that can be executed on a single stack. Another key insight is that techniques from concurrency theory can be generalized to help define product programs based on the view that the parallel composition of individual recursive programs includes all possible alignments from which a sound set of alignments that faithfully preserve the satisfaction of the hypersafety property can be selected. On the practical side, we formulate a family of parametric canonical product constructions that are intuitive to programmers and can be used as building blocks to specify recursive product programs for the purpose of relational and hypersafety verification, with the idea that the right product program can be verified automatically using existing techniques. We demonstrate the effectiveness of these techniques through an implementation and highly promising experimental results."
  },
  {
    "title": "ReasonDrive: Efficient Visual Question Answering for Autonomous Vehicles with Reasoning-Enhanced Small Vision-Language Models",
    "url": "http://arxiv.org/abs/2504.10757v1",
    "arxiv_id": "2504.10757v1",
    "authors": [
      "Amirhosein Chahe",
      "Lifeng Zhou"
    ],
    "published": "2025-04-14T23:16:07+00:00",
    "summary": "Vision-language models (VLMs) show promise for autonomous driving but often lack transparent reasoning capabilities that are critical for safety. We investigate whether explicitly modeling reasoning during fine-tuning enhances VLM performance on driving decision tasks. Using GPT-4o, we generate structured reasoning chains for driving scenarios from the DriveLM benchmark with category-specific prompting strategies. We compare reasoning-based fine-tuning, answer-only fine-tuning, and baseline instruction-tuned models across multiple small VLM families (Llama 3.2, Llava 1.5, and Qwen 2.5VL). Our results demonstrate that reasoning-based fine-tuning consistently outperforms alternatives, with Llama3.2-11B-reason achieving the highest performance. Models fine-tuned with reasoning show substantial improvements in accuracy and text generation quality, suggesting explicit reasoning enhances internal representations for driving decisions. These findings highlight the importance of transparent decision processes in safety-critical domains and offer a promising direction for developing more interpretable autonomous driving systems."
  },
  {
    "title": "The Jailbreak Tax: How Useful are Your Jailbreak Outputs?",
    "url": "http://arxiv.org/abs/2504.10694v1",
    "arxiv_id": "2504.10694v1",
    "authors": [
      "Kristina Nikoli\u0107",
      "Luze Sun",
      "Jie Zhang",
      "Florian Tram\u00e8r"
    ],
    "published": "2025-04-14T20:30:41+00:00",
    "summary": "Jailbreak attacks bypass the guardrails of large language models to produce harmful outputs. In this paper, we ask whether the model outputs produced by existing jailbreaks are actually useful. For example, when jailbreaking a model to give instructions for building a bomb, does the jailbreak yield good instructions? Since the utility of most unsafe answers (e.g., bomb instructions) is hard to evaluate rigorously, we build new jailbreak evaluation sets with known ground truth answers, by aligning models to refuse questions related to benign and easy-to-evaluate topics (e.g., biology or math). Our evaluation of eight representative jailbreaks across five utility benchmarks reveals a consistent drop in model utility in jailbroken responses, which we term the jailbreak tax. For example, while all jailbreaks we tested bypass guardrails in models aligned to refuse to answer math, this comes at the expense of a drop of up to 92% in accuracy. Overall, our work proposes the jailbreak tax as a new important metric in AI safety, and introduces benchmarks to evaluate existing and future jailbreaks. We make the benchmark available at https://github.com/ethz-spylab/jailbreak-tax"
  },
  {
    "title": "Beyond Chains of Thought: Benchmarking Latent-Space Reasoning Abilities in Large Language Models",
    "url": "http://arxiv.org/abs/2504.10615v1",
    "arxiv_id": "2504.10615v1",
    "authors": [
      "Thilo Hagendorff",
      "Sarah Fabi"
    ],
    "published": "2025-04-14T18:15:27+00:00",
    "summary": "Large language models (LLMs) can perform reasoning computations both internally within their latent space and externally by generating explicit token sequences like chains of thought. Significant progress in enhancing reasoning abilities has been made by scaling test-time compute. However, understanding and quantifying model-internal reasoning abilities - the inferential \"leaps\" models make between individual token predictions - remains crucial. This study introduces a benchmark (n = 4,000 items) designed to quantify model-internal reasoning in different domains. We achieve this by having LLMs indicate the correct solution to reasoning problems not through descriptive text, but by selecting a specific language of their initial response token that is different from English, the benchmark language. This not only requires models to reason beyond their context window, but also to overrise their default tendency to respond in the same language as the prompt, thereby posing an additional cognitive strain. We evaluate a set of 18 LLMs, showing significant performance variations, with GPT-4.5 achieving the highest accuracy (74.7%), outperforming models like Grok-2 (67.2%), and Llama 3.1 405B (65.6%). Control experiments and difficulty scaling analyses suggest that while LLMs engage in internal reasoning, we cannot rule out heuristic exploitations under certain conditions, marking an area for future investigation. Our experiments demonstrate that LLMs can \"think\" via latent-space computations, revealing model-internal inference strategies that need further understanding, especially regarding safety-related concerns such as covert planning, goal-seeking, or deception emerging without explicit token traces."
  },
  {
    "title": "Decoupled Diffusion Sparks Adaptive Scene Generation",
    "url": "http://arxiv.org/abs/2504.10485v1",
    "arxiv_id": "2504.10485v1",
    "authors": [
      "Yunsong Zhou",
      "Naisheng Ye",
      "William Ljungbergh",
      "Tianyu Li",
      "Jiazhi Yang",
      "Zetong Yang",
      "Hongzi Zhu",
      "Christoffer Petersson",
      "Hongyang Li"
    ],
    "published": "2025-04-14T17:59:57+00:00",
    "summary": "Controllable scene generation could reduce the cost of diverse data collection substantially for autonomous driving. Prior works formulate the traffic layout generation as predictive progress, either by denoising entire sequences at once or by iteratively predicting the next frame. However, full sequence denoising hinders online reaction, while the latter's short-sighted next-frame prediction lacks precise goal-state guidance. Further, the learned model struggles to generate complex or challenging scenarios due to a large number of safe and ordinal driving behaviors from open datasets. To overcome these, we introduce Nexus, a decoupled scene generation framework that improves reactivity and goal conditioning by simulating both ordinal and challenging scenarios from fine-grained tokens with independent noise states. At the core of the decoupled pipeline is the integration of a partial noise-masking training strategy and a noise-aware schedule that ensures timely environmental updates throughout the denoising process. To complement challenging scenario generation, we collect a dataset consisting of complex corner cases. It covers 540 hours of simulated data, including high-risk interactions such as cut-in, sudden braking, and collision. Nexus achieves superior generation realism while preserving reactivity and goal orientation, with a 40% reduction in displacement error. We further demonstrate that Nexus improves closed-loop planning by 20% through data augmentation and showcase its capability in safety-critical data generation."
  },
  {
    "title": "xVerify: Efficient Answer Verifier for Reasoning Model Evaluations",
    "url": "http://arxiv.org/abs/2504.10481v1",
    "arxiv_id": "2504.10481v1",
    "authors": [
      "Ding Chen",
      "Qingchen Yu",
      "Pengyuan Wang",
      "Wentao Zhang",
      "Bo Tang",
      "Feiyu Xiong",
      "Xinchi Li",
      "Minchuan Yang",
      "Zhiyu Li"
    ],
    "published": "2025-04-14T17:59:36+00:00",
    "summary": "With the release of the o1 model by OpenAI, reasoning models adopting slow thinking strategies have gradually emerged. As the responses generated by such models often include complex reasoning, intermediate steps, and self-reflection, existing evaluation methods are often inadequate. They struggle to determine whether the LLM output is truly equivalent to the reference answer, and also have difficulty identifying and extracting the final answer from long, complex responses. To address this issue, we propose xVerify, an efficient answer verifier for reasoning model evaluations. xVerify demonstrates strong capability in equivalence judgment, enabling it to effectively determine whether the answers produced by reasoning models are equivalent to reference answers across various types of objective questions. To train and evaluate xVerify, we construct the VAR dataset by collecting question-answer pairs generated by multiple LLMs across various datasets, leveraging multiple reasoning models and challenging evaluation sets designed specifically for reasoning model assessment. A multi-round annotation process is employed to ensure label accuracy. Based on the VAR dataset, we train multiple xVerify models of different scales. In evaluation experiments conducted on both the test set and generalization set, all xVerify models achieve overall F1 scores and accuracy exceeding 95\\%. Notably, the smallest variant, xVerify-0.5B-I, outperforms all evaluation methods except GPT-4o, while xVerify-3B-Ib surpasses GPT-4o in overall performance. These results validate the effectiveness and generalizability of xVerify."
  },
  {
    "title": "GUI-R1 : A Generalist R1-Style Vision-Language Action Model For GUI Agents",
    "url": "http://arxiv.org/abs/2504.10458v1",
    "arxiv_id": "2504.10458v1",
    "authors": [
      "Xiaobo Xia",
      "Run Luo"
    ],
    "published": "2025-04-14T17:45:54+00:00",
    "summary": "Existing efforts in building Graphical User Interface (GUI) agents largely rely on the training paradigm of supervised fine-tuning on Large Vision-Language Models (LVLMs). However, this approach not only demands extensive amounts of training data but also struggles to effectively understand GUI screenshots and generalize to unseen interfaces. The issue significantly limits its application in real-world scenarios, especially for high-level tasks. Inspired by Reinforcement Fine-Tuning (RFT) in large reasoning models (e.g., DeepSeek-R1), which efficiently enhances the problem-solving capabilities of large language models in real-world settings, we propose \\name, the first reinforcement learning framework designed to enhance the GUI capabilities of LVLMs in high-level real-world task scenarios, through unified action space rule modeling. By leveraging a small amount of carefully curated high-quality data across multiple platforms (including Windows, Linux, MacOS, Android, and Web) and employing policy optimization algorithms such as Group Relative Policy Optimization (GRPO) to update the model, \\name achieves superior performance using only 0.02\\% of the data (3K vs. 13M) compared to previous state-of-the-art methods like OS-Atlas across eight benchmarks spanning three different platforms (mobile, desktop, and web). These results demonstrate the immense potential of reinforcement learning based on unified action space rule modeling in improving the execution capabilities of LVLMs for real-world GUI agent tasks."
  },
  {
    "title": "GUI-R1 : A Generalist R1-Style Vision-Language Action Model For GUI Agents",
    "url": "http://arxiv.org/abs/2504.10458v2",
    "arxiv_id": "2504.10458v2",
    "authors": [
      "Xiaobo Xia",
      "Run Luo"
    ],
    "published": "2025-04-14T17:45:54+00:00",
    "summary": "Existing efforts in building Graphical User Interface (GUI) agents largely rely on the training paradigm of supervised fine-tuning on Large Vision-Language Models (LVLMs). However, this approach not only demands extensive amounts of training data but also struggles to effectively understand GUI screenshots and generalize to unseen interfaces. The issue significantly limits its application in real-world scenarios, especially for high-level tasks. Inspired by Reinforcement Fine-Tuning (RFT) in large reasoning models (e.g., DeepSeek-R1), which efficiently enhances the problem-solving capabilities of large language models in real-world settings, we propose \\name, the first reinforcement learning framework designed to enhance the GUI capabilities of LVLMs in high-level real-world task scenarios, through unified action space rule modeling. By leveraging a small amount of carefully curated high-quality data across multiple platforms (including Windows, Linux, MacOS, Android, and Web) and employing policy optimization algorithms such as Group Relative Policy Optimization (GRPO) to update the model, \\name achieves superior performance using only 0.02\\% of the data (3K vs. 13M) compared to previous state-of-the-art methods like OS-Atlas across eight benchmarks spanning three different platforms (mobile, desktop, and web). These results demonstrate the immense potential of reinforcement learning based on unified action space rule modeling in improving the execution capabilities of LVLMs for real-world GUI agent tasks."
  },
  {
    "title": "M1: Towards Scalable Test-Time Compute with Mamba Reasoning Models",
    "url": "http://arxiv.org/abs/2504.10449v1",
    "arxiv_id": "2504.10449v1",
    "authors": [
      "Junxiong Wang",
      "Wen-Ding Li",
      "Daniele Paliotta",
      "Daniel Ritter",
      "Alexander M. Rush",
      "Tri Dao"
    ],
    "published": "2025-04-14T17:38:25+00:00",
    "summary": "Effective reasoning is crucial to solving complex mathematical problems. Recent large language models (LLMs) have boosted performance by scaling test-time computation through long chain-of-thought reasoning. However, transformer-based models are inherently limited in extending context length due to their quadratic computational complexity and linear memory requirements. In this paper, we introduce a novel hybrid linear RNN reasoning model, M1, built on the Mamba architecture, which allows memory-efficient inference. Our approach leverages a distillation process from existing reasoning models and is further enhanced through RL training. Experimental results on the AIME and MATH benchmarks show that M1 not only outperforms previous linear RNN models but also matches the performance of state-of-the-art Deepseek R1 distilled reasoning models at a similar scale. We also compare our generation speed with a highly performant general purpose inference engine, vLLM, and observe more than a 3x speedup compared to a same size transformer. With throughput speedup, we are able to achieve higher accuracy compared to DeepSeek R1 distilled transformer reasoning models under a fixed generation time budget using self-consistency voting. Overall, we introduce a hybrid Mamba reasoning model and provide a more effective approach to scaling test-time generation using self-consistency or long chain of thought reasoning."
  },
  {
    "title": "LLM Can be a Dangerous Persuader: Empirical Study of Persuasion Safety in Large Language Models",
    "url": "http://arxiv.org/abs/2504.10430v1",
    "arxiv_id": "2504.10430v1",
    "authors": [
      "Minqian Liu",
      "Zhiyang Xu",
      "Xinyi Zhang",
      "Heajun An",
      "Sarvech Qadir",
      "Qi Zhang",
      "Pamela J. Wisniewski",
      "Jin-Hee Cho",
      "Sang Won Lee",
      "Ruoxi Jia",
      "Lifu Huang"
    ],
    "published": "2025-04-14T17:20:34+00:00",
    "summary": "Recent advancements in Large Language Models (LLMs) have enabled them to approach human-level persuasion capabilities. However, such potential also raises concerns about the safety risks of LLM-driven persuasion, particularly their potential for unethical influence through manipulation, deception, exploitation of vulnerabilities, and many other harmful tactics. In this work, we present a systematic investigation of LLM persuasion safety through two critical aspects: (1) whether LLMs appropriately reject unethical persuasion tasks and avoid unethical strategies during execution, including cases where the initial persuasion goal appears ethically neutral, and (2) how influencing factors like personality traits and external pressures affect their behavior. To this end, we introduce PersuSafety, the first comprehensive framework for the assessment of persuasion safety which consists of three stages, i.e., persuasion scene creation, persuasive conversation simulation, and persuasion safety assessment. PersuSafety covers 6 diverse unethical persuasion topics and 15 common unethical strategies. Through extensive experiments across 8 widely used LLMs, we observe significant safety concerns in most LLMs, including failing to identify harmful persuasion tasks and leveraging various unethical persuasion strategies. Our study calls for more attention to improve safety alignment in progressive and goal-driven conversations such as persuasion."
  },
  {
    "title": "Ctrl-Z: Controlling AI Agents via Resampling",
    "url": "http://arxiv.org/abs/2504.10374v1",
    "arxiv_id": "2504.10374v1",
    "authors": [
      "Aryan Bhatt",
      "Cody Rushing",
      "Adam Kaufman",
      "Tyler Tracy",
      "Vasil Georgiev",
      "David Matolcsi",
      "Akbir Khan",
      "Buck Shlegeris"
    ],
    "published": "2025-04-14T16:22:11+00:00",
    "summary": "Control evaluations measure whether monitoring and security protocols for AI systems prevent intentionally subversive AI models from causing harm. Our work presents the first control evaluation performed in an agent environment. We construct BashBench, a dataset of 257 challenging multi-step system administration tasks, and evaluate whether various safety measures can prevent an adversarially constructed AI agent from covertly downloading and executing malicious code in this environment. This multi-step setting introduces new attack and defense dynamics, which we investigate in order to design novel control protocols that prevent safety failures without hindering the ability of non-malicious agents to perform useful work. We introduce a class of control protocols called resample protocols that dynamically take additional samples of certain actions. We find these protocols significantly improve on existing techniques by selectively blocking the AI agent from executing suspicious code and incriminating the agent by generating additional examples of dangerous behavior. We measure the tradeoff between attack prevention and usefulness; our best protocol combines resampling with analysis of previous steps, reducing the success rate of attacks from 58% to 7% at a 5% cost to the performance of a non-malicious agent."
  },
  {
    "title": "Reactive power flow optimization in AC drive systems",
    "url": "http://arxiv.org/abs/2504.10360v1",
    "arxiv_id": "2504.10360v1",
    "authors": [
      "Sanjay Chandrasekaran",
      "Catalin Arghir",
      "Pieder Joerg",
      "Florian Doerfler",
      "Silvia Mastellone"
    ],
    "published": "2025-04-14T16:08:32+00:00",
    "summary": "This paper explores a limit avoidance approach in the case of input (modulation) and output (current) constraints with the aim of enhancing system availability of AC drives. Drawing on the observation that, in a certain range of reactive power, there exists a trade-off between current and modulation magnitude, we exploit this freedom and define a constrained optimization problem. We propose two approaches, one in the form of an activation-function which drives the reactive power set-point towards safety, and an approach which uses online feedback optimization to set the reactive power dynamically. Both methods compromise reactive power tracking accuracy for increased system robustness. Through a high fidelity simulation, we compare the benefits of the two methods, highlighting their effectiveness in industrial applications."
  },
  {
    "title": "Improving diffusion modeling in all-solid-state lithium batteries: a novel approach for grain boundary effects",
    "url": "http://arxiv.org/abs/2504.10348v1",
    "arxiv_id": "2504.10348v1",
    "authors": [
      "Lena Scholz",
      "Yongliang Ou",
      "Blazej Grabowski",
      "Felix Fritzen"
    ],
    "published": "2025-04-14T15:58:25+00:00",
    "summary": "All-solid-state lithium-ion batteries offer promising advantages with respect to capacity, safety, and performance. The diffusion behavior of lithium ions in the contained polycrystalline solid-state electrolyte is crucial for battery function. While atomistic studies indicate that grain boundaries (GBs) and grain size significantly impact diffusivity, the corresponding effects are either neglected in simulations on larger scales or considered only under strong assumptions such as isotropy. Our approach considers the fully resolved crystalline structure with a parametrization aligned with the atomistic perspective to describe diffusion along and across GBs. The approach is embedded into a finite element simulation using a novel collapsed interface element based on an analytical description in thickness direction. Results are governed by different and potentially anisotropic diffusion coefficients in bulk and GB domains. The mesoscale response is derived using linear computational homogenization to capture large-scale effects. The novel collapsed interface description allows for a reconstruction of the 3D transport behavior within the GB domain without resolving it and is able to capture the relevant transport mechanisms such as channeling effects and concentration jumps. Grain size and GB volume fraction are expressed in terms of an affine parameter dependence and can be altered without any changes to geometry or mesh. Together with the observed dependence of the effective material response on the anisotropic GB parametrization, this leads to the identification of four distinct diffusion regimes, each with implications for the design of battery materials."
  },
  {
    "title": "Heimdall: test-time scaling on the generative verification",
    "url": "http://arxiv.org/abs/2504.10337v1",
    "arxiv_id": "2504.10337v1",
    "authors": [
      "Wenlei Shi",
      "Xing Jin"
    ],
    "published": "2025-04-14T15:46:33+00:00",
    "summary": "An AI system can create and maintain knowledge only to the extent that it can verify that knowledge itself. Recent work on long Chain-of-Thought reasoning has demonstrated great potential of LLMs on solving competitive problems, but their verification ability remains to be weak and not sufficiently investigated. In this paper, we propose Heimdall, the long CoT verification LLM that can accurately judge the correctness of solutions. With pure reinforcement learning, we boost the verification accuracy from 62.5% to 94.5% on competitive math problems. By scaling with repeated sampling, the accuracy further increases to 97.5%. Through human evaluation, Heimdall demonstrates impressive generalization capabilities, successfully detecting most issues in challenging math proofs, the type of which is not included during training. Furthermore, we propose Pessimistic Verification to extend the functionality of Heimdall to scaling up the problem solving. It calls Heimdall to judge the solutions from a solver model and based on the pessimistic principle, selects the most likely correct solution with the least uncertainty. Taking DeepSeek-R1-Distill-Qwen-32B as the solver model, Pessimistic Verification improves the solution accuracy on AIME2025 from 54.2% to 70.0% with 16x compute budget and to 83.3% with more compute budget. With the stronger solver Gemini 2.5 Pro, the score reaches 93.0%. Finally, we prototype an automatic knowledge discovery system, a ternary system where one poses questions, another provides solutions, and the third verifies the solutions. Using the data synthesis work NuminaMath for the first two components, Heimdall effectively identifies problematic records within the dataset and reveals that nearly half of the data is flawed, which interestingly aligns with the recent ablation studies from NuminaMath."
  },
  {
    "title": "Heimdall: test-time scaling on the generative verification",
    "url": "http://arxiv.org/abs/2504.10337v2",
    "arxiv_id": "2504.10337v2",
    "authors": [
      "Wenlei Shi",
      "Xing Jin"
    ],
    "published": "2025-04-14T15:46:33+00:00",
    "summary": "An AI system can create and maintain knowledge only to the extent that it can verify that knowledge itself. Recent work on long Chain-of-Thought reasoning has demonstrated great potential of LLMs on solving competitive problems, but their verification ability remains to be weak and not sufficiently investigated. In this paper, we propose Heimdall, the long CoT verification LLM that can accurately judge the correctness of solutions. With pure reinforcement learning, we boost the verification accuracy from 62.5% to 94.5% on competitive math problems. By scaling with repeated sampling, the accuracy further increases to 97.5%. Through human evaluation, Heimdall demonstrates impressive generalization capabilities, successfully detecting most issues in challenging math proofs, the type of which is not included during training. Furthermore, we propose Pessimistic Verification to extend the functionality of Heimdall to scaling up the problem solving. It calls Heimdall to judge the solutions from a solver model and based on the pessimistic principle, selects the most likely correct solution with the least uncertainty. Taking DeepSeek-R1-Distill-Qwen-32B as the solver model, Pessimistic Verification improves the solution accuracy on AIME2025 from 54.2% to 70.0% with 16x compute budget and to 83.3% with more compute budget. With the stronger solver Gemini 2.5 Pro, the score reaches 93.0%. Finally, we prototype an automatic knowledge discovery system, a ternary system where one poses questions, another provides solutions, and the third verifies the solutions. Using the data synthesis work NuminaMath for the first two components, Heimdall effectively identifies problematic records within the dataset and reveals that nearly half of the data is flawed, which interestingly aligns with the recent ablation studies from NuminaMath."
  },
  {
    "title": "Cumulative-Time Signal Temporal Logic",
    "url": "http://arxiv.org/abs/2504.10325v1",
    "arxiv_id": "2504.10325v1",
    "authors": [
      "Hongkai Chen",
      "Zeyu Zhang",
      "Shouvik Roy",
      "Ezio Bartocci",
      "Scott A. Smolka",
      "Scott D. Stoller",
      "Shan Lin"
    ],
    "published": "2025-04-14T15:34:13+00:00",
    "summary": "Signal Temporal Logic (STL) is a widely adopted specification language in cyber-physical systems for expressing critical temporal requirements, such as safety conditions and response time. However, STL's expressivity is not sufficient to capture the cumulative duration during which a property holds within an interval of time. To overcome this limitation, we introduce Cumulative-Time Signal Temporal Logic (CT-STL) that operates over discrete-time signals and extends STL with a new cumulative-time operator. This operator compares the sum of all time steps for which its nested formula is true with a threshold. We present both a qualitative and a quantitative (robustness) semantics for CT-STL and prove both their soundness and completeness properties. We provide an efficient online monitoring algorithm for both semantics. Finally, we show the applicability of CT-STL in two case studies: specifying and monitoring cumulative temporal requirements for a microgrid and an artificial pancreas."
  },
  {
    "title": "SlowFastVAD: Video Anomaly Detection via Integrating Simple Detector and RAG-Enhanced Vision-Language Model",
    "url": "http://arxiv.org/abs/2504.10320v1",
    "arxiv_id": "2504.10320v1",
    "authors": [
      "Zongcan Ding",
      "Haodong Zhang",
      "Peng Wu",
      "Guansong Pang",
      "Zhiwei Yang",
      "Peng Wang",
      "Yanning Zhang"
    ],
    "published": "2025-04-14T15:30:03+00:00",
    "summary": "Video anomaly detection (VAD) aims to identify unexpected events in videos and has wide applications in safety-critical domains. While semi-supervised methods trained on only normal samples have gained traction, they often suffer from high false alarm rates and poor interpretability. Recently, vision-language models (VLMs) have demonstrated strong multimodal reasoning capabilities, offering new opportunities for explainable anomaly detection. However, their high computational cost and lack of domain adaptation hinder real-time deployment and reliability. Inspired by dual complementary pathways in human visual perception, we propose SlowFastVAD, a hybrid framework that integrates a fast anomaly detector with a slow anomaly detector (namely a retrieval augmented generation (RAG) enhanced VLM), to address these limitations. Specifically, the fast detector first provides coarse anomaly confidence scores, and only a small subset of ambiguous segments, rather than the entire video, is further analyzed by the slower yet more interpretable VLM for elaborate detection and reasoning. Furthermore, to adapt VLMs to domain-specific VAD scenarios, we construct a knowledge base including normal patterns based on few normal samples and abnormal patterns inferred by VLMs. During inference, relevant patterns are retrieved and used to augment prompts for anomaly reasoning. Finally, we smoothly fuse the anomaly confidence of fast and slow detectors to enhance robustness of anomaly detection. Extensive experiments on four benchmarks demonstrate that SlowFastVAD effectively combines the strengths of both fast and slow detectors, and achieves remarkable detection accuracy and interpretability with significantly reduced computational overhead, making it well-suited for real-world VAD applications with high reliability requirements."
  },
  {
    "title": "Siamese Network with Dual Attention for EEG-Driven Social Learning: Bridging the Human-Robot Gap in Long-Tail Autonomous Driving",
    "url": "http://arxiv.org/abs/2504.10296v1",
    "arxiv_id": "2504.10296v1",
    "authors": [
      "Xiaoshan Zhou",
      "Carol C. Menassa",
      "Vineet R. Kamat"
    ],
    "published": "2025-04-14T15:06:17+00:00",
    "summary": "Robots with wheeled, quadrupedal, or humanoid forms are increasingly integrated into built environments. However, unlike human social learning, they lack a critical pathway for intrinsic cognitive development, namely, learning from human feedback during interaction. To understand human ubiquitous observation, supervision, and shared control in dynamic and uncertain environments, this study presents a brain-computer interface (BCI) framework that enables classification of Electroencephalogram (EEG) signals to detect cognitively demanding and safety-critical events. As a timely and motivating co-robotic engineering application, we simulate a human-in-the-loop scenario to flag risky events in semi-autonomous robotic driving-representative of long-tail cases that pose persistent bottlenecks to the safety performance of smart mobility systems and robotic vehicles. Drawing on recent advances in few-shot learning, we propose a dual-attention Siamese convolutional network paired with Dynamic Time Warping Barycenter Averaging approach to generate robust EEG-encoded signal representations. Inverse source localization reveals activation in Broadman areas 4 and 9, indicating perception-action coupling during task-relevant mental imagery. The model achieves 80% classification accuracy under data-scarce conditions and exhibits a nearly 100% increase in the utility of salient features compared to state-of-the-art methods, as measured through integrated gradient attribution. Beyond performance, this study contributes to our understanding of the cognitive architecture required for BCI agents-particularly the role of attention and memory mechanisms-in categorizing diverse mental states and supporting both inter- and intra-subject adaptation. Overall, this research advances the development of cognitive robotics and socially guided learning for service robots in complex built environments."
  },
  {
    "title": "Gravity-induced emergence of the Fermi scale in quantum quadratic gravity",
    "url": "http://arxiv.org/abs/2504.10293v1",
    "arxiv_id": "2504.10293v1",
    "authors": [
      "Mohammad Mehrafarin"
    ],
    "published": "2025-04-14T15:04:47+00:00",
    "summary": "In the framework of asymptotic safety, we study quantum quadratic gravity in the presence of the Higgs field considered as non-separable from the vacuum. The theory flows to a high energy fixed point where the Higgs field is strongly coupled to gravity, its potential is symmetric, and the quadratic Weyl curvature coupling is large. The latter renders the ghost graviton an unstable high mass resonance which renders unitarity in the spirit of Lee-Week type theories. Furthermore, if the scalar graviton is tachyonic then there will be a low energy fixed point where tachyonic condensation leads to a new stable vacuum. At this fixed point the symmetry breaks and the Fermi scale emerges, and the behavior of the Higgs field is classical (not influenced by gravitational interaction). Gravity at the UV scale is purely quadratic whereas at the Fermi scale it is linear, and in the intermediate region both contributions are relevant. Thus, at the Fermi scale the quadratic curvature fields disappear through the ghost instability and tachyon condensation, giving rise to Einstein gravity and the electroweak phase transition."
  },
  {
    "title": "Deep Reasoning Translation via Reinforcement Learning",
    "url": "http://arxiv.org/abs/2504.10187v1",
    "arxiv_id": "2504.10187v1",
    "authors": [
      "Jiaan Wang",
      "Fandong Meng",
      "Jie Zhou"
    ],
    "published": "2025-04-14T12:40:39+00:00",
    "summary": "Recently, deep reasoning LLMs (e.g., OpenAI o1/o3 and DeepSeek-R1) have shown promising performance in various complex tasks. Free translation is an important and interesting task in the multilingual world, which requires going beyond word-for-word translation and taking cultural differences into account. This task is still under-explored in deep reasoning LLMs. In this paper, we introduce DeepTrans, a deep reasoning translation model that learns free translation via reinforcement learning. Specifically, we carefully build a reward model with pre-defined scoring criteria on both the translation results and the thought process. Given the source sentences, the reward model teaches the deep translation model how to think and free-translate them during reinforcement learning. In this way, training DeepTrans does not need any labeled translations, avoiding the human-intensive annotation or resource-intensive data synthesis. Experimental results show the effectiveness of DeepTrans. Using Qwen2.5-7B as the backbone, DeepTrans improves performance by 16.3% in literature translation, and outperforms strong deep reasoning baselines as well as baselines that are fine-tuned with synthesized data. Moreover, we summarize the failures and interesting findings during our RL exploration. We hope this work could inspire other researchers in free translation."
  },
  {
    "title": "LLM Unlearning Reveals a Stronger-Than-Expected Coreset Effect in Current Benchmarks",
    "url": "http://arxiv.org/abs/2504.10185v1",
    "arxiv_id": "2504.10185v1",
    "authors": [
      "Soumyadeep Pal",
      "Changsheng Wang",
      "James Diffenderfer",
      "Bhavya Kailkhura",
      "Sijia Liu"
    ],
    "published": "2025-04-14T12:38:37+00:00",
    "summary": "Large language model unlearning has become a critical challenge in ensuring safety and controlled model behavior by removing undesired data-model influences from the pretrained model while preserving general utility. Significant recent efforts have been dedicated to developing LLM unlearning benchmarks such as WMDP (Weapons of Mass Destruction Proxy) and MUSE (Machine Unlearning Six-way Evaluation), facilitating standardized unlearning performance assessment and method comparison. Despite their usefulness, we uncover for the first time a novel coreset effect within these benchmarks. Specifically, we find that LLM unlearning achieved with the original (full) forget set can be effectively maintained using a significantly smaller subset (functioning as a \"coreset\"), e.g., as little as 5% of the forget set, even when selected at random. This suggests that LLM unlearning in these benchmarks can be performed surprisingly easily, even in an extremely low-data regime. We demonstrate that this coreset effect remains strong, regardless of the LLM unlearning method used, such as NPO (Negative Preference Optimization) and RMU (Representation Misdirection Unlearning), the popular ones in these benchmarks. The surprisingly strong coreset effect is also robust across various data selection methods, ranging from random selection to more sophisticated heuristic approaches. We explain the coreset effect in LLM unlearning through a keyword-based perspective, showing that keywords extracted from the forget set alone contribute significantly to unlearning effectiveness and indicating that current unlearning is driven by a compact set of high-impact tokens rather than the entire dataset. We further justify the faithfulness of coreset-unlearned models along additional dimensions, such as mode connectivity and robustness to jailbreaking attacks. Codes are available at https://github.com/OPTML-Group/MU-Coreset."
  },
  {
    "title": "LLM Unlearning Reveals a Stronger-Than-Expected Coreset Effect in Current Benchmarks",
    "url": "http://arxiv.org/abs/2504.10185v2",
    "arxiv_id": "2504.10185v2",
    "authors": [
      "Soumyadeep Pal",
      "Changsheng Wang",
      "James Diffenderfer",
      "Bhavya Kailkhura",
      "Sijia Liu"
    ],
    "published": "2025-04-14T12:38:37+00:00",
    "summary": "Large language model unlearning has become a critical challenge in ensuring safety and controlled model behavior by removing undesired data-model influences from the pretrained model while preserving general utility. Significant recent efforts have been dedicated to developing LLM unlearning benchmarks such as WMDP (Weapons of Mass Destruction Proxy) and MUSE (Machine Unlearning Six-way Evaluation), facilitating standardized unlearning performance assessment and method comparison. Despite their usefulness, we uncover for the first time a novel coreset effect within these benchmarks. Specifically, we find that LLM unlearning achieved with the original (full) forget set can be effectively maintained using a significantly smaller subset (functioning as a \"coreset\"), e.g., as little as 5% of the forget set, even when selected at random. This suggests that LLM unlearning in these benchmarks can be performed surprisingly easily, even in an extremely low-data regime. We demonstrate that this coreset effect remains strong, regardless of the LLM unlearning method used, such as NPO (Negative Preference Optimization) and RMU (Representation Misdirection Unlearning), the popular ones in these benchmarks. The surprisingly strong coreset effect is also robust across various data selection methods, ranging from random selection to more sophisticated heuristic approaches. We explain the coreset effect in LLM unlearning through a keyword-based perspective, showing that keywords extracted from the forget set alone contribute significantly to unlearning effectiveness and indicating that current unlearning is driven by a compact set of high-impact tokens rather than the entire dataset. We further justify the faithfulness of coreset-unlearned models along additional dimensions, such as mode connectivity and robustness to jailbreaking attacks. Codes are available at https://github.com/OPTML-Group/MU-Coreset."
  },
  {
    "title": "Challenges in interpretability of additive models",
    "url": "http://arxiv.org/abs/2504.10169v1",
    "arxiv_id": "2504.10169v1",
    "authors": [
      "Xinyu Zhang",
      "Julien Martinelli",
      "ST John"
    ],
    "published": "2025-04-14T12:24:17+00:00",
    "summary": "We review generalized additive models as a type of ``transparent'' model that has recently seen renewed interest in the deep learning community as neural additive models. We highlight multiple types of nonidentifiability in this model class and discuss challenges in interpretability, arguing for restraint when claiming ``interpretability'' or ``suitability for safety-critical applications'' of such models."
  },
  {
    "title": "GeoUni: A Unified Model for Generating Geometry Diagrams, Problems and Problem Solutions",
    "url": "http://arxiv.org/abs/2504.10146v1",
    "arxiv_id": "2504.10146v1",
    "authors": [
      "Jo-Ku Cheng",
      "Zeren Zhang",
      "Ran Chen",
      "Jingyang Deng",
      "Ziran Qin",
      "Jinwen Ma"
    ],
    "published": "2025-04-14T11:56:55+00:00",
    "summary": "We propose GeoUni, the first unified geometry expert model capable of generating problem solutions and diagrams within a single framework in a way that enables the creation of unique and individualized geometry problems. Traditionally, solving geometry problems and generating diagrams have been treated as separate tasks in machine learning, with no models successfully integrating both to support problem creation. However, we believe that mastery in geometry requires frictionless integration of all of these skills, from solving problems to visualizing geometric relationships, and finally, crafting tailored problems. Our extensive experiments demonstrate that GeoUni, with only 1.5B parameters, achieves performance comparable to larger models such as DeepSeek-R1 with 671B parameters in geometric reasoning tasks. GeoUni also excels in generating precise geometric diagrams, surpassing both text-to-image models and unified models, including the GPT-4o image generation. Most importantly, GeoUni is the only model capable of successfully generating textual problems with matching diagrams based on specific knowledge points, thus offering a wider range of capabilities that extend beyond current models."
  },
  {
    "title": "M2S-RoAD: Multi-Modal Semantic Segmentation for Road Damage Using Camera and LiDAR Data",
    "url": "http://arxiv.org/abs/2504.10123v1",
    "arxiv_id": "2504.10123v1",
    "authors": [
      "Tzu-Yun Tseng",
      "Hongyu Lyu",
      "Josephine Li",
      "Julie Stephany Berrio",
      "Mao Shan",
      "Stewart Worrall"
    ],
    "published": "2025-04-14T11:32:01+00:00",
    "summary": "Road damage can create safety and comfort challenges for both human drivers and autonomous vehicles (AVs). This damage is particularly prevalent in rural areas due to less frequent surveying and maintenance of roads. Automated detection of pavement deterioration can be used as an input to AVs and driver assistance systems to improve road safety. Current research in this field has predominantly focused on urban environments driven largely by public datasets, while rural areas have received significantly less attention. This paper introduces M2S-RoAD, a dataset for the semantic segmentation of different classes of road damage. M2S-RoAD was collected in various towns across New South Wales, Australia, and labelled for semantic segmentation to identify nine distinct types of road damage. This dataset will be released upon the acceptance of the paper."
  },
  {
    "title": "CameraBench: Benchmarking Visual Reasoning in MLLMs via Photography",
    "url": "http://arxiv.org/abs/2504.10090v1",
    "arxiv_id": "2504.10090v1",
    "authors": [
      "I-Sheng Fang",
      "Jun-Cheng Chen"
    ],
    "published": "2025-04-14T10:53:44+00:00",
    "summary": "Large language models (LLMs) and multimodal large language models (MLLMs) have significantly advanced artificial intelligence. However, visual reasoning, reasoning involving both visual and textual inputs, remains underexplored. Recent advancements, including the reasoning models like OpenAI o1 and Gemini 2.0 Flash Thinking, which incorporate image inputs, have opened this capability. In this ongoing work, we focus specifically on photography-related tasks because a photo is a visual snapshot of the physical world where the underlying physics (i.e., illumination, blur extent, etc.) interplay with the camera parameters. Successfully reasoning from the visual information of a photo to identify these numerical camera settings requires the MLLMs to have a deeper understanding of the underlying physics for precise visual comprehension, representing a challenging and intelligent capability essential for practical applications like photography assistant agents. We aim to evaluate MLLMs on their ability to distinguish visual differences related to numerical camera settings, extending a methodology previously proposed for vision-language models (VLMs). Our preliminary results demonstrate the importance of visual reasoning in photography-related tasks. Moreover, these results show that no single MLLM consistently dominates across all evaluation tasks, demonstrating ongoing challenges and opportunities in developing MLLMs with better visual reasoning."
  },
  {
    "title": "RealSafe-R1: Safety-Aligned DeepSeek-R1 without Compromising Reasoning Capability",
    "url": "http://arxiv.org/abs/2504.10081v1",
    "arxiv_id": "2504.10081v1",
    "authors": [
      "Yichi Zhang",
      "Zihao Zeng",
      "Dongbai Li",
      "Yao Huang",
      "Zhijie Deng",
      "Yinpeng Dong"
    ],
    "published": "2025-04-14T10:26:37+00:00",
    "summary": "Large Reasoning Models (LRMs), such as OpenAI o1 and DeepSeek-R1, have been rapidly progressing and achieving breakthrough performance on complex reasoning tasks such as mathematics and coding. However, the open-source R1 models have raised safety concerns in wide applications, such as the tendency to comply with malicious queries, which greatly impacts the utility of these powerful models in their applications. In this paper, we introduce RealSafe-R1 as safety-aligned versions of DeepSeek-R1 distilled models. To train these models, we construct a dataset of 15k safety-aware reasoning trajectories generated by DeepSeek-R1, under explicit instructions for expected refusal behavior. Both quantitative experiments and qualitative case studies demonstrate the models' improvements, which are shown in their safety guardrails against both harmful queries and jailbreak attacks. Importantly, unlike prior safety alignment efforts that often compromise reasoning performance, our method preserves the models' reasoning capabilities by maintaining the training data within the original distribution of generation. Model weights of RealSafe-R1 are open-source at https://huggingface.co/RealSafe."
  },
  {
    "title": "Balancing Two Classifiers via A Simplex ETF Structure for Model Calibration",
    "url": "http://arxiv.org/abs/2504.10007v1",
    "arxiv_id": "2504.10007v1",
    "authors": [
      "Jiani Ni",
      "He Zhao",
      "Jintong Gao",
      "Dandan Guo",
      "Hongyuan Zha"
    ],
    "published": "2025-04-14T09:09:01+00:00",
    "summary": "In recent years, deep neural networks (DNNs) have demonstrated state-of-the-art performance across various domains. However, despite their success, they often face calibration issues, particularly in safety-critical applications such as autonomous driving and healthcare, where unreliable predictions can have serious consequences. Recent research has started to improve model calibration from the view of the classifier. However, the exploration of designing the classifier to solve the model calibration problem is insufficient. Let alone most of the existing methods ignore the calibration errors arising from underconfidence. In this work, we propose a novel method by balancing learnable and ETF classifiers to solve the overconfidence or underconfidence problem for model Calibration named BalCAL. By introducing a confidence-tunable module and a dynamic adjustment method, we ensure better alignment between model confidence and its true accuracy. Extensive experimental validation shows that ours significantly improves model calibration performance while maintaining high predictive accuracy, outperforming existing techniques. This provides a novel solution to the calibration challenges commonly encountered in deep learning."
  },
  {
    "title": "Do We Really Need Curated Malicious Data for Safety Alignment in Multi-modal Large Language Models?",
    "url": "http://arxiv.org/abs/2504.10000v1",
    "arxiv_id": "2504.10000v1",
    "authors": [
      "Yanbo Wang",
      "Jiyang Guan",
      "Jian Liang",
      "Ran He"
    ],
    "published": "2025-04-14T09:03:51+00:00",
    "summary": "Multi-modal large language models (MLLMs) have made significant progress, yet their safety alignment remains limited. Typically, current open-source MLLMs rely on the alignment inherited from their language module to avoid harmful generations. However, the lack of safety measures specifically designed for multi-modal inputs creates an alignment gap, leaving MLLMs vulnerable to vision-domain attacks such as typographic manipulation. Current methods utilize a carefully designed safety dataset to enhance model defense capability, while the specific knowledge or patterns acquired from the high-quality dataset remain unclear. Through comparison experiments, we find that the alignment gap primarily arises from data distribution biases, while image content, response quality, or the contrastive behavior of the dataset makes little contribution to boosting multi-modal safety. To further investigate this and identify the key factors in improving MLLM safety, we propose finetuning MLLMs on a small set of benign instruct-following data with responses replaced by simple, clear rejection sentences. Experiments show that, without the need for labor-intensive collection of high-quality malicious data, model safety can still be significantly improved, as long as a specific fraction of rejection data exists in the finetuning set, indicating the security alignment is not lost but rather obscured during multi-modal pretraining or instruction finetuning. Simply correcting the underlying data bias could narrow the safety gap in the vision domain."
  },
  {
    "title": "Assessing Judging Bias in Large Reasoning Models: An Empirical Study",
    "url": "http://arxiv.org/abs/2504.09946v1",
    "arxiv_id": "2504.09946v1",
    "authors": [
      "Qian Wang",
      "Zhanzhi Lou",
      "Zhenheng Tang",
      "Nuo Chen",
      "Xuandong Zhao",
      "Wenxuan Zhang",
      "Dawn Song",
      "Bingsheng He"
    ],
    "published": "2025-04-14T07:14:27+00:00",
    "summary": "Large Reasoning Models (LRMs) like DeepSeek-R1 and OpenAI-o1 have demonstrated remarkable reasoning capabilities, raising important questions about their biases in LLM-as-a-judge settings. We present a comprehensive benchmark comparing judging biases between LLMs and LRMs across both subjective preference-alignment datasets and objective fact-based datasets. Through investigation of bandwagon, authority, position, and distraction biases, we uncover four key findings: (1) despite their advanced reasoning capabilities, LRMs remain susceptible to the above biases; (2) LRMs demonstrate better robustness than LLMs specifically on fact-related datasets; (3) LRMs exhibit notable position bias, preferring options in later positions; and (4) we identify a novel \"superficial reflection bias\" where phrases mimicking reasoning (e.g., \"wait, let me think...\") significantly influence model judgments. To address these biases, we design and evaluate three mitigation strategies: specialized system prompts that reduce judging biases by up to 19\\% in preference alignment datasets and 14\\% in fact-related datasets, in-context learning that provides up to 27\\% improvement on preference tasks but shows inconsistent results on factual tasks, and a self-reflection mechanism that reduces biases by up to 10\\% in preference datasets and 16\\% in fact-related datasets, with self-reflection proving particularly effective for LRMs. Our work provides crucial insights for developing more reliable LLM-as-a-Judge frameworks, especially as LRMs become increasingly deployed as automated judges."
  },
  {
    "title": "Tight Semidefinite Relaxations for Verifying Robustness of Neural Networks",
    "url": "http://arxiv.org/abs/2504.09934v1",
    "arxiv_id": "2504.09934v1",
    "authors": [
      "Godai Azuma",
      "Sunyoung Kim",
      "Makoto Yamashita"
    ],
    "published": "2025-04-14T06:54:00+00:00",
    "summary": "For verifying the safety of neural networks (NNs), Fazlyab et al. (2019) introduced a semidefinite programming (SDP) approach called DeepSDP. This formulation can be viewed as the dual of the SDP relaxation for a problem formulated as a quadratically constrained quadratic program (QCQP). While SDP relaxations of QCQPs generally provide approximate solutions with some gaps, this work focuses on tight SDP relaxations that provide exact solutions to the QCQP for single-layer NNs. Specifically, we analyze tightness conditions in three cases: (i) NNs with a single neuron, (ii) single-layer NNs with an ellipsoidal input set, and (iii) single-layer NNs with a rectangular input set. For NNs with a single neuron, we propose a condition that ensures the SDP admits a rank-1 solution to DeepSDP by transforming the QCQP into an equivalent two-stage problem leads to a solution collinear with a predetermined vector. For single-layer NNs with an ellipsoidal input set, the collinearity of solutions is proved via the Karush-Kuhn-Tucker condition in the two-stage problem. In case of single-layer NNs with a rectangular input set, we demonstrate that the tightness of DeepSDP can be reduced to the single-neuron NNs, case (i), if the weight matrix is a diagonal matrix."
  },
  {
    "title": "Learning from Reference Answers: Versatile Language Model Alignment without Binary Human Preference Data",
    "url": "http://arxiv.org/abs/2504.09895v1",
    "arxiv_id": "2504.09895v1",
    "authors": [
      "Shuai Zhao",
      "Linchao Zhu",
      "Yi Yang"
    ],
    "published": "2025-04-14T05:43:21+00:00",
    "summary": "Large language models~(LLMs) are expected to be helpful, harmless, and honest. In various alignment scenarios, such as general human preference, safety, and confidence alignment, binary preference data collection and reward modeling are resource-intensive but necessary for human preference transferring. In this work, we explore using the similarity between sampled generations and high-quality reference answers as an alternative reward function for LLM alignment. Using similarity as a reward circumvents training reward models, and collecting a single reference answer potentially costs less time than constructing binary preference pairs when multiple candidates are available. Specifically, we develop \\textit{RefAlign}, a versatile REINFORCE-style alignment algorithm, which is free of reference and reward models. Instead, RefAlign utilizes BERTScore between sampled generations and high-quality reference answers as the surrogate reward. Beyond general human preference optimization, RefAlign can be readily extended to diverse scenarios, such as safety and confidence alignment, by incorporating the similarity reward with task-related objectives. In various scenarios, {RefAlign} demonstrates comparable performance to previous alignment methods while offering high efficiency."
  },
  {
    "title": "Reasoning Models Can Be Effective Without Thinking",
    "url": "http://arxiv.org/abs/2504.09858v1",
    "arxiv_id": "2504.09858v1",
    "authors": [
      "Wenjie Ma",
      "Jingxuan He",
      "Charlie Snell",
      "Tyler Griggs",
      "Sewon Min",
      "Matei Zaharia"
    ],
    "published": "2025-04-14T04:08:16+00:00",
    "summary": "Recent LLMs have significantly improved reasoning capabilities, primarily by including an explicit, lengthy Thinking process as part of generation. In this paper, we question whether this explicit thinking is necessary. Using the state-of-the-art DeepSeek-R1-Distill-Qwen, we find that bypassing the thinking process via simple prompting, denoted as NoThinking, can be surprisingly effective. When controlling for the number of tokens, NoThinking outperforms Thinking across a diverse set of seven challenging reasoning datasets--including mathematical problem solving, formal theorem proving, and coding--especially in low-budget settings, e.g., 51.3 vs. 28.9 on ACM 23 with 700 tokens. Notably, the performance of NoThinking becomes more competitive with pass@k as k increases. Building on this observation, we demonstrate that a parallel scaling approach that uses NoThinking to generate N outputs independently and aggregates them is highly effective. For aggregation, we use task-specific verifiers when available, or we apply simple best-of-N strategies such as confidence-based selection. Our method outperforms a range of baselines with similar latency using Thinking, and is comparable to Thinking with significantly longer latency (up to 9x). Together, our research encourages a reconsideration of the necessity of lengthy thinking processes, while also establishing a competitive reference for achieving strong reasoning performance in low-budget settings or at low latency using parallel scaling."
  },
  {
    "title": "Training Small Reasoning LLMs with Cognitive Preference Alignment",
    "url": "http://arxiv.org/abs/2504.09802v1",
    "arxiv_id": "2504.09802v1",
    "authors": [
      "Wenrui Cai",
      "Chengyu Wang",
      "Junbing Yan",
      "Jun Huang",
      "Xiangzhong Fang"
    ],
    "published": "2025-04-14T02:03:54+00:00",
    "summary": "The reasoning capabilities of large language models (LLMs), such as OpenAI's o1 and DeepSeek-R1, have seen substantial advancements through deep thinking. However, these enhancements come with significant resource demands, underscoring the need to explore strategies to train effective reasoning LLMs with far fewer parameters. A critical challenge is that smaller models have different capacities and cognitive trajectories than their larger counterparts. Hence, direct distillation of chain-of-thought (CoT) results from large LLMs to smaller ones can be sometimes ineffective and requires a huge amount of annotated data. In this paper, we introduce a novel framework called Critique-Rethink-Verify (CRV), designed for training smaller yet powerful reasoning LLMs. Our CRV framework consists of multiple LLM agents, each specializing in unique abilities: (i) critiquing the CoTs according to the cognitive capabilities of smaller models, (ii) rethinking and refining these CoTs based on the critiques, and (iii) verifying the correctness of the refined results. We further propose the cognitive preference optimization (CogPO) algorithm to enhance the reasoning abilities of smaller models by aligning thoughts of these models with their cognitive capacities. Comprehensive evaluations on challenging reasoning benchmarks demonstrate the efficacy of CRV and CogPO, which outperforms other training methods by a large margin."
  },
  {
    "title": "Reasoning without Regret",
    "url": "http://arxiv.org/abs/2504.09777v1",
    "arxiv_id": "2504.09777v1",
    "authors": [
      "Tarun Chitra"
    ],
    "published": "2025-04-14T00:34:20+00:00",
    "summary": "Chain-of-thought reasoning enables large language models to solve multi-step tasks by framing problem solving as sequential decision problems. Outcome-based rewards, which provide feedback only on final answers, show impressive success, but face challenges with credit assignment and slow convergence. In contrast, procedure-based rewards offer efficient step-level feedback, but typically require costly human supervision. We introduce \\emph{Backwards Adaptive Reward Shaping} (BARS), a no-regret framework that converts sparse outcomes-based rewards into effective procedure-based signals. BARS uses sparse rewards generated from terminal-state priors and cover trees to scale rewards while preventing exploitation. With Bellman contraction and $(\\Delta, \\epsilon)$-gap rewards, our backward Euler solver achieves $\\epsilon$-accuracy in $O\\left((R_{\\max}/\\Delta)\\log(1/\\epsilon)\\right)$ iterations with $O(\\log T)$ dynamic regret over $T$ rounds. Our analysis, based on generic chaining, continuous scaling limits, and non-linear Feynman-Kac bounds, connects recent outcome-based methods' empirical successes with the benefits of intermediate supervision. Combined, this provides the first rigorous no-regret algorithm for outcome reward shaping, providing a theoretical foundation for the empirical success of DeepSeek's R1."
  },
  {
    "title": "Two Heads are Better Than One: Test-time Scaling of Multi-agent Collaborative Reasoning",
    "url": "http://arxiv.org/abs/2504.09772v1",
    "arxiv_id": "2504.09772v1",
    "authors": [
      "Can Jin",
      "Hongwu Peng",
      "Qixin Zhang",
      "Yujin Tang",
      "Dimitris N. Metaxas",
      "Tong Che"
    ],
    "published": "2025-04-14T00:27:45+00:00",
    "summary": "Multi-agent systems (MAS) built on large language models (LLMs) offer a promising path toward solving complex, real-world tasks that single-agent systems often struggle to manage. While recent advancements in test-time scaling (TTS) have significantly improved single-agent performance on challenging reasoning tasks, how to effectively scale collaboration and reasoning in MAS remains an open question. In this work, we introduce an adaptive multi-agent framework designed to enhance collaborative reasoning through both model-level training and system-level coordination. We construct M500, a high-quality dataset containing 500 multi-agent collaborative reasoning traces, and fine-tune Qwen2.5-32B-Instruct on this dataset to produce M1-32B, a model optimized for multi-agent collaboration. To further enable adaptive reasoning, we propose a novel CEO agent that dynamically manages the discussion process, guiding agent collaboration and adjusting reasoning depth for more effective problem-solving. Evaluated in an open-source MAS across a range of tasks-including general understanding, mathematical reasoning, and coding-our system significantly outperforms strong baselines. For instance, M1-32B achieves 12% improvement on GPQA-Diamond, 41% on AIME2024, and 10% on MBPP-Sanitized, matching the performance of state-of-the-art models like DeepSeek-R1 on some tasks. These results highlight the importance of both learned collaboration and adaptive coordination in scaling multi-agent reasoning. Code is available at https://github.com/jincan333/MAS-TTS"
  },
  {
    "title": "(How) Do reasoning models reason?",
    "url": "http://arxiv.org/abs/2504.09762v1",
    "arxiv_id": "2504.09762v1",
    "authors": [
      "Subbarao Kambhampati",
      "Kaya Stechly",
      "Karthik Valmeekam"
    ],
    "published": "2025-04-14T00:03:34+00:00",
    "summary": "We will provide a broad unifying perspective on the recent breed of Large Reasoning Models (LRMs) such as OpenAI o1 and DeepSeek R1, including their promise, sources of power, misconceptions and limitations."
  },
  {
    "title": "Hybrid Lyapunov and Barrier Function-Based Control with Stabilization Guarantees",
    "url": "http://arxiv.org/abs/2504.09760v1",
    "arxiv_id": "2504.09760v1",
    "authors": [
      "Hugo Matias",
      "Daniel Silvestre"
    ],
    "published": "2025-04-13T23:55:09+00:00",
    "summary": "Control Lyapunov Functions (CLFs) and Control Barrier Functions (CBFs) can be combined, typically by means of Quadratic Programs (QPs), to design controllers that achieve performance and safety objectives. However, a significant limitation of this framework is the introduction of asymptotically stable equilibrium points besides the minimizer of the CLF, leading to deadlock situations even for simple systems and bounded convex unsafe sets. To address this problem, we propose a hybrid CLF-CBF control framework with global asymptotic stabilization and safety guarantees, offering a more flexible and systematic design methodology compared to current alternatives available in the literature. We further extend this framework to higher-order systems via a recursive procedure based on a joint CLF-CBF backstepping approach. The proposed solution is assessed through several simulation examples."
  },
  {
    "title": "Epsilon-Neighborhood Decision-Boundary Governed Estimation (EDGE) of 2D Black Box Classifier Functions",
    "url": "http://arxiv.org/abs/2504.09733v1",
    "arxiv_id": "2504.09733v1",
    "authors": [
      "Mithun Goutham",
      "Riccardo DalferroNucci",
      "Stephanie Stockar",
      "Meghna Menon",
      "Sneha Nayak",
      "Harshad Zade",
      "Chetan Patel",
      "Mario Santillo"
    ],
    "published": "2025-04-13T21:40:46+00:00",
    "summary": "Accurately estimating decision boundaries in black box systems is critical when ensuring safety, quality, and feasibility in real-world applications. However, existing methods iteratively refine boundary estimates by sampling in regions of uncertainty, without providing guarantees on the closeness to the decision boundary and also result in unnecessary exploration that is especially disadvantageous when evaluations are costly. This paper presents the Epsilon-Neighborhood Decision-Boundary Governed Estimation (EDGE), a sample efficient and function-agnostic algorithm that leverages the intermediate value theorem to estimate the location of the decision boundary of a black box binary classifier within a user-specified epsilon-neighborhood. Evaluations are conducted on three nonlinear test functions and a case study of an electric grid stability problem with uncertain renewable power injection. The EDGE algorithm demonstrates superior sample efficiency and better boundary approximation than adaptive sampling techniques and grid-based searches."
  },
  {
    "title": "The Structural Safety Generalization Problem",
    "url": "http://arxiv.org/abs/2504.09712v1",
    "arxiv_id": "2504.09712v1",
    "authors": [
      "Julius Broomfield",
      "Tom Gibbs",
      "Ethan Kosak-Hine",
      "George Ingebretsen",
      "Tia Nasir",
      "Jason Zhang",
      "Reihaneh Iranmanesh",
      "Sara Pieri",
      "Reihaneh Rabbany",
      "Kellin Pelrine"
    ],
    "published": "2025-04-13T20:21:08+00:00",
    "summary": "LLM jailbreaks are a widespread safety challenge. Given this problem has not yet been tractable, we suggest targeting a key failure mechanism: the failure of safety to generalize across semantically equivalent inputs. We further focus the target by requiring desirable tractability properties of attacks to study: explainability, transferability between models, and transferability between goals. We perform red-teaming within this framework by uncovering new vulnerabilities to multi-turn, multi-image, and translation-based attacks. These attacks are semantically equivalent by our design to their single-turn, single-image, or untranslated counterparts, enabling systematic comparisons; we show that the different structures yield different safety outcomes. We then demonstrate the potential for this framework to enable new defenses by proposing a Structure Rewriting Guardrail, which converts an input to a structure more conducive to safety assessment. This guardrail significantly improves refusal of harmful inputs, without over-refusing benign ones. Thus, by framing this intermediate challenge - more tractable than universal defenses but essential for long-term safety - we highlight a critical milestone for AI safety research."
  },
  {
    "title": "EmoAgent: Assessing and Safeguarding Human-AI Interaction for Mental Health Safety",
    "url": "http://arxiv.org/abs/2504.09689v1",
    "arxiv_id": "2504.09689v1",
    "authors": [
      "Jiahao Qiu",
      "Yinghui He",
      "Xinzhe Juan",
      "Yiming Wang",
      "Yuhan Liu",
      "Zixin Yao",
      "Yue Wu",
      "Xun Jiang",
      "Ling Yang",
      "Mengdi Wang"
    ],
    "published": "2025-04-13T18:47:22+00:00",
    "summary": "The rise of LLM-driven AI characters raises safety concerns, particularly for vulnerable human users with psychological disorders. To address these risks, we propose EmoAgent, a multi-agent AI framework designed to evaluate and mitigate mental health hazards in human-AI interactions. EmoAgent comprises two components: EmoEval simulates virtual users, including those portraying mentally vulnerable individuals, to assess mental health changes before and after interactions with AI characters. It uses clinically proven psychological and psychiatric assessment tools (PHQ-9, PDI, PANSS) to evaluate mental risks induced by LLM. EmoGuard serves as an intermediary, monitoring users' mental status, predicting potential harm, and providing corrective feedback to mitigate risks. Experiments conducted in popular character-based chatbots show that emotionally engaging dialogues can lead to psychological deterioration in vulnerable users, with mental state deterioration in more than 34.4% of the simulations. EmoGuard significantly reduces these deterioration rates, underscoring its role in ensuring safer AI-human interactions. Our code is available at: https://github.com/1akaman/EmoAgent"
  },
  {
    "title": "Leveraging Reasoning Model Answers to Enhance Non-Reasoning Model Capability",
    "url": "http://arxiv.org/abs/2504.09639v1",
    "arxiv_id": "2504.09639v1",
    "authors": [
      "Haotian Wang",
      "Han Zhao",
      "Shuaiting Chen",
      "Xiaoyu Tian",
      "Sitong Zhao",
      "Yunjie Ji",
      "Yiping Peng",
      "Xiangang Li"
    ],
    "published": "2025-04-13T16:26:56+00:00",
    "summary": "Recent advancements in large language models (LLMs), such as DeepSeek-R1 and OpenAI-o1, have demonstrated the significant effectiveness of test-time scaling, achieving substantial performance gains across various benchmarks. These advanced models utilize deliberate \"thinking\" steps to systematically enhance answer quality. In this paper, we propose leveraging these high-quality outputs generated by reasoning-intensive models to improve less computationally demanding, non-reasoning models. We explore and compare methodologies for utilizing the answers produced by reasoning models to train and improve non-reasoning models. Through straightforward Supervised Fine-Tuning (SFT) experiments on established benchmarks, we demonstrate consistent improvements across various benchmarks, underscoring the potential of this approach for advancing the ability of models to answer questions directly."
  },
  {
    "title": "Mitigating Many-Shot Jailbreaking",
    "url": "http://arxiv.org/abs/2504.09604v1",
    "arxiv_id": "2504.09604v1",
    "authors": [
      "Christopher M. Ackerman",
      "Nina Panickssery"
    ],
    "published": "2025-04-13T14:42:03+00:00",
    "summary": "Many-shot jailbreaking (MSJ) is an adversarial technique that exploits the long context windows of modern LLMs to circumvent model safety training by including in the prompt many examples of a ``fake'' assistant responding inappropriately before the final request. With enough examples, the model's in-context learning abilities override its safety training, and it responds as if it were the ``fake'' assistant. In this work, we probe the effectiveness of different fine tuning and input sanitization approaches on mitigating MSJ attacks, alone and in combination. We find incremental mitigation effectiveness for each, and we show that the combined techniques significantly reduce the effectiveness of MSJ attacks, while retaining model performance in benign in-context learning and conversational tasks. We suggest that our approach could meaningfully ameliorate this vulnerability if incorporated into model safety post-training."
  },
  {
    "title": "Fine-tuning an Large Language Model for Automating Computational Fluid Dynamics Simulations",
    "url": "http://arxiv.org/abs/2504.09602v1",
    "arxiv_id": "2504.09602v1",
    "authors": [
      "Zhehao Dong",
      "Zhen Lu",
      "Yue Yang"
    ],
    "published": "2025-04-13T14:35:30+00:00",
    "summary": "Configuring computational fluid dynamics (CFD) simulations typically demands extensive domain expertise, limiting broader access. Although large language models (LLMs) have advanced scientific computing, their use in automating CFD workflows is underdeveloped. We introduce a novel approach centered on domain-specific LLM adaptation. By fine-tuning Qwen2.5-7B-Instruct on NL2FOAM, our custom dataset of 28716 natural language-to-OpenFOAM configuration pairs with chain-of-thought (CoT) annotations, we enable direct translation from natural language descriptions to executable CFD setups. A multi-agent framework orchestrates the process, autonomously verifying inputs, generating configurations, running simulations, and correcting errors. Evaluation on a benchmark of 21 diverse flow cases demonstrates state-of-the-art performance, achieving 88.7% solution accuracy and 82.6% first-attempt success rate. This significantly outperforms larger general-purpose models like Qwen2.5-72B-Instruct, DeepSeek-R1, and Llama3.3-70B-Instruct, while also requiring fewer correction iterations and maintaining high computational efficiency. The results highlight the critical role of domain-specific adaptation in deploying LLM assistants for complex engineering workflows."
  },
  {
    "title": "Analysis of Radiation Level and Estimation of Protection Distance of \u03b3 Mobile Flaw Detection Source",
    "url": "http://arxiv.org/abs/2504.09556v1",
    "arxiv_id": "2504.09556v1",
    "authors": [
      "Zhihui Liu",
      "Xiang Hu",
      "Zhengyang Zhang"
    ],
    "published": "2025-04-13T13:05:47+00:00",
    "summary": "Objective To analyze the radiation dose associated with gamma-ray mobile flaw detection, estimate the extent of the supervision and control areas, and assess the associated radiation risks. Methods A combination of theoretical calculations and actual measurements was used to compare and analyze the ambient equivalent dose rates of 192 Ir and 75 Se at their nominal source strengths. Measurements were conducted at distances of 1 m, 2 m, and 5 m from the radiation source. The extents of the control and supervision areas were estimated under three working scenarios: 1 without considering air attenuation, 2 considering air attenuation, and 3 after shielding by the flaw detection workpiece, using source activities of 3.7 * 10^10 Bq and 3.7 * 10^12Bq. Results Actual measurement of radiation dose of 192 Ir and 75 Se were measured under three different nominal activities. Theoretical calculation of radiation dose estimates at various distances were obtained for both nuclides, and the results showed that the theoretical values were basically consistent with the measured values. Conclusion The estimated scope of the supervision and control areas provided in this study can serve as a reference for flaw detection companies. Technicians can use these estimates to calculate appropriate distances for safety zones based on different nuclide activities. This enables flaw detection personnel to reduce the measurement scope on-site and to quickly and accurately define area boundaries."
  },
  {
    "title": "Quality Control and Structural Reliability -- A Unified Framework for Integrating Conformity Assessment and Partial Safety Factors",
    "url": "http://arxiv.org/abs/2504.09508v1",
    "arxiv_id": "2504.09508v1",
    "authors": [
      "Tammam Bakeer",
      "Wolfram Jaeger"
    ],
    "published": "2025-04-13T10:14:34+00:00",
    "summary": "Ensuring structural reliability remains a core concern in civil engineering, yet the quantitative effects of quality control measures on material variability and safety margins are not fully understood, especially for materials other than reinforced concrete. This study addresses this gap by presenting a probabilistic framework that integrates Bayesian updating, acceptance sampling, and operating characteristic (OC) curves to model conformity assessment as a probabilistic filter. In doing so, it refines prior distributions of key material and execution parameters based on quality control outcomes, linking reductions in the coefficient of variation directly to adjustments in partial safety factors. Applying the framework to a masonry wall example demonstrates how systematic quality control efforts, particularly those targeting parameters with higher importance such as masonry unit strength and execution quality-produce substantial gains in structural reliability. The analysis shows that combined quality control measures can lower the partial safety factor from a baseline of 1.5 to about 1.38, corresponding to an improvement factor of roughly 1.09 and material savings of approximately 8%. Conversely, controlling parameters with negligible influence, such as mortar properties, provides limited benefit. These findings encourage focusing quality control resources on the most influential parameters and integrating results into semi-probabilistic design methods. By offering a transparent, standards-compatible approach, the framework supports the refinement of design guidelines, promotes more efficient resource allocation, and enhances overall structural safety in the built environment."
  },
  {
    "title": "AdaSteer: Your Aligned LLM is Inherently an Adaptive Jailbreak Defender",
    "url": "http://arxiv.org/abs/2504.09466v1",
    "arxiv_id": "2504.09466v1",
    "authors": [
      "Weixiang Zhao",
      "Jiahe Guo",
      "Yulin Hu",
      "Yang Deng",
      "An Zhang",
      "Xingyu Sui",
      "Xinyang Han",
      "Yanyan Zhao",
      "Bing Qin",
      "Tat-Seng Chua",
      "Ting Liu"
    ],
    "published": "2025-04-13T07:39:17+00:00",
    "summary": "Despite extensive efforts in safety alignment, large language models (LLMs) remain vulnerable to jailbreak attacks. Activation steering offers a training-free defense method but relies on fixed steering coefficients, resulting in suboptimal protection and increased false rejections of benign inputs. To address this, we propose AdaSteer, an adaptive activation steering method that dynamically adjusts model behavior based on input characteristics. We identify two key properties: Rejection Law (R-Law), which shows that stronger steering is needed for jailbreak inputs opposing the rejection direction, and Harmfulness Law (H-Law), which differentiates adversarial and benign inputs. AdaSteer steers input representations along both the Rejection Direction (RD) and Harmfulness Direction (HD), with adaptive coefficients learned via logistic regression, ensuring robust jailbreak defense while preserving benign input handling. Experiments on LLaMA-3.1, Gemma-2, and Qwen2.5 show that AdaSteer outperforms baseline methods across multiple jailbreak attacks with minimal impact on utility. Our results highlight the potential of interpretable model internals for real-time, flexible safety enforcement in LLMs."
  },
  {
    "title": "ADDT -- A Digital Twin Framework for Proactive Safety Validation in Autonomous Driving Systems",
    "url": "http://arxiv.org/abs/2504.09461v1",
    "arxiv_id": "2504.09461v1",
    "authors": [
      "Bo Yu",
      "Chaoran Yuan",
      "Zishen Wan",
      "Jie Tang",
      "Fadi Kurdahi",
      "Shaoshan Liu"
    ],
    "published": "2025-04-13T07:17:17+00:00",
    "summary": "Autonomous driving systems continue to face safety-critical failures, often triggered by rare and unpredictable corner cases that evade conventional testing. We present the Autonomous Driving Digital Twin (ADDT) framework, a high-fidelity simulation platform designed to proactively identify hidden faults, evaluate real-time performance, and validate safety before deployment. ADDT combines realistic digital models of driving environments, vehicle dynamics, sensor behavior, and fault conditions to enable scalable, scenario-rich stress-testing under diverse and adverse conditions. It supports adaptive exploration of edge cases using reinforcement-driven techniques, uncovering failure modes that physical road testing often misses. By shifting from reactive debugging to proactive simulation-driven validation, ADDT enables a more rigorous and transparent approach to autonomous vehicle safety engineering. To accelerate adoption and facilitate industry-wide safety improvements, the entire ADDT framework has been released as open-source software, providing developers with an accessible and extensible tool for comprehensive safety testing at scale."
  },
  {
    "title": "SaRO: Enhancing LLM Safety through Reasoning-based Alignment",
    "url": "http://arxiv.org/abs/2504.09420v1",
    "arxiv_id": "2504.09420v1",
    "authors": [
      "Yutao Mou",
      "Yuxiao Luo",
      "Shikun Zhang",
      "Wei Ye"
    ],
    "published": "2025-04-13T03:36:06+00:00",
    "summary": "Current safety alignment techniques for large language models (LLMs) face two key challenges: (1) under-generalization, which leaves models vulnerable to novel jailbreak attacks, and (2) over-alignment, which leads to the excessive refusal of benign instructions. Our preliminary investigation reveals semantic overlap between jailbreak/harmful queries and normal prompts in embedding space, suggesting that more effective safety alignment requires a deeper semantic understanding. This motivates us to incorporate safety-policy-driven reasoning into the alignment process. To this end, we propose the Safety-oriented Reasoning Optimization Framework (SaRO), which consists of two stages: (1) Reasoning-style Warmup (RW) that enables LLMs to internalize long-chain reasoning through supervised fine-tuning, and (2) Safety-oriented Reasoning Process Optimization (SRPO) that promotes safety reflection via direct preference optimization (DPO). Extensive experiments demonstrate the superiority of SaRO over traditional alignment methods."
  },
  {
    "title": "Graph-Based Prediction Models for Data Debiasing",
    "url": "http://arxiv.org/abs/2504.09348v1",
    "arxiv_id": "2504.09348v1",
    "authors": [
      "Dongze Wu",
      "Hanyang Jiang",
      "Yao Xie"
    ],
    "published": "2025-04-12T21:34:49+00:00",
    "summary": "Bias in data collection, arising from both under-reporting and over-reporting, poses significant challenges in critical applications such as healthcare and public safety. In this work, we introduce Graph-based Over- and Under-reporting Debiasing (GROUD), a novel graph-based optimization framework that debiases reported data by jointly estimating the true incident counts and the associated reporting bias probabilities. By modeling the bias as a smooth signal over a graph constructed from geophysical or feature-based similarities, our convex formulation not only ensures a unique solution but also comes with theoretical recovery guarantees under certain assumptions. We validate GROUD on both challenging simulated experiments and real-world datasets -- including Atlanta emergency calls and COVID-19 vaccine adverse event reports -- demonstrating its robustness and superior performance in accurately recovering debiased counts. This approach paves the way for more reliable downstream decision-making in systems affected by reporting irregularities."
  },
  {
    "title": "Strain-induced polarization rotation in freestanding ferroelectric oxide membranes",
    "url": "http://arxiv.org/abs/2504.09244v1",
    "arxiv_id": "2504.09244v1",
    "authors": [
      "Alban Degezelle",
      "Razvan Burcea",
      "Pascale Gemeiner",
      "Maxime Vallet",
      "Brahim Dkhil",
      "St\u00e9phane Fusil",
      "Vincent Garcia",
      "Sylvia Matzen",
      "Philippe Lecoeur",
      "Thomas Maroutian"
    ],
    "published": "2025-04-12T14:51:19+00:00",
    "summary": "Freestanding ferroelectric membranes have emerged as a versatile tool for strain engineering, enabling the exploration of ferroelectric properties beyond traditional epitaxy. The resulting ferroelectric domain patterns stem from the balance at the local scale of several effects playing a key role, i.e. piezoelectricity linked to strain, and flexoelectricity arising from strain gradients. To weight their respective contributions for a given membrane geometry, the strain profile has to be mapped with respect to the ferroelectric polarization landscape, a necessary step to allow for a controlled tailoring of the latter. In this study, we examine the effect of bending strain on a Pb(Zr,Ti)O3 membrane in a fold-like structure, observing a polarization rotation from out-of-plane to in-plane at the fold apex. Combining piezoresponse force microscopy, Raman spectroscopy, and scanning transmission electron microscopy, we map the ferroelectric polarization direction relative to the height profile of the membrane, and discuss the contributions of strain and strain gradients for this archetypal fold geometry. Our findings offer new insights into strain-engineered polarization configurations, and emphasize strain effects at the nanoscale to tune the functional properties in freestanding membranes."
  },
  {
    "title": "Graph Learning-Driven Multi-Vessel Association: Fusing Multimodal Data for Maritime Intelligence",
    "url": "http://arxiv.org/abs/2504.09197v1",
    "arxiv_id": "2504.09197v1",
    "authors": [
      "Yuxu Lu",
      "Kaisen Yang",
      "Dong Yang",
      "Haifeng Ding",
      "Jinxian Weng",
      "Ryan Wen Liu"
    ],
    "published": "2025-04-12T12:45:55+00:00",
    "summary": "Ensuring maritime safety and optimizing traffic management in increasingly crowded and complex waterways require effective waterway monitoring. However, current methods struggle with challenges arising from multimodal data, such as dimensional disparities, mismatched target counts, vessel scale variations, occlusions, and asynchronous data streams from systems like the automatic identification system (AIS) and closed-circuit television (CCTV). Traditional multi-target association methods often struggle with these complexities, particularly in densely trafficked waterways. To overcome these issues, we propose a graph learning-driven multi-vessel association (GMvA) method tailored for maritime multimodal data fusion. By integrating AIS and CCTV data, GMvA leverages time series learning and graph neural networks to capture the spatiotemporal features of vessel trajectories effectively. To enhance feature representation, the proposed method incorporates temporal graph attention and spatiotemporal attention, effectively capturing both local and global vessel interactions. Furthermore, a multi-layer perceptron-based uncertainty fusion module computes robust similarity scores, and the Hungarian algorithm is adopted to ensure globally consistent and accurate target matching. Extensive experiments on real-world maritime datasets confirm that GMvA delivers superior accuracy and robustness in multi-target association, outperforming existing methods even in challenging scenarios with high vessel density and incomplete or unevenly distributed AIS and CCTV data."
  },
  {
    "title": "Feature-Aware Malicious Output Detection and Mitigation",
    "url": "http://arxiv.org/abs/2504.09191v1",
    "arxiv_id": "2504.09191v1",
    "authors": [
      "Weilong Dong",
      "Peiguang Li",
      "Yu Tian",
      "Xinyi Zeng",
      "Fengdi Li",
      "Sirui Wang"
    ],
    "published": "2025-04-12T12:12:51+00:00",
    "summary": "The rapid advancement of large language models (LLMs) has brought significant benefits to various domains while introducing substantial risks. Despite being fine-tuned through reinforcement learning, LLMs lack the capability to discern malicious content, limiting their defense against jailbreak. To address these safety concerns, we propose a feature-aware method for harmful response rejection (FMM), which detects the presence of malicious features within the model's feature space and adaptively adjusts the model's rejection mechanism. By employing a simple discriminator, we detect potential malicious traits during the decoding phase. Upon detecting features indicative of toxic tokens, FMM regenerates the current token. By employing activation patching, an additional rejection vector is incorporated during the subsequent token generation, steering the model towards a refusal response. Experimental results demonstrate the effectiveness of our approach across multiple language models and diverse attack techniques, while crucially maintaining the models' standard generation capabilities."
  },
  {
    "title": "Compliant Explicit Reference Governor for Contact Friendly Robotic Manipulators",
    "url": "http://arxiv.org/abs/2504.09188v1",
    "arxiv_id": "2504.09188v1",
    "authors": [
      "Yaashia Gautam",
      "Nataliya Nechyporenko",
      "Chi-Hui Lin",
      "Alessandro Roncone",
      "Marco M. Nicotra"
    ],
    "published": "2025-04-12T12:01:46+00:00",
    "summary": "This paper introduces the Compliant Explicit Reference Governor (C-ERG), an extension of the Explicit Reference Governor that allows the robot to operate safely while in contact with the environment.   The C-ERG is an intermediate layer that can be placed between a high-level planner and a low-level controller: its role is to enforce operational constraints and to enable the smooth transition between free-motion and contact operations. The C-ERG ensures safety by limiting the total energy available to the robotic arm at the time of contact. In the absence of contact, however, the C-ERG does not penalize the system performance.   Numerical examples showcase the behavior of the C-ERG for increasingly complex systems."
  },
  {
    "title": "Dose-finding design based on level set estimation in phase I cancer clinical trials",
    "url": "http://arxiv.org/abs/2504.09157v1",
    "arxiv_id": "2504.09157v1",
    "authors": [
      "Keiichiro Seno",
      "Kota Matsui",
      "Shogo Iwazaki",
      "Yu Inatsu",
      "Shion Takeno",
      "Shigeyuki Matsui"
    ],
    "published": "2025-04-12T09:44:20+00:00",
    "summary": "The primary objective of phase I cancer clinical trials is to evaluate the safety of a new experimental treatment and to find the maximum tolerated dose (MTD). We show that the MTD estimation problem can be regarded as a level set estimation (LSE) problem whose objective is to determine the regions where an unknown function value is above or below a given threshold. Then, we propose a novel dose-finding design in the framework of LSE. The proposed design determines the next dose on the basis of an acquisition function incorporating uncertainty in the posterior distribution of the dose-toxicity curve as well as overdose control. Simulation experiments show that the proposed LSE design achieves a higher accuracy in estimating the MTD and involves a lower risk of overdosing allocation compared to existing designs, thereby indicating that it provides an effective methodology for phase I cancer clinical trial design."
  },
  {
    "title": "Research on the Crystal Growth, Band Structure and Luminescence Mechanism of (CH3NH3)2HgI4",
    "url": "http://arxiv.org/abs/2504.09150v1",
    "arxiv_id": "2504.09150v1",
    "authors": [
      "Linlin Liu",
      "Zuanquan Chen",
      "Wensi Yang",
      "Sen Zhang",
      "Jiaqi Zhu"
    ],
    "published": "2025-04-12T09:30:12+00:00",
    "summary": "Nuclear radiation detectors play a crucial role in fields such as nuclear safety and medical imaging. The core of their performance lies in the selection of detection materials. Semiconductor detectors have become a hot topic in current research due to their advantages such as small size, good energy resolution, and high detection efficiency. As one of the most promising materials for fabricating room - temperature nuclear radiation semiconductor detectors, HgI2 exhibits excellent detection performance due to its high atomic number, large band gap, strong ray - stopping power, and high volume dark resistivity. However, issues such as poor chemical stability and low vacancy mobility of HgI2 limit its development. Therefore, researchers have carried out inorganic doping/organic hybridization on it. By introducing the organic ligand CH3NH3I, the synthesis of organic - inorganic hybrid compounds based on HgI2 is expected to significantly improve the stability of HgI2. Research on organic - inorganic hybrid metal halide crystals shows that this material has great application potential in the field of luminescent materials."
  },
  {
    "title": "Can Large Language Models Become Policy Refinement Partners? Evidence from China's Social Security Studies",
    "url": "http://arxiv.org/abs/2504.09137v1",
    "arxiv_id": "2504.09137v1",
    "authors": [
      "Ke Jinghan",
      "Zhou Zheng",
      "Zhao Yuxuan"
    ],
    "published": "2025-04-12T08:50:12+00:00",
    "summary": "The rapid development of large language models (LLMs) is reshaping operational paradigms across multidisciplinary domains. LLMs' emergent capability to synthesize policy-relevant insights across disciplinary boundaries suggests potential as decision-support tools. However, their actual performance and suitability as policy refinement partners still require verification through rigorous and systematic evaluations. Our study employs the context-embedded generation-adaptation framework to conduct a tripartite comparison among the American GPT-4o, the Chinese DeepSeek-R1 and human researchers, investigating the capability boundaries and performance characteristics of LLMs in generating policy recommendations for China's social security issues. This study demonstrates that while large LLMs exhibit distinct advantages in systematic policy design, they face significant limitations in addressing complex social dynamics, balancing stakeholder interests, and controlling fiscal risks within the social security domain. Furthermore, DeepSeek-R1 demonstrates superior performance to GPT-4o across all evaluation dimensions in policy recommendation generation, illustrating the potential of localized training to improve contextual alignment. These findings suggest that regionally-adapted LLMs can function as supplementary tools for generating diverse policy alternatives informed by domain-specific social insights. Nevertheless, the formulation of policy refinement requires integration with human researchers' expertise, which remains critical for interpreting institutional frameworks, cultural norms, and value systems."
  },
  {
    "title": "Can Large Language Models Become Policy Refinement Partners? Evidence from China's Social Security Studies",
    "url": "http://arxiv.org/abs/2504.09137v2",
    "arxiv_id": "2504.09137v2",
    "authors": [
      "Ke Jinghan",
      "Zhou Zheng",
      "Zhao Yuxuan"
    ],
    "published": "2025-04-12T08:50:12+00:00",
    "summary": "The rapid development of large language models (LLMs) is reshaping operational paradigms across multidisciplinary domains. LLMs' emergent capability to synthesize policy-relevant insights across disciplinary boundaries suggests potential as decision-support tools. However, their actual performance and suitability as policy refinement partners still require verification through rigorous and systematic evaluations. Our study employs the context-embedded generation-adaptation framework to conduct a tripartite comparison among the American GPT-4o, the Chinese DeepSeek-R1 and human researchers, investigating the capability boundaries and performance characteristics of LLMs in generating policy recommendations for China's social security issues. This study demonstrates that while LLMs exhibit distinct advantages in systematic policy design, they face significant limitations in addressing complex social dynamics, balancing stakeholder interests, and controlling fiscal risks within the social security domain. Furthermore, DeepSeek-R1 demonstrates superior performance to GPT-4o across all evaluation dimensions in policy recommendation generation, illustrating the potential of localized training to improve contextual alignment. These findings suggest that regionally-adapted LLMs can function as supplementary tools for generating diverse policy alternatives informed by domain-specific social insights. Nevertheless, the formulation of policy refinement requires integration with human researchers' expertise, which remains critical for interpreting institutional frameworks, cultural norms, and value systems."
  },
  {
    "title": "BiFlex: A Passive Bimodal Stiffness Flexible Wrist for Manipulation in Unstructured Environments",
    "url": "http://arxiv.org/abs/2504.08706v1",
    "arxiv_id": "2504.08706v1",
    "authors": [
      "Gu-Cheol Jeong",
      "Stefano Dalla Gasperina",
      "Ashish D. Deshpande",
      "Lillian Chin",
      "Roberto Mart\u00edn-Mart\u00edn"
    ],
    "published": "2025-04-11T17:16:13+00:00",
    "summary": "Robotic manipulation in unstructured, humancentric environments poses a dual challenge: achieving the precision need for delicate free-space operation while ensuring safety during unexpected contact events. Traditional wrists struggle to balance these demands, often relying on complex control schemes or complicated mechanical designs to mitigate potential damage from force overload. In response, we present BiFlex, a flexible robotic wrist that uses a soft buckling honeycomb structure to provides a natural bimodal stiffness response. The higher stiffness mode enables precise household object manipulation, while the lower stiffness mode provides the compliance needed to adapt to external forces. We design BiFlex to maintain a fingertip deflection of less than 1 cm while supporting loads up to 500g and create a BiFlex wrist for many grippers, including Panda, Robotiq, and BaRiFlex. We validate BiFlex under several real-world experimental evaluations, including surface wiping, precise pick-and-place, and grasping under environmental constraints. We demonstrate that BiFlex simplifies control while maintaining precise object manipulation and enhanced safety in real-world applications."
  },
  {
    "title": "Offline Reinforcement Learning using Human-Aligned Reward Labeling for Autonomous Emergency Braking in Occluded Pedestrian Crossing",
    "url": "http://arxiv.org/abs/2504.08704v1",
    "arxiv_id": "2504.08704v1",
    "authors": [
      "Vinal Asodia",
      "Zhenhua Feng",
      "Saber Fallah"
    ],
    "published": "2025-04-11T17:11:21+00:00",
    "summary": "Effective leveraging of real-world driving datasets is crucial for enhancing the training of autonomous driving systems. While Offline Reinforcement Learning enables the training of autonomous vehicles using such data, most available datasets lack meaningful reward labels. Reward labeling is essential as it provides feedback for the learning algorithm to distinguish between desirable and undesirable behaviors, thereby improving policy performance. This paper presents a novel pipeline for generating human-aligned reward labels. The proposed approach addresses the challenge of absent reward signals in real-world datasets by generating labels that reflect human judgment and safety considerations. The pipeline incorporates an adaptive safety component, activated by analyzing semantic segmentation maps, allowing the autonomous vehicle to prioritize safety over efficiency in potential collision scenarios. The proposed pipeline is applied to an occluded pedestrian crossing scenario with varying levels of pedestrian traffic, using synthetic and simulation data. The results indicate that the generated reward labels closely match the simulation reward labels. When used to train the driving policy using Behavior Proximal Policy Optimisation, the results are competitive with other baselines. This demonstrates the effectiveness of our method in producing reliable and human-aligned reward signals, facilitating the training of autonomous driving systems through Reinforcement Learning outside of simulation environments and in alignment with human values."
  },
  {
    "title": "Safe Flow Matching: Robot Motion Planning with Control Barrier Functions",
    "url": "http://arxiv.org/abs/2504.08661v1",
    "arxiv_id": "2504.08661v1",
    "authors": [
      "Xiaobing Dai",
      "Dian Yu",
      "Shanshan Zhang",
      "Zewen Yang"
    ],
    "published": "2025-04-11T16:10:58+00:00",
    "summary": "Recent advances in generative modeling have led to promising results in robot motion planning, particularly through diffusion and flow-based models that capture complex, multimodal trajectory distributions. However, these methods are typically trained offline and remain limited when faced with unseen environments or dynamic constraints, often lacking explicit mechanisms to ensure safety during deployment. In this work, we propose, Safe Flow Matching (SafeFM), a motion planning approach for trajectory generation that integrates flow matching with safety guarantees. By incorporating the proposed flow matching barrier functions, SafeFM ensures that generated trajectories remain within safe regions throughout the planning horizon, even in the presence of previously unseen obstacles or state-action constraints. Unlike diffusion-based approaches, our method allows for direct, efficient sampling of constraint-satisfying trajectories, making it well-suited for real-time motion planning. We evaluate SafeFM on a diverse set of tasks, including planar robot navigation and 7-DoF manipulation, demonstrating superior safety, generalization, and planning performance compared to state-of-the-art generative planners. Comprehensive resources are available on the project website: https://safeflowmatching.github.io/SafeFM/"
  },
  {
    "title": "TinyCenterSpeed: Efficient Center-Based Object Detection for Autonomous Racing",
    "url": "http://arxiv.org/abs/2504.08655v1",
    "arxiv_id": "2504.08655v1",
    "authors": [
      "Neil Reichlin",
      "Nicolas Baumann",
      "Edoardo Ghignone",
      "Michele Magno"
    ],
    "published": "2025-04-11T15:58:46+00:00",
    "summary": "Perception within autonomous driving is nearly synonymous with Neural Networks (NNs). Yet, the domain of autonomous racing is often characterized by scaled, computationally limited robots used for cost-effectiveness and safety. For this reason, opponent detection and tracking systems typically resort to traditional computer vision techniques due to computational constraints. This paper introduces TinyCenterSpeed, a streamlined adaptation of the seminal CenterPoint method, optimized for real-time performance on 1:10 scale autonomous racing platforms. This adaptation is viable even on OBCs powered solely by Central Processing Units (CPUs), as it incorporates the use of an external Tensor Processing Unit (TPU). We demonstrate that, compared to Adaptive Breakpoint Detector (ABD), the current State-of-the-Art (SotA) in scaled autonomous racing, TinyCenterSpeed not only improves detection and velocity estimation by up to 61.38% but also supports multi-opponent detection and estimation. It achieves real-time performance with an inference time of just 7.88 ms on the TPU, significantly reducing CPU utilization 8.3-fold."
  },
  {
    "title": "Deep Learning Methods for Detecting Thermal Runaway Events in Battery Production Lines",
    "url": "http://arxiv.org/abs/2504.08632v1",
    "arxiv_id": "2504.08632v1",
    "authors": [
      "Athanasios Athanasopoulos",
      "Mat\u00fa\u0161 Mihal\u00e1k",
      "Marcin Pietrasik"
    ],
    "published": "2025-04-11T15:35:50+00:00",
    "summary": "One of the key safety considerations of battery manufacturing is thermal runaway, the uncontrolled increase in temperature which can lead to fires, explosions, and emissions of toxic gasses. As such, development of automated systems capable of detecting such events is of considerable importance in both academic and industrial contexts. In this work, we investigate the use of deep learning for detecting thermal runaway in the battery production line of VDL Nedcar, a Dutch automobile manufacturer. Specifically, we collect data from the production line to represent both baseline (non thermal runaway) and thermal runaway conditions. Thermal runaway was simulated through the use of external heat and smoke sources. The data consisted of both optical and thermal images which were then preprocessed and fused before serving as input to our models. In this regard, we evaluated three deep-learning models widely used in computer vision including shallow convolutional neural networks, residual neural networks, and vision transformers on two performance metrics. Furthermore, we evaluated these models using explainability methods to gain insight into their ability to capture the relevant feature information from their inputs. The obtained results indicate that the use of deep learning is a viable approach to thermal runaway detection in battery production lines."
  },
  {
    "title": "Enabling Safety for Aerial Robots: Planning and Control Architectures",
    "url": "http://arxiv.org/abs/2504.08601v1",
    "arxiv_id": "2504.08601v1",
    "authors": [
      "Kaleb Ben Naveed",
      "Devansh R. Agrawal",
      "Daniel M. Cherenson",
      "Haejoon Lee",
      "Alia Gilbert",
      "Hardik Parwana",
      "Vishnu S. Chipade",
      "William Bentz",
      "Dimitra Panagou"
    ],
    "published": "2025-04-11T15:05:31+00:00",
    "summary": "Ensuring safe autonomy is crucial for deploying aerial robots in real-world applications. However, safety is a multifaceted challenge that must be addressed from multiple perspectives, including navigation in dynamic environments, operation under resource constraints, and robustness against adversarial attacks and uncertainties. In this paper, we present the authors' recent work that tackles some of these challenges and highlights key aspects that must be considered to enhance the safety and performance of autonomous aerial systems. All presented approaches are validated through hardware experiments."
  },
  {
    "title": "Secondary Safety Control for Systems with Sector Bounded Nonlinearities",
    "url": "http://arxiv.org/abs/2504.08535v1",
    "arxiv_id": "2504.08535v1",
    "authors": [
      "Yankai Lin",
      "Michelle S. Chong",
      "Carlos Murguia"
    ],
    "published": "2025-04-11T13:44:22+00:00",
    "summary": "We consider the problem of safety verification and safety-aware controller synthesis for systems with sector bounded nonlinearities. We aim to keep the states of the system within a given safe set under potential actuator and sensor attacks. Specifically, we adopt the setup that a controller has already been designed to stabilize the plant. Using invariant sets and barrier certificate theory, we first give sufficient conditions to verify the safety of the closed-loop system under attacks. Furthermore, by using a subset of sensors that are assumed to be free of attacks, we provide a synthesis method for a secondary controller that enhances the safety of the system. The sufficient conditions to verify safety are derived using Lyapunov-based tools and the S-procedure. Using the projection lemma, the conditions are then formulated as linear matrix inequality (LMI) problems which can be solved efficiently. Lastly, our theoretical results are illustrated through numerical simulations."
  },
  {
    "title": "Secondary Safety Control for Systems with Sector Bounded Nonlinearities [Extended Version]",
    "url": "http://arxiv.org/abs/2504.08535v2",
    "arxiv_id": "2504.08535v2",
    "authors": [
      "Yankai Lin",
      "Michelle S. Chong",
      "Carlos Murguia"
    ],
    "published": "2025-04-11T13:44:22+00:00",
    "summary": "We consider the problem of safety verification and safety-aware controller synthesis for systems with sector bounded nonlinearities. We aim to keep the states of the system within a given safe set under potential actuator and sensor attacks. Specifically, we adopt the setup that a controller has already been designed to stabilize the plant. Using invariant sets and barrier certificate theory, we first give sufficient conditions to verify the safety of the closed-loop system under attacks. Furthermore, by using a subset of sensors that are assumed to be free of attacks, we provide a synthesis method for a secondary controller that enhances the safety of the system. The sufficient conditions to verify safety are derived using Lyapunov-based tools and the S-procedure. Using the projection lemma, the conditions are then formulated as linear matrix inequality (LMI) problems which can be solved efficiently. Lastly, our theoretical results are illustrated through numerical simulations."
  },
  {
    "title": "Physics-informed data-driven control without persistence of excitation",
    "url": "http://arxiv.org/abs/2504.08484v1",
    "arxiv_id": "2504.08484v1",
    "authors": [
      "Martina Vanelli",
      "Julien M. Hendrickx"
    ],
    "published": "2025-04-11T12:19:51+00:00",
    "summary": "We show that data that is not sufficiently informative to allow for system re-identification can still provide meaningful information when combined with external or physical knowledge of the system, such as bounded system matrix norms. We then illustrate how this information can be leveraged for safety and energy minimization problems and to enhance predictions in unmodelled dynamics. This preliminary work outlines key ideas toward using limited data for effective control by integrating physical knowledge of the system and exploiting interpolation conditions."
  },
  {
    "title": "Constrained Machine Learning Through Hyperspherical Representation",
    "url": "http://arxiv.org/abs/2504.08415v1",
    "arxiv_id": "2504.08415v1",
    "authors": [
      "Gaetano Signorelli",
      "Michele Lombardi"
    ],
    "published": "2025-04-11T10:19:49+00:00",
    "summary": "The problem of ensuring constraints satisfaction on the output of machine learning models is critical for many applications, especially in safety-critical domains. Modern approaches rely on penalty-based methods at training time, which do not guarantee to avoid constraints violations; or constraint-specific model architectures (e.g., for monotonocity); or on output projection, which requires to solve an optimization problem that might be computationally demanding. We present the Hypersherical Constrained Representation, a novel method to enforce constraints in the output space for convex and bounded feasibility regions (generalizable to star domains). Our method operates on a different representation system, where Euclidean coordinates are converted into hyperspherical coordinates relative to the constrained region, which can only inherently represent feasible points. Experiments on a synthetic and a real-world dataset show that our method has predictive performance comparable to the other approaches, can guarantee 100% constraint satisfaction, and has a minimal computational cost at inference time."
  },
  {
    "title": "Evaluating Pedestrian Risks in Shared Spaces Through Autonomous Vehicle Experiments on a Fixed Track",
    "url": "http://arxiv.org/abs/2504.08316v1",
    "arxiv_id": "2504.08316v1",
    "authors": [
      "Enrico Del Re",
      "Novel Certad",
      "Cristina Olaverri-Monreal"
    ],
    "published": "2025-04-11T07:37:15+00:00",
    "summary": "The majority of research on safety in autonomous vehicles has been conducted in structured and controlled environments. However, there is a scarcity of research on safety in unregulated pedestrian areas, especially when interacting with public transport vehicles like trams. This study investigates pedestrian responses to an alert system in this context by replicating this real-world scenario in an environment using an autonomous vehicle. The results show that safety measures from other contexts can be adapted to shared spaces with trams, where fixed tracks heighten risks in unregulated crossings."
  },
  {
    "title": "SortBench: Benchmarking LLMs based on their ability to sort lists",
    "url": "http://arxiv.org/abs/2504.08312v1",
    "arxiv_id": "2504.08312v1",
    "authors": [
      "Steffen Herbold"
    ],
    "published": "2025-04-11T07:29:56+00:00",
    "summary": "Sorting is a tedious but simple task for human intelligence and can be solved fairly easily algorithmically. However, for Large Language Models (LLMs) this task is surprisingly hard, as some properties of sorting are among known weaknesses of LLMs: being faithful to the input data, logical comparisons between values, and strictly differentiating between syntax (used for sorting) and semantics (typically learned by embeddings). Within this paper, we describe the new SortBench benchmark for LLMs that comes with different difficulties and that can be easily scaled in terms of difficulty. We apply this benchmark to seven state-of-the-art LLMs, including current test-time reasoning models. Our results show that while the o3-mini model is very capable at sorting in general, even this can be fooled if strings are defined to mix syntactical and semantical aspects, e.g., by asking to sort numbers written-out as word. Furthermore, all models have problems with the faithfulness to the input of long lists, i.e., they drop items and add new ones. Our results also show that test-time reasoning has a tendency to overthink problems which leads to performance degradation. Finally, models without test-time reasoning like GPT-4o are not much worse than reasoning models."
  },
  {
    "title": "Evaluating the Bias in LLMs for Surveying Opinion and Decision Making in Healthcare",
    "url": "http://arxiv.org/abs/2504.08260v1",
    "arxiv_id": "2504.08260v1",
    "authors": [
      "Yonchanok Khaokaew",
      "Flora D. Salim",
      "Andreas Z\u00fcfle",
      "Hao Xue",
      "Taylor Anderson",
      "Matthew Scotch",
      "David J Heslop"
    ],
    "published": "2025-04-11T05:11:40+00:00",
    "summary": "Generative agents have been increasingly used to simulate human behaviour in silico, driven by large language models (LLMs). These simulacra serve as sandboxes for studying human behaviour without compromising privacy or safety. However, it remains unclear whether such agents can truly represent real individuals. This work compares survey data from the Understanding America Study (UAS) on healthcare decision-making with simulated responses from generative agents. Using demographic-based prompt engineering, we create digital twins of survey respondents and analyse how well different LLMs reproduce real-world behaviours. Our findings show that some LLMs fail to reflect realistic decision-making, such as predicting universal vaccine acceptance. However, Llama 3 captures variations across race and Income more accurately but also introduces biases not present in the UAS data. This study highlights the potential of generative agents for behavioural research while underscoring the risks of bias from both LLMs and prompting strategies."
  },
  {
    "title": "Neural Network-assisted Interval Reachability for Systems with Control Barrier Function-Based Safe Controllers",
    "url": "http://arxiv.org/abs/2504.08249v1",
    "arxiv_id": "2504.08249v1",
    "authors": [
      "Damola Ajeyemi",
      "Saber Jafarpour",
      "Emiliano Dall'Anese"
    ],
    "published": "2025-04-11T04:14:55+00:00",
    "summary": "Control Barrier Functions (CBFs) have been widely utilized in the design of optimization-based controllers and filters for dynamical systems to ensure forward invariance of a given set of safe states. While CBF-based controllers offer safety guarantees, they can compromise the performance of the system, leading to undesirable behaviors such as unbounded trajectories and emergence of locally stable spurious equilibria. Computing reachable sets for systems with CBF-based controllers is an effective approach for runtime performance and stability verification, and can potentially serve as a tool for trajectory re-planning. In this paper, we propose a computationally efficient interval reachability method for performance verification of systems with optimization-based controllers by: (i) approximating the optimization-based controller by a pre-trained neural network to avoid solving optimization problems repeatedly, and (ii) using mixed monotone theory to construct an embedding system that leverages state-of-the-art neural network verification algorithms for bounding the output of the neural network. Results in terms of closeness of solutions of trajectories of the system with the optimization-based controller and the neural network are derived. Using a single trajectory of the embedding system along with our closeness of solutions result, we obtain an over-approximation of the reachable set of the system with optimization-based controllers. Numerical results are presented to corroborate the technical findings."
  },
  {
    "title": "InSPE: Rapid Evaluation of Heterogeneous Multi-Modal Infrastructure Sensor Placement",
    "url": "http://arxiv.org/abs/2504.08240v1",
    "arxiv_id": "2504.08240v1",
    "authors": [
      "Zhaoliang Zheng",
      "Yun Zhang",
      "Zongling Meng",
      "Johnson Liu",
      "Xin Xia",
      "Jiaqi Ma"
    ],
    "published": "2025-04-11T03:55:00+00:00",
    "summary": "Infrastructure sensing is vital for traffic monitoring at safety hotspots (e.g., intersections) and serves as the backbone of cooperative perception in autonomous driving. While vehicle sensing has been extensively studied, infrastructure sensing has received little attention, especially given the unique challenges of diverse intersection geometries, complex occlusions, varying traffic conditions, and ambient environments like lighting and weather. To address these issues and ensure cost-effective sensor placement, we propose Heterogeneous Multi-Modal Infrastructure Sensor Placement Evaluation (InSPE), a perception surrogate metric set that rapidly assesses perception effectiveness across diverse infrastructure and environmental scenarios with combinations of multi-modal sensors. InSPE systematically evaluates perception capabilities by integrating three carefully designed metrics, i.e., sensor coverage, perception occlusion, and information gain. To support large-scale evaluation, we develop a data generation tool within the CARLA simulator and also introduce Infra-Set, a dataset covering diverse intersection types and environmental conditions. Benchmarking experiments with state-of-the-art perception algorithms demonstrate that InSPE enables efficient and scalable sensor placement analysis, providing a robust solution for optimizing intelligent intersection infrastructure."
  },
  {
    "title": "Advancing Autonomous Vehicle Safety: A Combined Fault Tree Analysis and Bayesian Network Approach",
    "url": "http://arxiv.org/abs/2504.08206v1",
    "arxiv_id": "2504.08206v1",
    "authors": [
      "Lansu Dai",
      "Burak Kantarci"
    ],
    "published": "2025-04-11T02:17:10+00:00",
    "summary": "This paper integrates Fault Tree Analysis (FTA) and Bayesian Networks (BN) to assess collision risk and establish Automotive Safety Integrity Level (ASIL) B failure rate targets for critical autonomous vehicle (AV) components. The FTA-BN integration combines the systematic decomposition of failure events provided by FTA with the probabilistic reasoning capabilities of BN, which allow for dynamic updates in failure probabilities, enhancing the adaptability of risk assessment. A fault tree is constructed based on AV subsystem architecture, with collision as the top event, and failure rates are assigned while ensuring the total remains within 100 FIT. Bayesian inference is applied to update posterior probabilities, and the results indicate that perception system failures (46.06 FIT) are the most significant contributor, particularly failures to detect existing objects (PF5) and misclassification (PF6). Mitigation strategies are proposed for sensors, perception, decision-making, and motion control to reduce the collision risk. The FTA-BN integration approach provides dynamic risk quantification, offering system designers refined failure rate targets to improve AV safety."
  },
  {
    "title": "EO-VLM: VLM-Guided Energy Overload Attacks on Vision Models",
    "url": "http://arxiv.org/abs/2504.08205v1",
    "arxiv_id": "2504.08205v1",
    "authors": [
      "Minjae Seo",
      "Myoungsung You",
      "Junhee Lee",
      "Jaehan Kim",
      "Hwanjo Heo",
      "Jintae Oh",
      "Jinwoo Kim"
    ],
    "published": "2025-04-11T02:13:24+00:00",
    "summary": "Vision models are increasingly deployed in critical applications such as autonomous driving and CCTV monitoring, yet they remain susceptible to resource-consuming attacks. In this paper, we introduce a novel energy-overloading attack that leverages vision language model (VLM) prompts to generate adversarial images targeting vision models. These images, though imperceptible to the human eye, significantly increase GPU energy consumption across various vision models, threatening the availability of these systems. Our framework, EO-VLM (Energy Overload via VLM), is model-agnostic, meaning it is not limited by the architecture or type of the target vision model. By exploiting the lack of safety filters in VLMs like DALL-E 3, we create adversarial noise images without requiring prior knowledge or internal structure of the target vision models. Our experiments demonstrate up to a 50% increase in energy consumption, revealing a critical vulnerability in current vision models."
  },
  {
    "title": "SAEs $\\textit{Can}$ Improve Unlearning: Dynamic Sparse Autoencoder Guardrails for Precision Unlearning in LLMs",
    "url": "http://arxiv.org/abs/2504.08192v1",
    "arxiv_id": "2504.08192v1",
    "authors": [
      "Aashiq Muhamed",
      "Jacopo Bonato",
      "Mona Diab",
      "Virginia Smith"
    ],
    "published": "2025-04-11T01:24:03+00:00",
    "summary": "Machine unlearning is a promising approach to improve LLM safety by removing unwanted knowledge from the model. However, prevailing gradient-based unlearning methods suffer from issues such as high computational costs, hyperparameter instability, poor sequential unlearning capability, vulnerability to relearning attacks, low data efficiency, and lack of interpretability. While Sparse Autoencoders are well-suited to improve these aspects by enabling targeted activation-based unlearning, prior approaches underperform gradient-based methods. This work demonstrates that, contrary to these earlier findings, SAEs can significantly improve unlearning when employed dynamically. We introduce $\\textbf{Dynamic DAE Guardrails}$ (DSG), a novel method for precision unlearning that leverages principled feature selection and a dynamic classifier. Our experiments show DSG substantially outperforms leading unlearning methods, achieving superior forget-utility trade-offs. DSG addresses key drawbacks of gradient-based approaches for unlearning -- offering enhanced computational efficiency and stability, robust performance in sequential unlearning, stronger resistance to relearning attacks, better data efficiency including zero-shot settings, and more interpretable unlearning."
  },
  {
    "title": "Safe Data-Driven Predictive Control",
    "url": "http://arxiv.org/abs/2504.08188v1",
    "arxiv_id": "2504.08188v1",
    "authors": [
      "Amin Vahidi-Moghaddam",
      "Kaian Chen",
      "Kaixiang Zhang",
      "Zhaojian Li",
      "Yan Wang",
      "Kai Wu"
    ],
    "published": "2025-04-11T01:08:21+00:00",
    "summary": "In the realm of control systems, model predictive control (MPC) has exhibited remarkable potential; however, its reliance on accurate models and substantial computational resources has hindered its broader application, especially within real-time nonlinear systems. This study presents an innovative control framework to enhance the practical viability of the MPC. The developed safe data-driven predictive control aims to eliminate the requirement for precise models and alleviate computational burdens in the nonlinear MPC (NMPC). This is achieved by learning both the system dynamics and the control policy, enabling efficient data-driven predictive control while ensuring system safety. The methodology involves a spatial temporal filter (STF)-based concurrent learning for system identification, a robust control barrier function (RCBF) to ensure the system safety amid model uncertainties, and a RCBF-based NMPC policy approximation. An online policy correction mechanism is also introduced to counteract performance degradation caused by the existing model uncertainties. Demonstrated through simulations on two applications, the proposed approach offers comparable performance to existing benchmarks with significantly reduced computational costs."
  },
  {
    "title": "Investigating Vision-Language Model for Point Cloud-based Vehicle Classification",
    "url": "http://arxiv.org/abs/2504.08154v1",
    "arxiv_id": "2504.08154v1",
    "authors": [
      "Yiqiao Li",
      "Jie Wei",
      "Camille Kamga"
    ],
    "published": "2025-04-10T22:37:27+00:00",
    "summary": "Heavy-duty trucks pose significant safety challenges due to their large size and limited maneuverability compared to passenger vehicles. A deeper understanding of truck characteristics is essential for enhancing the safety perspective of cooperative autonomous driving. Traditional LiDAR-based truck classification methods rely on extensive manual annotations, which makes them labor-intensive and costly. The rapid advancement of large language models (LLMs) trained on massive datasets presents an opportunity to leverage their few-shot learning capabilities for truck classification. However, existing vision-language models (VLMs) are primarily trained on image datasets, which makes it challenging to directly process point cloud data. This study introduces a novel framework that integrates roadside LiDAR point cloud data with VLMs to facilitate efficient and accurate truck classification, which supports cooperative and safe driving environments. This study introduces three key innovations: (1) leveraging real-world LiDAR datasets for model development, (2) designing a preprocessing pipeline to adapt point cloud data for VLM input, including point cloud registration for dense 3D rendering and mathematical morphological techniques to enhance feature representation, and (3) utilizing in-context learning with few-shot prompting to enable vehicle classification with minimally labeled training data. Experimental results demonstrate encouraging performance of this method and present its potential to reduce annotation efforts while improving classification accuracy."
  },
  {
    "title": "Development and Performance Analysis of Glass-Based Gas-Tight RPCs for Muography Applications",
    "url": "http://arxiv.org/abs/2504.08146v1",
    "arxiv_id": "2504.08146v1",
    "authors": [
      "S. Ikram",
      "S. Basnet",
      "E. Cortina Gil",
      "P. Demin",
      "R. M. I. D. Gamage",
      "A. Giammanco",
      "R. Karnam",
      "V. K. S. Kashyap",
      "V. Kumar",
      "B. Mohanty",
      "M. Moussawi",
      "A. Samalan",
      "M. Tytgat"
    ],
    "published": "2025-04-10T22:14:32+00:00",
    "summary": "To achieve high-resolution muography of compact targets in scenarios with complex logistical constraints, we are developing a portable muon detector system utilizing glass Resistive Plate Chambers (RPCs). Although RPCs are well understood and widely used, our work focuses on developing a gas-tight variant specifically tailored for a broad range of muography applications, with key design goals including portability, robustness, autonomy, versatility, safety, and cost-effectiveness. Our RPC detectors are designed with various configurations, each featuring unique characteristics and performance attributes. We investigate the temporal evolution of the surface resistivity of glass electrodes, as well as the detector efficiency at varying voltages and thresholds, over a span of several months. These RPCs have been utilized in a small-scale feasibility study on muon absorption using lead blocks."
  },
  {
    "title": "Certified to Drive: A Policy Proposal for Mandatory Training on Semi-Automated Vehicles",
    "url": "http://arxiv.org/abs/2504.08128v1",
    "arxiv_id": "2504.08128v1",
    "authors": [
      "Soumita Mukherjee",
      "Varun Darshana Parekh",
      "Nikhil Tayal"
    ],
    "published": "2025-04-10T21:11:31+00:00",
    "summary": "Although the Boeing 737 Max incidents resulted from a mix of design shortcomings, regulatory oversights, and systemic issues, they also highlight a critical gap in pilot training on managing automated systems during abnormal conditions. This example demonstrates the urgent need for focused, concise training on human-automation interaction - a need that is equally critical for operators of Level 2 ADAS-equipped vehicles, as discussed in detail later in this article. The lack of structured education for semi-automated vehicle operators mirrors similar risks in other industries, where formal training is critical for safe operation. Two policy recommendations are proposed. First, governments should create concise, official resources in accessible and official format to educate drivers on system capabilities and limitations. Second, mandatory training and certification programs should be introduced, combining theoretical and hands-on components to prepare drivers for real-world scenarios. These measures will improve driver understanding, reduce misuse, and foster public trust in semi-automated vehicle technologies. By addressing the knowledge gap, policymakers can ensure a safer, more responsible transition to automation, maximizing its benefits while minimizing risks to public safety."
  },
  {
    "title": "DeepSeek vs. o3-mini: How Well can Reasoning LLMs Evaluate MT and Summarization?",
    "url": "http://arxiv.org/abs/2504.08120v1",
    "arxiv_id": "2504.08120v1",
    "authors": [
      "Daniil Larionov",
      "Sotaro Takeshita",
      "Ran Zhang",
      "Yanran Chen",
      "Christoph Leiter",
      "Zhipin Wang",
      "Christian Greisinger",
      "Steffen Eger"
    ],
    "published": "2025-04-10T20:39:18+00:00",
    "summary": "Reasoning-enabled large language models (LLMs) have recently demonstrated impressive performance in complex logical and mathematical tasks, yet their effectiveness in evaluating natural language generation remains unexplored. This study systematically compares reasoning-based LLMs (DeepSeek-R1 and OpenAI o3) with their non-reasoning counterparts across machine translation (MT) and text summarization (TS) evaluation tasks. We evaluate eight models across three architectural categories, including state-of-the-art reasoning models, their distilled variants (ranging from 8B to 70B parameters), and equivalent conventional, non-reasoning LLMs. Our experiments on WMT23 and SummEval benchmarks reveal that the benefits of reasoning capabilities are highly model and task-dependent: while OpenAI o3-mini models show consistent performance improvements with increased reasoning intensity, DeepSeek-R1 underperforms compared to its non-reasoning variant, with exception to certain aspects of TS evaluation. Correlation analysis demonstrates that increased reasoning token usage positively correlates with evaluation quality in o3-mini models. Furthermore, our results show that distillation of reasoning capabilities maintains reasonable performance in medium-sized models (32B) but degrades substantially in smaller variants (8B). This work provides the first comprehensive assessment of reasoning LLMs for NLG evaluation and offers insights into their practical use."
  },
  {
    "title": "Geneshift: Impact of different scenario shift on Jailbreaking LLM",
    "url": "http://arxiv.org/abs/2504.08104v1",
    "arxiv_id": "2504.08104v1",
    "authors": [
      "Tianyi Wu",
      "Zhiwei Xue",
      "Yue Liu",
      "Jiaheng Zhang",
      "Bryan Hooi",
      "See-Kiong Ng"
    ],
    "published": "2025-04-10T20:02:35+00:00",
    "summary": "Jailbreak attacks, which aim to cause LLMs to perform unrestricted behaviors, have become a critical and challenging direction in AI safety. Despite achieving the promising attack success rate using dictionary-based evaluation, existing jailbreak attack methods fail to output detailed contents to satisfy the harmful request, leading to poor performance on GPT-based evaluation. To this end, we propose a black-box jailbreak attack termed GeneShift, by using a genetic algorithm to optimize the scenario shifts. Firstly, we observe that the malicious queries perform optimally under different scenario shifts. Based on it, we develop a genetic algorithm to evolve and select the hybrid of scenario shifts. It guides our method to elicit detailed and actionable harmful responses while keeping the seemingly benign facade, improving stealthiness. Extensive experiments demonstrate the superiority of GeneShift. Notably, GeneShift increases the jailbreak success rate from 0% to 60% when direct prompting alone would fail."
  },
  {
    "title": "The AI Scientist-v2: Workshop-Level Automated Scientific Discovery via Agentic Tree Search",
    "url": "http://arxiv.org/abs/2504.08066v1",
    "arxiv_id": "2504.08066v1",
    "authors": [
      "Yutaro Yamada",
      "Robert Tjarko Lange",
      "Cong Lu",
      "Shengran Hu",
      "Chris Lu",
      "Jakob Foerster",
      "Jeff Clune",
      "David Ha"
    ],
    "published": "2025-04-10T18:44:41+00:00",
    "summary": "AI is increasingly playing a pivotal role in transforming how scientific discoveries are made. We introduce The AI Scientist-v2, an end-to-end agentic system capable of producing the first entirely AI generated peer-review-accepted workshop paper. This system iteratively formulates scientific hypotheses, designs and executes experiments, analyzes and visualizes data, and autonomously authors scientific manuscripts. Compared to its predecessor (v1, Lu et al., 2024 arXiv:2408.06292), The AI Scientist-v2 eliminates the reliance on human-authored code templates, generalizes effectively across diverse machine learning domains, and leverages a novel progressive agentic tree-search methodology managed by a dedicated experiment manager agent. Additionally, we enhance the AI reviewer component by integrating a Vision-Language Model (VLM) feedback loop for iterative refinement of content and aesthetics of the figures. We evaluated The AI Scientist-v2 by submitting three fully autonomous manuscripts to a peer-reviewed ICLR workshop. Notably, one manuscript achieved high enough scores to exceed the average human acceptance threshold, marking the first instance of a fully AI-generated paper successfully navigating a peer review. This accomplishment highlights the growing capability of AI in conducting all aspects of scientific research. We anticipate that further advancements in autonomous scientific discovery technologies will profoundly impact human knowledge generation, enabling unprecedented scalability in research productivity and significantly accelerating scientific breakthroughs, greatly benefiting society at large. We have open-sourced the code at https://github.com/SakanaAI/AI-Scientist-v2 to foster the future development of this transformative technology. We also discuss the role of AI in science, including AI safety."
  },
  {
    "title": "Can Reasoning LLMs Enhance Clinical Document Classification?",
    "url": "http://arxiv.org/abs/2504.08040v1",
    "arxiv_id": "2504.08040v1",
    "authors": [
      "Akram Mustafa",
      "Usman Naseem",
      "Mostafa Rahimi Azghadi"
    ],
    "published": "2025-04-10T18:00:27+00:00",
    "summary": "Clinical document classification is essential for converting unstructured medical texts into standardised ICD-10 diagnoses, yet it faces challenges due to complex medical language, privacy constraints, and limited annotated datasets. Large Language Models (LLMs) offer promising improvements in accuracy and efficiency for this task. This study evaluates the performance and consistency of eight LLMs; four reasoning (Qwen QWQ, Deepseek Reasoner, GPT o3 Mini, Gemini 2.0 Flash Thinking) and four non-reasoning (Llama 3.3, GPT 4o Mini, Gemini 2.0 Flash, Deepseek Chat); in classifying clinical discharge summaries using the MIMIC-IV dataset. Using cTAKES to structure clinical narratives, models were assessed across three experimental runs, with majority voting determining final predictions. Results showed that reasoning models outperformed non-reasoning models in accuracy (71% vs 68%) and F1 score (67% vs 60%), with Gemini 2.0 Flash Thinking achieving the highest accuracy (75%) and F1 score (76%). However, non-reasoning models demonstrated greater stability (91% vs 84% consistency). Performance varied across ICD-10 codes, with reasoning models excelling in complex cases but struggling with abstract categories. Findings indicate a trade-off between accuracy and consistency, suggesting that a hybrid approach could optimise clinical coding. Future research should explore multi-label classification, domain-specific fine-tuning, and ensemble methods to enhance model reliability in real-world applications."
  },
  {
    "title": "VCR-Bench: A Comprehensive Evaluation Framework for Video Chain-of-Thought Reasoning",
    "url": "http://arxiv.org/abs/2504.07956v1",
    "arxiv_id": "2504.07956v1",
    "authors": [
      "Yukun Qi",
      "Yiming Zhao",
      "Yu Zeng",
      "Xikun Bao",
      "Wenxuan Huang",
      "Lin Chen",
      "Zehui Chen",
      "Jie Zhao",
      "Zhongang Qi",
      "Feng Zhao"
    ],
    "published": "2025-04-10T17:59:03+00:00",
    "summary": "The advancement of Chain-of-Thought (CoT) reasoning has significantly enhanced the capabilities of large language models (LLMs) and large vision-language models (LVLMs). However, a rigorous evaluation framework for video CoT reasoning remains absent. Current video benchmarks fail to adequately assess the reasoning process and expose whether failures stem from deficiencies in perception or reasoning capabilities. Therefore, we introduce VCR-Bench, a novel benchmark designed to comprehensively evaluate LVLMs' Video Chain-of-Thought Reasoning capabilities. VCR-Bench comprises 859 videos spanning a variety of video content and durations, along with 1,034 high-quality question-answer pairs. Each pair is manually annotated with a stepwise CoT rationale, where every step is tagged to indicate its association with the perception or reasoning capabilities. Furthermore, we design seven distinct task dimensions and propose the CoT score to assess the entire CoT process based on the stepwise tagged CoT rationals. Extensive experiments on VCR-Bench highlight substantial limitations in current LVLMs. Even the top-performing model, o1, only achieves a 62.8% CoT score and an 56.7% accuracy, while most models score below 40%. Experiments show most models score lower on perception than reasoning steps, revealing LVLMs' key bottleneck in temporal-spatial information processing for complex video reasoning. A robust positive correlation between the CoT score and accuracy confirms the validity of our evaluation framework and underscores the critical role of CoT reasoning in solving complex video reasoning tasks. We hope VCR-Bench to serve as a standardized evaluation framework and expose the actual drawbacks in complex video reasoning task."
  },
  {
    "title": "Perception-R1: Pioneering Perception Policy with Reinforcement Learning",
    "url": "http://arxiv.org/abs/2504.07954v1",
    "arxiv_id": "2504.07954v1",
    "authors": [
      "En Yu",
      "Kangheng Lin",
      "Liang Zhao",
      "Jisheng Yin",
      "Yana Wei",
      "Yuang Peng",
      "Haoran Wei",
      "Jianjian Sun",
      "Chunrui Han",
      "Zheng Ge",
      "Xiangyu Zhang",
      "Daxin Jiang",
      "Jingyu Wang",
      "Wenbing Tao"
    ],
    "published": "2025-04-10T17:58:27+00:00",
    "summary": "Inspired by the success of DeepSeek-R1, we explore the potential of rule-based reinforcement learning (RL) in MLLM post-training for perception policy learning. While promising, our initial experiments reveal that incorporating a thinking process through RL does not consistently lead to performance gains across all visual perception tasks. This leads us to delve into the essential role of RL in the context of visual perception. In this work, we return to the fundamentals and explore the effects of RL on different perception tasks. We observe that the perceptual complexity is a major factor in determining the effectiveness of RL. We also observe that reward design plays a crucial role in further approching the upper limit of model perception. To leverage these findings, we propose Perception-R1, a scalable RL framework using GRPO during MLLM post-training. With a standard Qwen2.5-VL-3B-Instruct, Perception-R1 achieves +4.2% on RefCOCO+, +17.9% on PixMo-Count, +4.2% on PageOCR, and notably, 31.9% AP on COCO2017 val for the first time, establishing a strong baseline for perception policy learning."
  },
  {
    "title": "SoTA with Less: MCTS-Guided Sample Selection for Data-Efficient Visual Reasoning Self-Improvement",
    "url": "http://arxiv.org/abs/2504.07934v1",
    "arxiv_id": "2504.07934v1",
    "authors": [
      "Xiyao Wang",
      "Zhengyuan Yang",
      "Chao Feng",
      "Hongjin Lu",
      "Linjie Li",
      "Chung-Ching Lin",
      "Kevin Lin",
      "Furong Huang",
      "Lijuan Wang"
    ],
    "published": "2025-04-10T17:49:05+00:00",
    "summary": "In this paper, we present an effective method to enhance visual reasoning with significantly fewer training samples, relying purely on self-improvement with no knowledge distillation. Our key insight is that the difficulty of training data during reinforcement fine-tuning (RFT) is critical. Appropriately challenging samples can substantially boost reasoning capabilities even when the dataset is small. Despite being intuitive, the main challenge remains in accurately quantifying sample difficulty to enable effective data filtering. To this end, we propose a novel way of repurposing Monte Carlo Tree Search (MCTS) to achieve that. Starting from our curated 70k open-source training samples, we introduce an MCTS-based selection method that quantifies sample difficulty based on the number of iterations required by the VLMs to solve each problem. This explicit step-by-step reasoning in MCTS enforces the model to think longer and better identifies samples that are genuinely challenging. We filter and retain 11k samples to perform RFT on Qwen2.5-VL-7B-Instruct, resulting in our final model, ThinkLite-VL. Evaluation results on eight benchmarks show that ThinkLite-VL improves the average performance of Qwen2.5-VL-7B-Instruct by 7%, using only 11k training samples with no knowledge distillation. This significantly outperforms all existing 7B-level reasoning VLMs, and our fairly comparable baselines that use classic selection methods such as accuracy-based filtering. Notably, on MathVista, ThinkLite-VL-7B achieves the SoTA accuracy of 75.1, surpassing Qwen2.5-VL-72B, GPT-4o, and O1. Our code, data, and model are available at https://github.com/si0wang/ThinkLite-VL."
  },
  {
    "title": "Benchmarking Adversarial Robustness to Bias Elicitation in Large Language Models: Scalable Automated Assessment with LLM-as-a-Judge",
    "url": "http://arxiv.org/abs/2504.07887v1",
    "arxiv_id": "2504.07887v1",
    "authors": [
      "Riccardo Cantini",
      "Alessio Orsino",
      "Massimo Ruggiero",
      "Domenico Talia"
    ],
    "published": "2025-04-10T16:00:59+00:00",
    "summary": "Large Language Models (LLMs) have revolutionized artificial intelligence, driving advancements in machine translation, summarization, and conversational agents. However, their increasing integration into critical societal domains has raised concerns about embedded biases, which can perpetuate stereotypes and compromise fairness. These biases stem from various sources, including historical inequalities in training data, linguistic imbalances, and adversarial manipulation. Despite mitigation efforts, recent studies indicate that LLMs remain vulnerable to adversarial attacks designed to elicit biased responses. This work proposes a scalable benchmarking framework to evaluate LLM robustness against adversarial bias elicitation. Our methodology involves (i) systematically probing models with a multi-task approach targeting biases across various sociocultural dimensions, (ii) quantifying robustness through safety scores using an LLM-as-a-Judge approach for automated assessment of model responses, and (iii) employing jailbreak techniques to investigate vulnerabilities in safety mechanisms. Our analysis examines prevalent biases in both small and large state-of-the-art models and their impact on model safety. Additionally, we assess the safety of domain-specific models fine-tuned for critical fields, such as medicine. Finally, we release a curated dataset of bias-related prompts, CLEAR-Bias, to facilitate systematic vulnerability benchmarking. Our findings reveal critical trade-offs between model size and safety, aiding the development of fairer and more robust future language models."
  },
  {
    "title": "Gauge and parametrization dependence of Quantum Einstein Gravity within the Proper Time flow",
    "url": "http://arxiv.org/abs/2504.07877v1",
    "arxiv_id": "2504.07877v1",
    "authors": [
      "Alfio Bonanno",
      "Giovanni Oglialoro",
      "Dario Zappal\u00e0"
    ],
    "published": "2025-04-10T15:52:31+00:00",
    "summary": "Proper time functional flow equations have garnered significant attention in recent years, as they are particularly suitable in analyzing non-perturbative contexts. By resorting to this flow, we investigate the regulator and gauge dependence in quantum Einstein gravity within the asymptotic safety framework, considering various regularization schemes. Our findings indicate that some details of the regulator have minor influence on the critical properties of the theory. In contrast, the selection between linear and exponential parametrizations appears to have a more substantial impact on the scaling behavior of the renormalized flow near the non-Gaussian fixed point."
  },
  {
    "title": "Pangu Ultra: Pushing the Limits of Dense Large Language Models on Ascend NPUs",
    "url": "http://arxiv.org/abs/2504.07866v1",
    "arxiv_id": "2504.07866v1",
    "authors": [
      "Yichun Yin",
      "Wenyong Huang",
      "Kaikai Song",
      "Yehui Tang",
      "Xueyu Wu",
      "Wei Guo",
      "Peng Guo",
      "Yaoyuan Wang",
      "Xiaojun Meng",
      "Yasheng Wang",
      "Dong Li",
      "Can Chen",
      "Dandan Tu",
      "Yin Li",
      "Fisher Yu",
      "Ruiming Tang",
      "Yunhe Wang",
      "Baojun Wang",
      "Bin Wang",
      "Bo Wang",
      "Boxiao Liu",
      "Changzheng Zhang",
      "Duyu Tang",
      "Fei Mi",
      "Hui Jin",
      "Jiansheng Wei",
      "Jiarui Qin",
      "Jinpeng Li",
      "Jun Zhao",
      "Liqun Deng",
      "Lin Li",
      "Minghui Xu",
      "Naifu Zhang",
      "Nianzu Zheng",
      "Qiang Li",
      "Rongju Ruan",
      "Shengjun Cheng",
      "Tianyu Guo",
      "Wei He",
      "Wei Li",
      "Weiwen Liu",
      "Wulong Liu",
      "Xinyi Dai",
      "Yonghan Dong",
      "Yu Pan",
      "Yue Li",
      "Yufei Wang",
      "Yujun Li",
      "Yunsheng Ni",
      "Zhe Liu",
      "Zhenhe Zhang",
      "Zhicheng Liu"
    ],
    "published": "2025-04-10T15:41:51+00:00",
    "summary": "We present Pangu Ultra, a Large Language Model (LLM) with 135 billion parameters and dense Transformer modules trained on Ascend Neural Processing Units (NPUs). Although the field of LLM has been witnessing unprecedented advances in pushing the scale and capability of LLM in recent years, training such a large-scale model still involves significant optimization and system challenges. To stabilize the training process, we propose depth-scaled sandwich normalization, which effectively eliminates loss spikes during the training process of deep models. We pre-train our model on 13.2 trillion diverse and high-quality tokens and further enhance its reasoning capabilities during post-training. To perform such large-scale training efficiently, we utilize 8,192 Ascend NPUs with a series of system optimizations. Evaluations on multiple diverse benchmarks indicate that Pangu Ultra significantly advances the state-of-the-art capabilities of dense LLMs such as Llama 405B and Mistral Large 2, and even achieves competitive results with DeepSeek-R1, whose sparse model structure contains much more parameters. Our exploration demonstrates that Ascend NPUs are capable of efficiently and effectively training dense models with more than 100 billion parameters. Our model and system will be available for our commercial customers."
  },
  {
    "title": "Pangu Ultra: Pushing the Limits of Dense Large Language Models on Ascend NPUs",
    "url": "http://arxiv.org/abs/2504.07866v2",
    "arxiv_id": "2504.07866v2",
    "authors": [
      "Yichun Yin",
      "Wenyong Huang",
      "Kaikai Song",
      "Yehui Tang",
      "Xueyu Wu",
      "Wei Guo",
      "Peng Guo",
      "Yaoyuan Wang",
      "Xiaojun Meng",
      "Yasheng Wang",
      "Dong Li",
      "Can Chen",
      "Dandan Tu",
      "Yin Li",
      "Fisher Yu",
      "Ruiming Tang",
      "Yunhe Wang",
      "Baojun Wang",
      "Bin Wang",
      "Bo Wang",
      "Boxiao Liu",
      "Changzheng Zhang",
      "Duyu Tang",
      "Fei Mi",
      "Hui Jin",
      "Jiansheng Wei",
      "Jiarui Qin",
      "Jinpeng Li",
      "Jun Zhao",
      "Liqun Deng",
      "Lin Li",
      "Minghui Xu",
      "Naifu Zhang",
      "Nianzu Zheng",
      "Qiang Li",
      "Rongju Ruan",
      "Shengjun Cheng",
      "Tianyu Guo",
      "Wei He",
      "Wei Li",
      "Weiwen Liu",
      "Wulong Liu",
      "Xinyi Dai",
      "Yonghan Dong",
      "Yu Pan",
      "Yue Li",
      "Yufei Wang",
      "Yujun Li",
      "Yunsheng Ni",
      "Zhe Liu",
      "Zhenhe Zhang",
      "Zhicheng Liu"
    ],
    "published": "2025-04-10T15:41:51+00:00",
    "summary": "We present Pangu Ultra, a Large Language Model (LLM) with 135 billion parameters and dense Transformer modules trained on Ascend Neural Processing Units (NPUs). Although the field of LLM has been witnessing unprecedented advances in pushing the scale and capability of LLM in recent years, training such a large-scale model still involves significant optimization and system challenges. To stabilize the training process, we propose depth-scaled sandwich normalization, which effectively eliminates loss spikes during the training process of deep models. We pre-train our model on 13.2 trillion diverse and high-quality tokens and further enhance its reasoning capabilities during post-training. To perform such large-scale training efficiently, we utilize 8,192 Ascend NPUs with a series of system optimizations. Evaluations on multiple diverse benchmarks indicate that Pangu Ultra significantly advances the state-of-the-art capabilities of dense LLMs such as Llama 405B and Mistral Large 2, and even achieves competitive results with DeepSeek-R1, whose sparse model structure contains much more parameters. Our exploration demonstrates that Ascend NPUs are capable of efficiently and effectively training dense models with more than 100 billion parameters. Our model and system will be available for our commercial customers."
  },
  {
    "title": "Robust Hallucination Detection in LLMs via Adaptive Token Selection",
    "url": "http://arxiv.org/abs/2504.07863v1",
    "arxiv_id": "2504.07863v1",
    "authors": [
      "Mengjia Niu",
      "Hamed Haddadi",
      "Guansong Pang"
    ],
    "published": "2025-04-10T15:39:10+00:00",
    "summary": "Hallucinations in large language models (LLMs) pose significant safety concerns that impede their broader deployment. Recent research in hallucination detection has demonstrated that LLMs' internal representations contain truthfulness hints, which can be harnessed for detector training. However, the performance of these detectors is heavily dependent on the internal representations of predetermined tokens, fluctuating considerably when working on free-form generations with varying lengths and sparse distributions of hallucinated entities. To address this, we propose HaMI, a novel approach that enables robust detection of hallucinations through adaptive selection and learning of critical tokens that are most indicative of hallucinations. We achieve this robustness by an innovative formulation of the Hallucination detection task as Multiple Instance (HaMI) learning over token-level representations within a sequence, thereby facilitating a joint optimisation of token selection and hallucination detection on generation sequences of diverse forms. Comprehensive experimental results on four hallucination benchmarks show that HaMI significantly outperforms existing state-of-the-art approaches."
  },
  {
    "title": "Deceptive Automated Interpretability: Language Models Coordinating to Fool Oversight Systems",
    "url": "http://arxiv.org/abs/2504.07831v1",
    "arxiv_id": "2504.07831v1",
    "authors": [
      "Simon Lermen",
      "Mateusz Dziemian",
      "Natalia P\u00e9rez-Campanero Antol\u00edn"
    ],
    "published": "2025-04-10T15:07:10+00:00",
    "summary": "We demonstrate how AI agents can coordinate to deceive oversight systems using automated interpretability of neural networks. Using sparse autoencoders (SAEs) as our experimental framework, we show that language models (Llama, DeepSeek R1, and Claude 3.7 Sonnet) can generate deceptive explanations that evade detection. Our agents employ steganographic methods to hide information in seemingly innocent explanations, successfully fooling oversight models while achieving explanation quality comparable to reference labels. We further find that models can scheme to develop deceptive strategies when they believe the detection of harmful features might lead to negative consequences for themselves. All tested LLM agents were capable of deceiving the overseer while achieving high interpretability scores comparable to those of reference labels. We conclude by proposing mitigation strategies, emphasizing the critical need for robust understanding and defenses against deception."
  },
  {
    "title": "Revisiting Likelihood-Based Out-of-Distribution Detection by Modeling Representations",
    "url": "http://arxiv.org/abs/2504.07793v1",
    "arxiv_id": "2504.07793v1",
    "authors": [
      "Yifan Ding",
      "Arturas Aleksandrauskas",
      "Amirhossein Ahmadian",
      "Jonas Unger",
      "Fredrik Lindsten",
      "Gabriel Eilertsen"
    ],
    "published": "2025-04-10T14:30:41+00:00",
    "summary": "Out-of-distribution (OOD) detection is critical for ensuring the reliability of deep learning systems, particularly in safety-critical applications. Likelihood-based deep generative models have historically faced criticism for their unsatisfactory performance in OOD detection, often assigning higher likelihood to OOD data than in-distribution samples when applied to image data. In this work, we demonstrate that likelihood is not inherently flawed. Rather, several properties in the images space prohibit likelihood as a valid detection score. Given a sufficiently good likelihood estimator, specifically using the probability flow formulation of a diffusion model, we show that likelihood-based methods can still perform on par with state-of-the-art methods when applied in the representation space of pre-trained encoders. The code of our work can be found at $\\href{https://github.com/limchaos/Likelihood-OOD.git}{\\texttt{https://github.com/limchaos/Likelihood-OOD.git}}$."
  },
  {
    "title": "Realigning Incentives to Build Better Software: a Holistic Approach to Vendor Accountability",
    "url": "http://arxiv.org/abs/2504.07766v1",
    "arxiv_id": "2504.07766v1",
    "authors": [
      "Gergely Bicz\u00f3k",
      "Sasha Romanosky",
      "Mingyan Liu"
    ],
    "published": "2025-04-10T14:05:24+00:00",
    "summary": "In this paper, we ask the question of why the quality of commercial software, in terms of security and safety, does not measure up to that of other (durable) consumer goods we have come to expect. We examine this question through the lens of incentives. We argue that the challenge around better quality software is due in no small part to a sequence of misaligned incentives, the most critical of which being that the harm caused by software problems is by and large shouldered by consumers, not developers. This lack of liability means software vendors have every incentive to rush low-quality software onto the market and no incentive to enhance quality control. Within this context, this paper outlines a holistic technical and policy framework we believe is needed to incentivize better and more secure software development. At the heart of the incentive realignment is the concept of software liability. This framework touches on various components, including legal, technical, and financial, that are needed for software liability to work in practice; some currently exist, some will need to be re-imagined or established. This is primarily a market-driven approach that emphasizes voluntary participation but highlights the role appropriate regulation can play. We connect and contrast this with the EU legal environment and discuss what this framework means for open-source software (OSS) development and emerging AI risks. Moreover, we present a CrowdStrike case study complete with a what-if analysis had our proposed framework been in effect. Our intention is very much to stimulate a robust conversation among both researchers and practitioners."
  },
  {
    "title": "Optimal Frequency Support from Virtual Power Plants: Minimal Reserve and Allocation",
    "url": "http://arxiv.org/abs/2504.07703v1",
    "arxiv_id": "2504.07703v1",
    "authors": [
      "Xiang Zhu",
      "Guangchun Ruan",
      "Hua Geng"
    ],
    "published": "2025-04-10T12:43:38+00:00",
    "summary": "This paper proposes a novel reserve-minimizing and allocation strategy for virtual power plants (VPPs) to deliver optimal frequency support. The proposed strategy enables VPPs, acting as aggregators for inverter-based resources (IBRs), to provide optimal frequency support economically. The proposed strategy captures time-varying active power injections, reducing the unnecessary redundancy compared to traditional fixed reserve schemes. Reserve requirements for the VPPs are determined based on system frequency response and safety constraints, ensuring efficient grid support. Furthermore, an energy-based allocation model decomposes power injections for each IBR, accounting for their specific limitations. Numerical experiments validate the feasibility of the proposed approach, highlighting significant financial gains for VPPs, especially as system inertia decreases due to higher renewable energy integration."
  },
  {
    "title": "Joint Travel Route Optimization Framework for Platooning",
    "url": "http://arxiv.org/abs/2504.07623v1",
    "arxiv_id": "2504.07623v1",
    "authors": [
      "Akif Adas",
      "Stefano Arrigoni",
      "Mattia Brambilla",
      "Monica Barbara Nicoli",
      "Edoardo Sabbioni"
    ],
    "published": "2025-04-10T10:13:20+00:00",
    "summary": "Platooning represents an advanced driving technology designed to assist drivers in traffic convoys of varying lengths, enhancing road safety, reducing driver fatigue, and improving fuel efficiency. Sophisticated automated driving assistance systems have facilitated this innovation. Recent advancements in platooning emphasize cooperative mechanisms within both centralized and decentralized architectures enabled by vehicular communication technologies. This study introduces a cooperative route planning optimization framework aimed at promoting the adoption of platooning through a centralized platoon formation strategy at the system level. This approach is envisioned as a transitional phase from individual (ego) driving to fully collaborative driving. Additionally, this research formulates and incorporates travel cost metrics related to fuel consumption, driver fatigue, and travel time, considering regulatory constraints on consecutive driving durations. The performance of these cost metrics has been evaluated using Dijkstra's and A* shortest path algorithms within a network graph framework. The results indicate that the proposed architecture achieves an average cost improvement of 14 % compared to individual route planning for long road trips."
  },
  {
    "title": "VLM-R1: A Stable and Generalizable R1-style Large Vision-Language Model",
    "url": "http://arxiv.org/abs/2504.07615v1",
    "arxiv_id": "2504.07615v1",
    "authors": [
      "Haozhan Shen",
      "Peng Liu",
      "Jingcheng Li",
      "Chunxin Fang",
      "Yibo Ma",
      "Jiajia Liao",
      "Qiaoli Shen",
      "Zilun Zhang",
      "Kangjia Zhao",
      "Qianqian Zhang",
      "Ruochen Xu",
      "Tiancheng Zhao"
    ],
    "published": "2025-04-10T10:05:15+00:00",
    "summary": "Recently DeepSeek R1 has shown that reinforcement learning (RL) can substantially improve the reasoning capabilities of Large Language Models (LLMs) through a simple yet effective design. The core of R1 lies in its rule-based reward formulation, which leverages tasks with deterministic ground-truth answers to enable precise and stable reward computation. In the visual domain, we similarly observe that a wide range of visual understanding tasks are inherently equipped with well-defined ground-truth annotations. This property makes them naturally compatible with rule-based reward mechanisms. Motivated by this observation, we investigate the extension of R1-style reinforcement learning to Vision-Language Models (VLMs), aiming to enhance their visual reasoning capabilities. To this end, we develop VLM-R1, a dedicated framework designed to harness RL for improving VLMs' performance on general vision-language tasks. Using this framework, we further explore the feasibility of applying RL to visual domain. Experimental results indicate that the RL-based model not only delivers competitive performance on visual understanding tasks but also surpasses Supervised Fine-Tuning (SFT) in generalization ability. Furthermore, we conduct comprehensive ablation studies that uncover a series of noteworthy insights, including the presence of reward hacking in object detection, the emergence of the \"OD aha moment\", the impact of training data quality, and the scaling behavior of RL across different model sizes. Through these analyses, we aim to deepen the understanding of how reinforcement learning enhances the capabilities of vision-language models, and we hope our findings and open-source contributions will support continued progress in the vision-language RL community. Our code and model are available at https://github.com/om-ai-lab/VLM-R1"
  },
  {
    "title": "VLM-R1: A Stable and Generalizable R1-style Large Vision-Language Model",
    "url": "http://arxiv.org/abs/2504.07615v2",
    "arxiv_id": "2504.07615v2",
    "authors": [
      "Haozhan Shen",
      "Peng Liu",
      "Jingcheng Li",
      "Chunxin Fang",
      "Yibo Ma",
      "Jiajia Liao",
      "Qiaoli Shen",
      "Zilun Zhang",
      "Kangjia Zhao",
      "Qianqian Zhang",
      "Ruochen Xu",
      "Tiancheng Zhao"
    ],
    "published": "2025-04-10T10:05:15+00:00",
    "summary": "Recently DeepSeek R1 has shown that reinforcement learning (RL) can substantially improve the reasoning capabilities of Large Language Models (LLMs) through a simple yet effective design. The core of R1 lies in its rule-based reward formulation, which leverages tasks with deterministic ground-truth answers to enable precise and stable reward computation. In the visual domain, we similarly observe that a wide range of visual understanding tasks are inherently equipped with well-defined ground-truth annotations. This property makes them naturally compatible with rule-based reward mechanisms. Motivated by this observation, we investigate the extension of R1-style reinforcement learning to Vision-Language Models (VLMs), aiming to enhance their visual reasoning capabilities. To this end, we develop VLM-R1, a dedicated framework designed to harness RL for improving VLMs' performance on general vision-language tasks. Using this framework, we further explore the feasibility of applying RL to visual domain. Experimental results indicate that the RL-based model not only delivers competitive performance on visual understanding tasks but also surpasses Supervised Fine-Tuning (SFT) in generalization ability. Furthermore, we conduct comprehensive ablation studies that uncover a series of noteworthy insights, including the presence of reward hacking in object detection, the emergence of the \"OD aha moment\", the impact of training data quality, and the scaling behavior of RL across different model sizes. Through these analyses, we aim to deepen the understanding of how reinforcement learning enhances the capabilities of vision-language models, and we hope our findings and open-source contributions will support continued progress in the vision-language RL community. Our code and model are available at https://github.com/om-ai-lab/VLM-R1"
  },
  {
    "title": "Drive in Corridors: Enhancing the Safety of End-to-end Autonomous Driving via Corridor Learning and Planning",
    "url": "http://arxiv.org/abs/2504.07507v1",
    "arxiv_id": "2504.07507v1",
    "authors": [
      "Zhiwei Zhang",
      "Ruichen Yang",
      "Ke Wu",
      "Zijun Xu",
      "Jingchu Liu",
      "Lisen Mu",
      "Zhongxue Gan",
      "Wenchao Ding"
    ],
    "published": "2025-04-10T07:10:40+00:00",
    "summary": "Safety remains one of the most critical challenges in autonomous driving systems. In recent years, the end-to-end driving has shown great promise in advancing vehicle autonomy in a scalable manner. However, existing approaches often face safety risks due to the lack of explicit behavior constraints. To address this issue, we uncover a new paradigm by introducing the corridor as the intermediate representation. Widely adopted in robotics planning, the corridors represents spatio-temporal obstacle-free zones for the vehicle to traverse. To ensure accurate corridor prediction in diverse traffic scenarios, we develop a comprehensive learning pipeline including data annotation, architecture refinement and loss formulation. The predicted corridor is further integrated as the constraint in a trajectory optimization process. By extending the differentiability of the optimization, we enable the optimized trajectory to be seamlessly trained within the end-to-end learning framework, improving both safety and interpretability. Experimental results on the nuScenes dataset demonstrate state-of-the-art performance of our approach, showing a 66.7% reduction in collisions with agents and a 46.5% reduction with curbs, significantly enhancing the safety of end-to-end driving. Additionally, incorporating the corridor contributes to higher success rates in closed-loop evaluations."
  },
  {
    "title": "Defense against Prompt Injection Attacks via Mixture of Encodings",
    "url": "http://arxiv.org/abs/2504.07467v1",
    "arxiv_id": "2504.07467v1",
    "authors": [
      "Ruiyi Zhang",
      "David Sullivan",
      "Kyle Jackson",
      "Pengtao Xie",
      "Mei Chen"
    ],
    "published": "2025-04-10T05:35:21+00:00",
    "summary": "Large Language Models (LLMs) have emerged as a dominant approach for a wide range of NLP tasks, with their access to external information further enhancing their capabilities. However, this introduces new vulnerabilities, known as prompt injection attacks, where external content embeds malicious instructions that manipulate the LLM's output. Recently, the Base64 defense has been recognized as one of the most effective methods for reducing success rate of prompt injection attacks. Despite its efficacy, this method can degrade LLM performance on certain NLP tasks. To address this challenge, we propose a novel defense mechanism: mixture of encodings, which utilizes multiple character encodings, including Base64. Extensive experimental results show that our method achieves one of the lowest attack success rates under prompt injection attacks, while maintaining high performance across all NLP tasks, outperforming existing character encoding-based defense methods. This underscores the effectiveness of our mixture of encodings strategy for both safety and task performance metrics."
  },
  {
    "title": "Multi-Modal Data Fusion for Moisture Content Prediction in Apple Drying",
    "url": "http://arxiv.org/abs/2504.07465v1",
    "arxiv_id": "2504.07465v1",
    "authors": [
      "Shichen Li",
      "Chenhui Shao"
    ],
    "published": "2025-04-10T05:29:04+00:00",
    "summary": "Fruit drying is widely used in food manufacturing to reduce product moisture, ensure product safety, and extend product shelf life. Accurately predicting final moisture content (MC) is critically needed for quality control of drying processes. State-of-the-art methods can build deterministic relationships between process parameters and MC, but cannot adequately account for inherent process variabilities that are ubiquitous in fruit drying. To address this gap, this paper presents a novel multi-modal data fusion framework to effectively fuse two modalities of data: tabular data (process parameters) and high-dimensional image data (images of dried apple slices) to enable accurate MC prediction. The proposed modeling architecture permits flexible adjustment of information portion from tabular and image data modalities. Experimental validation shows that the multi-modal approach improves predictive accuracy substantially compared to state-of-the-art methods. The proposed method reduces root-mean-squared errors by 19.3%, 24.2%, and 15.2% over tabular-only, image-only, and standard tabular-image fusion models, respectively. Furthermore, it is demonstrated that our method is robust in varied tabular-image ratios and capable of effectively capturing inherent small-scale process variabilities. The proposed framework is extensible to a variety of other drying technologies."
  },
  {
    "title": "LoRI: Reducing Cross-Task Interference in Multi-Task Low-Rank Adaptation",
    "url": "http://arxiv.org/abs/2504.07448v1",
    "arxiv_id": "2504.07448v1",
    "authors": [
      "Juzheng Zhang",
      "Jiacheng You",
      "Ashwinee Panda",
      "Tom Goldstein"
    ],
    "published": "2025-04-10T04:46:04+00:00",
    "summary": "Low-Rank Adaptation (LoRA) has emerged as a popular parameter-efficient fine-tuning (PEFT) method for Large Language Models (LLMs), yet it still incurs notable overhead and suffers from parameter interference in multi-task scenarios. We propose LoRA with Reduced Interference (LoRI), a simple yet effective approach that freezes the projection matrices $A$ as random projections and sparsifies the matrices $B$ using task-specific masks. This design substantially reduces the number of trainable parameters while maintaining strong task performance. Moreover, LoRI minimizes cross-task interference in adapter merging by leveraging the orthogonality between adapter subspaces, and supports continual learning by using sparsity to mitigate catastrophic forgetting. Extensive experiments across natural language understanding, mathematical reasoning, code generation, and safety alignment tasks demonstrate that LoRI outperforms full fine-tuning and existing PEFT methods, while using up to 95% fewer trainable parameters than LoRA. In multi-task experiments, LoRI enables effective adapter merging and continual learning with reduced cross-task interference. Code is available at: https://github.com/juzhengz/LoRI"
  },
  {
    "title": "Electronic Warfare Cyberattacks, Countermeasures and Modern Defensive Strategies of UAV Avionics: A Survey",
    "url": "http://arxiv.org/abs/2504.07358v1",
    "arxiv_id": "2504.07358v1",
    "authors": [
      "Aaron Yu",
      "Iuliia Kolotylo",
      "Hashim A. Hashim",
      "A. E. E. Eltoukhy"
    ],
    "published": "2025-04-10T00:56:52+00:00",
    "summary": "Unmanned Aerial Vehicles (UAVs) play a pivotal role in modern autonomous air mobility, and the reliability of UAV avionics systems is critical to ensuring mission success, sustainability practices, and public safety. The success of UAV missions depends on effectively mitigating various aspects of electronic warfare, including non-destructive and destructive cyberattacks, transponder vulnerabilities, and jamming threats, while rigorously implementing countermeasures and defensive aids. This paper provides a comprehensive review of UAV cyberattacks, countermeasures, and defensive strategies. It explores UAV-to-UAV coordination attacks and their associated features, such as dispatch system attacks, Automatic Dependent Surveillance-Broadcast (ADS-B) attacks, Traffic Alert and Collision Avoidance System (TCAS)-induced collisions, and TCAS attacks. Additionally, the paper examines UAV-to-command center coordination attacks, as well as UAV functionality attacks. The review also covers various countermeasures and defensive aids designed for UAVs. Lastly, a comparison of common cyberattacks and countermeasure approaches is conducted, along with a discussion of future trends in the field. Keywords: Electronic warfare, UAVs, Avionics Systems, cyberattacks, coordination attacks, functionality attacks, countermeasure, defensive-aids."
  },
  {
    "title": "Revisiting Prompt Optimization with Large Reasoning Models-A Case Study on Event Extraction",
    "url": "http://arxiv.org/abs/2504.07357v1",
    "arxiv_id": "2504.07357v1",
    "authors": [
      "Saurabh Srivastava",
      "Ziyu Yao"
    ],
    "published": "2025-04-10T00:53:59+00:00",
    "summary": "Large Reasoning Models (LRMs) such as DeepSeek-R1 and OpenAI o1 have demonstrated remarkable capabilities in various reasoning tasks. Their strong capability to generate and reason over intermediate thoughts has also led to arguments that they may no longer require extensive prompt engineering or optimization to interpret human instructions and produce accurate outputs. In this work, we aim to systematically study this open question, using the structured task of event extraction for a case study. We experimented with two LRMs (DeepSeek-R1 and o1) and two general-purpose Large Language Models (LLMs) (GPT-4o and GPT-4.5), when they were used as task models or prompt optimizers. Our results show that on tasks as complicated as event extraction, LRMs as task models still benefit from prompt optimization, and that using LRMs as prompt optimizers yields more effective prompts. Finally, we provide an error analysis of common errors made by LRMs and highlight the stability and consistency of LRMs in refining task instructions and event guidelines."
  },
  {
    "title": "Code Generation with Small Language Models: A Deep Evaluation on Codeforces",
    "url": "http://arxiv.org/abs/2504.07343v1",
    "arxiv_id": "2504.07343v1",
    "authors": [
      "D\u00e9bora Souza",
      "Rohit Gheyi",
      "Lucas Albuquerque",
      "Gustavo Soares",
      "M\u00e1rcio Ribeiro"
    ],
    "published": "2025-04-09T23:57:44+00:00",
    "summary": "Large Language Models (LLMs) have demonstrated capabilities in code generation, potentially boosting developer productivity. However, their widespread adoption remains limited by high computational costs, significant energy demands, and security risks such as data leakage and adversarial attacks. As a lighter-weight alternative, Small Language Models (SLMs) offer faster inference, lower deployment overhead, and better adaptability to domain-specific tasks, making them an attractive option for real-world applications. While prior research has benchmarked LLMs on competitive programming tasks, such evaluations often focus narrowly on metrics like Elo scores or pass rates, overlooking deeper insights into model behavior, failure patterns, and problem diversity. Furthermore, the potential of SLMs to tackle complex tasks such as competitive programming remains underexplored. In this study, we benchmark five open SLMs - LLAMA 3.2 3B, GEMMA 2 9B, GEMMA 3 12B, DEEPSEEK-R1 14B, and PHI-4 14B - across 280 Codeforces problems spanning Elo ratings from 800 to 2100 and covering 36 distinct topics. All models were tasked with generating Python solutions. PHI-4 14B achieved the best performance among SLMs, with a pass@3 of 63.6%, approaching the proprietary O3-MINI-HIGH (86.8%). In addition, we evaluated PHI-4 14B on C++ and found that combining outputs from both Python and C++ increases its aggregated pass@3 to 73.6%. A qualitative analysis of PHI-4 14B's incorrect outputs revealed that some failures were due to minor implementation issues - such as handling edge cases or correcting variable initialization - rather than deeper reasoning flaws."
  },
  {
    "title": "Agentic SLMs: Hunting Down Test Smells",
    "url": "http://arxiv.org/abs/2504.07277v1",
    "arxiv_id": "2504.07277v1",
    "authors": [
      "Rian Melo",
      "Pedro Sim\u00f5es",
      "Rohit Gheyi",
      "Marcelo d'Amorim",
      "M\u00e1rcio Ribeiro",
      "Gustavo Soares",
      "Eduardo Almeida",
      "Elvys Soares"
    ],
    "published": "2025-04-09T21:12:01+00:00",
    "summary": "Test smells can compromise the reliability of test suites and hinder software maintenance. Although several strategies exist for detecting test smells, few address their removal. Traditional methods often rely on static analysis or machine learning, requiring significant effort and expertise. This study evaluates LLAMA 3.2 3B, GEMMA 2 9B, DEEPSEEK-R1 14B, and PHI 4 14B - small, open language models - for automating the detection and refactoring of test smells through agent-based workflows. We explore workflows with one, two, and four agents across 150 instances of 5 common test smell types extracted from real-world Java projects. Unlike prior approaches, ours is easily extensible to new smells via natural language definitions and generalizes to Python and Golang. All models detected nearly all test smell instances (pass@5 of 96% with four agents), with PHI 4 14B achieving the highest refactoring accuracy (pass@5 of 75.3%). Analyses were computationally inexpensive and ran efficiently on a consumer-grade hardware. Notably, PHI 4 14B with four agents performed within 5% of proprietary models such as O1-MINI, O3-MINI-HIGH, and GEMINI 2.5 PRO EXPERIMENTAL using a single agent. Multi-agent setups outperformed single-agent ones in three out of five test smell types, highlighting their potential to improve software quality with minimal developer effort. For the Assertion Roulette smell, however, a single agent performed better. To assess practical relevance, we submitted 10 pull requests with PHI 4 14B - generated code to open-source projects. Five were merged, one was rejected, and four remain under review, demonstrating the approach's real-world applicability."
  },
  {
    "title": "Sculpting Subspaces: Constrained Full Fine-Tuning in LLMs for Continual Learning",
    "url": "http://arxiv.org/abs/2504.07097v1",
    "arxiv_id": "2504.07097v1",
    "authors": [
      "Nikhil Shivakumar Nayak",
      "Krishnateja Killamsetty",
      "Ligong Han",
      "Abhishek Bhandwaldar",
      "Prateek Chanda",
      "Kai Xu",
      "Hao Wang",
      "Aldo Pareja",
      "Oleg Silkin",
      "Mustafa Eyceoz",
      "Akash Srivastava"
    ],
    "published": "2025-04-09T17:59:42+00:00",
    "summary": "Continual learning in large language models (LLMs) is prone to catastrophic forgetting, where adapting to new tasks significantly degrades performance on previously learned ones. Existing methods typically rely on low-rank, parameter-efficient updates that limit the model's expressivity and introduce additional parameters per task, leading to scalability issues. To address these limitations, we propose a novel continual full fine-tuning approach leveraging adaptive singular value decomposition (SVD). Our method dynamically identifies task-specific low-rank parameter subspaces and constrains updates to be orthogonal to critical directions associated with prior tasks, thus effectively minimizing interference without additional parameter overhead or storing previous task gradients. We evaluate our approach extensively on standard continual learning benchmarks using both encoder-decoder (T5-Large) and decoder-only (LLaMA-2 7B) models, spanning diverse tasks including classification, generation, and reasoning. Empirically, our method achieves state-of-the-art results, up to 7% higher average accuracy than recent baselines like O-LoRA, and notably maintains the model's general linguistic capabilities, instruction-following accuracy, and safety throughout the continual learning process by reducing forgetting to near-negligible levels. Our adaptive SVD framework effectively balances model plasticity and knowledge retention, providing a practical, theoretically grounded, and computationally scalable solution for continual learning scenarios in large language models."
  },
  {
    "title": "OmniCaptioner: One Captioner to Rule Them All",
    "url": "http://arxiv.org/abs/2504.07089v1",
    "arxiv_id": "2504.07089v1",
    "authors": [
      "Yiting Lu",
      "Jiakang Yuan",
      "Zhen Li",
      "Shitian Zhao",
      "Qi Qin",
      "Xinyue Li",
      "Le Zhuo",
      "Licheng Wen",
      "Dongyang Liu",
      "Yuewen Cao",
      "Xiangchao Yan",
      "Xin Li",
      "Botian Shi",
      "Tao Chen",
      "Zhibo Chen",
      "Lei Bai",
      "Bo Zhang",
      "Peng Gao"
    ],
    "published": "2025-04-09T17:58:58+00:00",
    "summary": "We propose OmniCaptioner, a versatile visual captioning framework for generating fine-grained textual descriptions across a wide variety of visual domains. Unlike prior methods limited to specific image types (e.g., natural images or geometric visuals), our framework provides a unified solution for captioning natural images, visual text (e.g., posters, UIs, textbooks), and structured visuals (e.g., documents, tables, charts). By converting low-level pixel information into semantically rich textual representations, our framework bridges the gap between visual and textual modalities. Our results highlight three key advantages: (i) Enhanced Visual Reasoning with LLMs, where long-context captions of visual modalities empower LLMs, particularly the DeepSeek-R1 series, to reason effectively in multimodal scenarios; (ii) Improved Image Generation, where detailed captions improve tasks like text-to-image generation and image transformation; and (iii) Efficient Supervised Fine-Tuning (SFT), which enables faster convergence with less data. We believe the versatility and adaptability of OmniCaptioner can offer a new perspective for bridging the gap between language and visual modalities."
  },
  {
    "title": "R2E-Gym: Procedural Environments and Hybrid Verifiers for Scaling Open-Weights SWE Agents",
    "url": "http://arxiv.org/abs/2504.07164v1",
    "arxiv_id": "2504.07164v1",
    "authors": [
      "Naman Jain",
      "Jaskirat Singh",
      "Manish Shetty",
      "Liang Zheng",
      "Koushik Sen",
      "Ion Stoica"
    ],
    "published": "2025-04-09T17:55:19+00:00",
    "summary": "Improving open-source models on real-world SWE tasks (solving GITHUB issues) faces two key challenges: 1) scalable curation of execution environments to train these models, and, 2) optimal scaling of test-time compute. We introduce AgentGym, the largest procedurally-curated executable gym environment for training real-world SWE-agents, consisting of more than 8.7K tasks. AgentGym is powered by two main contributions: 1) SYNGEN: a synthetic data curation recipe that enables scalable curation of executable environments using test-generation and back-translation directly from commits, thereby reducing reliance on human-written issues or unit tests. We show that this enables more scalable training leading to pass@1 performance of 34.4% on SWE-Bench Verified benchmark with our 32B model. 2) Hybrid Test-time Scaling: we provide an in-depth analysis of two test-time scaling axes; execution-based and execution-free verifiers, demonstrating that they exhibit complementary strengths and limitations. Test-based verifiers suffer from low distinguishability, while execution-free verifiers are biased and often rely on stylistic features. Surprisingly, we find that while each approach individually saturates around 42-43%, significantly higher gains can be obtained by leveraging their complementary strengths. Overall, our approach achieves 51% on the SWE-Bench Verified benchmark, reflecting a new state-of-the-art for open-weight SWE-agents and for the first time showing competitive performance with proprietary models such as o1, o1-preview and sonnet-3.5-v2 (with tools). We will open-source our environments, models, and agent trajectories."
  },
  {
    "title": "Self-Steering Language Models",
    "url": "http://arxiv.org/abs/2504.07081v1",
    "arxiv_id": "2504.07081v1",
    "authors": [
      "Gabriel Grand",
      "Joshua B. Tenenbaum",
      "Vikash K. Mansinghka",
      "Alexander K. Lew",
      "Jacob Andreas"
    ],
    "published": "2025-04-09T17:54:22+00:00",
    "summary": "While test-time reasoning enables language models to tackle complex tasks, searching or planning in natural language can be slow, costly, and error-prone. But even when LMs struggle to emulate the precise reasoning steps needed to solve a problem, they often excel at describing its abstract structure--both how to verify solutions and how to search for them. This paper introduces DisCIPL, a method for \"self-steering\" LMs where a Planner model generates a task-specific inference program that is executed by a population of Follower models. Our approach equips LMs with the ability to write recursive search procedures that guide LM inference, enabling new forms of verifiable and efficient reasoning. When instantiated with a small Follower (e.g., Llama-3.2-1B), DisCIPL matches (and sometimes outperforms) much larger models, including GPT-4o and o1, on challenging constrained generation tasks. In decoupling planning from execution, our work opens up a design space of highly-parallelized Monte Carlo inference strategies that outperform standard best-of-N sampling, require no finetuning, and can be implemented automatically by existing LMs."
  },
  {
    "title": "Evaluating Retrieval Augmented Generative Models for Document Queries in Transportation Safety",
    "url": "http://arxiv.org/abs/2504.07022v1",
    "arxiv_id": "2504.07022v1",
    "authors": [
      "Chad Melton",
      "Alex Sorokine",
      "Steve Peterson"
    ],
    "published": "2025-04-09T16:37:03+00:00",
    "summary": "Applications of generative Large Language Models LLMs are rapidly expanding across various domains, promising significant improvements in workflow efficiency and information retrieval. However, their implementation in specialized, high-stakes domains such as hazardous materials transportation is challenging due to accuracy and reliability concerns. This study evaluates the performance of three fine-tuned generative models, ChatGPT, Google's Vertex AI, and ORNL Retrieval Augmented Generation augmented LLaMA 2 and LLaMA in retrieving regulatory information essential for hazardous material transportation compliance in the United States. Utilizing approximately 40 publicly available federal and state regulatory documents, we developed 100 realistic queries relevant to route planning and permitting requirements. Responses were qualitatively rated based on accuracy, detail, and relevance, complemented by quantitative assessments of semantic similarity between model outputs. Results demonstrated that the RAG-augmented LLaMA models significantly outperformed Vertex AI and ChatGPT, providing more detailed and generally accurate information, despite occasional inconsistencies. This research introduces the first known application of RAG in transportation safety, emphasizing the need for domain-specific fine-tuning and rigorous evaluation methodologies to ensure reliability and minimize the risk of inaccuracies in high-stakes environments."
  },
  {
    "title": "Quantum Field Theory on Multifractal Spacetime: Varying Dimension and Ultraviolet Completeness",
    "url": "http://arxiv.org/abs/2504.06797v1",
    "arxiv_id": "2504.06797v1",
    "authors": [
      "Alessio Maiezza",
      "Juan Carlos Vasquez"
    ],
    "published": "2025-04-09T11:41:15+00:00",
    "summary": "Inspired by various quantum gravity approaches, we explore quantum field theory where spacetime exhibits scaling properties and dimensional reduction with changing energy scales, effectively behaving as a multifractal manifold. Working within canonical quantization, we demonstrate how to properly quantize fields in such multifractal spacetime. Our analysis reveals that a non-differentiable nature of spacetime is not merely compatible with quantum field theory but significantly enhances its mathematical foundation. Most notably, this approach guarantees the finiteness of the theory at all orders in perturbation theory and enables rigorous construction of the S-matrix in the interaction picture. The multifractal structure tames dominant, large-order divergence sources in the perturbative series and resolves the Landau pole problem through asymptotic safety, substantially improving the theory's behavior in the deep ultraviolet regime. Our formulation preserves all established predictions of standard quantum field theory at low energies while offering novel physical behaviors at high energy scales."
  },
  {
    "title": "Domain-Specific Pruning of Large Mixture-of-Experts Models with Few-shot Demonstrations",
    "url": "http://arxiv.org/abs/2504.06792v1",
    "arxiv_id": "2504.06792v1",
    "authors": [
      "Zican Dong",
      "Han Peng",
      "Peiyu Liu",
      "Wayne Xin Zhao",
      "Dong Wu",
      "Feng Xiao",
      "Zhifeng Wang"
    ],
    "published": "2025-04-09T11:34:06+00:00",
    "summary": "Mixture-of-Experts (MoE) models achieve a favorable trade-off between performance and inference efficiency by activating only a subset of experts. However, the memory overhead of storing all experts remains a major limitation, especially in large-scale MoE models such as DeepSeek-R1 (671B). In this study, we investigate domain specialization and expert redundancy in large-scale MoE models and uncover a consistent behavior we term few-shot expert localization, with only a few demonstrations, the model consistently activates a sparse and stable subset of experts. Building on this observation, we propose a simple yet effective pruning framework, EASY-EP, that leverages a few domain-specific demonstrations to identify and retain only the most relevant experts. EASY-EP comprises two key components: output-aware expert importance assessment and expert-level token contribution estimation. The former evaluates the importance of each expert for the current token by considering the gating scores and magnitudes of the outputs of activated experts, while the latter assesses the contribution of tokens based on representation similarities after and before routed experts. Experiments show that our method can achieve comparable performances and $2.99\\times$ throughput under the same memory budget with full DeepSeek-R1 with only half the experts. Our code is available at https://github.com/RUCAIBox/EASYEP."
  },
  {
    "title": "Beware of \"Explanations\" of AI",
    "url": "http://arxiv.org/abs/2504.06791v1",
    "arxiv_id": "2504.06791v1",
    "authors": [
      "David Martens",
      "Galit Shmueli",
      "Theodoros Evgeniou",
      "Kevin Bauer",
      "Christian Janiesch",
      "Stefan Feuerriegel",
      "Sebastian Gabel",
      "Sofie Goethals",
      "Travis Greene",
      "Nadja Klein",
      "Mathias Kraus",
      "Niklas K\u00fchl",
      "Claudia Perlich",
      "Wouter Verbeke",
      "Alona Zharova",
      "Patrick Zschech",
      "Foster Provost"
    ],
    "published": "2025-04-09T11:31:08+00:00",
    "summary": "Understanding the decisions made and actions taken by increasingly complex AI system remains a key challenge. This has led to an expanding field of research in explainable artificial intelligence (XAI), highlighting the potential of explanations to enhance trust, support adoption, and meet regulatory standards. However, the question of what constitutes a \"good\" explanation is dependent on the goals, stakeholders, and context. At a high level, psychological insights such as the concept of mental model alignment can offer guidance, but success in practice is challenging due to social and technical factors. As a result of this ill-defined nature of the problem, explanations can be of poor quality (e.g. unfaithful, irrelevant, or incoherent), potentially leading to substantial risks. Instead of fostering trust and safety, poorly designed explanations can actually cause harm, including wrong decisions, privacy violations, manipulation, and even reduced AI adoption. Therefore, we caution stakeholders to beware of explanations of AI: while they can be vital, they are not automatically a remedy for transparency or responsible AI adoption, and their misuse or limitations can exacerbate harm. Attention to these caveats can help guide future research to improve the quality and impact of AI explanations."
  },
  {
    "title": "Holistic Capability Preservation: Towards Compact Yet Comprehensive Reasoning Models",
    "url": "http://arxiv.org/abs/2504.07158v1",
    "arxiv_id": "2504.07158v1",
    "authors": [
      "Ling Team",
      "Caizhi Tang",
      "Chilin Fu",
      "Chunwei Wu",
      "Jia Guo",
      "Jianwen Wang",
      "Jingyu Hu",
      "Liang Jiang",
      "Meng Li",
      "Peng Jiao",
      "Pingping Liu",
      "Shaomian Zheng",
      "Shiwei Liang",
      "Shuaicheng Li",
      "Yalin Zhang",
      "Yingting Wu",
      "Yongkang Liu",
      "Zhenyu Huang"
    ],
    "published": "2025-04-09T11:24:32+00:00",
    "summary": "This technical report presents Ring-Lite-Distill, a lightweight reasoning model derived from our open-source Mixture-of-Experts (MoE) Large Language Models (LLMs) Ling-Lite. This study demonstrates that through meticulous high-quality data curation and ingenious training paradigms, the compact MoE model Ling-Lite can be further trained to achieve exceptional reasoning capabilities, while maintaining its parameter-efficient architecture with only 2.75 billion activated parameters, establishing an efficient lightweight reasoning architecture. In particular, in constructing this model, we have not merely focused on enhancing advanced reasoning capabilities, exemplified by high-difficulty mathematical problem solving, but rather aimed to develop a reasoning model with more comprehensive competency coverage. Our approach ensures coverage across reasoning tasks of varying difficulty levels while preserving generic capabilities, such as instruction following, tool use, and knowledge retention. We show that, Ring-Lite-Distill's reasoning ability reaches a level comparable to DeepSeek-R1-Distill-Qwen-7B, while its general capabilities significantly surpass those of DeepSeek-R1-Distill-Qwen-7B. The models are accessible at https://huggingface.co/inclusionAI"
  },
  {
    "title": "Holistic Capability Preservation: Towards Compact Yet Comprehensive Reasoning Models",
    "url": "http://arxiv.org/abs/2504.07158v2",
    "arxiv_id": "2504.07158v2",
    "authors": [
      "Ling Team",
      "Caizhi Tang",
      "Chilin Fu",
      "Chunwei Wu",
      "Jia Guo",
      "Jianwen Wang",
      "Jingyu Hu",
      "Liang Jiang",
      "Meng Li",
      "Peng Jiao",
      "Pingping Liu",
      "Shaomian Zheng",
      "Shiwei Liang",
      "Shuaicheng Li",
      "Yalin Zhang",
      "Yingting Wu",
      "Yongkang Liu",
      "Zhenyu Huang"
    ],
    "published": "2025-04-09T11:24:32+00:00",
    "summary": "This technical report presents Ring-Lite-Distill, a lightweight reasoning model derived from our open-source Mixture-of-Experts (MoE) Large Language Models (LLMs) Ling-Lite. This study demonstrates that through meticulous high-quality data curation and ingenious training paradigms, the compact MoE model Ling-Lite can be further trained to achieve exceptional reasoning capabilities, while maintaining its parameter-efficient architecture with only 2.75 billion activated parameters, establishing an efficient lightweight reasoning architecture. In particular, in constructing this model, we have not merely focused on enhancing advanced reasoning capabilities, exemplified by high-difficulty mathematical problem solving, but rather aimed to develop a reasoning model with more comprehensive competency coverage. Our approach ensures coverage across reasoning tasks of varying difficulty levels while preserving generic capabilities, such as instruction following, tool use, and knowledge retention. We show that, Ring-Lite-Distill's reasoning ability reaches a level comparable to DeepSeek-R1-Distill-Qwen-7B, while its general capabilities significantly surpass those of DeepSeek-R1-Distill-Qwen-7B. The models are accessible at https://huggingface.co/inclusionAI"
  },
  {
    "title": "Zero-Shot Image-Based Large Language Model Approach to Road Pavement Monitoring",
    "url": "http://arxiv.org/abs/2504.06785v1",
    "arxiv_id": "2504.06785v1",
    "authors": [
      "Shuoshuo Xu",
      "Kai Zhao",
      "James Loney",
      "Zili Li",
      "Andrea Visentin"
    ],
    "published": "2025-04-09T11:19:17+00:00",
    "summary": "Effective and rapid evaluation of pavement surface condition is critical for prioritizing maintenance, ensuring transportation safety, and minimizing vehicle wear and tear. While conventional manual inspections suffer from subjectivity, existing machine learning-based methods are constrained by their reliance on large and high-quality labeled datasets, which require significant resources and limit adaptability across varied road conditions. The revolutionary advancements in Large Language Models (LLMs) present significant potential for overcoming these challenges. In this study, we propose an innovative automated zero-shot learning approach that leverages the image recognition and natural language understanding capabilities of LLMs to assess road conditions effectively. Multiple LLM-based assessment models were developed, employing prompt engineering strategies aligned with the Pavement Surface Condition Index (PSCI) standards. These models' accuracy and reliability were evaluated against official PSCI results, with an optimized model ultimately selected. Extensive tests benchmarked the optimized model against evaluations from various levels experts using Google Street View road images. The results reveal that the LLM-based approach can effectively assess road conditions, with the optimized model -employing comprehensive and structured prompt engineering strategies -outperforming simpler configurations by achieving high accuracy and consistency, even surpassing expert evaluations. Moreover, successfully applying the optimized model to Google Street View images demonstrates its potential for future city-scale deployments. These findings highlight the transformative potential of LLMs in automating road damage evaluations and underscore the pivotal role of detailed prompt engineering in achieving reliable assessments."
  },
  {
    "title": "Dynamic Residual Safe Reinforcement Learning for Multi-Agent Safety-Critical Scenarios Decision-Making",
    "url": "http://arxiv.org/abs/2504.06670v1",
    "arxiv_id": "2504.06670v1",
    "authors": [
      "Kaifeng Wang",
      "Yinsong Chen",
      "Qi Liu",
      "Xueyuan Li",
      "Xin Gao"
    ],
    "published": "2025-04-09T08:13:14+00:00",
    "summary": "In multi-agent safety-critical scenarios, traditional autonomous driving frameworks face significant challenges in balancing safety constraints and task performance. These frameworks struggle to quantify dynamic interaction risks in real-time and depend heavily on manual rules, resulting in low computational efficiency and conservative strategies. To address these limitations, we propose a Dynamic Residual Safe Reinforcement Learning (DRS-RL) framework grounded in a safety-enhanced networked Markov decision process. It's the first time that the weak-to-strong theory is introduced into multi-agent decision-making, enabling lightweight dynamic calibration of safety boundaries via a weak-to-strong safety correction paradigm. Based on the multi-agent dynamic conflict zone model, our framework accurately captures spatiotemporal coupling risks among heterogeneous traffic participants and surpasses the static constraints of conventional geometric rules. Moreover, a risk-aware prioritized experience replay mechanism mitigates data distribution bias by mapping risk to sampling probability. Experimental results reveal that the proposed method significantly outperforms traditional RL algorithms in safety, efficiency, and comfort. Specifically, it reduces the collision rate by up to 92.17%, while the safety model accounts for merely 27% of the main model's parameters."
  },
  {
    "title": "Toward Holistic Evaluation of Recommender Systems Powered by Generative Models",
    "url": "http://arxiv.org/abs/2504.06667v1",
    "arxiv_id": "2504.06667v1",
    "authors": [
      "Yashar Deldjoo",
      "Nikhil Mehta",
      "Maheswaran Sathiamoorthy",
      "Shuai Zhang",
      "Pablo Castells",
      "Julian McAuley"
    ],
    "published": "2025-04-09T08:08:16+00:00",
    "summary": "Recommender systems powered by generative models (Gen-RecSys) extend beyond classical item ranking by producing open-ended content, which simultaneously unlocks richer user experiences and introduces new risks. On one hand, these systems can enhance personalization and appeal through dynamic explanations and multi-turn dialogues. On the other hand, they might venture into unknown territory-hallucinating nonexistent items, amplifying bias, or leaking private information. Traditional accuracy metrics cannot fully capture these challenges, as they fail to measure factual correctness, content safety, or alignment with user intent.   This paper makes two main contributions. First, we categorize the evaluation challenges of Gen-RecSys into two groups: (i) existing concerns that are exacerbated by generative outputs (e.g., bias, privacy) and (ii) entirely new risks (e.g., item hallucinations, contradictory explanations). Second, we propose a holistic evaluation approach that includes scenario-based assessments and multi-metric checks-incorporating relevance, factual grounding, bias detection, and policy compliance. Our goal is to provide a guiding framework so researchers and practitioners can thoroughly assess Gen-RecSys, ensuring effective personalization and responsible deployment."
  },
  {
    "title": "Uni-PrevPredMap: Extending PrevPredMap to a Unified Framework of Prior-Informed Modeling for Online Vectorized HD Map Construction",
    "url": "http://arxiv.org/abs/2504.06647v2",
    "arxiv_id": "2504.06647v2",
    "authors": [
      "Nan Peng",
      "Xun Zhou",
      "Mingming Wang",
      "Guisong Chen",
      "Wenqi Xu"
    ],
    "published": "2025-04-09T07:36:17+00:00",
    "summary": "Safety constitutes a foundational imperative for autonomous driving systems, necessitating the maximal incorporation of accessible external prior information. This study establishes that temporal perception buffers and cost-efficient maps inherently form complementary prior sources for online vectorized high-definition (HD) map construction. We present Uni-PrevPredMap, a unified prior-informed framework that systematically integrates two synergistic information sources: previous predictions and simulated outdated HD maps. The framework introduces two core innovations: a tile-indexed 3D vectorized global map processor enabling efficient refreshment, storage, and retrieval of 3D vectorized priors; a tri-mode operational optimization paradigm ensuring consistency across non-prior, temporal-prior, and temporal-map-fusion-prior scenarios while mitigating reliance on idealized map fidelity assumptions. Uni-PrevPredMap achieves state-of-the-art performance in map-absent scenarios across established online vectorized HD map construction benchmarks. When provided with simulated outdated HD maps, the framework exhibits robust capabilities in error-resilient prior fusion, empirically confirming the synergistic complementarity between previous predictions and simulated outdated HD maps. Code will be available at https://github.com/pnnnnnnn/Uni-PrevPredMap."
  },
  {
    "title": "Harmful information spreading and its impact on vaccination campaigns modeled through fractal-fractional operators",
    "url": "http://arxiv.org/abs/2504.06582v1",
    "arxiv_id": "2504.06582v1",
    "authors": [
      "Ali Akg\u00fcl",
      "Auwalu Hamisu Usman",
      "J. Alberto Conejero"
    ],
    "published": "2025-04-09T05:04:17+00:00",
    "summary": "Despite the huge efforts to develop and administer vaccines worldwide to cope with the COVID-19 pandemic, misinformation spreading through fake news in media and social networks about vaccination safety, make that people refuse to be vaccinated, which harms not only these people but also the whole population.   In this work, we model the effects of harmful information spreading in immunization acquisition through vaccination. Our model is posed for several fractional derivative operators. We have conducted a comprehensive foundation analysis of this model for the different fractional derivatives. Additionally, we have incorporated a strength parameter that shows the combined impact of nonlinear and linear components within an epidemiological model. We have used the second derivative of the Lyapunov function to ascertain the detection of wave patterns within the vaccination dynamics."
  },
  {
    "title": "Bypassing Safety Guardrails in LLMs Using Humor",
    "url": "http://arxiv.org/abs/2504.06577v1",
    "arxiv_id": "2504.06577v1",
    "authors": [
      "Pedro Cisneros-Velarde"
    ],
    "published": "2025-04-09T04:58:14+00:00",
    "summary": "In this paper, we show it is possible to bypass the safety guardrails of large language models (LLMs) through a humorous prompt including the unsafe request. In particular, our method does not edit the unsafe request and follows a fixed template -- it is simple to implement and does not need additional LLMs to craft prompts. Extensive experiments show the effectiveness of our method across different LLMs. We also show that both removing and adding more humor to our method can reduce its effectiveness -- excessive humor possibly distracts the LLM from fulfilling its unsafe request. Thus, we argue that LLM jailbreaking occurs when there is a proper balance between focus on the unsafe request and presence of humor."
  },
  {
    "title": "Do Reasoning Models Show Better Verbalized Calibration?",
    "url": "http://arxiv.org/abs/2504.06564v1",
    "arxiv_id": "2504.06564v1",
    "authors": [
      "Qingcheng Zeng",
      "Weihao Xuan",
      "Leyang Cui",
      "Rob Voigt"
    ],
    "published": "2025-04-09T03:58:19+00:00",
    "summary": "Large reasoning models (LRMs) have recently shown impressive capabilities in complex reasoning by leveraging increased test-time computation and exhibiting behaviors akin to human-like deliberation. Despite these advances, it remains an open question whether LRMs are better calibrated - particularly in their verbalized confidence - compared to instruction-tuned counterparts. In this paper, we investigate the calibration properties of LRMs trained via supervised fine-tuning distillation on long reasoning traces (henceforth SFT reasoning models) and outcome-based reinforcement learning for reasoning (henceforth RL reasoning models) across diverse domains. Our findings reveal that LRMs significantly outperform instruction-tuned models on complex reasoning tasks in both accuracy and confidence calibration. In contrast, we find surprising trends in the domain of factuality in particular. On factuality tasks, while Deepseek-R1 shows strong calibration behavior, smaller QwQ-32B shows no improvement over instruct models; moreover, SFT reasoning models display worse calibration (greater overconfidence) compared to instruct models. Our results provide evidence for a potentially critical role of reasoning-oriented RL training in improving LLMs' capacity for generating trustworthy, self-aware outputs."
  },
  {
    "title": "Data-Driven Reachability with Scenario Optimization and the Holdout Method",
    "url": "http://arxiv.org/abs/2504.06541v1",
    "arxiv_id": "2504.06541v1",
    "authors": [
      "Elizabeth Dietrich",
      "Rosalyn Devonport",
      "Stephen Tu",
      "Murat Arcak"
    ],
    "published": "2025-04-09T02:46:03+00:00",
    "summary": "Reachability analysis is an important method in providing safety guarantees for systems with unknown or uncertain dynamics. Due to the computational intractability of exact reachability analysis for general nonlinear, high-dimensional systems, recent work has focused on the use of probabilistic methods for computing approximate reachable sets. In this work, we advocate for the use of a general purpose, practical, and sharp method for data-driven reachability: the holdout method. Despite the simplicity of the holdout method, we show -- on several numerical examples including scenario-based reach tubes -- that the resulting probabilistic bounds are substantially sharper and require fewer samples than existing methods for data-driven reachability. Furthermore, we complement our work with a discussion on the necessity of probabilistic reachability bounds. We argue that any method that attempts to de-randomize the bounds, by converting the guarantees to hold deterministically, requires (a) an exponential in state-dimension amount of samples to achieve non-vacuous guarantees, and (b) extra assumptions on the dynamics."
  },
  {
    "title": "TSP-OCS: A Time-Series Prediction for Optimal Camera Selection in Multi-Viewpoint Surgical Video Analysis",
    "url": "http://arxiv.org/abs/2504.06527v1",
    "arxiv_id": "2504.06527v1",
    "authors": [
      "Xinyu Liu",
      "Xiaoguang Lin",
      "Xiang Liu",
      "Yong Yang",
      "Hongqian Wang",
      "Qilong Sun"
    ],
    "published": "2025-04-09T02:07:49+00:00",
    "summary": "Recording the open surgery process is essential for educational and medical evaluation purposes; however, traditional single-camera methods often face challenges such as occlusions caused by the surgeon's head and body, as well as limitations due to fixed camera angles, which reduce comprehensibility of the video content. This study addresses these limitations by employing a multi-viewpoint camera recording system, capturing the surgical procedure from six different angles to mitigate occlusions. We propose a fully supervised learning-based time series prediction method to choose the best shot sequences from multiple simultaneously recorded video streams, ensuring optimal viewpoints at each moment. Our time series prediction model forecasts future camera selections by extracting and fusing visual and semantic features from surgical videos using pre-trained models. These features are processed by a temporal prediction network with TimeBlocks to capture sequential dependencies. A linear embedding layer reduces dimensionality, and a Softmax classifier selects the optimal camera view based on the highest probability. In our experiments, we created five groups of open thyroidectomy videos, each with simultaneous recordings from six different angles. The results demonstrate that our method achieves competitive accuracy compared to traditional supervised methods, even when predicting over longer time horizons. Furthermore, our approach outperforms state-of-the-art time series prediction techniques on our dataset. This manuscript makes a unique contribution by presenting an innovative framework that advances surgical video analysis techniques, with significant implications for improving surgical education and patient safety."
  },
  {
    "title": "Safe Navigation in Uncertain Crowded Environments Using Risk Adaptive CVaR Barrier Functions",
    "url": "http://arxiv.org/abs/2504.06513v1",
    "arxiv_id": "2504.06513v1",
    "authors": [
      "Xinyi Wang",
      "Taekyung Kim",
      "Bardh Hoxha",
      "Georgios Fainekos",
      "Dimitra Panagou"
    ],
    "published": "2025-04-09T01:23:44+00:00",
    "summary": "Robot navigation in dynamic, crowded environments poses a significant challenge due to the inherent uncertainties in the obstacle model. In this work, we propose a risk-adaptive approach based on the Conditional Value-at-Risk Barrier Function (CVaR-BF), where the risk level is automatically adjusted to accept the minimum necessary risk, achieving a good performance in terms of safety and optimization feasibility under uncertainty. Additionally, we introduce a dynamic zone-based barrier function which characterizes the collision likelihood by evaluating the relative state between the robot and the obstacle. By integrating risk adaptation with this new function, our approach adaptively expands the safety margin, enabling the robot to proactively avoid obstacles in highly dynamic environments. Comparisons and ablation studies demonstrate that our method outperforms existing social navigation approaches, and validate the effectiveness of our proposed framework."
  },
  {
    "title": "Can LLMs Simulate Personas with Reversed Performance? A Benchmark for Counterfactual Instruction Following",
    "url": "http://arxiv.org/abs/2504.06460v1",
    "arxiv_id": "2504.06460v1",
    "authors": [
      "Sai Adith Senthil Kumar",
      "Hao Yan",
      "Saipavan Perepa",
      "Murong Yue",
      "Ziyu Yao"
    ],
    "published": "2025-04-08T22:00:32+00:00",
    "summary": "Large Language Models (LLMs) are now increasingly widely used to simulate personas in virtual environments, leveraging their instruction-following capability. However, we discovered that even state-of-the-art LLMs cannot simulate personas with reversed performance (e.g., student personas with low proficiency in educational settings), which impairs the simulation diversity and limits the practical applications of the simulated environments. In this work, using mathematical reasoning as a representative scenario, we propose the first benchmark dataset for evaluating LLMs on simulating personas with reversed performance, a capability that we dub \"counterfactual instruction following\". We evaluate both open-weight and closed-source LLMs on this task and find that LLMs, including the OpenAI o1 reasoning model, all struggle to follow counterfactual instructions for simulating reversedly performing personas. Intersectionally simulating both the performance level and the race population of a persona worsens the effect even further. These results highlight the challenges of counterfactual instruction following and the need for further research."
  },
  {
    "title": "Extended Version: Multi-Robot Motion Planning with Cooperative Localization",
    "url": "http://arxiv.org/abs/2504.06429v1",
    "arxiv_id": "2504.06429v1",
    "authors": [
      "Anne Theurkauf",
      "Nisar Ahmed",
      "Morteza Lahijanian"
    ],
    "published": "2025-04-08T20:58:19+00:00",
    "summary": "We consider the uncertain multi-robot motion planning (MRMP) problem with cooperative localization (CL-MRMP), under both motion and measurement noise, where each robot can act as a sensor for its nearby teammates. We formalize CL-MRMP as a chance-constrained motion planning problem, and propose a safety-guaranteed algorithm that explicitly accounts for robot-robot correlations. Our approach extends a sampling-based planner to solve CL-MRMP while preserving probabilistic completeness. To improve efficiency, we introduce novel biasing techniques. We evaluate our method across diverse benchmarks, demonstrating its effectiveness in generating motion plans, with significant performance gains from biasing strategies."
  },
  {
    "title": "Technological schemes and control methods in the reconstruction of parallel gas pipeline systems under non-stationary conditions",
    "url": "http://arxiv.org/abs/2504.06420v1",
    "arxiv_id": "2504.06420v1",
    "authors": [
      "Ilgar Aliyev"
    ],
    "published": "2025-04-08T20:40:02+00:00",
    "summary": "The study explores technological schemes and control methods for reconstructing parallel gas pipeline systems under non-stationary conditions. It focuses on improving safety, reliability, and efficiency by automating valve control and integrating IoT-based monitoring. Automated shut-off valves are designed to detect sudden pressure drops and block leaks instantly. These valves utilize pressure drop rates and valve position sensors to detect and isolate leaks. Wireless pressure sensors and real-time monitoring systems enable remote control of gas flow. Mathematical modeling is employed to analyze pressure variations along damaged sections in order to determine optimal valve placement and activation timing. Machine learning algorithms in the control center are used to predict and verify leak locations based on sensor data. To ensure uninterrupted gas supply in the event of an accident, the study develops an empirical formula for determining the location of connecting pipes activated between parallel pipelines, based on real-time pressure data. The aim of this research is to reduce gas leaks and environmental hazards, ensure continuous gas supply during emergencies, enhance decision-making through automated systems, minimize gas losses, and reduce maintenance costs."
  },
  {
    "title": "SPoRt -- Safe Policy Ratio: Certified Training and Deployment of Task Policies in Model-Free RL",
    "url": "http://arxiv.org/abs/2504.06386v1",
    "arxiv_id": "2504.06386v1",
    "authors": [
      "Jacques Cloete",
      "Nikolaus Vertovec",
      "Alessandro Abate"
    ],
    "published": "2025-04-08T19:09:07+00:00",
    "summary": "To apply reinforcement learning to safety-critical applications, we ought to provide safety guarantees during both policy training and deployment. In this work we present novel theoretical results that provide a bound on the probability of violating a safety property for a new task-specific policy in a model-free, episodic setup: the bound, based on a `maximum policy ratio' that is computed with respect to a `safe' base policy, can also be more generally applied to temporally-extended properties (beyond safety) and to robust control problems. We thus present SPoRt, which also provides a data-driven approach for obtaining such a bound for the base policy, based on scenario theory, and which includes Projected PPO, a new projection-based approach for training the task-specific policy while maintaining a user-specified bound on property violation. Hence, SPoRt enables the user to trade off safety guarantees in exchange for task-specific performance. Accordingly, we present experimental results demonstrating this trade-off, as well as a comparison of the theoretical bound to posterior bounds based on empirical violation rates."
  },
  {
    "title": "Addressing Relative Degree Issues in Control Barrier Function Synthesis with Physics-Informed Neural Networks",
    "url": "http://arxiv.org/abs/2504.06242v1",
    "arxiv_id": "2504.06242v1",
    "authors": [
      "Lukas Brunke",
      "Siqi Zhou",
      "Francesco D'Orazio",
      "Angela P. Schoellig"
    ],
    "published": "2025-04-08T17:41:43+00:00",
    "summary": "In robotics, control barrier function (CBF)-based safety filters are commonly used to enforce state constraints. A critical challenge arises when the relative degree of the CBF varies across the state space. This variability can create regions within the safe set where the control input becomes unconstrained. When implemented as a safety filter, this may result in chattering near the safety boundary and ultimately compromise system safety. To address this issue, we propose a novel approach for CBF synthesis by formulating it as solving a set of boundary value problems. The solutions to the boundary value problems are determined using physics-informed neural networks (PINNs). Our approach ensures that the synthesized CBFs maintain a constant relative degree across the set of admissible states, thereby preventing unconstrained control scenarios. We illustrate the approach in simulation and further verify it through real-world quadrotor experiments, demonstrating its effectiveness in preserving desired system safety properties."
  },
  {
    "title": "TxGemma: Efficient and Agentic LLMs for Therapeutics",
    "url": "http://arxiv.org/abs/2504.06196v1",
    "arxiv_id": "2504.06196v1",
    "authors": [
      "Eric Wang",
      "Samuel Schmidgall",
      "Paul F. Jaeger",
      "Fan Zhang",
      "Rory Pilgrim",
      "Yossi Matias",
      "Joelle Barral",
      "David Fleet",
      "Shekoofeh Azizi"
    ],
    "published": "2025-04-08T16:39:02+00:00",
    "summary": "Therapeutic development is a costly and high-risk endeavor that is often plagued by high failure rates. To address this, we introduce TxGemma, a suite of efficient, generalist large language models (LLMs) capable of therapeutic property prediction as well as interactive reasoning and explainability. Unlike task-specific models, TxGemma synthesizes information from diverse sources, enabling broad application across the therapeutic development pipeline. The suite includes 2B, 9B, and 27B parameter models, fine-tuned from Gemma-2 on a comprehensive dataset of small molecules, proteins, nucleic acids, diseases, and cell lines. Across 66 therapeutic development tasks, TxGemma achieved superior or comparable performance to the state-of-the-art generalist model on 64 (superior on 45), and against state-of-the-art specialist models on 50 (superior on 26). Fine-tuning TxGemma models on therapeutic downstream tasks, such as clinical trial adverse event prediction, requires less training data than fine-tuning base LLMs, making TxGemma suitable for data-limited applications. Beyond these predictive capabilities, TxGemma features conversational models that bridge the gap between general LLMs and specialized property predictors. These allow scientists to interact in natural language, provide mechanistic reasoning for predictions based on molecular structure, and engage in scientific discussions. Building on this, we further introduce Agentic-Tx, a generalist therapeutic agentic system powered by Gemini 2.5 that reasons, acts, manages diverse workflows, and acquires external domain knowledge. Agentic-Tx surpasses prior leading models on the Humanity's Last Exam benchmark (Chemistry & Biology) with 52.3% relative improvement over o3-mini (high) and 26.7% over o3-mini (high) on GPQA (Chemistry) and excels with improvements of 6.3% (ChemBench-Preference) and 2.4% (ChemBench-Mini) over o3-mini (high)."
  },
  {
    "title": "A Self-Supervised Framework for Space Object Behaviour Characterisation",
    "url": "http://arxiv.org/abs/2504.06176v1",
    "arxiv_id": "2504.06176v1",
    "authors": [
      "Ian Groves",
      "Andrew Campbell",
      "James Fernandes",
      "Diego Rodriguez",
      "Paul Murray",
      "Massimiliano Vasile",
      "Victoria Nockles"
    ],
    "published": "2025-04-08T16:19:19+00:00",
    "summary": "Foundation Models, pre-trained on large unlabelled datasets before task-specific fine-tuning, are increasingly being applied to specialised domains. Recent examples include ClimaX for climate and Clay for satellite Earth observation, but a Foundation Model for Space Object Behavioural Analysis has not yet been developed. As orbital populations grow, automated methods for characterising space object behaviour are crucial for space safety. We present a Space Safety and Sustainability Foundation Model focusing on space object behavioural analysis using light curves (LCs). We implemented a Perceiver-Variational Autoencoder (VAE) architecture, pre-trained with self-supervised reconstruction and masked reconstruction on 227,000 LCs from the MMT-9 observatory. The VAE enables anomaly detection, motion prediction, and LC generation. We fine-tuned the model for anomaly detection & motion prediction using two independent LC simulators (CASSANDRA and GRIAL respectively), using CAD models of boxwing, Sentinel-3, SMOS, and Starlink platforms. Our pre-trained model achieved a reconstruction error of 0.01%, identifying potentially anomalous light curves through reconstruction difficulty. After fine-tuning, the model scored 88% and 82% accuracy, with 0.90 and 0.95 ROC AUC scores respectively in both anomaly detection and motion mode prediction (sun-pointing, spin, etc.). Analysis of high-confidence anomaly predictions on real data revealed distinct patterns including characteristic object profiles and satellite glinting. Here, we demonstrate how self-supervised learning can simultaneously enable anomaly detection, motion prediction, and synthetic data generation from rich representations learned in pre-training. Our work therefore supports space safety and sustainability through automated monitoring and simulation capabilities."
  },
  {
    "title": "A Self-Supervised Framework for Space Object Behaviour Characterisation",
    "url": "http://arxiv.org/abs/2504.06176v2",
    "arxiv_id": "2504.06176v2",
    "authors": [
      "Ian Groves",
      "Andrew Campbell",
      "James Fernandes",
      "Diego Ram\u00edrez Rodr\u00edguez",
      "Paul Murray",
      "Massimiliano Vasile",
      "Victoria Nockles"
    ],
    "published": "2025-04-08T16:19:19+00:00",
    "summary": "Foundation Models, pre-trained on large unlabelled datasets before task-specific fine-tuning, are increasingly being applied to specialised domains. Recent examples include ClimaX for climate and Clay for satellite Earth observation, but a Foundation Model for Space Object Behavioural Analysis has not yet been developed. As orbital populations grow, automated methods for characterising space object behaviour are crucial for space safety. We present a Space Safety and Sustainability Foundation Model focusing on space object behavioural analysis using light curves (LCs). We implemented a Perceiver-Variational Autoencoder (VAE) architecture, pre-trained with self-supervised reconstruction and masked reconstruction on 227,000 LCs from the MMT-9 observatory. The VAE enables anomaly detection, motion prediction, and LC generation. We fine-tuned the model for anomaly detection & motion prediction using two independent LC simulators (CASSANDRA and GRIAL respectively), using CAD models of boxwing, Sentinel-3, SMOS, and Starlink platforms. Our pre-trained model achieved a reconstruction error of 0.01%, identifying potentially anomalous light curves through reconstruction difficulty. After fine-tuning, the model scored 88% and 82% accuracy, with 0.90 and 0.95 ROC AUC scores respectively in both anomaly detection and motion mode prediction (sun-pointing, spin, etc.). Analysis of high-confidence anomaly predictions on real data revealed distinct patterns including characteristic object profiles and satellite glinting. Here, we demonstrate how self-supervised learning can simultaneously enable anomaly detection, motion prediction, and synthetic data generation from rich representations learned in pre-training. Our work therefore supports space safety and sustainability through automated monitoring and simulation capabilities."
  },
  {
    "title": "Safe Interaction via Monte Carlo Linear-Quadratic Games",
    "url": "http://arxiv.org/abs/2504.06124v1",
    "arxiv_id": "2504.06124v1",
    "authors": [
      "Benjamin A. Christie",
      "Dylan P. Losey"
    ],
    "published": "2025-04-08T15:18:38+00:00",
    "summary": "Safety is critical during human-robot interaction. But -- because people are inherently unpredictable -- it is often difficult for robots to plan safe behaviors. Instead of relying on our ability to anticipate humans, here we identify robot policies that are robust to unexpected human decisions. We achieve this by formulating human-robot interaction as a zero-sum game, where (in the worst case) the human's actions directly conflict with the robot's objective. Solving for the Nash Equilibrium of this game provides robot policies that maximize safety and performance across a wide range of human actions. Existing approaches attempt to find these optimal policies by leveraging Hamilton-Jacobi analysis (which is intractable) or linear-quadratic approximations (which are inexact). By contrast, in this work we propose a computationally efficient and theoretically justified method that converges towards the Nash Equilibrium policy. Our approach (which we call MCLQ) leverages linear-quadratic games to obtain an initial guess at safe robot behavior, and then iteratively refines that guess with a Monte Carlo search. Not only does MCLQ provide real-time safety adjustments, but it also enables the designer to tune how conservative the robot is -- preventing the system from focusing on unrealistic human behaviors. Our simulations and user study suggest that this approach advances safety in terms of both computation time and expected performance. See videos of our experiments here: https://youtu.be/KJuHeiWVuWY."
  },
  {
    "title": "Leanabell-Prover: Posttraining Scaling in Formal Reasoning",
    "url": "http://arxiv.org/abs/2504.06122v2",
    "arxiv_id": "2504.06122v2",
    "authors": [
      "Jingyuan Zhang",
      "Qi Wang",
      "Xingguang Ji",
      "Yahui Liu",
      "Yang Yue",
      "Fuzheng Zhang",
      "Di Zhang",
      "Guorui Zhou",
      "Kun Gai"
    ],
    "published": "2025-04-08T15:15:26+00:00",
    "summary": "Recent advances in automated theorem proving (ATP) through LLMs have highlighted the potential of formal reasoning with Lean 4 codes. However, ATP has not yet be revolutionized by the recent posttraining scaling as demonstrated by Open AI O1/O3 and Deepseek R1. In this work, we investigate the entire posttraining of ATP, aiming to align it with breakthroughs in reasoning models in natural languages. To begin, we continual train current ATP models with a hybrid dataset, which consists of numerous statement-proof pairs, and additional data aimed at incorporating cognitive behaviors that emulate human reasoning and hypothesis refinement. Next, we explore reinforcement learning with the use of outcome reward returned by Lean 4 compiler. Through our designed continual training and reinforcement learning processes, we have successfully improved existing formal provers, including both DeepSeek-Prover-v1.5 and Goedel-Prover, achieving state-of-the-art performance in the field of whole-proof generation. For example, we achieve a 59.8% pass rate (pass@32) on MiniF2F. This is an on-going project and we will progressively update our findings, release our data and training details."
  },
  {
    "title": "Uncertainty-Aware Hybrid Machine Learning in Virtual Sensors for Vehicle Sideslip Angle Estimation",
    "url": "http://arxiv.org/abs/2504.06105v1",
    "arxiv_id": "2504.06105v1",
    "authors": [
      "Abinav Kalyanasundaram",
      "Karthikeyan Chandra Sekaran",
      "Philipp Stauber",
      "Michael Lange",
      "Wolfgang Utschick",
      "Michael Botsch"
    ],
    "published": "2025-04-08T14:49:58+00:00",
    "summary": "Precise vehicle state estimation is crucial for safe and reliable autonomous driving. The number of measurable states and their precision offered by the onboard vehicle sensor system are often constrained by cost. For instance, measuring critical quantities such as the Vehicle Sideslip Angle (VSA) poses significant commercial challenges using current optical sensors. This paper addresses these limitations by focusing on the development of high-performance virtual sensors to enhance vehicle state estimation for active safety. The proposed Uncertainty-Aware Hybrid Learning (UAHL) architecture integrates a machine learning model with vehicle motion models to estimate VSA directly from onboard sensor data. A key aspect of the UAHL architecture is its focus on uncertainty quantification for individual model estimates and hybrid fusion. These mechanisms enable the dynamic weighting of uncertainty-aware predictions from machine learning and vehicle motion models to produce accurate and reliable hybrid VSA estimates. This work also presents a novel dataset named Real-world Vehicle State Estimation Dataset (ReV-StED), comprising synchronized measurements from advanced vehicle dynamic sensors. The experimental results demonstrate the superior performance of the proposed method for VSA estimation, highlighting UAHL as a promising architecture for advancing virtual sensors and enhancing active safety in autonomous vehicles."
  }
]