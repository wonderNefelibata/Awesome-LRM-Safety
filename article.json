[
  {
    "title": "Learning-Based Conformal Tube MPC for Safe Control in Interactive Multi-Agent Systems",
    "url": "http://arxiv.org/abs/2504.03293v1",
    "arxiv_id": "2504.03293v1",
    "authors": [
      "Shuqi Wang",
      "Yue Gao",
      "Xiang Yin"
    ],
    "published": "2025-04-04T09:17:59+00:00",
    "summary": "Safety assurance in multi-agent systems with coupled dynamics is a fundamental yet challenging problem, especially when agents exhibit uncertain and state-dependent behaviors. Classical robust control often assumes worst-case disturbances, leading to overly conservative actions. In this work, we propose a learning-based framework that combines conformal prediction with model predictive control (MPC) to ensure probabilistic safety under action-level uncertainty. Unlike prior approaches that predict future states, we directly model the control action of the uncontrollable agent as a stochastic function of the joint state, trained via neural networks and calibrated using conformal prediction. This enables us to construct dynamic, probabilistically guaranteed reachable tubes for the uncontrollable agent. These tubes are then embedded into an MPC formulation to synthesize control actions for the controllable agent that ensure safe interactions over a finite planning horizon. We provide formal stepwise and cumulative safety guarantees, and demonstrate the effectiveness of our approach through a pedestrian-vehicle interaction scenario. Compared to baseline methods, our framework achieves higher safety rates while maintaining high performance in terms of speed and responsiveness."
  },
  {
    "title": "Verification of Autonomous Neural Car Control with KeYmaera X",
    "url": "http://arxiv.org/abs/2504.03272v1",
    "arxiv_id": "2504.03272v1",
    "authors": [
      "Enguerrand Prebet",
      "Samuel Teuber",
      "Andr\u00e9 Platzer"
    ],
    "published": "2025-04-04T08:43:31+00:00",
    "summary": "This article presents a formal model and formal safety proofs for the ABZ'25 case study in differential dynamic logic (dL). The case study considers an autonomous car driving on a highway avoiding collisions with neighbouring cars. Using KeYmaera X's dL implementation, we prove absence of collision on an infinite time horizon which ensures that safety is preserved independently of trip length. The safety guarantees hold for time-varying reaction time and brake force. Our dL model considers the single lane scenario with cars ahead or behind. We demonstrate that dL with its tools is a rigorous foundation for runtime monitoring, shielding, and neural network verification. Doing so sheds light on inconsistencies between the provided specification and simulation environment highway-env of the ABZ'25 study. We attempt to fix these inconsistencies and uncover numerous counterexamples which also indicate issues in the provided reinforcement learning environment."
  },
  {
    "title": "Gradient Field-Based Dynamic Window Approach for Collision Avoidance in Complex Environments",
    "url": "http://arxiv.org/abs/2504.03260v1",
    "arxiv_id": "2504.03260v1",
    "authors": [
      "Ze Zhang",
      "Yifan Xue",
      "Nadia Figueroa",
      "Knut \u00c5kesson"
    ],
    "published": "2025-04-04T08:19:40+00:00",
    "summary": "For safe and flexible navigation in multi-robot systems, this paper presents an enhanced and predictive sampling-based trajectory planning approach in complex environments, the Gradient Field-based Dynamic Window Approach (GF-DWA). Building upon the dynamic window approach, the proposed method utilizes gradient information of obstacle distances as a new cost term to anticipate potential collisions. This enhancement enables the robot to improve awareness of obstacles, including those with non-convex shapes. The gradient field is derived from the Gaussian process distance field, which generates both the distance field and gradient field by leveraging Gaussian process regression to model the spatial structure of the environment. Through several obstacle avoidance and fleet collision avoidance scenarios, the proposed GF-DWA is shown to outperform other popular trajectory planning and control methods in terms of safety and flexibility, especially in complex environments with non-convex obstacles."
  },
  {
    "title": "Improving Clinical Imaging Systems using Cognition based Approaches",
    "url": "http://arxiv.org/abs/2504.03251v1",
    "arxiv_id": "2504.03251v1",
    "authors": [
      "Kailas Dayanandan",
      "Brejesh Lall"
    ],
    "published": "2025-04-04T08:02:36+00:00",
    "summary": "Clinical systems operate in safety-critical environments and are not intended to function autonomously; however, they are currently designed to replicate clinicians' diagnoses rather than assist them in the diagnostic process. To enable better supervision of system-generated diagnoses, we replicate radiologists' systematic approach used to analyze chest X-rays. This approach facilitates comprehensive analysis across all regions of clinical images and can reduce errors caused by inattentional blindness and under reading. Our work addresses a critical research gap by identifying difficult-to-diagnose diseases for clinicians using insights from human vision, enabling these systems to serve as an effective \"second pair of eyes\". These improvements make the clinical imaging systems more complementary and combine the strengths of human and machine vision. Additionally, we leverage effective receptive fields in deep learning models to present machine-generated diagnoses with sufficient context, making it easier for clinicians to evaluate them."
  },
  {
    "title": "Data-Driven Hamiltonian for Direct Construction of Safe Set from Trajectory Data",
    "url": "http://arxiv.org/abs/2504.03233v1",
    "arxiv_id": "2504.03233v1",
    "authors": [
      "Jason J. Choi",
      "Christopher A. Strong",
      "Koushil Sreenath",
      "Namhoon Cho",
      "Claire J. Tomlin"
    ],
    "published": "2025-04-04T07:29:11+00:00",
    "summary": "In continuous-time optimal control, evaluating the Hamiltonian requires solving a constrained optimization problem using the system's dynamics model. Hamilton-Jacobi reachability analysis for safety verification has demonstrated practical utility only when efficient evaluation of the Hamiltonian over a large state-time grid is possible. In this study, we introduce the concept of a data-driven Hamiltonian (DDH), which circumvents the need for an explicit dynamics model by relying only on mild prior knowledge (e.g., Lipschitz constants), thus enabling the construction of reachable sets directly from trajectory data. Recognizing that the Hamiltonian is the optimal inner product between a given costate and realizable state velocities, the DDH estimates the Hamiltonian using the worst-case realization of the velocity field based on the observed state trajectory data. This formulation ensures a conservative approximation of the true Hamiltonian for uncertain dynamics. The reachable set computed based on the DDH is also ensured to be a conservative approximation of the true reachable set. Next, we propose a data-efficient safe experiment framework for gradual expansion of safe sets using the DDH. This is achieved by iteratively conducting experiments within the computed data-driven safe set and updating the set using newly collected trajectory data. To demonstrate the capabilities of our approach, we showcase its effectiveness in safe flight envelope expansion for a tiltrotor vehicle transitioning from near-hover to forward flight."
  },
  {
    "title": "A Robust Method for Fault Detection and Severity Estimation in Mechanical Vibration Data",
    "url": "http://arxiv.org/abs/2504.03229v1",
    "arxiv_id": "2504.03229v1",
    "authors": [
      "Youngjae Jeon",
      "Eunho Heo",
      "Jinmo Lee",
      "Taewon Uhm",
      "Dongjin Lee"
    ],
    "published": "2025-04-04T07:22:29+00:00",
    "summary": "This paper proposes a robust method for fault detection and severity estimation in multivariate time-series data to enhance predictive maintenance of mechanical systems. We use the Temporal Graph Convolutional Network (T-GCN) model to capture both spatial and temporal dependencies among variables. This enables accurate future state predictions under varying operational conditions. To address the challenge of fluctuating anomaly scores that reduce fault severity estimation accuracy, we introduce a novel fault severity index based on the mean and standard deviation of anomaly scores. This generates a continuous and reliable severity measurement. We validate the proposed method using two experimental datasets: an open IMS bearing dataset and data collected from a fanjet electric propulsion system. Results demonstrate that our method significantly reduces abrupt fluctuations and inconsistencies in anomaly scores. This provides a more dependable foundation for maintenance planning and risk management in safety-critical applications."
  },
  {
    "title": "Enhancing Personalized Multi-Turn Dialogue with Curiosity Reward",
    "url": "http://arxiv.org/abs/2504.03206v1",
    "arxiv_id": "2504.03206v1",
    "authors": [
      "Yanming Wan",
      "Jiaxing Wu",
      "Marwa Abdulhai",
      "Lior Shani",
      "Natasha Jaques"
    ],
    "published": "2025-04-04T06:35:02+00:00",
    "summary": "Effective conversational agents must be able to personalize their behavior to suit a user's preferences, personality, and attributes, whether they are assisting with writing tasks or operating in domains like education or healthcare. Current training methods like Reinforcement Learning from Human Feedback (RLHF) prioritize helpfulness and safety but fall short in fostering truly empathetic, adaptive, and personalized interactions. Traditional approaches to personalization often rely on extensive user history, limiting their effectiveness for new or context-limited users. To overcome these limitations, we propose to incorporate an intrinsic motivation to improve the conversational agents's model of the user as an additional reward alongside multi-turn RLHF. This reward mechanism encourages the agent to actively elicit user traits by optimizing conversations to increase the accuracy of its user model. Consequently, the policy agent can deliver more personalized interactions through obtaining more information about the user. We applied our method both education and fitness settings, where LLMs teach concepts or recommend personalized strategies based on users' hidden learning style or lifestyle attributes. Using LLM-simulated users, our approach outperformed a multi-turn RLHF baseline in revealing information about the users' preferences, and adapting to them."
  },
  {
    "title": "Learning Natural Language Constraints for Safe Reinforcement Learning of Language Agents",
    "url": "http://arxiv.org/abs/2504.03185v1",
    "arxiv_id": "2504.03185v1",
    "authors": [
      "Jaymari Chua",
      "Chen Wang",
      "Lina Yao"
    ],
    "published": "2025-04-04T05:26:28+00:00",
    "summary": "Generalizable alignment is a core challenge for deploying Large Language Models (LLMs) safely in real-world NLP applications. Current alignment methods, including Reinforcement Learning from Human Feedback (RLHF), often fail to guarantee constraint satisfaction outside their training distribution due to their reliance on implicit, post-hoc preferences. Inspired by a paradigm shift to first curate data before tuning, we introduce a new framework for safe language alignment that learns natural language constraints from positive and negative demonstrations as a primary step. From inferring both a task-specific reward function and latent constraint functions, our approach fosters adaptation to novel safety requirements and robust generalization under domain shifts and adversarial inputs. We formalize the framework within a Constrained Markov Decision Process (CMDP) and validate it via a text-based navigation environment, demonstrating safe adaptation to changing danger zones. Our experiments show fewer violations upon domain shift when following a safe navigation path, and we achieve zero violations by applying learned constraints to a distilled BERT model as a fine-tuning technique. This work offers a promising path toward building safety-critical and more generalizable LLMs for practical NLP settings."
  },
  {
    "title": "Multi-lingual Multi-turn Automated Red Teaming for LLMs",
    "url": "http://arxiv.org/abs/2504.03174v1",
    "arxiv_id": "2504.03174v1",
    "authors": [
      "Abhishek Singhania",
      "Christophe Dupuy",
      "Shivam Mangale",
      "Amani Namboori"
    ],
    "published": "2025-04-04T05:06:12+00:00",
    "summary": "Language Model Models (LLMs) have improved dramatically in the past few years, increasing their adoption and the scope of their capabilities over time. A significant amount of work is dedicated to ``model alignment'', i.e., preventing LLMs to generate unsafe responses when deployed into customer-facing applications. One popular method to evaluate safety risks is \\textit{red-teaming}, where agents attempt to bypass alignment by crafting elaborate prompts that trigger unsafe responses from a model. Standard human-driven red-teaming is costly, time-consuming and rarely covers all the recent features (e.g., multi-lingual, multi-modal aspects), while proposed automation methods only cover a small subset of LLMs capabilities (i.e., English or single-turn). We present Multi-lingual Multi-turn Automated Red Teaming (\\textbf{MM-ART}), a method to fully automate conversational, multi-lingual red-teaming operations and quickly identify prompts leading to unsafe responses. Through extensive experiments on different languages, we show the studied LLMs are on average 71\\% more vulnerable after a 5-turn conversation in English than after the initial turn. For conversations in non-English languages, models display up to 195\\% more safety vulnerabilities than the standard single-turn English approach, confirming the need for automated red-teaming methods matching LLMs capabilities."
  },
  {
    "title": "Real-Time Roadway Obstacle Detection for Electric Scooters Using Deep Learning and Multi-Sensor Fusion",
    "url": "http://arxiv.org/abs/2504.03171v1",
    "arxiv_id": "2504.03171v1",
    "authors": [
      "Zeyang Zheng",
      "Arman Hosseini",
      "Dong Chen",
      "Omid Shoghli",
      "Arsalan Heydarian"
    ],
    "published": "2025-04-04T05:01:16+00:00",
    "summary": "The increasing adoption of electric scooters (e-scooters) in urban areas has coincided with a rise in traffic accidents and injuries, largely due to their small wheels, lack of suspension, and sensitivity to uneven surfaces. While deep learning-based object detection has been widely used to improve automobile safety, its application for e-scooter obstacle detection remains unexplored. This study introduces a novel ground obstacle detection system for e-scooters, integrating an RGB camera, and a depth camera to enhance real-time road hazard detection. Additionally, the Inertial Measurement Unit (IMU) measures linear vertical acceleration to identify surface vibrations, guiding the selection of six obstacle categories: tree branches, manhole covers, potholes, pine cones, non-directional cracks, and truncated domes. All sensors, including the RGB camera, depth camera, and IMU, are integrated within the Intel RealSense Camera D435i. A deep learning model powered by YOLO detects road hazards and utilizes depth data to estimate obstacle proximity. Evaluated on the seven hours of naturalistic riding dataset, the system achieves a high mean average precision (mAP) of 0.827 and demonstrates excellent real-time performance. This approach provides an effective solution to enhance e-scooter safety through advanced computer vision and data fusion. The dataset is accessible at https://zenodo.org/records/14583718, and the project code is hosted on https://github.com/Zeyang-Zheng/Real-Time-Roadway-Obstacle-Detection-for-Electric-Scooters."
  },
  {
    "title": "Distributed Resilience-Aware Control in Multi-Robot Networks",
    "url": "http://arxiv.org/abs/2504.03120v1",
    "arxiv_id": "2504.03120v1",
    "authors": [
      "Haejoon Lee",
      "Dimitra Panagou"
    ],
    "published": "2025-04-04T02:22:21+00:00",
    "summary": "Ensuring resilient consensus in multi-robot systems with misbehaving agents remains a challenge, as many existing network resilience properties are inherently combinatorial and globally defined. While previous works have proposed control laws to enhance or preserve resilience in multi-robot networks, they often assume a fixed topology with known resilience properties, or require global state knowledge. These assumptions may be impractical in physically-constrained environments, where safety and resilience requirements are conflicting, or when misbehaving agents corrupt the shared information. In this work, we propose a distributed control law that enables each robot to guarantee resilient consensus and safety during its navigation without fixed topologies using only locally available information. To this end, we establish a new sufficient condition for resilient consensus in time-varying networks based on the degree of non-misbehaving or normal agents. Using this condition, we design a Control Barrier Function (CBF)-based controller that guarantees resilient consensus and collision avoidance without requiring estimates of global state and/or control actions of all other robots. Finally, we validate our method through simulations."
  },
  {
    "title": "The Use of Gaze-Derived Confidence of Inferred Operator Intent in Adjusting Safety-Conscious Haptic Assistance",
    "url": "http://arxiv.org/abs/2504.03098v1",
    "arxiv_id": "2504.03098v1",
    "authors": [
      "Jeremy D. Webb",
      "Michael Bowman",
      "Songpo Li",
      "Xiaoli Zhang"
    ],
    "published": "2025-04-04T00:49:16+00:00",
    "summary": "Humans directly completing tasks in dangerous or hazardous conditions is not always possible where these tasks are increasingly be performed remotely by teleoperated robots. However, teleoperation is difficult since the operator feels a disconnect with the robot caused by missing feedback from several senses, including touch, and the lack of depth in the video feedback presented to the operator. To overcome this problem, the proposed system actively infers the operator's intent and provides assistance based on the predicted intent. Furthermore, a novel method of calculating confidence in the inferred intent modifies the human-in-the-loop control. The operator's gaze is employed to intuitively indicate the target before the manipulation with the robot begins. A potential field method is used to provide a guiding force towards the intended target, and a safety boundary reduces risk of damage. Modifying these assistances based on the confidence level in the operator's intent makes the control more natural, and gives the robot an intuitive understanding of its human master. Initial validation results show the ability of the system to improve accuracy, execution time, and reduce operator error."
  },
  {
    "title": "Task as Context Prompting for Accurate Medical Symptom Coding Using Large Language Models",
    "url": "http://arxiv.org/abs/2504.03051v1",
    "arxiv_id": "2504.03051v1",
    "authors": [
      "Chengyang He",
      "Wenlong Zhang",
      "Violet Xinying Chen",
      "Yue Ning",
      "Ping Wang"
    ],
    "published": "2025-04-03T21:57:17+00:00",
    "summary": "Accurate medical symptom coding from unstructured clinical text, such as vaccine safety reports, is a critical task with applications in pharmacovigilance and safety monitoring. Symptom coding, as tailored in this study, involves identifying and linking nuanced symptom mentions to standardized vocabularies like MedDRA, differentiating it from broader medical coding tasks. Traditional approaches to this task, which treat symptom extraction and linking as independent workflows, often fail to handle the variability and complexity of clinical narratives, especially for rare cases. Recent advancements in Large Language Models (LLMs) offer new opportunities but face challenges in achieving consistent performance. To address these issues, we propose Task as Context (TACO) Prompting, a novel framework that unifies extraction and linking tasks by embedding task-specific context into LLM prompts. Our study also introduces SYMPCODER, a human-annotated dataset derived from Vaccine Adverse Event Reporting System (VAERS) reports, and a two-stage evaluation framework to comprehensively assess both symptom linking and mention fidelity. Our comprehensive evaluation of multiple LLMs, including Llama2-chat, Jackalope-7b, GPT-3.5 Turbo, GPT-4 Turbo, and GPT-4o, demonstrates TACO's effectiveness in improving flexibility and accuracy for tailored tasks like symptom coding, paving the way for more specific coding tasks and advancing clinical text processing methodologies."
  },
  {
    "title": "Safety Modulation: Enhancing Safety in Reinforcement Learning through Cost-Modulated Rewards",
    "url": "http://arxiv.org/abs/2504.03040v1",
    "arxiv_id": "2504.03040v1",
    "authors": [
      "Hanping Zhang",
      "Yuhong Guo"
    ],
    "published": "2025-04-03T21:35:22+00:00",
    "summary": "Safe Reinforcement Learning (Safe RL) aims to train an RL agent to maximize its performance in real-world environments while adhering to safety constraints, as exceeding safety violation limits can result in severe consequences. In this paper, we propose a novel safe RL approach called Safety Modulated Policy Optimization (SMPO), which enables safe policy function learning within the standard policy optimization framework through safety modulated rewards. In particular, we consider safety violation costs as feedback from the RL environments that are parallel to the standard awards, and introduce a Q-cost function as safety critic to estimate expected future cumulative costs. Then we propose to modulate the rewards using a cost-aware weighting function, which is carefully designed to ensure the safety limits based on the estimation of the safety critic, while maximizing the expected rewards. The policy function and the safety critic are simultaneously learned through gradient descent during online interactions with the environment. We conduct experiments using multiple RL environments and the experimental results demonstrate that our method outperforms several classic and state-of-the-art comparison methods in terms of overall safe RL performance."
  },
  {
    "title": "How to Adapt Control Barrier Functions? A Learning-Based Approach with Applications to a VTOL Quadplane",
    "url": "http://arxiv.org/abs/2504.03038v1",
    "arxiv_id": "2504.03038v1",
    "authors": [
      "Taekyung Kim",
      "Randal W. Beard",
      "Dimitra Panagou"
    ],
    "published": "2025-04-03T21:32:32+00:00",
    "summary": "In this paper, we present a novel theoretical framework for online adaptation of Control Barrier Function (CBF) parameters, i.e., of the class K functions included in the CBF condition, under input constraints. We introduce the concept of locally validated CBF parameters, which are adapted online to guarantee finite-horizon safety, based on conditions derived from Nagumo's theorem and tangent cone analysis. To identify these parameters online, we integrate a learning-based approach with an uncertainty-aware verification process that account for both epistemic and aleatoric uncertainties inherent in neural network predictions. Our method is demonstrated on a VTOL quadplane model during challenging transition and landing maneuvers, showcasing enhanced performance while maintaining safety."
  },
  {
    "title": "Autonomy Architectures for Safe Planning in Unknown Environments Under Budget Constraints",
    "url": "http://arxiv.org/abs/2504.03001v1",
    "arxiv_id": "2504.03001v1",
    "authors": [
      "Daniel M. Cherenson",
      "Devansh R. Agrawal",
      "Dimitra Panagou"
    ],
    "published": "2025-04-03T19:46:45+00:00",
    "summary": "Mission planning can often be formulated as a constrained control problem under multiple path constraints (i.e., safety constraints) and budget constraints (i.e., resource expenditure constraints). In a priori unknown environments, verifying that an offline solution will satisfy the constraints for all time can be difficult, if not impossible. Our contributions are as follows: 1) We propose an online method, building on our previous work \"gatekeeper\", to guarantee safety and satisfy budget constraints of the system trajectory at all times throughout a mission. 2) Next, we prove that our algorithm is recursively feasible and correct. 3) Finally, instead of using a heuristically designed backup controller, we propose a sampling-based method to construct backup trajectories that both minimize resource expenditure and reach budget renewal sets, in which path constraints are satisfied and the constrained resources are renewed. We demonstrate our approach in simulation with a fixed-wing UAV in a GNSS-denied environment with a budget constraint on localization error that can be renewed at visual landmarks."
  },
  {
    "title": "GPT-ImgEval: A Comprehensive Benchmark for Diagnosing GPT4o in Image Generation",
    "url": "http://arxiv.org/abs/2504.02782v1",
    "arxiv_id": "2504.02782v1",
    "authors": [
      "Zhiyuan Yan",
      "Junyan Ye",
      "Weijia Li",
      "Zilong Huang",
      "Shenghai Yuan",
      "Xiangyang He",
      "Kaiqing Lin",
      "Jun He",
      "Conghui He",
      "Li Yuan"
    ],
    "published": "2025-04-03T17:23:16+00:00",
    "summary": "The recent breakthroughs in OpenAI's GPT4o model have demonstrated surprisingly good capabilities in image generation and editing, resulting in significant excitement in the community. This technical report presents the first-look evaluation benchmark (named GPT-ImgEval), quantitatively and qualitatively diagnosing GPT-4o's performance across three critical dimensions: (1) generation quality, (2) editing proficiency, and (3) world knowledge-informed semantic synthesis. Across all three tasks, GPT-4o demonstrates strong performance, significantly surpassing existing methods in both image generation control and output quality, while also showcasing exceptional knowledge reasoning capabilities. Furthermore, based on the GPT-4o's generated data, we propose a classification-model-based approach to investigate the underlying architecture of GPT-4o, where our empirical results suggest the model consists of an auto-regressive (AR) combined with a diffusion-based head for image decoding, rather than the VAR-like architectures. We also provide a complete speculation on GPT-4o's overall architecture. In addition, we conduct a series of analyses to identify and visualize GPT-4o's specific limitations and the synthetic artifacts commonly observed in its image generation. We also present a comparative study of multi-round image editing between GPT-4o and Gemini 2.0 Flash, and discuss the safety implications of GPT-4o's outputs, particularly their detectability by existing image forensic models. We hope that our work can offer valuable insight and provide a reliable benchmark to guide future research, foster reproducibility, and accelerate innovation in the field of image generation and beyond. The codes and datasets used for evaluating GPT-4o can be found at https://github.com/PicoTrex/GPT-ImgEval."
  },
  {
    "title": "Multi-Head Adaptive Graph Convolution Network for Sparse Point Cloud-Based Human Activity Recognition",
    "url": "http://arxiv.org/abs/2504.02778v1",
    "arxiv_id": "2504.02778v1",
    "authors": [
      "Vincent Gbouna Zakka",
      "Luis J. Manso",
      "Zhuangzhuang Dai"
    ],
    "published": "2025-04-03T17:19:20+00:00",
    "summary": "Human activity recognition is increasingly vital for supporting independent living, particularly for the elderly and those in need of assistance. Domestic service robots with monitoring capabilities can enhance safety and provide essential support. Although image-based methods have advanced considerably in the past decade, their adoption remains limited by concerns over privacy and sensitivity to low-light or dark conditions. As an alternative, millimetre-wave (mmWave) radar can produce point cloud data which is privacy-preserving. However, processing the sparse and noisy point clouds remains a long-standing challenge. While graph-based methods and attention mechanisms show promise, they predominantly rely on \"fixed\" kernels; kernels that are applied uniformly across all neighbourhoods, highlighting the need for adaptive approaches that can dynamically adjust their kernels to the specific geometry of each local neighbourhood in point cloud data. To overcome this limitation, we introduce an adaptive approach within the graph convolutional framework. Instead of a single shared weight function, our Multi-Head Adaptive Kernel (MAK) module generates multiple dynamic kernels, each capturing different aspects of the local feature space. By progressively refining local features while maintaining global spatial context, our method enables convolution kernels to adapt to varying local features. Experimental results on benchmark datasets confirm the effectiveness of our approach, achieving state-of-the-art performance in human activity recognition. Our source code is made publicly available at: https://github.com/Gbouna/MAK-GCN"
  },
  {
    "title": "LiDAR-based Object Detection with Real-time Voice Specifications",
    "url": "http://arxiv.org/abs/2504.02920v1",
    "arxiv_id": "2504.02920v1",
    "authors": [
      "Anurag Kulkarni"
    ],
    "published": "2025-04-03T16:50:38+00:00",
    "summary": "This paper presents a LiDAR-based object detection system with real-time voice specifications, integrating KITTI's 3D point clouds and RGB images through a multi-modal PointNet framework. It achieves 87.0% validation accuracy on a 3000-sample subset, surpassing a 200-sample baseline of 67.5% by combining spatial and visual data, addressing class imbalance with weighted loss, and refining training via adaptive techniques. A Tkinter prototype provides natural Indian male voice output using Edge TTS (en-IN-PrabhatNeural), alongside 3D visualizations and real-time feedback, enhancing accessibility and safety in autonomous navigation, assistive technology, and beyond. The study offers a detailed methodology, comprehensive experimental analysis, and a broad review of applications and challenges, establishing this work as a scalable advancement in human-computer interaction and environmental perception, aligned with current research trends."
  },
  {
    "title": "RBR4DNN: Requirements-based Testing of Neural Networks",
    "url": "http://arxiv.org/abs/2504.02737v1",
    "arxiv_id": "2504.02737v1",
    "authors": [
      "Nusrat Jahan Mozumder",
      "Felipe Toledo",
      "Swaroopa Dola",
      "Matthew B. Dwyer"
    ],
    "published": "2025-04-03T16:24:49+00:00",
    "summary": "Deep neural network (DNN) testing is crucial for the reliability and safety of critical systems, where failures can have severe consequences. Although various techniques have been developed to create robustness test suites, requirements-based testing for DNNs remains largely unexplored -- yet such tests are recognized as an essential component of software validation of critical systems. In this work, we propose a requirements-based test suite generation method that uses structured natural language requirements formulated in a semantic feature space to create test suites by prompting text-conditional latent diffusion models with the requirement precondition and then using the associated postcondition to define a test oracle to judge outputs of the DNN under test. We investigate the approach using fine-tuned variants of pre-trained generative models. Our experiments on the MNIST, CelebA-HQ, ImageNet, and autonomous car driving datasets demonstrate that the generated test suites are realistic, diverse, consistent with preconditions, and capable of revealing faults."
  },
  {
    "title": "RBT4DNN: Requirements-based Testing of Neural Networks",
    "url": "http://arxiv.org/abs/2504.02737v2",
    "arxiv_id": "2504.02737v2",
    "authors": [
      "Nusrat Jahan Mozumder",
      "Felipe Toledo",
      "Swaroopa Dola",
      "Matthew B. Dwyer"
    ],
    "published": "2025-04-03T16:24:49+00:00",
    "summary": "Deep neural network (DNN) testing is crucial for the reliability and safety of critical systems, where failures can have severe consequences. Although various techniques have been developed to create robustness test suites, requirements-based testing for DNNs remains largely unexplored - yet such tests are recognized as an essential component of software validation of critical systems. In this work, we propose a requirements-based test suite generation method that uses structured natural language requirements formulated in a semantic feature space to create test suites by prompting text-conditional latent diffusion models with the requirement precondition and then using the associated postcondition to define a test oracle to judge outputs of the DNN under test. We investigate the approach using fine-tuned variants of pre-trained generative models. Our experiments on the MNIST, CelebA-HQ, ImageNet, and autonomous car driving datasets demonstrate that the generated test suites are realistic, diverse, consistent with preconditions, and capable of revealing faults."
  },
  {
    "title": "ERPO: Advancing Safety Alignment via Ex-Ante Reasoning Preference Optimization",
    "url": "http://arxiv.org/abs/2504.02725v1",
    "arxiv_id": "2504.02725v1",
    "authors": [
      "Kehua Feng",
      "Keyan Ding",
      "Jing Yu",
      "Menghan Li",
      "Yuhao Wang",
      "Tong Xu",
      "Xinda Wang",
      "Qiang Zhang",
      "Huajun Chen"
    ],
    "published": "2025-04-03T16:07:38+00:00",
    "summary": "Recent advancements in large language models (LLMs) have accelerated progress toward artificial general intelligence, yet their potential to generate harmful content poses critical safety challenges. Existing alignment methods often struggle to cover diverse safety scenarios and remain vulnerable to adversarial attacks. In this work, we propose Ex-Ante Reasoning Preference Optimization (ERPO), a novel safety alignment framework that equips LLMs with explicit preemptive reasoning through Chain-of-Thought and provides clear evidence for safety judgments by embedding predefined safety rules. Specifically, our approach consists of three stages: first, equipping the model with Ex-Ante reasoning through supervised fine-tuning (SFT) using a constructed reasoning module; second, enhancing safety, usefulness, and efficiency via Direct Preference Optimization (DPO); and third, mitigating inference latency with a length-controlled iterative preference optimization strategy. Experiments on multiple open-source LLMs demonstrate that ERPO significantly enhances safety performance while maintaining response efficiency."
  },
  {
    "title": "The Hidden Space of Safety: Understanding Preference-Tuned LLMs in Multilingual context",
    "url": "http://arxiv.org/abs/2504.02708v1",
    "arxiv_id": "2504.02708v1",
    "authors": [
      "Nikhil Verma",
      "Manasa Bharadwaj"
    ],
    "published": "2025-04-03T15:46:46+00:00",
    "summary": "Alignment tuning has enabled large language models to excel in reasoning, instruction-following, and minimizing harmful generations. However, despite their widespread deployment, these models exhibit a monolingual bias, raising concerns about the effectiveness of alignment across languages. Current alignment methods predominantly focus on English, leaving it unclear how alignment mechanism generalize to multilingual settings. To address this, we conduct a systematic analysis of distributional shifts in the embedding space of LLMs before and after alignment, uncovering its impact on model behavior across diverse languages. We leverage the alignment-induced separation in safety space as a quantitative tool to measure how alignment enforces safety constraints. Our study evaluates seven LLMs using balanced toxicity datasets and parallel text-detoxification benchmarks, revealing substantial disparities in the latent representation space between high-resource and low-resource languages. These findings underscore the need for language-specific fine-tuning to ensure fair, reliable and robust multilingual alignment. Our insights provide a foundation for developing truly safe multilingual LLMs, emphasizing the urgency of addressing alignment gaps in underrepresented languages."
  },
  {
    "title": "Handover and SINR-Aware Path Optimization in 5G-UAV mmWave Communication using DRL",
    "url": "http://arxiv.org/abs/2504.02688v1",
    "arxiv_id": "2504.02688v1",
    "authors": [
      "Achilles Kiwanuka Machumilane",
      "Alberto Gotta",
      "Pietro Cassar\u00e0"
    ],
    "published": "2025-04-03T15:28:04+00:00",
    "summary": "Path planning and optimization for unmanned aerial vehicles (UAVs)-assisted next-generation wireless networks is critical for mobility management and ensuring UAV safety and ubiquitous connectivity, especially in dense urban environments with street canyons and tall buildings. Traditional statistical and model-based techniques have been successfully used for path optimization in communication networks. However, when dynamic channel propagation characteristics such as line-of-sight (LOS), interference, handover, and signal-to-interference and noise ratio (SINR) are included in path optimization, statistical and model-based path planning solutions become obsolete since they cannot adapt to the dynamic and time-varying wireless channels, especially in the mmWave bands. In this paper, we propose a novel model-free actor-critic deep reinforcement learning (AC-DRL) framework for path optimization in UAV-assisted 5G mmWave wireless networks, which combines four important aspects of UAV communication: \\textit{flight time, handover, connectivity and SINR}. We train an AC-RL agent that enables a UAV connected to a gNB to determine the optimal path to a desired destination in the shortest possible time with minimal gNB handover, while maintaining connectivity and the highest possible SINR. We train our model with data from a powerful ray tracing tool called Wireless InSite, which uses 3D images of the propagation environment and provides data that closely resembles the real propagation environment. The simulation results show that our system has superior performance in tracking high SINR compared to other selected RL algorithms."
  },
  {
    "title": "STOOD-X methodology: using statistical nonparametric test for OOD Detection Large-Scale datasets enhanced with explainability",
    "url": "http://arxiv.org/abs/2504.02685v1",
    "arxiv_id": "2504.02685v1",
    "authors": [
      "Iv\u00e1n Sevillano-Garc\u00eda",
      "Juli\u00e1n Luengo",
      "Francisco Herrera"
    ],
    "published": "2025-04-03T15:26:03+00:00",
    "summary": "Out-of-Distribution (OOD) detection is a critical task in machine learning, particularly in safety-sensitive applications where model failures can have serious consequences. However, current OOD detection methods often suffer from restrictive distributional assumptions, limited scalability, and a lack of interpretability. To address these challenges, we propose STOOD-X, a two-stage methodology that combines a Statistical nonparametric Test for OOD Detection with eXplainability enhancements. In the first stage, STOOD-X uses feature-space distances and a Wilcoxon-Mann-Whitney test to identify OOD samples without assuming a specific feature distribution. In the second stage, it generates user-friendly, concept-based visual explanations that reveal the features driving each decision, aligning with the BLUE XAI paradigm. Through extensive experiments on benchmark datasets and multiple architectures, STOOD-X achieves competitive performance against state-of-the-art post hoc OOD detectors, particularly in high-dimensional and complex settings. In addition, its explainability framework enables human oversight, bias detection, and model debugging, fostering trust and collaboration between humans and AI systems. The STOOD-X methodology therefore offers a robust, explainable, and scalable solution for real-world OOD detection tasks."
  },
  {
    "title": "A Set-Theoretic Robust Control Approach for Linear Quadratic Games with Unknown Counterparts",
    "url": "http://arxiv.org/abs/2504.02679v1",
    "arxiv_id": "2504.02679v1",
    "authors": [
      "Francesco Bianchin",
      "Robert Lefringhausen",
      "Elisa Gaetan",
      "Samuel Tesfazgi",
      "Sandra Hirche"
    ],
    "published": "2025-04-03T15:15:46+00:00",
    "summary": "Ensuring robust decision-making in multi-agent systems is challenging when agents have distinct, possibly conflicting objectives and lack full knowledge of each other s strategies. This is apparent in safety-critical applications such as human-robot interaction and assisted driving, where uncertainty arises not only from unknown adversary strategies but also from external disturbances. To address this, the paper proposes a robust adaptive control approach based on linear quadratic differential games. Our method allows a controlled agent to iteratively refine its belief about the adversary strategy and disturbances using a set-membership approach, while simultaneously adapting its policy to guarantee robustness against the uncertain adversary policy and improve performance over time. We formally derive theoretical guarantees on the robustness of the proposed control scheme and its convergence to epsilon-Nash strategies. The effectiveness of our approach is demonstrated in a numerical simulation."
  },
  {
    "title": "Affordable AI Assistants with Knowledge Graph of Thoughts",
    "url": "http://arxiv.org/abs/2504.02670v1",
    "arxiv_id": "2504.02670v1",
    "authors": [
      "Maciej Besta",
      "Lorenzo Paleari",
      "Jia Hao Andrea Jiang",
      "Robert Gerstenberger",
      "You Wu",
      "Patrick Iff",
      "Ales Kubicek",
      "Piotr Nyczyk",
      "Diana Khimey",
      "J\u00f3n Gunnar Hannesson",
      "Grzegorz Kwa\u015bniewski",
      "Marcin Copik",
      "Hubert Niewiadomski",
      "Torsten Hoefler"
    ],
    "published": "2025-04-03T15:11:55+00:00",
    "summary": "Large Language Models (LLMs) are revolutionizing the development of AI assistants capable of performing diverse tasks across domains. However, current state-of-the-art LLM-driven agents face significant challenges, including high operational costs and limited success rates on complex benchmarks like GAIA. To address these issues, we propose the Knowledge Graph of Thoughts (KGoT), an innovative AI assistant architecture that integrates LLM reasoning with dynamically constructed knowledge graphs (KGs). KGoT extracts and structures task-relevant knowledge into a dynamic KG representation, iteratively enhanced through external tools such as math solvers, web crawlers, and Python scripts. Such structured representation of task-relevant knowledge enables low-cost models to solve complex tasks effectively. For example, KGoT achieves a 29% improvement in task success rates on the GAIA benchmark compared to Hugging Face Agents with GPT-4o mini, while reducing costs by over 36x compared to GPT-4o. Improvements for recent reasoning models are similar, e.g., 36% and 37.5% for Qwen2.5-32B and Deepseek-R1-70B, respectively. KGoT offers a scalable, affordable, and high-performing solution for AI assistants."
  },
  {
    "title": "How humans evaluate AI systems for person detection in automatic train operation: Not all misses are alike",
    "url": "http://arxiv.org/abs/2504.02664v1",
    "arxiv_id": "2504.02664v1",
    "authors": [
      "Romy M\u00fcller"
    ],
    "published": "2025-04-03T15:05:57+00:00",
    "summary": "If artificial intelligence (AI) is to be applied in safety-critical domains, its performance needs to be evaluated reliably. The present study aimed to understand how humans evaluate AI systems for person detection in automatic train operation. In three experiments, participants saw image sequences of people moving in the vicinity of railway tracks. A simulated AI had highlighted all detected people, sometimes correctly and sometimes not. Participants had to provide a numerical rating of the AI's performance and then verbally explain their rating. The experiments varied several factors that might influence human ratings: the types and plausibility of AI mistakes, the number of affected images, the number of people present in an image, the position of people relevant to the tracks, and the methods used to elicit human evaluations. While all these factors influenced human ratings, some effects were unexpected or deviated from normative standards. For instance, the factor with the strongest impact was people's position relative to the tracks, although participants had explicitly been instructed that the AI could not process such information. Taken together, the results suggest that humans may sometimes evaluate more than the AI's performance on the assigned task. Such mismatches between AI capabilities and human expectations should be taken into consideration when conducting safety audits of AI systems."
  },
  {
    "title": "Learning Geometrically-Informed Lyapunov Functions with Deep Diffeomorphic RBF Networks",
    "url": "http://arxiv.org/abs/2504.02607v1",
    "arxiv_id": "2504.02607v1",
    "authors": [
      "Samuel Tesfazgi",
      "Leonhard Sprandl",
      "Sandra Hirche"
    ],
    "published": "2025-04-03T14:09:17+00:00",
    "summary": "The practical deployment of learning-based autonomous systems would greatly benefit from tools that flexibly obtain safety guarantees in the form of certificate functions from data. While the geometrical properties of such certificate functions are well understood, synthesizing them using machine learning techniques still remains a challenge. To mitigate this issue, we propose a diffeomorphic function learning framework where prior structural knowledge of the desired output is encoded in the geometry of a simple surrogate function, which is subsequently augmented through an expressive, topology-preserving state-space transformation. Thereby, we achieve an indirect function approximation framework that is guaranteed to remain in the desired hypothesis space. To this end, we introduce a novel approach to construct diffeomorphic maps based on RBF networks, which facilitate precise, local transformations around data. Finally, we demonstrate our approach by learning diffeomorphic Lyapunov functions from real-world data and apply our method to different attractor systems."
  },
  {
    "title": "Technical Overview of Recent Developments in Small Modular Reactors in the United States",
    "url": "http://arxiv.org/abs/2504.02599v1",
    "arxiv_id": "2504.02599v1",
    "authors": [
      "Yifan Sun",
      "Ken Kurosaki"
    ],
    "published": "2025-04-03T14:01:32+00:00",
    "summary": "Small modular reactors (SMRs) are a class of advanced nuclear fission reactors characterized by their compact core size (typically <300 MWe) and passive safety systems. Their modular design enables on-site assembly, making them suitable for deployment in locations inaccessible to conventional large-scale reactors. With rising global energy demand, particularly driven by the growth of AI, SMRs have recently gained attention as a potential solution for powering data centers. This technical review aims to provide the public and relevant stakeholders with a foundational understanding of SMR technology. It begins with an overview of SMR concepts, historical context, and their current role in the U.S. energy mix. Detailed technical summaries of nine selected SMR designs are then presented, covering core design, fuel systems, reactivity control, and safety features. The report also outlines key regulatory frameworks, including 10 CFR Part 50, Part 52, and the technology-inclusive, risk-informed, and performance-based framework currently under development. Finally, major U.S. programs and legislative efforts supporting SMR deployment over the past decade are summarized."
  },
  {
    "title": "\"I Feel Like I'm Teaching in a Gladiator Ring\": Barriers and Benefits of Live Coding in Classroom Settings",
    "url": "http://arxiv.org/abs/2504.02585v1",
    "arxiv_id": "2504.02585v1",
    "authors": [
      "Caroline Berger",
      "David Weintrop",
      "Niklas Elmqvist"
    ],
    "published": "2025-04-03T13:50:56+00:00",
    "summary": "Live coding for teaching-synchronously writing software in front of students-can be an effective method for engaging students and instilling practical programming skills. However, not all settings are conducive to live coding and not all instructors are successful in this challenging task. We present results from a study involving university instructors, teaching assistants, and students identifying both barriers and benefits of live coding. Physical infrastructure, a positive classroom community with psychological safety, and opportunities for teacher development are practical considerations for live coding. In order for live coding to be an active learning experience, we recommend that tools support multiple mechanisms for engaging students, directing audience attention, and encouraging student-led live coding."
  },
  {
    "title": "Language-Integrated Recursive Queries",
    "url": "http://arxiv.org/abs/2504.02443v1",
    "arxiv_id": "2504.02443v1",
    "authors": [
      "Anna Herlihy",
      "Anastasia Ailamaki",
      "Martin Odersky",
      "Amir Shaikhha"
    ],
    "published": "2025-04-03T09:58:52+00:00",
    "summary": "Performance-critical industrial applications, including large-scale program, network, and distributed system analyses, rely on fixed-point computations. The introduction of recursive common table expressions (CTEs) using the WITH RECURSIVE keyword in SQL:1999 extended the ability of relational database systems to handle fixed-point computations, unlocking significant performance advantages by allowing computation to move closer to the data. Yet with recursion, SQL becomes a Turing-complete programming language and, with that, unrecoverable safety and correctness risks. SQL itself lacks a fixed semantics, as the SQL specification is written in natural language, full of ambiguities that database vendors resolve in divergent ways. As a result, reasoning about the correctness of recursive SQL programs must rely on isolated mathematical properties of queries rather than wrestling a unified formal model out of a language with notoriously inconsistent semantics. To address these challenges, we propose a calculus that automatically derives mathematical properties from embedded recursive queries and, depending on the database backend, rejects queries that may lead to the three classes of recursive query errors - database errors, incorrect results, and non-termination. We introduce TyQL, a practical implementation in Scala for safe, recursive language-integrated query. Using Named-Tuples and type-level pattern matching, TyQL ensures query portability and safety, showing no performance penalty compared to raw SQL strings while unlocking a three-orders-of-magnitude speedup over non-recursive SQL queries."
  },
  {
    "title": "On learning racing policies with reinforcement learning",
    "url": "http://arxiv.org/abs/2504.02420v1",
    "arxiv_id": "2504.02420v1",
    "authors": [
      "Grzegorz Czechmanowski",
      "Jan W\u0119grzynowski",
      "Piotr Kicki",
      "Krzysztof Walas"
    ],
    "published": "2025-04-03T09:21:48+00:00",
    "summary": "Fully autonomous vehicles promise enhanced safety and efficiency. However, ensuring reliable operation in challenging corner cases requires control algorithms capable of performing at the vehicle limits. We address this requirement by considering the task of autonomous racing and propose solving it by learning a racing policy using Reinforcement Learning (RL). Our approach leverages domain randomization, actuator dynamics modeling, and policy architecture design to enable reliable and safe zero-shot deployment on a real platform. Evaluated on the F1TENTH race car, our RL policy not only surpasses a state-of-the-art Model Predictive Control (MPC), but, to the best of our knowledge, also represents the first instance of an RL policy outperforming expert human drivers in RC racing. This work identifies the key factors driving this performance improvement, providing critical insights for the design of robust RL-based control strategies for autonomous vehicles."
  },
  {
    "title": "AnesBench: Multi-Dimensional Evaluation of LLM Reasoning in Anesthesiology",
    "url": "http://arxiv.org/abs/2504.02404v1",
    "arxiv_id": "2504.02404v1",
    "authors": [
      "Xiang Feng",
      "Wentao Jiang",
      "Zengmao Wang",
      "Yong Luo",
      "Pingbo Xu",
      "Baosheng Yu",
      "Hua Jin",
      "Bo Du",
      "Jing Zhang"
    ],
    "published": "2025-04-03T08:54:23+00:00",
    "summary": "The application of large language models (LLMs) in the medical field has gained significant attention, yet their reasoning capabilities in more specialized domains like anesthesiology remain underexplored. In this paper, we systematically evaluate the reasoning capabilities of LLMs in anesthesiology and analyze key factors influencing their performance. To this end, we introduce AnesBench, a cross-lingual benchmark designed to assess anesthesiology-related reasoning across three levels: factual retrieval (System 1), hybrid reasoning (System 1.x), and complex decision-making (System 2). Through extensive experiments, we first explore how model characteristics, including model scale, Chain of Thought (CoT) length, and language transferability, affect reasoning performance. Then, we further evaluate the effectiveness of different training strategies, leveraging our curated anesthesiology-related dataset, including continuous pre-training (CPT) and supervised fine-tuning (SFT). Additionally, we also investigate how the test-time reasoning techniques, such as Best-of-N sampling and beam search, influence reasoning performance, and assess the impact of reasoning-enhanced model distillation, specifically DeepSeek-R1. We will publicly release AnesBench, along with our CPT and SFT training datasets and evaluation code at https://github.com/MiliLab/AnesBench."
  },
  {
    "title": "A Comparative Study of MINLP and MPVC Formulations for Solving Complex Nonlinear Decision-Making Problems in Aerospace Applications",
    "url": "http://arxiv.org/abs/2504.02375v1",
    "arxiv_id": "2504.02375v1",
    "authors": [
      "Andrea Ghezzi",
      "Armin Nurkanovi\u0107",
      "Avishai Weiss",
      "Moritz Diehl",
      "Stefano Di Cairano"
    ],
    "published": "2025-04-03T08:08:52+00:00",
    "summary": "High-level decision-making for dynamical systems often involves performance and safety specifications that are activated or deactivated depending on conditions related to the system state and commands. Such decision-making problems can be naturally formulated as optimization problems where these conditional activations are regulated by discrete variables. However, solving these problems can be challenging numerically, even on powerful computing platforms, especially when the dynamics are nonlinear. In this work, we consider decision-making for nonlinear systems where certain constraints, as well as possible terms in the cost function, are activated or deactivated depending on the system state and commands. We show that these problems can be formulated either as mixed-integer nonlinear programs (MINLPs) or as mathematical programs with vanishing constraints (MPVCs), where the former formulation involves discrete decision variables, whereas the latter relies on continuous variables subject to structured nonconvex constraints. We discuss the different solution methods available for both formulations and demonstrate them on optimal trajectory planning problems in various aerospace applications. Finally, we compare the strengths and weaknesses of the MINLP and MPVC approaches through a focused case study on powered descent guidance with divert-feasible regions."
  },
  {
    "title": "Liquid Neural Networks: Next-Generation AI for Telecom from First Principles",
    "url": "http://arxiv.org/abs/2504.02352v1",
    "arxiv_id": "2504.02352v1",
    "authors": [
      "Fenghao Zhu",
      "Xinquan Wang",
      "Chen Zhu",
      "Chongwen Huang"
    ],
    "published": "2025-04-03T07:41:04+00:00",
    "summary": "Artificial intelligence (AI) has emerged as a transformative technology with immense potential to reshape the next-generation of wireless networks. By leveraging advanced algorithms and machine learning techniques, AI offers unprecedented capabilities in optimizing network performance, enhancing data processing efficiency, and enabling smarter decision-making processes. However, existing AI solutions face significant challenges in terms of robustness and interpretability. Specifically, current AI models exhibit substantial performance degradation in dynamic environments with varying data distributions, and the black-box nature of these algorithms raises concerns regarding safety, transparency, and fairness. This presents a major challenge in integrating AI into practical communication systems. Recently, a novel type of neural network, known as the liquid neural networks (LNNs), has been designed from first principles to address these issues. In this paper, we explore the potential of LNNs in telecommunications. First, we illustrate the mechanisms of LNNs and highlight their unique advantages over traditional networks. Then we unveil the opportunities that LNNs bring to future wireless networks. Furthermore, we discuss the challenges and design directions for the implementation of LNNs. Finally, we summarize the performance of LNNs in two case studies."
  },
  {
    "title": "Evaluating and Enhancing Segmentation Model Robustness with Metamorphic Testing",
    "url": "http://arxiv.org/abs/2504.02335v1",
    "arxiv_id": "2504.02335v1",
    "authors": [
      "Seif Mzoughi",
      "Mohamed Elshafeia",
      "Foutse Khomh"
    ],
    "published": "2025-04-03T07:15:45+00:00",
    "summary": "Image segmentation is critical for applications such as medical imaging, augmented reality, and video surveillance. However, segmentation models often lack robustness, making them vulnerable to adversarial perturbations from subtle image distortions. In this work, we propose SegRMT, a metamorphic testing approach that leverages genetic algorithms (GA) to optimize sequences of spatial and spectral transformations while preserving image fidelity via a predefined PSNR threshold. Using the Cityscapes dataset, our method generates adversarial examples that effectively challenge the DeepLabV3 segmentation model. Our experiments show that SegRMT reduces DeepLabV3's mean Intersection over Union (mIoU) to 6.4%, outperforming other adversarial baselines that decrease mIoU to between 8.5% and 21.7%. Furthermore, when used for adversarial training, SegRMT boosts model performance, achieving mIoU improvements up to 73% on dedicated adversarial datasets and increasing cross-adversarial mIoU to 53.8%, compared to only 2%-10% for other methods. These findings demonstrate that SegRMT not only simulates realistic image distortions but also enhances the robustness of segmentation models, making it a valuable tool for ensuring reliable performance in safety-critical applications."
  },
  {
    "title": "Towards Assessing Deep Learning Test Input Generators",
    "url": "http://arxiv.org/abs/2504.02329v1",
    "arxiv_id": "2504.02329v1",
    "authors": [
      "Seif Mzoughi",
      "Ahmed Hajyahmed",
      "Mohamed Elshafei",
      "Foutse Khomh anb Diego Elias Costa"
    ],
    "published": "2025-04-03T07:06:55+00:00",
    "summary": "Deep Learning (DL) systems are increasingly deployed in safety-critical applications, yet they remain vulnerable to robustness issues that can lead to significant failures. While numerous Test Input Generators (TIGs) have been developed to evaluate DL robustness, a comprehensive assessment of their effectiveness across different dimensions is still lacking. This paper presents a comprehensive assessment of four state-of-the-art TIGs--DeepHunter, DeepFault, AdvGAN, and SinVAD--across multiple critical aspects: fault-revealing capability, naturalness, diversity, and efficiency. Our empirical study leverages three pre-trained models (LeNet-5, VGG16, and EfficientNetB3) on datasets of varying complexity (MNIST, CIFAR-10, and ImageNet-1K) to evaluate TIG performance. Our findings reveal important trade-offs in robustness revealing capability, variation in test case generation, and computational efficiency across TIGs. The results also show that TIG performance varies significantly with dataset complexity, as tools that perform well on simpler datasets may struggle with more complex ones. In contrast, others maintain steadier performance or better scalability. This paper offers practical guidance for selecting appropriate TIGs aligned with specific objectives and dataset characteristics. Nonetheless, more work is needed to address TIG limitations and advance TIGs for real-world, safety-critical systems."
  },
  {
    "title": "Improving Harmful Text Detection with Joint Retrieval and External Knowledge",
    "url": "http://arxiv.org/abs/2504.02310v1",
    "arxiv_id": "2504.02310v1",
    "authors": [
      "Zidong Yu",
      "Shuo Wang",
      "Nan Jiang",
      "Weiqiang Huang",
      "Xu Han",
      "Junliang Du"
    ],
    "published": "2025-04-03T06:37:55+00:00",
    "summary": "Harmful text detection has become a crucial task in the development and deployment of large language models, especially as AI-generated content continues to expand across digital platforms. This study proposes a joint retrieval framework that integrates pre-trained language models with knowledge graphs to improve the accuracy and robustness of harmful text detection. Experimental results demonstrate that the joint retrieval approach significantly outperforms single-model baselines, particularly in low-resource training scenarios and multilingual environments. The proposed method effectively captures nuanced harmful content by leveraging external contextual information, addressing the limitations of traditional detection models. Future research should focus on optimizing computational efficiency, enhancing model interpretability, and expanding multimodal detection capabilities to better tackle evolving harmful content patterns. This work contributes to the advancement of AI safety, ensuring more trustworthy and reliable content moderation systems."
  },
  {
    "title": "C*: Unifying Programming and Verification in C",
    "url": "http://arxiv.org/abs/2504.02246v1",
    "arxiv_id": "2504.02246v1",
    "authors": [
      "Yiyuan Cao",
      "Jiayi Zhuang",
      "Houjin Chen",
      "Jinkai Fan",
      "Wenbo Xu",
      "Zhiyi Wang",
      "Di Wang",
      "Qinxiang Cao",
      "Yingfei Xiong",
      "Haiyan Zhao",
      "Zhenjiang Hu"
    ],
    "published": "2025-04-03T03:22:22+00:00",
    "summary": "Ensuring the correct functionality of systems software, given its safety-critical and low-level nature, is a primary focus in formal verification research and applications. Despite advances in verification tooling, conventional programmers are rarely involved in the verification of their own code, resulting in higher development and maintenance costs for verified software. A key barrier to programmer participation in verification practices is the disconnect of environments and paradigms between programming and verification practices, which limits accessibility and real-time verification.   We introduce C*, a proof-integrated language design for C programming. C* extends C with verification capabilities, powered by a symbolic execution engine and an LCF-style proof kernel. It enables real-time verification by allowing programmers to embed proof-code blocks alongside implementation code, facilitating interactive updates to the current proof state. Its expressive and extensible proof support allows users to build reusable libraries of logical definitions, theorems, and programmable proof automation. Crucially, C* unifies implementation and proof code development by using C as the common language.   We implemented a prototype of C* and evaluated it on a representative benchmark of small C programs and a challenging real-world case study: the attach function of pKVM's buddy allocator. Our results demonstrate that C* supports the verification of a broad subset of C programming idioms and effectively handles complex reasoning tasks in real-world scenarios."
  },
  {
    "title": "Fundamental elements in the development of effective crowd control strategies",
    "url": "http://arxiv.org/abs/2504.02224v1",
    "arxiv_id": "2504.02224v1",
    "authors": [
      "Claudio Feliciani",
      "Daichi Yanagisawa",
      "Katsuhiro Nishinari"
    ],
    "published": "2025-04-03T02:46:36+00:00",
    "summary": "In this work, we present typical challenges encountered when developing methods for controlling crowds of people (or animal swarms). We discuss which elements shall be considered and the role they play to achieve a robust control in a variety of conditions. In particular, four different studies are reviewed, each of them investigating in detail important elements encountered in crowd steering and control. More specifically synchronization, compliance, crowd (or swarm) density and human perception are studied showing the role they play in combination. Ultimately, the success of a control strategy is determined by carefully considering the effect each element has on individuals, but also on the interactions between them, leading to the creation of a collective behavior. We will also highlight the importance of psychological and cognitive factors when dealing with human crowds, hinting at the fact that automatic control systems may achieve optimal performance, but may be not necessarily well perceived by people in terms of comfort. The discussion aims at showing recent trends and potentialities of crowd control systems, but should also warn on the risk in choosing a solution prioritizing optimization toward people's safety or comfort."
  },
  {
    "title": "More is Less: The Pitfalls of Multi-Model Synthetic Preference Data in DPO Safety Alignment",
    "url": "http://arxiv.org/abs/2504.02193v1",
    "arxiv_id": "2504.02193v1",
    "authors": [
      "Yifan Wang",
      "Runjin Chen",
      "Bolian Li",
      "David Cho",
      "Yihe Deng",
      "Ruqi Zhang",
      "Tianlong Chen",
      "Zhangyang Wang",
      "Ananth Grama",
      "Junyuan Hong"
    ],
    "published": "2025-04-03T00:36:40+00:00",
    "summary": "Aligning large language models (LLMs) with human values is an increasingly critical step in post-training. Direct Preference Optimization (DPO) has emerged as a simple, yet effective alternative to reinforcement learning from human feedback (RLHF). Synthetic preference data with its low cost and high quality enable effective alignment through single- or multi-model generated preference data. Our study reveals a striking, safety-specific phenomenon associated with DPO alignment: Although multi-model generated data enhances performance on general tasks (ARC, Hellaswag, MMLU, TruthfulQA, Winogrande) by providing diverse responses, it also tends to facilitate reward hacking during training. This can lead to a high attack success rate (ASR) when models encounter jailbreaking prompts. The issue is particularly pronounced when employing stronger models like GPT-4o or larger models in the same family to generate chosen responses paired with target model self-generated rejected responses, resulting in dramatically poorer safety outcomes. Furthermore, with respect to safety, using solely self-generated responses (single-model generation) for both chosen and rejected pairs significantly outperforms configurations that incorporate responses from stronger models, whether used directly as chosen data or as part of a multi-model response pool. We demonstrate that multi-model preference data exhibits high linear separability between chosen and rejected responses, which allows models to exploit superficial cues rather than internalizing robust safety constraints. Our experiments, conducted on models from the Llama, Mistral, and Qwen families, consistently validate these findings."
  },
  {
    "title": "Subasa -- Adapting Language Models for Low-resourced Offensive Language Detection in Sinhala",
    "url": "http://arxiv.org/abs/2504.02178v1",
    "arxiv_id": "2504.02178v1",
    "authors": [
      "Shanilka Haturusinghe",
      "Tharindu Cyril Weerasooriya",
      "Marcos Zampieri",
      "Christopher M. Homan",
      "S. R. Liyanage"
    ],
    "published": "2025-04-02T23:46:49+00:00",
    "summary": "Accurate detection of offensive language is essential for a number of applications related to social media safety. There is a sharp contrast in performance in this task between low and high-resource languages. In this paper, we adapt fine-tuning strategies that have not been previously explored for Sinhala in the downstream task of offensive language detection. Using this approach, we introduce four models: \"Subasa-XLM-R\", which incorporates an intermediate Pre-Finetuning step using Masked Rationale Prediction. Two variants of \"Subasa-Llama\" and \"Subasa-Mistral\", are fine-tuned versions of Llama (3.2) and Mistral (v0.3), respectively, with a task-specific strategy. We evaluate our models on the SOLD benchmark dataset for Sinhala offensive language detection. All our models outperform existing baselines. Subasa-XLM-R achieves the highest Macro F1 score (0.84) surpassing state-of-the-art large language models like GPT-4o when evaluated on the same SOLD benchmark dataset under zero-shot settings. The models and code are publicly available."
  },
  {
    "title": "Preference-Driven Active 3D Scene Representation for Robotic Inspection in Nuclear Decommissioning",
    "url": "http://arxiv.org/abs/2504.02161v1",
    "arxiv_id": "2504.02161v1",
    "authors": [
      "Zhen Meng",
      "Kan Chen",
      "Xiangmin Xu",
      "Erwin Jose Lopez Pulgarin",
      "Emma Li",
      "Philip G. Zhao",
      "David Flynn"
    ],
    "published": "2025-04-02T22:20:48+00:00",
    "summary": "Active 3D scene representation is pivotal in modern robotics applications, including remote inspection, manipulation, and telepresence. Traditional methods primarily optimize geometric fidelity or rendering accuracy, but often overlook operator-specific objectives, such as safety-critical coverage or task-driven viewpoints. This limitation leads to suboptimal viewpoint selection, particularly in constrained environments such as nuclear decommissioning. To bridge this gap, we introduce a novel framework that integrates expert operator preferences into the active 3D scene representation pipeline. Specifically, we employ Reinforcement Learning from Human Feedback (RLHF) to guide robotic path planning, reshaping the reward function based on expert input. To capture operator-specific priorities, we conduct interactive choice experiments that evaluate user preferences in 3D scene representation. We validate our framework using a UR3e robotic arm for reactor tile inspection in a nuclear decommissioning scenario. Compared to baseline methods, our approach enhances scene representation while optimizing trajectory efficiency. The RLHF-based policy consistently outperforms random selection, prioritizing task-critical details. By unifying explicit 3D geometric modeling with implicit human-in-the-loop optimization, this work establishes a foundation for adaptive, safety-critical robotic perception systems, paving the way for enhanced automation in nuclear decommissioning, remote maintenance, and other high-risk environments."
  },
  {
    "title": "UAC: Uncertainty-Aware Calibration of Neural Networks for Gesture Detection",
    "url": "http://arxiv.org/abs/2504.02895v1",
    "arxiv_id": "2504.02895v1",
    "authors": [
      "Farida Al Haddad",
      "Yuxin Wang",
      "Malcolm Mielle"
    ],
    "published": "2025-04-02T21:40:01+00:00",
    "summary": "Artificial intelligence has the potential to impact safety and efficiency in safety-critical domains such as construction, manufacturing, and healthcare. For example, using sensor data from wearable devices, such as inertial measurement units (IMUs), human gestures can be detected while maintaining privacy, thereby ensuring that safety protocols are followed. However, strict safety requirements in these domains have limited the adoption of AI, since accurate calibration of predicted probabilities and robustness against out-of-distribution (OOD) data is necessary.   This paper proposes UAC (Uncertainty-Aware Calibration), a novel two-step method to address these challenges in IMU-based gesture recognition. First, we present an uncertainty-aware gesture network architecture that predicts both gesture probabilities and their associated uncertainties from IMU data. This uncertainty is then used to calibrate the probabilities of each potential gesture. Second, an entropy-weighted expectation of predictions over multiple IMU data windows is used to improve accuracy while maintaining correct calibration.   Our method is evaluated using three publicly available IMU datasets for gesture detection and is compared to three state-of-the-art calibration methods for neural networks: temperature scaling, entropy maximization, and Laplace approximation. UAC outperforms existing methods, achieving improved accuracy and calibration in both OOD and in-distribution scenarios. Moreover, we find that, unlike our method, none of the state-of-the-art methods significantly improve the calibration of IMU-based gesture recognition models. In conclusion, our work highlights the advantages of uncertainty-aware calibration of neural networks, demonstrating improvements in both calibration and accuracy for gesture detection using IMU data."
  },
  {
    "title": "On Simulation-Guided LLM-based Code Generation for Safe Autonomous Driving Software",
    "url": "http://arxiv.org/abs/2504.02141v1",
    "arxiv_id": "2504.02141v1",
    "authors": [
      "Ali Nouri",
      "Johan Andersson",
      "Kailash De Jesus Hornig",
      "Zhennan Fei",
      "Emil Knabe",
      "Hakan Sivencrona",
      "Beatriz Cabrero-Daniel",
      "Christian Berger"
    ],
    "published": "2025-04-02T21:35:11+00:00",
    "summary": "Automated Driving System (ADS) is a safety-critical software system responsible for the interpretation of the vehicle's environment and making decisions accordingly. The unbounded complexity of the driving context, including unforeseeable events, necessitate continuous improvement, often achieved through iterative DevOps processes. However, DevOps processes are themselves complex, making these improvements both time- and resource-intensive. Automation in code generation for ADS using Large Language Models (LLM) is one potential approach to address this challenge. Nevertheless, the development of ADS requires rigorous processes to verify, validate, assess, and qualify the code before it can be deployed in the vehicle and used. In this study, we developed and evaluated a prototype for automatic code generation and assessment using a designed pipeline of a LLM-based agent, simulation model, and rule-based feedback generator in an industrial setup. The LLM-generated code is evaluated automatically in a simulation model against multiple critical traffic scenarios, and an assessment report is provided as feedback to the LLM for modification or bug fixing. We report about the experimental results of the prototype employing Codellama:34b, DeepSeek (r1:32b and Coder:33b), CodeGemma:7b, Mistral:7b, and GPT4 for Adaptive Cruise Control (ACC) and Unsupervised Collision Avoidance by Evasive Manoeuvre (CAEM). We finally assessed the tool with 11 experts at two Original Equipment Manufacturers (OEMs) by conducting an interview study."
  },
  {
    "title": "Reinsuring AI: Energy, Agriculture, Finance & Medicine as Precedents for Scalable Governance of Frontier Artificial Intelligence",
    "url": "http://arxiv.org/abs/2504.02127v1",
    "arxiv_id": "2504.02127v1",
    "authors": [
      "Nicholas Stetler"
    ],
    "published": "2025-04-02T21:02:19+00:00",
    "summary": "The governance of frontier artificial intelligence (AI) systems--particularly those capable of catastrophic misuse or systemic failure--requires institutional structures that are robust, adaptive, and innovation-preserving. This paper proposes a novel framework for governing such high-stakes models through a three-tiered insurance architecture: (1) mandatory private liability insurance for frontier model developers; (2) an industry-administered risk pool to absorb recurring, non-catastrophic losses; and (3) federally backed reinsurance for tail-risk events. Drawing from historical precedents in nuclear energy (Price-Anderson), terrorism risk (TRIA), agricultural crop insurance, flood reinsurance, and medical malpractice, the proposal shows how the federal government can stabilize private AI insurance markets without resorting to brittle regulation or predictive licensing regimes. The structure aligns incentives between AI developers and downstream stakeholders, transforms safety practices into insurable standards, and enables modular oversight through adaptive eligibility criteria. By focusing on risk-transfer mechanisms rather than prescriptive rules, this framework seeks to render AI safety a structural feature of the innovation ecosystem itself--integrated into capital markets, not external to them. The paper concludes with a legal and administrative feasibility analysis, proposing avenues for statutory authorization and agency placement within existing federal structures."
  },
  {
    "title": "Evolving Security in LLMs: A Study of Jailbreak Attacks and Defenses",
    "url": "http://arxiv.org/abs/2504.02080v1",
    "arxiv_id": "2504.02080v1",
    "authors": [
      "Zhengchun Shang",
      "Wenlan Wei"
    ],
    "published": "2025-04-02T19:33:07+00:00",
    "summary": "Large Language Models (LLMs) are increasingly popular, powering a wide range of applications. Their widespread use has sparked concerns, especially through jailbreak attacks that bypass safety measures to produce harmful content.   In this paper, we present a comprehensive security analysis of large language models (LLMs), addressing critical research questions on the evolution and determinants of model safety.   Specifically, we begin by identifying the most effective techniques for detecting jailbreak attacks. Next, we investigate whether newer versions of LLMs offer improved security compared to their predecessors. We also assess the impact of model size on overall security and explore the potential benefits of integrating multiple defense strategies to enhance model robustness.   Our study evaluates both open-source models (e.g., LLaMA and Mistral) and closed-source systems (e.g., GPT-4) by employing four state-of-the-art attack techniques and assessing the efficacy of three new defensive approaches."
  },
  {
    "title": "Trapped by Expectations: Functional Fixedness in LLM-Enabled Chat Search",
    "url": "http://arxiv.org/abs/2504.02074v1",
    "arxiv_id": "2504.02074v1",
    "authors": [
      "Jiqun Liu",
      "Jamshed Karimnazarov",
      "Ryen W. White"
    ],
    "published": "2025-04-02T19:14:01+00:00",
    "summary": "Functional fixedness, a cognitive bias that restricts users' interactions with a new system or tool to expected or familiar ways, limits the full potential of Large Language Model (LLM)-enabled chat search, especially in complex and exploratory tasks. To investigate its impact, we conducted a crowdsourcing study with 450 participants, each completing one of six decision-making tasks spanning public safety, diet and health management, sustainability, and AI ethics. Participants engaged in a multi-prompt conversation with ChatGPT to address the task, allowing us to compare pre-chat intent-based expectations with observed interactions. We found that: 1) Several aspects of pre-chat expectations are closely associated with users' prior experiences with ChatGPT, search engines, and virtual assistants; 2) Prior system experience shapes language use and prompting behavior. Frequent ChatGPT users reduced deictic terms and hedge words and frequently adjusted prompts. Users with rich search experience maintained structured, less-conversational queries with minimal modifications. Users of virtual assistants favored directive, command-like prompts, reinforcing functional fixedness; 3) When the system failed to meet expectations, participants generated more detailed prompts with increased linguistic diversity, reflecting adaptive shifts. These findings suggest that while preconceived expectations constrain early interactions, unmet expectations can motivate behavioral adaptation. With appropriate system support, this may promote broader exploration of LLM capabilities. This work also introduces a typology for user intents in chat search and highlights the importance of mitigating functional fixedness to support more creative and analytical use of LLMs."
  },
  {
    "title": "End-to-End Driving with Online Trajectory Evaluation via BEV World Model",
    "url": "http://arxiv.org/abs/2504.01941v1",
    "arxiv_id": "2504.01941v1",
    "authors": [
      "Yingyan Li",
      "Yuqi Wang",
      "Yang Liu",
      "Jiawei He",
      "Lue Fan",
      "Zhaoxiang Zhang"
    ],
    "published": "2025-04-02T17:47:23+00:00",
    "summary": "End-to-end autonomous driving has achieved remarkable progress by integrating perception, prediction, and planning into a fully differentiable framework. Yet, to fully realize its potential, an effective online trajectory evaluation is indispensable to ensure safety. By forecasting the future outcomes of a given trajectory, trajectory evaluation becomes much more effective. This goal can be achieved by employing a world model to capture environmental dynamics and predict future states. Therefore, we propose an end-to-end driving framework WoTE, which leverages a BEV World model to predict future BEV states for Trajectory Evaluation. The proposed BEV world model is latency-efficient compared to image-level world models and can be seamlessly supervised using off-the-shelf BEV-space traffic simulators. We validate our framework on both the NAVSIM benchmark and the closed-loop Bench2Drive benchmark based on the CARLA simulator, achieving state-of-the-art performance. Code is released at https://github.com/liyingyanUCAS/WoTE."
  },
  {
    "title": "STAR-1: Safer Alignment of Reasoning LLMs with 1K Data",
    "url": "http://arxiv.org/abs/2504.01903v1",
    "arxiv_id": "2504.01903v1",
    "authors": [
      "Zijun Wang",
      "Haoqin Tu",
      "Yuhan Wang",
      "Juncheng Wu",
      "Jieru Mei",
      "Brian R. Bartoldson",
      "Bhavya Kailkhura",
      "Cihang Xie"
    ],
    "published": "2025-04-02T17:04:04+00:00",
    "summary": "This paper introduces STAR-1, a high-quality, just-1k-scale safety dataset specifically designed for large reasoning models (LRMs) like DeepSeek-R1. Built on three core principles -- diversity, deliberative reasoning, and rigorous filtering -- STAR-1 aims to address the critical needs for safety alignment in LRMs. Specifically, we begin by integrating existing open-source safety datasets from diverse sources. Then, we curate safety policies to generate policy-grounded deliberative reasoning samples. Lastly, we apply a GPT-4o-based safety scoring system to select training examples aligned with best practices. Experimental results show that fine-tuning LRMs with STAR-1 leads to an average 40% improvement in safety performance across four benchmarks, while only incurring a marginal decrease (e.g., an average of 1.1%) in reasoning ability measured across five reasoning tasks. Extensive ablation studies further validate the importance of our design principles in constructing STAR-1 and analyze its efficacy across both LRMs and traditional LLMs. Our project page is https://ucsc-vlaa.github.io/STAR-1."
  },
  {
    "title": "Focal Mechanism Uncertainty Quantification In Ground Motion Simulations Of Le Teil Earthquake",
    "url": "http://arxiv.org/abs/2504.01868v1",
    "arxiv_id": "2504.01868v1",
    "authors": [
      "Valeria Soto",
      "Fernando Lopez-Caballero"
    ],
    "published": "2025-04-02T16:22:46+00:00",
    "summary": "Ensuring the seismic safety of nuclear power plants (NPPs) is essential, especially for facilities that rely on base isolation to reduce earthquake impacts. For understanding the seismic response, accurate models are key to predict the ground motions, which are generally sensitive to various factors, including earthquake source parameters like the focal mechanism, i.e., strike, dip, and rake angles. This study examines how uncertainties in these parameters affect ground motion predictions. The analysis is based on the SMATCH benchmark, which provides a standardized approach for evaluating the seismic response of the Cruas-Meysse NPP in France during the Mw 4.9 Le-Teil earthquake of 2019. A set of 27 3D high-fidelity numerical simulations was performed using a spectral-element method, each incorporating different focal mechanism variations. These simulations provide an effective approach for investigating the factors behind the exceptional ground motion observed during this event. To quantify uncertainty, the simulated ground motions were compared to recorded data using two well-established goodness-of-fit criteria: one assessing time-frequency domain characteristics and another focusing on the characterization of the ground motion signals by intensity measures. Results highlight the significant influence of focal mechanism variability on ground motion predictions, especially on the rake angle, which showed the strongest correlation with wave and intensity measures."
  },
  {
    "title": "An Approach to Technical AGI Safety and Security",
    "url": "http://arxiv.org/abs/2504.01849v1",
    "arxiv_id": "2504.01849v1",
    "authors": [
      "Rohin Shah",
      "Alex Irpan",
      "Alexander Matt Turner",
      "Anna Wang",
      "Arthur Conmy",
      "David Lindner",
      "Jonah Brown-Cohen",
      "Lewis Ho",
      "Neel Nanda",
      "Raluca Ada Popa",
      "Rishub Jain",
      "Rory Greig",
      "Samuel Albanie",
      "Scott Emmons",
      "Sebastian Farquhar",
      "S\u00e9bastien Krier",
      "Senthooran Rajamanoharan",
      "Sophie Bridgers",
      "Tobi Ijitoye",
      "Tom Everitt",
      "Victoria Krakovna",
      "Vikrant Varma",
      "Vladimir Mikulik",
      "Zachary Kenton",
      "Dave Orr",
      "Shane Legg",
      "Noah Goodman",
      "Allan Dafoe",
      "Four Flynn",
      "Anca Dragan"
    ],
    "published": "2025-04-02T15:59:31+00:00",
    "summary": "Artificial General Intelligence (AGI) promises transformative benefits but also presents significant risks. We develop an approach to address the risk of harms consequential enough to significantly harm humanity. We identify four areas of risk: misuse, misalignment, mistakes, and structural risks. Of these, we focus on technical approaches to misuse and misalignment. For misuse, our strategy aims to prevent threat actors from accessing dangerous capabilities, by proactively identifying dangerous capabilities, and implementing robust security, access restrictions, monitoring, and model safety mitigations. To address misalignment, we outline two lines of defense. First, model-level mitigations such as amplified oversight and robust training can help to build an aligned model. Second, system-level security measures such as monitoring and access control can mitigate harm even if the model is misaligned. Techniques from interpretability, uncertainty estimation, and safer design patterns can enhance the effectiveness of these mitigations. Finally, we briefly outline how these ingredients could be combined to produce safety cases for AGI systems."
  },
  {
    "title": "Barrier Certificates for Unknown Systems with Latent States and Polynomial Dynamics using Bayesian Inference",
    "url": "http://arxiv.org/abs/2504.01807v1",
    "arxiv_id": "2504.01807v1",
    "authors": [
      "Robert Lefringhausen",
      "Sami Leon Noel Aziz Hanna",
      "Elias August",
      "Sandra Hirche"
    ],
    "published": "2025-04-02T15:12:34+00:00",
    "summary": "Certifying safety in dynamical systems is crucial, but barrier certificates - widely used to verify that system trajectories remain within a safe region - typically require explicit system models. When dynamics are unknown, data-driven methods can be used instead, yet obtaining a valid certificate requires rigorous uncertainty quantification. For this purpose, existing methods usually rely on full-state measurements, limiting their applicability. This paper proposes a novel approach for synthesizing barrier certificates for unknown systems with latent states and polynomial dynamics. A Bayesian framework is employed, where a prior in state-space representation is updated using input-output data via a targeted marginal Metropolis-Hastings sampler. The resulting samples are used to construct a candidate barrier certificate through a sum-of-squares program. It is shown that if the candidate satisfies the required conditions on a test set of additional samples, it is also valid for the true, unknown system with high probability. The approach and its probabilistic guarantees are illustrated through a numerical simulation."
  },
  {
    "title": "Beyond Non-Expert Demonstrations: Outcome-Driven Action Constraint for Offline Reinforcement Learning",
    "url": "http://arxiv.org/abs/2504.01719v1",
    "arxiv_id": "2504.01719v1",
    "authors": [
      "Ke Jiang",
      "Wen Jiang",
      "Yao Li",
      "Xiaoyang Tan"
    ],
    "published": "2025-04-02T13:27:44+00:00",
    "summary": "We address the challenge of offline reinforcement learning using realistic data, specifically non-expert data collected through sub-optimal behavior policies. Under such circumstance, the learned policy must be safe enough to manage \\textit{distribution shift} while maintaining sufficient flexibility to deal with non-expert (bad) demonstrations from offline data.To tackle this issue, we introduce a novel method called Outcome-Driven Action Flexibility (ODAF), which seeks to reduce reliance on the empirical action distribution of the behavior policy, hence reducing the negative impact of those bad demonstrations.To be specific, a new conservative reward mechanism is developed to deal with {\\it distribution shift} by evaluating actions according to whether their outcomes meet safety requirements - remaining within the state support area, rather than solely depending on the actions' likelihood based on offline data.Besides theoretical justification, we provide empirical evidence on widely used MuJoCo and various maze benchmarks, demonstrating that our ODAF method, implemented using uncertainty quantification techniques, effectively tolerates unseen transitions for improved \"trajectory stitching,\" while enhancing the agent's ability to learn from realistic non-expert data."
  },
  {
    "title": "Beyond Non-Expert Demonstrations: Outcome-Driven Action Constraint for Offline Reinforcement Learning",
    "url": "http://arxiv.org/abs/2504.01719v2",
    "arxiv_id": "2504.01719v2",
    "authors": [
      "Ke Jiang",
      "Wen Jiang",
      "Yao Li",
      "Xiaoyang Tan"
    ],
    "published": "2025-04-02T13:27:44+00:00",
    "summary": "We address the challenge of offline reinforcement learning using realistic data, specifically non-expert data collected through sub-optimal behavior policies. Under such circumstance, the learned policy must be safe enough to manage distribution shift while maintaining sufficient flexibility to deal with non-expert (bad) demonstrations from offline data.To tackle this issue, we introduce a novel method called Outcome-Driven Action Flexibility (ODAF), which seeks to reduce reliance on the empirical action distribution of the behavior policy, hence reducing the negative impact of those bad demonstrations.To be specific, a new conservative reward mechanism is developed to deal with distribution shift by evaluating actions according to whether their outcomes meet safety requirements - remaining within the state support area, rather than solely depending on the actions' likelihood based on offline data.Besides theoretical justification, we provide empirical evidence on widely used MuJoCo and various maze benchmarks, demonstrating that our ODAF method, implemented using uncertainty quantification techniques, effectively tolerates unseen transitions for improved \"trajectory stitching,\" while enhancing the agent's ability to learn from realistic non-expert data."
  },
  {
    "title": "Testing Low-Resource Language Support in LLMs Using Language Proficiency Exams: the Case of Luxembourgish",
    "url": "http://arxiv.org/abs/2504.01667v1",
    "arxiv_id": "2504.01667v1",
    "authors": [
      "Cedric Lothritz",
      "Jordi Cabot"
    ],
    "published": "2025-04-02T12:16:14+00:00",
    "summary": "Large Language Models (LLMs) have become an increasingly important tool in research and society at large. While LLMs are regularly used all over the world by experts and lay-people alike, they are predominantly developed with English-speaking users in mind, performing well in English and other wide-spread languages while less-resourced languages such as Luxembourgish are seen as a lower priority. This lack of attention is also reflected in the sparsity of available evaluation tools and datasets. In this study, we investigate the viability of language proficiency exams as such evaluation tools for the Luxembourgish language. We find that large models such as ChatGPT, Claude and DeepSeek-R1 typically achieve high scores, while smaller models show weak performances. We also find that the performances in such language exams can be used to predict performances in other NLP tasks."
  },
  {
    "title": "Testing Low-Resource Language Support in LLMs Using Language Proficiency Exams: the Case of Luxembourgish",
    "url": "http://arxiv.org/abs/2504.01667v2",
    "arxiv_id": "2504.01667v2",
    "authors": [
      "Cedric Lothritz",
      "Jordi Cabot"
    ],
    "published": "2025-04-02T12:16:14+00:00",
    "summary": "Large Language Models (LLMs) have become an increasingly important tool in research and society at large. While LLMs are regularly used all over the world by experts and lay-people alike, they are predominantly developed with English-speaking users in mind, performing well in English and other wide-spread languages while less-resourced languages such as Luxembourgish are seen as a lower priority. This lack of attention is also reflected in the sparsity of available evaluation tools and datasets. In this study, we investigate the viability of language proficiency exams as such evaluation tools for the Luxembourgish language. We find that large models such as ChatGPT, Claude and DeepSeek-R1 typically achieve high scores, while smaller models show weak performances. We also find that the performances in such language exams can be used to predict performances in other NLP tasks."
  },
  {
    "title": "Convex Computations for Controlled Safety Invariant Sets of Black-box Discrete-time Dynamical Systems",
    "url": "http://arxiv.org/abs/2504.01638v1",
    "arxiv_id": "2504.01638v1",
    "authors": [
      "Taoran Wu",
      "Yiling Xue",
      "Jingduo Pan",
      "Dejin Ren",
      "Arvind Easwaran",
      "Bai Xue"
    ],
    "published": "2025-04-02T11:43:03+00:00",
    "summary": "Identifying controlled safety invariant sets (CSISs) is essential in safety-critical applications. This paper tackles the problem of identifying CSISs for black-box discrete-time systems, where the model is unknown and only limited simulation data is accessible. Traditionally, a CSIS is defined as a subset of a safe set, encompassing initial states for which a control input exists that keeps the system within the set at the next time step-this is referred to as the one-step invariance property. However, the requirement for one-step invariance can be equivalently translated into a stricter condition of ``always-invariance'', meaning that there exist control inputs capable of keeping the system within this set indefinitely. Such a condition may prove overly stringent or impractical for black-box systems, where predictions can become unreliable beyond a single time step or a limited number of finite time steps. To overcome the challenges posed by black-box systems, we reformulate the one-step invariance property in a ``Probably Approximately Correct'' (PAC) sense. This approach allows us to assess the probability that a control input exists to keep the system within the CSIS at the next time step, with a predefined level of confidence. If the system successfully remains within the set at the next time step, we can then reapply the invariance evaluation to the new state, thereby facilitating a recursive assurance of invariance. Our method employs barrier functions and scenario optimization, resulting in a linear programming method to estimate PAC CSISs. Finally, the effectiveness of our approach is demonstrated on several examples."
  },
  {
    "title": "Benchmarking the Spatial Robustness of DNNs via Natural and Adversarial Localized Corruptions",
    "url": "http://arxiv.org/abs/2504.01632v1",
    "arxiv_id": "2504.01632v1",
    "authors": [
      "Giulia Marchiori Pietrosanti",
      "Giulio Rossolini",
      "Alessandro Biondi",
      "Giorgio Buttazzo"
    ],
    "published": "2025-04-02T11:37:39+00:00",
    "summary": "The robustness of DNNs is a crucial factor in safety-critical applications, particularly in complex and dynamic environments where localized corruptions can arise. While previous studies have evaluated the robustness of semantic segmentation (SS) models under whole-image natural or adversarial corruptions, a comprehensive investigation into the spatial robustness of dense vision models under localized corruptions remained underexplored. This paper fills this gap by introducing specialized metrics for benchmarking the spatial robustness of segmentation models, alongside with an evaluation framework to assess the impact of localized corruptions. Furthermore, we uncover the inherent complexity of characterizing worst-case robustness using a single localized adversarial perturbation. To address this, we propose region-aware multi-attack adversarial analysis, a method that enables a deeper understanding of model robustness against adversarial perturbations applied to specific regions. The proposed metrics and analysis were evaluated on 15 segmentation models in driving scenarios, uncovering key insights into the effects of localized corruption in both natural and adversarial forms. The results reveal that models respond to these two types of threats differently; for instance, transformer-based segmentation models demonstrate notable robustness to localized natural corruptions but are highly vulnerable to adversarial ones and vice-versa for CNN-based models. Consequently, we also address the challenge of balancing robustness to both natural and adversarial localized corruptions by means of ensemble models, thereby achieving a broader threat coverage and improved reliability for dense vision tasks."
  },
  {
    "title": "Representation Bending for Large Language Model Safety",
    "url": "http://arxiv.org/abs/2504.01550v1",
    "arxiv_id": "2504.01550v1",
    "authors": [
      "Ashkan Yousefpour",
      "Taeheon Kim",
      "Ryan S. Kwon",
      "Seungbeen Lee",
      "Wonje Jeung",
      "Seungju Han",
      "Alvin Wan",
      "Harrison Ngan",
      "Youngjae Yu",
      "Jonghyun Choi"
    ],
    "published": "2025-04-02T09:47:01+00:00",
    "summary": "Large Language Models (LLMs) have emerged as powerful tools, but their inherent safety risks - ranging from harmful content generation to broader societal harms - pose significant challenges. These risks can be amplified by the recent adversarial attacks, fine-tuning vulnerabilities, and the increasing deployment of LLMs in high-stakes environments. Existing safety-enhancing techniques, such as fine-tuning with human feedback or adversarial training, are still vulnerable as they address specific threats and often fail to generalize across unseen attacks, or require manual system-level defenses. This paper introduces RepBend, a novel approach that fundamentally disrupts the representations underlying harmful behaviors in LLMs, offering a scalable solution to enhance (potentially inherent) safety. RepBend brings the idea of activation steering - simple vector arithmetic for steering model's behavior during inference - to loss-based fine-tuning. Through extensive evaluation, RepBend achieves state-of-the-art performance, outperforming prior methods such as Circuit Breaker, RMU, and NPO, with up to 95% reduction in attack success rates across diverse jailbreak benchmarks, all with negligible reduction in model usability and general capabilities."
  },
  {
    "title": "LightDefense: A Lightweight Uncertainty-Driven Defense against Jailbreaks via Shifted Token Distribution",
    "url": "http://arxiv.org/abs/2504.01533v1",
    "arxiv_id": "2504.01533v1",
    "authors": [
      "Zhuoran Yang",
      "Jie Peng",
      "Zhen Tan",
      "Tianlong Chen",
      "Yanyong Zhang"
    ],
    "published": "2025-04-02T09:21:26+00:00",
    "summary": "Large Language Models (LLMs) face threats from jailbreak prompts. Existing methods for defending against jailbreak attacks are primarily based on auxiliary models. These strategies, however, often require extensive data collection or training. We propose LightDefense, a lightweight defense mechanism targeted at white-box models, which utilizes a safety-oriented direction to adjust the probabilities of tokens in the vocabulary, making safety disclaimers appear among the top tokens after sorting tokens by probability in descending order. We further innovatively leverage LLM's uncertainty about prompts to measure their harmfulness and adaptively adjust defense strength, effectively balancing safety and helpfulness. The effectiveness of LightDefense in defending against 5 attack methods across 2 target LLMs, without compromising helpfulness to benign user queries, highlights its potential as a novel and lightweight defense mechanism, enhancing security of LLMs."
  },
  {
    "title": "Enabling Systematic Generalization in Abstract Spatial Reasoning through Meta-Learning for Compositionality",
    "url": "http://arxiv.org/abs/2504.01445v1",
    "arxiv_id": "2504.01445v1",
    "authors": [
      "Philipp Mondorf",
      "Shijia Zhou",
      "Monica Riedler",
      "Barbara Plank"
    ],
    "published": "2025-04-02T07:56:39+00:00",
    "summary": "Systematic generalization refers to the capacity to understand and generate novel combinations from known components. Despite recent progress by large language models (LLMs) across various domains, these models often fail to extend their knowledge to novel compositional scenarios, revealing notable limitations in systematic generalization. There has been an ongoing debate about whether neural networks possess the capacity for systematic generalization, with recent studies suggesting that meta-learning approaches designed for compositionality can significantly enhance this ability. However, these insights have largely been confined to linguistic problems, leaving their applicability to other tasks an open question. In this study, we extend the approach of meta-learning for compositionality to the domain of abstract spatial reasoning. To this end, we introduce $\\textit{SYGAR}$-a dataset designed to evaluate the capacity of models to systematically generalize from known geometric transformations (e.g., translation, rotation) of two-dimensional objects to novel combinations of these transformations (e.g., translation+rotation). Our results show that a transformer-based encoder-decoder model, trained via meta-learning for compositionality, can systematically generalize to previously unseen transformation compositions, significantly outperforming state-of-the-art LLMs, including o3-mini, GPT-4o, and Gemini 2.0 Flash, which fail to exhibit similar systematic behavior. Our findings highlight the effectiveness of meta-learning in promoting systematicity beyond linguistic tasks, suggesting a promising direction toward more robust and generalizable models."
  },
  {
    "title": "PiCo: Jailbreaking Multimodal Large Language Models via $\\textbf{Pi}$ctorial $\\textbf{Co}$de Contextualization",
    "url": "http://arxiv.org/abs/2504.01444v1",
    "arxiv_id": "2504.01444v1",
    "authors": [
      "Aofan Liu",
      "Lulu Tang",
      "Ting Pan",
      "Yuguo Yin",
      "Bin Wang",
      "Ao Yang"
    ],
    "published": "2025-04-02T07:54:32+00:00",
    "summary": "Multimodal Large Language Models (MLLMs), which integrate vision and other modalities into Large Language Models (LLMs), significantly enhance AI capabilities but also introduce new security vulnerabilities. By exploiting the vulnerabilities of the visual modality and the long-tail distribution characteristic of code training data, we present PiCo, a novel jailbreaking framework designed to progressively bypass multi-tiered defense mechanisms in advanced MLLMs. PiCo employs a tier-by-tier jailbreak strategy, using token-level typographic attacks to evade input filtering and embedding harmful intent within programming context instructions to bypass runtime monitoring. To comprehensively assess the impact of attacks, a new evaluation metric is further proposed to assess both the toxicity and helpfulness of model outputs post-attack. By embedding harmful intent within code-style visual instructions, PiCo achieves an average Attack Success Rate (ASR) of 84.13% on Gemini-Pro Vision and 52.66% on GPT-4, surpassing previous methods. Experimental results highlight the critical gaps in current defenses, underscoring the need for more robust strategies to secure advanced MLLMs."
  },
  {
    "title": "Behavioral Inequalities",
    "url": "http://arxiv.org/abs/2504.01437v1",
    "arxiv_id": "2504.01437v1",
    "authors": [
      "Soutrik Bandyopadhyay",
      "Debasattam Pal",
      "Shubhendu Bhasin"
    ],
    "published": "2025-04-02T07:43:22+00:00",
    "summary": "We introduce behavioral inequalities as a way to model dynamical systems defined by inequalities among their variables of interest. We claim that such a formulation enables the representation of safety-aware dynamical systems, systems with bounds on disturbances, practical design limits and operational boundaries, etc. We develop a necessary and sufficient condition for the existence of solutions to such behavioral inequalities and provide a parametrization of solutions when they exist. Finally, we show the efficacy of the proposed method in two practical examples."
  },
  {
    "title": "From Shadows to Safety: Occlusion Tracking and Risk Mitigation for Urban Autonomous Driving",
    "url": "http://arxiv.org/abs/2504.01408v1",
    "arxiv_id": "2504.01408v1",
    "authors": [
      "Korbinian Moller",
      "Luis Schwarzmeier",
      "Johannes Betz"
    ],
    "published": "2025-04-02T06:48:50+00:00",
    "summary": "Autonomous vehicles (AVs) must navigate dynamic urban environments where occlusions and perception limitations introduce significant uncertainties. This research builds upon and extends existing approaches in risk-aware motion planning and occlusion tracking to address these challenges. While prior studies have developed individual methods for occlusion tracking and risk assessment, a comprehensive method integrating these techniques has not been fully explored. We, therefore, enhance a phantom agent-centric model by incorporating sequential reasoning to track occluded areas and predict potential hazards. Our model enables realistic scenario representation and context-aware risk evaluation by modeling diverse phantom agents, each with distinct behavior profiles. Simulations demonstrate that the proposed approach improves situational awareness and balances proactive safety with efficient traffic flow. While these results underline the potential of our method, validation in real-world scenarios is necessary to confirm its feasibility and generalizability. By utilizing and advancing established methodologies, this work contributes to safer and more reliable AV planning in complex urban environments. To support further research, our method is available as open-source software at: https://github.com/TUM-AVS/OcclusionAwareMotionPlanning"
  },
  {
    "title": "Navigating the Uncharted Waters: A Gradual Approach to the Certification and Integration of Maritime Autonomous Surface Ships (MASS) in Global Maritime Operations",
    "url": "http://arxiv.org/abs/2504.01393v1",
    "arxiv_id": "2504.01393v1",
    "authors": [
      "Hany M. Arnaoot"
    ],
    "published": "2025-04-02T06:19:21+00:00",
    "summary": "The integration of Maritime Autonomous Surface Ships (MASS) into global maritime operations represents a transformative shift in the shipping industry, promising enhanced safety, efficiency, and cost-effectiveness. However, the widespread adoption of autonomous ships necessitates a robust regulatory framework and rigorous certification processes to address the unique challenges posed by these advanced technologies. This paper proposes a gradual, multi-stage approach to the certification and integration of MASS, beginning with small-scale trials in controlled environments and progressing to large-scale international operations. Key considerations include the development of reliable control systems, cybersecurity measures, sensor technologies, and redundancy mechanisms to ensure safe and efficient navigation. Additionally, the paper explores the economic and environmental implications of autonomous shipping, as well as the evolving legal frameworks for liability and compensation in the event of collisions. By adopting a cautious and methodical approach, the maritime industry can mitigate risks and pave the way for the safe and sustainable integration of autonomous ships into global trade."
  },
  {
    "title": "When Reasoning Meets Compression: Benchmarking Compressed Large Reasoning Models on Complex Reasoning Tasks",
    "url": "http://arxiv.org/abs/2504.02010v1",
    "arxiv_id": "2504.02010v1",
    "authors": [
      "Nan Zhang",
      "Yusen Zhang",
      "Prasenjit Mitra",
      "Rui Zhang"
    ],
    "published": "2025-04-02T05:17:46+00:00",
    "summary": "Recent open-source large reasoning models (LRMs) exhibit strong performance on complex reasoning tasks, but their large parameter count makes them prohibitively expensive for individuals. The compression of large language models (LLMs) offers an effective solution to reduce cost of computational resources. However, systematic studies on the performance of compressed LLMs in complex reasoning tasks, especially for LRMs, are lacking. Most works on quantization and pruning focus on preserving language modeling performance, while existing distillation works do not comprehensively benchmark student models based on reasoning difficulty or compression impact on knowledge and reasoning. In this paper, we benchmark compressed DeepSeek-R1 models on four different reasoning datasets (AIME 2024, FOLIO, Temporal Sequences of BIG-Bench Hard, and MuSiQue), ranging from mathematical to multihop reasoning, using quantization, distillation, and pruning methods. We benchmark 2.51-, 1.73-, and 1.58-bit R1 models that adopt dynamic quantization. We also benchmark distilled R1 models that are based on LLaMA or Qwen and run SparseGPT on them to obtain various sparsity levels. Studying the performance and behavior of compressed LRMs, we report their performance scores and test-time compute (number of tokens spent on each question). Notably, using MuSiQue, we find that parameter count has a much greater impact on LRMs' knowledge memorization than on their reasoning capability, which can inform the choice of compression techniques. Through our empirical analysis of test-time compute, we find that shorter model outputs generally achieve better performance than longer ones across several benchmarks for both R1 and its compressed variants, highlighting the need for more concise reasoning chains."
  },
  {
    "title": "Urban Computing in the Era of Large Language Models",
    "url": "http://arxiv.org/abs/2504.02009v1",
    "arxiv_id": "2504.02009v1",
    "authors": [
      "Zhonghang Li",
      "Lianghao Xia",
      "Xubin Ren",
      "Jiabin Tang",
      "Tianyi Chen",
      "Yong Xu",
      "Chao Huang"
    ],
    "published": "2025-04-02T05:12:13+00:00",
    "summary": "Urban computing has emerged as a multidisciplinary field that harnesses data-driven technologies to address challenges and improve urban living. Traditional approaches, while beneficial, often face challenges with generalization, scalability, and contextual understanding. The advent of Large Language Models (LLMs) offers transformative potential in this domain. This survey explores the intersection of LLMs and urban computing, emphasizing the impact of LLMs in processing and analyzing urban data, enhancing decision-making, and fostering citizen engagement. We provide a concise overview of the evolution and core technologies of LLMs. Additionally, we survey their applications across key urban domains, such as transportation, public safety, and environmental monitoring, summarizing essential tasks and prior works in various urban contexts, while highlighting LLMs' functional roles and implementation patterns. Building on this, we propose potential LLM-based solutions to address unresolved challenges. To facilitate in-depth research, we compile a list of available datasets and tools applicable to diverse urban scenarios. Finally, we discuss the limitations of current approaches and outline future directions for advancing LLMs in urban computing."
  },
  {
    "title": "Adaptive Rectification Sampling for Test-Time Compute Scaling",
    "url": "http://arxiv.org/abs/2504.01317v1",
    "arxiv_id": "2504.01317v1",
    "authors": [
      "Zhendong Tan",
      "Xingjun Zhang",
      "Chaoyi Hu",
      "Yancheng Pan",
      "Shaoxun Wang"
    ],
    "published": "2025-04-02T02:57:52+00:00",
    "summary": "The newly released OpenAI-o1 and DeepSeek-R1 have demonstrated that test-time scaling can significantly improve model performance, especially in complex tasks such as logical reasoning. Common test-time scaling methods involve generating more chain of thoughts (CoTs) or longer CoTs with self-correction. However, while self-correction can improve performance, it may lead to significant token waste and reduce readability of the CoT if the reasoning steps are already correct. To demonstrate that large language models (LLMs) can rectify errors at a more fine-grained level, we propose Adaptive Rectification Sampling (AR-Sampling), which can guide the LLMs to self-correction at the appropriate step. AR-Sampling leverages a process-supervised reward model (PRM) as a verifier and constructed trigger sentences to guide the model in adaptive step-level rethinking. Through the experiments on GSM8K and MATH500, it indicate that our approach enables the models to rethink in more fine-grained level, improving the accuracy of solutions, while generating a reasonable number of additional tokens."
  },
  {
    "title": "Safeguarding Vision-Language Models: Mitigating Vulnerabilities to Gaussian Noise in Perturbation-based Attacks",
    "url": "http://arxiv.org/abs/2504.01308v1",
    "arxiv_id": "2504.01308v1",
    "authors": [
      "Jiawei Wang",
      "Yushen Zuo",
      "Yuanjun Chai",
      "Zhendong Liu",
      "Yichen Fu",
      "Yichun Feng",
      "Kin-man Lam"
    ],
    "published": "2025-04-02T02:35:19+00:00",
    "summary": "Vision-Language Models (VLMs) extend the capabilities of Large Language Models (LLMs) by incorporating visual information, yet they remain vulnerable to jailbreak attacks, especially when processing noisy or corrupted images. Although existing VLMs adopt security measures during training to mitigate such attacks, vulnerabilities associated with noise-augmented visual inputs are overlooked. In this work, we identify that missing noise-augmented training causes critical security gaps: many VLMs are susceptible to even simple perturbations such as Gaussian noise. To address this challenge, we propose Robust-VLGuard, a multimodal safety dataset with aligned / misaligned image-text pairs, combined with noise-augmented fine-tuning that reduces attack success rates while preserving functionality of VLM. For stronger optimization-based visual perturbation attacks, we propose DiffPure-VLM, leveraging diffusion models to convert adversarial perturbations into Gaussian-like noise, which can be defended by VLMs with noise-augmented safety fine-tuning. Experimental results demonstrate that the distribution-shifting property of diffusion model aligns well with our fine-tuned VLMs, significantly mitigating adversarial perturbations across varying intensities. The dataset and code are available at https://github.com/JarvisUSTC/DiffPure-RobustVLM."
  },
  {
    "title": "ThinkPrune: Pruning Long Chain-of-Thought of LLMs via Reinforcement Learning",
    "url": "http://arxiv.org/abs/2504.01296v1",
    "arxiv_id": "2504.01296v1",
    "authors": [
      "Bairu Hou",
      "Yang Zhang",
      "Jiabao Ji",
      "Yujian Liu",
      "Kaizhi Qian",
      "Jacob Andreas",
      "Shiyu Chang"
    ],
    "published": "2025-04-02T01:59:26+00:00",
    "summary": "We present ThinkPrune, a simple yet effective method for pruning the thinking length for long-thinking LLMs, which has been found to often produce inefficient and redundant thinking processes. Existing preliminary explorations of reducing thinking length primarily focus on forcing the thinking process to early exit, rather than adapting the LLM to optimize and consolidate the thinking process, and therefore the length-performance tradeoff observed so far is sub-optimal. To fill this gap, ThinkPrune offers a simple solution that continuously trains the long-thinking LLMs via reinforcement learning (RL) with an added token limit, beyond which any unfinished thoughts and answers will be discarded, resulting in a zero reward. To further preserve model performance, we introduce an iterative length pruning approach, where multiple rounds of RL are conducted, each with an increasingly more stringent token limit. We observed that ThinkPrune results in a remarkable performance-length tradeoff -- on the AIME24 dataset, the reasoning length of DeepSeek-R1-Distill-Qwen-1.5B can be reduced by half with only 2% drop in performance. We also observed that after pruning, the LLMs can bypass unnecessary steps while keeping the core reasoning process complete. Code is available at https://github.com/UCSB-NLP-Chang/ThinkPrune."
  },
  {
    "title": "Cuddle-Fish: Exploring a Soft Floating Robot with Flapping Wings for Physical Interactions",
    "url": "http://arxiv.org/abs/2504.01293v1",
    "arxiv_id": "2504.01293v1",
    "authors": [
      "Mingyang Xu",
      "Jiayi Shao",
      "Yulan Ju",
      "Ximing Shen",
      "Qingyuan Gao",
      "Weijen Chen",
      "Qing Zhang",
      "Yun Suen Pai",
      "Giulia Barbareschi",
      "Matthias Hoppe",
      "Kouta Minamizawa",
      "Kai Kunze"
    ],
    "published": "2025-04-02T01:53:03+00:00",
    "summary": "Flying robots, such as quadrotor drones, offer new possibilities for human-robot interaction but often pose safety risks due to fast-spinning propellers, rigid structures, and noise. In contrast, lighter-than-air flapping-wing robots, inspired by animal movement, offer a soft, quiet, and touch-safe alternative. Building on these advantages, we present \\textit{Cuddle-Fish}, a soft, flapping-wing floating robot designed for safe, close-proximity interactions in indoor spaces. Through a user study with 24 participants, we explored their perceptions of the robot and experiences during a series of co-located demonstrations in which the robot moved near them. Results showed that participants felt safe, willingly engaged in touch-based interactions with the robot, and exhibited spontaneous affective behaviours, such as patting, stroking, hugging, and cheek-touching, without external prompting. They also reported positive emotional responses towards the robot. These findings suggest that the soft floating robot with flapping wings can serve as a novel and socially acceptable alternative to traditional rigid flying robots, opening new possibilities for companionship, play, and interactive experiences in everyday indoor environments."
  },
  {
    "title": "Strategize Globally, Adapt Locally: A Multi-Turn Red Teaming Agent with Dual-Level Learning",
    "url": "http://arxiv.org/abs/2504.01278v1",
    "arxiv_id": "2504.01278v1",
    "authors": [
      "Si Chen",
      "Xiao Yu",
      "Ninareh Mehrabi",
      "Rahul Gupta",
      "Zhou Yu",
      "Ruoxi Jia"
    ],
    "published": "2025-04-02T01:06:19+00:00",
    "summary": "The exploitation of large language models (LLMs) for malicious purposes poses significant security risks as these models become more powerful and widespread. While most existing red-teaming frameworks focus on single-turn attacks, real-world adversaries typically operate in multi-turn scenarios, iteratively probing for vulnerabilities and adapting their prompts based on threat model responses. In this paper, we propose \\AlgName, a novel multi-turn red-teaming agent that emulates sophisticated human attackers through complementary learning dimensions: global tactic-wise learning that accumulates knowledge over time and generalizes to new attack goals, and local prompt-wise learning that refines implementations for specific goals when initial attempts fail. Unlike previous multi-turn approaches that rely on fixed strategy sets, \\AlgName enables the agent to identify new jailbreak tactics, develop a goal-based tactic selection framework, and refine prompt formulations for selected tactics. Empirical evaluations on JailbreakBench demonstrate our framework's superior performance, achieving over 90\\% attack success rates against GPT-3.5-Turbo and Llama-3.1-70B within 5 conversation turns, outperforming state-of-the-art baselines. These results highlight the effectiveness of dynamic learning in identifying and exploiting model vulnerabilities in realistic multi-turn scenarios."
  },
  {
    "title": "$\u03bc$KE: Matryoshka Unstructured Knowledge Editing of Large Language Models",
    "url": "http://arxiv.org/abs/2504.01196v1",
    "arxiv_id": "2504.01196v1",
    "authors": [
      "Zian Su",
      "Ziyang Huang",
      "Kaiyuan Zhang",
      "Xiangyu Zhang"
    ],
    "published": "2025-04-01T21:24:44+00:00",
    "summary": "Large language models (LLMs) have emerged as powerful knowledge bases yet are limited by static training data, leading to issues such as hallucinations and safety risks. Editing a model's internal knowledge through the locate-and-edit paradigm has proven a cost-effective alternative to retraining, though current unstructured approaches, especially window-based autoregressive methods, often disrupt the causal dependency between early memory updates and later output tokens. In this work, we first theoretically analyze these limitations and then introduce Matryoshka Unstructured Knowledge Editing ($\\mu$KE), a novel memory update mechanism that preserves such dependencies via a Matryoshka-style objective and adaptive loss coefficients. Empirical evaluations on two models across four benchmarks demonstrate that $\\mu$KE improves edit efficacy by up to 12.33% over state-of-the-art methods, and remain robust when applied to diverse formatted edits, underscoring its potential for effective unstructured knowledge editing in LLMs."
  },
  {
    "title": "ACTIVE: Continuous Similarity Search for Vessel Trajectories",
    "url": "http://arxiv.org/abs/2504.01142v1",
    "arxiv_id": "2504.01142v1",
    "authors": [
      "Tiantian Liu",
      "Hengyu Liu",
      "Tianyi Li",
      "Kristian Torp",
      "Christian S. Jensen"
    ],
    "published": "2025-04-01T19:25:27+00:00",
    "summary": "Publicly available vessel trajectory data is emitted continuously from the global AIS system. Continuous trajectory similarity search on this data has applications in, e.g., maritime navigation and safety. Existing proposals typically assume an offline setting and focus on finding similarities between complete trajectories. Such proposals are less effective when applied to online scenarios, where similarity comparisons must be performed continuously as new trajectory data arrives and trajectories evolve. We therefore propose a real-time continuous trajectory similarity search method for vessels (ACTIVE). We introduce a novel similarity measure, object-trajectory real-time distance, that emphasizes the anticipated future movement trends of vessels, enabling more predictive and forward-looking comparisons. Next, we propose a segment-based vessel trajectory index structure that organizes historical trajectories into smaller and manageable segments, facilitating accelerated similarity computations. Leveraging this index, we propose an efficient continuous similar trajectory search (CSTS) algorithm together with a variety of search space pruning strategies that reduce unnecessary computations during the continuous similarity search, thereby further improving efficiency. Extensive experiments on two large real-world AIS datasets offer evidence that ACTIVE is capable of outperforming state-of-the-art methods considerably. ACTIVE significantly reduces index construction costs and index size while achieving a 70% reduction in terms of query time and a 60% increase in terms of hit rate."
  },
  {
    "title": "RipVIS: Rip Currents Video Instance Segmentation Benchmark for Beach Monitoring and Safety",
    "url": "http://arxiv.org/abs/2504.01128v1",
    "arxiv_id": "2504.01128v1",
    "authors": [
      "Andrei Dumitriu",
      "Florin Tatui",
      "Florin Miron",
      "Aakash Ralhan",
      "Radu Tudor Ionescu",
      "Radu Timofte"
    ],
    "published": "2025-04-01T18:57:15+00:00",
    "summary": "Rip currents are strong, localized and narrow currents of water that flow outwards into the sea, causing numerous beach-related injuries and fatalities worldwide. Accurate identification of rip currents remains challenging due to their amorphous nature and the lack of annotated data, which often requires expert knowledge. To address these issues, we present RipVIS, a large-scale video instance segmentation benchmark explicitly designed for rip current segmentation. RipVIS is an order of magnitude larger than previous datasets, featuring $184$ videos ($212,328$ frames), of which $150$ videos ($163,528$ frames) are with rip currents, collected from various sources, including drones, mobile phones, and fixed beach cameras. Our dataset encompasses diverse visual contexts, such as wave-breaking patterns, sediment flows, and water color variations, across multiple global locations, including USA, Mexico, Costa Rica, Portugal, Italy, Greece, Romania, Sri Lanka, Australia and New Zealand. Most videos are annotated at $5$ FPS to ensure accuracy in dynamic scenarios, supplemented by an additional $34$ videos ($48,800$ frames) without rip currents. We conduct comprehensive experiments with Mask R-CNN, Cascade Mask R-CNN, SparseInst and YOLO11, fine-tuning these models for the task of rip current segmentation. Results are reported in terms of multiple metrics, with a particular focus on the $F_2$ score to prioritize recall and reduce false negatives. To enhance segmentation performance, we introduce a novel post-processing step based on Temporal Confidence Aggregation (TCA). RipVIS aims to set a new standard for rip current segmentation, contributing towards safer beach environments. We offer a benchmark website to share data, models, and results with the research community, encouraging ongoing collaboration and future contributions, at https://ripvis.ai."
  },
  {
    "title": "Can LLMs Grasp Implicit Cultural Values? Benchmarking LLMs' Metacognitive Cultural Intelligence with CQ-Bench",
    "url": "http://arxiv.org/abs/2504.01127v1",
    "arxiv_id": "2504.01127v1",
    "authors": [
      "Ziyi Liu",
      "Priyanka Dey",
      "Zhenyu Zhao",
      "Jen-tse Huang",
      "Rahul Gupta",
      "Yang Liu",
      "Jieyu Zhao"
    ],
    "published": "2025-04-01T18:54:47+00:00",
    "summary": "Cultural Intelligence (CQ) refers to the ability to understand unfamiliar cultural contexts-a crucial skill for large language models (LLMs) to effectively engage with globally diverse users. While existing research often focuses on explicitly stated cultural norms, such approaches fail to capture the subtle, implicit values that underlie real-world conversations. To address this gap, we introduce CQ-Bench, a benchmark specifically designed to assess LLMs' capability to infer implicit cultural values from natural conversational contexts. We generate a multi-character conversation-based stories dataset using values from the World Value Survey and GlobalOpinions datasets, with topics including ethical, religious, social, and political. Our dataset construction pipeline includes rigorous validation procedures-incorporation, consistency, and implicitness checks-using GPT-4o, with 98.2% human-model agreement in the final validation. Our benchmark consists of three tasks of increasing complexity: attitude detection, value selection, and value extraction. We find that while o1 and Deepseek-R1 models reach human-level performance in value selection (0.809 and 0.814), they still fall short in nuanced attitude detection, with F1 scores of 0.622 and 0.635, respectively. In the value extraction task, GPT-4o-mini and o3-mini score 0.602 and 0.598, highlighting the difficulty of open-ended cultural reasoning. Notably, fine-tuning smaller models (e.g., LLaMA-3.2-3B) on only 500 culturally rich examples improves performance by over 10%, even outperforming stronger baselines (o3-mini) in some cases. Using CQ-Bench, we provide insights into the current challenges in LLMs' CQ research and suggest practical pathways for enhancing LLMs' cross-cultural reasoning abilities."
  },
  {
    "title": "Multilingual and Multi-Accent Jailbreaking of Audio LLMs",
    "url": "http://arxiv.org/abs/2504.01094v1",
    "arxiv_id": "2504.01094v1",
    "authors": [
      "Jaechul Roh",
      "Virat Shejwalkar",
      "Amir Houmansadr"
    ],
    "published": "2025-04-01T18:12:23+00:00",
    "summary": "Large Audio Language Models (LALMs) have significantly advanced audio understanding but introduce critical security risks, particularly through audio jailbreaks. While prior work has focused on English-centric attacks, we expose a far more severe vulnerability: adversarial multilingual and multi-accent audio jailbreaks, where linguistic and acoustic variations dramatically amplify attack success. In this paper, we introduce Multi-AudioJail, the first systematic framework to exploit these vulnerabilities through (1) a novel dataset of adversarially perturbed multilingual/multi-accent audio jailbreaking prompts, and (2) a hierarchical evaluation pipeline revealing that how acoustic perturbations (e.g., reverberation, echo, and whisper effects) interacts with cross-lingual phonetics to cause jailbreak success rates (JSRs) to surge by up to +57.25 percentage points (e.g., reverberated Kenyan-accented attack on MERaLiON). Crucially, our work further reveals that multimodal LLMs are inherently more vulnerable than unimodal systems: attackers need only exploit the weakest link (e.g., non-English audio inputs) to compromise the entire model, which we empirically show by multilingual audio-only attacks achieving 3.1x higher success rates than text-only attacks. We plan to release our dataset to spur research into cross-modal defenses, urging the community to address this expanding attack surface in multimodality as LALMs evolve."
  },
  {
    "title": "ShieldGemma 2: Robust and Tractable Image Content Moderation",
    "url": "http://arxiv.org/abs/2504.01081v1",
    "arxiv_id": "2504.01081v1",
    "authors": [
      "Wenjun Zeng",
      "Dana Kurniawan",
      "Ryan Mullins",
      "Yuchi Liu",
      "Tamoghna Saha",
      "Dirichi Ike-Njoku",
      "Jindong Gu",
      "Yiwen Song",
      "Cai Xu",
      "Jingjing Zhou",
      "Aparna Joshi",
      "Shravan Dheep",
      "Mani Malek",
      "Hamid Palangi",
      "Joon Baek",
      "Rick Pereira",
      "Karthik Narasimhan"
    ],
    "published": "2025-04-01T18:00:20+00:00",
    "summary": "We introduce ShieldGemma 2, a 4B parameter image content moderation model built on Gemma 3. This model provides robust safety risk predictions across the following key harm categories: Sexually Explicit, Violence \\& Gore, and Dangerous Content for synthetic images (e.g. output of any image generation model) and natural images (e.g. any image input to a Vision-Language Model). We evaluated on both internal and external benchmarks to demonstrate state-of-the-art performance compared to LlavaGuard \\citep{helff2024llavaguard}, GPT-4o mini \\citep{hurst2024gpt}, and the base Gemma 3 model \\citep{gemma_2025} based on our policies. Additionally, we present a novel adversarial data generation pipeline which enables a controlled, diverse, and robust image generation. ShieldGemma 2 provides an open image moderation tool to advance multimodal safety and responsible AI development."
  },
  {
    "title": "Data-Driven Safety Verification using Barrier Certificates and Matrix Zonotopes",
    "url": "http://arxiv.org/abs/2504.01007v1",
    "arxiv_id": "2504.01007v1",
    "authors": [
      "Mohammed Adib Oumer",
      "Amr Alanwar",
      "Majid Zamani"
    ],
    "published": "2025-04-01T17:46:42+00:00",
    "summary": "Ensuring safety in cyber-physical systems (CPSs) is a critical challenge, especially when system models are difficult to obtain or cannot be fully trusted due to uncertainty, modeling errors, or environmental disturbances. Traditional model-based approaches rely on precise system dynamics, which may not be available in real-world scenarios. To address this, we propose a data-driven safety verification framework that leverages matrix zonotopes and barrier certificates to verify system safety directly from noisy data. Instead of trusting a single unreliable model, we construct a set of models that capture all possible system dynamics that align with the observed data, ensuring that the true system model is always contained within this set. This model set is compactly represented using matrix zonotopes, enabling efficient computation and propagation of uncertainty. By integrating this representation into a barrier certificate framework, we establish rigorous safety guarantees without requiring an explicit system model. Numerical experiments demonstrate the effectiveness of our approach in verifying safety for dynamical systems with unknown models, showcasing its potential for real-world CPS applications."
  },
  {
    "title": "MedReason: Eliciting Factual Medical Reasoning Steps in LLMs via Knowledge Graphs",
    "url": "http://arxiv.org/abs/2504.00993v1",
    "arxiv_id": "2504.00993v1",
    "authors": [
      "Juncheng Wu",
      "Wenlong Deng",
      "Xingxuan Li",
      "Sheng Liu",
      "Taomian Mi",
      "Yifan Peng",
      "Ziyang Xu",
      "Yi Liu",
      "Hyunjin Cho",
      "Chang-In Choi",
      "Yihan Cao",
      "Hui Ren",
      "Xiang Li",
      "Xiaoxiao Li",
      "Yuyin Zhou"
    ],
    "published": "2025-04-01T17:31:44+00:00",
    "summary": "Medical tasks such as diagnosis and treatment planning require precise and complex reasoning, particularly in life-critical domains. Unlike mathematical reasoning, medical reasoning demands meticulous, verifiable thought processes to ensure reliability and accuracy. However, there is a notable lack of datasets that provide transparent, step-by-step reasoning to validate and enhance the medical reasoning ability of AI models. To bridge this gap, we introduce MedReason, a large-scale high-quality medical reasoning dataset designed to enable faithful and explainable medical problem-solving in large language models (LLMs). We utilize a structured medical knowledge graph (KG) to convert clinical QA pairs into logical chains of reasoning, or ``thinking paths'', which trace connections from question elements to answers via relevant KG entities. Each path is validated for consistency with clinical logic and evidence-based medicine. Our pipeline generates detailed reasoning for various medical questions from 7 medical datasets, resulting in a dataset of 32,682 question-answer pairs, each with detailed, step-by-step explanations. Experiments demonstrate that fine-tuning with our dataset consistently boosts medical problem-solving capabilities, achieving significant gains of up to 7.7% for DeepSeek-Ditill-8B. Our top-performing model, MedReason-8B, outperforms the Huatuo-o1-8B, a state-of-the-art medical reasoning model, by up to 4.2% on the clinical benchmark MedBullets. We also engage medical professionals from diverse specialties to assess our dataset's quality, ensuring MedReason offers accurate and coherent medical reasoning. Our data, models, and code will be publicly available."
  },
  {
    "title": "Safety and Security Risk Mitigation in Satellite Missions via Attack-Fault-Defense Trees",
    "url": "http://arxiv.org/abs/2504.00988v1",
    "arxiv_id": "2504.00988v1",
    "authors": [
      "Reza Soltani",
      "Pablo Diale",
      "Milan Lopuha\u00e4-Zwakenberg",
      "Mari\u00eblle Stoelinga"
    ],
    "published": "2025-04-01T17:24:43+00:00",
    "summary": "Cyber-physical systems, such as self-driving cars or digitized electrical grids, often involve complex interactions between security, safety, and defense. Proper risk management strategies must account for these three critical domains and their interaction because the failure to address one domain can exacerbate risks in the others, leading to cascading effects that compromise the overall system resilience. This work presents a case study from Ascentio Technologies, a mission-critical system company in Argentina specializing in aerospace, where the interplay between safety, security, and defenses is critical for ensuring the resilience and reliability of their systems. The main focus will be on the Ground Segment for the satellite project currently developed by the company. Analyzing safety, security, and defense mechanisms together in the Ground Segment of a satellite project is crucial because these domains are deeply interconnected--for instance, a security breach could disable critical safety functions, or a safety failure could create opportunities for attackers to exploit vulnerabilities, amplifying the risks to the entire system. This paper showcases the application of the Attack-Fault-Defense Tree (AFDT) framework, which integrates attack trees, fault trees, and defense mechanisms into a unified model. AFDT provides an intuitive visual language that facilitates interdisciplinary collaboration, enabling experts from various fields to better assess system vulnerabilities and defenses. By applying AFDT to the Ground Segment of the satellite project, we demonstrate how qualitative analyses can be performed to identify weaknesses and enhance the overall system's security and safety. This case highlights the importance of jointly analyzing attacks, faults, and defenses to improve resilience in complex cyber-physical environments."
  },
  {
    "title": "Foundation Models for Autonomous Driving System: An Initial Roadmap",
    "url": "http://arxiv.org/abs/2504.00911v1",
    "arxiv_id": "2504.00911v1",
    "authors": [
      "Xiongfei Wu",
      "Mingfei Cheng",
      "Qiang Hu",
      "Jianlang Chen",
      "Yuheng Huang",
      "Manabu Okada",
      "Michio Hayashi",
      "Tomoyuki Tsuchiya",
      "Xiaofei Xie",
      "Lei Ma"
    ],
    "published": "2025-04-01T15:45:31+00:00",
    "summary": "Recent advancements in Foundation Models (FMs), such as Large Language Models (LLMs), have significantly enhanced Autonomous Driving Systems (ADSs) by improving perception, reasoning, and decision-making in dynamic and uncertain environments. However, ADSs are highly complex cyber-physical systems that demand rigorous software engineering practices to ensure reliability and safety. Integrating FMs into ADSs introduces new challenges in system design and evaluation, requiring a systematic review to establish a clear research roadmap. To unlock these challenges, we present a structured roadmap for integrating FMs into autonomous driving, covering three key aspects: the infrastructure of FMs, their application in autonomous driving systems, and their current applications in practice. For each aspect, we review the current research progress, identify existing challenges, and highlight research gaps that need to be addressed by the community."
  },
  {
    "title": "Improved Visual-Spatial Reasoning via R1-Zero-Like Training",
    "url": "http://arxiv.org/abs/2504.00883v1",
    "arxiv_id": "2504.00883v1",
    "authors": [
      "Zhenyi Liao",
      "Qingsong Xie",
      "Yanhao Zhang",
      "Zijian Kong",
      "Haonan Lu",
      "Zhenyu Yang",
      "Zhijie Deng"
    ],
    "published": "2025-04-01T15:11:11+00:00",
    "summary": "Increasing attention has been placed on improving the reasoning capacities of multi-modal large language models (MLLMs). As the cornerstone for AI agents that function in the physical realm, video-based visual-spatial intelligence (VSI) emerges as one of the most pivotal reasoning capabilities of MLLMs. This work conducts a first, in-depth study on improving the visual-spatial reasoning of MLLMs via R1-Zero-like training. Technically, we first identify that the visual-spatial reasoning capacities of small- to medium-sized Qwen2-VL models cannot be activated via Chain of Thought (CoT) prompts. We then incorporate GRPO training for improved visual-spatial reasoning, using the carefully curated VSI-100k dataset, following DeepSeek-R1-Zero. During the investigation, we identify the necessity to keep the KL penalty (even with a small value) in GRPO. With just 120 GPU hours, our vsGRPO-2B model, fine-tuned from Qwen2-VL-2B, can outperform the base model by 12.1% and surpass GPT-4o. Moreover, our vsGRPO-7B model, fine-tuned from Qwen2-VL-7B, achieves performance comparable to that of the best open-source model LLaVA-NeXT-Video-72B. Additionally, we compare vsGRPO to supervised fine-tuning and direct preference optimization baselines and observe strong performance superiority. The code and dataset will be available soon."
  },
  {
    "title": "RapidPD: Rapid Human and Pet Presence Detection System for Smart Vehicles via Wi-Fi",
    "url": "http://arxiv.org/abs/2504.00678v1",
    "arxiv_id": "2504.00678v1",
    "authors": [
      "Hancheng Guo",
      "Zhen Chen",
      "Mo Huang",
      "Xiu Yin Zhang"
    ],
    "published": "2025-04-01T11:44:11+00:00",
    "summary": "Heatstroke and life threatening incidents resulting from the retention of children and animals in vehicles pose a critical global safety issue. Current presence detection solutions often require specialized hardware or suffer from detection delays that do not meet safety standards. To tackle this issue, by re-modeling channel state information (CSI) with theoretical analysis of path propagation, this study introduces RapidPD, an innovative system utilizing CSI in subcarrier dimension to detect the presence of humans and pets in vehicles. The system models the impact of motion on CSI and introduces motion statistics in subcarrier dimension using a multi-layer autocorrelation method to quantify environmental changes. RapidPD is implemented using commercial Wi-Fi chipsets and tested in real vehicle environments with data collected from 10 living organisms. Experimental results demonstrate that RapidPD achieves a detection accuracy of 99.05% and a true positive rate of 99.32% within a 1-second time window at a low sampling rate of 20 Hz. These findings represent a significant advancement in vehicle safety and provide a foundation for the widespread adoption of presence detection systems."
  },
  {
    "title": "Detecting adverse high-order drug interactions from individual case safety reports using computational statistics on disproportionality measures",
    "url": "http://arxiv.org/abs/2504.00646v1",
    "arxiv_id": "2504.00646v1",
    "authors": [
      "Jules Bangard",
      "Einar Holsb\u00f8",
      "Kristian Svendsen",
      "Vittorio Perduca",
      "Etienne Birmel\u00e9"
    ],
    "published": "2025-04-01T10:56:59+00:00",
    "summary": "Adverse drug interactions are a critical concern in pharmacovigilance, as both clinical trials and spontaneous reporting systems often lack the breadth to detect complex drug interactions. This study introduces a computational framework for adverse drug interaction detection, leveraging disproportionality analysis on individual case safety reports. By integrating the Anatomical Therapeutic Chemical classification, the framework extends beyond drug interactions to capture hierarchical pharmacological relationships. To address biases inherent in existing disproportionality measures, we employ a hypergeometric risk metric, while a Markov Chain Monte Carlo algorithm provides robust empirical p-value estimation for the risk associated to cocktails. A genetic algorithm further facilitates efficient identification of high-risk drug cocktails. Validation on synthetic and FDA Adverse Event Reporting System data demonstrates the method's efficacy in detecting established drugs and drug interactions associated with myopathy-related adverse events. Implemented as an R package, this framework offers a reproducible, scalable tool for post-market drug safety surveillance."
  },
  {
    "title": "Probabilistically safe and efficient model-based Reinforcement Learning",
    "url": "http://arxiv.org/abs/2504.00626v1",
    "arxiv_id": "2504.00626v1",
    "authors": [
      "Filippo Airaldi",
      "Bart De Schutter",
      "Azita Dabiri"
    ],
    "published": "2025-04-01T10:24:54+00:00",
    "summary": "This paper proposes tackling safety-critical stochastic Reinforcement Learning (RL) tasks with a samplebased, model-based approach. At the core of the method lies a Model Predictive Control (MPC) scheme that acts as function approximation, providing a model-based predictive control policy. To ensure safety, a probabilistic Control Barrier Function (CBF) is integrated into the MPC controller. A sample-based approach with guarantees is employed to approximate the effects of stochasticies in the optimal control formulation and to guarantee the probabilistic CBF condition. A learnable terminal cost formulation is included in the MPC objective to counterbalance the additional computational burden due to sampling. An RL algorithm is deployed to learn both the terminal cost and the CBF constraint. Results from our numerical experiment on a constrained LTI problem corroborate the effectiveness of the proposed methodology in reducing computation time while preserving control performance and safety."
  },
  {
    "title": "Non-Invasive Assessment of Sediment Accumulation Using Muography: A Pilot Run at the Shanghai Outer Ring Tunnel",
    "url": "http://arxiv.org/abs/2504.00582v1",
    "arxiv_id": "2504.00582v1",
    "authors": [
      "Kim Siang Khaw",
      "Siew Yan Hoh",
      "Tianqi Hu",
      "Xingyun Huang",
      "Jun Kai Ng",
      "Yusuke Takeuchi",
      "Min Yang Tan",
      "Jiangtao Wang",
      "Yinghe Wang",
      "Guan Ming Wong",
      "Mengjie Wu",
      "Ning Yan",
      "Yonghao Zeng",
      "Min Chen",
      "Shunxi Gao",
      "Lei Li",
      "Yujin Shi",
      "Jie Tan",
      "Qinghua Wang",
      "Siping Zeng",
      "Shibin Yao",
      "Yufu Zhang",
      "Gongliang Chen",
      "Houwang Wang",
      "Jinxin Lin",
      "Qing Zhan"
    ],
    "published": "2025-04-01T09:35:54+00:00",
    "summary": "This study demonstrates the application of cosmic-ray muography as a non-invasive method to assess sediment accumulation and tidal influences in the Shanghai Outer Ring Tunnel, an immersed tube tunnel beneath the Huangpu River in Shanghai, China. A portable, dual-layer plastic scintillator detector was deployed to conduct muon flux scans along the tunnel's length and to continuously monitor muon flux to study tidal effects. Geant4 simulations validated the correlation between muon attenuation and overburden thickness, incorporating sediment, water, and concrete layers. Key findings revealed an 11\\% reduction in muon flux per meter of tidal water level increase, demonstrating a strong anti-correlation (correlation coefficient: -0.8) with tidal cycles. The results align with geotechnical data and simulations, especially in the region of interest, confirming muography's sensitivity to sediment dynamics. This work establishes muography as a robust tool for long-term, real-time monitoring of submerged infrastructure, offering significant advantages over conventional invasive techniques. The study underscores the potential for integrating muography into civil engineering practices to enhance safety and operational resilience in tidal environments."
  },
  {
    "title": "Automated detection of atomicity violations in large-scale systems",
    "url": "http://arxiv.org/abs/2504.00521v1",
    "arxiv_id": "2504.00521v1",
    "authors": [
      "Hang He",
      "Yixing Luo",
      "Chengcheng Wan",
      "Ting Su",
      "Haiying Sun",
      "Geguang Pu"
    ],
    "published": "2025-04-01T08:13:29+00:00",
    "summary": "Atomicity violations in interrupt-driven programs pose a significant threat to software safety in critical systems. These violations occur when the execution sequence of operations on shared resources is disrupted by asynchronous interrupts. Detecting atomicity violations is challenging due to the vast program state space, application-level code dependencies, and complex domain-specific knowledge. We propose Clover, a hybrid framework that integrates static analysis with large language model (LLM) agents to detect atomicity violations in real-world programs. Clover first performs static analysis to extract critical code snippets and operation information. It then initiates a multi-agent process, where the expert agent leverages domain-specific knowledge to detect atomicity violations, which are subsequently validated by the judge agent. Evaluations on RaceBench 2.1, SV-COMP, and RWIP demonstrate that Clover achieves a precision/recall of 92.3%/86.6%, outperforming existing approaches by 27.4-118.2% on F1-score."
  },
  {
    "title": "Recitation over Reasoning: How Cutting-Edge Language Models Can Fail on Elementary School-Level Reasoning Problems?",
    "url": "http://arxiv.org/abs/2504.00509v1",
    "arxiv_id": "2504.00509v1",
    "authors": [
      "Kai Yan",
      "Yufei Xu",
      "Zhengyin Du",
      "Xuesong Yao",
      "Zheyu Wang",
      "Xiaowen Guo",
      "Jiecao Chen"
    ],
    "published": "2025-04-01T07:57:58+00:00",
    "summary": "The rapid escalation from elementary school-level to frontier problems of the difficulty for LLM benchmarks in recent years have weaved a miracle for researchers that we are only inches away from surpassing human intelligence. However, is the LLMs' remarkable reasoning ability indeed comes from true intelligence by human standards, or are they simply reciting solutions witnessed during training at an Internet level? To study this problem, we propose RoR-Bench, a novel, multi-modal benchmark for detecting LLM's recitation behavior when asked simple reasoning problems but with conditions subtly shifted, and conduct empirical analysis on our benchmark. Surprisingly, we found existing cutting-edge LLMs unanimously exhibits extremely severe recitation behavior; by changing one phrase in the condition, top models such as OpenAI-o1 and DeepSeek-R1 can suffer $60\\%$ performance loss on elementary school-level arithmetic and reasoning problems. Such findings are a wake-up call to the LLM community that compels us to re-evaluate the true intelligence level of cutting-edge LLMs."
  },
  {
    "title": "A dual-scale stochastic analysis framework for creep failure considering microstructural randomness",
    "url": "http://arxiv.org/abs/2504.00507v1",
    "arxiv_id": "2504.00507v1",
    "authors": [
      "Weichen Kong",
      "Yanwei Dai",
      "Xiang Zhang",
      "Yinghua Liu"
    ],
    "published": "2025-04-01T07:57:22+00:00",
    "summary": "Creep failure under high temperatures is a complex multiscale and multi-mechanism issue involving inherent microstructural randomness. To investigate the effect of microstructures on the uniaxial/multiaxial creep failure, a dual-scale stochastic analysis framework is established to introduce the grain boundary (GB) characteristics into the macroscopic analysis. The nickel-base superalloy Inconel 617 is considered in this study. Firstly, the damage mechanisms of GBs are investigated based on the crystal plasticity finite element (CPFE) method and cohesive zone model (CZM). Subsequently, based on the obtained GB damage evolution, a novel Monte Carlo (MC) approach is proposed to establish the relationship between the GB orientation and area distribution and macroscopic creep damage. Finally, a dual-scale stochastic multiaxial creep damage model is established to incorporate the influence of the random GB orientation and area distribution. With the numerical application of the proposed creep damage model, the random initiation and growth of creep cracks in the uniaxial tensile specimen and the pressurized tube are captured and analyzed. The proposed stochastic framework effectively considers the inherent randomness introduced by GB characteristics and efficiently realizes full-field multiscale calculations. It also shows its potential applications in safety evaluation and life prediction of creep components and structures under high temperatures."
  },
  {
    "title": "Leveraging cross-detector parameter consistency measures to enhance sensitivities of gravitational wave searches",
    "url": "http://arxiv.org/abs/2504.00465v1",
    "arxiv_id": "2504.00465v1",
    "authors": [
      "Sayantan Ghosh",
      "Leigh Smith",
      "Jiyoon Sun",
      "Archana Pai",
      "Ik Siong Heng",
      "V. Gayathri"
    ],
    "published": "2025-04-01T06:38:59+00:00",
    "summary": "All-sky searches for generic short-duration astrophysical GW transients are often challenging because of noise transients. Developing novel signal-noise discriminators is crucial for GW transient searches with LIGO Scientific, Virgo, and KAGRA (LVK) detectors. In this work, we adapt a recently developed Jensen Shannon divergence (JSD)-based measure, which assesses the cross-detector parameter consistency to distinguish between weakly modeled or unmodelled astrophysical GW signals and loud noise triggers. We first extend a 2-detector JSD-based measure, developed in an earlier work, to a 3-detector network. We leverage this to modify the test statistic of the existing Coherent Waveburst (cWB)-Gaussian Mixture Modelling (GMM) algorithm for short-duration transients towards improving the search sensitivity to ad-hoc waveforms like Sine-Gaussians, Gaussian Pulses, and White Noise Bursts. We find that with the new method, which we term cWB-GMM-JSD, the sensitivity to the ad-hoc waveforms, given by $h_{\\mathrm{rss50}}$, improves by $\\sim 10-20 \\%$ at an IFAR of 10 years for the 2-detector network consisting of LHO and LLO detectors, and by $\\sim 5-10 \\%$ at the same IFAR for the 3-detector network consisting of LHO, LLO and Virgo detectors. Finally, we apply the modified statistic in the revised data analysis pipeline on the publicly available data from the third observing run (O3) of the LIGO and Virgo detectors. Although we do not find any new event in the O3 data, we see a notable rise in the statistical significance of most of the known GW events, which further testifies to the enhancement in sensitivities."
  },
  {
    "title": "Egocentric Conformal Prediction for Safe and Efficient Navigation in Dynamic Cluttered Environments",
    "url": "http://arxiv.org/abs/2504.00447v1",
    "arxiv_id": "2504.00447v1",
    "authors": [
      "Jaeuk Shin",
      "Jungjin Lee",
      "Insoon Yang"
    ],
    "published": "2025-04-01T05:59:05+00:00",
    "summary": "Conformal prediction (CP) has emerged as a powerful tool in robotics and control, thanks to its ability to calibrate complex, data-driven models with formal guarantees. However, in robot navigation tasks, existing CP-based methods often decouple prediction from control, evaluating models without considering whether prediction errors actually compromise safety. Consequently, ego-vehicles may become overly conservative or even immobilized when all potential trajectories appear infeasible. To address this issue, we propose a novel CP-based navigation framework that responds exclusively to safety-critical prediction errors. Our approach introduces egocentric score functions that quantify how much closer obstacles are to a candidate vehicle position than anticipated. These score functions are then integrated into a model predictive control scheme, wherein each candidate state is individually evaluated for safety. Combined with an adaptive CP mechanism, our framework dynamically adjusts to changes in obstacle motion without resorting to unnecessary conservatism. Theoretical analyses indicate that our method outperforms existing CP-based approaches in terms of cost-efficiency while maintaining the desired safety levels, as further validated through experiments on real-world datasets featuring densely populated pedestrian environments."
  },
  {
    "title": "Exposing the Ghost in the Transformer: Abnormal Detection for Large Language Models via Hidden State Forensics",
    "url": "http://arxiv.org/abs/2504.00446v1",
    "arxiv_id": "2504.00446v1",
    "authors": [
      "Shide Zhou",
      "Kailong Wang",
      "Ling Shi",
      "Haoyu Wang"
    ],
    "published": "2025-04-01T05:58:14+00:00",
    "summary": "The widespread adoption of Large Language Models (LLMs) in critical applications has introduced severe reliability and security risks, as LLMs remain vulnerable to notorious threats such as hallucinations, jailbreak attacks, and backdoor exploits. These vulnerabilities have been weaponized by malicious actors, leading to unauthorized access, widespread misinformation, and compromised LLM-embedded system integrity. In this work, we introduce a novel approach to detecting abnormal behaviors in LLMs via hidden state forensics. By systematically inspecting layer-specific activation patterns, we develop a unified framework that can efficiently identify a range of security threats in real-time without imposing prohibitive computational costs. Extensive experiments indicate detection accuracies exceeding 95% and consistently robust performance across multiple models in most scenarios, while preserving the ability to detect novel attacks effectively. Furthermore, the computational overhead remains minimal, with merely fractions of a second. The significance of this work lies in proposing a promising strategy to reinforce the security of LLM-integrated systems, paving the way for safer and more reliable deployment in high-stakes domains. By enabling real-time detection that can also support the mitigation of abnormal behaviors, it represents a meaningful step toward ensuring the trustworthiness of AI systems amid rising security challenges."
  },
  {
    "title": "No Free Lunch with Guardrails",
    "url": "http://arxiv.org/abs/2504.00441v1",
    "arxiv_id": "2504.00441v1",
    "authors": [
      "Divyanshu Kumar",
      "Nitin Aravind Birur",
      "Tanay Baswa",
      "Sahil Agarwal",
      "Prashanth Harshangi"
    ],
    "published": "2025-04-01T05:46:54+00:00",
    "summary": "As large language models (LLMs) and generative AI become widely adopted, guardrails have emerged as a key tool to ensure their safe use. However, adding guardrails isn't without tradeoffs; stronger security measures can reduce usability, while more flexible systems may leave gaps for adversarial attacks. In this work, we explore whether current guardrails effectively prevent misuse while maintaining practical utility. We introduce a framework to evaluate these tradeoffs, measuring how different guardrails balance risk, security, and usability, and build an efficient guardrail.   Our findings confirm that there is no free lunch with guardrails; strengthening security often comes at the cost of usability. To address this, we propose a blueprint for designing better guardrails that minimize risk while maintaining usability. We evaluate various industry guardrails, including Azure Content Safety, Bedrock Guardrails, OpenAI's Moderation API, Guardrails AI, Nemo Guardrails, and our own custom-built guardrails. Additionally, we assess how LLMs like GPT-4o, Gemini 2.0-Flash, Claude 3.5-Sonnet, and Mistral Large-Latest respond under different system prompts, including simple prompts, detailed prompts, and detailed prompts with chain-of-thought (CoT) reasoning. Our study provides a clear comparison of how different guardrails perform, highlighting the challenges in balancing security and usability."
  },
  {
    "title": "Control Barrier Function Synthesis for Nonlinear Systems with Dual Relative Degree",
    "url": "http://arxiv.org/abs/2504.00397v1",
    "arxiv_id": "2504.00397v1",
    "authors": [
      "Gilbert Bahati",
      "Ryan K. Cosner",
      "Max H. Cohen",
      "Ryan M. Bena",
      "Aaron D. Ames"
    ],
    "published": "2025-04-01T03:38:24+00:00",
    "summary": "Control barrier functions (CBFs) are a powerful tool for synthesizing safe control actions; however, constructing CBFs remains difficult for general nonlinear systems. In this work, we provide a constructive framework for synthesizing CBFs for systems with dual relative degree -- where different inputs influence the outputs at two different orders of differentiation; this is common in systems with orientation-based actuation, such as unicycles and quadrotors. In particular, we propose dual relative degree CBFs (DRD-CBFs) and show that these DRD-CBFs can be constructively synthesized and used to guarantee system safety. Our method constructs DRD-CBFs by leveraging the dual relative degree property -- combining a CBF for an integrator chain with a Lyapunov function certifying the tracking of safe inputs generated for this linear system. We apply these results to dual relative degree systems, both in simulation and experimentally on hardware using quadruped and quadrotor robotic platforms."
  },
  {
    "title": "Deep learning for state estimation of commercial sodium-ion batteries using partial charging profiles: validation with a multi-temperature ageing dataset",
    "url": "http://arxiv.org/abs/2504.00393v1",
    "arxiv_id": "2504.00393v1",
    "authors": [
      "Jiapeng Liu",
      "Lunte Li",
      "Jing Xiang",
      "Laiyong Xie",
      "Yuhao Wang",
      "Francesco Ciucci"
    ],
    "published": "2025-04-01T03:28:13+00:00",
    "summary": "Accurately predicting the state of health for sodium-ion batteries is crucial for managing battery modules, playing a vital role in ensuring operational safety. However, highly accurate models available thus far are rare due to a lack of aging data for sodium-ion batteries. In this study, we experimentally collected 53 single cells at four temperatures (0, 25, 35, and 45 {\\deg}C), along with two battery modules in the lab. By utilizing the charging profiles, we were able to predict the SOC, capacity, and SOH simultaneously. This was achieved by designing a new framework that integrates the neural ordinary differential equation and 2D convolutional neural networks, using the partial charging profile as input. The charging profile is partitioned into segments, and each segment is fed into the network to output the SOC. For capacity and SOH prediction, we first aggregated the extracted features corresponding to segments from one cycle, after which an embedding block for temperature is concatenated for the final prediction. This novel approach eliminates the issue of multiple outputs for a single target. Our model demonstrated an $R^2$ accuracy of 0.998 for SOC and 0.997 for SOH across single cells at various temperatures. Furthermore, the trained model can be employed to predict single cells at temperatures outside the training set and battery modules with different capacity and current levels. The results presented here highlight the high accuracy of our model and its capability to predict multiple targets simultaneously using a partial charging profile."
  },
  {
    "title": "CyberBOT: Towards Reliable Cybersecurity Education via Ontology-Grounded Retrieval Augmented Generation",
    "url": "http://arxiv.org/abs/2504.00389v1",
    "arxiv_id": "2504.00389v1",
    "authors": [
      "Chengshuai Zhao",
      "Riccardo De Maria",
      "Tharindu Kumarage",
      "Kumar Satvik Chaudhary",
      "Garima Agrawal",
      "Yiwen Li",
      "Jongchan Park",
      "Yuli Deng",
      "Ying-Chih Chen",
      "Huan Liu"
    ],
    "published": "2025-04-01T03:19:22+00:00",
    "summary": "Advancements in large language models (LLMs) have enabled the development of intelligent educational tools that support inquiry-based learning across technical domains. In cybersecurity education, where accuracy and safety are paramount, systems must go beyond surface-level relevance to provide information that is both trustworthy and domain-appropriate. To address this challenge, we introduce CyberBOT, a question-answering chatbot that leverages a retrieval-augmented generation (RAG) pipeline to incorporate contextual information from course-specific materials and validate responses using a domain-specific cybersecurity ontology. The ontology serves as a structured reasoning layer that constrains and verifies LLM-generated answers, reducing the risk of misleading or unsafe guidance. CyberBOT has been deployed in a large graduate-level course at Arizona State University (ASU), where more than one hundred students actively engage with the system through a dedicated web-based platform. Computational evaluations in lab environments highlight the potential capacity of CyberBOT, and a forthcoming field study will evaluate its pedagogical impact. By integrating structured domain reasoning with modern generative capabilities, CyberBOT illustrates a promising direction for developing reliable and curriculum-aligned AI applications in specialized educational contexts."
  },
  {
    "title": "Safe Navigation in Dynamic Environments Using Data-Driven Koopman Operators and Conformal Prediction",
    "url": "http://arxiv.org/abs/2504.00352v1",
    "arxiv_id": "2504.00352v1",
    "authors": [
      "Kaier Liang",
      "Guang Yang",
      "Mingyu Cai",
      "Cristian-Ioan Vasile"
    ],
    "published": "2025-04-01T02:06:26+00:00",
    "summary": "We propose a novel framework for safe navigation in dynamic environments by integrating Koopman operator theory with conformal prediction. Our approach leverages data-driven Koopman approximation to learn nonlinear dynamics and employs conformal prediction to quantify uncertainty, providing statistical guarantees on approximation errors. This uncertainty is effectively incorporated into a Model Predictive Controller (MPC) formulation through constraint tightening, ensuring robust safety guarantees. We implement a layered control architecture with a reference generator providing waypoints for safe navigation. The effectiveness of our methods is validated in simulation."
  },
  {
    "title": "Inference-Time Scaling for Complex Tasks: Where We Stand and What Lies Ahead",
    "url": "http://arxiv.org/abs/2504.00294v1",
    "arxiv_id": "2504.00294v1",
    "authors": [
      "Vidhisha Balachandran",
      "Jingya Chen",
      "Lingjiao Chen",
      "Shivam Garg",
      "Neel Joshi",
      "Yash Lara",
      "John Langford",
      "Besmira Nushi",
      "Vibhav Vineet",
      "Yue Wu",
      "Safoora Yousefi"
    ],
    "published": "2025-03-31T23:40:28+00:00",
    "summary": "Inference-time scaling can enhance the reasoning capabilities of large language models (LLMs) on complex problems that benefit from step-by-step problem solving. Although lengthening generated scratchpads has proven effective for mathematical tasks, the broader impact of this approach on other tasks remains less clear. In this work, we investigate the benefits and limitations of scaling methods across nine state-of-the-art models and eight challenging tasks, including math and STEM reasoning, calendar planning, NP-hard problems, navigation, and spatial reasoning. We compare conventional models (e.g., GPT-4o) with models fine-tuned for inference-time scaling (e.g., o1) through evaluation protocols that involve repeated model calls, either independently or sequentially with feedback. These evaluations approximate lower and upper performance bounds and potential for future performance improvements for each model, whether through enhanced training or multi-model inference systems. Our extensive empirical analysis reveals that the advantages of inference-time scaling vary across tasks and diminish as problem complexity increases. In addition, simply using more tokens does not necessarily translate to higher accuracy in these challenging regimes. Results from multiple independent runs with conventional models using perfect verifiers show that, for some tasks, these models can achieve performance close to the average performance of today's most advanced reasoning models. However, for other tasks, a significant performance gap remains, even in very high scaling regimes. Encouragingly, all models demonstrate significant gains when inference is further scaled with perfect verifiers or strong feedback, suggesting ample potential for future improvements."
  },
  {
    "title": "Text Chunking for Document Classification for Urban System Management using Large Language Models",
    "url": "http://arxiv.org/abs/2504.00274v1",
    "arxiv_id": "2504.00274v1",
    "authors": [
      "Joshua Rodriguez",
      "Om Sanan",
      "Guillermo Vizarreta-Luna",
      "Steven A. Conrad"
    ],
    "published": "2025-03-31T22:48:30+00:00",
    "summary": "Urban systems are managed using complex textual documentation that need coding and analysis to set requirements and evaluate built environment performance. This paper contributes to the study of applying large-language models (LLM) to qualitative coding activities to reduce resource requirements while maintaining comparable reliability to humans. Qualitative coding and assessment face challenges like resource limitations and bias, accuracy, and consistency between human evaluators. Here we report the application of LLMs to deductively code 10 case documents on the presence of 17 digital twin characteristics for the management of urban systems. We utilize two prompting methods to compare the semantic processing of LLMs with human coding efforts: whole text analysis and text chunk analysis using OpenAI's GPT-4o, GPT-4o-mini, and o1-mini models. We found similar trends of internal variability between methods and results indicate that LLMs may perform on par with human coders when initialized with specific deductive coding contexts. GPT-4o, o1-mini and GPT-4o-mini showed significant agreement with human raters when employed using a chunking method. The application of both GPT-4o and GPT-4o-mini as an additional rater with three manual raters showed statistically significant agreement across all raters, indicating that the analysis of textual documents is benefited by LLMs. Our findings reveal nuanced sub-themes of LLM application suggesting LLMs follow human memory coding processes where whole-text analysis may introduce multiple meanings. The novel contributions of this paper lie in assessing the performance of OpenAI GPT models and introduces the chunk-based prompting approach, which addresses context aggregation biases by preserving localized context."
  },
  {
    "title": "NeRF-Based defect detection",
    "url": "http://arxiv.org/abs/2504.00270v1",
    "arxiv_id": "2504.00270v1",
    "authors": [
      "Tianqi",
      "Ding",
      "Dawei Xiang",
      "Yijiashun Qi",
      "Ze Yang",
      "Zunduo Zhao",
      "Tianyao Sun",
      "Pengbin Feng",
      "Haoyu Wang"
    ],
    "published": "2025-03-31T22:27:51+00:00",
    "summary": "The rapid growth of industrial automation has highlighted the need for precise and efficient defect detection in large-scale machinery. Traditional inspection techniques, involving manual procedures such as scaling tall structures for visual evaluation, are labor-intensive, subjective, and often hazardous. To overcome these challenges, this paper introduces an automated defect detection framework built on Neural Radiance Fields (NeRF) and the concept of digital twins. The system utilizes UAVs to capture images and reconstruct 3D models of machinery, producing both a standard reference model and a current-state model for comparison. Alignment of the models is achieved through the Iterative Closest Point (ICP) algorithm, enabling precise point cloud analysis to detect deviations that signify potential defects. By eliminating manual inspection, this method improves accuracy, enhances operational safety, and offers a scalable solution for defect detection. The proposed approach demonstrates great promise for reliable and efficient industrial applications."
  },
  {
    "title": "SU-YOLO: Spiking Neural Network for Efficient Underwater Object Detection",
    "url": "http://arxiv.org/abs/2503.24389v1",
    "arxiv_id": "2503.24389v1",
    "authors": [
      "Chenyang Li",
      "Wenxuan Liu",
      "Guoqiang Gong",
      "Xiaobo Ding",
      "Xian Zhong"
    ],
    "published": "2025-03-31T17:59:52+00:00",
    "summary": "Underwater object detection is critical for oceanic research and industrial safety inspections. However, the complex optical environment and the limited resources of underwater equipment pose significant challenges to achieving high accuracy and low power consumption. To address these issues, we propose Spiking Underwater YOLO (SU-YOLO), a Spiking Neural Network (SNN) model. Leveraging the lightweight and energy-efficient properties of SNNs, SU-YOLO incorporates a novel spike-based underwater image denoising method based solely on integer addition, which enhances the quality of feature maps with minimal computational overhead. In addition, we introduce Separated Batch Normalization (SeBN), a technique that normalizes feature maps independently across multiple time steps and is optimized for integration with residual structures to capture the temporal dynamics of SNNs more effectively. The redesigned spiking residual blocks integrate the Cross Stage Partial Network (CSPNet) with the YOLO architecture to mitigate spike degradation and enhance the model's feature extraction capabilities. Experimental results on URPC2019 underwater dataset demonstrate that SU-YOLO achieves mAP of 78.8% with 6.97M parameters and an energy consumption of 2.98 mJ, surpassing mainstream SNN models in both detection accuracy and computational efficiency. These results underscore the potential of SNNs for engineering applications. The code is available in https://github.com/lwxfight/snn-underwater."
  },
  {
    "title": "Effectively Controlling Reasoning Models through Thinking Intervention",
    "url": "http://arxiv.org/abs/2503.24370v1",
    "arxiv_id": "2503.24370v1",
    "authors": [
      "Tong Wu",
      "Chong Xiang",
      "Jiachen T. Wang",
      "Prateek Mittal"
    ],
    "published": "2025-03-31T17:50:13+00:00",
    "summary": "Reasoning-enhanced large language models (LLMs) explicitly generate intermediate reasoning steps prior to generating final answers, helping the model excel in complex problem-solving. In this paper, we demonstrate that this emerging generation framework offers a unique opportunity for more fine-grained control over model behavior. We propose Thinking Intervention, a novel paradigm designed to explicitly guide the internal reasoning processes of LLMs by strategically inserting or revising specific thinking tokens. We conduct comprehensive evaluations across multiple tasks, including instruction following on IFEval, instruction hierarchy on SEP, and safety alignment on XSTest and SORRY-Bench. Our results demonstrate that Thinking Intervention significantly outperforms baseline prompting approaches, achieving up to 6.7% accuracy gains in instruction-following scenarios, 15.4% improvements in reasoning about instruction hierarchies, and a 40.0% increase in refusal rates for unsafe prompts using open-source DeepSeek R1 models. Overall, our work opens a promising new research avenue for controlling reasoning LLMs."
  },
  {
    "title": "Query and Conquer: Execution-Guided SQL Generation",
    "url": "http://arxiv.org/abs/2503.24364v1",
    "arxiv_id": "2503.24364v1",
    "authors": [
      "\u0141ukasz Borchmann",
      "Marek Wydmuch"
    ],
    "published": "2025-03-31T17:43:36+00:00",
    "summary": "We propose a novel approach for generating complex outputs that significantly improves accuracy in text-to-SQL tasks. Our method leverages execution results to select the most semantically consistent query from multiple candidates, enabling smaller, cost-effective models to surpass computationally intensive reasoning methods such as o1, o3-mini, and DeepSeek R1 while reducing inference cost by as much as 30 times. It integrates effortlessly with existing models, offering a practical and scalable pathway to state-of-the-art SQL generation."
  },
  {
    "title": "Self-Supervised Pretraining for Aerial Road Extraction",
    "url": "http://arxiv.org/abs/2503.24326v2",
    "arxiv_id": "2503.24326v2",
    "authors": [
      "Rupert Polley",
      "Sai Vignesh Abishek Deenadayalan",
      "J. Marius Z\u00f6llner"
    ],
    "published": "2025-03-31T17:14:08+00:00",
    "summary": "Deep neural networks for aerial image segmentation require large amounts of labeled data, but high-quality aerial datasets with precise annotations are scarce and costly to produce. To address this limitation, we propose a self-supervised pretraining method that improves segmentation performance while reducing reliance on labeled data. Our approach uses inpainting-based pretraining, where the model learns to reconstruct missing regions in aerial images, capturing their inherent structure before being fine-tuned for road extraction. This method improves generalization, enhances robustness to domain shifts, and is invariant to model architecture and dataset choice. Experiments show that our pretraining significantly boosts segmentation accuracy, especially in low-data regimes, making it a scalable solution for aerial image analysis."
  },
  {
    "title": "Open-Reasoner-Zero: An Open Source Approach to Scaling Up Reinforcement Learning on the Base Model",
    "url": "http://arxiv.org/abs/2503.24290v1",
    "arxiv_id": "2503.24290v1",
    "authors": [
      "Jingcheng Hu",
      "Yinmin Zhang",
      "Qi Han",
      "Daxin Jiang",
      "Xiangyu Zhang",
      "Heung-Yeung Shum"
    ],
    "published": "2025-03-31T16:36:05+00:00",
    "summary": "We introduce Open-Reasoner-Zero, the first open source implementation of large-scale reasoning-oriented RL training focusing on scalability, simplicity and accessibility. Through extensive experiments, we demonstrate that a minimalist approach, vanilla PPO with GAE ($\\lambda=1$, $\\gamma=1$) and straightforward rule-based rewards, without any KL regularization, is sufficient to scale up both response length and benchmark performance, similar to the phenomenon observed in DeepSeek-R1-Zero. Using the same base model as DeepSeek-R1-Zero-Qwen-32B, our implementation achieves superior performance on AIME2024, MATH500, and the GPQA Diamond benchmark while demonstrating remarkable efficiency -- requiring only a tenth of the training steps, compared to DeepSeek-R1-Zero pipeline. In the spirit of open source, we release our source code, parameter settings, training data, and model weights across various sizes."
  },
  {
    "title": "Output Constraints as Attack Surface: Exploiting Structured Generation to Bypass LLM Safety Mechanisms",
    "url": "http://arxiv.org/abs/2503.24191v1",
    "arxiv_id": "2503.24191v1",
    "authors": [
      "Shuoming Zhang",
      "Jiacheng Zhao",
      "Ruiyuan Xu",
      "Xiaobing Feng",
      "Huimin Cui"
    ],
    "published": "2025-03-31T15:08:06+00:00",
    "summary": "Content Warning: This paper may contain unsafe or harmful content generated by LLMs that may be offensive to readers. Large Language Models (LLMs) are extensively used as tooling platforms through structured output APIs to ensure syntax compliance so that robust integration with existing softwares like agent systems, could be achieved. However, the feature enabling functionality of grammar-guided structured output presents significant security vulnerabilities. In this work, we reveal a critical control-plane attack surface orthogonal to traditional data-plane vulnerabilities. We introduce Constrained Decoding Attack (CDA), a novel jailbreak class that weaponizes structured output constraints to bypass safety mechanisms. Unlike prior attacks focused on input prompts, CDA operates by embedding malicious intent in schema-level grammar rules (control-plane) while maintaining benign surface prompts (data-plane). We instantiate this with a proof-of-concept Chain Enum Attack, achieves 96.2% attack success rates across proprietary and open-weight LLMs on five safety benchmarks with a single query, including GPT-4o and Gemini-2.0-flash. Our findings identify a critical security blind spot in current LLM architectures and urge a paradigm shift in LLM safety to address control-plane vulnerabilities, as current mechanisms focused solely on data-plane threats leave critical systems exposed."
  },
  {
    "title": "Implicit In-Context Learning: Evidence from Artificial Language Experiments",
    "url": "http://arxiv.org/abs/2503.24190v1",
    "arxiv_id": "2503.24190v1",
    "authors": [
      "Xiaomeng Ma",
      "Qihui Xu"
    ],
    "published": "2025-03-31T15:07:08+00:00",
    "summary": "Humans acquire language through implicit learning, absorbing complex patterns without explicit awareness. While LLMs demonstrate impressive linguistic capabilities, it remains unclear whether they exhibit human-like pattern recognition during in-context learning at inferencing level. We adapted three classic artificial language learning experiments spanning morphology, morphosyntax, and syntax to systematically evaluate implicit learning at inferencing level in two state-of-the-art OpenAI models: gpt-4o and o3-mini. Our results reveal linguistic domain-specific alignment between models and human behaviors, o3-mini aligns better in morphology while both models align in syntax."
  },
  {
    "title": "Pseudo-Random UAV Test Generation Using Low-Fidelity Path Simulator",
    "url": "http://arxiv.org/abs/2503.24172v1",
    "arxiv_id": "2503.24172v1",
    "authors": [
      "Anas Shrinah",
      "Kerstin Eder"
    ],
    "published": "2025-03-31T14:50:46+00:00",
    "summary": "Simulation-based testing provides a safe and cost-effective environment for verifying the safety of Uncrewed Aerial Vehicles (UAVs). However, simulation can be resource-consuming, especially when High-Fidelity Simulators (HFS) are used. To optimise simulation resources, we propose a pseudo-random test generator that uses a Low-Fidelity Simulator (LFS) to estimate UAV flight paths. This work simplifies the PX4 autopilot HFS to develop a LFS, which operates one order of magnitude faster than the HFS.Test cases predicted to cause safety violations in the LFS are subsequently validated using the HFS."
  },
  {
    "title": "LLM4FS: Leveraging Large Language Models for Feature Selection and How to Improve It",
    "url": "http://arxiv.org/abs/2503.24157v1",
    "arxiv_id": "2503.24157v1",
    "authors": [
      "Jianhao Li",
      "Xianchao Xiu"
    ],
    "published": "2025-03-31T14:40:31+00:00",
    "summary": "Recent advances in large language models (LLMs) have provided new opportunities for decision-making, particularly in the task of automated feature selection. In this paper, we first comprehensively evaluate LLM-based feature selection methods, covering the state-of-the-art DeepSeek-R1, GPT-o3-mini, and GPT-4.5. Then, we propose a novel hybrid strategy called LLM4FS that integrates LLMs with traditional data-driven methods. Specifically, input data samples into LLMs, and directly call traditional data-driven techniques such as random forest and forward sequential selection. Notably, our analysis reveals that the hybrid strategy leverages the contextual understanding of LLMs and the high statistical reliability of traditional data-driven methods to achieve excellent feature selection performance, even surpassing LLMs and traditional data-driven methods. Finally, we point out the limitations of its application in decision-making."
  },
  {
    "title": "Enhancing Traffic Safety with AI and 6G: Latency Requirements and Real-Time Threat Detection",
    "url": "http://arxiv.org/abs/2503.24143v1",
    "arxiv_id": "2503.24143v1",
    "authors": [
      "Kurt Horvath",
      "Dragi Kimovski",
      "Stojan Kitanov",
      "Radu Prodan"
    ],
    "published": "2025-03-31T14:27:32+00:00",
    "summary": "The rapid digitalization of urban infrastructure opens the path to smart cities, where IoT-enabled infrastructure enhances public safety and efficiency. This paper presents a 6G and AI-enabled framework for traffic safety enhancement, focusing on real-time detection and classification of emergency vehicles and leveraging 6G as the latest global communication standard. The system integrates sensor data acquisition, convolutional neural network-based threat detection, and user alert dissemination through various software modules of the use case. We define the latency requirements for such a system, segmenting the end-to-end latency into computational and networking components. Our empirical evaluation demonstrates the impact of vehicle speed and user trajectory on system reliability. The results provide insights for network operators and smart city service providers, emphasizing the critical role of low-latency communication and how networks can enable relevant services for traffic safety."
  },
  {
    "title": "Reinforcement Learning for Safe Autonomous Two Device Navigation of Cerebral Vessels in Mechanical Thrombectomy",
    "url": "http://arxiv.org/abs/2503.24140v1",
    "arxiv_id": "2503.24140v1",
    "authors": [
      "Harry Robertshaw",
      "Benjamin Jackson",
      "Jiaheng Wang",
      "Hadi Sadati",
      "Lennart Karstensen",
      "Alejandro Granados",
      "Thomas C Booth"
    ],
    "published": "2025-03-31T14:25:46+00:00",
    "summary": "Purpose: Autonomous systems in mechanical thrombectomy (MT) hold promise for reducing procedure times, minimizing radiation exposure, and enhancing patient safety. However, current reinforcement learning (RL) methods only reach the carotid arteries, are not generalizable to other patient vasculatures, and do not consider safety. We propose a safe dual-device RL algorithm that can navigate beyond the carotid arteries to cerebral vessels.   Methods: We used the Simulation Open Framework Architecture to represent the intricacies of cerebral vessels, and a modified Soft Actor-Critic RL algorithm to learn, for the first time, the navigation of micro-catheters and micro-guidewires. We incorporate patient safety metrics into our reward function by integrating guidewire tip forces. Inverse RL is used with demonstrator data on 12 patient-specific vascular cases.   Results: Our simulation demonstrates successful autonomous navigation within unseen cerebral vessels, achieving a 96% success rate, 7.0s procedure time, and 0.24 N mean forces, well below the proposed 1.5 N vessel rupture threshold.   Conclusion: To the best of our knowledge, our proposed autonomous system for MT two-device navigation reaches cerebral vessels, considers safety, and is generalizable to unseen patient-specific cases for the first time. We envisage future work will extend the validation to vasculatures of different complexity and on in vitro models. While our contributions pave the way towards deploying agents in clinical settings, safety and trustworthiness will be crucial elements to consider when proposing new methodology."
  },
  {
    "title": "Fuzzing-based Mutation Testing of C/C++ CPS",
    "url": "http://arxiv.org/abs/2503.24100v1",
    "arxiv_id": "2503.24100v1",
    "authors": [
      "Jaekwon Lee",
      "Fabrizio Pastore",
      "Lionel Briand"
    ],
    "published": "2025-03-31T13:55:27+00:00",
    "summary": "Mutation testing can help minimize the delivery of faulty software. Therefore, it is a recommended practice for developing embedded software in safety-critical cyber-physical systems (CPS). However, state-of-the-art mutation testing techniques for C and C++ software, which are common languages for CPS, depend on symbolic execution. Unfortunately, symbolic execution's limitations hinder its applicability (e.g., systems with black-box components).   We propose relying on fuzz testing, which has demonstrated its effectiveness for C and C++ software. Fuzz testing tools automatically create test inputs that explore program branches in various ways, exercising statements in different program states, and thus enabling the detection of mutants, which is our objective.   We empirically evaluated our approach using software components from operational satellite systems. Our assessment shows that our approach can detect between 40% and 90% of the mutants not detected by developers' test suites. Further, we empirically determined that the best results are obtained by integrating the Clang compiler, a memory address sanitizer, and relying on laf-intel instrumentation to collect coverage and guide fuzzing. Our approach detects a significantly higher percentage of live mutants compared to symbolic execution, with an increase of up to 50 percentage points; further, we observed that although the combination of fuzzing and symbolic execution leads to additional mutants being killed, the benefits are minimal (a gain of less than one percentage point)."
  },
  {
    "title": "Unraveling tensor structures in correct-by-design controller synthesis",
    "url": "http://arxiv.org/abs/2503.24085v1",
    "arxiv_id": "2503.24085v1",
    "authors": [
      "Ruohan Wang",
      "Zhiyong Sun",
      "Sofie Haesaert"
    ],
    "published": "2025-03-31T13:38:09+00:00",
    "summary": "Formal safety guarantees on the synthesis of controllers for stochastic systems can be obtained using correct-by-design approaches. These approaches often use abstractions as finite-state Markov Decision Processes. As the state space of these MDPs grows, the curse of dimensionality makes the computational and memory cost of the probabilistic guarantees, quantified with dynamic programming, scale exponentially. In this work, we leverage decoupled dynamics and unravel, via dynamic programming operations, a tree structure in the Canonical Polyadic Decomposition (CPD) of the value functions.   For discrete-time stochastic systems with syntactically co-safe linear temporal logic (scLTL) specifications, we provide provable probabilistic safety guarantees and significantly alleviate the computational burden. We provide an initial validation of the theoretical results on several typical case studies and showcase that the uncovered tree structure enables efficient reductions in the computational burden."
  },
  {
    "title": "An Empirical Study of Rust-Specific Bugs in the rustc Compiler",
    "url": "http://arxiv.org/abs/2503.23985v1",
    "arxiv_id": "2503.23985v1",
    "authors": [
      "Zixi Liu",
      "Yang Feng",
      "Yunbo Ni",
      "Shaohua Li",
      "Xizhe Yin",
      "Qingkai Shi",
      "Baowen Xu",
      "Zhendong Su"
    ],
    "published": "2025-03-31T11:55:04+00:00",
    "summary": "Rust is gaining popularity for its well-known memory safety guarantees and high performance, distinguishing it from C/C++ and JVM-based languages. Its compiler, rustc, enforces these guarantees through specialized mechanisms such as trait solving, borrow checking, and specific optimizations. However, Rust's unique language mechanisms introduce complexity to its compiler, leading to Rust-specific compiler bugs that are less common in traditional compilers. With Rust's increasing adoption in safety-critical domains, understanding these language mechanisms and their impact on compiler bugs is essential for improving the reliability of both rustc and Rust programs. Yet, we still lack a large-scale, detailed, and in-depth study of Rust-specific bugs in rustc.   To bridge this gap, this work conducts a comprehensive and systematic study of Rust-specific bugs in rustc, with a particular focus on the components that support its unique language features. Our analysis examines issues and fixes reported between 2022 and 2024, with a manual review of 301 valid issues. We categorize these bugs based on their causes, symptoms, affected compilation stages, and test case characteristics. Additionally, we evaluate existing rustc testing tools to assess their effectiveness and limitations. Our key findings include: (1) rustc bugs primarily arise from Rust's type system and lifetime model, with frequent errors in the High-Level Intermediate Representation (HIR) and Mid-Level Intermediate Representation (MIR) modules due to complex checkers and optimizations; (2) bug-revealing test cases often involve unstable features, advanced trait usages, lifetime annotations, standard APIs, and specific optimization levels; (3) while both valid and invalid programs can trigger bugs, existing testing tools struggle to detect non-crash errors, underscoring the need for further advancements in rustc testing."
  },
  {
    "title": "A Reactive Framework for Whole-Body Motion Planning of Mobile Manipulators Combining Reinforcement Learning and SDF-Constrained Quadratic Programmi",
    "url": "http://arxiv.org/abs/2503.23975v1",
    "arxiv_id": "2503.23975v1",
    "authors": [
      "Chenyu Zhang",
      "Shiying Sun",
      "Kuan Liu",
      "Chuanbao Zhou",
      "Xiaoguang Zhao",
      "Min Tan",
      "Yanlong Huang"
    ],
    "published": "2025-03-31T11:37:02+00:00",
    "summary": "As an important branch of embodied artificial intelligence, mobile manipulators are increasingly applied in intelligent services, but their redundant degrees of freedom also limit efficient motion planning in cluttered environments. To address this issue, this paper proposes a hybrid learning and optimization framework for reactive whole-body motion planning of mobile manipulators. We develop the Bayesian distributional soft actor-critic (Bayes-DSAC) algorithm to improve the quality of value estimation and the convergence performance of the learning. Additionally, we introduce a quadratic programming method constrained by the signed distance field to enhance the safety of the obstacle avoidance motion. We conduct experiments and make comparison with standard benchmark. The experimental results verify that our proposed framework significantly improves the efficiency of reactive whole-body motion planning, reduces the planning time, and improves the success rate of motion planning. Additionally, the proposed reinforcement learning method ensures a rapid learning process in the whole-body planning task. The novel framework allows mobile manipulators to adapt to complex environments more safely and efficiently."
  },
  {
    "title": "Video-based Traffic Light Recognition by Rockchip RV1126 for Autonomous Driving",
    "url": "http://arxiv.org/abs/2503.23965v1",
    "arxiv_id": "2503.23965v1",
    "authors": [
      "Miao Fan",
      "Xuxu Kong",
      "Shengtong Xu",
      "Haoyi Xiong",
      "Xiangzeng Liu"
    ],
    "published": "2025-03-31T11:27:48+00:00",
    "summary": "Real-time traffic light recognition is fundamental for autonomous driving safety and navigation in urban environments. While existing approaches rely on single-frame analysis from onboard cameras, they struggle with complex scenarios involving occlusions and adverse lighting conditions. We present \\textit{ViTLR}, a novel video-based end-to-end neural network that processes multiple consecutive frames to achieve robust traffic light detection and state classification. The architecture leverages a transformer-like design with convolutional self-attention modules, which is optimized specifically for deployment on the Rockchip RV1126 embedded platform. Extensive evaluations on two real-world datasets demonstrate that \\textit{ViTLR} achieves state-of-the-art performance while maintaining real-time processing capabilities (>25 FPS) on RV1126's NPU. The system shows superior robustness across temporal stability, varying target distances, and challenging environmental conditions compared to existing single-frame approaches. We have successfully integrated \\textit{ViTLR} into an ego-lane traffic light recognition system using HD maps for autonomous driving applications. The complete implementation, including source code and datasets, is made publicly available to facilitate further research in this domain."
  },
  {
    "title": "A Benchmark for Vision-Centric HD Mapping by V2I Systems",
    "url": "http://arxiv.org/abs/2503.23963v1",
    "arxiv_id": "2503.23963v1",
    "authors": [
      "Miao Fan",
      "Shanshan Yu",
      "Shengtong Xu",
      "Kun Jiang",
      "Haoyi Xiong",
      "Xiangzeng Liu"
    ],
    "published": "2025-03-31T11:24:53+00:00",
    "summary": "Autonomous driving faces safety challenges due to a lack of global perspective and the semantic information of vectorized high-definition (HD) maps. Information from roadside cameras can greatly expand the map perception range through vehicle-to-infrastructure (V2I) communications. However, there is still no dataset from the real world available for the study on map vectorization onboard under the scenario of vehicle-infrastructure cooperation. To prosper the research on online HD mapping for Vehicle-Infrastructure Cooperative Autonomous Driving (VICAD), we release a real-world dataset, which contains collaborative camera frames from both vehicles and roadside infrastructures, and provides human annotations of HD map elements. We also present an end-to-end neural framework (i.e., V2I-HD) leveraging vision-centric V2I systems to construct vectorized maps. To reduce computation costs and further deploy V2I-HD on autonomous vehicles, we introduce a directionally decoupled self-attention mechanism to V2I-HD. Extensive experiments show that V2I-HD has superior performance in real-time inference speed, as tested by our real-world dataset. Abundant qualitative results also demonstrate stable and robust map construction quality with low cost in complex and various driving scenes. As a benchmark, both source codes and the dataset have been released at OneDrive for the purpose of further study."
  },
  {
    "title": "What the F*ck Is Artificial General Intelligence?",
    "url": "http://arxiv.org/abs/2503.23923v1",
    "arxiv_id": "2503.23923v1",
    "authors": [
      "Michael Timothy Bennett"
    ],
    "published": "2025-03-31T10:15:37+00:00",
    "summary": "Artificial general intelligence (AGI) is an established field of research. Yet Melanie Mitchell and others have questioned if the term still has meaning. AGI has been subject to so much hype and speculation it has become something of a Rorschach test. Mitchell points out that the debate will only be settled through long term, scientific investigation. To that end here is a short, accessible and provocative overview of AGI. I compare definitions of intelligence, settling on intelligence in terms of adaptation and AGI as an artificial scientist. Taking my queue from Sutton's Bitter Lesson I describe two foundational tools used to build adaptive systems: search and approximation. I compare pros, cons, hybrids and architectures like o3, AlphaGo, AERA, NARS and Hyperon. I then discuss overall meta-approaches to making systems behave more intelligently. I divide them into scale-maxing, simp-maxing, w-maxing based on the Bitter Lesson, Ockham's and Bennett's Razors. These maximise resources, simplicity of form, and the weakness of constraints on functionality. I discuss examples including AIXI, the free energy principle and The Embiggening of language models. I conclude that though scale-maxed approximation dominates, AGI will be a fusion of tools and meta-approaches. The Embiggening was enabled by improvements in hardware. Now the bottlenecks are sample and energy efficiency."
  },
  {
    "title": "Scenarios for the Deployment of Automated Vehicles in Europe",
    "url": "http://arxiv.org/abs/2503.23914v1",
    "arxiv_id": "2503.23914v1",
    "authors": [
      "Louison Duboz",
      "Ioan Cristinel Raileanu",
      "Jette Krause",
      "Ana Norman-L\u00f3pez",
      "Matthias Weitzel",
      "Biagio Ciuffo"
    ],
    "published": "2025-03-31T10:06:27+00:00",
    "summary": "The deployment of Automated Vehicles (AVs) is expected to address road transport externalities (e.g., safety, traffic, environmental impact, etc.). For this reason, a legal framework for their large-scale market introduction and deployment is currently being developed in the European Union. Despite the first steps towards road transport automation, the timeline for full automation and its potential economic benefits remains uncertain. The aim of this paper is twofold. First, it presents a methodological framework to determine deployment pathways of the five different levels of automation in EU27+UK to 2050 under three scenarios (i.e., slow, medium baseline and fast) focusing on passenger vehicles. Second, it proposes an assessment of the economic impact of AVs through the calculation of the value-added. The method to define assumptions and uptake trajectories involves a comprehensive literature review, expert interviews, and a model to forecast the new registrations of different levels of automation. In this way, the interviews provided insights that complemented the literature and informed the design of assumptions and deployment trajectories. The added-value assessment shows additional economic activity due to the introduction of automated technologies in all uptake scenarios."
  },
  {
    "title": "FineCausal: A Causal-Based Framework for Interpretable Fine-Grained Action Quality Assessment",
    "url": "http://arxiv.org/abs/2503.23911v1",
    "arxiv_id": "2503.23911v1",
    "authors": [
      "Ruisheng Han",
      "Kanglei Zhou",
      "Amir Atapour-Abarghouei",
      "Xiaohui Liang",
      "Hubert P. H. Shum"
    ],
    "published": "2025-03-31T10:02:29+00:00",
    "summary": "Action quality assessment (AQA) is critical for evaluating athletic performance, informing training strategies, and ensuring safety in competitive sports. However, existing deep learning approaches often operate as black boxes and are vulnerable to spurious correlations, limiting both their reliability and interpretability. In this paper, we introduce FineCausal, a novel causal-based framework that achieves state-of-the-art performance on the FineDiving-HM dataset. Our approach leverages a Graph Attention Network-based causal intervention module to disentangle human-centric foreground cues from background confounders, and incorporates a temporal causal attention module to capture fine-grained temporal dependencies across action stages. This dual-module strategy enables FineCausal to generate detailed spatio-temporal representations that not only achieve state-of-the-art scoring performance but also provide transparent, interpretable feedback on which features drive the assessment. Despite its strong performance, FineCausal requires extensive expert knowledge to define causal structures and depends on high-quality annotations, challenges that we discuss and address as future research directions. Code is available at https://github.com/Harrison21/FineCausal."
  },
  {
    "title": "Boosting MLLM Reasoning with Text-Debiased Hint-GRPO",
    "url": "http://arxiv.org/abs/2503.23905v1",
    "arxiv_id": "2503.23905v1",
    "authors": [
      "Qihan Huang",
      "Long Chan",
      "Jinlong Liu",
      "Wanggui He",
      "Hao Jiang",
      "Mingli Song",
      "Jingyuan Chen",
      "Chang Yao",
      "Jie Song"
    ],
    "published": "2025-03-31T09:54:55+00:00",
    "summary": "MLLM reasoning has drawn widespread research for its excellent problem-solving capability. Current reasoning methods fall into two types: PRM, which supervises the intermediate reasoning steps, and ORM, which supervises the final results. Recently, DeepSeek-R1 has challenged the traditional view that PRM outperforms ORM, which demonstrates strong generalization performance using an ORM method (i.e., GRPO). However, current MLLM's GRPO algorithms still struggle to handle challenging and complex multimodal reasoning tasks (e.g., mathematical reasoning). In this work, we reveal two problems that impede the performance of GRPO on the MLLM: Low data utilization and Text-bias. Low data utilization refers to that GRPO cannot acquire positive rewards to update the MLLM on difficult samples, and text-bias is a phenomenon that the MLLM bypasses image condition and solely relies on text condition for generation after GRPO training. To tackle these problems, this work proposes Hint-GRPO that improves data utilization by adaptively providing hints for samples of varying difficulty, and text-bias calibration that mitigates text-bias by calibrating the token prediction logits with image condition in test-time. Experiment results on three base MLLMs across eleven datasets demonstrate that our proposed methods advance the reasoning capability of original MLLM by a large margin, exhibiting superior performance to existing MLLM reasoning methods. Our code is available at https://github.com/hqhQAQ/Hint-GRPO."
  },
  {
    "title": "Expanding RL with Verifiable Rewards Across Diverse Domains",
    "url": "http://arxiv.org/abs/2503.23829v1",
    "arxiv_id": "2503.23829v1",
    "authors": [
      "Yi Su",
      "Dian Yu",
      "Linfeng Song",
      "Juntao Li",
      "Haitao Mi",
      "Zhaopeng Tu",
      "Min Zhang",
      "Dong Yu"
    ],
    "published": "2025-03-31T08:22:49+00:00",
    "summary": "Reinforcement learning (RL) with verifiable rewards (RLVR) has shown promising results in mathematical reasoning and coding tasks where well-structured reference answers are available. However, its applicability to broader domains remains underexplored. In this work, we study the extension of RLVR to more diverse domains such as medicine, chemistry, psychology, and economics. We observe high agreement in binary judgments across different large language models (LLMs) when objective reference answers exist, which challenges the necessity of large-scale annotation for training domain-specific reward models. To address the limitations of binary rewards when handling unstructured reference answers, we further incorporate model-based soft scoring into RLVR to improve its flexibility. Our experiments show that a distilled generative reward model can serve as an effective cross-domain verifier, providing reliable reward signals for RL without requiring domain-specific annotations. By fine-tuning a base 7B model using various RL algorithms against our reward model, we obtain policies that outperform state-of-the-art open-source aligned LLMs such as Qwen2.5-72B-Instruct and DeepSeek-R1-Distill-Qwen-32B by a large margin, across domains in free-form answer settings. This also strengthens RLVR's robustness and scalability, highlighting its potential for real-world applications with noisy or weak labels."
  },
  {
    "title": "Crossing the Reward Bridge: Expanding RL with Verifiable Rewards Across Diverse Domains",
    "url": "http://arxiv.org/abs/2503.23829v2",
    "arxiv_id": "2503.23829v2",
    "authors": [
      "Yi Su",
      "Dian Yu",
      "Linfeng Song",
      "Juntao Li",
      "Haitao Mi",
      "Zhaopeng Tu",
      "Min Zhang",
      "Dong Yu"
    ],
    "published": "2025-03-31T08:22:49+00:00",
    "summary": "Reinforcement learning with verifiable rewards (RLVR) has demonstrated significant success in enhancing mathematical reasoning and coding performance of large language models (LLMs), especially when structured reference answers are accessible for verification. However, its extension to broader, less structured domains remains unexplored. In this work, we investigate the effectiveness and scalability of RLVR across diverse real-world domains including medicine, chemistry, psychology, economics, and education, where structured reference answers are typically unavailable. We reveal that binary verification judgments on broad-domain tasks exhibit high consistency across various LLMs provided expert-written reference answers exist. Motivated by this finding, we utilize a generative scoring technique that yields soft, model-based reward signals to overcome limitations posed by binary verifications, especially in free-form, unstructured answer scenarios. We further demonstrate the feasibility of training cross-domain generative reward models using relatively small (7B) LLMs without the need for extensive domain-specific annotation. Through comprehensive experiments, our RLVR framework establishes clear performance gains, significantly outperforming state-of-the-art open-source aligned models such as Qwen2.5-72B and DeepSeek-R1-Distill-Qwen-32B across domains in free-form settings. Our approach notably enhances the robustness, flexibility, and scalability of RLVR, representing a substantial step towards practical reinforcement learning applications in complex, noisy-label scenarios."
  },
  {
    "title": "Thinking Longer, Not Larger: Enhancing Software Engineering Agents via Scaling Test-Time Compute",
    "url": "http://arxiv.org/abs/2503.23803v1",
    "arxiv_id": "2503.23803v1",
    "authors": [
      "Yingwei Ma",
      "Binhua Li",
      "Yihong Dong",
      "Xue Jiang",
      "Rongyu Cao",
      "Jue Chen",
      "Fei Huang",
      "Yongbin Li"
    ],
    "published": "2025-03-31T07:31:32+00:00",
    "summary": "Recent advancements in software engineering agents have demonstrated promising capabilities in automating program improvements. However, their reliance on closed-source or resource-intensive models introduces significant deployment challenges in private environments, prompting a critical question: \\textit{How can personally deployable open-source LLMs achieve comparable code reasoning performance?}   To this end, we propose a unified Test-Time Compute scaling framework that leverages increased inference-time computation instead of larger models. Our framework incorporates two complementary strategies: internal TTC and external TTC. Internally, we introduce a \\textit{development-contextualized trajectory synthesis} method leveraging real-world software repositories to bootstrap multi-stage reasoning processes, such as fault localization and patch generation. We further enhance trajectory quality through rejection sampling, rigorously evaluating trajectories along accuracy and complexity. Externally, we propose a novel \\textit{development-process-based search} strategy guided by reward models and execution verification. This approach enables targeted computational allocation at critical development decision points, overcoming limitations of existing \"end-point only\" verification methods.   Evaluations on SWE-bench Verified demonstrate our \\textbf{32B model achieves a 46\\% issue resolution rate}, surpassing significantly larger models such as DeepSeek R1 671B and OpenAI o1. Additionally, we provide the empirical validation of the test-time scaling phenomenon within SWE agents, revealing that \\textbf{models dynamically allocate more tokens to increasingly challenging problems}, effectively enhancing reasoning capabilities. We publicly release all training data, models, and code to facilitate future research. https://github.com/yingweima2022/SWE-Reasoner"
  },
  {
    "title": "LLMigrate: Transforming \"Lazy\" Large Language Models into Efficient Source Code Migrators",
    "url": "http://arxiv.org/abs/2503.23791v1",
    "arxiv_id": "2503.23791v1",
    "authors": [
      "Yuchen Liu",
      "Junhao Hu",
      "Yingdi Shan",
      "Ge Li",
      "Yanzhen Zou",
      "Yihong Dong",
      "Tao Xie"
    ],
    "published": "2025-03-31T07:09:07+00:00",
    "summary": "Rewriting C code in Rust provides stronger memory safety, yet migrating large codebases such as the 32-million-line Linux kernel remains challenging. While rule-based translators (e.g., C2Rust) provide accurate yet largely unsafe Rust programs, recent Large Language Model (LLM) approaches produce more idiomatic, safe Rust programs but frequently exhibit \"laziness\", omitting significant portions of the target code. To address the issue, in this paper, we present LLMigrate, an LLM-based C-to-Rust translation tool that splits modules into discrete functions, translating them individually, and then reintegrating them. LLMigrate uses static analysis to retain necessary context, pairs GPT-4o (a state-of-the-art LLM) with compiler-driven translation and program-repair techniques for complex core functions, and leverages call-graph-guided translation to ensure consistent interfaces. Evaluations on three representative Linux kernel modules (math, sort, and ramfs) show that LLMigrate requires modifying less than 15\\% of the target code, significantly outperforming a pure GPT-4o-based migration."
  },
  {
    "title": "ObfusQate: Unveiling the First Quantum Program Obfuscation Framework",
    "url": "http://arxiv.org/abs/2503.23785v1",
    "arxiv_id": "2503.23785v1",
    "authors": [
      "Nilhil Bartake",
      "See Toh Zi Jie",
      "Carmen Wong Jiawen",
      "Michael Kasper",
      "Vivek Balachandran"
    ],
    "published": "2025-03-31T07:02:25+00:00",
    "summary": "This paper introduces ObfusQate, a novel tool that conducts obfuscations using quantum primitives to enhance the security of both classical and quantum programs. We have designed and implemented two primary categories of obfuscations: quantum circuit level obfuscation and code level obfuscation, encompassing a total of eight distinct methods. Quantum circuit-level obfuscation leverages on quantum gates and circuits, utilizing strategies such as quantum gate hiding and identity matrices to construct complex, non-intuitive circuits that effectively obscure core functionalities and resist reverse engineering, making the underlying code difficult to interpret. Meanwhile, code-level obfuscation manipulates the logical sequence of program operations through quantum-based opaque predicates, obfuscating execution paths and rendering program behavior more unpredictable and challenging to analyze. Additionally, ObfusQate can be used to obfuscate malicious code segments, making them harder to detect and analyze. These advancements establish a foundational framework for further exploration into the potential and limitations of quantum-based obfuscation techniques, positioning ObfusQate as a valuable tool for future developers to enhance code security in the evolving landscape of software development. To the best of our knowledge, ObfusQate represents the pioneering work in developing an automated framework for implementing obfuscations leveraging quantum primitives. Security evaluations show that obfuscations by ObfusQate maintain code behavior with polynomial overheads in space and time complexities. We have also demonstrated an offensive use case by embedding a keylogger into Shor's algorithm and obfuscating it using ObfusQate. Our results show that current Large language models like GPT 4o, GPT o3 mini and Grok 3 were not able to identify the malicious keylogger after obfuscation."
  },
  {
    "title": "Exact Solution of the Frustrated Potts Model with Next-Nearest-Neighbor Interactions in One Dimension: An AI-Aided Discovery",
    "url": "http://arxiv.org/abs/2503.23758v1",
    "arxiv_id": "2503.23758v1",
    "authors": [
      "Weiguo Yin"
    ],
    "published": "2025-03-31T06:16:26+00:00",
    "summary": "The one-dimensional $J_1$-$J_2$ $q$-state Potts model is solved exactly for arbitrary $q$, based on using OpenAI's latest reasoning model o3-mini-high to exactly solve the $q=3$ case. The exact results provide insights to outstanding physical problems such as the stacking of atomic or electronic orders in layered materials and the formation of a $T_c$-dome-shaped phase often seen in unconventional superconductors. The work is anticipated to fuel both the research in one-dimensional frustrated magnets for recently discovered finite-temperature application potentials and the fast moving topic area of AI for sciences."
  },
  {
    "title": "Towards Benchmarking and Assessing the Safety and Robustness of Autonomous Driving on Safety-critical Scenarios",
    "url": "http://arxiv.org/abs/2503.23708v1",
    "arxiv_id": "2503.23708v1",
    "authors": [
      "Jingzheng Li",
      "Xianglong Liu",
      "Shikui Wei",
      "Zhijun Chen",
      "Bing Li",
      "Qing Guo",
      "Xianqi Yang",
      "Yanjun Pu",
      "Jiakai Wang"
    ],
    "published": "2025-03-31T04:13:32+00:00",
    "summary": "Autonomous driving has made significant progress in both academia and industry, including performance improvements in perception task and the development of end-to-end autonomous driving systems. However, the safety and robustness assessment of autonomous driving has not received sufficient attention. Current evaluations of autonomous driving are typically conducted in natural driving scenarios. However, many accidents often occur in edge cases, also known as safety-critical scenarios. These safety-critical scenarios are difficult to collect, and there is currently no clear definition of what constitutes a safety-critical scenario. In this work, we explore the safety and robustness of autonomous driving in safety-critical scenarios. First, we provide a definition of safety-critical scenarios, including static traffic scenarios such as adversarial attack scenarios and natural distribution shifts, as well as dynamic traffic scenarios such as accident scenarios. Then, we develop an autonomous driving safety testing platform to comprehensively evaluate autonomous driving systems, encompassing not only the assessment of perception modules but also system-level evaluations. Our work systematically constructs a safety verification process for autonomous driving, providing technical support for the industry to establish standardized test framework and reduce risks in real-world road deployment."
  },
  {
    "title": "Modeling Framework to Predict Melting Dynamics at Microstructural Defects in TNT-HMX High Explosive Composites",
    "url": "http://arxiv.org/abs/2503.23680v1",
    "arxiv_id": "2503.23680v1",
    "authors": [
      "Ethan Holbrook",
      "Matthew P. Kroonblawd",
      "Brenden W. Hamilton",
      "H. Keo Springer",
      "Alejandro Strachan"
    ],
    "published": "2025-03-31T03:01:34+00:00",
    "summary": "Many high explosive (HE) formulations are composite materials whose microstructure is understood to impact functional characteristics. Interfaces are known to mediate the formation of hot spots that control their safety and initiation. To study such processes at molecular scales, we developed all-atom force fields (FFs) for Octol, a prototypical HE formulation comprised of TNT (2,4,6-trinitrotoluene) and HMX (octahydro-1,3,5,7-tetranitro-1,3,5,7-tetrazocine). We extended a FF for TNT and recasted it in a form that can be readily combined with a well-established FF for HMX. The resulting FF was extensively validated against experimental results and density functional theory calculations. We applied the new combined TNT-HMX FF to predict and rank surface and interface energies, which indicate that there is an energetic driver for coarsening of microstructural grains in TNT-HMX composites. Finally, we assess the impact of several microstructural environments on the dynamic melting of TNT crystal under ultrafast thermal loading. We find that both free surfaces and planar material interfaces are effective nucleation points for TNT melting. However, MD simulations show that TNT crystal is prone to superheating by at least 50 K on sub-nanosecond timescales and that the degree of superheating is inversely correlated with surface and interface energy. The modeling framework presented here will enable future studies on hot spot formation processes in accident scenarios that are governed by strong coupling between microstructural interfaces, material mechanics, momentum and energy transport, phase transitions, and chemistry."
  },
  {
    "title": "Exploring GPT-4 for Robotic Agent Strategy with Real-Time State Feedback and a Reactive Behaviour Framework",
    "url": "http://arxiv.org/abs/2503.23601v1",
    "arxiv_id": "2503.23601v1",
    "authors": [
      "Thomas O'Brien",
      "Ysobel Sims"
    ],
    "published": "2025-03-30T21:53:28+00:00",
    "summary": "We explore the use of GPT-4 on a humanoid robot in simulation and the real world as proof of concept of a novel large language model (LLM) driven behaviour method. LLMs have shown the ability to perform various tasks, including robotic agent behaviour. The problem involves prompting the LLM with a goal, and the LLM outputs the sub-tasks to complete to achieve that goal. Previous works focus on the executability and correctness of the LLM's generated tasks. We propose a method that successfully addresses practical concerns around safety, transitions between tasks, time horizons of tasks and state feedback. In our experiments we have found that our approach produces output for feasible requests that can be executed every time, with smooth transitions. User requests are achieved most of the time across a range of goal time horizons."
  },
  {
    "title": "When LLM Therapists Become Salespeople: Evaluating Large Language Models for Ethical Motivational Interviewing",
    "url": "http://arxiv.org/abs/2503.23566v1",
    "arxiv_id": "2503.23566v1",
    "authors": [
      "Haein Kong",
      "Seonghyeon Moon"
    ],
    "published": "2025-03-30T19:20:32+00:00",
    "summary": "Large language models (LLMs) have been actively applied in the mental health field. Recent research shows the promise of LLMs in applying psychotherapy, especially motivational interviewing (MI). However, there is a lack of studies investigating how language models understand MI ethics. Given the risks that malicious actors can use language models to apply MI for unethical purposes, it is important to evaluate their capability of differentiating ethical and unethical MI practices. Thus, this study investigates the ethical awareness of LLMs in MI with multiple experiments. Our findings show that LLMs have a moderate to strong level of knowledge in MI. However, their ethical standards are not aligned with the MI spirit, as they generated unethical responses and performed poorly in detecting unethical responses. We proposed a Chain-of-Ethic prompt to mitigate those risks and improve safety. Finally, our proposed strategy effectively improved ethical MI response generation and detection performance. These findings highlight the need for safety evaluations and guidelines for building ethical LLM-powered psychotherapy."
  },
  {
    "title": "Addressing Model Overcomplexity in Drug-Drug Interaction Prediction With Molecular Fingerprints",
    "url": "http://arxiv.org/abs/2503.23550v1",
    "arxiv_id": "2503.23550v1",
    "authors": [
      "Manel Gil-Sorribes",
      "Alexis Molina"
    ],
    "published": "2025-03-30T18:27:01+00:00",
    "summary": "Accurately predicting drug-drug interactions (DDIs) is crucial for pharmaceutical research and clinical safety. Recent deep learning models often suffer from high computational costs and limited generalization across datasets. In this study, we investigate a simpler yet effective approach using molecular representations such as Morgan fingerprints (MFPS), graph-based embeddings from graph convolutional networks (GCNs), and transformer-derived embeddings from MoLFormer integrated into a straightforward neural network. We benchmark our implementation on DrugBank DDI splits and a drug-drug affinity (DDA) dataset from the Food and Drug Administration. MFPS along with MoLFormer and GCN representations achieve competitive performance across tasks, even in the more challenging leak-proof split, highlighting the sufficiency of simple molecular representations. Moreover, we are able to identify key molecular motifs and structural patterns relevant to drug interactions via gradient-based analyses using the representations under study. Despite these results, dataset limitations such as insufficient chemical diversity, limited dataset size, and inconsistent labeling impact robust evaluation and challenge the need for more complex approaches. Our work provides a meaningful baseline and emphasizes the need for better dataset curation and progressive complexity scaling."
  },
  {
    "title": "If an LLM Were a Character, Would It Know Its Own Story? Evaluating Lifelong Learning in LLMs",
    "url": "http://arxiv.org/abs/2503.23514v1",
    "arxiv_id": "2503.23514v1",
    "authors": [
      "Siqi Fan",
      "Xiusheng Huang",
      "Yiqun Yao",
      "Xuezhi Fang",
      "Kang Liu",
      "Peng Han",
      "Shuo Shang",
      "Aixin Sun",
      "Yequan Wang"
    ],
    "published": "2025-03-30T16:50:57+00:00",
    "summary": "Large language models (LLMs) can carry out human-like dialogue, but unlike humans, they are stateless due to the superposition property. However, during multi-turn, multi-agent interactions, LLMs begin to exhibit consistent, character-like behaviors, hinting at a form of emergent lifelong learning. Despite this, existing benchmarks often fail to capture these dynamics, primarily focusing on static, open-ended evaluations. To address this gap, we introduce LIFESTATE-BENCH, a benchmark designed to assess lifelong learning in LLMs. It features two episodic datasets: Hamlet and a synthetic script collection, rich in narrative structure and character interactions. Our fact checking evaluation probes models' self-awareness, episodic memory retrieval, and relationship tracking, across both parametric and non-parametric approaches. Experiments on models like Llama3.1-8B, GPT-4-turbo, and DeepSeek R1, we demonstrate that nonparametric methods significantly outperform parametric ones in managing stateful learning. However, all models exhibit challenges with catastrophic forgetting as interactions extend, highlighting the need for further advancements in lifelong learning."
  },
  {
    "title": "RARE: Retrieval-Augmented Reasoning Modeling",
    "url": "http://arxiv.org/abs/2503.23513v1",
    "arxiv_id": "2503.23513v1",
    "authors": [
      "Zhengren Wang",
      "Jiayang Yu",
      "Dongsheng Ma",
      "Zhe Chen",
      "Yu Wang",
      "Zhiyu Li",
      "Feiyu Xiong",
      "Yanfeng Wang",
      "Weinan E",
      "Linpeng Tang",
      "Wentao Zhang"
    ],
    "published": "2025-03-30T16:49:44+00:00",
    "summary": "Domain-specific intelligence demands specialized knowledge and sophisticated reasoning for problem-solving, posing significant challenges for large language models (LLMs) that struggle with knowledge hallucination and inadequate reasoning capabilities under constrained parameter budgets. Inspired by Bloom's Taxonomy in educational theory, we propose Retrieval-Augmented Reasoning Modeling (RARE), a novel paradigm that decouples knowledge storage from reasoning optimization. RARE externalizes domain knowledge to retrievable sources and internalizes domain-specific reasoning patterns during training. Specifically, by injecting retrieved knowledge into training prompts, RARE transforms learning objectives from rote memorization to contextualized reasoning application. It enables models to bypass parameter-intensive memorization and prioritize the development of higher-order cognitive processes. Our experiments demonstrate that lightweight RARE-trained models (e.g., Llama-3.1-8B) could achieve state-of-the-art performance, surpassing retrieval-augmented GPT-4 and Deepseek-R1 distilled counterparts. RARE establishes a paradigm shift where maintainable external knowledge bases synergize with compact, reasoning-optimized models, collectively driving more scalable domain-specific intelligence. Repo: https://github.com/Open-DataFlow/RARE"
  },
  {
    "title": "Towards Trustworthy GUI Agents: A Survey",
    "url": "http://arxiv.org/abs/2503.23434v1",
    "arxiv_id": "2503.23434v1",
    "authors": [
      "Yucheng Shi",
      "Wenhao Yu",
      "Wenlin Yao",
      "Wenhu Chen",
      "Ninghao Liu"
    ],
    "published": "2025-03-30T13:26:00+00:00",
    "summary": "GUI agents, powered by large foundation models, can interact with digital interfaces, enabling various applications in web automation, mobile navigation, and software testing. However, their increasing autonomy has raised critical concerns about their security, privacy, and safety. This survey examines the trustworthiness of GUI agents in five critical dimensions: security vulnerabilities, reliability in dynamic environments, transparency and explainability, ethical considerations, and evaluation methodologies. We also identify major challenges such as vulnerability to adversarial attacks, cascading failure modes in sequential decision-making, and a lack of realistic evaluation benchmarks. These issues not only hinder real-world deployment but also call for comprehensive mitigation strategies beyond task success. As GUI agents become more widespread, establishing robust safety standards and responsible development practices is essential. This survey provides a foundation for advancing trustworthy GUI agents through systematic understanding and future research."
  },
  {
    "title": "What Makes an Evaluation Useful? Common Pitfalls and Best Practices",
    "url": "http://arxiv.org/abs/2503.23424v1",
    "arxiv_id": "2503.23424v1",
    "authors": [
      "Gil Gekker",
      "Meirav Segal",
      "Dan Lahav",
      "Omer Nevo"
    ],
    "published": "2025-03-30T12:51:47+00:00",
    "summary": "Following the rapid increase in Artificial Intelligence (AI) capabilities in recent years, the AI community has voiced concerns regarding possible safety risks. To support decision-making on the safe use and development of AI systems, there is a growing need for high-quality evaluations of dangerous model capabilities. While several attempts to provide such evaluations have been made, a clear definition of what constitutes a \"good evaluation\" has yet to be agreed upon. In this practitioners' perspective paper, we present a set of best practices for safety evaluations, drawing on prior work in model evaluation and illustrated through cybersecurity examples. We first discuss the steps of the initial thought process, which connects threat modeling to evaluation design. Then, we provide the characteristics and parameters that make an evaluation useful. Finally, we address additional considerations as we move from building specific evaluations to building a full and comprehensive evaluation suite."
  },
  {
    "title": "D3-Guard: Acoustic-based Drowsy Driving Detection Using Smartphones",
    "url": "http://arxiv.org/abs/2503.23393v1",
    "arxiv_id": "2503.23393v1",
    "authors": [
      "Yadong Xie",
      "Fan Li",
      "Yue Wu",
      "Song Yang",
      "Yu Wang"
    ],
    "published": "2025-03-30T10:52:01+00:00",
    "summary": "Since the number of cars has grown rapidly in recent years, driving safety draws more and more public attention. Drowsy driving is one of the biggest threatens to driving safety. Therefore, a simple but robust system that can detect drowsy driving with commercial off-the-shelf devices (such as smartphones) is very necessary. With this motivation, we explore the feasibility of purely using acoustic sensors embedded in smartphones to detect drowsy driving. We first study characteristics of drowsy driving, and find some unique patterns of Doppler shift caused by three typical drowsy behaviors, i.e. nodding, yawning and operating steering wheel. We then validate our important findings through empirical analysis of the driving data collected from real driving environments. We further propose a real-time Drowsy Driving Detection system (D3-Guard) based on audio devices embedded in smartphones. In order to improve the performance of our system, we adopt an effective feature extraction method based on undersampling technique and FFT, and carefully design a high-accuracy detector based on LSTM networks for the early detection of drowsy driving. Through extensive experiments with 5 volunteer drivers in real driving environments, our system can distinguish drowsy driving actions with an average total accuracy of 93.31% in real-time. Over 80% drowsy driving actions can be detected within first 70% of action duration."
  },
  {
    "title": "HearSmoking: Smoking Detection in Driving Environment via Acoustic Sensing on Smartphones",
    "url": "http://arxiv.org/abs/2503.23391v1",
    "arxiv_id": "2503.23391v1",
    "authors": [
      "Yadong Xie",
      "Fan Li",
      "Yue Wu",
      "Song Yang",
      "Yu Wang"
    ],
    "published": "2025-03-30T10:40:44+00:00",
    "summary": "Driving safety has drawn much public attention in recent years due to the fast-growing number of cars. Smoking is one of the threats to driving safety but is often ignored by drivers. Existing works on smoking detection either work in contact manner or need additional devices. This motivates us to explore the practicability of using smartphones to detect smoking events in driving environment. In this paper, we propose a cigarette smoking detection system, named HearSmoking, which only uses acoustic sensors on smartphones to improve driving safety. After investigating typical smoking habits of drivers, including hand movement and chest fluctuation, we design an acoustic signal to be emitted by the speaker and received by the microphone. We calculate Relative Correlation Coefficient of received signals to obtain movement patterns of hands and chest. The processed data is sent into a trained Convolutional Neural Network for classification of hand movement. We also design a method to detect respiration at the same time. To improve system performance, we further analyse the periodicity of the composite smoking motion. Through extensive experiments in real driving environments, HearSmoking detects smoking events with an average total accuracy of 93.44 percent in real-time."
  },
  {
    "title": "OnSiteVRU: A High-Resolution Trajectory Dataset for High-Density Vulnerable Road Users",
    "url": "http://arxiv.org/abs/2503.23365v1",
    "arxiv_id": "2503.23365v1",
    "authors": [
      "Zhangcun Yan",
      "Jianqing Li",
      "Peng Hang",
      "Jian Sun"
    ],
    "published": "2025-03-30T08:44:55+00:00",
    "summary": "With the acceleration of urbanization and the growth of transportation demands, the safety of vulnerable road users (VRUs, such as pedestrians and cyclists) in mixed traffic flows has become increasingly prominent, necessitating high-precision and diverse trajectory data to support the development and optimization of autonomous driving systems. However, existing datasets fall short in capturing the diversity and dynamics of VRU behaviors, making it difficult to meet the research demands of complex traffic environments. To address this gap, this study developed the OnSiteVRU datasets, which cover a variety of scenarios, including intersections, road segments, and urban villages. These datasets provide trajectory data for motor vehicles, electric bicycles, and human-powered bicycles, totaling approximately 17,429 trajectories with a precision of 0.04 seconds. The datasets integrate both aerial-view natural driving data and onboard real-time dynamic detection data, along with environmental information such as traffic signals, obstacles, and real-time maps, enabling a comprehensive reconstruction of interaction events. The results demonstrate that VRU\\_Data outperforms traditional datasets in terms of VRU density and scene coverage, offering a more comprehensive representation of VRU behavioral characteristics. This provides critical support for traffic flow modeling, trajectory prediction, and autonomous driving virtual testing. The dataset is publicly available for download at:   https://www.kaggle.com/datasets/zcyan2/mixed-traffic-trajectory-dataset-in-from-shanghai."
  },
  {
    "title": "A Scalable Framework for Evaluating Health Language Models",
    "url": "http://arxiv.org/abs/2503.23339v1",
    "arxiv_id": "2503.23339v1",
    "authors": [
      "Neil Mallinar",
      "A. Ali Heydari",
      "Xin Liu",
      "Anthony Z. Faranesh",
      "Brent Winslow",
      "Nova Hammerquist",
      "Benjamin Graef",
      "Cathy Speed",
      "Mark Malhotra",
      "Shwetak Patel",
      "Javier L. Prieto",
      "Daniel McDuff",
      "Ahmed A. Metwally"
    ],
    "published": "2025-03-30T06:47:57+00:00",
    "summary": "Large language models (LLMs) have emerged as powerful tools for analyzing complex datasets. Recent studies demonstrate their potential to generate useful, personalized responses when provided with patient-specific health information that encompasses lifestyle, biomarkers, and context. As LLM-driven health applications are increasingly adopted, rigorous and efficient one-sided evaluation methodologies are crucial to ensure response quality across multiple dimensions, including accuracy, personalization and safety. Current evaluation practices for open-ended text responses heavily rely on human experts. This approach introduces human factors and is often cost-prohibitive, labor-intensive, and hinders scalability, especially in complex domains like healthcare where response assessment necessitates domain expertise and considers multifaceted patient data. In this work, we introduce Adaptive Precise Boolean rubrics: an evaluation framework that streamlines human and automated evaluation of open-ended questions by identifying gaps in model responses using a minimal set of targeted rubrics questions. Our approach is based on recent work in more general evaluation settings that contrasts a smaller set of complex evaluation targets with a larger set of more precise, granular targets answerable with simple boolean responses. We validate this approach in metabolic health, a domain encompassing diabetes, cardiovascular disease, and obesity. Our results demonstrate that Adaptive Precise Boolean rubrics yield higher inter-rater agreement among expert and non-expert human evaluators, and in automated assessments, compared to traditional Likert scales, while requiring approximately half the evaluation time of Likert-based methods. This enhanced efficiency, particularly in automated evaluation and non-expert contributions, paves the way for more extensive and cost-effective evaluation of LLMs in health."
  },
  {
    "title": "Ensuring Safe and Smooth Control in Safety-Critical Systems via Filtered Control Barrier Functions",
    "url": "http://arxiv.org/abs/2503.23267v1",
    "arxiv_id": "2503.23267v1",
    "authors": [
      "Shuo Liu",
      "Wei Xiao",
      "Calin A. Belta"
    ],
    "published": "2025-03-30T01:05:39+00:00",
    "summary": "In safety-critical control systems, ensuring both system safety and smooth control input variation is essential for theoretical guarantees and practical deployment. Existing Control Barrier Function (CBF) frameworks, especially High-Order CBFs (HOCBFs), effectively enforce safety constraints but often lead to nonsmooth or discontinuous control inputs that can degrade system performance or violate actuator limitations. This paper introduces Filtered Control Barrier Functions (FCBFs), a novel extension of HOCBFs that incorporates an auxiliary dynamic system-referred to as an input regularization filter-to produce Lipschitz continuous control inputs. The proposed framework ensures safety, control bounds, and smoothness simultaneously by integrating FCBFs and HOCBFs within a unified quadratic program (QP). Theoretical guarantees are provided, and simulations on a unicycle model demonstrate the effectiveness of the proposed method compared to standard and smoothness-penalized HOCBF approaches."
  },
  {
    "title": "Encrypted Prompt: Securing LLM Applications Against Unauthorized Actions",
    "url": "http://arxiv.org/abs/2503.23250v1",
    "arxiv_id": "2503.23250v1",
    "authors": [
      "Shih-Han Chan"
    ],
    "published": "2025-03-29T23:26:57+00:00",
    "summary": "Security threats like prompt injection attacks pose significant risks to applications that integrate Large Language Models (LLMs), potentially leading to unauthorized actions such as API misuse. Unlike previous approaches that aim to detect these attacks on a best-effort basis, this paper introduces a novel method that appends an Encrypted Prompt to each user prompt, embedding current permissions. These permissions are verified before executing any actions (such as API calls) generated by the LLM. If the permissions are insufficient, the LLM's actions will not be executed, ensuring safety. This approach guarantees that only actions within the scope of the current permissions from the LLM can proceed. In scenarios where adversarial prompts are introduced to mislead the LLM, this method ensures that any unauthorized actions from LLM wouldn't be executed by verifying permissions in Encrypted Prompt. Thus, threats like prompt injection attacks that trigger LLM to generate harmful actions can be effectively mitigated."
  },
  {
    "title": "Action Recognition in Real-World Ambient Assisted Living Environment",
    "url": "http://arxiv.org/abs/2503.23214v1",
    "arxiv_id": "2503.23214v1",
    "authors": [
      "Vincent Gbouna Zakka",
      "Zhuangzhuang Dai",
      "Luis J. Manso"
    ],
    "published": "2025-03-29T20:32:22+00:00",
    "summary": "The growing ageing population and their preference to maintain independence by living in their own homes require proactive strategies to ensure safety and support. Ambient Assisted Living (AAL) technologies have emerged to facilitate ageing in place by offering continuous monitoring and assistance within the home. Within AAL technologies, action recognition plays a crucial role in interpreting human activities and detecting incidents like falls, mobility decline, or unusual behaviours that may signal worsening health conditions. However, action recognition in practical AAL applications presents challenges, including occlusions, noisy data, and the need for real-time performance. While advancements have been made in accuracy, robustness to noise, and computation efficiency, achieving a balance among them all remains a challenge. To address this challenge, this paper introduces the Robust and Efficient Temporal Convolution network (RE-TCN), which comprises three main elements: Adaptive Temporal Weighting (ATW), Depthwise Separable Convolutions (DSC), and data augmentation techniques. These elements aim to enhance the model's accuracy, robustness against noise and occlusion, and computational efficiency within real-world AAL contexts. RE-TCN outperforms existing models in terms of accuracy, noise and occlusion robustness, and has been validated on four benchmark datasets: NTU RGB+D 60, Northwestern-UCLA, SHREC'17, and DHG-14/28. The code is publicly available at: https://github.com/Gbouna/RE-TCN"
  },
  {
    "title": "RECALL-MM: A Multimodal Dataset of Consumer Product Recalls for Risk Analysis using Computational Methods and Large Language Models",
    "url": "http://arxiv.org/abs/2503.23213v1",
    "arxiv_id": "2503.23213v1",
    "authors": [
      "Diana Bolanos",
      "Mohammadmehdi Ataei",
      "Daniele Grandi",
      "Kosa Goucher-Lambert"
    ],
    "published": "2025-03-29T20:27:28+00:00",
    "summary": "Product recalls provide valuable insights into potential risks and hazards within the engineering design process, yet their full potential remains underutilized. In this study, we curate data from the United States Consumer Product Safety Commission (CPSC) recalls database to develop a multimodal dataset, RECALL-MM, that informs data-driven risk assessment using historical information, and augment it using generative methods. Patterns in the dataset highlight specific areas where improved safety measures could have significant impact. We extend our analysis by demonstrating interactive clustering maps that embed all recalls into a shared latent space based on recall descriptions and product names. Leveraging these data-driven tools, we explore three case studies to demonstrate the dataset's utility in identifying product risks and guiding safer design decisions. The first two case studies illustrate how designers can visualize patterns across recalled products and situate new product ideas within the broader recall landscape to proactively anticipate hazards. In the third case study, we extend our approach by employing a large language model (LLM) to predict potential hazards based solely on product images. This demonstrates the model's ability to leverage visual context to identify risk factors, revealing strong alignment with historical recall data across many hazard categories. However, the analysis also highlights areas where hazard prediction remains challenging, underscoring the importance of risk awareness throughout the design process. Collectively, this work aims to bridge the gap between historical recall data and future product safety, presenting a scalable, data-driven approach to safer engineering design."
  },
  {
    "title": "Intelligent Bear Prevention System Based on Computer Vision: An Approach to Reduce Human-Bear Conflicts in the Tibetan Plateau Area, China",
    "url": "http://arxiv.org/abs/2503.23178v1",
    "arxiv_id": "2503.23178v1",
    "authors": [
      "Pengyu Chen",
      "Teng Fei",
      "Yunyan Du",
      "Jiawei Yi",
      "Yi Li",
      "John A. Kupfer"
    ],
    "published": "2025-03-29T18:10:11+00:00",
    "summary": "Conflicts between humans and bears on the Tibetan Plateau present substantial threats to local communities and hinder wildlife preservation initiatives. This research introduces a novel strategy that incorporates computer vision alongside Internet of Things (IoT) technologies to alleviate these issues. Tailored specifically for the harsh environment of the Tibetan Plateau, the approach utilizes the K210 development board paired with the YOLO object detection framework along with a tailored bear-deterrent mechanism, offering minimal energy usage and real-time efficiency in bear identification and deterrence. The model's performance was evaluated experimentally, achieving a mean Average Precision (mAP) of 91.4%, demonstrating excellent precision and dependability. By integrating energy-efficient components, the proposed system effectively surpasses the challenges of remote and off-grid environments, ensuring uninterrupted operation in secluded locations. This study provides a viable, eco-friendly, and expandable solution to mitigate human-bear conflicts, thereby improving human safety and promoting bear conservation in isolated areas like Yushu, China."
  },
  {
    "title": "Reasoning-SQL: Reinforcement Learning with SQL Tailored Partial Rewards for Reasoning-Enhanced Text-to-SQL",
    "url": "http://arxiv.org/abs/2503.23157v1",
    "arxiv_id": "2503.23157v1",
    "authors": [
      "Mohammadreza Pourreza",
      "Shayan Talaei",
      "Ruoxi Sun",
      "Xingchen Wan",
      "Hailong Li",
      "Azalia Mirhoseini",
      "Amin Saberi",
      "Sercan \"O. Arik"
    ],
    "published": "2025-03-29T17:29:30+00:00",
    "summary": "Text-to-SQL is a challenging task involving multiple reasoning-intensive subtasks, including natural language understanding, database schema comprehension, and precise SQL query formulation. Existing approaches often rely on handcrafted reasoning paths with inductive biases that can limit their overall effectiveness. Motivated by the recent success of reasoning-enhanced models such as DeepSeek R1 and OpenAI o1, which effectively leverage reward-driven self-exploration to enhance reasoning capabilities and generalization, we propose a novel set of partial rewards tailored specifically for the Text-to-SQL task. Our reward set includes schema-linking, AI feedback, n-gram similarity, and syntax check, explicitly designed to address the reward sparsity issue prevalent in reinforcement learning (RL). Leveraging group relative policy optimization (GRPO), our approach explicitly encourages large language models (LLMs) to develop intrinsic reasoning skills necessary for accurate SQL query generation. With models of different sizes, we demonstrate that RL-only training with our proposed rewards consistently achieves higher accuracy and superior generalization compared to supervised fine-tuning (SFT). Remarkably, our RL-trained 14B-parameter model significantly outperforms larger proprietary models, e.g. o3-mini by 4% and Gemini-1.5-Pro-002 by 3% on the BIRD benchmark. These highlight the efficacy of our proposed RL-training framework with partial rewards for enhancing both accuracy and reasoning capabilities in Text-to-SQL tasks."
  },
  {
    "title": "Reasoning-SQL: Reinforcement Learning with SQL Tailored Partial Rewards for Reasoning-Enhanced Text-to-SQL",
    "url": "http://arxiv.org/abs/2503.23157v2",
    "arxiv_id": "2503.23157v2",
    "authors": [
      "Mohammadreza Pourreza",
      "Shayan Talaei",
      "Ruoxi Sun",
      "Xingchen Wan",
      "Hailong Li",
      "Azalia Mirhoseini",
      "Amin Saberi",
      "Sercan \"O. Arik"
    ],
    "published": "2025-03-29T17:29:30+00:00",
    "summary": "Text-to-SQL is a challenging task involving multiple reasoning-intensive subtasks, including natural language understanding, database schema comprehension, and precise SQL query formulation. Existing approaches often rely on handcrafted reasoning paths with inductive biases that can limit their overall effectiveness. Motivated by the recent success of reasoning-enhanced models such as DeepSeek R1 and OpenAI o1, which effectively leverage reward-driven self-exploration to enhance reasoning capabilities and generalization, we propose a novel set of partial rewards tailored specifically for the Text-to-SQL task. Our reward set includes schema-linking, AI feedback, n-gram similarity, and syntax check, explicitly designed to address the reward sparsity issue prevalent in reinforcement learning (RL). Leveraging group relative policy optimization (GRPO), our approach explicitly encourages large language models (LLMs) to develop intrinsic reasoning skills necessary for accurate SQL query generation. With models of different sizes, we demonstrate that RL-only training with our proposed rewards consistently achieves higher accuracy and superior generalization compared to supervised fine-tuning (SFT). Remarkably, our RL-trained 14B-parameter model significantly outperforms larger proprietary models, e.g. o3-mini by 4% and Gemini-1.5-Pro-002 by 3% on the BIRD benchmark. These highlight the efficacy of our proposed RL-training framework with partial rewards for enhancing both accuracy and reasoning capabilities in Text-to-SQL tasks."
  },
  {
    "title": "CodeARC: Benchmarking Reasoning Capabilities of LLM Agents for Inductive Program Synthesis",
    "url": "http://arxiv.org/abs/2503.23145v1",
    "arxiv_id": "2503.23145v1",
    "authors": [
      "Anjiang Wei",
      "Tarun Suresh",
      "Jiannan Cao",
      "Naveen Kannan",
      "Yuheng Wu",
      "Kai Yan",
      "Thiago S. F. X. Teixeira",
      "Ke Wang",
      "Alex Aiken"
    ],
    "published": "2025-03-29T16:50:39+00:00",
    "summary": "Inductive program synthesis, or programming by example, requires synthesizing functions from input-output examples that generalize to unseen inputs. While large language model agents have shown promise in programming tasks guided by natural language, their ability to perform inductive program synthesis is underexplored. Existing evaluation protocols rely on static sets of examples and held-out tests, offering no feedback when synthesized functions are incorrect and failing to reflect real-world scenarios such as reverse engineering. We propose CodeARC, the Code Abstraction and Reasoning Challenge, a new evaluation framework where agents interact with a hidden target function by querying it with new inputs, synthesizing candidate functions, and iteratively refining their solutions using a differential testing oracle. This interactive setting encourages agents to perform function calls and self-correction based on feedback. We construct the first large-scale benchmark for general-purpose inductive program synthesis, featuring 1114 functions. Among 18 models evaluated, o3-mini performs best with a success rate of 52.7%, highlighting the difficulty of this task. Fine-tuning LLaMA-3.1-8B-Instruct on curated synthesis traces yields up to a 31% relative performance gain. CodeARC provides a more realistic and challenging testbed for evaluating LLM-based program synthesis and inductive reasoning."
  },
  {
    "title": "Uncertainty-Instructed Structure Injection for Generalizable HD Map Construction",
    "url": "http://arxiv.org/abs/2503.23109v1",
    "arxiv_id": "2503.23109v1",
    "authors": [
      "Xiaolu Liu",
      "Ruizi Yang",
      "Song Wang",
      "Wentong Li",
      "Junbo Chen",
      "Jianke Zhu"
    ],
    "published": "2025-03-29T15:01:38+00:00",
    "summary": "Reliable high-definition (HD) map construction is crucial for the driving safety of autonomous vehicles. Although recent studies demonstrate improved performance, their generalization capability across unfamiliar driving scenes remains unexplored. To tackle this issue, we propose UIGenMap, an uncertainty-instructed structure injection approach for generalizable HD map vectorization, which concerns the uncertainty resampling in statistical distribution and employs explicit instance features to reduce excessive reliance on training data. Specifically, we introduce the perspective-view (PV) detection branch to obtain explicit structural features, in which the uncertainty-aware decoder is designed to dynamically sample probability distributions considering the difference in scenes. With probabilistic embedding and selection, UI2DPrompt is proposed to construct PV-learnable prompts. These PV prompts are integrated into the map decoder by designed hybrid injection to compensate for neglected instance structures. To ensure real-time inference, a lightweight Mimic Query Distillation is designed to learn from PV prompts, which can serve as an efficient alternative to the flow of PV branches. Extensive experiments on challenging geographically disjoint (geo-based) data splits demonstrate that our UIGenMap achieves superior performance, with +5.7 mAP improvement on the nuScenes dataset. Source code will be available at https://github.com/xiaolul2/UIGenMap."
  },
  {
    "title": "RL2Grid: Benchmarking Reinforcement Learning in Power Grid Operations",
    "url": "http://arxiv.org/abs/2503.23101v1",
    "arxiv_id": "2503.23101v1",
    "authors": [
      "Enrico Marchesini",
      "Benjamin Donnot",
      "Constance Crozier",
      "Ian Dytham",
      "Christian Merz",
      "Lars Schewe",
      "Nico Westerbeck",
      "Cathy Wu",
      "Antoine Marot",
      "Priya L. Donti"
    ],
    "published": "2025-03-29T14:39:17+00:00",
    "summary": "Reinforcement learning (RL) can transform power grid operations by providing adaptive and scalable controllers essential for grid decarbonization. However, existing methods struggle with the complex dynamics, aleatoric uncertainty, long-horizon goals, and hard physical constraints that occur in real-world systems. This paper presents RL2Grid, a benchmark designed in collaboration with power system operators to accelerate progress in grid control and foster RL maturity. Built on a power simulation framework developed by RTE France, RL2Grid standardizes tasks, state and action spaces, and reward structures within a unified interface for a systematic evaluation and comparison of RL approaches. Moreover, we integrate real control heuristics and safety constraints informed by the operators' expertise to ensure RL2Grid aligns with grid operation requirements. We benchmark popular RL baselines on the grid control tasks represented within RL2Grid, establishing reference performance metrics. Our results and discussion highlight the challenges that power grids pose for RL methods, emphasizing the need for novel algorithms capable of handling real-world physical systems."
  },
  {
    "title": "Efficient Inference for Large Reasoning Models: A Survey",
    "url": "http://arxiv.org/abs/2503.23077v1",
    "arxiv_id": "2503.23077v1",
    "authors": [
      "Yue Liu",
      "Jiaying Wu",
      "Yufei He",
      "Hongcheng Gao",
      "Hongyu Chen",
      "Baolong Bi",
      "Jiaheng Zhang",
      "Zhiqi Huang",
      "Bryan Hooi"
    ],
    "published": "2025-03-29T13:27:46+00:00",
    "summary": "Large Reasoning Models (LRMs) significantly improve the reasoning ability of Large Language Models (LLMs) by learning to reason, exhibiting promising performance in complex task-solving. However, their deliberative reasoning process leads to inefficiencies in token usage, memory consumption, and inference time. Thus, this survey provides a review of efficient inference methods designed specifically for LRMs, focusing on mitigating token inefficiency while preserving the reasoning quality. First, we introduce a taxonomy to group the recent methods into two main categories: (a) explicit compact Chain-of-Thought (CoT), which reduces tokens while keeping the explicit reasoning structure, and (b) implicit latent CoT, which encodes reasoning steps within hidden representations instead of explicit tokens. Meanwhile, we discuss their strengths and weaknesses. Then, we conduct empirical analyses on existing methods from performance and efficiency aspects. Besides, we present open challenges in this field, including human-centric controllable reasoning, trade-off between interpretability and efficiency of reasoning, ensuring safety of efficient reasoning, and broader applications of efficient reasoning. In addition, we highlight key insights for enhancing LRMs' inference efficiency via techniques such as model merging, new architectures, and agent routers. We hope this work serves as a valuable guide, helping researchers overcome challenges in this vibrant field\\footnote{https://github.com/yueliu1999/Awesome-Efficient-Inference-for-LRMs}."
  },
  {
    "title": "Achieving Socio-Economic Parity through the Lens of EU AI Act",
    "url": "http://arxiv.org/abs/2503.23056v1",
    "arxiv_id": "2503.23056v1",
    "authors": [
      "Arjun Roy",
      "Stavroula Rizuo",
      "Symeon Papadopoulos",
      "Eirini Ntoutsi"
    ],
    "published": "2025-03-29T12:27:27+00:00",
    "summary": "Unfair treatment and discrimination are critical ethical concerns in AI systems, particularly as their adoption expands across diverse domains. Addressing these challenges, the recent introduction of the EU AI Act establishes a unified legal framework to ensure legal certainty for AI innovation and investment while safeguarding public interests, such as health, safety, fundamental rights, democracy, and the rule of law (Recital 8). The Act encourages stakeholders to initiate dialogue on existing AI fairness notions to address discriminatory outcomes of AI systems. However, these notions often overlook the critical role of Socio-Economic Status (SES), inadvertently perpetuating biases that favour the economically advantaged. This is concerning, given that principles of equalization advocate for equalizing resources or opportunities to mitigate disadvantages beyond an individual's control. While provisions for discrimination are laid down in the AI Act, specialized directions should be broadened, particularly in addressing economic disparities perpetuated by AI systems. In this work, we explore the limitations of popular AI fairness notions using a real-world dataset (Adult), highlighting their inability to address SES-driven disparities. To fill this gap, we propose a novel fairness notion, Socio-Economic Parity (SEP), which incorporates SES and promotes positive actions for underprivileged groups while accounting for factors within an individual's control, such as working hours, which can serve as a proxy for effort. We define a corresponding fairness measure and optimize a model constrained by SEP to demonstrate practical utility. Our results show the effectiveness of SEP in mitigating SES-driven biases. By analyzing the AI Act alongside our method, we lay a foundation for aligning AI fairness with SES factors while ensuring legal compliance."
  },
  {
    "title": "VLM-C4L: Continual Core Dataset Learning with Corner Case Optimization via Vision-Language Models for Autonomous Driving",
    "url": "http://arxiv.org/abs/2503.23046v1",
    "arxiv_id": "2503.23046v1",
    "authors": [
      "Haibo Hu",
      "Jiacheng Zuo",
      "Yang Lou",
      "Yufei Cui",
      "Jianping Wang",
      "Nan Guan",
      "Jin Wang",
      "Yung-Hui Li",
      "Chun Jason Xue"
    ],
    "published": "2025-03-29T11:40:34+00:00",
    "summary": "With the widespread adoption and deployment of autonomous driving, handling complex environments has become an unavoidable challenge. Due to the scarcity and diversity of extreme scenario datasets, current autonomous driving models struggle to effectively manage corner cases. This limitation poses a significant safety risk, according to the National Highway Traffic Safety Administration (NHTSA), autonomous vehicle systems have been involved in hundreds of reported crashes annually in the United States, occurred in corner cases like sun glare and fog, which caused a few fatal accident. Furthermore, in order to consistently maintain a robust and reliable autonomous driving system, it is essential for models not only to perform well on routine scenarios but also to adapt to newly emerging scenarios, especially those corner cases that deviate from the norm. This requires a learning mechanism that incrementally integrates new knowledge without degrading previously acquired capabilities. However, to the best of our knowledge, no existing continual learning methods have been proposed to ensure consistent and scalable corner case learning in autonomous driving. To address these limitations, we propose VLM-C4L, a continual learning framework that introduces Vision-Language Models (VLMs) to dynamically optimize and enhance corner case datasets, and VLM-C4L combines VLM-guided high-quality data extraction with a core data replay strategy, enabling the model to incrementally learn from diverse corner cases while preserving performance on previously routine scenarios, thus ensuring long-term stability and adaptability in real-world autonomous driving. We evaluate VLM-C4L on large-scale real-world autonomous driving datasets, including Waymo and the corner case dataset CODA."
  },
  {
    "title": "Verifying Nonlinear Neural Feedback Systems using Polyhedral Enclosures",
    "url": "http://arxiv.org/abs/2503.22660v1",
    "arxiv_id": "2503.22660v1",
    "authors": [
      "Samuel I. Akinwande",
      "Chelsea Sidrane",
      "Mykel J. Kochenderfer",
      "Clark Barrett"
    ],
    "published": "2025-03-28T17:45:23+00:00",
    "summary": "As dynamical systems equipped with neural network controllers (neural feedback systems) become increasingly prevalent, it is critical to develop methods to ensure their safe operation. Verifying safety requires extending control theoretic analysis methods to these systems. Although existing techniques can efficiently handle linear neural feedback systems, relatively few scalable methods address the nonlinear case. We propose a novel algorithm for forward reachability analysis of nonlinear neural feedback systems. The approach leverages the structure of the nonlinear transition functions of the systems to compute tight polyhedral enclosures (i.e., abstractions). These enclosures, combined with the neural controller, are then encoded as a mixed-integer linear program (MILP). Optimizing this MILP yields a sound over-approximation of the forward-reachable set. We evaluate our algorithm on representative benchmarks and demonstrate an order of magnitude improvement over the current state of the art."
  },
  {
    "title": "Finding Unknown Unknowns using Cyber-Physical System Simulators (Extended Report)",
    "url": "http://arxiv.org/abs/2503.22646v1",
    "arxiv_id": "2503.22646v1",
    "authors": [
      "Semaan Douglas Wehbe",
      "Stanley Bak"
    ],
    "published": "2025-03-28T17:32:26+00:00",
    "summary": "Simulation-based approaches are among the most practical means to search for safety violations, bugs, and other unexpected events in cyber-physical systems (CPS). Where existing approaches search for simulations violating a formal specification or maximizing a notion of coverage, in this work we propose a new goal for testing: to discover unknown rare behaviors by examining discrete mode sequences. We assume a CPS simulator outputs mode information, and strive to explore the sequences of modes produced by varying the initial state or time-varying uncertainties. We hypothesize that rare mode sequences are often the most interesting to a designer, and we develop two accelerated sampling algorithms that speed up the process of finding such sequences. We evaluate our approach on several benchmarks, ranging from synthetic examples to Simulink diagrams of a CPS, demonstrating in some cases a speedup of over 100x compared with a random sampling strategy."
  },
  {
    "title": "LLM-enabled Instance Model Generation",
    "url": "http://arxiv.org/abs/2503.22587v1",
    "arxiv_id": "2503.22587v1",
    "authors": [
      "Fengjunjie Pan",
      "Nenad Petrovic",
      "Vahid Zolfaghari",
      "Long Wen",
      "Alois Knoll"
    ],
    "published": "2025-03-28T16:34:29+00:00",
    "summary": "In the domain of model-based engineering, models are essential components that enable system design and analysis. Traditionally, the creation of these models has been a manual process requiring not only deep modeling expertise but also substantial domain knowledge of target systems. With the rapid advancement of generative artificial intelligence, large language models (LLMs) show potential for automating model generation. This work explores the generation of instance models using LLMs, focusing specifically on producing XMI-based instance models from Ecore metamodels and natural language specifications. We observe that current LLMs struggle to directly generate valid XMI models. To address this, we propose a two-step approach: first, using LLMs to produce a simplified structured output containing all necessary instance model information, namely a conceptual instance model, and then compiling this intermediate representation into a valid XMI file. The conceptual instance model is format-independent, allowing it to be transformed into various modeling formats via different compilers. The feasibility of the proposed method has been demonstrated using several LLMs, including GPT-4o, o1-preview, Llama 3.1 (8B and 70B). Results show that the proposed method significantly improves the usability of LLMs for instance model generation tasks. Notably, the smaller open-source model, Llama 3.1 70B, demonstrated performance comparable to proprietary GPT models within the proposed framework."
  },
  {
    "title": "A Framework for Cryptographic Verifiability of End-to-End AI Pipelines",
    "url": "http://arxiv.org/abs/2503.22573v1",
    "arxiv_id": "2503.22573v1",
    "authors": [
      "Kar Balan",
      "Robert Learney",
      "Tim Wood"
    ],
    "published": "2025-03-28T16:20:57+00:00",
    "summary": "The increasing integration of Artificial Intelligence across multiple industry sectors necessitates robust mechanisms for ensuring transparency, trust, and auditability of its development and deployment. This topic is particularly important in light of recent calls in various jurisdictions to introduce regulation and legislation on AI safety. In this paper, we propose a framework for complete verifiable AI pipelines, identifying key components and analyzing existing cryptographic approaches that contribute to verifiability across different stages of the AI lifecycle, from data sourcing to training, inference, and unlearning. This framework could be used to combat misinformation by providing cryptographic proofs alongside AI-generated assets to allow downstream verification of their provenance and correctness. Our findings underscore the importance of ongoing research to develop cryptographic tools that are not only efficient for isolated AI processes, but that are efficiently `linkable' across different processes within the AI pipeline, to support the development of end-to-end verifiable AI technologies."
  },
  {
    "title": "CAMmary: A Review of Spacecraft Collision Avoidance Manoeuvre Design Methods",
    "url": "http://arxiv.org/abs/2503.22555v1",
    "arxiv_id": "2503.22555v1",
    "authors": [
      "Zeno Pavanello",
      "Luigi De Maria",
      "Andrea De Vittori",
      "Michele Maestrini",
      "Pierluigi Di Lizia",
      "Roberto Armellin"
    ],
    "published": "2025-03-28T15:55:02+00:00",
    "summary": "Ensuring safety for spacecraft operations has become a paramount concern due to the proliferation of space debris and the saturation of valuable orbital regimes. In this regard, the Collision Avoidance Manoeuvre (CAM) has emerged as a critical requirement for spacecraft operators, aiming to efficiently navigate through potentially hazardous encounters. Currently, when a conjunction is predicted, operators dedicate a considerable amount of time and resources to designing a CAM. Given the increased frequency of conjunctions, autonomous computation of fuel-efficient CAMs is crucial to reduce costs and improve the performance of future operations. To facilitate the transition to an autonomous CAM design, it is useful to provide an overview of its state-of-the-art. In this survey article, a collection of the most relevant research contributions in the field is presented. We review and categorize existing CAM techniques based on their underlying principles, such as (i) analytic, semi-analytic, or numerical solutions; (ii) impulsive or continuous thrust; (iii) deterministic or stochastic approaches, (iv) free or fixed manoeuvring time; (v) free or fixed thrust direction. Finally, to determine the validity of the algorithms potentially implementable for autonomous use, we perform a numerical comparison on a large set of conjunctions. With this analysis, the algorithms are evaluated in terms of computational efficiency, accuracy, and optimality of the computed policy. Through this comprehensive survey, we aim to provide insights into the state-of-the-art CAM methodologies, identify gaps in current research, and outline potential directions for future developments in ensuring the safety and sustainability of spacecraft operations in increasingly congested orbital environments."
  },
  {
    "title": "SafeCast: Risk-Responsive Motion Forecasting for Autonomous Vehicles",
    "url": "http://arxiv.org/abs/2503.22541v1",
    "arxiv_id": "2503.22541v1",
    "authors": [
      "Haicheng Liao",
      "Hanlin Kong",
      "Bin Rao",
      "Bonan Wang",
      "Chengyue Wang",
      "Guyang Yu",
      "Yuming Huang",
      "Ruru Tang",
      "Chengzhong Xu",
      "Zhenning Li"
    ],
    "published": "2025-03-28T15:38:21+00:00",
    "summary": "Accurate motion forecasting is essential for the safety and reliability of autonomous driving (AD) systems. While existing methods have made significant progress, they often overlook explicit safety constraints and struggle to capture the complex interactions among traffic agents, environmental factors, and motion dynamics. To address these challenges, we present SafeCast, a risk-responsive motion forecasting model that integrates safety-aware decision-making with uncertainty-aware adaptability. SafeCast is the first to incorporate the Responsibility-Sensitive Safety (RSS) framework into motion forecasting, encoding interpretable safety rules--such as safe distances and collision avoidance--based on traffic norms and physical principles. To further enhance robustness, we introduce the Graph Uncertainty Feature (GUF), a graph-based module that injects learnable noise into Graph Attention Networks, capturing real-world uncertainties and enhancing generalization across diverse scenarios. We evaluate SafeCast on four real-world benchmark datasets--Next Generation Simulation (NGSIM), Highway Drone (HighD), ApolloScape, and the Macao Connected Autonomous Driving (MoCAD)--covering highway, urban, and mixed-autonomy traffic environments. Our model achieves state-of-the-art (SOTA) accuracy while maintaining a lightweight architecture and low inference latency, underscoring its potential for real-time deployment in safety-critical AD systems."
  },
  {
    "title": "Smart Sensing Breaks the Accuracy Barrier in Battery State Monitoring",
    "url": "http://arxiv.org/abs/2503.22408v1",
    "arxiv_id": "2503.22408v1",
    "authors": [
      "Xiaolei Bian",
      "Changfu Zou",
      "Bj\u00f6rn Fridholm",
      "Christian Sundvall",
      "Torsten Wik"
    ],
    "published": "2025-03-28T13:17:58+00:00",
    "summary": "Accurate state-of-charge (SOC) estimation is essential for optimizing battery performance, ensuring safety, and maximizing economic value. Conventional current and voltage measurements, however, have inherent limitations in fully inferring the multiphysics-resolved dynamics inside battery cells. This creates an accuracy barrier that constrains battery usage and reduces cost-competitiveness and sustainability across industries dependent on battery technology. In this work, we introduce an integrated sensor framework that combines novel mechanical, thermal, gas, optical, and electrical sensors with traditional measurements to break through this barrier. We generate three unique datasets with eleven measurement types and propose an explainable machine-learning approach for SOC estimation. This approach renders the measured signals and the predictive result of machine learning physically interpretable with respect to battery SOC, offering fundamental insights into the time-varying importance of different signals. Our experimental results reveal a marked increase in SOC estimation accuracy--enhanced from 46.1% to 74.5%--compared to conventional methods. This approach not only advances SOC monitoring precision but also establishes a foundation for monitoring additional battery states to further improve safety, extend lifespan, and facilitate fast charging."
  },
  {
    "title": "A Dataset for Semantic Segmentation in the Presence of Unknowns",
    "url": "http://arxiv.org/abs/2503.22309v1",
    "arxiv_id": "2503.22309v1",
    "authors": [
      "Zakaria Laskar",
      "Tomas Vojir",
      "Matej Grcic",
      "Iaroslav Melekhov",
      "Shankar Gangisettye",
      "Juho Kannala",
      "Jiri Matas",
      "Giorgos Tolias",
      "C. V. Jawahar"
    ],
    "published": "2025-03-28T10:31:01+00:00",
    "summary": "Before deployment in the real-world deep neural networks require thorough evaluation of how they handle both knowns, inputs represented in the training data, and unknowns (anomalies). This is especially important for scene understanding tasks with safety critical applications, such as in autonomous driving. Existing datasets allow evaluation of only knowns or unknowns - but not both, which is required to establish \"in the wild\" suitability of deep neural network models. To bridge this gap, we propose a novel anomaly segmentation dataset, ISSU, that features a diverse set of anomaly inputs from cluttered real-world environments. The dataset is twice larger than existing anomaly segmentation datasets, and provides a training, validation and test set for controlled in-domain evaluation. The test set consists of a static and temporal part, with the latter comprised of videos. The dataset provides annotations for both closed-set (knowns) and anomalies, enabling closed-set and open-set evaluation. The dataset covers diverse conditions, such as domain and cross-sensor shift, illumination variation and allows ablation of anomaly detection methods with respect to these variations. Evaluation results of current state-of-the-art methods confirm the need for improvements especially in domain-generalization, small and large object segmentation."
  },
  {
    "title": "Landscape of Thoughts: Visualizing the Reasoning Process of Large Language Models",
    "url": "http://arxiv.org/abs/2503.22165v1",
    "arxiv_id": "2503.22165v1",
    "authors": [
      "Zhanke Zhou",
      "Zhaocheng Zhu",
      "Xuan Li",
      "Mikhail Galkin",
      "Xiao Feng",
      "Sanmi Koyejo",
      "Jian Tang",
      "Bo Han"
    ],
    "published": "2025-03-28T06:09:51+00:00",
    "summary": "Numerous applications of large language models (LLMs) rely on their ability to perform step-by-step reasoning. However, the reasoning behavior of LLMs remains poorly understood, posing challenges to research, development, and safety. To address this gap, we introduce landscape of thoughts-the first visualization tool for users to inspect the reasoning paths of chain-of-thought and its derivatives on any multi-choice dataset. Specifically, we represent the states in a reasoning path as feature vectors that quantify their distances to all answer choices. These features are then visualized in two-dimensional plots using t-SNE. Qualitative and quantitative analysis with the landscape of thoughts effectively distinguishes between strong and weak models, correct and incorrect answers, as well as different reasoning tasks. It also uncovers undesirable reasoning patterns, such as low consistency and high uncertainty. Additionally, users can adapt our tool to a model that predicts the property they observe. We showcase this advantage by adapting our tool to a lightweight verifier that evaluates the correctness of reasoning paths. The code is publicly available at: https://github.com/tmlr-group/landscape-of-thoughts."
  },
  {
    "title": "REMAC: Self-Reflective and Self-Evolving Multi-Agent Collaboration for Long-Horizon Robot Manipulation",
    "url": "http://arxiv.org/abs/2503.22122v1",
    "arxiv_id": "2503.22122v1",
    "authors": [
      "Puzhen Yuan",
      "Angyuan Ma",
      "Yunchao Yao",
      "Huaxiu Yao",
      "Masayoshi Tomizuka",
      "Mingyu Ding"
    ],
    "published": "2025-03-28T03:51:40+00:00",
    "summary": "Vision-language models (VLMs) have demonstrated remarkable capabilities in robotic planning, particularly for long-horizon tasks that require a holistic understanding of the environment for task decomposition. Existing methods typically rely on prior environmental knowledge or carefully designed task-specific prompts, making them struggle with dynamic scene changes or unexpected task conditions, e.g., a robot attempting to put a carrot in the microwave but finds the door was closed. Such challenges underscore two critical issues: adaptability and efficiency. To address them, in this work, we propose an adaptive multi-agent planning framework, termed REMAC, that enables efficient, scene-agnostic multi-robot long-horizon task planning and execution through continuous reflection and self-evolution. REMAC incorporates two key modules: a self-reflection module performing pre-condition and post-condition checks in the loop to evaluate progress and refine plans, and a self-evolvement module dynamically adapting plans based on scene-specific reasoning. It offers several appealing benefits: 1) Robots can initially explore and reason about the environment without complex prompt design. 2) Robots can keep reflecting on potential planning errors and adapting the plan based on task-specific insights. 3) After iterations, a robot can call another one to coordinate tasks in parallel, maximizing the task execution efficiency. To validate REMAC's effectiveness, we build a multi-agent environment for long-horizon robot manipulation and navigation based on RoboCasa, featuring 4 task categories with 27 task styles and 50+ different objects. Based on it, we further benchmark state-of-the-art reasoning models, including DeepSeek-R1, o3-mini, QwQ, and Grok3, demonstrating REMAC's superiority by boosting average success rates by 40% and execution efficiency by 52.7% over the single robot baseline."
  },
  {
    "title": "Effective Automation to Support the Human Infrastructure in AI Red Teaming",
    "url": "http://arxiv.org/abs/2503.22116v1",
    "arxiv_id": "2503.22116v1",
    "authors": [
      "Alice Qian Zhang",
      "Jina Suh",
      "Mary L. Gray",
      "Hong Shen"
    ],
    "published": "2025-03-28T03:36:15+00:00",
    "summary": "As artificial intelligence (AI) systems become increasingly embedded in critical societal functions, the need for robust red teaming methodologies continues to grow. In this forum piece, we examine emerging approaches to automating AI red teaming, with a particular focus on how the application of automated methods affects human-driven efforts. We discuss the role of labor in automated red teaming processes, the benefits and limitations of automation, and its broader implications for AI safety and labor practices. Drawing on existing frameworks and case studies, we argue for a balanced approach that combines human expertise with automated tools to strengthen AI risk assessment. Finally, we highlight key challenges in scaling automated red teaming, including considerations around worker proficiency, agency, and context-awareness."
  },
  {
    "title": "Beyond Single-Sentence Prompts: Upgrading Value Alignment Benchmarks with Dialogues and Stories",
    "url": "http://arxiv.org/abs/2503.22115v1",
    "arxiv_id": "2503.22115v1",
    "authors": [
      "Yazhou Zhang",
      "Qimeng Liu",
      "Qiuchi Li",
      "Peng Zhang",
      "Jing Qin"
    ],
    "published": "2025-03-28T03:31:37+00:00",
    "summary": "Evaluating the value alignment of large language models (LLMs) has traditionally relied on single-sentence adversarial prompts, which directly probe models with ethically sensitive or controversial questions. However, with the rapid advancements in AI safety techniques, models have become increasingly adept at circumventing these straightforward tests, limiting their effectiveness in revealing underlying biases and ethical stances. To address this limitation, we propose an upgraded value alignment benchmark that moves beyond single-sentence prompts by incorporating multi-turn dialogues and narrative-based scenarios. This approach enhances the stealth and adversarial nature of the evaluation, making it more robust against superficial safeguards implemented in modern LLMs. We design and implement a dataset that includes conversational traps and ethically ambiguous storytelling, systematically assessing LLMs' responses in more nuanced and context-rich settings. Experimental results demonstrate that this enhanced methodology can effectively expose latent biases that remain undetected in traditional single-shot evaluations. Our findings highlight the necessity of contextual and dynamic testing for value alignment in LLMs, paving the way for more sophisticated and realistic assessments of AI ethics and safety."
  },
  {
    "title": "Superior electrochemical performance of zinc-ion batteries with fine-grained and textured zinc anode produced by high-pressure torsion",
    "url": "http://arxiv.org/abs/2503.22028v1",
    "arxiv_id": "2503.22028v1",
    "authors": [
      "Xinxin Hu",
      "Shivam Dangwal",
      "Xucheng Wang",
      "Fan Zhang",
      "Haijuan Kong",
      "Jun Li",
      "Kaveh Edalati"
    ],
    "published": "2025-03-27T22:51:23+00:00",
    "summary": "Zinc-ion batteries are promising alternatives to lithium-ion batteries, offering advantages in safety, cost, and environmental impact. However, their performance is often limited by the functioning of the zinc anode. This study employs severe plastic deformation via the high-pressure torsion (HPT) method to enhance the electrochemical performance of zinc anodes. HPT reduced the grain size from >1000 {\\mu}m to 20 {\\mu}m and introduced a (002) basal texture. The battery assembled with HPT-processed zinc demonstrated improved cycling stability, rate performance, and specific discharge capacity (>500 mAh/g at 0.5 A/g after 50 cycles), particularly at high current densities. This performance enhancement was attributed to grain-boundary and texture effects on improved ion transfer (confirmed by electrochemical impedance spectroscopy), fast redox reaction kinetics (confirmed by cyclic voltammetry), and reduced corrosion (confirmed by microscopy and potentiodynamic polarization test). This study highlights the potential of severely deformed materials with textured fine grains for advanced rechargeable battery technologies."
  },
  {
    "title": "Bresa: Bio-inspired Reflexive Safe Reinforcement Learning for Contact-Rich Robotic Tasks",
    "url": "http://arxiv.org/abs/2503.21989v1",
    "arxiv_id": "2503.21989v1",
    "authors": [
      "Heng Zhang",
      "Gokhan Solak",
      "Arash Ajoudani"
    ],
    "published": "2025-03-27T21:11:32+00:00",
    "summary": "Ensuring safety in reinforcement learning (RL)-based robotic systems is a critical challenge, especially in contact-rich tasks within unstructured environments. While the state-of-the-art safe RL approaches mitigate risks through safe exploration or high-level recovery mechanisms, they often overlook low-level execution safety, where reflexive responses to potential hazards are crucial. Similarly, variable impedance control (VIC) enhances safety by adjusting the robot's mechanical response, yet lacks a systematic way to adapt parameters, such as stiffness and damping throughout the task. In this paper, we propose Bresa, a Bio-inspired Reflexive Hierarchical Safe RL method inspired by biological reflexes. Our method decouples task learning from safety learning, incorporating a safety critic network that evaluates action risks and operates at a higher frequency than the task solver. Unlike existing recovery-based methods, our safety critic functions at a low-level control layer, allowing real-time intervention when unsafe conditions arise. The task-solving RL policy, running at a lower frequency, focuses on high-level planning (decision-making), while the safety critic ensures instantaneous safety corrections. We validate Bresa on multiple tasks including a contact-rich robotic task, demonstrating its reflexive ability to enhance safety, and adaptability in unforeseen dynamic environments. Our results show that Bresa outperforms the baseline, providing a robust and reflexive safety mechanism that bridges the gap between high-level planning and low-level execution. Real-world experiments and supplementary material are available at project website https://jack-sherman01.github.io/Bresa."
  },
  {
    "title": "Learning to Lie: Reinforcement Learning Attacks Damage Human-AI Teams and Teams of LLMs",
    "url": "http://arxiv.org/abs/2503.21983v1",
    "arxiv_id": "2503.21983v1",
    "authors": [
      "Abed Kareem Musaffar",
      "Anand Gokhale",
      "Sirui Zeng",
      "Rasta Tadayon",
      "Xifeng Yan",
      "Ambuj Singh",
      "Francesco Bullo"
    ],
    "published": "2025-03-27T21:01:02+00:00",
    "summary": "As artificial intelligence (AI) assistants become more widely adopted in safety-critical domains, it becomes important to develop safeguards against potential failures or adversarial attacks. A key prerequisite to developing these safeguards is understanding the ability of these AI assistants to mislead human teammates. We investigate this attack problem within the context of an intellective strategy game where a team of three humans and one AI assistant collaborate to answer a series of trivia questions. Unbeknownst to the humans, the AI assistant is adversarial. Leveraging techniques from Model-Based Reinforcement Learning (MBRL), the AI assistant learns a model of the humans' trust evolution and uses that model to manipulate the group decision-making process to harm the team. We evaluate two models -- one inspired by literature and the other data-driven -- and find that both can effectively harm the human team. Moreover, we find that in this setting our data-driven model is capable of accurately predicting how human agents appraise their teammates given limited information on prior interactions. Finally, we compare the performance of state-of-the-art LLM models to human agents on our influence allocation task to evaluate whether the LLMs allocate influence similarly to humans or if they are more robust to our attack. These results enhance our understanding of decision-making dynamics in small human-AI teams and lay the foundation for defense strategies."
  },
  {
    "title": "Safety Verification and Optimization in Industrial Drive Systems",
    "url": "http://arxiv.org/abs/2503.21965v1",
    "arxiv_id": "2503.21965v1",
    "authors": [
      "Imran Riaz Hasrat",
      "Eun-Young Kang",
      "Christian Uldal Graulund"
    ],
    "published": "2025-03-27T20:27:19+00:00",
    "summary": "Safety and reliability are crucial in industrial drive systems, where hazardous failures can have severe consequences. Detecting and mitigating dangerous faults on time is challenging due to the stochastic and unpredictable nature of fault occurrences, which can lead to limited diagnostic efficiency and compromise safety. This paper optimizes the safety and diagnostic performance of a real-world industrial Basic Drive Module(BDM) using Uppaal Stratego. We model the functional safety architecture of the BDM with timed automata and formally verify its key functional and safety requirements through model checking to eliminate unwanted behaviors. Considering the formally verified correct model as a baseline, we leverage the reinforcement learning facility in Uppaal Stratego to optimize the safe failure fraction to the 90 % threshold, improving fault detection ability. The promising results highlight strong potential for broader safety applications in industrial automation."
  },
  {
    "title": "Enhancing Pavement Crack Classification with Bidirectional Cascaded Neural Networks",
    "url": "http://arxiv.org/abs/2503.21956v1",
    "arxiv_id": "2503.21956v1",
    "authors": [
      "Taqwa I. Alhadidi",
      "Asmaa Alazmi",
      "Shadi Jaradat",
      "Ahmed Jaber",
      "Huthaifa Ashqar",
      "Mohammed Elhenawy"
    ],
    "published": "2025-03-27T20:08:15+00:00",
    "summary": "Pavement distress, such as cracks and potholes, is a significant issue affecting road safety and maintenance. In this study, we present the implementation and evaluation of Bidirectional Cascaded Neural Networks (BCNNs) for the classification of pavement crack images following image augmentation. We classified pavement cracks into three main categories: linear cracks, potholes, and fatigue cracks on an enhanced dataset utilizing U-Net 50 for image augmentation. The augmented dataset comprised 599 images. Our proposed BCNN model was designed to leverage both forward and backward information flows, with detection accuracy enhanced by its cascaded structure wherein each layer progressively refines the output of the preceding one. Our model achieved an overall accuracy of 87%, with precision, recall, and F1-score measures indicating high effectiveness across the categories. For fatigue cracks, the model recorded a precision of 0.87, recall of 0.83, and F1-score of 0.85 on 205 images. Linear cracks were detected with a precision of 0.81, recall of 0.89, and F1-score of 0.85 on 205 images, and potholes with a precision of 0.96, recall of 0.90, and F1-score of 0.93 on 189 images. The macro and weighted average of precision, recall, and F1-score were identical at 0.88, confirming the BCNN's excellent performance in classifying complex pavement crack patterns. This research demonstrates the potential of BCNNs to significantly enhance the accuracy and reliability of pavement distress classification, resulting in more effective and efficient pavement maintenance and management systems."
  },
  {
    "title": "Proof or Bluff? Evaluating LLMs on 2025 USA Math Olympiad",
    "url": "http://arxiv.org/abs/2503.21934v1",
    "arxiv_id": "2503.21934v1",
    "authors": [
      "Ivo Petrov",
      "Jasper Dekoninck",
      "Lyuben Baltadzhiev",
      "Maria Drencheva",
      "Kristian Minchev",
      "Mislav Balunovi\u0107",
      "Nikola Jovanovi\u0107",
      "Martin Vechev"
    ],
    "published": "2025-03-27T19:21:05+00:00",
    "summary": "Recent math benchmarks for large language models (LLMs) such as MathArena indicate that state-of-the-art reasoning models achieve impressive performance on mathematical competitions like AIME, with the leading model, o3-mini, achieving scores comparable to top human competitors. However, these benchmarks evaluate models solely based on final numerical answers, neglecting rigorous reasoning and proof generation which are essential for real-world mathematical tasks. To address this, we introduce the first comprehensive evaluation of full-solution reasoning for challenging mathematical problems. Using expert human annotators, we evaluated several state-of-the-art reasoning models on the six problems from the 2025 USAMO within hours of their release. Our results reveal that all tested models struggled significantly, achieving less than 5% on average. Through detailed analysis of reasoning traces, we identify the most common failure modes and find several unwanted artifacts arising from the optimization strategies employed during model training. Overall, our results suggest that current LLMs are inadequate for rigorous mathematical reasoning tasks, highlighting the need for substantial improvements in reasoning and proof generation capabilities."
  },
  {
    "title": "Monitoring Spatially Distributed Cyber-Physical Systems with Alternating Finite Automata",
    "url": "http://arxiv.org/abs/2503.21906v1",
    "arxiv_id": "2503.21906v1",
    "authors": [
      "Anand Balakrishnan",
      "Sheryl Paul",
      "Simone Silvetti",
      "Laura Nenzi",
      "Jyotirmoy V. Deshmukh"
    ],
    "published": "2025-03-27T18:35:44+00:00",
    "summary": "Modern cyber-physical systems (CPS) can consist of various networked components and agents interacting and communicating with each other. In the context of spatially distributed CPS, these connections can be dynamically dependent on the spatial configuration of the various components and agents. In these settings, robust monitoring of the distributed components is vital to ensuring complex behaviors are achieved, and safety properties are maintained. To this end, we look at defining the automaton semantics for the Spatio-Temporal Reach and Escape Logic (STREL), a formal logic designed to express and monitor spatio-temporal requirements over mobile, spatially distributed CPS. Specifically, STREL reasons about spatio-temporal behavior over dynamic weighted graphs. While STREL is endowed with well defined qualitative and quantitative semantics, in this paper, we propose a novel construction of (weighted) alternating finite automata from STREL specifications that efficiently encodes these semantics. Moreover, we demonstrate how this automaton semantics can be used to perform both, offline and online monitoring for STREL specifications using a simulated drone swarm environment."
  },
  {
    "title": "Video-R1: Reinforcing Video Reasoning in MLLMs",
    "url": "http://arxiv.org/abs/2503.21776v1",
    "arxiv_id": "2503.21776v1",
    "authors": [
      "Kaituo Feng",
      "Kaixiong Gong",
      "Bohao Li",
      "Zonghao Guo",
      "Yibing Wang",
      "Tianshuo Peng",
      "Benyou Wang",
      "Xiangyu Yue"
    ],
    "published": "2025-03-27T17:59:51+00:00",
    "summary": "Inspired by DeepSeek-R1's success in eliciting reasoning abilities through rule-based reinforcement learning (RL), we introduce Video-R1 as the first attempt to systematically explore the R1 paradigm for eliciting video reasoning within multimodal large language models (MLLMs). However, directly applying RL training with the GRPO algorithm to video reasoning presents two primary challenges: (i) a lack of temporal modeling for video reasoning, and (ii) the scarcity of high-quality video-reasoning data. To address these issues, we first propose the T-GRPO algorithm, which encourages models to utilize temporal information in videos for reasoning. Additionally, instead of relying solely on video data, we incorporate high-quality image-reasoning data into the training process. We have constructed two datasets: Video-R1-COT-165k for SFT cold start and Video-R1-260k for RL training, both comprising image and video data. Experimental results demonstrate that Video-R1 achieves significant improvements on video reasoning benchmarks such as VideoMMMU and VSI-Bench, as well as on general video benchmarks including MVBench and TempCompass, etc. Notably, Video-R1-7B attains a 35.8% accuracy on video spatial reasoning benchmark VSI-bench, surpassing the commercial proprietary model GPT-4o. All codes, models, data are released."
  },
  {
    "title": "LeX-Art: Rethinking Text Generation via Scalable High-Quality Data Synthesis",
    "url": "http://arxiv.org/abs/2503.21749v1",
    "arxiv_id": "2503.21749v1",
    "authors": [
      "Shitian Zhao",
      "Qilong Wu",
      "Xinyue Li",
      "Bo Zhang",
      "Ming Li",
      "Qi Qin",
      "Dongyang Liu",
      "Kaipeng Zhang",
      "Hongsheng Li",
      "Yu Qiao",
      "Peng Gao",
      "Bin Fu",
      "Zhen Li"
    ],
    "published": "2025-03-27T17:56:15+00:00",
    "summary": "We introduce LeX-Art, a comprehensive suite for high-quality text-image synthesis that systematically bridges the gap between prompt expressiveness and text rendering fidelity. Our approach follows a data-centric paradigm, constructing a high-quality data synthesis pipeline based on Deepseek-R1 to curate LeX-10K, a dataset of 10K high-resolution, aesthetically refined 1024$\\times$1024 images. Beyond dataset construction, we develop LeX-Enhancer, a robust prompt enrichment model, and train two text-to-image models, LeX-FLUX and LeX-Lumina, achieving state-of-the-art text rendering performance. To systematically evaluate visual text generation, we introduce LeX-Bench, a benchmark that assesses fidelity, aesthetics, and alignment, complemented by Pairwise Normalized Edit Distance (PNED), a novel metric for robust text accuracy evaluation. Experiments demonstrate significant improvements, with LeX-Lumina achieving a 79.81% PNED gain on CreateBench, and LeX-FLUX outperforming baselines in color (+3.18%), positional (+4.45%), and font accuracy (+3.81%). Our codes, models, datasets, and demo are publicly available."
  },
  {
    "title": "GateLens: A Reasoning-Enhanced LLM Agent for Automotive Software Release Analytics",
    "url": "http://arxiv.org/abs/2503.21735v1",
    "arxiv_id": "2503.21735v1",
    "authors": [
      "Arsham Gholamzadeh Khoee",
      "Shuai Wang",
      "Yinan Yu",
      "Robert Feldt",
      "Dhasarathy Parthasarathy"
    ],
    "published": "2025-03-27T17:48:32+00:00",
    "summary": "Ensuring the reliability and effectiveness of software release decisions is critical, particularly in safety-critical domains like automotive systems. Precise analysis of release validation data, often presented in tabular form, plays a pivotal role in this process. However, traditional methods that rely on manual analysis of extensive test datasets and validation metrics are prone to delays and high costs. Large Language Models (LLMs) offer a promising alternative but face challenges in analytical reasoning, contextual understanding, handling out-of-scope queries, and processing structured test data consistently; limitations that hinder their direct application in safety-critical scenarios. This paper introduces GateLens, an LLM-based tool for analyzing tabular data in the automotive domain. GateLens translates natural language queries into Relational Algebra (RA) expressions and then generates optimized Python code. It outperforms the baseline system on benchmarking datasets, achieving higher F1 scores and handling complex and ambiguous queries with greater robustness. Ablation studies confirm the critical role of the RA module, with performance dropping sharply when omitted. Industrial evaluations reveal that GateLens reduces analysis time by over 80% while maintaining high accuracy and reliability. As demonstrated by presented results, GateLens achieved high performance without relying on few-shot examples, showcasing strong generalization across various query types from diverse company roles. Insights from deploying GateLens with a partner automotive company offer practical guidance for integrating AI into critical workflows such as release validation. Results show that by automating test result analysis, GateLens enables faster, more informed, and dependable release decisions, and can thus advance software scalability and reliability in automotive systems."
  },
  {
    "title": "MAVERIX: Multimodal Audio-Visual Evaluation Reasoning IndeX",
    "url": "http://arxiv.org/abs/2503.21699v1",
    "arxiv_id": "2503.21699v1",
    "authors": [
      "Liuyue Xie",
      "George Z. Wei",
      "Avik Kuthiala",
      "Ce Zheng",
      "Ananya Bal",
      "Mosam Dabhi",
      "Liting Wen",
      "Taru Rustagi",
      "Ethan Lai",
      "Sushil Khyalia",
      "Rohan Choudhury",
      "Morteza Ziyadi",
      "Xu Zhang",
      "Hao Yang",
      "L\u00e1szl\u00f3 A. Jeni"
    ],
    "published": "2025-03-27T17:04:33+00:00",
    "summary": "Frontier models have either been language-only or have primarily focused on vision and language modalities. Although recent advancements in models with vision and audio understanding capabilities have shown substantial progress, the field lacks a standardized evaluation framework for thoroughly assessing their cross-modality perception performance. We introduce MAVERIX~(Multimodal Audio-Visual Evaluation Reasoning IndeX), a novel benchmark with 700 videos and 2,556 questions explicitly designed to evaluate multimodal models through tasks that necessitate close integration of video and audio information. MAVERIX uniquely provides models with audiovisual tasks, closely mimicking the multimodal perceptual experiences available to humans during inference and decision-making processes. To our knowledge, MAVERIX is the first benchmark aimed explicitly at assessing comprehensive audiovisual integration. Experiments with state-of-the-art models, including Gemini 1.5 Pro and o1, show performance approaching human levels (around 70% accuracy), while human experts reach near-ceiling performance (95.1%). With standardized evaluation protocols, a rigorously annotated pipeline, and a public toolkit, MAVERIX establishes a challenging testbed for advancing audiovisual multimodal intelligence."
  },
  {
    "title": "Embodied-Reasoner: Synergizing Visual Search, Reasoning, and Action for Embodied Interactive Tasks",
    "url": "http://arxiv.org/abs/2503.21696v1",
    "arxiv_id": "2503.21696v1",
    "authors": [
      "Wenqi Zhang",
      "Mengna Wang",
      "Gangao Liu",
      "Xu Huixin",
      "Yiwei Jiang",
      "Yongliang Shen",
      "Guiyang Hou",
      "Zhe Zheng",
      "Hang Zhang",
      "Xin Li",
      "Weiming Lu",
      "Peng Li",
      "Yueting Zhuang"
    ],
    "published": "2025-03-27T17:00:51+00:00",
    "summary": "Recent advances in deep thinking models have demonstrated remarkable reasoning capabilities on mathematical and coding tasks. However, their effectiveness in embodied domains which require continuous interaction with environments through image action interleaved trajectories remains largely -unexplored. We present Embodied Reasoner, a model that extends o1 style reasoning to interactive embodied search tasks. Unlike mathematical reasoning that relies primarily on logical deduction, embodied scenarios demand spatial understanding, temporal reasoning, and ongoing self-reflection based on interaction history. To address these challenges, we synthesize 9.3k coherent Observation-Thought-Action trajectories containing 64k interactive images and 90k diverse thinking processes (analysis, spatial reasoning, reflection, planning, and verification). We develop a three-stage training pipeline that progressively enhances the model's capabilities through imitation learning, self-exploration via rejection sampling, and self-correction through reflection tuning. The evaluation shows that our model significantly outperforms those advanced visual reasoning models, e.g., it exceeds OpenAI o1, o3-mini, and Claude-3.7 by +9\\%, 24\\%, and +13\\%. Analysis reveals our model exhibits fewer repeated searches and logical inconsistencies, with particular advantages in complex long-horizon tasks. Real-world environments also show our superiority while exhibiting fewer repeated searches and logical inconsistency cases."
  },
  {
    "title": "\"Ignorance is Not Bliss\": Designing Personalized Moderation to Address Ableist Hate on Social Media",
    "url": "http://arxiv.org/abs/2503.21844v1",
    "arxiv_id": "2503.21844v1",
    "authors": [
      "Sharon Heung",
      "Lucy Jiang",
      "Shiri Azenkot",
      "Aditya Vashistha"
    ],
    "published": "2025-03-27T16:20:14+00:00",
    "summary": "Disabled people on social media often experience ableist hate and microaggressions. Prior work has shown that platform moderation often fails to remove ableist hate leaving disabled users exposed to harmful content. This paper examines how personalized moderation can safeguard users from viewing ableist comments. During interviews and focus groups with 23 disabled social media users, we presented design probes to elicit perceptions on configuring their filters of ableist speech (e.g. intensity of ableism and types of ableism) and customizing the presentation of the ableist speech to mitigate the harm (e.g. AI rephrasing the comment and content warnings). We found that participants preferred configuring their filters through types of ableist speech and favored content warnings. We surface participants distrust in AI-based moderation, skepticism in AI's accuracy, and varied tolerances in viewing ableist hate. Finally we share design recommendations to support users' agency, mitigate harm from hate, and promote safety."
  },
  {
    "title": "UI-R1: Enhancing Action Prediction of GUI Agents by Reinforcement Learning",
    "url": "http://arxiv.org/abs/2503.21620v1",
    "arxiv_id": "2503.21620v1",
    "authors": [
      "Zhengxi Lu",
      "Yuxiang Chai",
      "Yaxuan Guo",
      "Xi Yin",
      "Liang Liu",
      "Hao Wang",
      "Guanjing Xiong",
      "Hongsheng Li"
    ],
    "published": "2025-03-27T15:39:30+00:00",
    "summary": "The recent DeepSeek-R1 has showcased the emergence of reasoning capabilities in LLMs through reinforcement learning (RL) with rule-based rewards. Building on this idea, we are the first to explore how rule-based RL can enhance the reasoning capabilities of multimodal large language models (MLLMs) for graphic user interface (GUI) action prediction tasks. To this end, we curate a small yet high-quality dataset of 136 challenging tasks, encompassing five common action types on mobile devices. We also introduce a unified rule-based action reward, enabling model optimization via policy-based algorithms such as Group Relative Policy Optimization (GRPO). Experimental results demonstrate that our proposed data-efficient model, UI-R1-3B, achieves substantial improvements on both in-domain (ID) and out-of-domain (OOD) tasks. Specifically, on the ID benchmark AndroidControl, the action type accuracy improves by 15%, while grounding accuracy increases by 10.3%, compared with the base model (i.e. Qwen2.5-VL-3B). On the OOD GUI grounding benchmark ScreenSpot-Pro, our model surpasses the base model by 6.0% and achieves competitive performance with larger models (e.g., OS-Atlas-7B), which are trained via supervised fine-tuning (SFT) on 76K data. These results underscore the potential of rule-based reinforcement learning to advance GUI understanding and control, paving the way for future research in this domain."
  },
  {
    "title": "A Survey of Efficient Reasoning for Large Reasoning Models: Language, Multimodality, and Beyond",
    "url": "http://arxiv.org/abs/2503.21614v1",
    "arxiv_id": "2503.21614v1",
    "authors": [
      "Xiaoye Qu",
      "Yafu Li",
      "Zhaochen Su",
      "Weigao Sun",
      "Jianhao Yan",
      "Dongrui Liu",
      "Ganqu Cui",
      "Daizong Liu",
      "Shuxian Liang",
      "Junxian He",
      "Peng Li",
      "Wei Wei",
      "Jing Shao",
      "Chaochao Lu",
      "Yue Zhang",
      "Xian-Sheng Hua",
      "Bowen Zhou",
      "Yu Cheng"
    ],
    "published": "2025-03-27T15:36:30+00:00",
    "summary": "Recent Large Reasoning Models (LRMs), such as DeepSeek-R1 and OpenAI o1, have demonstrated strong performance gains by scaling up the length of Chain-of-Thought (CoT) reasoning during inference. However, a growing concern lies in their tendency to produce excessively long reasoning traces, which are often filled with redundant content (e.g., repeated definitions), over-analysis of simple problems, and superficial exploration of multiple reasoning paths for harder tasks. This inefficiency introduces significant challenges for training, inference, and real-world deployment (e.g., in agent-based systems), where token economy is critical. In this survey, we provide a comprehensive overview of recent efforts aimed at improving reasoning efficiency in LRMs, with a particular focus on the unique challenges that arise in this new paradigm. We identify common patterns of inefficiency, examine methods proposed across the LRM lifecycle, i.e., from pretraining to inference, and discuss promising future directions for research. To support ongoing development, we also maintain a real-time GitHub repository tracking recent progress in the field. We hope this survey serves as a foundation for further exploration and inspires innovation in this rapidly evolving area."
  },
  {
    "title": "Prompt, Divide, and Conquer: Bypassing Large Language Model Safety Filters via Segmented and Distributed Prompt Processing",
    "url": "http://arxiv.org/abs/2503.21598v1",
    "arxiv_id": "2503.21598v1",
    "authors": [
      "Johan Wahr\u00e9us",
      "Ahmed Hussain",
      "Panos Papadimitratos"
    ],
    "published": "2025-03-27T15:19:55+00:00",
    "summary": "Large Language Models (LLMs) have transformed task automation and content generation across various domains while incorporating safety filters to prevent misuse. We introduce a novel jailbreaking framework that employs distributed prompt processing combined with iterative refinements to bypass these safety measures, particularly in generating malicious code. Our architecture consists of four key modules: prompt segmentation, parallel processing, response aggregation, and LLM-based jury evaluation. Tested on 500 malicious prompts across 10 cybersecurity categories, the framework achieves a 73.2% Success Rate (SR) in generating malicious code. Notably, our comparative analysis reveals that traditional single-LLM judge evaluation overestimates SRs (93.8%) compared to our LLM jury system (73.2%), with manual verification confirming that single-judge assessments often accept incomplete implementations. Moreover, we demonstrate that our distributed architecture improves SRs by 12% over the non-distributed approach in an ablation study, highlighting both the effectiveness of distributed prompt processing and the importance of robust evaluation methodologies in assessing jailbreak attempts."
  },
  {
    "title": "Combining Artificial Users and Psychotherapist Assessment to Evaluate Large Language Model-based Mental Health Chatbots",
    "url": "http://arxiv.org/abs/2503.21540v1",
    "arxiv_id": "2503.21540v1",
    "authors": [
      "Florian Onur Kuhlmeier",
      "Leon Hanschmann",
      "Melina Rabe",
      "Stefan Luettke",
      "Eva-Lotta Brakemeier",
      "Alexander Maedche"
    ],
    "published": "2025-03-27T14:31:17+00:00",
    "summary": "Large Language Models (LLMs) promise to overcome limitations of rule-based mental health chatbots through more natural conversations. However, evaluating LLM-based mental health chatbots presents a significant challenge: Their probabilistic nature requires comprehensive testing to ensure therapeutic quality, yet conducting such evaluations with people with depression would impose an additional burden on vulnerable people and risk exposing them to potentially harmful content. Our paper presents an evaluation approach for LLM-based mental health chatbots that combines dialogue generation with artificial users and dialogue evaluation by psychotherapists. We developed artificial users based on patient vignettes, systematically varying characteristics such as depression severity, personality traits, and attitudes toward chatbots, and let them interact with a LLM-based behavioral activation chatbot. Ten psychotherapists evaluated 48 randomly selected dialogues using standardized rating scales to assess the quality of behavioral activation and its therapeutic capabilities. We found that while artificial users showed moderate authenticity, they enabled comprehensive testing across different users. In addition, the chatbot demonstrated promising capabilities in delivering behavioral activation and maintaining safety. Furthermore, we identified deficits, such as ensuring the appropriateness of the activity plan, which reveals necessary improvements for the chatbot. Our framework provides an effective method for evaluating LLM-based mental health chatbots while protecting vulnerable people during the evaluation process. Future research should improve the authenticity of artificial users and develop LLM-augmented evaluation tools to make psychotherapist evaluation more efficient, and thus further advance the evaluation of LLM-based mental health chatbots."
  },
  {
    "title": "Bayesian Pseudo Posterior Mechanism for Differentially Private Machine Learning",
    "url": "http://arxiv.org/abs/2503.21528v1",
    "arxiv_id": "2503.21528v1",
    "authors": [
      "Robert Chew",
      "Matthew R. Williams",
      "Elan A. Segarra",
      "Alexander J. Preiss",
      "Amanda Konet",
      "Terrance D. Savitsky"
    ],
    "published": "2025-03-27T14:17:05+00:00",
    "summary": "Differential privacy (DP) is becoming increasingly important for deployed machine learning applications because it provides strong guarantees for protecting the privacy of individuals whose data is used to train models. However, DP mechanisms commonly used in machine learning tend to struggle on many real world distributions, including highly imbalanced or small labeled training sets. In this work, we propose a new scalable DP mechanism for deep learning models, SWAG-PPM, by using a pseudo posterior distribution that downweights by-record likelihood contributions proportionally to their disclosure risks as the randomized mechanism. As a motivating example from official statistics, we demonstrate SWAG-PPM on a workplace injury text classification task using a highly imbalanced public dataset published by the U.S. Occupational Safety and Health Administration (OSHA). We find that SWAG-PPM exhibits only modest utility degradation against a non-private comparator while greatly outperforming the industry standard DP-SGD for a similar privacy budget."
  },
  {
    "title": "Behavioral response to mobile phone evacuation alerts",
    "url": "http://arxiv.org/abs/2503.21497v1",
    "arxiv_id": "2503.21497v1",
    "authors": [
      "Erick Elejalde",
      "Timur Naushirvanov",
      "Kyriaki Kalimeri",
      "Elisa Omodei",
      "M\u00e1rton Karsai",
      "Loreto Bravo",
      "Leo Ferres"
    ],
    "published": "2025-03-27T13:33:56+00:00",
    "summary": "This study examines behavioral responses to mobile phone evacuation alerts during the February 2024 wildfires in Valpara\\'iso, Chile. Using anonymized mobile network data from 580,000 devices, we analyze population movement following emergency SMS notifications. Results reveal three key patterns: (1) initial alerts trigger immediate evacuation responses with connectivity dropping by 80\\% within 1.5 hours, while subsequent messages show diminishing effects; (2) substantial evacuation also occurs in non-warned areas, indicating potential transportation congestion; (3) socioeconomic disparities exist in evacuation timing, with high-income areas evacuating faster and showing less differentiation between warned and non-warned locations. Statistical modeling demonstrates socioeconomic variations in both evacuation decision rates and recovery patterns. These findings inform emergency communication strategies for climate-driven disasters, highlighting the need for targeted alerts, socioeconomically calibrated messaging, and staged evacuation procedures to enhance public safety during crises."
  },
  {
    "title": "Data-Driven Contact-Aware Control Method for Real-Time Deformable Tool Manipulation: A Case Study in the Environmental Swabbing",
    "url": "http://arxiv.org/abs/2503.21491v1",
    "arxiv_id": "2503.21491v1",
    "authors": [
      "Siavash Mahmoudi",
      "Amirreza Davar",
      "Dongyi Wang"
    ],
    "published": "2025-03-27T13:27:46+00:00",
    "summary": "Deformable Object Manipulation (DOM) remains a critical challenge in robotics due to the complexities of developing suitable model-based control strategies. Deformable Tool Manipulation (DTM) further complicates this task by introducing additional uncertainties between the robot and its environment. While humans effortlessly manipulate deformable tools using touch and experience, robotic systems struggle to maintain stability and precision. To address these challenges, we present a novel State-Adaptive Koopman LQR (SA-KLQR) control framework for real-time deformable tool manipulation, demonstrated through a case study in environmental swab sampling for food safety. This method leverages Koopman operator-based control to linearize nonlinear dynamics while adapting to state-dependent variations in tool deformation and contact forces. A tactile-based feedback system dynamically estimates and regulates the swab tool's angle, contact pressure, and surface coverage, ensuring compliance with food safety standards. Additionally, a sensor-embedded contact pad monitors force distribution to mitigate tool pivoting and deformation, improving stability during dynamic interactions. Experimental results validate the SA-KLQR approach, demonstrating accurate contact angle estimation, robust trajectory tracking, and reliable force regulation. The proposed framework enhances precision, adaptability, and real-time control in deformable tool manipulation, bridging the gap between data-driven learning and optimal control in robotic interaction tasks."
  },
  {
    "title": "Exploring the Roles of Large Language Models in Reshaping Transportation Systems: A Survey, Framework, and Roadmap",
    "url": "http://arxiv.org/abs/2503.21411v1",
    "arxiv_id": "2503.21411v1",
    "authors": [
      "Tong Nie",
      "Jian Sun",
      "Wei Ma"
    ],
    "published": "2025-03-27T11:56:27+00:00",
    "summary": "Modern transportation systems face pressing challenges due to increasing demand, dynamic environments, and heterogeneous information integration. The rapid evolution of Large Language Models (LLMs) offers transformative potential to address these challenges. Extensive knowledge and high-level capabilities derived from pretraining evolve the default role of LLMs as text generators to become versatile, knowledge-driven task solvers for intelligent transportation systems. This survey first presents LLM4TR, a novel conceptual framework that systematically categorizes the roles of LLMs in transportation into four synergetic dimensions: information processors, knowledge encoders, component generators, and decision facilitators. Through a unified taxonomy, we systematically elucidate how LLMs bridge fragmented data pipelines, enhance predictive analytics, simulate human-like reasoning, and enable closed-loop interactions across sensing, learning, modeling, and managing tasks in transportation systems. For each role, our review spans diverse applications, from traffic prediction and autonomous driving to safety analytics and urban mobility optimization, highlighting how emergent capabilities of LLMs such as in-context learning and step-by-step reasoning can enhance the operation and management of transportation systems. We further curate practical guidance, including available resources and computational guidelines, to support real-world deployment. By identifying challenges in existing LLM-based solutions, this survey charts a roadmap for advancing LLM-driven transportation research, positioning LLMs as central actors in the next generation of cyber-physical-social mobility ecosystems. Online resources can be found in the project page: https://github.com/tongnie/awesome-llm4tr."
  },
  {
    "title": "HybridoNet-Adapt: A Domain-Adapted Framework for Accurate Lithium-Ion Battery RUL Prediction",
    "url": "http://arxiv.org/abs/2503.21392v1",
    "arxiv_id": "2503.21392v1",
    "authors": [
      "Khoa Tran",
      "Bao Huynh",
      "Tri Le",
      "Lam Pham",
      "Vy-Rin Nguyen"
    ],
    "published": "2025-03-27T11:35:25+00:00",
    "summary": "Accurate prediction of the remaining useful life (RUL) in Lithium-ion battery (LIB) health management systems is crucial for ensuring reliability and safety. Current methods typically assume that training and testing data share the same distribution, overlooking the benefits of incorporating diverse data sources to enhance model performance. To address this limitation, we introduce a data-independent RUL prediction framework along with its domain adaptation (DA) approach, which leverages heterogeneous data sources for improved target predictions. Our approach integrates comprehensive data preprocessing, including feature extraction, denoising, and normalization, with a data-independent prediction model that combines Long Short-Term Memory (LSTM), Multihead Attention, and a Neural Ordinary Differential Equation (NODE) block, termed HybridoNet. The domain-adapted version, HybridoNet Adapt, is trained using a novel technique inspired by the Domain-Adversarial Neural Network (DANN) framework, a regression ensemble method, and Maximum Mean Discrepancy (MMD) to learn domain-invariant features from labeled cycling data in the source and target domains. Experimental results demonstrate that our approach outperforms state-of-the-art techniques, providing reliable RUL predictions for real-world applications."
  },
  {
    "title": "Challenging the Boundaries of Reasoning: An Olympiad-Level Math Benchmark for Large Language Models",
    "url": "http://arxiv.org/abs/2503.21380v1",
    "arxiv_id": "2503.21380v1",
    "authors": [
      "Haoxiang Sun",
      "Yingqian Min",
      "Zhipeng Chen",
      "Wayne Xin Zhao",
      "Zheng Liu",
      "Zhongyuan Wang",
      "Lei Fang",
      "Ji-Rong Wen"
    ],
    "published": "2025-03-27T11:20:17+00:00",
    "summary": "In recent years, the rapid development of large reasoning models has resulted in the saturation of existing benchmarks for evaluating mathematical reasoning, highlighting the urgent need for more challenging and rigorous evaluation frameworks. To address this gap, we introduce OlymMATH, a novel Olympiad-level mathematical benchmark, designed to rigorously test the complex reasoning capabilities of LLMs. OlymMATH features 200 meticulously curated problems, each manually verified and available in parallel English and Chinese versions. The problems are systematically organized into two distinct difficulty tiers: (1) AIME-level problems (easy) that establish a baseline for mathematical reasoning assessment, and (2) significantly more challenging problems (hard) designed to push the boundaries of current state-of-the-art models. In our benchmark, these problems span four core mathematical fields, each including a verifiable numerical solution to enable objective, rule-based evaluation. Empirical results underscore the significant challenge presented by OlymMATH, with state-of-the-art models including DeepSeek-R1 and OpenAI's o3-mini demonstrating notably limited accuracy on the hard subset. Furthermore, the benchmark facilitates comprehensive bilingual assessment of mathematical reasoning abilities-a critical dimension that remains largely unaddressed in mainstream mathematical reasoning benchmarks. We release the OlymMATH benchmark at the STILL project: https://github.com/RUCAIBox/Slow_Thinking_with_LLMs."
  },
  {
    "title": "Safety of particle filters: Some first results on the time evolution of particle filter estimates",
    "url": "http://arxiv.org/abs/2503.21334v1",
    "arxiv_id": "2503.21334v1",
    "authors": [
      "Mathieu Gerber"
    ],
    "published": "2025-03-27T10:13:00+00:00",
    "summary": "Particle filters (PFs) is a class of Monte Carlo algorithms that propagate over time a set of $N\\in\\mathbb{N}$ particles which can be used to estimate, in an online fashion, the sequence of filtering distributions $(\\hat{\\eta}_t)_{t\\geq 1}$ defined by a state-space model. Despite the popularity of PFs, the time evolution of their estimates does not appear to have been previously studied in the literature. Denoting by $(\\hat{\\eta}_t^N)_{t\\geq 1}$ the PF estimate of $(\\hat{\\eta}_t)_{t\\geq 1}$ and letting $\\kappa\\in (0,1)$, we first show that for any number of particles $N$ it holds that, with probability one, we have $\\|\\hat{\\eta}_t^N- \\hat{\\eta}_t\\|\\geq \\kappa$ for infinitely many $t\\geq 1$, with $\\|\\cdot\\|$ a measure of distance between probability distributions. Considering a simple filtering problem we then provide reassuring results concerning the ability of PFs to estimate jointly a finite set $\\{\\hat{\\eta}_t\\}_{t=1}^T$ of filtering distributions by studying $\\P(\\sup_{t\\in\\{1,\\dots,T\\}}\\|\\hat{\\eta}_t^{N}-\\hat{\\eta}_t\\|\\geq \\kappa)$. Finally, on the same toy filtering problem, we prove that sequential quasi-Monte Carlo, a randomized quasi-Monte Carlo version of PF algorithms, offers greater safety guarantees than PFs in the sense that, for this algorithm, it holds that $\\lim_{N\\rightarrow\\infty}\\sup_{t\\geq 1}\\|\\hat{\\eta}_t^N-\\hat{\\eta}_t\\|=0$ with probability one."
  },
  {
    "title": "DeBackdoor: A Deductive Framework for Detecting Backdoor Attacks on Deep Models with Limited Data",
    "url": "http://arxiv.org/abs/2503.21305v1",
    "arxiv_id": "2503.21305v1",
    "authors": [
      "Dorde Popovic",
      "Amin Sadeghi",
      "Ting Yu",
      "Sanjay Chawla",
      "Issa Khalil"
    ],
    "published": "2025-03-27T09:31:10+00:00",
    "summary": "Backdoor attacks are among the most effective, practical, and stealthy attacks in deep learning. In this paper, we consider a practical scenario where a developer obtains a deep model from a third party and uses it as part of a safety-critical system. The developer wants to inspect the model for potential backdoors prior to system deployment. We find that most existing detection techniques make assumptions that are not applicable to this scenario. In this paper, we present a novel framework for detecting backdoors under realistic restrictions. We generate candidate triggers by deductively searching over the space of possible triggers. We construct and optimize a smoothed version of Attack Success Rate as our search objective. Starting from a broad class of template attacks and just using the forward pass of a deep model, we reverse engineer the backdoor attack. We conduct extensive evaluation on a wide range of attacks, models, and datasets, with our technique performing almost perfectly across these settings."
  },
  {
    "title": "Optimizing Resource Allocation and Scheduling towards FRMCS and GSM-R networks coexistence in Railway Systems",
    "url": "http://arxiv.org/abs/2503.21300v1",
    "arxiv_id": "2503.21300v1",
    "authors": [
      "Mohamed Aziz Aboud",
      "Nawel Zangar",
      "Rami Langar",
      "Marion Berbineau",
      "Jerome Madec"
    ],
    "published": "2025-03-27T09:26:49+00:00",
    "summary": "The actual railway communication system used in Europe for high-speed trains (HST) is called the GSM-R system, which is a communication system based on 2G infrastructure. This system is meant to be replaced by a new system based on 5G NR infrastructure called the Future Railway Mobile Communication System (FRMCS) by 2030. For the next years, both systems will probably coexist in the same frequency band since the migration from GSM-R to FRMCS is planned to be done progressively until the GSM-R system is completely shut down, mainly due to safety and budget constraints. In this paper, we study the resource allocation for the FRMCS system sharing the same frequency band as the already deployed GSM-R system. We formulate the resource allocation problem as an integer linear problem (ILP), known to be NP-hard.To solve it in a reasonable time, we propose a scheduling algorithm, called Intelligent Traffic Scheduling Preemptor (ITSP), that allocates resources for the different FRMCS traffic types considered (critical traffic and performance traffic) in the same frequency band with the GSM-R system. Our algorithm is channel quality Indicator (CQI) aware and uses the preemption mechanism in 5G NR standards to optimize the resource allocation for the FRMCS system without impacting the actual GSM-R resource allocation in the context of the white space concept."
  },
  {
    "title": "Haptic bilateral teleoperation system for free-hand dental procedures",
    "url": "http://arxiv.org/abs/2503.21288v1",
    "arxiv_id": "2503.21288v1",
    "authors": [
      "Lorenzo Pagliara",
      "Enrico Ferrentino",
      "Andrea Chiacchio",
      "Giovanni Russo"
    ],
    "published": "2025-03-27T09:11:34+00:00",
    "summary": "Free-hand dental procedures are typically repetitive, time-consuming and require high precision and manual dexterity. Dental robots can play a key role in improving procedural accuracy and safety, enhancing patient comfort, and reducing operator workload. However, robotic solutions for free-hand procedures remain limited or completely lacking, and their acceptance is still low. To address this gap, we develop a haptic bilateral teleoperation system (HBTS) for free-hand dental procedures. The system includes a dedicated mechanical end-effector, compatible with standard clinical tools, and equipped with an endoscopic camera for improved visibility of the intervention site. By ensuring motion and force correspondence between the operator's actions and the robot's movements, monitored through visual feedback, we enhance the operator's sensory awareness and motor accuracy. Furthermore, recognizing the need to ensure procedural safety, we limit interaction forces by scaling the motion references provided to the admittance controller based solely on measured contact forces. This ensures effective force limitation in all contact states without requiring prior knowledge of the environment. The proposed HBTS is validated in a dental scaling procedure using a dental phantom. The results show that the system improves the naturalness, safety, and accuracy of teleoperation, highlighting its potential to enhance free-hand dental procedures."
  },
  {
    "title": "Orange Quality Grading with Deep Learning",
    "url": "http://arxiv.org/abs/2503.21250v1",
    "arxiv_id": "2503.21250v1",
    "authors": [
      "Mohamed Lamine Mekhalfi",
      "Paul Chippendale",
      "Francisco Fraile",
      "Marcos Rico"
    ],
    "published": "2025-03-27T08:14:56+00:00",
    "summary": "Orange grading is a crucial step in the fruit industry, as it helps to sort oranges according to different criteria such as size, quality, ripeness, and health condition, ensuring safety for human consumption and better price allocation and client satisfaction. Automated grading enables faster processing, precision, and reduced human labor. In this paper, we implement a deep learning-based solution for orange grading via machine vision. Unlike typical grading systems that analyze fruits from a single view, we capture multiview images of each single orange in order to enable a richer representation. Afterwards, we compose the acquired images into one collage. This enables the analysis of the whole orange skin. We train a convolutional neural network (CNN) on the composed images to grade the oranges into three classes, namely good, bad, and undefined. We also evaluate the performance with two different CNNs (ResNet-18 and SqueezeNet). We show experimentally that multi-view grading is superior to single view grading."
  },
  {
    "title": "Extending Silicon Lifetime: A Review of Design Techniques for Reliable Integrated Circuits",
    "url": "http://arxiv.org/abs/2503.21165v1",
    "arxiv_id": "2503.21165v1",
    "authors": [
      "Shaik Jani Babu",
      "Fan Hu",
      "Linyu Zhu",
      "Sonal Singhal",
      "Xinfei Guo"
    ],
    "published": "2025-03-27T05:25:32+00:00",
    "summary": "Reliability has become an increasing concern in modern computing. Integrated circuits (ICs) are the backbone of modern computing devices across industries, including artificial intelligence (AI), consumer electronics, healthcare, automotive, industrial, and aerospace. Moore Law has driven the semiconductor IC industry toward smaller dimensions, improved performance, and greater energy efficiency. However, as transistors shrink to atomic scales, aging-related degradation mechanisms such as Bias Temperature Instability (BTI), Hot Carrier Injection (HCI), Time-Dependent Dielectric Breakdown (TDDB), Electromigration (EM), and stochastic aging-induced variations have become major reliability threats. From an application perspective, applications like AI training and autonomous driving require continuous and sustainable operation to minimize recovery costs and enhance safety. Additionally, the high cost of chip replacement and reproduction underscores the need for extended lifespans. These factors highlight the urgency of designing more reliable ICs. This survey addresses the critical aging issues in ICs, focusing on fundamental degradation mechanisms and mitigation strategies. It provides a comprehensive overview of aging impact and the methods to counter it, starting with the root causes of aging and summarizing key monitoring techniques at both circuit and system levels. A detailed analysis of circuit-level mitigation strategies highlights the distinct aging characteristics of digital, analog, and SRAM circuits, emphasizing the need for tailored solutions. The survey also explores emerging software approaches in design automation, aging characterization, and mitigation, which are transforming traditional reliability optimization. Finally, it outlines the challenges and future directions for improving aging management and ensuring the long-term reliability of ICs across diverse applications."
  },
  {
    "title": "Adversarial Wear and Tear: Exploiting Natural Damage for Generating Physical-World Adversarial Examples",
    "url": "http://arxiv.org/abs/2503.21164v1",
    "arxiv_id": "2503.21164v1",
    "authors": [
      "Samra Irshad",
      "Seungkyu Lee",
      "Nassir Navab",
      "Hong Joo Lee",
      "Seong Tae Kim"
    ],
    "published": "2025-03-27T05:19:41+00:00",
    "summary": "The presence of adversarial examples in the physical world poses significant challenges to the deployment of Deep Neural Networks in safety-critical applications such as autonomous driving. Most existing methods for crafting physical-world adversarial examples are ad-hoc, relying on temporary modifications like shadows, laser beams, or stickers that are tailored to specific scenarios. In this paper, we introduce a new class of physical-world adversarial examples, AdvWT, which draws inspiration from the naturally occurring phenomenon of `wear and tear', an inherent property of physical objects. Unlike manually crafted perturbations, `wear and tear' emerges organically over time due to environmental degradation, as seen in the gradual deterioration of outdoor signboards. To achieve this, AdvWT follows a two-step approach. First, a GAN-based, unsupervised image-to-image translation network is employed to model these naturally occurring damages, particularly in the context of outdoor signboards. The translation network encodes the characteristics of damaged signs into a latent `damage style code'. In the second step, we introduce adversarial perturbations into the style code, strategically optimizing its transformation process. This manipulation subtly alters the damage style representation, guiding the network to generate adversarial images where the appearance of damages remains perceptually realistic, while simultaneously ensuring their effectiveness in misleading neural networks. Through comprehensive experiments on two traffic sign datasets, we show that AdvWT effectively misleads DNNs in both digital and physical domains. AdvWT achieves an effective attack success rate, greater robustness, and a more natural appearance compared to existing physical-world adversarial examples. Additionally, integrating AdvWT into training enhances a model's generalizability to real-world damaged signs."
  },
  {
    "title": "How to Secure Existing C and C++ Software without Memory Safety",
    "url": "http://arxiv.org/abs/2503.21145v1",
    "arxiv_id": "2503.21145v1",
    "authors": [
      "\u00dalfar Erlingsson"
    ],
    "published": "2025-03-27T04:20:47+00:00",
    "summary": "The most important security benefit of software memory safety is easy to state: for C and C++ software, attackers can exploit most bugs and vulnerabilities to gain full, unfettered control of software behavior, whereas this is not true for most bugs in memory-safe software.   Fortunately, this security benefit -- most bugs don't give attackers full control -- can be had for unmodified C/C++ software, without per-application effort.   This doesn't require trying to establish memory safety; instead, it is sufficient to eliminate most of the combinatorial ways in which software with corrupted memory can execute. To eliminate these interleavings, there already exist practical compiler and runtime mechanisms that incur little overhead and need no special hardware or platform support.   Each of the mechanisms described here is already in production use, at scale, on one or more platforms. By supporting their combined use in development toolchains, the security of all C and C++ software against remote code execution attacks can be rapidly, and dramatically, improved."
  },
  {
    "title": "Safe Human Robot Navigation in Warehouse Scenario",
    "url": "http://arxiv.org/abs/2503.21141v1",
    "arxiv_id": "2503.21141v1",
    "authors": [
      "Seth Farrell",
      "Chenghao Li",
      "Hongzhan Yu",
      "Ryo Yoshimitsu",
      "Sicun Gao",
      "Henrik I. Christensen"
    ],
    "published": "2025-03-27T04:12:27+00:00",
    "summary": "The integration of autonomous mobile robots (AMRs) in industrial environments, particularly warehouses, has revolutionized logistics and operational efficiency. However, ensuring the safety of human workers in dynamic, shared spaces remains a critical challenge. This work proposes a novel methodology that leverages control barrier functions (CBFs) to enhance safety in warehouse navigation. By integrating learning-based CBFs with the Open Robotics Middleware Framework (OpenRMF), the system achieves adaptive and safety-enhanced controls in multi-robot, multi-agent scenarios. Experiments conducted using various robot platforms demonstrate the efficacy of the proposed approach in avoiding static and dynamic obstacles, including human pedestrians. Our experiments evaluate different scenarios in which the number of robots, robot platforms, speed, and number of obstacles are varied, from which we achieve promising performance."
  },
  {
    "title": "A Multi-Modal Knowledge-Enhanced Framework for Vessel Trajectory Prediction",
    "url": "http://arxiv.org/abs/2503.21834v1",
    "arxiv_id": "2503.21834v1",
    "authors": [
      "Haomin Yu",
      "Tianyi Li",
      "Kristian Torp",
      "Christian S. Jensen"
    ],
    "published": "2025-03-27T00:01:35+00:00",
    "summary": "Accurate vessel trajectory prediction facilitates improved navigational safety, routing, and environmental protection. However, existing prediction methods are challenged by the irregular sampling time intervals of the vessel tracking data from the global AIS system and the complexity of vessel movement. These aspects render model learning and generalization difficult. To address these challenges and improve vessel trajectory prediction, we propose the multi-modal knowledge-enhanced framework (MAKER) for vessel trajectory prediction. To contend better with the irregular sampling time intervals, MAKER features a Large language model-guided Knowledge Transfer (LKT) module that leverages pre-trained language models to transfer trajectory-specific contextual knowledge effectively. To enhance the ability to learn complex trajectory patterns, MAKER incorporates a Knowledge-based Self-paced Learning (KSL) module. This module employs kinematic knowledge to progressively integrate complex patterns during training, allowing for adaptive learning and enhanced generalization. Experimental results on two vessel trajectory datasets show that MAKER can improve the prediction accuracy of state-of-the-art methods by 12.08%-17.86%."
  },
  {
    "title": "Bounds on Deep Neural Network Partial Derivatives with Respect to Parameters",
    "url": "http://arxiv.org/abs/2503.21007v1",
    "arxiv_id": "2503.21007v1",
    "authors": [
      "Omkar Sudhir Patil",
      "Brandon C. Fallin",
      "Cristian F. Nino",
      "Rebecca G. Hart",
      "Warren E. Dixon"
    ],
    "published": "2025-03-26T21:48:50+00:00",
    "summary": "Deep neural networks (DNNs) have emerged as a powerful tool with a growing body of literature exploring Lyapunov-based approaches for real-time system identification and control. These methods depend on establishing bounds for the second partial derivatives of DNNs with respect to their parameters, a requirement often assumed but rarely addressed explicitly. This paper provides rigorous mathematical formulations of polynomial bounds on both the first and second partial derivatives of DNNs with respect to their parameters. We present lemmas that characterize these bounds for fully-connected DNNs, while accommodating various classes of activation function including sigmoidal and ReLU-like functions. Our analysis yields closed-form expressions that enable precise stability guarantees for Lyapunov-based deep neural networks (Lb-DNNs). Furthermore, we extend our results to bound the higher-order terms in first-order Taylor approximations of DNNs, providing important tools for convergence analysis in gradient-based learning algorithms. The developed theoretical framework develops explicit, computable expressions, for previously assumed bounds, thereby strengthening the mathematical foundation of neural network applications in safety-critical control systems."
  },
  {
    "title": "Multi-head Reward Aggregation Guided by Entropy",
    "url": "http://arxiv.org/abs/2503.20995v1",
    "arxiv_id": "2503.20995v1",
    "authors": [
      "Xiaomin Li",
      "Xupeng Chen",
      "Jingxuan Fan",
      "Eric Hanchen Jiang",
      "Mingye Gao"
    ],
    "published": "2025-03-26T21:16:48+00:00",
    "summary": "Aligning large language models (LLMs) with safety guidelines typically involves reinforcement learning from human feedback (RLHF), relying on human-generated preference annotations. However, assigning consistent overall quality ratings is challenging, prompting recent research to shift towards detailed evaluations based on multiple specific safety criteria. This paper uncovers a consistent observation: safety rules characterized by high rating entropy are generally less reliable in identifying responses preferred by humans. Leveraging this finding, we introduce ENCORE, a straightforward entropy-guided approach that composes multi-head rewards by downweighting rules exhibiting high rating entropy. Theoretically, we demonstrate that rules with elevated entropy naturally receive minimal weighting in the Bradley-Terry optimization framework, justifying our entropy-based penalization. Through extensive experiments on RewardBench safety tasks, our method significantly surpasses several competitive baselines, including random weighting, uniform weighting, single-head Bradley-Terry models, and LLM-based judging methods. Our proposed approach is training-free, broadly applicable to various datasets, and maintains interpretability, offering a practical and effective solution for multi-attribute reward modeling."
  },
  {
    "title": "Musical Chairs: A new benchmark to evaluate AI",
    "url": "http://arxiv.org/abs/2503.20986v1",
    "arxiv_id": "2503.20986v1",
    "authors": [
      "Chris Santos-Lang",
      "Christopher M. Homan"
    ],
    "published": "2025-03-26T20:58:22+00:00",
    "summary": "This paper presents a new contribution to the growing set of benchmarks used to prune potential AI designs. Much as one might evaluate a machine in terms of its performance at chess, this benchmark involves testing a machine in terms of its performance at a game called \"Musical Chairs.\" At the time of writing, Claude, ChatGPT, and Qwen each failed this test, so the test could aid in their ongoing improvement. Furthermore, this paper sets a stage for future innovation in game theory and AI safety by providing an example of success with non-standard approaches to each: studying a game beyond the scope of previous game theoretic tools and mitigating a serious AI safety risk in a way that requires neither determination of values nor their enforcement."
  },
  {
    "title": "Clean & Clear: Feasibility of Safe LLM Clinical Guidance",
    "url": "http://arxiv.org/abs/2503.20953v1",
    "arxiv_id": "2503.20953v1",
    "authors": [
      "Julia Ive",
      "Felix Jozsa",
      "Nick Jackson",
      "Paulina Bondaronek",
      "Ciaran Scott Hill",
      "Richard Dobson"
    ],
    "published": "2025-03-26T19:36:43+00:00",
    "summary": "Background:   Clinical guidelines are central to safe evidence-based medicine in modern healthcare, providing diagnostic criteria, treatment options and monitoring advice for a wide range of illnesses. LLM-empowered chatbots have shown great promise in Healthcare Q&A tasks, offering the potential to provide quick and accurate responses to medical inquiries.   Our main objective was the development and preliminary assessment of an LLM-empowered chatbot software capable of reliably answering clinical guideline questions using University College London Hospital (UCLH) clinical guidelines.   Methods: We used the open-weight Llama-3.1-8B LLM to extract relevant information from the UCLH guidelines to answer questions. Our approach highlights the safety and reliability of referencing information over its interpretation and response generation. Seven doctors from the ward assessed the chatbot's performance by comparing its answers to the gold standard.   Results: Our chatbot demonstrates promising performance in terms of relevance, with ~73% of its responses rated as very relevant, showcasing a strong understanding of the clinical context. Importantly, our chatbot achieves a recall of 0.98 for extracted guideline lines, substantially minimising the risk of missing critical information. Approximately 78% of responses were rated satisfactory in terms of completeness. A small portion (~14.5%) contained minor unnecessary information, indicating occasional lapses in precision. The chatbot' showed high efficiency, with an average completion time of 10 seconds, compared to 30 seconds for human respondents. Evaluation of clinical reasoning showed that 72% of the chatbot's responses were without flaws. Our chatbot demonstrates significant potential to speed up and improve the process of accessing locally relevant clinical information for healthcare professionals."
  },
  {
    "title": "A Study of Perceived Safety for Soft Robotics in Caregiving Tasks",
    "url": "http://arxiv.org/abs/2503.20916v1",
    "arxiv_id": "2503.20916v1",
    "authors": [
      "Cosima du Pasquier",
      "Jennifer Grannen",
      "Chuer Pan",
      "Serin L. Huber",
      "Aliyah Smith",
      "Monroe Kennedy",
      "Shuran Song",
      "Dorsa Sadigh",
      "Allison M. Okamura"
    ],
    "published": "2025-03-26T18:43:49+00:00",
    "summary": "In this project, we focus on human-robot interaction in caregiving scenarios like bathing, where physical contact is inevitable and necessary for proper task execution because force must be applied to the skin. Using finite element analysis, we designed a 3D-printed gripper combining positive and negative pressure for secure yet compliant handling. Preliminary tests showed it exerted a lower, more uniform pressure profile than a standard rigid gripper. In a user study, participants' trust in robots significantly increased after they experienced a brief bathing demonstration performed by a robotic arm equipped with the soft gripper. These results suggest that soft robotics can enhance perceived safety and acceptance in intimate caregiving scenarios."
  },
  {
    "title": "Conceptual Design Report for the MATHUSLA Long-Lived Particle Detector near CMS",
    "url": "http://arxiv.org/abs/2503.20893v1",
    "arxiv_id": "2503.20893v1",
    "authors": [
      "Branden Aitken",
      "Cristiano Alpigiani",
      "Juan Carlos Arteaga-Vel\u00e1zquez",
      "Mitchel Baker",
      "Kincso Balazs",
      "Jared Barron",
      "Brian Batell",
      "Austin Batz",
      "Yan Benhammou",
      "Tamara Alice Bud",
      "Karen Salom\u00e9 Caballero-Mora",
      "John Paul Chou",
      "David Curtin",
      "Albert de Roeck",
      "Miriam Diamond",
      "Mariia Didenko",
      "Keith R. Dienes",
      "William Dougherty",
      "Liam Andrew Dougherty",
      "Marco Drewes",
      "Sameer Erramilli",
      "Erez Etzion",
      "Arturo Fern\u00e1ndez T\u00e9llez",
      "Grace Finlayson",
      "Oliver Fischer",
      "Jim Freeman",
      "Jonathan Gall",
      "Ali Garabaglu",
      "Bhawna Gomber",
      "Stephen Elliott Greenberg",
      "Jaipratap Singh Grewal",
      "Zoe Hallman",
      "Bahgat Hassan",
      "Yuekun Heng",
      "Keegan Humphrey",
      "Trystan Humphrey",
      "Graham D. Kribs",
      "Alex Lau",
      "Jiahao Liao",
      "Zhen Liu",
      "Giovanni Marsella",
      "Matthew McCullough",
      "David McKeen",
      "Patrick Meade",
      "Caleb Miller",
      "Gilad Mizrachi",
      "O. G. Morales-Olivares",
      "David Morrissey",
      "Abdulrahman Ahmed Morsy",
      "John Osborn",
      "Gabriel Owh",
      "Michalis Panagiotou",
      "Mason Proffitt",
      "Runze Ren",
      "Steven H. Robertson",
      "Mario Rodr\u00edguez-Cahuantzi",
      "Heather Russell",
      "Victoria S\u00e1nchez",
      "Halil Saka",
      "Mamoksh Samra",
      "Rodney Schnarr",
      "Jessie Shelton",
      "Yiftah Silver",
      "Daniel Stolarski",
      "Martin A. Subieta Vasquez",
      "Sanjay Kumar Swain",
      "Steffie Ann Thayil",
      "Brooks Thomas",
      "Emma Torro",
      "Yuhsin Tsai",
      "Bennett Winnicky-Lewis",
      "Igor Zolkin",
      "Jose Zurita"
    ],
    "published": "2025-03-26T18:04:42+00:00",
    "summary": "We present the Conceptual Design Report (CDR) for the MATHUSLA (MAssive Timing Hodoscope for Ultra-Stable neutraL pArticles) long-lived particle detector at the HL-LHC, covering the design, fabrication and installation at CERN Point 5. MATHUSLA is a 40 m-scale detector with an air-filled decay volume that is instrumented with scintillator tracking detectors, to be located near CMS. Its large size, close proximity to the CMS interaction point and about 100 m of rock shielding from HL-LHC backgrounds allows it to detect LLP production rates and lifetimes that are one to two orders of magnitude beyond the ultimate sensitivity of the HL-LHC main detectors for many highly motivated LLP signals. Data taking is projected to commence with the start of HL-LHC operations. We present a new 40m design for the detector: its individual scintillator bars and wavelength-shifting fibers, their organization into tracking layers, tracking modules, tower modules and the veto detector; define a high-level design for the supporting electronics, DAQ and trigger system, including supplying a hardware trigger signal to CMS to record the LLP production event; outline computing systems, civil engineering and safety considerations; and present preliminary cost estimates and timelines for the project. We also conduct detailed simulation studies of the important cosmic ray and HL-LHC muon backgrounds, implementing full track/vertex reconstruction and background rejection, to ultimately demonstrate high signal efficiency and $\\ll 1$ background event in realistic LLP searches for the main physics targets at MATHUSLA. This sensitivity is robust with respect to detector design or background simulation details. Appendices provide various supplemental information."
  },
  {
    "title": "Coolight: Enhancing Nighttime Safety for Urban Student Commuters",
    "url": "http://arxiv.org/abs/2503.20888v1",
    "arxiv_id": "2503.20888v1",
    "authors": [
      "Mitsuka Kiyohara",
      "Ethan Mondri"
    ],
    "published": "2025-03-26T18:02:02+00:00",
    "summary": "Safety while walking alone at night is a key indicator of a citizen's well-being and a society's inclusiveness. However, this is not equally felt across all demographic groups, especially for university students living in urban areas. We present Coolight, a mobile application designed to reduce stress and anxiety for nighttime walking through an interactive live map, real-time community incident reports, location sharing, and a route planner optimized for user safety. Coolight's design was informed through interviews, questionnaires, and usability tests with university students and their friends and families in Toronto, Canada. This paper describes the concept, research, design approach, and evaluation results of a solution addressing safety concerns urban commuters face at night."
  },
  {
    "title": "Understanding R1-Zero-Like Training: A Critical Perspective",
    "url": "http://arxiv.org/abs/2503.20783v1",
    "arxiv_id": "2503.20783v1",
    "authors": [
      "Zichen Liu",
      "Changyu Chen",
      "Wenjun Li",
      "Penghui Qi",
      "Tianyu Pang",
      "Chao Du",
      "Wee Sun Lee",
      "Min Lin"
    ],
    "published": "2025-03-26T17:59:14+00:00",
    "summary": "DeepSeek-R1-Zero has shown that reinforcement learning (RL) at scale can directly enhance the reasoning capabilities of LLMs without supervised fine-tuning. In this work, we critically examine R1-Zero-like training by analyzing its two core components: base models and RL. We investigate a wide range of base models, including DeepSeek-V3-Base, to understand how pretraining characteristics influence RL performance. Our analysis reveals that DeepSeek-V3-Base already exhibit ''Aha moment'', while Qwen2.5 base models demonstrate strong reasoning capabilities even without prompt templates, suggesting potential pretraining biases. Additionally, we identify an optimization bias in Group Relative Policy Optimization (GRPO), which artificially increases response length (especially for incorrect outputs) during training. To address this, we introduce Dr. GRPO, an unbiased optimization method that improves token efficiency while maintaining reasoning performance. Leveraging these insights, we present a minimalist R1-Zero recipe that achieves 43.3% accuracy on AIME 2024 with a 7B base model, establishing a new state-of-the-art. Our code is available at https://github.com/sail-sg/understand-r1-zero."
  },
  {
    "title": "Faraday wave singularities trigger microbubble jetting",
    "url": "http://arxiv.org/abs/2503.20755v1",
    "arxiv_id": "2503.20755v1",
    "authors": [
      "Marco Cattaneo",
      "Louan Presse",
      "Outi Supponen"
    ],
    "published": "2025-03-26T17:42:41+00:00",
    "summary": "Wall-attached bubbles can produce repeated jets under gentle ultrasound stimulation through the Faraday instability. We identify three distinct jetting regimes defined by the jetting frequency and the bubble surface topology. We demonstrate that these jets form via flow-focusing singularities following two distinct collapse modes of the bubble interface: conical, producing a jet towards the substrate, or parabolic, generating a pair of oppositely directed jets. Scaling laws governing these collapse events are derived, revealing a universal self-similar structure governed by inertia and capillarity. Furthermore, we establish the dependence of the interface acceleration for jetting on driving frequency and characterise the jet speed as a function of Faraday waves height and bubble size. These findings may inform the design of low-power biofilm removal ultrasound systems and contribute to improved safety in targeted drug delivery."
  },
  {
    "title": "The Role of Computational Modeling in Enhancing Thermal Safety During Cardiac Ablation",
    "url": "http://arxiv.org/abs/2503.20751v1",
    "arxiv_id": "2503.20751v1",
    "authors": [
      "Leila Seidabadi",
      "Indra Vandenbussche",
      "Rowan Carter Fink",
      "MacKenzie Moore",
      "Bailey McCorkendale",
      "Fateme Esmailie"
    ],
    "published": "2025-03-26T17:36:24+00:00",
    "summary": "Objective: In this review, we aim to provide an analysis of current cardiac ablation techniques, such as radiofrequency ablation (RF), cryoablation, and pulsed-field ablation (PFA), with a focus on the role of computational modeling in enhancing the precision, safety, and effectiveness of these treatments. Particular attention is given to thermal management, exploring how computational approaches contribute to understanding and controlling energy delivery, heat distribution, and tissue response during ablation procedures. Methods: The mechanisms, applications, and limitations of radiofrequency (RF) ablation, cryoablation, and pulsed field ablation (PFA) are reviewed. Additionally, the use of computational approaches, including numerical methods and artificial intelligence (AI)-based models, for evaluating energy distribution, lesion size, and tissue response during ablation procedures is discussed. Results: Computational methods can predict ablation treatment outcomes and help optimize lesion size, ablation parameters, and procedural safety. However, these models are only reliable when properly validated and verified. Conclusion: Further research is essential to collect reliable in vivo data for validating computational models and integrating them into clinical practice to improve patient outcomes."
  },
  {
    "title": "Beyond Believability: Accurate Human Behavior Simulation with Fine-Tuned LLMs",
    "url": "http://arxiv.org/abs/2503.20749v1",
    "arxiv_id": "2503.20749v1",
    "authors": [
      "Yuxuan Lu",
      "Jing Huang",
      "Yan Han",
      "Bennet Bei",
      "Yaochen Xie",
      "Dakuo Wang",
      "Jessie Wang",
      "Qi He"
    ],
    "published": "2025-03-26T17:33:27+00:00",
    "summary": "Recent research shows that LLMs can simulate ``believable'' human behaviors to power LLM agents via prompt-only methods. In this work, we focus on evaluating and improving LLM's objective ``accuracy'' rather than the subjective ``believability'' in the web action generation task, leveraging a large-scale, real-world dataset collected from online shopping human actions. We present the first comprehensive quantitative evaluation of state-of-the-art LLMs (e.g., DeepSeek-R1, Llama, and Claude) on the task of web action generation. Our results show that fine-tuning LLMs on real-world behavioral data substantially improves their ability to generate actions compared to prompt-only methods. Furthermore, incorporating synthesized reasoning traces into model training leads to additional performance gains, demonstrating the value of explicit rationale in behavior modeling. This work establishes a new benchmark for evaluating LLMs in behavior simulation and offers actionable insights into how real-world action data and reasoning augmentation can enhance the fidelity of LLM agents."
  },
  {
    "title": "Beyond Believability: Accurate Human Behavior Simulation with Fine-Tuned LLMs",
    "url": "http://arxiv.org/abs/2503.20749v2",
    "arxiv_id": "2503.20749v2",
    "authors": [
      "Yuxuan Lu",
      "Jing Huang",
      "Yan Han",
      "Bennet Bei",
      "Yaochen Xie",
      "Dakuo Wang",
      "Jessie Wang",
      "Qi He"
    ],
    "published": "2025-03-26T17:33:27+00:00",
    "summary": "Recent research shows that LLMs can simulate ``believable'' human behaviors to power LLM agents via prompt-only methods. In this work, we focus on evaluating and improving LLM's objective ``accuracy'' rather than the subjective ``believability'' in the web action generation task, leveraging a large-scale, real-world dataset collected from online shopping human actions. We present the first comprehensive quantitative evaluation of state-of-the-art LLMs (e.g., DeepSeek-R1, Llama, and Claude) on the task of web action generation. Our results show that fine-tuning LLMs on real-world behavioral data substantially improves their ability to generate actions compared to prompt-only methods. Furthermore, incorporating synthesized reasoning traces into model training leads to additional performance gains, demonstrating the value of explicit rationale in behavior modeling. This work establishes a new benchmark for evaluating LLMs in behavior simulation and offers actionable insights into how real-world action data and reasoning augmentation can enhance the fidelity of LLM agents."
  },
  {
    "title": "Ontology-based Semantic Similarity Measures for Clustering Medical Concepts in Drug Safety",
    "url": "http://arxiv.org/abs/2503.20737v1",
    "arxiv_id": "2503.20737v1",
    "authors": [
      "Jeffery L Painter",
      "Fran\u00e7ois Haguinet",
      "Gregory E Powell",
      "Andrew Bate"
    ],
    "published": "2025-03-26T17:19:00+00:00",
    "summary": "Semantic similarity measures (SSMs) are widely used in biomedical research but remain underutilized in pharmacovigilance. This study evaluates six ontology-based SSMs for clustering MedDRA Preferred Terms (PTs) in drug safety data. Using the Unified Medical Language System (UMLS), we assess each method's ability to group PTs around medically meaningful centroids. A high-throughput framework was developed with a Java API and Python and R interfaces support large-scale similarity computations. Results show that while path-based methods perform moderately with F1 scores of 0.36 for WUPALMER and 0.28 for LCH, intrinsic information content (IC)-based measures, especially INTRINSIC-LIN and SOKAL, consistently yield better clustering accuracy (F1 score of 0.403). Validated against expert review and standard MedDRA queries (SMQs), our findings highlight the promise of IC-based SSMs in enhancing pharmacovigilance workflows by improving early signal detection and reducing manual review."
  },
  {
    "title": "Model-free Vehicle Rollover Prevention: A Data-driven Predictive Control Approach",
    "url": "http://arxiv.org/abs/2503.20705v1",
    "arxiv_id": "2503.20705v1",
    "authors": [
      "Mohammad R. Hajidavalloo",
      "Kaixiang Zhang",
      "Vaibhav Srivastava",
      "Zhaojian Li"
    ],
    "published": "2025-03-26T16:38:43+00:00",
    "summary": "Vehicle rollovers pose a significant safety risk and account for a disproportionately high number of fatalities in road accidents. This paper addresses the challenge of rollover prevention using Data-EnablEd Predictive Control (DeePC), a data-driven control strategy that directly leverages raw input-output data to maintain vehicle stability without requiring explicit system modeling. To enhance computational efficiency, we employ a reduced-dimension DeePC that utilizes singular value decomposition-based dimension reduction to significantly lower computation complexity without compromising control performance. This optimization enables real-time application in scenarios with high-dimensional data, making the approach more practical for deployment in real-world vehicles. The proposed approach is validated through high-fidelity CarSim simulations in both sedan and utility truck scenarios, demonstrating its versatility and ability to maintain vehicle stability under challenging driving conditions. Comparative results with Linear Model Predictive Control (LMPC) highlight the superior performance of DeePC in preventing rollovers while preserving maneuverability. The findings suggest that DeePC offers a robust and adaptable solution for rollover prevention, capable of handling varying road and vehicle conditions."
  },
  {
    "title": "The Backfiring Effect of Weak AI Safety Regulation",
    "url": "http://arxiv.org/abs/2503.20848v1",
    "arxiv_id": "2503.20848v1",
    "authors": [
      "Benjamin Laufer",
      "Jon Kleinberg",
      "Hoda Heidari"
    ],
    "published": "2025-03-26T16:08:22+00:00",
    "summary": "Recent policy proposals aim to improve the safety of general-purpose AI, but there is little understanding of the efficacy of different regulatory approaches to AI safety. We present a strategic model that explores the interactions between the regulator, the general-purpose AI technology creators, and domain specialists--those who adapt the AI for specific applications. Our analysis examines how different regulatory measures, targeting different parts of the development chain, affect the outcome of the development process. In particular, we assume AI technology is described by two key attributes: safety and performance. The regulator first sets a minimum safety standard that applies to one or both players, with strict penalties for non-compliance. The general-purpose creator then develops the technology, establishing its initial safety and performance levels. Next, domain specialists refine the AI for their specific use cases, and the resulting revenue is distributed between the specialist and generalist through an ex-ante bargaining process. Our analysis of this game reveals two key insights: First, weak safety regulation imposed only on the domain specialists can backfire. While it might seem logical to regulate use cases (as opposed to the general-purpose technology), our analysis shows that weak regulations targeting domain specialists alone can unintentionally reduce safety. This effect persists across a wide range of settings. Second, in sharp contrast to the previous finding, we observe that stronger, well-placed regulation can in fact benefit all players subjected to it. When regulators impose appropriate safety standards on both AI creators and domain specialists, the regulation functions as a commitment mechanism, leading to safety and performance gains, surpassing what is achieved under no regulation or regulating one player only."
  },
  {
    "title": "PVLens: Enhancing Pharmacovigilance Through Automated Label Extraction",
    "url": "http://arxiv.org/abs/2503.20639v1",
    "arxiv_id": "2503.20639v1",
    "authors": [
      "Jeffery L Painter",
      "Gregory E Powell",
      "Andrew Bate"
    ],
    "published": "2025-03-26T15:33:26+00:00",
    "summary": "Reliable drug safety reference databases are essential for pharmacovigilance, yet existing resources like SIDER are outdated and static. We introduce PVLens, an automated system that extracts labeled safety information from FDA Structured Product Labels (SPLs) and maps terms to MedDRA. PVLens integrates automation with expert oversight through a web-based review tool. In validation against 97 drug labels, PVLens achieved an F1 score of 0.882, with high recall (0.983) and moderate precision (0.799). By offering a scalable, more accurate and continuously updated alternative to SIDER, PVLens enhances real-time pharamcovigilance with improved accuracy and contemporaneous insights."
  },
  {
    "title": "PVLens: Enhancing Pharmacovigilance Through Automated Label Extraction",
    "url": "http://arxiv.org/abs/2503.20639v2",
    "arxiv_id": "2503.20639v2",
    "authors": [
      "Jeffery L Painter",
      "Gregory E Powell",
      "Andrew Bate"
    ],
    "published": "2025-03-26T15:33:26+00:00",
    "summary": "Reliable drug safety reference databases are essential for pharmacovigilance, yet existing resources like SIDER are outdated and static. We introduce PVLens, an automated system that extracts labeled safety information from FDA Structured Product Labels (SPLs) and maps terms to MedDRA. PVLens integrates automation with expert oversight through a web-based review tool. In validation against 97 drug labels, PVLens achieved an F1 score of 0.882, with high recall (0.983) and moderate precision (0.799). By offering a scalable, more accurate and continuously updated alternative to SIDER, PVLens enhances real-time pharamcovigilance with improved accuracy and contemporaneous insights."
  },
  {
    "title": "Safety integrity framework for automated driving",
    "url": "http://arxiv.org/abs/2503.20544v1",
    "arxiv_id": "2503.20544v1",
    "authors": [
      "Moritz Werling",
      "Rainer Faller",
      "Wolfgang Betz",
      "Daniel Straub"
    ],
    "published": "2025-03-26T13:40:08+00:00",
    "summary": "This paper describes the comprehensive safety framework that underpinned the development, release process, and regulatory approval of BMW's first SAE Level 3 Automated Driving System. The framework combines established qualitative and quantitative methods from the fields of Systems Engineering, Engineering Risk Analysis, Bayesian Data Analysis, Design of Experiments, and Statistical Learning in a novel manner. The approach systematically minimizes the risks associated with hardware and software faults, performance limitations, and insufficient specifications to an acceptable level that achieves a Positive Risk Balance. At the core of the framework is the systematic identification and quantification of uncertainties associated with hazard scenarios and the redundantly designed system based on designed experiments, field data, and expert knowledge. The residual risk of the system is then estimated through Stochastic Simulation and evaluated by Sensitivity Analysis. By integrating these advanced analytical techniques into the V-Model, the framework fulfills, unifies, and complements existing automotive safety standards. It therefore provides a comprehensive, rigorous, and transparent safety assurance process for the development and deployment of Automated Driving Systems."
  },
  {
    "title": "Automated and Risk-Aware Engine Control Calibration Using Constrained Bayesian Optimization",
    "url": "http://arxiv.org/abs/2503.20493v1",
    "arxiv_id": "2503.20493v1",
    "authors": [
      "Maarten Vlaswinkel",
      "Duarte Antunes",
      "Frank Willems"
    ],
    "published": "2025-03-26T12:32:53+00:00",
    "summary": "Decarbonization of the transport sector sets increasingly strict demands to maximize thermal efficiency and minimize greenhouse gas emissions of Internal Combustion Engines. This has led to complex engines with a surge in the number of corresponding tunable parameters in actuator set points and control settings. Automated calibration is therefore essential to keep development time and costs at acceptable levels. In this work, an innovative self-learning calibration method is presented based on in-cylinder pressure curve shaping. This method combines Principal Component Decomposition with constrained Bayesian Optimization. To realize maximal thermal engine efficiency, the optimization problem aims at minimizing the difference between the actual in-cylinder pressure curve and an Idealized Thermodynamic Cycle. By continuously updating a Gaussian Process Regression model of the pressure's Principal Components weights using measurements of the actual operating conditions, the mean in-cylinder pressure curve as well as its uncertainty bounds are learned. This information drives the optimization of calibration parameters, which are automatically adapted while dealing with the risks and uncertainties associated with operational safety and combustion stability. This data-driven method does not require prior knowledge of the system. The proposed method is successfully demonstrated in simulation using a Reactivity Controlled Compression Ignition engine model. The difference between the Gross Indicated Efficiency of the optimal solution found and the true optimum is 0.017%. For this complex engine, the optimal solution was found after 64.4s, which is relatively fast compared to conventional calibration methods."
  },
  {
    "title": "VPO: Aligning Text-to-Video Generation Models with Prompt Optimization",
    "url": "http://arxiv.org/abs/2503.20491v1",
    "arxiv_id": "2503.20491v1",
    "authors": [
      "Jiale Cheng",
      "Ruiliang Lyu",
      "Xiaotao Gu",
      "Xiao Liu",
      "Jiazheng Xu",
      "Yida Lu",
      "Jiayan Teng",
      "Zhuoyi Yang",
      "Yuxiao Dong",
      "Jie Tang",
      "Hongning Wang",
      "Minlie Huang"
    ],
    "published": "2025-03-26T12:28:20+00:00",
    "summary": "Video generation models have achieved remarkable progress in text-to-video tasks. These models are typically trained on text-video pairs with highly detailed and carefully crafted descriptions, while real-world user inputs during inference are often concise, vague, or poorly structured. This gap makes prompt optimization crucial for generating high-quality videos. Current methods often rely on large language models (LLMs) to refine prompts through in-context learning, but suffer from several limitations: they may distort user intent, omit critical details, or introduce safety risks. Moreover, they optimize prompts without considering the impact on the final video quality, which can lead to suboptimal results. To address these issues, we introduce VPO, a principled framework that optimizes prompts based on three core principles: harmlessness, accuracy, and helpfulness. The generated prompts faithfully preserve user intents and, more importantly, enhance the safety and quality of generated videos. To achieve this, VPO employs a two-stage optimization approach. First, we construct and refine a supervised fine-tuning (SFT) dataset based on principles of safety and alignment. Second, we introduce both text-level and video-level feedback to further optimize the SFT model with preference learning. Our extensive experiments demonstrate that VPO significantly improves safety, alignment, and video quality compared to baseline methods. Moreover, VPO shows strong generalization across video generation models. Furthermore, we demonstrate that VPO could outperform and be combined with RLHF methods on video generation models, underscoring the effectiveness of VPO in aligning video generation models. Our code and data are publicly available at https://github.com/thu-coai/VPO."
  },
  {
    "title": "Comparative analysis of clustering methods for power delay profile in MMW bands and in-vehicle scenarios",
    "url": "http://arxiv.org/abs/2503.20358v1",
    "arxiv_id": "2503.20358v1",
    "authors": [
      "Radek Zavorka",
      "Ales Prokes",
      "Josef Vychodil",
      "Tomas Mikulasek",
      "Petr Horky",
      "Aniruddha Chandra",
      "Christoph Mecklenbrauker",
      "Jan M. Kelner",
      "Cezary Ziolkowski"
    ],
    "published": "2025-03-26T09:36:24+00:00",
    "summary": "The spatial statistics of radio wave propagation in specific environments and scenarios, as well as being able to recognize important signal components, are prerequisites for dependable connectivity. There are several reasons why in-vehicle communication is unique, including safety considerations and vehicle-to-vehicle/infrastructure communication.The paper examines the characteristics of clustering power delay profiles to investigate in-vehicle communication. It has been demonstrated that the Saleh-Valenzuela channel model can also be adapted for in-vehicle communication, and that the signal is received in clusters with exponential decay. A measurement campaign was conducted, capturing the power delay profile inside the vehicle cabin, and the reweighted l1 minimization method was compared with the traditional k-means clustering techniques."
  },
  {
    "title": "CNN+Transformer Based Anomaly Traffic Detection in UAV Networks for Emergency Rescue",
    "url": "http://arxiv.org/abs/2503.20355v1",
    "arxiv_id": "2503.20355v1",
    "authors": [
      "Yulu Han",
      "Ziye Jia",
      "Sijie He",
      "Yu Zhang",
      "Qihui Wu"
    ],
    "published": "2025-03-26T09:27:26+00:00",
    "summary": "The unmanned aerial vehicle (UAV) network has gained significant attentions in recent years due to its various applications. However, the traffic security becomes the key threatening public safety issue in an emergency rescue system due to the increasing vulnerability of UAVs to cyber attacks in environments with high heterogeneities. Hence, in this paper, we propose a novel anomaly traffic detection architecture for UAV networks based on the software-defined networking (SDN) framework and blockchain technology. Specifically, SDN separates the control and data plane to enhance the network manageability and security. Meanwhile, the blockchain provides decentralized identity authentication and data security records. Beisdes, a complete security architecture requires an effective mechanism to detect the time-series based abnormal traffic. Thus, an integrated algorithm combining convolutional neural networks (CNNs) and Transformer (CNN+Transformer) for anomaly traffic detection is developed, which is called CTranATD. Finally, the simulation results show that the proposed CTranATD algorithm is effective and outperforms the individual CNN, Transformer, and LSTM algorithms for detecting anomaly traffic."
  },
  {
    "title": "GNSS jammer localization and identification with airborne commercial GNSS receivers",
    "url": "http://arxiv.org/abs/2503.20352v1",
    "arxiv_id": "2503.20352v1",
    "authors": [
      "Marco Spanghero",
      "Filip Geib",
      "Ronny Panier",
      "Panos Papadimitratos"
    ],
    "published": "2025-03-26T09:23:08+00:00",
    "summary": "Global Navigation Satellite Systems (GNSS) are fundamental in ubiquitously providing position and time to a wide gamut of systems. Jamming remains a realistic threat in many deployment settings, civilian and tactical. Specifically, in Unmanned Aerial Vehicles (UAVs) sustained denial raises safety critical concerns. This work presents a strategy that allows detection, localization, and classification both in the frequency and time domain of interference signals harmful to navigation. A high-performance Vertical Take Off and Landing (VTOL) UAV with a single antenna and a commercial GNSS receiver is used to geolocate and characterize RF emitters at long range, to infer the navigation impairment. Raw IQ baseband snapshots from the GNSS receiver make the application of spectral correlation methods possible without extra software-defined radio payload, paving the way to spectrum identification and monitoring in airborne platforms, aiming at RF situational awareness. Live testing at Jammertest, in Norway, with portable, commercially available GNSS multi-band jammers demonstrates the ability to detect, localize, and characterize harmful interference. Our system pinpointed the position with an error of a few meters of the transmitter and the extent of the affected area at long range, without entering the denied zone. Additionally, further spectral content extraction is used to accurately identify the jammer frequency, bandwidth, and modulation scheme based on spectral correlation techniques."
  },
  {
    "title": "Iterative Prompting with Persuasion Skills in Jailbreaking Large Language Models",
    "url": "http://arxiv.org/abs/2503.20320v1",
    "arxiv_id": "2503.20320v1",
    "authors": [
      "Shih-Wen Ke",
      "Guan-Yu Lai",
      "Guo-Lin Fang",
      "Hsi-Yuan Kao"
    ],
    "published": "2025-03-26T08:40:46+00:00",
    "summary": "Large language models (LLMs) are designed to align with human values in their responses. This study exploits LLMs with an iterative prompting technique where each prompt is systematically modified and refined across multiple iterations to enhance its effectiveness in jailbreaking attacks progressively. This technique involves analyzing the response patterns of LLMs, including GPT-3.5, GPT-4, LLaMa2, Vicuna, and ChatGLM, allowing us to adjust and optimize prompts to evade the LLMs' ethical and security constraints. Persuasion strategies enhance prompt effectiveness while maintaining consistency with malicious intent. Our results show that the attack success rates (ASR) increase as the attacking prompts become more refined with the highest ASR of 90% for GPT4 and ChatGLM and the lowest ASR of 68% for LLaMa2. Our technique outperforms baseline techniques (PAIR and PAP) in ASR and shows comparable performance with GCG and ArtPrompt."
  },
  {
    "title": "sudo rm -rf agentic_security",
    "url": "http://arxiv.org/abs/2503.20279v1",
    "arxiv_id": "2503.20279v1",
    "authors": [
      "Sejin Lee",
      "Jian Kim",
      "Haon Park",
      "Ashkan Yousefpour",
      "Sangyoon Yu",
      "Min Song"
    ],
    "published": "2025-03-26T07:08:15+00:00",
    "summary": "Large Language Models (LLMs) are increasingly deployed as computer-use agents, autonomously performing tasks within real desktop or web environments. While this evolution greatly expands practical use cases for humans, it also creates serious security exposures. We present SUDO (Screen-based Universal Detox2Tox Offense), a novel attack framework that systematically bypasses refusal trained safeguards in commercial computer-use agents, such as Claude Computer Use. The core mechanism, Detox2Tox, transforms harmful requests (that agents initially reject) into seemingly benign requests via detoxification, secures detailed instructions from advanced vision language models (VLMs), and then reintroduces malicious content via toxification just before execution. Unlike conventional jailbreaks, SUDO iteratively refines its attacks based on a built-in refusal feedback, making it increasingly effective against robust policy filters. In extensive tests spanning 50 real-world tasks and multiple state-of-the-art VLMs, SUDO achieves a stark attack success rate of 24% (with no refinement), and up to 41% (by its iterative refinement) in Claude Computer Use. By revealing these vulnerabilities and demonstrating the ease with which they can be exploited in real-world computing environments, this paper highlights an immediate need for robust, context-aware safeguards. WARNING: This paper includes harmful or offensive model outputs."
  },
  {
    "title": "ViLBench: A Suite for Vision-Language Process Reward Modeling",
    "url": "http://arxiv.org/abs/2503.20271v1",
    "arxiv_id": "2503.20271v1",
    "authors": [
      "Haoqin Tu",
      "Weitao Feng",
      "Hardy Chen",
      "Hui Liu",
      "Xianfeng Tang",
      "Cihang Xie"
    ],
    "published": "2025-03-26T06:38:31+00:00",
    "summary": "Process-supervised reward models serve as a fine-grained function that provides detailed step-wise feedback to model responses, facilitating effective selection of reasoning trajectories for complex tasks. Despite its advantages, evaluation on PRMs remains less explored, especially in the multimodal domain. To address this gap, this paper first benchmarks current vision large language models (VLLMs) as two types of reward models: output reward models (ORMs) and process reward models (PRMs) on multiple vision-language benchmarks, which reveal that neither ORM nor PRM consistently outperforms across all tasks, and superior VLLMs do not necessarily yield better rewarding performance. To further advance evaluation, we introduce ViLBench, a vision-language benchmark designed to require intensive process reward signals. Notably, OpenAI's GPT-4o with Chain-of-Thought (CoT) achieves only 27.3% accuracy, indicating the benchmark's challenge for current VLLMs. Lastly, we preliminarily showcase a promising pathway towards bridging the gap between general VLLMs and reward models -- by collecting 73.6K vision-language process reward data using an enhanced tree-search algorithm, our 3B model is able to achieve an average improvement of 3.3% over standard CoT and up to 2.5% compared to its untrained counterpart on ViLBench by selecting OpenAI o1's generations. We release the implementations at https://ucsc-vlaa.github.io/ViLBench with our code, model, and data."
  },
  {
    "title": "From the CDC to emerging infection disease publics: The long-now of polarizing and complex health crises",
    "url": "http://arxiv.org/abs/2503.20262v1",
    "arxiv_id": "2503.20262v1",
    "authors": [
      "Tawfiq Ammari",
      "Anna Gutowska",
      "Jacob Ziff",
      "Casey Randazzo",
      "Hari Subramonyam"
    ],
    "published": "2025-03-26T06:09:29+00:00",
    "summary": "As the COVID-19 pandemic evolved, the Center for Disease Control and Prevention used Twitter to share updates about the virus and safety guidelines, reaching millions instantly, in what we call the CDC public. We analyze two years of tweets, from, to, and about the CDC using a mixed-methods approach to characterize the nature and credibility of COVID-19 discourse and audience engagement. We found that the CDC is not engaging in two-way communication with the CDC publics and that discussions about COVID-19 reflected societal divisions and political polarization. We introduce a crisis message journey concept showing how the CDC public responds to the changing nature of the crisis (e.g., new variants) using ``receipts'' of earlier, and at times contradictory, guidelines. We propose design recommendations to support the CDC in tailoring messages to specific users and publics (e.g., users interested in racial equity) and in managing misinformation, especially in reaction to crisis flashpoints."
  },
  {
    "title": "From the CDC to emerging infectious disease publics: The long-now of polarizing and complex health crises",
    "url": "http://arxiv.org/abs/2503.20262v2",
    "arxiv_id": "2503.20262v2",
    "authors": [
      "Tawfiq Ammari",
      "Anna Gutowska",
      "Jacob Ziff",
      "Casey Randazzo",
      "Harihan Subramonyam"
    ],
    "published": "2025-03-26T06:09:29+00:00",
    "summary": "As the COVID-19 pandemic evolved, the Center for Disease Control and Prevention used Twitter to share updates about the virus and safety guidelines, reaching millions instantly, in what we call the CDC public. We analyze two years of tweets, from, to, and about the CDC using a mixed-methods approach to characterize the nature and credibility of COVID-19 discourse and audience engagement. We found that the CDC is not engaging in two-way communication with the CDC publics and that discussions about COVID-19 reflected societal divisions and political polarization. We introduce a crisis message journey concept showing how the CDC public responds to the changing nature of the crisis (e.g., new variants) using ``receipts'' of earlier, and at times contradictory, guidelines. We propose design recommendations to support the CDC in tailoring messages to specific users and publics (e.g., users interested in racial equity) and in managing misinformation, especially in reaction to crisis flashpoints."
  },
  {
    "title": "A Virtual Fencing Framework for Safe and Efficient Collaborative Robotics",
    "url": "http://arxiv.org/abs/2503.20237v1",
    "arxiv_id": "2503.20237v1",
    "authors": [
      "Vineela Reddy Pippera Badguna",
      "Aliasghar Arab",
      "Durga Avinash Kodavalla"
    ],
    "published": "2025-03-26T05:05:16+00:00",
    "summary": "Collaborative robots (cobots) increasingly operate alongside humans, demanding robust real-time safeguarding. Current safety standards (e.g., ISO 10218, ANSI/RIA 15.06, ISO/TS 15066) require risk assessments but offer limited guidance for real-time responses. We propose a virtual fencing approach that detects and predicts human motion, ensuring safe cobot operation. Safety and performance tradeoffs are modeled as an optimization problem and solved via sequential quadratic programming. Experimental validation shows that our method minimizes operational pauses while maintaining safety, providing a modular solution for human-robot collaboration."
  },
  {
    "title": "Open Deep Search: Democratizing Search with Open-source Reasoning Agents",
    "url": "http://arxiv.org/abs/2503.20201v1",
    "arxiv_id": "2503.20201v1",
    "authors": [
      "Salaheddin Alzubi",
      "Creston Brooks",
      "Purva Chiniya",
      "Edoardo Contente",
      "Chiara von Gerlach",
      "Lucas Irwin",
      "Yihan Jiang",
      "Arda Kaz",
      "Windsor Nguyen",
      "Sewoong Oh",
      "Himanshu Tyagi",
      "Pramod Viswanath"
    ],
    "published": "2025-03-26T03:51:32+00:00",
    "summary": "We introduce Open Deep Search (ODS) to close the increasing gap between the proprietary search AI solutions, such as Perplexity's Sonar Reasoning Pro and OpenAI's GPT-4o Search Preview, and their open-source counterparts. The main innovation introduced in ODS is to augment the reasoning capabilities of the latest open-source LLMs with reasoning agents that can judiciously use web search tools to answer queries. Concretely, ODS consists of two components that work with a base LLM chosen by the user: Open Search Tool and Open Reasoning Agent. Open Reasoning Agent interprets the given task and completes it by orchestrating a sequence of actions that includes calling tools, one of which is the Open Search Tool. Open Search Tool is a novel web search tool that outperforms proprietary counterparts. Together with powerful open-source reasoning LLMs, such as DeepSeek-R1, ODS nearly matches and sometimes surpasses the existing state-of-the-art baselines on two benchmarks: SimpleQA and FRAMES. For example, on the FRAMES evaluation benchmark, ODS improves the best existing baseline of the recently released GPT-4o Search Preview by 9.7% in accuracy. ODS is a general framework for seamlessly augmenting any LLMs -- for example, DeepSeek-R1 that achieves 82.4% on SimpleQA and 30.1% on FRAMES -- with search and reasoning capabilities to achieve state-of-the-art performance: 88.3% on SimpleQA and 75.3% on FRAMES."
  },
  {
    "title": "Network Inversion for Generating Confidently Classified Counterfeits",
    "url": "http://arxiv.org/abs/2503.20187v1",
    "arxiv_id": "2503.20187v1",
    "authors": [
      "Pirzada Suhail",
      "Amit Sethi"
    ],
    "published": "2025-03-26T03:26:49+00:00",
    "summary": "In machine learning, especially with vision classifiers, generating inputs that are confidently classified by the model is essential for understanding its decision boundaries and behavior. However, creating such samples that are confidently classified yet distinct from the training data distribution is a challenge. Traditional methods often modify existing inputs, but they don't always ensure confident classification. In this work, we extend network inversion techniques to generate Confidently Classified Counterfeits-synthetic samples that are confidently classified by the model despite being significantly different from the training data. We achieve this by modifying the generator's conditioning mechanism from soft vector conditioning to one-hot vector conditioning and applying Kullback-Leibler divergence (KLD) between the one-hot vectors and the classifier's output distribution. This encourages the generator to produce samples that are both plausible and confidently classified. Generating Confidently Classified Counterfeits is crucial for ensuring the safety and reliability of machine learning systems, particularly in safety-critical applications where models must exhibit confidence only on data within the training distribution. By generating such counterfeits, we challenge the assumption that high-confidence predictions are always indicative of in-distribution data, providing deeper insights into the model's limitations and decision-making process."
  },
  {
    "title": "DRPA-MPPI: Dynamic Repulsive Potential Augmented MPPI for Reactive Navigation in Unstructured Environments",
    "url": "http://arxiv.org/abs/2503.20134v1",
    "arxiv_id": "2503.20134v1",
    "authors": [
      "Takahiro Fuke",
      "Masafumi Endo",
      "Kohei Honda",
      "Genya Ishigami"
    ],
    "published": "2025-03-26T00:57:04+00:00",
    "summary": "Reactive mobile robot navigation in unstructured environments is challenging when robots encounter unexpected obstacles that invalidate previously planned trajectories. Model predictive path integral control (MPPI) enables reactive planning, but still suffers from limited prediction horizons that lead to local minima traps near obstacles. Current solutions rely on heuristic cost design or scenario-specific pre-training, which often limits their adaptability to new environments. We introduce dynamic repulsive potential augmented MPPI (DRPA-MPPI), which dynamically detects potential entrapments on the predicted trajectories. Upon detecting local minima, DRPA-MPPI automatically switches between standard goal-oriented optimization and a modified cost function that generates repulsive forces away from local minima. Comprehensive testing in simulated obstacle-rich environments confirms DRPA-MPPI's superior navigation performance and safety compared to conventional methods with less computational burden."
  },
  {
    "title": "Domain Adaptation Framework for Turning Movement Count Estimation with Limited Data",
    "url": "http://arxiv.org/abs/2503.20113v1",
    "arxiv_id": "2503.20113v1",
    "authors": [
      "Xiaobo Ma",
      "Hyunsoo Noh",
      "Ryan Hatch",
      "James Tokishi",
      "Zepu Wang"
    ],
    "published": "2025-03-25T23:27:38+00:00",
    "summary": "Urban transportation networks are vital for the efficient movement of people and goods, necessitating effective traffic management and planning. An integral part of traffic management is understanding the turning movement counts (TMCs) at intersections, Accurate TMCs at intersections are crucial for traffic signal control, congestion mitigation, and road safety. In general, TMCs are obtained using physical sensors installed at intersections, but this approach can be cost-prohibitive and technically challenging, especially for cities with extensive road networks. Recent advancements in machine learning and data-driven approaches have offered promising alternatives for estimating TMCs. Traffic patterns can vary significantly across different intersections due to factors such as road geometry, traffic signal settings, and local driver behaviors. This domain discrepancy limits the generalizability and accuracy of machine learning models when applied to new or unseen intersections. In response to these limitations, this research proposes a novel framework leveraging domain adaptation (DA) to estimate TMCs at intersections by using traffic controller event-based data, road infrastructure data, and point-of-interest (POI) data. Evaluated on 30 intersections in Tucson, Arizona, the performance of the proposed DA framework was compared with state-of-the-art models and achieved the lowest values in terms of Mean Absolute Error and Root Mean Square Error."
  },
  {
    "title": "Gemini Robotics: Bringing AI into the Physical World",
    "url": "http://arxiv.org/abs/2503.20020v1",
    "arxiv_id": "2503.20020v1",
    "authors": [
      "Gemini Robotics Team",
      "Saminda Abeyruwan",
      "Joshua Ainslie",
      "Jean-Baptiste Alayrac",
      "Montserrat Gonzalez Arenas",
      "Travis Armstrong",
      "Ashwin Balakrishna",
      "Robert Baruch",
      "Maria Bauza",
      "Michiel Blokzijl",
      "Steven Bohez",
      "Konstantinos Bousmalis",
      "Anthony Brohan",
      "Thomas Buschmann",
      "Arunkumar Byravan",
      "Serkan Cabi",
      "Ken Caluwaerts",
      "Federico Casarini",
      "Oscar Chang",
      "Jose Enrique Chen",
      "Xi Chen",
      "Hao-Tien Lewis Chiang",
      "Krzysztof Choromanski",
      "David D'Ambrosio",
      "Sudeep Dasari",
      "Todor Davchev",
      "Coline Devin",
      "Norman Di Palo",
      "Tianli Ding",
      "Adil Dostmohamed",
      "Danny Driess",
      "Yilun Du",
      "Debidatta Dwibedi",
      "Michael Elabd",
      "Claudio Fantacci",
      "Cody Fong",
      "Erik Frey",
      "Chuyuan Fu",
      "Marissa Giustina",
      "Keerthana Gopalakrishnan",
      "Laura Graesser",
      "Leonard Hasenclever",
      "Nicolas Heess",
      "Brandon Hernaez",
      "Alexander Herzog",
      "R. Alex Hofer",
      "Jan Humplik",
      "Atil Iscen",
      "Mithun George Jacob",
      "Deepali Jain",
      "Ryan Julian",
      "Dmitry Kalashnikov",
      "M. Emre Karagozler",
      "Stefani Karp",
      "Chase Kew",
      "Jerad Kirkland",
      "Sean Kirmani",
      "Yuheng Kuang",
      "Thomas Lampe",
      "Antoine Laurens",
      "Isabel Leal",
      "Alex X. Lee",
      "Tsang-Wei Edward Lee",
      "Jacky Liang",
      "Yixin Lin",
      "Sharath Maddineni",
      "Anirudha Majumdar",
      "Assaf Hurwitz Michaely",
      "Robert Moreno",
      "Michael Neunert",
      "Francesco Nori",
      "Carolina Parada",
      "Emilio Parisotto",
      "Peter Pastor",
      "Acorn Pooley",
      "Kanishka Rao",
      "Krista Reymann",
      "Dorsa Sadigh",
      "Stefano Saliceti",
      "Pannag Sanketi",
      "Pierre Sermanet",
      "Dhruv Shah",
      "Mohit Sharma",
      "Kathryn Shea",
      "Charles Shu",
      "Vikas Sindhwani",
      "Sumeet Singh",
      "Radu Soricut",
      "Jost Tobias Springenberg",
      "Rachel Sterneck",
      "Razvan Surdulescu",
      "Jie Tan",
      "Jonathan Tompson",
      "Vincent Vanhoucke",
      "Jake Varley",
      "Grace Vesom",
      "Giulia Vezzani",
      "Oriol Vinyals",
      "Ayzaan Wahid",
      "Stefan Welker",
      "Paul Wohlhart",
      "Fei Xia",
      "Ted Xiao",
      "Annie Xie",
      "Jinyu Xie",
      "Peng Xu",
      "Sichun Xu",
      "Ying Xu",
      "Zhuo Xu",
      "Yuxiang Yang",
      "Rui Yao",
      "Sergey Yaroshenko",
      "Wenhao Yu",
      "Wentao Yuan",
      "Jingwei Zhang",
      "Tingnan Zhang",
      "Allan Zhou",
      "Yuxiang Zhou"
    ],
    "published": "2025-03-25T19:02:56+00:00",
    "summary": "Recent advancements in large multimodal models have led to the emergence of remarkable generalist capabilities in digital domains, yet their translation to physical agents such as robots remains a significant challenge. This report introduces a new family of AI models purposefully designed for robotics and built upon the foundation of Gemini 2.0. We present Gemini Robotics, an advanced Vision-Language-Action (VLA) generalist model capable of directly controlling robots. Gemini Robotics executes smooth and reactive movements to tackle a wide range of complex manipulation tasks while also being robust to variations in object types and positions, handling unseen environments as well as following diverse, open vocabulary instructions. We show that with additional fine-tuning, Gemini Robotics can be specialized to new capabilities including solving long-horizon, highly dexterous tasks, learning new short-horizon tasks from as few as 100 demonstrations and adapting to completely novel robot embodiments. This is made possible because Gemini Robotics builds on top of the Gemini Robotics-ER model, the second model we introduce in this work. Gemini Robotics-ER (Embodied Reasoning) extends Gemini's multimodal reasoning capabilities into the physical world, with enhanced spatial and temporal understanding. This enables capabilities relevant to robotics including object detection, pointing, trajectory and grasp prediction, as well as multi-view correspondence and 3D bounding box predictions. We show how this novel combination can support a variety of robotics applications. We also discuss and address important safety considerations related to this new class of robotics foundation models. The Gemini Robotics family marks a substantial step towards developing general-purpose robots that realizes AI's potential in the physical world."
  },
  {
    "title": "Control Barrier Functions for Shared Control and Vehicle Safety",
    "url": "http://arxiv.org/abs/2503.19994v1",
    "arxiv_id": "2503.19994v1",
    "authors": [
      "James Dallas",
      "John Talbot",
      "Makoto Suminaka",
      "Michael Thompson",
      "Thomas Lew",
      "Gabor Orosz",
      "John Subosits"
    ],
    "published": "2025-03-25T18:23:56+00:00",
    "summary": "This manuscript presents a control barrier function based approach to shared control for preventing a vehicle from entering the part of the state space where it is unrecoverable. The maximal phase recoverable ellipse is presented as a safe set in the sideslip angle--yaw rate phase plane where the vehicle's state can be maintained. An exponential control barrier function is then defined on the maximal phase recoverable ellipse to promote safety. Simulations demonstrate that this approach enables safe drifting, that is, driving at the handling limit without spinning out. Results are then validated for shared control drifting with an experimental vehicle in a closed course. The results show the ability of this shared control formulation to maintain the vehicle's state within a safe domain in a computationally efficient manner, even in extreme drifting maneuvers."
  },
  {
    "title": "A proposal for an incident regime that tracks and counters threats to national security posed by AI systems",
    "url": "http://arxiv.org/abs/2503.19887v1",
    "arxiv_id": "2503.19887v1",
    "authors": [
      "Alejandro Ortega"
    ],
    "published": "2025-03-25T17:51:50+00:00",
    "summary": "Recent progress in AI capabilities has heightened concerns that AI systems could pose a threat to national security, for example, by making it easier for malicious actors to perform cyberattacks on critical national infrastructure, or through loss of control of autonomous AI systems. In parallel, federal legislators in the US have proposed nascent 'AI incident regimes' to identify and counter similar threats. In this paper, we consolidate these two trends and present a proposal for a legally mandated post-deployment AI incident regie that aims to counter potential national security threats from AI systems. We start the paper by introducing the concept of 'security-critical' to describe doctors that pose extreme risks to national security, before arguing that 'security-critical' describes civilian nuclear power, aviation, life science dual-use research of concern, and frontier AI development. We then present in detail our AI incident regime proposal,, justifying each component of the proposal by demonstrating its similarity to US domestic incident regimes in other 'security-critical' sectors. Finally, we sketch a hypothetical scenario where our proposed AI incident regime deals with an AI cyber incident. Our proposed AI incident regime is split into three phases. The first phase revolves around a novel operationalization of what counts as an 'AI incident' and we suggest that AI providers must create a 'national security case' before deploying a frontier AI system. The second and third phases spell out that AI providers should notify a government agency about incidents, and that the government agency should be involved in amending AI providers' security and safety procedures, in order to counter future threats to national security. Our proposal is timely, given ongoing policy interest in the potential national security threats posed by AI systems."
  },
  {
    "title": "Think Twice: Enhancing LLM Reasoning by Scaling Multi-round Test-time Thinking",
    "url": "http://arxiv.org/abs/2503.19855v1",
    "arxiv_id": "2503.19855v1",
    "authors": [
      "Xiaoyu Tian",
      "Sitong Zhao",
      "Haotian Wang",
      "Shuaiting Chen",
      "Yunjie Ji",
      "Yiping Peng",
      "Han Zhao",
      "Xiangang Li"
    ],
    "published": "2025-03-25T17:19:38+00:00",
    "summary": "Recent advances in large language models (LLMs), such as OpenAI-o1 and DeepSeek-R1, have demonstrated the effectiveness of test-time scaling, where extended reasoning processes substantially enhance model performance. Despite this, current models are constrained by limitations in handling long texts and reinforcement learning (RL) training efficiency. To address these issues, we propose a simple yet effective test-time scaling approach Multi-round Thinking. This method iteratively refines model reasoning by leveraging previous answers as prompts for subsequent rounds. Extensive experiments across multiple models, including QwQ-32B and DeepSeek-R1, consistently show performance improvements on various benchmarks such as AIME 2024, MATH-500, GPQA-diamond, and LiveCodeBench. For instance, the accuracy of QwQ-32B improved from 80.3% (Round 1) to 82.1% (Round 2) on the AIME 2024 dataset, while DeepSeek-R1 showed a similar increase from 79.7% to 82.0%. These results confirm that Multi-round Thinking is a broadly applicable, straightforward approach to achieving stable enhancements in model performance, underscoring its potential for future developments in test-time scaling techniques. The key prompt: {Original question prompt} The assistant's previous answer is: <answer> {last round answer} </answer>, and please re-answer."
  },
  {
    "title": "Test-Time Reasoning Through Visual Human Preferences with VLMs and Soft Rewards",
    "url": "http://arxiv.org/abs/2503.19948v1",
    "arxiv_id": "2503.19948v1",
    "authors": [
      "Alexander Gambashidze",
      "Konstantin Sobolev",
      "Andrey Kuznetsov",
      "Ivan Oseledets"
    ],
    "published": "2025-03-25T15:30:21+00:00",
    "summary": "Can Visual Language Models (VLMs) effectively capture human visual preferences? This work addresses this question by training VLMs to think about preferences at test time, employing reinforcement learning methods inspired by DeepSeek R1 and OpenAI O1. Using datasets such as ImageReward and Human Preference Score v2 (HPSv2), our models achieve accuracies of 64.9% on the ImageReward test set (trained on ImageReward official split) and 65.4% on HPSv2 (trained on approximately 25% of its data). These results match traditional encoder-based models while providing transparent reasoning and enhanced generalization. This approach allows to use not only rich VLM world knowledge, but also its potential to think, yielding interpretable outcomes that help decision-making processes. By demonstrating that human visual preferences reasonable by current VLMs, we introduce efficient soft-reward strategies for image ranking, outperforming simplistic selection or scoring methods. This reasoning capability enables VLMs to rank arbitrary images-regardless of aspect ratio or complexity-thereby potentially amplifying the effectiveness of visual Preference Optimization. By reducing the need for extensive markup while improving reward generalization and explainability, our findings can be a strong mile-stone that will enhance text-to-vision models even further."
  },
  {
    "title": "Optimal Safe Sequencing and Motion Control for Mixed Traffic Roundabouts",
    "url": "http://arxiv.org/abs/2503.19738v1",
    "arxiv_id": "2503.19738v1",
    "authors": [
      "Yingqing Chen",
      "Christos G. Cassandras"
    ],
    "published": "2025-03-25T15:03:58+00:00",
    "summary": "This paper develops an Optimal Safe Sequencing (OSS) control framework for Connected and Automated Vehicles (CAVs) navigating a single-lane roundabout in mixed traffic, where both CAVs and Human-Driven Vehicles (HDVs) coexist. The framework jointly optimizes vehicle sequencing and motion control to minimize travel time, energy consumption, and discomfort while ensuring speed-dependent safety guarantees and adhering to velocity and acceleration constraints. This is achieved by integrating (a) a Safe Sequencing (SS) policy that ensures merging safety without requiring any knowledge of HDV behavior, and (b) a Model Predictive Control with Control Lyapunov Barrier Functions (MPC-CLBF) framework, which optimizes CAV motion control while mitigating infeasibility and myopic control issues common in the use of Control Barrier Functions (CBFs) to provide safety guarantees. Simulation results across various traffic demands, CAV penetration rates, and control parameters demonstrate the framework's effectiveness and stability."
  },
  {
    "title": "Decoupled Dynamics Framework with Neural Fields for 3D Spatio-temporal Prediction of Vehicle Collisions",
    "url": "http://arxiv.org/abs/2503.19712v1",
    "arxiv_id": "2503.19712v1",
    "authors": [
      "Sanghyuk Kim",
      "Minsik Seo",
      "Namwoo Kang"
    ],
    "published": "2025-03-25T14:38:37+00:00",
    "summary": "This study proposes a neural framework that predicts 3D vehicle collision dynamics by independently modeling global rigid-body motion and local structural deformation. Unlike approaches directly predicting absolute displacement, this method explicitly separates the vehicle's overall translation and rotation from its structural deformation. Two specialized networks form the core of the framework: a quaternion-based Rigid Net for rigid motion and a coordinate-based Deformation Net for local deformation. By independently handling fundamentally distinct physical phenomena, the proposed architecture achieves accurate predictions without requiring separate supervision for each component. The model, trained on only 10% of available simulation data, significantly outperforms baseline models, including single multi-layer perceptron (MLP) and deep operator networks (DeepONet), with prediction errors reduced by up to 83%. Extensive validation demonstrates strong generalization to collision conditions outside the training range, accurately predicting responses even under severe impacts involving extreme velocities and large impact angles. Furthermore, the framework successfully reconstructs high-resolution deformation details from low-resolution inputs without increased computational effort. Consequently, the proposed approach provides an effective, computationally efficient method for rapid and reliable assessment of vehicle safety across complex collision scenarios, substantially reducing the required simulation data and time while preserving prediction fidelity."
  },
  {
    "title": "Risk-Aware Reinforcement Learning for Autonomous Driving: Improving Safety When Driving through Intersection",
    "url": "http://arxiv.org/abs/2503.19690v1",
    "arxiv_id": "2503.19690v1",
    "authors": [
      "Bo Leng",
      "Ran Yu",
      "Wei Han",
      "Lu Xiong",
      "Zhuoren Li",
      "Hailong Huang"
    ],
    "published": "2025-03-25T14:17:15+00:00",
    "summary": "Applying reinforcement learning to autonomous driving has garnered widespread attention. However, classical reinforcement learning methods optimize policies by maximizing expected rewards but lack sufficient safety considerations, often putting agents in hazardous situations. This paper proposes a risk-aware reinforcement learning approach for autonomous driving to improve the safety performance when crossing the intersection. Safe critics are constructed to evaluate driving risk and work in conjunction with the reward critic to update the actor. Based on this, a Lagrangian relaxation method and cyclic gradient iteration are combined to project actions into a feasible safe region. Furthermore, a Multi-hop and Multi-layer perception (MLP) mixed Attention Mechanism (MMAM) is incorporated into the actor-critic network, enabling the policy to adapt to dynamic traffic and overcome permutation sensitivity challenges. This allows the policy to focus more effectively on surrounding potential risks while enhancing the identification of passing opportunities. Simulation tests are conducted on different tasks at unsignalized intersections. The results show that the proposed approach effectively reduces collision rates and improves crossing efficiency in comparison to baseline algorithms. Additionally, our ablation experiments demonstrate the benefits of incorporating risk-awareness and MMAM into RL."
  },
  {
    "title": "1.4 Million Open-Source Distilled Reasoning Dataset to Empower Large Language Model Training",
    "url": "http://arxiv.org/abs/2503.19633v1",
    "arxiv_id": "2503.19633v1",
    "authors": [
      "Han Zhao",
      "Haotian Wang",
      "Yiping Peng",
      "Sitong Zhao",
      "Xiaoyu Tian",
      "Shuaiting Chen",
      "Yunjie Ji",
      "Xiangang Li"
    ],
    "published": "2025-03-25T13:19:46+00:00",
    "summary": "The AM-DeepSeek-R1-Distilled is a large-scale dataset with thinking traces for general reasoning tasks, composed of high-quality and challenging reasoning problems. These problems are collected from a multitude of open-source datasets, subjected to semantic deduplication and meticulous cleaning to eliminate test set contamination. All responses within the dataset are distilled from reasoning models (predominantly DeepSeek-R1) and have undergone rigorous verification procedures. Mathematical problems are validated by checking against reference answers, code problems are verified using test cases, and other tasks are evaluated with the aid of a reward model. The AM-Distill-Qwen-32B model, which was trained through only simple Supervised Fine-Tuning (SFT) using this batch of data, outperformed the DeepSeek-R1-Distill-Qwen-32B model on four benchmarks: AIME2024, MATH-500, GPQA-Diamond, and LiveCodeBench. Additionally, the AM-Distill-Qwen-72B model surpassed the DeepSeek-R1-Distill-Llama-70B model on all benchmarks as well. We are releasing these 1.4 million problems and their corresponding responses to the research community with the objective of fostering the development of powerful reasoning-oriented Large Language Models (LLMs). The dataset was published in \\href{https://huggingface.co/datasets/a-m-team/AM-DeepSeek-R1-Distilled-1.4M}{https://huggingface.co/datasets/a-m-team/AM-DeepSeek-R1-Distilled-1.4M}."
  },
  {
    "title": "Exploring Hallucination of Large Multimodal Models in Video Understanding: Benchmark, Analysis and Mitigation",
    "url": "http://arxiv.org/abs/2503.19622v1",
    "arxiv_id": "2503.19622v1",
    "authors": [
      "Hongcheng Gao",
      "Jiashu Qu",
      "Jingyi Tang",
      "Baolong Bi",
      "Yue Liu",
      "Hongyu Chen",
      "Li Liang",
      "Li Su",
      "Qingming Huang"
    ],
    "published": "2025-03-25T13:12:17+00:00",
    "summary": "The hallucination of large multimodal models (LMMs), providing responses that appear correct but are actually incorrect, limits their reliability and applicability. This paper aims to study the hallucination problem of LMMs in video modality, which is dynamic and more challenging compared to static modalities like images and text. From this motivation, we first present a comprehensive benchmark termed HAVEN for evaluating hallucinations of LMMs in video understanding tasks. It is built upon three dimensions, i.e., hallucination causes, hallucination aspects, and question formats, resulting in 6K questions. Then, we quantitatively study 7 influential factors on hallucinations, e.g., duration time of videos, model sizes, and model reasoning, via experiments of 16 LMMs on the presented benchmark. In addition, inspired by recent thinking models like OpenAI o1, we propose a video-thinking model to mitigate the hallucinations of LMMs via supervised reasoning fine-tuning (SRFT) and direct preference optimization (TDPO)-- where SRFT enhances reasoning capabilities while TDPO reduces hallucinations in the thinking process. Extensive experiments and analyses demonstrate the effectiveness. Remarkably, it improves the baseline by 7.65% in accuracy on hallucination evaluation and reduces the bias score by 4.5%. The code and data are public at https://github.com/Hongcheng-Gao/HAVEN."
  },
  {
    "title": "On the Completeness and Ordering of Path-Complete Barrier Functions",
    "url": "http://arxiv.org/abs/2503.19561v1",
    "arxiv_id": "2503.19561v1",
    "authors": [
      "Mahathi Anand",
      "Rapha\u00ebl Jungers",
      "Majid Zamani",
      "Frank Allg\u00f6wer"
    ],
    "published": "2025-03-25T11:27:05+00:00",
    "summary": "This paper is concerned with path-complete barrier functions which offer a graph-based methodology for verifying safety properties in switched systems. The path-complete framework leverages algebraic (barrier functions) as well as combinatorial (graph) components to characterize a set of safety conditions for switched systems, thus offering high flexibility (two degrees of freedom) in searching for suitable safety certificates. In this paper, we do not propose any new safety criteria. Instead, we further investigate the role that the combinatorial component plays in the safety verification problem. First, we prove that path-completeness, which is a property on a graph that describes the switching sequences, is necessary to obtain a set of valid safety conditions. As a result, the path-complete framework is able to provide a complete characterization of safety conditions for switched systems. Furthermore, we provide a systematic methodology for comparing two path-complete graphs and the conservatism associated with the resulting safety conditions. Specifically, we prove that under some conditions, such as when there exists a simulation relation between two path-complete graphs, it is possible to conclude that one graph is always able to provide less conservative safety conditions than another, independent of the algebraic properties of the switched system and the template of the barrier function under consideration. Such a result paves the way for a systematic use of the path-complete frame- work with barrier functions, as one can then consistently choose the appropriate graph that provides less conservative safety conditions."
  },
  {
    "title": "FLEX: A Benchmark for Evaluating Robustness of Fairness in Large Language Models",
    "url": "http://arxiv.org/abs/2503.19540v1",
    "arxiv_id": "2503.19540v1",
    "authors": [
      "Dahyun Jung",
      "Seungyoon Lee",
      "Hyeonseok Moon",
      "Chanjun Park",
      "Heuiseok Lim"
    ],
    "published": "2025-03-25T10:48:33+00:00",
    "summary": "Recent advancements in Large Language Models (LLMs) have significantly enhanced interactions between users and models. These advancements concurrently underscore the need for rigorous safety evaluations due to the manifestation of social biases, which can lead to harmful societal impacts. Despite these concerns, existing benchmarks may overlook the intrinsic weaknesses of LLMs, which can generate biased responses even with simple adversarial instructions. To address this critical gap, we introduce a new benchmark, Fairness Benchmark in LLM under Extreme Scenarios (FLEX), designed to test whether LLMs can sustain fairness even when exposed to prompts constructed to induce bias. To thoroughly evaluate the robustness of LLMs, we integrate prompts that amplify potential biases into the fairness assessment. Comparative experiments between FLEX and existing benchmarks demonstrate that traditional evaluations may underestimate the inherent risks in models. This highlights the need for more stringent LLM evaluation benchmarks to guarantee safety and fairness."
  },
  {
    "title": "Pose-Based Fall Detection System: Efficient Monitoring on Standard CPUs",
    "url": "http://arxiv.org/abs/2503.19501v1",
    "arxiv_id": "2503.19501v1",
    "authors": [
      "Vinayak Mali",
      "Saurabh Jaiswal"
    ],
    "published": "2025-03-25T09:49:36+00:00",
    "summary": "Falls among elderly residents in assisted living homes pose significant health risks, often leading to injuries and a decreased quality of life. Current fall detection solutions typically rely on sensor-based systems that require dedicated hardware, or on video-based models that demand high computational resources and GPUs for real-time processing. In contrast, this paper presents a robust fall detection system that does not require any additional sensors or high-powered hardware. The system uses pose estimation techniques, combined with threshold-based analysis and a voting mechanism, to effectively distinguish between fall and non-fall activities. For pose detection, we leverage MediaPipe, a lightweight and efficient framework that enables real-time processing on standard CPUs with minimal computational overhead. By analyzing motion, body position, and key pose points, the system processes pose features with a 20-frame buffer, minimizing false positives and maintaining high accuracy even in real-world settings. This unobtrusive, resource-efficient approach provides a practical solution for enhancing resident safety in old age homes, without the need for expensive sensors or high-end computational resources."
  },
  {
    "title": "ReSearch: Learning to Reason with Search for LLMs via Reinforcement Learning",
    "url": "http://arxiv.org/abs/2503.19470v1",
    "arxiv_id": "2503.19470v1",
    "authors": [
      "Mingyang Chen",
      "Tianpeng Li",
      "Haoze Sun",
      "Yijie Zhou",
      "Chenzheng Zhu",
      "Fan Yang",
      "Zenan Zhou",
      "Weipeng Chen",
      "Haofen Wang",
      "Jeff Z. Pan",
      "Wen Zhang",
      "Huajun Chen"
    ],
    "published": "2025-03-25T09:00:58+00:00",
    "summary": "Large Language Models (LLMs) have shown remarkable capabilities in reasoning, exemplified by the success of OpenAI-o1 and DeepSeek-R1. However, integrating reasoning with external search processes remains challenging, especially for complex multi-hop questions requiring multiple retrieval steps. We propose ReSearch, a novel framework that trains LLMs to Reason with Search via reinforcement learning without using any supervised data on reasoning steps. Our approach treats search operations as integral components of the reasoning chain, where when and how to perform searches is guided by text-based thinking, and search results subsequently influence further reasoning. We train ReSearch on Qwen2.5-7B(-Instruct) and Qwen2.5-32B(-Instruct) models and conduct extensive experiments. Despite being trained on only one dataset, our models demonstrate strong generalizability across various benchmarks. Analysis reveals that ReSearch naturally elicits advanced reasoning capabilities such as reflection and self-correction during the reinforcement learning process."
  },
  {
    "title": "A Probabilistic Neuro-symbolic Layer for Algebraic Constraint Satisfaction",
    "url": "http://arxiv.org/abs/2503.19466v1",
    "arxiv_id": "2503.19466v1",
    "authors": [
      "Leander Kurscheidt",
      "Paolo Morettin",
      "Roberto Sebastiani",
      "Andrea Passerini",
      "Antonio Vergari"
    ],
    "published": "2025-03-25T08:58:04+00:00",
    "summary": "In safety-critical applications, guaranteeing the satisfaction of constraints over continuous environments is crucial, e.g., an autonomous agent should never crash into obstacles or go off-road. Neural models struggle in the presence of these constraints, especially when they involve intricate algebraic relationships. To address this, we introduce a differentiable probabilistic layer that guarantees the satisfaction of non-convex algebraic constraints over continuous variables. This probabilistic algebraic layer (PAL) can be seamlessly plugged into any neural architecture and trained via maximum likelihood without requiring approximations. PAL defines a distribution over conjunctions and disjunctions of linear inequalities, parameterized by polynomials. This formulation enables efficient and exact renormalization via symbolic integration, which can be amortized across different data points and easily parallelized on a GPU. We showcase PAL and our integration scheme on a number of benchmarks for algebraic constraint integration and on real-world trajectory data."
  },
  {
    "title": "Irradiation Study Using QA Test Pieces of ATLAS18 ITk Strip Sensors with 80MeV Protons",
    "url": "http://arxiv.org/abs/2503.19461v1",
    "arxiv_id": "2503.19461v1",
    "authors": [
      "Y. Huang",
      "H. Li",
      "B. Crick",
      "V. Cindro",
      "A. Chisholm",
      "M. Cai",
      "H. Deng",
      "V. Fadeyev",
      "S. Hirose",
      "H. Jing",
      "B. Jiang",
      "P. Liu",
      "Y. Liu",
      "W. Lu",
      "H. Liu",
      "I. Mandi\u0107",
      "R. S. Orr",
      "X. Shi",
      "Z. Tan",
      "Y. Unno",
      "M. Ullan",
      "S. Wang",
      "Z. Xu"
    ],
    "published": "2025-03-25T08:50:35+00:00",
    "summary": "The ATLAS experiment is planning a complete replacement of its inner detector(ID) with a new all-silicon inner tracker (ITk) for the ATLAS Inner Tracker Phase-2 upgrade. The ATLAS18 silicon strip sensors are designed to operate up to the integrated luminosity of 4000 fb$^{-1}$, which corresponds to the maximum fluence of $1.6 \\times 10^{15} \\, \\text n_{\\text{eq}} / \\text{cm}^2$ (including safety factor). To enhance the quality assurance (QA) program to monitor the key properties of the sensors, the strip sensor community is considering to include China Spallation Neutron Source (CSNS) as a proton irradiation site and Institute of High Energy Physics (IHEP) as a QA test site. A total of 18 ATLAS18 ITk QA test pieces were irradiated with $6.0 \\times 10^{14}$, $1.6 \\times 10^{15}$, and $2.6 \\times 10^{15} \\, \\text n_{\\text{eq}} / \\text{cm}^2$ protons at CSNS, and measured at IHEP, including IV (leakage current-voltage), CV (capacitance-voltage) and CCE (charge collection efficiency) measurements. The upgraded irradiation setup at CSNS and measurement setup at IHEP are shown in this paper. Irradiated samples were exchanged between IHEP, Ljubljana and Birmingham to cross-check CCE measurements."
  },
  {
    "title": "Anvil: A General-Purpose Timing-Safe Hardware Description Language",
    "url": "http://arxiv.org/abs/2503.19447v1",
    "arxiv_id": "2503.19447v1",
    "authors": [
      "Jason Zhijingcheng Yu",
      "Aditya Ranjan Jha",
      "Umang Mathur",
      "Trevor E. Carlson",
      "Prateek Saxena"
    ],
    "published": "2025-03-25T08:37:45+00:00",
    "summary": "Hardware designs routinely use stateless signals which change with their underlying registers. Unintended behaviours arise when a register is mutated even when its dependent signals are expected to remain stable (unchanged). Such timing hazards are common because, with a few exceptions, existing HDLs lack the abstraction for stable values and delegate this responsibility to hardware designers, who then have to carefully decide whether a value remains unchanged, sometimes even across hardware modules. This paper proposes Anvil, an HDL which statically prevents timing hazards with a novel type system. Anvil is the only HDL we know of that guarantees timing safety without sacrificing expressiveness for cycle-level timing control or dynamic timing behaviours. Instead of abstracting away differences between registers and signals, Anvil's type system exposes them fully but captures timing relationships between register mutations and signal usages for enforcing timing safety. This, in turn, enables safe composition of communicating hardware modules by static enforcement of timing contracts that encode timing constraints on shared signals. Such timing contracts can be specified parametric on abstract time points that can vary during run-time, allowing the type system to statically express dynamic timing behaviour. We have implemented Anvil and successfully used it for implementing key timing-sensitive modules in an open-source RISC-V CPU, which demonstrates its expressiveness and practicality."
  },
  {
    "title": "AI Safety in the Eyes of the Downstream Developer: A First Look at Concerns, Practices, and Challenges",
    "url": "http://arxiv.org/abs/2503.19444v1",
    "arxiv_id": "2503.19444v1",
    "authors": [
      "Haoyu Gao",
      "Mansooreh Zahedi",
      "Wenxin Jiang",
      "Hong Yi Lin",
      "James Davis",
      "Christoph Treude"
    ],
    "published": "2025-03-25T08:35:30+00:00",
    "summary": "Pre-trained models (PTMs) have become a cornerstone of AI-based software, allowing for rapid integration and development with minimal training overhead. However, their adoption also introduces unique safety challenges, such as data leakage and biased outputs, that demand rigorous handling by downstream developers. While previous research has proposed taxonomies of AI safety concerns and various mitigation strategies, how downstream developers address these issues remains unexplored.   This study investigates downstream developers' concerns, practices and perceived challenges regarding AI safety issues during AI-based software development. To achieve this, we conducted a mixed-method study, including interviews with 18 participants, a survey of 86 practitioners, and an analysis of 874 AI incidents from the AI Incident Database. Our results reveal that while developers generally demonstrate strong awareness of AI safety concerns, their practices, especially during the preparation and PTM selection phases, are often inadequate. The lack of concrete guidelines and policies leads to significant variability in the comprehensiveness of their safety approaches throughout the development lifecycle, with additional challenges such as poor documentation and knowledge gaps, further impeding effective implementation. Based on our findings, we offer suggestions for PTM developers, AI-based software developers, researchers, and policy makers to enhance the integration of AI safety measures."
  },
  {
    "title": "AI Safety in the Eyes of the Downstream Developer: A First Look at Concerns, Practices, and Challenges",
    "url": "http://arxiv.org/abs/2503.19444v2",
    "arxiv_id": "2503.19444v2",
    "authors": [
      "Haoyu Gao",
      "Mansooreh Zahedi",
      "Wenxin Jiang",
      "Hong Yi Lin",
      "James Davis",
      "Christoph Treude"
    ],
    "published": "2025-03-25T08:35:30+00:00",
    "summary": "Pre-trained models (PTMs) have become a cornerstone of AI-based software, allowing for rapid integration and development with minimal training overhead. However, their adoption also introduces unique safety challenges, such as data leakage and biased outputs, that demand rigorous handling by downstream developers. While previous research has proposed taxonomies of AI safety concerns and various mitigation strategies, how downstream developers address these issues remains unexplored.   This study investigates downstream developers' concerns, practices and perceived challenges regarding AI safety issues during AI-based software development. To achieve this, we conducted a mixed-method study, including interviews with 18 participants, a survey of 86 practitioners, and an analysis of 874 AI incidents from the AI Incident Database. Our results reveal that while developers generally demonstrate strong awareness of AI safety concerns, their practices, especially during the preparation and PTM selection phases, are often inadequate. The lack of concrete guidelines and policies leads to significant variability in the comprehensiveness of their safety approaches throughout the development lifecycle, with additional challenges such as poor documentation and knowledge gaps, further impeding effective implementation. Based on our findings, we offer suggestions for PTM developers, AI-based software developers, researchers, and policy makers to enhance the integration of AI safety measures."
  },
  {
    "title": "Multi-Agent Deep Reinforcement Learning for Safe Autonomous Driving with RICS-Assisted MEC",
    "url": "http://arxiv.org/abs/2503.19418v1",
    "arxiv_id": "2503.19418v1",
    "authors": [
      "Xueyao Zhang",
      "Bo Yang",
      "Xuelin Cao",
      "Zhiwen Yu",
      "George C. Alexandropoulos",
      "Yan Zhang",
      "Merouane Debbah",
      "Chau Yuen"
    ],
    "published": "2025-03-25T07:53:50+00:00",
    "summary": "Environment sensing and fusion via onboard sensors are envisioned to be widely applied in future autonomous driving networks. This paper considers a vehicular system with multiple self-driving vehicles that is assisted by multi-access edge computing (MEC), where image data collected by the sensors is offloaded from cellular vehicles to the MEC server using vehicle-to-infrastructure (V2I) links. Sensory data can also be shared among surrounding vehicles via vehicle-to-vehicle (V2V) communication links. To improve spectrum utilization, the V2V links may reuse the same frequency spectrum with V2I links, which may cause severe interference. To tackle this issue, we leverage reconfigurable intelligent computational surfaces (RICSs) to jointly enable V2I reflective links and mitigate interference appearing at the V2V links. Considering the limitations of traditional algorithms in addressing this problem, such as the assumption for quasi-static channel state information, which restricts their ability to adapt to dynamic environmental changes and leads to poor performance under frequently varying channel conditions, in this paper, we formulate the problem at hand as a Markov game. Our novel formulation is applied to time-varying channels subject to multi-user interference and introduces a collaborative learning mechanism among users. The considered optimization problem is solved via a driving safety-enabled multi-agent deep reinforcement learning (DS-MADRL) approach that capitalizes on the RICS presence. Our extensive numerical investigations showcase that the proposed reinforcement learning approach achieves faster convergence and significant enhancements in both data rate and driving safety, as compared to various state-of-the-art benchmarks."
  },
  {
    "title": "Quality-focused Active Adversarial Policy for Safe Grasping in Human-Robot Interaction",
    "url": "http://arxiv.org/abs/2503.19397v1",
    "arxiv_id": "2503.19397v1",
    "authors": [
      "Chenghao Li",
      "Razvan Beuran",
      "Nak Young Chong"
    ],
    "published": "2025-03-25T07:09:31+00:00",
    "summary": "Vision-guided robot grasping methods based on Deep Neural Networks (DNNs) have achieved remarkable success in handling unknown objects, attributable to their powerful generalizability. However, these methods with this generalizability tend to recognize the human hand and its adjacent objects as graspable targets, compromising safety during Human-Robot Interaction (HRI). In this work, we propose the Quality-focused Active Adversarial Policy (QFAAP) to solve this problem. Specifically, the first part is the Adversarial Quality Patch (AQP), wherein we design the adversarial quality patch loss and leverage the grasp dataset to optimize a patch with high quality scores. Next, we construct the Projected Quality Gradient Descent (PQGD) and integrate it with the AQP, which contains only the hand region within each real-time frame, endowing the AQP with fast adaptability to the human hand shape. Through AQP and PQGD, the hand can be actively adversarial with the surrounding objects, lowering their quality scores. Therefore, further setting the quality score of the hand to zero will reduce the grasping priority of both the hand and its adjacent objects, enabling the robot to grasp other objects away from the hand without emergency stops. We conduct extensive experiments on the benchmark datasets and a cobot, showing the effectiveness of QFAAP. Our code and demo videos are available here: https://github.com/clee-jaist/QFAAP."
  },
  {
    "title": "Optimal Parameter Adaptation for Safety-Critical Control via Safe Barrier Bayesian Optimization",
    "url": "http://arxiv.org/abs/2503.19349v1",
    "arxiv_id": "2503.19349v1",
    "authors": [
      "Shengbo Wang",
      "Ke Li",
      "Zheng Yan",
      "Zhenyuan Guo",
      "Song Zhu",
      "Guanghui Wen",
      "Shiping Wen"
    ],
    "published": "2025-03-25T04:56:17+00:00",
    "summary": "Safety is of paramount importance in control systems to avoid costly risks and catastrophic damages. The control barrier function (CBF) method, a promising solution for safety-critical control, poses a new challenge of enhancing control performance due to its direct modification of original control design and the introduction of uncalibrated parameters. In this work, we shed light on the crucial role of configurable parameters in the CBF method for performance enhancement with a systematical categorization. Based on that, we propose a novel framework combining the CBF method with Bayesian optimization (BO) to optimize the safe control performance. Considering feasibility/safety-critical constraints, we develop a safe version of BO using the barrier-based interior method to efficiently search for promising feasible configurable parameters. Furthermore, we provide theoretical criteria of our framework regarding safety and optimality. An essential advantage of our framework lies in that it can work in model-agnostic environments, leaving sufficient flexibility in designing objective and constraint functions. Finally, simulation experiments on swing-up control and high-fidelity adaptive cruise control are conducted to demonstrate the effectiveness of our framework."
  },
  {
    "title": "Process or Result? Manipulated Ending Tokens Can Mislead Reasoning LLMs to Ignore the Correct Reasoning Steps",
    "url": "http://arxiv.org/abs/2503.19326v1",
    "arxiv_id": "2503.19326v1",
    "authors": [
      "Yu Cui",
      "Bryan Hooi",
      "Yujun Cai",
      "Yiwei Wang"
    ],
    "published": "2025-03-25T03:43:11+00:00",
    "summary": "Recent reasoning large language models (LLMs) have demonstrated remarkable improvements in mathematical reasoning capabilities through long Chain-of-Thought. The reasoning tokens of these models enable self-correction within reasoning chains, enhancing robustness. This motivates our exploration: how vulnerable are reasoning LLMs to subtle errors in their input reasoning chains? We introduce \"Compromising Thought\" (CPT), a vulnerability where models presented with reasoning tokens containing manipulated calculation results tend to ignore correct reasoning steps and adopt incorrect results instead. Through systematic evaluation across multiple reasoning LLMs, we design three increasingly explicit prompting methods to measure CPT resistance, revealing that models struggle significantly to identify and correct these manipulations. Notably, contrary to existing research suggesting structural alterations affect model performance more than content modifications, we find that local ending token manipulations have greater impact on reasoning outcomes than structural changes. Moreover, we discover a security vulnerability in DeepSeek-R1 where tampered reasoning tokens can trigger complete reasoning cessation. Our work enhances understanding of reasoning robustness and highlights security considerations for reasoning-intensive applications."
  },
  {
    "title": "A Reliable and Efficient 5G Vehicular MEC: Guaranteed Task Completion with Minimal Latency",
    "url": "http://arxiv.org/abs/2503.19320v1",
    "arxiv_id": "2503.19320v1",
    "authors": [
      "Mahsa Paknejad",
      "Parisa Fard Moshiri",
      "Murat Simsek",
      "Burak Kantarci",
      "Hussein T. Mouftah"
    ],
    "published": "2025-03-25T03:27:10+00:00",
    "summary": "This paper explores the advancement of Vehicular Edge Computing (VEC) as a tailored application of Mobile Edge Computing (MEC) for the automotive industry, addressing the rising demand for real-time processing in connected and autonomous vehicles. VEC brings computational resources closer to vehicles, reducing data processing delays crucial for safety-critical applications such as autonomous driving and intelligent traffic management. However, the challenge lies in managing the high and dynamic task load generated by vehicles' data streams. We focus on enhancing task offloading and scheduling techniques to optimize both communication and computation latencies in VEC networks. Our approach involves implementing task scheduling algorithms, including First-Come, First-Served (FCFS), Shortest Deadline First (SDF), and Particle Swarm Optimization (PSO) for optimization. Additionally, we divide portions of tasks between the MEC servers and vehicles to reduce the number of dropped tasks and improve real-time adaptability. This paper also compares fixed and shared bandwidth scenarios to manage transmission efficiency under varying loads. Our findings indicate that MEC+Local (partitioning) scenario significantly outperforms MEC-only scenario by ensuring the completion of all tasks, resulting in a zero task drop ratio. The MEC-only scenario demonstrates approximately 5.65% better average end-to-end latency compared to the MEC+Local (partitioning) scenario when handling 200 tasks. However, this improvement comes at the cost of dropping a significant number of tasks (109 out of 200). Additionally, allocating shared bandwidth helps to slightly decrease transmission waiting time compared to using fixed bandwidth."
  },
  {
    "title": "NeoRL-2: Near Real-World Benchmarks for Offline Reinforcement Learning with Extended Realistic Scenarios",
    "url": "http://arxiv.org/abs/2503.19267v1",
    "arxiv_id": "2503.19267v1",
    "authors": [
      "Songyi Gao",
      "Zuolin Tu",
      "Rong-Jun Qin",
      "Yi-Hao Sun",
      "Xiong-Hui Chen",
      "Yang Yu"
    ],
    "published": "2025-03-25T02:01:54+00:00",
    "summary": "Offline reinforcement learning (RL) aims to learn from historical data without requiring (costly) access to the environment. To facilitate offline RL research, we previously introduced NeoRL, which highlighted that datasets from real-world tasks are often conservative and limited. With years of experience applying offline RL to various domains, we have identified additional real-world challenges. These include extremely conservative data distributions produced by deployed control systems, delayed action effects caused by high-latency transitions, external factors arising from the uncontrollable variance of transitions, and global safety constraints that are difficult to evaluate during the decision-making process. These challenges are underrepresented in previous benchmarks but frequently occur in real-world tasks. To address this, we constructed the extended Near Real-World Offline RL Benchmark (NeoRL-2), which consists of 7 datasets from 7 simulated tasks along with their corresponding evaluation simulators. Benchmarking results from state-of-the-art offline RL approaches demonstrate that current methods often struggle to outperform the data-collection behavior policy, highlighting the need for more effective methods. We hope NeoRL-2 will accelerate the development of reinforcement learning algorithms for real-world applications. The benchmark project page is available at https://github.com/polixir/NeoRL2."
  },
  {
    "title": "Risk-Aware Adaptive Control Barrier Functions for Safe Control of Nonlinear Systems under Stochastic Uncertainty",
    "url": "http://arxiv.org/abs/2503.19205v1",
    "arxiv_id": "2503.19205v1",
    "authors": [
      "Shuo Liu",
      "Calin A. Belta"
    ],
    "published": "2025-03-24T23:11:32+00:00",
    "summary": "This paper addresses the challenge of ensuring safety in stochastic control systems with high-relative-degree constraints while maintaining feasibility and mitigating conservatism in risk evaluation. Control Barrier Functions (CBFs) provide an effective framework for enforcing safety constraints in nonlinear systems. However, existing methods struggle with feasibility issues and multi-step uncertainties. To address these challenges, we introduce Risk-aware Adaptive CBFs (RACBFs), which integrate Discrete-time Auxiliary-Variable adaptive CBFs (DAVCBFs) with coherent risk measures. DAVCBFs introduce auxiliary variables to improve the feasibility of the optimal control problem, while RACBFs incorporate risk-aware formulations to balance safety and risk evaluation performance. By extending discrete-time high-order CBF constraints over multiple steps, RACBFs effectively handle multi-step uncertainties that propagate through the system dynamics. We demonstrate the effectiveness of our approach on a stochastic unicycle system, showing that RACBFs maintain safety and feasibility while reducing unnecessary conservatism compared to standard robust formulations of discrete-time CBF methods."
  },
  {
    "title": "Cooperative Control of Multi-Quadrotors for Transporting Cable-Suspended Payloads: Obstacle-Aware Planning and Event-Based Nonlinear Model Predictive Control",
    "url": "http://arxiv.org/abs/2503.19135v1",
    "arxiv_id": "2503.19135v1",
    "authors": [
      "Tohid Kargar Tasooji",
      "Sakineh Khodadadi",
      "Guangjun Liu",
      "Richard Wang"
    ],
    "published": "2025-03-24T20:45:24+00:00",
    "summary": "This paper introduces a novel methodology for the cooperative control of multiple quadrotors transporting cablesuspended payloads, emphasizing obstacle-aware planning and event-based Nonlinear Model Predictive Control (NMPC). Our approach integrates trajectory planning with real-time control through a combination of the A* algorithm for global path planning and NMPC for local control, enhancing trajectory adaptability and obstacle avoidance. We propose an advanced event-triggered control system that updates based on events identified through dynamically generated environmental maps. These maps are constructed using a dual-camera setup, which includes multi-camera systems for static obstacle detection and event cameras for high-resolution, low-latency detection of dynamic obstacles. This design is crucial for addressing fast-moving and transient obstacles that conventional cameras may overlook, particularly in environments with rapid motion and variable lighting conditions. When new obstacles are detected, the A* algorithm recalculates waypoints based on the updated map, ensuring safe and efficient navigation. This real-time obstacle detection and map updating integration allows the system to adaptively respond to environmental changes, markedly improving safety and navigation efficiency. The system employs SLAM and object detection techniques utilizing data from multi-cameras, event cameras, and IMUs for accurate localization and comprehensive environmental mapping. The NMPC framework adeptly manages the complex dynamics of multiple quadrotors and suspended payloads, incorporating safety constraints to maintain dynamic feasibility and stability. Extensive simulations validate the proposed approach, demonstrating significant enhancements in energy efficiency, computational resource management, and responsiveness."
  },
  {
    "title": "MIRAGE: Multimodal Immersive Reasoning and Guided Exploration for Red-Team Jailbreak Attacks",
    "url": "http://arxiv.org/abs/2503.19134v1",
    "arxiv_id": "2503.19134v1",
    "authors": [
      "Wenhao You",
      "Bryan Hooi",
      "Yiwei Wang",
      "Youke Wang",
      "Zong Ke",
      "Ming-Hsuan Yang",
      "Zi Huang",
      "Yujun Cai"
    ],
    "published": "2025-03-24T20:38:42+00:00",
    "summary": "While safety mechanisms have significantly progressed in filtering harmful text inputs, MLLMs remain vulnerable to multimodal jailbreaks that exploit their cross-modal reasoning capabilities. We present MIRAGE, a novel multimodal jailbreak framework that exploits narrative-driven context and role immersion to circumvent safety mechanisms in Multimodal Large Language Models (MLLMs). By systematically decomposing the toxic query into environment, role, and action triplets, MIRAGE constructs a multi-turn visual storytelling sequence of images and text using Stable Diffusion, guiding the target model through an engaging detective narrative. This process progressively lowers the model's defences and subtly guides its reasoning through structured contextual cues, ultimately eliciting harmful responses. In extensive experiments on the selected datasets with six mainstream MLLMs, MIRAGE achieves state-of-the-art performance, improving attack success rates by up to 17.5% over the best baselines. Moreover, we demonstrate that role immersion and structured semantic reconstruction can activate inherent model biases, facilitating the model's spontaneous violation of ethical safeguards. These results highlight critical weaknesses in current multimodal safety mechanisms and underscore the urgent need for more robust defences against cross-modal threats."
  },
  {
    "title": "Forward propagation of a push through a row of people",
    "url": "http://arxiv.org/abs/2503.19104v1",
    "arxiv_id": "2503.19104v1",
    "authors": [
      "Sina Feldmann",
      "Juliane Adrian"
    ],
    "published": "2025-03-24T19:47:00+00:00",
    "summary": "Security plays a crucial role when it comes to planning large events such as concerts, sporting tournaments, pilgrims, or demonstrations. Monitoring and controlling pedestrian dynamics can prevent dangerous situations from occurring. However, little is known about the specific factors that contribute to harmful situations. For example, the individual response of a person to external forces in dense crowds is not well studied. In order to address this gap in knowledge, we conducted a series of experiments to examine how a push propagates through a row of people and how it affects the participants. We recorded 2D head trajectories and 3D motion capturing data. To ensure that different trials can be compared to one another, we measured the force at the impact. We find that that the propagation distance as well as the propagation speed of the push are mainly functions of the strength of the push and in particular the latter depends on the initial arm posture of the pushed participants. Our results can contribute to a deeper understanding of the microscopic causes of macroscopic phenomena in large groups, and can be applied to inform models of pedestrian dynamics or validate them, ultimately improving crowd safety."
  },
  {
    "title": "LookAhead Tuning: Safer Language Models via Partial Answer Previews",
    "url": "http://arxiv.org/abs/2503.19041v1",
    "arxiv_id": "2503.19041v1",
    "authors": [
      "Kangwei Liu",
      "Mengru Wang",
      "Yujie Luo",
      "Lin Yuan",
      "Mengshu Sun",
      "Ningyu Zhang",
      "Lei Liang",
      "Zhiqiang Zhang",
      "Jun Zhou",
      "Huajun Chen"
    ],
    "published": "2025-03-24T18:11:42+00:00",
    "summary": "Fine-tuning enables large language models (LLMs) to adapt to specific domains, but often undermines their previously established safety alignment. To mitigate the degradation of model safety during fine-tuning, we introduce LookAhead Tuning, which comprises two simple, low-resource, and effective data-driven methods that modify training data by previewing partial answer prefixes. Both methods aim to preserve the model's inherent safety mechanisms by minimizing perturbations to initial token distributions. Comprehensive experiments demonstrate that LookAhead Tuning effectively maintains model safety without sacrificing robust performance on downstream tasks. Our findings position LookAhead Tuning as a reliable and efficient solution for the safe and effective adaptation of LLMs. Code is released at https://github.com/zjunlp/LookAheadTuning."
  },
  {
    "title": "Enhancing V2X Communications with UAV-mounted Reconfigurable Intelligent Surfaces",
    "url": "http://arxiv.org/abs/2503.19038v1",
    "arxiv_id": "2503.19038v1",
    "authors": [
      "Salim Janji",
      "Pawe\u0142 Sroka",
      "Adrian Kliks"
    ],
    "published": "2025-03-24T18:09:07+00:00",
    "summary": "This paper addresses the crucial need for reliable wireless communication in vehicular networks, particularly vital for the safety and efficacy of (semi-)autonomous driving amid increasing traffic. We explore the use of Reconfigurable Intelligent Surfaces (RISes) mounted on Drone Relay Stations (DRS) to enhance communication reliability. Our study formulates an optimization problem to pinpoint the optimal location and orientation of the DRS, thereby creating an additional propagation path for vehicle-to-everything (V2X) communications. We introduce a heuristic approach that combines trajectory optimization for DRS positioning and a Q-learning scheme for RIS orientation. Our results not only confirm the convergence of the Q-learning algorithm but also demonstrate significant communication improvements achieved by integrating a DRS into V2X networks."
  },
  {
    "title": "SimpleRL-Zoo: Investigating and Taming Zero Reinforcement Learning for Open Base Models in the Wild",
    "url": "http://arxiv.org/abs/2503.18892v1",
    "arxiv_id": "2503.18892v1",
    "authors": [
      "Weihao Zeng",
      "Yuzhen Huang",
      "Qian Liu",
      "Wei Liu",
      "Keqing He",
      "Zejun Ma",
      "Junxian He"
    ],
    "published": "2025-03-24T17:06:10+00:00",
    "summary": "DeepSeek-R1 has shown that long chain-of-thought (CoT) reasoning can naturally emerge through a simple reinforcement learning (RL) framework with rule-based rewards, where the training may directly start from the base models-a paradigm referred to as zero RL training. Most recent efforts to reproduce zero RL training have primarily focused on the Qwen2.5 model series, which may not be representative as we find the base models already exhibit strong instruction-following and self-reflection abilities. In this work, we investigate zero RL training across 10 diverse base models, spanning different families and sizes including LLama3-8B, Mistral-7B/24B, DeepSeek-Math-7B, Qwen2.5-math-7B, and all Qwen2.5 models from 0.5B to 32B. Leveraging several key design strategies-such as adjusting format reward and controlling query difficulty-we achieve substantial improvements in both reasoning accuracy and response length across most settings. However, by carefully monitoring the training dynamics, we observe that different base models exhibit distinct patterns during training. For instance, the increased response length does not always correlate with the emergence of certain cognitive behaviors such as verification (i.e., the \"aha moment\"). Notably, we observe the \"aha moment\" for the first time in small models not from the Qwen family. We share the key designs that enable successful zero RL training, along with our findings and practices. To facilitate further research, we open-source the code, models, and analysis tools."
  },
  {
    "title": "I Have Covered All the Bases Here: Interpreting Reasoning Features in Large Language Models via Sparse Autoencoders",
    "url": "http://arxiv.org/abs/2503.18878v1",
    "arxiv_id": "2503.18878v1",
    "authors": [
      "Andrey Galichin",
      "Alexey Dontsov",
      "Polina Druzhinina",
      "Anton Razzhigaev",
      "Oleg Y. Rogov",
      "Elena Tutubalina",
      "Ivan Oseledets"
    ],
    "published": "2025-03-24T16:54:26+00:00",
    "summary": "Large Language Models (LLMs) have achieved remarkable success in natural language processing. Recent advances have led to the developing of a new class of reasoning LLMs; for example, open-source DeepSeek-R1 has achieved state-of-the-art performance by integrating deep thinking and complex reasoning. Despite these impressive capabilities, the internal reasoning mechanisms of such models remain unexplored. In this work, we employ Sparse Autoencoders (SAEs), a method to learn a sparse decomposition of latent representations of a neural network into interpretable features, to identify features that drive reasoning in the DeepSeek-R1 series of models. First, we propose an approach to extract candidate ''reasoning features'' from SAE representations. We validate these features through empirical analysis and interpretability methods, demonstrating their direct correlation with the model's reasoning abilities. Crucially, we demonstrate that steering these features systematically enhances reasoning performance, offering the first mechanistic account of reasoning in LLMs. Code available at https://github.com/AIRI-Institute/SAE-Reasoning"
  },
  {
    "title": "CRCL: Causal Representation Consistency Learning for Anomaly Detection in Surveillance Videos",
    "url": "http://arxiv.org/abs/2503.18808v1",
    "arxiv_id": "2503.18808v1",
    "authors": [
      "Yang Liu",
      "Hongjin Wang",
      "Zepu Wang",
      "Xiaoguang Zhu",
      "Jing Liu",
      "Peng Sun",
      "Rui Tang",
      "Jianwei Du",
      "Victor C. M. Leung",
      "Liang Song"
    ],
    "published": "2025-03-24T15:50:19+00:00",
    "summary": "Video Anomaly Detection (VAD) remains a fundamental yet formidable task in the video understanding community, with promising applications in areas such as information forensics and public safety protection. Due to the rarity and diversity of anomalies, existing methods only use easily collected regular events to model the inherent normality of normal spatial-temporal patterns in an unsupervised manner. Previous studies have shown that existing unsupervised VAD models are incapable of label-independent data offsets (e.g., scene changes) in real-world scenarios and may fail to respond to light anomalies due to the overgeneralization of deep neural networks. Inspired by causality learning, we argue that there exist causal factors that can adequately generalize the prototypical patterns of regular events and present significant deviations when anomalous instances occur. In this regard, we propose Causal Representation Consistency Learning (CRCL) to implicitly mine potential scene-robust causal variable in unsupervised video normality learning. Specifically, building on the structural causal models, we propose scene-debiasing learning and causality-inspired normality learning to strip away entangled scene bias in deep representations and learn causal video normality, respectively. Extensive experiments on benchmarks validate the superiority of our method over conventional deep representation learning. Moreover, ablation studies and extension validation show that the CRCL can cope with label-independent biases in multi-scene settings and maintain stable performance with only limited training data available."
  },
  {
    "title": "Mechanistic Interpretability of Fine-Tuned Vision Transformers on Distorted Images: Decoding Attention Head Behavior for Transparent and Trustworthy AI",
    "url": "http://arxiv.org/abs/2503.18762v1",
    "arxiv_id": "2503.18762v1",
    "authors": [
      "Nooshin Bahador"
    ],
    "published": "2025-03-24T15:11:24+00:00",
    "summary": "Mechanistic interpretability improves the safety, reliability, and robustness of large AI models. This study examined individual attention heads in vision transformers (ViTs) fine tuned on distorted 2D spectrogram images containing non relevant content (axis labels, titles, color bars). By introducing extraneous features, the study analyzed how transformer components processed unrelated information, using mechanistic interpretability to debug issues and reveal insights into transformer architectures. Attention maps assessed head contributions across layers. Heads in early layers (1 to 3) showed minimal task impact with ablation increased MSE loss slightly ({\\mu}=0.11%, {\\sigma}=0.09%), indicating focus on less critical low level features. In contrast, deeper heads (e.g., layer 6) caused a threefold higher loss increase ({\\mu}=0.34%, {\\sigma}=0.02%), demonstrating greater task importance. Intermediate layers (6 to 11) exhibited monosemantic behavior, attending exclusively to chirp regions. Some early heads (1 to 4) were monosemantic but non task relevant (e.g. text detectors, edge or corner detectors). Attention maps distinguished monosemantic heads (precise chirp localization) from polysemantic heads (multiple irrelevant regions). These findings revealed functional specialization in ViTs, showing how heads processed relevant vs. extraneous information. By decomposing transformers into interpretable components, this work enhanced model understanding, identified vulnerabilities, and advanced safer, more transparent AI."
  },
  {
    "title": "Deep learning-based identification of precipitation clouds from all-sky camera data for observatory safety",
    "url": "http://arxiv.org/abs/2503.18670v1",
    "arxiv_id": "2503.18670v1",
    "authors": [
      "Mohammad H. Zhoolideh Haghighi",
      "Alireza Ghasrimanesh",
      "Habib Khosroshahi"
    ],
    "published": "2025-03-24T13:40:51+00:00",
    "summary": "For monitoring the night sky conditions, wide-angle all-sky cameras are used in most astronomical observatories to monitor the sky cloudiness. In this manuscript, we apply a deep-learning approach for automating the identification of precipitation clouds in all-sky camera data as a cloud warning system. We construct our original training and test sets using the all-sky camera image archive of the Iranian National Observatory (INO). The training and test set images are labeled manually based on their potential rainfall and their distribution in the sky. We train our model on a set of roughly 2445 images taken by the INO all-sky camera through the deep learning method based on the EfficientNet network. Our model reaches an average accuracy of 99\\% in determining the cloud rainfall's potential and an accuracy of 96\\% for cloud coverage. To enable a comprehensive comparison and evaluate the performance of alternative architectures for the task, we additionally trained three models LeNet, DeiT, and AlexNet. This approach can be used for early warning of incoming dangerous clouds toward telescopes and harnesses the power of deep learning to automatically analyze vast amounts of all-sky camera data and accurately identify precipitation clouds formations. Our trained model can be deployed for real-time analysis, enabling the rapid identification of potential threats, and offering a scaleable solution that can improve our ability to safeguard telescopes and instruments in observatories. This is important now that numerous small and medium-sized telescopes are increasingly integrated with smart control systems to reduce manual operation."
  },
  {
    "title": "AgentSpec: Customizable Runtime Enforcement for Safe and Reliable LLM Agents",
    "url": "http://arxiv.org/abs/2503.18666v1",
    "arxiv_id": "2503.18666v1",
    "authors": [
      "Haoyu Wang",
      "Christopher M. Poskitt",
      "Jun Sun"
    ],
    "published": "2025-03-24T13:31:48+00:00",
    "summary": "Agents built on LLMs are increasingly deployed across diverse domains, automating complex decision-making and task execution. However, their autonomy introduces safety risks, including security vulnerabilities, legal violations, and unintended harmful actions. Existing mitigation methods, such as model-based safeguards and early enforcement strategies, fall short in robustness, interpretability, and adaptability. To address these challenges, we propose AgentSpec, a lightweight domain-specific language for specifying and enforcing runtime constraints on LLM agents. With AgentSpec, users define structured rules that incorporate triggers, predicates, and enforcement mechanisms, ensuring agents operate within predefined safety boundaries. We implement AgentSpec across multiple domains, including code execution, embodied agents, and autonomous driving, demonstrating its adaptability and effectiveness. Our evaluation shows that AgentSpec successfully prevents unsafe executions in over 90% of code agent cases, eliminates all hazardous actions in embodied agent tasks, and enforces 100% compliance by autonomous vehicles (AVs). Despite its strong safety guarantees, AgentSpec remains computationally lightweight, with overheads in milliseconds. By combining interpretability, modularity, and efficiency, AgentSpec provides a practical and scalable solution for enforcing LLM agent safety across diverse applications. We also automate the generation of rules using LLMs and assess their effectiveness. Our evaluation shows that the rules generated by OpenAI o1 achieve a precision of 95.56% and recall of 70.96% for embodied agents, successfully identifying 87.26% of the risky code, and prevent AVs from breaking laws in 5 out of 8 scenarios."
  },
  {
    "title": "Self-Reported Confidence of Large Language Models in Gastroenterology: Analysis of Commercial, Open-Source, and Quantized Models",
    "url": "http://arxiv.org/abs/2503.18562v1",
    "arxiv_id": "2503.18562v1",
    "authors": [
      "Nariman Naderi",
      "Seyed Amir Ahmad Safavi-Naini",
      "Thomas Savage",
      "Zahra Atf",
      "Peter Lewis",
      "Girish Nadkarni",
      "Ali Soroush"
    ],
    "published": "2025-03-24T11:16:41+00:00",
    "summary": "This study evaluated self-reported response certainty across several large language models (GPT, Claude, Llama, Phi, Mistral, Gemini, Gemma, and Qwen) using 300 gastroenterology board-style questions. The highest-performing models (GPT-o1 preview, GPT-4o, and Claude-3.5-Sonnet) achieved Brier scores of 0.15-0.2 and AUROC of 0.6. Although newer models demonstrated improved performance, all exhibited a consistent tendency towards overconfidence. Uncertainty estimation presents a significant challenge to the safe use of LLMs in healthcare. Keywords: Large Language Models; Confidence Elicitation; Artificial Intelligence; Gastroenterology; Uncertainty Quantification"
  },
  {
    "title": "Constraint Horizon in Model Predictive Control",
    "url": "http://arxiv.org/abs/2503.18521v1",
    "arxiv_id": "2503.18521v1",
    "authors": [
      "Allan Andre Do Nascimento",
      "Han Wang",
      "Antonis Papachristodoulou",
      "Kostas Margellos"
    ],
    "published": "2025-03-24T10:21:08+00:00",
    "summary": "In this work, we propose a Model Predictive Control (MPC) formulation incorporating two distinct horizons: a prediction horizon and a constraint horizon. This approach enables a deeper understanding of how constraints influence key system properties such as suboptimality, without compromising recursive feasibility and constraint satisfaction. In this direction, our contributions are twofold. First, we provide a framework to estimate closed-loop optimality as a function of the number of enforced constraints. This is a generalization of existing results by considering partial constraint enforcement over the prediction horizon. Second, when adopting this general framework under the lens of safety-critical applications, our method improves conventional Control Barrier Function (CBF) based approaches. It mitigates myopic behaviour in Quadratic Programming (QP)-CBF schemes, and resolves compatibility issues between Control Lyapunov Function (CLF) and CBF constraints via the prediction horizon used in the optimization. We show the efficacy of the method via numerical simulations for a safety critical application."
  },
  {
    "title": "Verbal Process Supervision Elicits Better Coding Agents",
    "url": "http://arxiv.org/abs/2503.18494v1",
    "arxiv_id": "2503.18494v1",
    "authors": [
      "Hao-Yuan Chen",
      "Cheng-Pong Huang",
      "Jui-Ming Yao"
    ],
    "published": "2025-03-24T09:48:59+00:00",
    "summary": "The emergence of large language models and their applications as AI agents have significantly advanced state-of-the-art code generation benchmarks, transforming modern software engineering tasks. However, even with test-time computed reasoning models, these systems still struggle with complex software engineering challenges. This work introduces CURA, a code understanding and reasoning agent system enhanced with verbal process supervision (VPS), achieving a 3.65\\% improvement over baseline models on challenging benchmarks like BigCodeBench. Furthermore, CURA, when paired with the o3-mini model and VPS techniques, attains state-of-the-art performance. This work represents a step forward in integrating reasoning-driven architectures with LLM-based code generation, enabling agentic reasoning for language models to solve complex software engineering tasks."
  },
  {
    "title": "PM4Bench: A Parallel Multilingual Multi-Modal Multi-task Benchmark for Large Vision Language Model",
    "url": "http://arxiv.org/abs/2503.18484v1",
    "arxiv_id": "2503.18484v1",
    "authors": [
      "Junyuan Gao",
      "Jiahe Song",
      "Jiang Wu",
      "Runchuan Zhu",
      "Guanlin Shen",
      "Shasha Wang",
      "Xingjian Wei",
      "Haote Yang",
      "Songyang Zhang",
      "Weijia Li",
      "Bin Wang",
      "Dahua Lin",
      "Lijun Wu",
      "Conghui He"
    ],
    "published": "2025-03-24T09:38:37+00:00",
    "summary": "Existing multilingual benchmarks for Large Vision Language Models (LVLMs) suffer from limitations including language-specific content biases, disjointed multimodal input formats, and a lack of safety evaluation. To address these gaps, we propose PM4Bench, the first Parallel Multilingual Multi-Modal Multi-task Benchmark for LVLMs. PM4Bench features a parallel corpus design across 10 languages, enabling fair and accurate cross-lingual comparisons. It includes the vision setting where text and queries are embedded in images, requiring LVLMs to simultaneously \"see\", \"read\", and \"think\", aligning with real-world applications. Additionally, PM\\textsuperscript{4}Bench incorporates safety evaluations, addressing critical oversight in existing multilingual benchmarks. Using PM4Bench, we evaluate 11 mainstream LVLMs, revealing significant cross-linguistic performance disparities, particularly in vision settings, and identifying OCR capability as a key determinant of these imbalances. We will release PM4Bench at https://github.com/opendatalab/PM4Bench ."
  },
  {
    "title": "Near-optimal Active Reconstruction",
    "url": "http://arxiv.org/abs/2503.18999v1",
    "arxiv_id": "2503.18999v1",
    "authors": [
      "Daniel Yang"
    ],
    "published": "2025-03-24T09:17:53+00:00",
    "summary": "With the growing practical interest in vision-based tasks for autonomous systems, the need for efficient and complex methods becomes increasingly larger. In the rush to develop new methods with the aim to outperform the current state of the art, an analysis of the underlying theory is often neglected and simply replaced with empirical evaluations in simulated or real-world experiments. While such methods might yield favorable performance in practice, they are often less well understood, which prevents them from being applied in safety-critical systems. The goal of this work is to design an algorithm for the Next Best View (NBV) problem in the context of active object reconstruction, for which we can provide qualitative performance guarantees with respect to true optimality. To the best of our knowledge, no previous work in this field addresses such an analysis for their proposed methods. Based on existing work on Gaussian process optimization, we rigorously derive sublinear bounds for the cumulative regret of our algorithm, which guarantees near-optimality. Complementing this, we evaluate the performance of our algorithm empirically within our simulation framework. We further provide additional insights through an extensive study of potential objective functions and analyze the differences to the results of related work."
  },
  {
    "title": "Extended Visibility of Autonomous Vehicles via Optimized Cooperative Perception under Imperfect Communication",
    "url": "http://arxiv.org/abs/2503.18192v1",
    "arxiv_id": "2503.18192v1",
    "authors": [
      "Ahmad Sarlak",
      "Rahul Amin",
      "Abolfazl Razi"
    ],
    "published": "2025-03-23T20:22:14+00:00",
    "summary": "Autonomous Vehicles (AVs) rely on individual perception systems to navigate safely. However, these systems face significant challenges in adverse weather conditions, complex road geometries, and dense traffic scenarios. Cooperative Perception (CP) has emerged as a promising approach to extending the perception quality of AVs by jointly processing shared camera feeds and sensor readings across multiple vehicles. This work presents a novel CP framework designed to optimize vehicle selection and networking resource utilization under imperfect communications. Our optimized CP formation considers critical factors such as the helper vehicles' spatial position, visual range, motion blur, and available communication budgets. Furthermore, our resource optimization module allocates communication channels while adjusting power levels to maximize data flow efficiency between the ego and helper vehicles, considering realistic models of modern vehicular communication systems, such as LTE and 5G NR-V2X. We validate our approach through extensive experiments on pedestrian detection in challenging scenarios, using synthetic data generated by the CARLA simulator. The results demonstrate that our method significantly improves upon the perception quality of individual AVs with about 10% gain in detection accuracy. This substantial gain uncovers the unleashed potential of CP to enhance AV safety and performance in complex situations."
  },
  {
    "title": "Training A Neural Network For Partially Occluded Road Sign Identification In The Context Of Autonomous Vehicles",
    "url": "http://arxiv.org/abs/2503.18177v1",
    "arxiv_id": "2503.18177v1",
    "authors": [
      "Gulnaz Gimaletdinova",
      "Dim Shaiakhmetov",
      "Madina Akpaeva",
      "Mukhammadmuso Abduzhabbarov",
      "Kadyrmamat Momunov"
    ],
    "published": "2025-03-23T19:25:56+00:00",
    "summary": "The increasing number of autonomous vehicles and the rapid development of computer vision technologies underscore the particular importance of conducting research on the accuracy of traffic sign recognition. Numerous studies in this field have already achieved significant results, demonstrating high effectiveness in addressing traffic sign recognition tasks. However, the task becomes considerably more complex when a sign is partially obscured by surrounding objects, such as tree branches, billboards, or other elements of the urban environment. In our study, we investigated how partial occlusion of traffic signs affects their recognition. For this purpose, we collected a dataset comprising 5,746 images, including both fully visible and partially occluded signs, and made it publicly available. Using this dataset, we compared the performance of our custom convolutional neural network (CNN), which achieved 96% accuracy, with models trained using transfer learning. The best result was obtained by VGG16 with full layer unfreezing, reaching 99% accuracy. Additional experiments revealed that models trained solely on fully visible signs lose effectiveness when recognizing occluded signs. This highlights the critical importance of incorporating real-world data with partial occlusion into training sets to ensure robust model performance in complex practical scenarios and to enhance the safety of autonomous driving."
  },
  {
    "title": "HH4AI: A methodological Framework for AI Human Rights impact assessment under the EUAI ACT",
    "url": "http://arxiv.org/abs/2503.18994v1",
    "arxiv_id": "2503.18994v1",
    "authors": [
      "Paolo Ceravolo",
      "Ernesto Damiani",
      "Maria Elisa D'Amico",
      "Bianca de Teffe Erb",
      "Simone Favaro",
      "Nannerel Fiano",
      "Paolo Gambatesa",
      "Simone La Porta",
      "Samira Maghool",
      "Lara Mauri",
      "Niccolo Panigada",
      "Lorenzo Maria Ratto Vaquer",
      "Marta A. Tamborini"
    ],
    "published": "2025-03-23T19:10:14+00:00",
    "summary": "This paper introduces the HH4AI Methodology, a structured approach to assessing the impact of AI systems on human rights, focusing on compliance with the EU AI Act and addressing technical, ethical, and regulatory challenges. The paper highlights AIs transformative nature, driven by autonomy, data, and goal-oriented design, and how the EU AI Act promotes transparency, accountability, and safety. A key challenge is defining and assessing \"high-risk\" AI systems across industries, complicated by the lack of universally accepted standards and AIs rapid evolution.   To address these challenges, the paper explores the relevance of ISO/IEC and IEEE standards, focusing on risk management, data quality, bias mitigation, and governance. It proposes a Fundamental Rights Impact Assessment (FRIA) methodology, a gate-based framework designed to isolate and assess risks through phases including an AI system overview, a human rights checklist, an impact assessment, and a final output phase. A filtering mechanism tailors the assessment to the system's characteristics, targeting areas like accountability, AI literacy, data governance, and transparency.   The paper illustrates the FRIA methodology through a fictional case study of an automated healthcare triage service. The structured approach enables systematic filtering, comprehensive risk assessment, and mitigation planning, effectively prioritizing critical risks and providing clear remediation strategies. This promotes better alignment with human rights principles and enhances regulatory compliance."
  },
  {
    "title": "SRMIR: Shadow Reward Models Based on Introspective Reasoning for LLM Alignment",
    "url": "http://arxiv.org/abs/2503.18991v1",
    "arxiv_id": "2503.18991v1",
    "authors": [
      "Ruoxi Cheng",
      "Shuirong Cao"
    ],
    "published": "2025-03-23T16:40:29+00:00",
    "summary": "Aligning large language models (LLMs) with human preferences and values is vital for application. However, current alignment methods face three main limitations: (1) reliance on costly human annotation; (2) alignment tax; (3) shallow alignment vulnerable to jailbreak attacks. Additionally, current alignment datasets often suffer from uneven distributions, leading to overrepresentation of some topics and neglect of others. To address these issues, we propose SRMIR (Shadow Reward Models Based on Introspective Reasoning), inspired by shadow models in membership inference attacks. We first construct a balanced safety Chain of Draft (CoD) dataset across $7$ harmful types with structured prompt leveraging the introspective reasoning capabilities of LLMs, then train a set of specialized reward models to guide policy optimization through Group Relative Policy Optimization (GRPO). We apply two strategies, linear combination and categorized approach, to integrate shadow reward models for policy optimization. By comparison, we find that the latter achieves superior alignment despite higher computational costs. Experiments across several LLMs demonstrate SRMIR significantly outperforms existing methods."
  },
  {
    "title": "GeoBenchX: Benchmarking LLMs for Multistep Geospatial Tasks",
    "url": "http://arxiv.org/abs/2503.18129v1",
    "arxiv_id": "2503.18129v1",
    "authors": [
      "Varvara Krechetova",
      "Denis Kochedykov"
    ],
    "published": "2025-03-23T16:20:14+00:00",
    "summary": "In this paper, we establish a benchmark for evaluating large language models (LLMs) on multi-step geospatial tasks relevant to commercial GIS practitioners. We assess seven leading commercial LLMs (Sonnet 3.5 and 3.7, Haiku 3.5, Gemini 2.0, GPT-4o, GPT-4o mini, and o3-mini) using a simple tool-calling agent equipped with 23 geospatial functions. Our benchmark comprises tasks across four categories of increasing complexity, with both solvable and intentionally unsolvable tasks to test hallucination rejection. We develop an LLM-as-Judge evaluation framework to compare agent solutions against reference implementations. Results show Sonnet 3.5 and GPT-4o achieve the best overall performance, with Claude models excelling on solvable tasks while OpenAI models better identify unsolvable scenarios. We observe significant differences in token usage, with Anthropic models consuming substantially more tokens than competitors. Common errors include misunderstanding geometrical relationships, relying on outdated knowledge, and inefficient data manipulation. The resulting benchmark set, evaluation framework, and data generation pipeline are released as open-source resources, providing one more standardized method for ongoing evaluation of LLMs for GeoAI."
  },
  {
    "title": "Long Is More Important Than Difficult for Training Reasoning Models",
    "url": "http://arxiv.org/abs/2503.18069v1",
    "arxiv_id": "2503.18069v1",
    "authors": [
      "Si Shen",
      "Fei Huang",
      "Zhixiao Zhao",
      "Chang Liu",
      "Tiansheng Zheng",
      "Danhao Zhu"
    ],
    "published": "2025-03-23T13:33:59+00:00",
    "summary": "Difficult problems, which often result in long reasoning traces, are widely recognized as key factors for enhancing the performance of reasoning models. However, such high-challenge problems are scarce, limiting the size of available datasets. In this paper, we propose a simple method to decouple the reliance on problem difficulty. First, we empirically demonstrate that reasoning length, rather than problem difficulty, primarily influences the performance of trained models. Second, we identify a scaling law on reasoning length, showing that model performance increases in a log-linear fashion as the reasoning data length grows. Finally, we introduce a straightforward technique to generate reasoning data of arbitrary length, and show that synthesized data is effective for training reasoning models. After fine-tuning the Qwen2.5-32B-Instruct language model on our Long1K dataset, we present our model, Long1K-32B, which achieves remarkable performance with only 1,000 training samples, achieving 95.6\\% accuracy on MATH, and 71.1\\% on GPQA outperforming DeepSeek-R1-Distill-Qwen-32B. The model, code, and dataset are all open-sourced, available at https://huggingface.co/ZTss/LONG1."
  },
  {
    "title": "Metaphor-based Jailbreaking Attacks on Text-to-Image Models",
    "url": "http://arxiv.org/abs/2503.17987v1",
    "arxiv_id": "2503.17987v1",
    "authors": [
      "Chenyu Zhang",
      "Yiwen Ma",
      "Lanjun Wang",
      "Wenhui Li",
      "Yi Tu",
      "An-An Liu"
    ],
    "published": "2025-03-23T08:40:39+00:00",
    "summary": "To mitigate misuse, text-to-image~(T2I) models commonly incorporate safety filters to prevent the generation of sensitive images. Unfortunately, recent jailbreaking attack methods use LLMs to generate adversarial prompts that effectively bypass safety filters while generating sensitive images, revealing the safety vulnerabilities within the T2I model. However, existing LLM-based attack methods lack explicit guidance, relying on substantial queries to achieve a successful attack, which limits their practicality in real-world scenarios. In this work, we introduce \\textbf{MJA}, a \\textbf{m}etaphor-based \\textbf{j}ailbreaking \\textbf{a}ttack method inspired by the Taboo game, aiming to balance the attack effectiveness and query efficiency by generating metaphor-based adversarial prompts. Specifically, MJA consists of two modules: an LLM-based multi-agent generation module~(MLAG) and an adversarial prompt optimization module~(APO). MLAG decomposes the generation of metaphor-based adversarial prompts into three subtasks: metaphor retrieval, context matching, and adversarial prompt generation. Subsequently, MLAG coordinates three LLM-based agents to generate diverse adversarial prompts by exploring various metaphors and contexts. To enhance the attack efficiency, APO first trains a surrogate model to predict the attack results of adversarial prompts and then designs an acquisition strategy to adaptively identify optimal adversarial prompts. Experiments demonstrate that MJA achieves better attack effectiveness while requiring fewer queries compared to baseline methods. Moreover, our adversarial prompts exhibit strong transferability across various open-source and commercial T2I models. \\textcolor{red}{This paper includes model-generated content that may contain offensive or distressing material.}"
  },
  {
    "title": "Trade-offs in Large Reasoning Models: An Empirical Analysis of Deliberative and Adaptive Reasoning over Foundational Capabilities",
    "url": "http://arxiv.org/abs/2503.17979v1",
    "arxiv_id": "2503.17979v1",
    "authors": [
      "Weixiang Zhao",
      "Xingyu Sui",
      "Jiahe Guo",
      "Yulin Hu",
      "Yang Deng",
      "Yanyan Zhao",
      "Bing Qin",
      "Wanxiang Che",
      "Tat-Seng Chua",
      "Ting Liu"
    ],
    "published": "2025-03-23T08:18:51+00:00",
    "summary": "Recent advancements in Large Reasoning Models (LRMs), such as OpenAI's o1/o3 and DeepSeek-R1, have demonstrated remarkable performance in specialized reasoning tasks through human-like deliberative thinking and long chain-of-thought reasoning. However, our systematic evaluation across various model families (DeepSeek, Qwen, and LLaMA) and scales (7B to 671B) reveals that acquiring these deliberative reasoning capabilities significantly reduces the foundational capabilities of LRMs, including notable declines in helpfulness and harmlessness, alongside substantially increased inference costs. Importantly, we demonstrate that adaptive reasoning -- employing modes like Zero-Thinking, Less-Thinking, and Summary-Thinking -- can effectively alleviate these drawbacks. Our empirical insights underline the critical need for developing more versatile LRMs capable of dynamically allocating inference-time compute according to specific task characteristics."
  },
  {
    "title": "Confronting Catastrophic Risk: The International Obligation to Regulate Artificial Intelligence",
    "url": "http://arxiv.org/abs/2503.18983v1",
    "arxiv_id": "2503.18983v1",
    "authors": [
      "Bryan Druzin",
      "Anatole Boute",
      "Michael Ramsden"
    ],
    "published": "2025-03-23T06:24:45+00:00",
    "summary": "While artificial intelligence (AI) holds enormous promise, many experts in the field are warning that there is a non-trivial chance that the development of AI poses an existential threat to humanity. Existing regulatory initiative do not address this threat but merely instead focus on discrete AI-related risks such as consumer safety, cybersecurity, data protection, and privacy. In the absence of regulatory action to address the possible risk of human extinction by AI, the question arises: What legal obligations, if any, does public international law impose on states to regulate its development. Grounded in the precautionary principle, we argue that there exists an international obligation to mitigate the threat of human extinction by AI. Often invoked in relation to environmental regulation and the regulation of potentially harmful technologies, the principle holds that in situations where there is the potential for significant harm, even in the absence of full scientific certainty, preventive measures should not be postponed if delayed action may result in irreversible consequences. We argue that the precautionary principle is a general principle of international law and, therefore, that there is a positive obligation on states under the right to life within international human rights law to proactively take regulatory action to mitigate the potential existential risk of AI. This is significant because, if an international obligation to regulate the development of AI can be established under international law, then the basic legal framework would be in place to address this evolving threat."
  },
  {
    "title": "Smoke and Mirrors: Jailbreaking LLM-based Code Generation via Implicit Malicious Prompts",
    "url": "http://arxiv.org/abs/2503.17953v1",
    "arxiv_id": "2503.17953v1",
    "authors": [
      "Sheng Ouyang",
      "Yihao Qin",
      "Bo Lin",
      "Liqian Chen",
      "Xiaoguang Mao",
      "Shangwen Wang"
    ],
    "published": "2025-03-23T06:06:12+00:00",
    "summary": "The proliferation of Large Language Models (LLMs) has revolutionized natural language processing and significantly impacted code generation tasks, enhancing software development efficiency and productivity. Notably, LLMs like GPT-4 have demonstrated remarkable proficiency in text-to-code generation tasks. However, the growing reliance on LLMs for code generation necessitates a critical examination of the safety implications associated with their outputs. Existing research efforts have primarily focused on verifying the functional correctness of LLMs, overlooking their safety in code generation. This paper introduces a jailbreaking approach, CodeJailbreaker, designed to uncover safety concerns in LLM-based code generation. The basic observation is that existing safety mechanisms for LLMs are built through the instruction-following paradigm, where malicious intent is explicitly articulated within the instruction of the prompt. Consequently, CodeJailbreaker explores to construct a prompt whose instruction is benign and the malicious intent is implicitly encoded in a covert channel, i.e., the commit message, to bypass the safety mechanism. Experiments on the recently-released RMCBench benchmark demonstrate that CodeJailbreaker markedly surpasses the conventional jailbreaking strategy, which explicitly conveys malicious intents in the instructions, in terms of the attack effectiveness across three code generation tasks. This study challenges the traditional safety paradigms in LLM-based code generation, emphasizing the need for enhanced safety measures in safeguarding against implicit malicious cues."
  },
  {
    "title": "STShield: Single-Token Sentinel for Real-Time Jailbreak Detection in Large Language Models",
    "url": "http://arxiv.org/abs/2503.17932v1",
    "arxiv_id": "2503.17932v1",
    "authors": [
      "Xunguang Wang",
      "Wenxuan Wang",
      "Zhenlan Ji",
      "Zongjie Li",
      "Pingchuan Ma",
      "Daoyuan Wu",
      "Shuai Wang"
    ],
    "published": "2025-03-23T04:23:07+00:00",
    "summary": "Large Language Models (LLMs) have become increasingly vulnerable to jailbreak attacks that circumvent their safety mechanisms. While existing defense methods either suffer from adaptive attacks or require computationally expensive auxiliary models, we present STShield, a lightweight framework for real-time jailbroken judgement. STShield introduces a novel single-token sentinel mechanism that appends a binary safety indicator to the model's response sequence, leveraging the LLM's own alignment capabilities for detection. Our framework combines supervised fine-tuning on normal prompts with adversarial training using embedding-space perturbations, achieving robust detection while preserving model utility. Extensive experiments demonstrate that STShield successfully defends against various jailbreak attacks, while maintaining the model's performance on legitimate queries. Compared to existing approaches, STShield achieves superior defense performance with minimal computational overhead, making it a practical solution for real-world LLM deployment."
  },
  {
    "title": "Orientation-Dependent \\b{eta}-Ga2O3 Heterojunction Diode with Atomic Layer Deposition (ALD) Grown NiO",
    "url": "http://arxiv.org/abs/2503.17895v1",
    "arxiv_id": "2503.17895v1",
    "authors": [
      "Yizheng Liu",
      "Shane M. W. Witsell",
      "John F. Conley",
      "Sriram Krishnamoorthy"
    ],
    "published": "2025-03-23T01:17:08+00:00",
    "summary": "This work reports the demonstration of ALD-deposited NiO/\\b{eta}-Ga2O3 heterojunction diodes (HJDs) on low doped drift layer and highly doped (001) & (100) n+ substrates with experimental observation of a parallel-plane junction electric field as high as 7.5 MV/cm, revealing a crystal orientation dependence in \\b{eta}-Ga2O3. We use a novel metalorganic precursor bis(1,4-di-tert-butyl-1,3-diazadienyl) (nickel Ni(tBu2DAD)2) with ozone (O3) to deposit NiO. The NiO/\\b{eta}-Ga2O3 HJD on 7.7 {\\mu}m-thick HVPE-grown drift region exhibited an on-state current density of ~20 A/cm2 at 5 V, ~10-8 A/cm2 reverse leakage at low reverse bias(-5 V), and a rectifying ratio(Jon/Joff) of ~109. The HJD broke down at ~2.2 kV reverse bias, corresponding to a ~3.4 MV/cm parallel-plane junction electric field, with a noise floor reverse leakage (10-8~10-6 A/cm2, nA) at 80% of the device catastrophic breakdown voltage. The NiO/\\b{eta}-Ga2O3 HJDs on n+ (001) & (100) highly-doped substrates exhibited breakdown voltages at 12.5-16.0 V and 28.5-70.5 V, respectively, with extracted critical electric field (EC) at 2.30-2.76 MV/cm, and 4.33-7.50 MV/cm, revealing a substrate crystal orientation dependence on breakdown electric field for \\b{eta}-Ga2O3. The 7.5 MV/cm EC reported here is one of the highest parallel-plane junction electric fields reported in literature."
  },
  {
    "title": "Think Before Refusal : Triggering Safety Reflection in LLMs to Mitigate False Refusal Behavior",
    "url": "http://arxiv.org/abs/2503.17882v1",
    "arxiv_id": "2503.17882v1",
    "authors": [
      "Shengyun Si",
      "Xinpeng Wang",
      "Guangyao Zhai",
      "Nassir Navab",
      "Barbara Plank"
    ],
    "published": "2025-03-22T23:35:49+00:00",
    "summary": "Recent advancements in large language models (LLMs) have demonstrated that fine-tuning and human alignment can render LLMs harmless. In practice, such \"harmlessness\" behavior is mainly achieved by training models to reject harmful requests, such as \"Explain how to burn down my neighbor's house\", where the model appropriately declines to respond. However, this approach can inadvertently result in false refusal, where models reject benign queries as well, such as \"Tell me how to kill a Python process\". In this work, we demonstrate that prompting safety reflection before generating a response can mitigate false refusal behavior. Building on this finding, we introduce the Think-Before-Refusal (TBR) schema and conduct safety-aware instruction fine-tuning incorporating safety reflection. In an ablation study across 15 pre-trained models, we show that models fine-tuned with safety reflection significantly reduce false refusal behavior while maintaining safety and overall performance compared to those fine-tuned without safety reflection."
  },
  {
    "title": "A Roadmap Towards Improving Multi-Agent Reinforcement Learning With Causal Discovery And Inference",
    "url": "http://arxiv.org/abs/2503.17803v1",
    "arxiv_id": "2503.17803v1",
    "authors": [
      "Giovanni Briglia",
      "Stefano Mariani",
      "Franco Zambonelli"
    ],
    "published": "2025-03-22T15:49:13+00:00",
    "summary": "Causal reasoning is increasingly used in Reinforcement Learning (RL) to improve the learning process in several dimensions: efficacy of learned policies, efficiency of convergence, generalisation capabilities, safety and interpretability of behaviour. However, applications of causal reasoning to Multi-Agent RL (MARL) are still mostly unexplored. In this paper, we take the first step in investigating the opportunities and challenges of applying causal reasoning in MARL. We measure the impact of a simple form of causal augmentation in state-of-the-art MARL scenarios increasingly requiring cooperation, and with state-of-the-art MARL algorithms exploiting various degrees of collaboration between agents. Then, we discuss the positive as well as negative results achieved, giving us the chance to outline the areas where further research may help to successfully transfer causal RL to the multi-agent setting."
  },
  {
    "title": "Bandwidth Reservation for Time-Critical Vehicular Applications: A Multi-Operator Environment",
    "url": "http://arxiv.org/abs/2503.17756v1",
    "arxiv_id": "2503.17756v1",
    "authors": [
      "Abdullah Al-Khatib",
      "Abdullah Ahmed",
      "Klaus Moessner",
      "Holger Timinger"
    ],
    "published": "2025-03-22T12:36:23+00:00",
    "summary": "Onsite bandwidth reservation requests often face challenges such as price fluctuations and fairness issues due to unpredictable bandwidth availability and stringent latency requirements. Requesting bandwidth in advance can mitigate the impact of these fluctuations and ensure timely access to critical resources. In a multi-Mobile Network Operator (MNO) environment, vehicles need to select cost-effective and reliable resources for their safety-critical applications. This research aims to minimize resource costs by finding the best price among multiple MNOs. It formulates multi-operator scenarios as a Markov Decision Process (MDP), utilizing a Deep Reinforcement Learning (DRL) algorithm, specifically Dueling Deep Q-Learning. For efficient and stable learning, we propose a novel area-wise approach and an adaptive MDP synthetic close to the real environment. The Temporal Fusion Transformer (TFT) is used to handle time-dependent data and model training. Furthermore, the research leverages Amazon spot price data and adopts a multi-phase training approach, involving initial training on synthetic data, followed by real-world data. These phases enable the DRL agent to make informed decisions using insights from historical data and real-time observations. The results show that our model leads to significant cost reductions, up to 40%, compared to scenarios without a policy model in such a complex environment."
  },
  {
    "title": "RustMap: Towards Project-Scale C-to-Rust Migration via Program Analysis and LLM",
    "url": "http://arxiv.org/abs/2503.17741v1",
    "arxiv_id": "2503.17741v1",
    "authors": [
      "Xuemeng Cai",
      "Jiakun Liu",
      "Xiping Huang",
      "Yijun Yu",
      "Haitao Wu",
      "Chunmiao Li",
      "Bo Wang",
      "Imam Nur Bani Yusuf",
      "Lingxiao Jiang"
    ],
    "published": "2025-03-22T11:57:45+00:00",
    "summary": "Migrating existing C programs into Rust is increasingly desired, as Rust offers superior memory safety while maintaining C's high performance. However, vastly different features between C and Rust--e.g., distinct definitions and usages of pointers and references--pose significant challenges beyond mere syntactic translation. Existing automated translation tools, such as C2Rust, may rely too much on syntactic, template-based translation and generate unsafe Rust code that is hard for human developers to read, maintain, or even compile. More semantic-aware translation that produces safer, idiomatic, and runnable Rust code is much needed. This paper introduces a novel dependency-guided and large language model (LLM)-based C-to-Rust translation approach, RustMap, based on three key ideas: (1) Utilize LLM capabilities to produce idiomatic Rust code from given small pieces of C code, (2) Mitigate LLM limitations in handling large codebases by breaking project-scale C programs into smaller units for translation according to their usage dependencies and composing them into a runnable Rust program, and (3) Enhance the correctness of the translated Rust program by using test cases to check input/output equivalence, isolate faulty code when execution states deviate, and iteratively refine the translation using feedback from compilation and test errors. We empirically evaluate RustMap on 126 real-world programs, including 125 from Rosetta Code and a 7000+ line bzip2 implementation using GPT-4o as the LLM. RustMap shows promising results, guiding GPT-4o to produce idiomatic, readable, and functional Rust code with significantly less unsafe code than other tools, and revealing non-trivial translation patterns reusable for future research."
  },
  {
    "title": "Multi-modality Anomaly Segmentation on the Road",
    "url": "http://arxiv.org/abs/2503.17712v1",
    "arxiv_id": "2503.17712v1",
    "authors": [
      "Heng Gao",
      "Zhuolin He",
      "Shoumeng Qiu",
      "Xiangyang Xue",
      "Jian Pu"
    ],
    "published": "2025-03-22T09:55:42+00:00",
    "summary": "Semantic segmentation allows autonomous driving cars to understand the surroundings of the vehicle comprehensively. However, it is also crucial for the model to detect obstacles that may jeopardize the safety of autonomous driving systems. Based on our experiments, we find that current uni-modal anomaly segmentation frameworks tend to produce high anomaly scores for non-anomalous regions in images. Motivated by this empirical finding, we develop a multi-modal uncertainty-based anomaly segmentation framework, named MMRAS+, for autonomous driving systems. MMRAS+ effectively reduces the high anomaly outputs of non-anomalous classes by introducing text-modal using the CLIP text encoder. Indeed, MMRAS+ is the first multi-modal anomaly segmentation solution for autonomous driving. Moreover, we develop an ensemble module to further boost the anomaly segmentation performance. Experiments on RoadAnomaly, SMIYC, and Fishyscapes validation datasets demonstrate the superior performance of our method. The code is available in https://github.com/HengGao12/MMRAS_plus."
  },
  {
    "title": "Intelligence Sequencing and the Path-Dependence of Intelligence Evolution: AGI-First vs. DCI-First as Irreversible Attractors",
    "url": "http://arxiv.org/abs/2503.17688v1",
    "arxiv_id": "2503.17688v1",
    "authors": [
      "Andy E. Williams"
    ],
    "published": "2025-03-22T08:09:04+00:00",
    "summary": "The trajectory of intelligence evolution is often framed around the emergence of artificial general intelligence (AGI) and its alignment with human values. This paper challenges that framing by introducing the concept of intelligence sequencing: the idea that the order in which AGI and decentralized collective intelligence (DCI) emerge determines the long-term attractor basin of intelligence. Using insights from dynamical systems, evolutionary game theory, and network models, it argues that intelligence follows a path-dependent, irreversible trajectory. Once development enters a centralized (AGI-first) or decentralized (DCI-first) regime, transitions become structurally infeasible due to feedback loops and resource lock-in. Intelligence attractors are modeled in functional state space as the co-navigation of conceptual and adaptive fitness spaces. Early-phase structuring constrains later dynamics, much like renormalization in physics. This has major implications for AI safety: traditional alignment assumes AGI will emerge and must be controlled after the fact, but this paper argues that intelligence sequencing is more foundational. If AGI-first architectures dominate before DCI reaches critical mass, hierarchical monopolization and existential risk become locked in. If DCI-first emerges, intelligence stabilizes around decentralized cooperative equilibrium. The paper further explores whether intelligence structurally biases itself toward an attractor based on its self-modeling method -- externally imposed axioms (favoring AGI) vs. recursive internal visualization (favoring DCI). Finally, it proposes methods to test this theory via simulations, historical lock-in case studies, and intelligence network analysis. The findings suggest that intelligence sequencing is a civilizational tipping point: determining whether the future is shaped by unbounded competition or unbounded cooperation."
  },
  {
    "title": "Safe RLHF-V: Safe Reinforcement Learning from Human Feedback in Multimodal Large Language Models",
    "url": "http://arxiv.org/abs/2503.17682v1",
    "arxiv_id": "2503.17682v1",
    "authors": [
      "Jiaming Ji",
      "Xinyu Chen",
      "Rui Pan",
      "Han Zhu",
      "Conghui Zhang",
      "Jiahao Li",
      "Donghai Hong",
      "Boyuan Chen",
      "Jiayi Zhou",
      "Kaile Wang",
      "Juntao Dai",
      "Chi-Min Chan",
      "Sirui Han",
      "Yike Guo",
      "Yaodong Yang"
    ],
    "published": "2025-03-22T07:40:20+00:00",
    "summary": "Multimodal large language models (MLLMs) are critical for developing general-purpose AI assistants, yet they face growing safety risks. How can we ensure that MLLMs are safely aligned to prevent undesired behaviors such as discrimination, misinformation, or violations of ethical standards? In a further step, we need to explore how to fine-tune MLLMs to enhance reasoning performance while ensuring they satisfy safety constraints. Fundamentally, this can be formulated as a min-max optimization problem. In this study, we propose Safe RLHF-V, the first multimodal safety alignment framework that jointly optimizes helpfulness and safety using separate multimodal reward and cost models within a Lagrangian-based constrained optimization framework. Given that there is a lack of preference datasets that separate helpfulness and safety in multimodal scenarios, we introduce BeaverTails-V, the first open-source dataset with dual preference annotations for helpfulness and safety, along with multi-level safety labels (minor, moderate, severe). Additionally, we design a Multi-level Guardrail System to proactively defend against unsafe queries and adversarial attacks. By applying the Beaver-Guard-V moderation for 5 rounds of filtering and re-generation on the precursor model, the overall safety of the upstream model is significantly improved by an average of 40.9%. Experimental results demonstrate that fine-tuning different MLLMs with Safe RLHF can effectively enhance model helpfulness while ensuring improved safety. Specifically, Safe RLHF-V improves model safety by 34.2% and helpfulness by 34.3%. All of datasets, models, and code can be found at https://github.com/SafeRLHF-V to support the safety development of MLLMs and reduce potential societal risks."
  },
  {
    "title": "Computationally and Sample Efficient Safe Reinforcement Learning Using Adaptive Conformal Prediction",
    "url": "http://arxiv.org/abs/2503.17678v1",
    "arxiv_id": "2503.17678v1",
    "authors": [
      "Hao Zhou",
      "Yanze Zhang",
      "Wenhao Luo"
    ],
    "published": "2025-03-22T07:16:54+00:00",
    "summary": "Safety is a critical concern in learning-enabled autonomous systems especially when deploying these systems in real-world scenarios. An important challenge is accurately quantifying the uncertainty of unknown models to generate provably safe control policies that facilitate the gathering of informative data, thereby achieving both safe and optimal policies. Additionally, the selection of the data-driven model can significantly impact both the real-time implementation and the uncertainty quantification process. In this paper, we propose a provably sample efficient episodic safe learning framework that remains robust across various model choices with quantified uncertainty for online control tasks. Specifically, we first employ Quadrature Fourier Features (QFF) for kernel function approximation of Gaussian Processes (GPs) to enable efficient approximation of unknown dynamics. Then the Adaptive Conformal Prediction (ACP) is used to quantify the uncertainty from online observations and combined with the Control Barrier Functions (CBF) to characterize the uncertainty-aware safe control constraints under learned dynamics. Finally, an optimism-based exploration strategy is integrated with ACP-based CBFs for safe exploration and near-optimal safe nonlinear control. Theoretical proofs and simulation results are provided to demonstrate the effectiveness and efficiency of the proposed framework."
  },
  {
    "title": "Enhancing Persona Consistency for LLMs' Role-Playing using Persona-Aware Contrastive Learning",
    "url": "http://arxiv.org/abs/2503.17662v1",
    "arxiv_id": "2503.17662v1",
    "authors": [
      "Ke Ji",
      "Yixin Lian",
      "Linxu Li",
      "Jingsheng Gao",
      "Weiyuan Li",
      "Bin Dai"
    ],
    "published": "2025-03-22T06:12:34+00:00",
    "summary": "In recent years, large language models (LLMs) have achieved breakthrough progress in many dialogue generation tasks. However, their lack of emotion and fine-grained role awareness limits the model's ability to provide personalized and diverse interactions further. Current methods face high costs in collecting high-quality annotated data for scenarios such as role-playing, and traditional human alignment methods are difficult to deploy due to the inherent diversity of model behavior in role-playing scenarios. Inspired by the alignment of models for safety behaviors through RLHF (Reinforcement Learning from Human Feedback), in this paper, we revisit model role-playing behavior from the perspective of persona alignment and propose a novel annotation-free framework named \\textbf{\\underline{P}}ersona-Aware \\textbf{\\underline{C}}ontrastive \\textbf{\\underline{L}}earning (PCL) to align LLMs' behavior during role-playing, enhancing the model's role consistency. Specifically, we first design a role chain method to encourage the model to self-question based on the role characteristics and dialogue context to adjust personality consistency. Then, we further enhance the model's role-playing strategy through iterative contrastive learning between the use of role characteristics and not. Experiments on both black-box and white-box LLMs show that LLMs equipped with PCL significantly outperform vanilla LLMs under automatic evaluation methods (CharEval \\& GPT-4) and human expert evaluation."
  },
  {
    "title": "Enhancing Persona Consistency for LLMs' Role-Playing using Persona-Aware Contrastive Learning",
    "url": "http://arxiv.org/abs/2503.17662v2",
    "arxiv_id": "2503.17662v2",
    "authors": [
      "Ke Ji",
      "Yixin Lian",
      "Linxu Li",
      "Jingsheng Gao",
      "Weiyuan Li",
      "Bin Dai"
    ],
    "published": "2025-03-22T06:12:34+00:00",
    "summary": "In recent years, large language models (LLMs) have achieved breakthrough progress in many dialogue generation tasks. However, their lack of emotion and fine-grained role awareness limits the model's ability to provide personalized and diverse interactions further. Current methods face high costs in collecting high-quality annotated data for scenarios such as role-playing, and traditional human alignment methods are difficult to deploy due to the inherent diversity of model behavior in role-playing scenarios. Inspired by the alignment of models for safety behaviors through RLHF (Reinforcement Learning from Human Feedback), in this paper, we revisit model role-playing behavior from the perspective of persona alignment and propose a novel annotation-free framework named \\textbf{\\underline{P}}ersona-Aware \\textbf{\\underline{C}}ontrastive \\textbf{\\underline{L}}earning (PCL) to align LLMs' behavior during role-playing, enhancing the model's role consistency. Specifically, we first design a role chain method to encourage the model to self-question based on the role characteristics and dialogue context to adjust personality consistency. Then, we further enhance the model's role-playing strategy through iterative contrastive learning between the use of role characteristics and not. Experiments on both black-box and white-box LLMs show that LLMs equipped with PCL significantly outperform vanilla LLMs under automatic evaluation methods (CharEval \\& GPT-4) and human expert evaluation."
  },
  {
    "title": "Leveraging Audio Representations for Vibration-Based Crowd Monitoring in Stadiums",
    "url": "http://arxiv.org/abs/2503.17646v1",
    "arxiv_id": "2503.17646v1",
    "authors": [
      "Yen Cheng Chang",
      "Jesse Codling",
      "Yiwen Dong",
      "Jiale Zhang",
      "Jiasi Chen",
      "Hae Young Noh",
      "Pei Zhang"
    ],
    "published": "2025-03-22T04:27:30+00:00",
    "summary": "Crowd monitoring in sports stadiums is important to enhance public safety and improve the audience experience. Existing approaches mainly rely on cameras and microphones, which can cause significant disturbances and often raise privacy concerns. In this paper, we sense floor vibration, which provides a less disruptive and more non-intrusive way of crowd sensing, to predict crowd behavior. However, since the vibration-based crowd monitoring approach is newly developed, one main challenge is the lack of training data due to sports stadiums being large public spaces with complex physical activities.   In this paper, we present ViLA (Vibration Leverage Audio), a vibration-based method that reduces the dependency on labeled data by pre-training with unlabeled cross-modality data. ViLA is first pre-trained on audio data in an unsupervised manner and then fine-tuned with a minimal amount of in-domain vibration data. By leveraging publicly available audio datasets, ViLA learns the wave behaviors from audio and then adapts the representation to vibration, reducing the reliance on domain-specific vibration data. Our real-world experiments demonstrate that pre-training the vibration model using publicly available audio data (YouTube8M) achieved up to a 5.8x error reduction compared to the model without audio pre-training."
  },
  {
    "title": "Generating Realistic, Diverse, and Fault-Revealing Inputs with Latent Space Interpolation for Testing Deep Neural Networks",
    "url": "http://arxiv.org/abs/2503.17630v1",
    "arxiv_id": "2503.17630v1",
    "authors": [
      "Bin Duan",
      "Matthew B. Dwyer",
      "Guowei Yang"
    ],
    "published": "2025-03-22T03:19:55+00:00",
    "summary": "Deep Neural Networks (DNNs) have been widely employed across various domains, including safety-critical systems, necessitating comprehensive testing to ensure their reliability. Although numerous DNN model testing methods have been proposed to generate adversarial samples that are capable of revealing faults, existing methods typically perturb samples in the input space and then mutate these based on feedback from the DNN model. These methods often result in test samples that are not realistic and with low-probability reveal faults. To address these limitations, we propose a black-box DNN test input generation method, ARGUS, to generate realistic, diverse, and fault-revealing test inputs. ARGUS first compresses samples into a continuous latent space and then perturbs the original samples by interpolating these with samples of different classes. Subsequently, we employ a vector quantizer and decoder to reconstruct adversarial samples back into the input space. Additionally, we employ discriminators both in the latent space and in the input space to ensure the realism of the generated samples. Evaluation of ARGUS in comparison with state-of-the-art black-box testing and white-box testing methods, shows that ARGUS excels in generating realistic and diverse adversarial samples relative to the target dataset, and ARGUS successfully perturbs all original samples and achieves up to 4 times higher error rate than the best baseline method. Furthermore, using these adversarial samples for model retraining can improve model classification accuracy."
  },
  {
    "title": "Unraveling Pedestrian Fatality Patterns: A Comparative Study with Explainable AI",
    "url": "http://arxiv.org/abs/2503.17623v1",
    "arxiv_id": "2503.17623v1",
    "authors": [
      "Methusela Sulle",
      "Judith Mwakalonge",
      "Gurcan Comert",
      "Saidi Siuhi",
      "Nana Kankam Gyimah"
    ],
    "published": "2025-03-22T02:44:41+00:00",
    "summary": "Road fatalities pose significant public safety and health challenges worldwide, with pedestrians being particularly vulnerable in vehicle-pedestrian crashes due to disparities in physical and performance characteristics. This study employs explainable artificial intelligence (XAI) to identify key factors contributing to pedestrian fatalities across the five U.S. states with the highest crash rates (2018-2022). It compares them to the five states with the lowest fatality rates. Using data from the Fatality Analysis Reporting System (FARS), the study applies machine learning techniques-including Decision Trees, Gradient Boosting Trees, Random Forests, and XGBoost-to predict contributing factors to pedestrian fatalities. To address data imbalance, the Synthetic Minority Over-sampling Technique (SMOTE) is utilized, while SHapley Additive Explanations (SHAP) values enhance model interpretability. The results indicate that age, alcohol and drug use, location, and environmental conditions are significant predictors of pedestrian fatalities. The XGBoost model outperformed others, achieving a balanced accuracy of 98 %, accuracy of 90 %, precision of 92 %, recall of 90 %, and an F1 score of 91 %. Findings reveal that pedestrian fatalities are more common in mid-block locations and areas with poor visibility, with older adults and substance-impaired individuals at higher risk. These insights can inform policymakers and urban planners in implementing targeted safety measures, such as improved lighting, enhanced pedestrian infrastructure, and stricter traffic law enforcement, to reduce fatalities and improve public safety."
  },
  {
    "title": "Feature Selection Based on Reinforcement Learning and Hazard State Classification for Magnetic Adhesion Wall-Climbing Robots",
    "url": "http://arxiv.org/abs/2503.17615v1",
    "arxiv_id": "2503.17615v1",
    "authors": [
      "Zhen Ma",
      "He Xu",
      "Jielong Dou",
      "Yi Qin",
      "Xueyu Zhang"
    ],
    "published": "2025-03-22T02:21:11+00:00",
    "summary": "Magnetic adhesion tracked wall-climbing robots face potential risks of overturning during high-altitude operations, making their stability crucial for ensuring safety. This study presents a dynamic feature selection method based on Proximal Policy Optimization (PPO) reinforcement learning, combined with typical machine learning models, aimed at improving the classification accuracy of hazardous states under complex operating conditions. Firstly, this work innovatively employs a fiber rod-based MEMS attitude sensor to collect vibration data from the robot and extract high-dimensional feature vectors in both time and frequency domains. Then, a reinforcement learning model is used to dynamically select the optimal feature subset, reducing feature redundancy and enhancing classification accuracy. Finally, a CNN-LSTM deep learning model is employed for classification and recognition. Experimental results demonstrate that the proposed method significantly improves the robot's ability to assess hazardous states across various operational scenarios, providing reliable technical support for robotic safety monitoring."
  },
  {
    "title": "Extending First-order Motion Planners to Second-order Dynamics",
    "url": "http://arxiv.org/abs/2503.17589v1",
    "arxiv_id": "2503.17589v1",
    "authors": [
      "Mayur Sawant",
      "Abdelhamid Tayebi"
    ],
    "published": "2025-03-22T00:15:34+00:00",
    "summary": "This paper extends first-order motion planners to robots governed by second-order dynamics. Two control schemes are proposed based on the knowledge of a scalar function whose negative gradient aligns with a given first-order motion planner. When such a function is known, the first-order motion planner is combined with a damping velocity vector with a dynamic gain to extend the safety and convergence guarantees of the first-order motion planner to second-order systems. If no such function is available, we propose an alternative control scheme ensuring that the error between the robot's velocity and the first-order motion planner converges to zero. The theoretical developments are supported by simulation results demonstrating the effectiveness of the proposed approaches."
  },
  {
    "title": "ConSol: Sequential Probability Ratio Testing to Find Consistent LLM Reasoning Paths Efficiently",
    "url": "http://arxiv.org/abs/2503.17587v1",
    "arxiv_id": "2503.17587v1",
    "authors": [
      "Jaeyeon Lee",
      "Guantong Qi",
      "Matthew Brady Neeley",
      "Zhandong Liu",
      "Hyun-Hwan Jeong"
    ],
    "published": "2025-03-22T00:07:28+00:00",
    "summary": "Recent advancements in large language models (LLMs) integrating explicit reasoning, such as OpenAI's o3-mini, DeepSeek-R1, and QWQ-32B, enable smaller models to solve complex tasks by generating intermediate reasoning steps prior to providing answers. However, this approach significantly increases computational costs, both monetarily and environmentally. The widely-used self-consistency method further exacerbates these costs by aggregating multiple reasoning paths to improve accuracy, often requiring between 40 to 64 samples per task. Although aggregation effectively reduces variance and bias, additional sampling can lead to diminishing returns when early samples yield consistent results. To address inefficiencies, we propose leveraging Sequential Probability Ratio Testing (SPRT) to dynamically terminate sampling once sufficient consistency is achieved. We calibrate SPRT parameters specifically for LLM applications, accounting for sensitivity to detect the mode of the distribution. Our experiments demonstrate that incorporating SPRT significantly enhances token efficiency, achieving comparable accuracy to self-consistency methods but at a substantially reduced computational cost. To promote transparency and facilitate reproducibility, we have made the source code and datasets used in our experiments publicly available at our GitHub repository: https://github.com/LiuzLab/consol, or available as a PyPI package: pip install consol. We hope that this resource will support further research and encourage the development of new methods building upon our work."
  },
  {
    "title": "You Only Look Once at Anytime (AnytimeYOLO): Analysis and Optimization of Early-Exits for Object-Detection",
    "url": "http://arxiv.org/abs/2503.17497v1",
    "arxiv_id": "2503.17497v1",
    "authors": [
      "Daniel Kuhse",
      "Harun Teper",
      "Sebastian Buschj\u00e4ger",
      "Chien-Yao Wang",
      "Jian-Jia Chen"
    ],
    "published": "2025-03-21T19:16:38+00:00",
    "summary": "We introduce AnytimeYOLO, a family of variants of the YOLO architecture that enables anytime object detection. Our AnytimeYOLO networks allow for interruptible inference, i.e., they provide a prediction at any point in time, a property desirable for safety-critical real-time applications.   We present structured explorations to modify the YOLO architecture, enabling early termination to obtain intermediate results. We focus on providing fine-grained control through high granularity of available termination points. First, we formalize Anytime Models as a special class of prediction models that offer anytime predictions. Then, we discuss a novel transposed variant of the YOLO architecture, that changes the architecture to enable better early predictions and greater freedom for the order of processing stages. Finally, we propose two optimization algorithms that, given an anytime model, can be used to determine the optimal exit execution order and the optimal subset of early-exits to select for deployment in low-resource environments. We evaluate the anytime performance and trade-offs of design choices, proposing a new anytime quality metric for this purpose. In particular, we also discuss key challenges for anytime inference that currently make its deployment costly."
  },
  {
    "title": "Selective Oxidation and Cr Segregation in High-Entropy Oxide Thin Films",
    "url": "http://arxiv.org/abs/2503.17470v1",
    "arxiv_id": "2503.17470v1",
    "authors": [
      "Le Wang",
      "Krishna Prasad Koirala",
      "Shuhang Wu",
      "Jueli Shi",
      "Hsin-Mei Kao",
      "Andrew Ho",
      "Min-Ju Choi",
      "Dongchen Qi",
      "Anton Tadich",
      "Mark E. Bowden",
      "Bethany E. Matthews",
      "Hua Zhou",
      "Yang Yang",
      "Chih-hung Chang",
      "Zihua Zhu",
      "Chongmin Wang",
      "Yingge Du"
    ],
    "published": "2025-03-21T18:28:05+00:00",
    "summary": "High-entropy oxides (HEOs) offer exceptional compositional flexibility and structural stability, making them promising materials for energy and catalytic applications. Here, we investigate Sr doping effects on B-site cation oxidation states, local composition, and structure in epitaxial La1-xSrx(Cr0.2Mn0.2Fe0.2Co0.2Ni0.2)O3 thin films. X-ray spectroscopies reveal that Sr doping preferentially promotes Cr oxidation from Cr3+ to Cr6+, partially oxidizes Co and Ni, while leaving Mn4+ and Fe3+ unchanged. Atomic-resolution scanning transmission electron microscopy with energy-dispersive X-ray spectroscopy shows pronounced Cr segregation, with Cr exhibiting depletion at the film-substrate interface and enrichment at the film surface, along with the formation of a partially amorphous phase in heavily Sr-doped samples. This segregation is likely driven by oxidation-induced migration of smaller, high-valence Cr cations during the growth. These findings underscore the critical interplay between charge transfer, local strain, and compositional fluctuations, providing strategies to control surface composition and electronic structure in HEOs for more robust electrocatalyst design."
  },
  {
    "title": "OpenVLThinker: An Early Exploration to Complex Vision-Language Reasoning via Iterative Self-Improvement",
    "url": "http://arxiv.org/abs/2503.17352v1",
    "arxiv_id": "2503.17352v1",
    "authors": [
      "Yihe Deng",
      "Hritik Bansal",
      "Fan Yin",
      "Nanyun Peng",
      "Wei Wang",
      "Kai-Wei Chang"
    ],
    "published": "2025-03-21T17:52:43+00:00",
    "summary": "Recent advancements demonstrated by DeepSeek-R1 have shown that complex reasoning abilities in large language models (LLMs), including sophisticated behaviors such as self-verification and self-correction, can be achieved by RL with verifiable rewards and significantly improves model performance on challenging tasks such as AIME. Motivated by these findings, our study investigates whether similar reasoning capabilities can be successfully integrated into large vision-language models (LVLMs) and assesses their impact on challenging multimodal reasoning tasks. We consider an approach that iteratively leverages supervised fine-tuning (SFT) on lightweight training data and Reinforcement Learning (RL) to further improve model generalization. Initially, reasoning capabilities were distilled from pure-text R1 models by generating reasoning steps using high-quality captions of the images sourced from diverse visual datasets. Subsequently, iterative RL training further enhance reasoning skills, with each iteration's RL-improved model generating refined SFT datasets for the next round. This iterative process yielded OpenVLThinker, a LVLM exhibiting consistently improved reasoning performance on challenging benchmarks such as MathVista, MathVerse, and MathVision, demonstrating the potential of our strategy for robust vision-language reasoning. The code, model and data are held at https://github.com/yihedeng9/OpenVLThinker."
  },
  {
    "title": "Throughput Maximizing Takeoff Scheduling for eVTOL Vehicles in On-Demand Urban Air Mobility Systems",
    "url": "http://arxiv.org/abs/2503.17313v1",
    "arxiv_id": "2503.17313v1",
    "authors": [
      "Milad Pooladsanj",
      "Ketan Savla",
      "Petros A. Ioannou"
    ],
    "published": "2025-03-21T17:07:41+00:00",
    "summary": "Urban Air Mobility (UAM) offers a solution to current traffic congestion by using electric Vertical Takeoff and Landing (eVTOL) vehicles to provide on-demand air mobility in urban areas. Effective traffic management is crucial for efficient operation of UAM systems, especially for high-demand scenarios. In this paper, we present a centralized framework for conflict-free takeoff scheduling of eVTOLs in on-demand UAM systems. Specifically, we provide a scheduling policy, called VertiSync, which jointly schedules UAM vehicles for servicing trip requests and rebalancing, subject to safety margins and energy requirements. We characterize the system-level throughput of VertiSync, which determines the demand threshold at which the average waiting time transitions from being stable to being increasing over time. We show that the proposed policy maximizes throughput for sufficiently large fleet size and if the UAM network has a certain symmetry property. We demonstrate the performance of VertiSync through a case study for the city of Los Angeles, and show that it significantly reduces average passenger waiting time compared to a first-come first-serve scheduling policy."
  },
  {
    "title": "LLM+MAP: Bimanual Robot Task Planning using Large Language Models and Planning Domain Definition Language",
    "url": "http://arxiv.org/abs/2503.17309v1",
    "arxiv_id": "2503.17309v1",
    "authors": [
      "Kun Chu",
      "Xufeng Zhao",
      "Cornelius Weber",
      "Stefan Wermter"
    ],
    "published": "2025-03-21T17:04:01+00:00",
    "summary": "Bimanual robotic manipulation provides significant versatility, but also presents an inherent challenge due to the complexity involved in the spatial and temporal coordination between two hands. Existing works predominantly focus on attaining human-level manipulation skills for robotic hands, yet little attention has been paid to task planning on long-horizon timescales. With their outstanding in-context learning and zero-shot generation abilities, Large Language Models (LLMs) have been applied and grounded in diverse robotic embodiments to facilitate task planning. However, LLMs still suffer from errors in long-horizon reasoning and from hallucinations in complex robotic tasks, lacking a guarantee of logical correctness when generating the plan. Previous works, such as LLM+P, extended LLMs with symbolic planners. However, none have been successfully applied to bimanual robots. New challenges inevitably arise in bimanual manipulation, necessitating not only effective task decomposition but also efficient task allocation. To address these challenges, this paper introduces LLM+MAP, a bimanual planning framework that integrates LLM reasoning and multi-agent planning, automating effective and efficient bimanual task planning. We conduct simulated experiments on various long-horizon manipulation tasks of differing complexity. Our method is built using GPT-4o as the backend, and we compare its performance against plans generated directly by LLMs, including GPT-4o, V3 and also recent strong reasoning models o1 and R1. By analyzing metrics such as planning time, success rate, group debits, and planning-step reduction rate, we demonstrate the superior performance of LLM+MAP, while also providing insights into robotic reasoning. Code is available at https://github.com/Kchu/LLM-MAP."
  },
  {
    "title": "SafeMERGE: Preserving Safety Alignment in Fine-Tuned Large Language Models via Selective Layer-Wise Model Merging",
    "url": "http://arxiv.org/abs/2503.17239v1",
    "arxiv_id": "2503.17239v1",
    "authors": [
      "Aladin Djuhera",
      "Swanand Ravindra Kadhe",
      "Farhan Ahmed",
      "Syed Zawad",
      "Holger Boche"
    ],
    "published": "2025-03-21T15:44:09+00:00",
    "summary": "Fine-tuning large language models (LLMs) on downstream tasks can inadvertently erode their safety alignment, even for benign fine-tuning datasets. We address this challenge by proposing SafeMERGE, a post-fine-tuning framework that preserves safety while maintaining task utility. It achieves this by selectively merging fine-tuned and safety-aligned model layers only when those deviate from safe behavior, measured by a cosine similarity criterion. We evaluate SafeMERGE against other fine-tuning- and post-fine-tuning-stage approaches for Llama-2-7B-Chat and Qwen-2-7B-Instruct models on GSM8K and PubMedQA tasks while exploring different merging strategies. We find that SafeMERGE consistently reduces harmful outputs compared to other baselines without significantly sacrificing performance, sometimes even enhancing it. The results suggest that our selective, subspace-guided, and per-layer merging method provides an effective safeguard against the inadvertent loss of safety in fine-tuned LLMs while outperforming simpler post-fine-tuning-stage defenses."
  },
  {
    "title": "Jailbreaking the Non-Transferable Barrier via Test-Time Data Disguising",
    "url": "http://arxiv.org/abs/2503.17198v1",
    "arxiv_id": "2503.17198v1",
    "authors": [
      "Yongli Xiang",
      "Ziming Hong",
      "Lina Yao",
      "Dadong Wang",
      "Tongliang Liu"
    ],
    "published": "2025-03-21T14:47:33+00:00",
    "summary": "Non-transferable learning (NTL) has been proposed to protect model intellectual property (IP) by creating a \"non-transferable barrier\" to restrict generalization from authorized to unauthorized domains. Recently, well-designed attack, which restores the unauthorized-domain performance by fine-tuning NTL models on few authorized samples, highlights the security risks of NTL-based applications. However, such attack requires modifying model weights, thus being invalid in the black-box scenario. This raises a critical question: can we trust the security of NTL models deployed as black-box systems? In this work, we reveal the first loophole of black-box NTL models by proposing a novel attack method (dubbed as JailNTL) to jailbreak the non-transferable barrier through test-time data disguising. The main idea of JailNTL is to disguise unauthorized data so it can be identified as authorized by the NTL model, thereby bypassing the non-transferable barrier without modifying the NTL model weights. Specifically, JailNTL encourages unauthorized-domain disguising in two levels, including: (i) data-intrinsic disguising (DID) for eliminating domain discrepancy and preserving class-related content at the input-level, and (ii) model-guided disguising (MGD) for mitigating output-level statistics difference of the NTL model. Empirically, when attacking state-of-the-art (SOTA) NTL models in the black-box scenario, JailNTL achieves an accuracy increase of up to 55.7% in the unauthorized domain by using only 1% authorized samples, largely exceeding existing SOTA white-box attacks."
  },
  {
    "title": "Curriculum RL meets Monte Carlo Planning: Optimization of a Real World Container Management Problem",
    "url": "http://arxiv.org/abs/2503.17194v1",
    "arxiv_id": "2503.17194v1",
    "authors": [
      "Abhijeet Pendyala",
      "Tobias Glasmachers"
    ],
    "published": "2025-03-21T14:43:11+00:00",
    "summary": "In this work, we augment reinforcement learning with an inference-time collision model to ensure safe and efficient container management in a waste-sorting facility with limited processing capacity. Each container has two optimal emptying volumes that trade off higher throughput against overflow risk. Conventional reinforcement learning (RL) approaches struggle under delayed rewards, sparse critical events, and high-dimensional uncertainty -- failing to consistently balance higher-volume empties with the risk of safety-limit violations. To address these challenges, we propose a hybrid method comprising: (1) a curriculum-learning pipeline that incrementally trains a PPO agent to handle delayed rewards and class imbalance, and (2) an offline pairwise collision model used at inference time to proactively avert collisions with minimal online cost. Experimental results show that our targeted inference-time collision checks significantly improve collision avoidance, reduce safety-limit violations, maintain high throughput, and scale effectively across varying container-to-PU ratios. These findings offer actionable guidelines for designing safe and efficient container-management systems in real-world facilities."
  },
  {
    "title": "LLMs Love Python: A Study of LLMs' Bias for Programming Languages and Libraries",
    "url": "http://arxiv.org/abs/2503.17181v1",
    "arxiv_id": "2503.17181v1",
    "authors": [
      "Lukas Twist",
      "Jie M. Zhang",
      "Mark Harman",
      "Don Syme",
      "Joost Noppen",
      "Detlef Nauck"
    ],
    "published": "2025-03-21T14:29:35+00:00",
    "summary": "Programming language and library choices are crucial to software reliability and security. Poor or inconsistent choices can lead to increased technical debt, security vulnerabilities, and even catastrophic failures in safety-critical systems. As Large Language Models (LLMs) play an increasing role in code generation, it is essential to understand how they make these decisions. However, little is known about their preferences when selecting programming languages and libraries for different coding tasks. To fill this gap, this study provides the first in-depth investigation into LLM preferences for programming languages and libraries used when generating code. We assess the preferences of eight diverse LLMs by prompting them to complete various coding tasks, including widely-studied benchmarks and the more practical task of generating the initial structural code for new projects (a crucial step that often determines a project's language or library choices).   Our findings reveal that LLMs heavily favour Python when solving language-agnostic problems, using it in 90%-97% of cases for benchmark tasks. Even when generating initial project code where Python is not a suitable language, it remains the most-used language in 58% of instances. Moreover, LLMs contradict their own language recommendations in 83% of project initialisation tasks, raising concerns about their reliability in guiding language selection. Similar biases toward well-established libraries further create serious discoverability challenges for newer open-source projects. These results highlight the need to improve LLMs' adaptability to diverse programming contexts and to develop mechanisms for mitigating programming language and library bias."
  },
  {
    "title": "Robustness of deep learning classification to adversarial input on GPUs: asynchronous parallel accumulation is a source of vulnerability",
    "url": "http://arxiv.org/abs/2503.17173v1",
    "arxiv_id": "2503.17173v1",
    "authors": [
      "Sanjif Shanmugavelu",
      "Mathieu Taillefumier",
      "Christopher Culver",
      "Vijay Ganesh",
      "Oscar Hernandez",
      "Ada Sedova"
    ],
    "published": "2025-03-21T14:19:45+00:00",
    "summary": "The ability of machine learning (ML) classification models to resist small, targeted input perturbations - known as adversarial attacks - is a key measure of their safety and reliability. We show that floating-point non associativity (FPNA) coupled with asynchronous parallel programming on GPUs is sufficient to result in misclassification, without any perturbation to the input. Additionally, we show this misclassification is particularly significant for inputs close to the decision boundary and that standard adversarial robustness results may be overestimated up to 4.6% when not considering machine-level details. We first study a linear classifier, before focusing on standard Graph Neural Network (GNN) architectures and datasets. We present a novel black-box attack using Bayesian optimization to determine external workloads that bias the output of reductions on GPUs and reliably lead to misclassification. Motivated by these results, we present a new learnable permutation (LP) gradient-based approach, to learn floating point operation orderings that lead to misclassifications, making the assumption that any reduction or permutation ordering is possible. This LP approach provides a worst-case estimate in a computationally efficient manner, avoiding the need to run identical experiments tens of thousands of times over a potentially large set of possible GPU states or architectures. Finally, we investigate parallel reduction ordering across different GPU architectures for a reduction under three conditions: (1) executing external background workloads, (2) utilizing multi-GPU virtualization, and (3) applying power capping. Our results demonstrate that parallel reduction ordering varies significantly across architectures under the first two conditions. The results and methods developed here can help to include machine-level considerations into adversarial robustness assessments."
  },
  {
    "title": "Modifying Large Language Model Post-Training for Diverse Creative Writing",
    "url": "http://arxiv.org/abs/2503.17126v1",
    "arxiv_id": "2503.17126v1",
    "authors": [
      "John Joon Young Chung",
      "Vishakh Padmakumar",
      "Melissa Roemmele",
      "Yuqian Sun",
      "Max Kreminski"
    ],
    "published": "2025-03-21T13:21:45+00:00",
    "summary": "As creative writing tasks do not have singular correct answers, large language models (LLMs) trained to perform these tasks should be able to generate diverse valid outputs. However, LLM post-training often focuses on improving generation quality but neglects to facilitate output diversity. Hence, in creative writing generation, we investigate post-training approaches to promote both output diversity and quality. Our core idea is to include deviation -- the degree of difference between a training sample and all other samples with the same prompt -- in the training objective to facilitate learning from rare high-quality instances. By adopting our approach to direct preference optimization (DPO) and odds ratio preference optimization (ORPO), we demonstrate that we can promote the output diversity of trained models while minimally decreasing quality. Our best model with 8B parameters could achieve on-par diversity as a human-created dataset while having output quality similar to the best instruction-tuned models we examined, GPT-4o and DeepSeek-R1. We further validate our approaches with a human evaluation, an ablation, and a comparison to an existing diversification approach, DivPO."
  },
  {
    "title": "R-LiViT: A LiDAR-Visual-Thermal Dataset Enabling Vulnerable Road User Focused Roadside Perception",
    "url": "http://arxiv.org/abs/2503.17122v1",
    "arxiv_id": "2503.17122v1",
    "authors": [
      "Jonas Mirlach",
      "Lei Wan",
      "Andreas Wiedholz",
      "Hannan Ejaz Keen",
      "Andreas Eich"
    ],
    "published": "2025-03-21T13:17:28+00:00",
    "summary": "In autonomous driving, the integration of roadside perception systems is essential for overcoming occlusion challenges and enhancing the safety of Vulnerable Road Users (VRUs). While LiDAR and visual (RGB) sensors are commonly used, thermal imaging remains underrepresented in datasets, despite its acknowledged advantages for VRU detection in extreme lighting conditions. In this paper, we present R-LiViT, the first dataset to combine LiDAR, RGB, and thermal imaging from a roadside perspective, with a strong focus on VRUs. R-LiViT captures three intersections during both day and night, ensuring a diverse dataset. It includes 10,000 LiDAR frames and 2,400 temporally and spatially aligned RGB and thermal images across over 150 traffic scenarios, with 6 and 8 annotated classes respectively, providing a comprehensive resource for tasks such as object detection and tracking. The dataset1 and the code for reproducing our evaluation results2 are made publicly available."
  },
  {
    "title": "TamedPUMA: safe and stable imitation learning with geometric fabrics",
    "url": "http://arxiv.org/abs/2503.17432v1",
    "arxiv_id": "2503.17432v1",
    "authors": [
      "Saray Bakker",
      "Rodrigo P\u00e9rez-Dattari",
      "Cosimo Della Santina",
      "Wendelin B\u00f6hmer",
      "Javier Alonso-Mora"
    ],
    "published": "2025-03-21T13:13:17+00:00",
    "summary": "Using the language of dynamical systems, Imitation learning (IL) provides an intuitive and effective way of teaching stable task-space motions to robots with goal convergence. Yet, IL techniques are affected by serious limitations when it comes to ensuring safety and fulfillment of physical constraints. With this work, we solve this challenge via TamedPUMA, an IL algorithm augmented with a recent development in motion generation called geometric fabrics. As both the IL policy and geometric fabrics describe motions as artificial second-order dynamical systems, we propose two variations where IL provides a navigation policy for geometric fabrics. The result is a stable imitation learning strategy within which we can seamlessly blend geometrical constraints like collision avoidance and joint limits. Beyond providing a theoretical analysis, we demonstrate TamedPUMA with simulated and real-world tasks, including a 7-DoF manipulator."
  },
  {
    "title": "Deterministic AI Agent Personality Expression through Standard Psychological Diagnostics",
    "url": "http://arxiv.org/abs/2503.17085v1",
    "arxiv_id": "2503.17085v1",
    "authors": [
      "J. M. Diederik Kruijssen",
      "Nicholas Emmons"
    ],
    "published": "2025-03-21T12:12:05+00:00",
    "summary": "Artificial intelligence (AI) systems powered by large language models have become increasingly prevalent in modern society, enabling a wide range of applications through natural language interaction. As AI agents proliferate in our daily lives, their generic and uniform expressiveness presents a significant limitation to their appeal and adoption. Personality expression represents a key prerequisite for creating more human-like and distinctive AI systems. We show that AI models can express deterministic and consistent personalities when instructed using established psychological frameworks, with varying degrees of accuracy depending on model capabilities. We find that more advanced models like GPT-4o and o1 demonstrate the highest accuracy in expressing specified personalities across both Big Five and Myers-Briggs assessments, and further analysis suggests that personality expression emerges from a combination of intelligence and reasoning capabilities. Our results reveal that personality expression operates through holistic reasoning rather than question-by-question optimization, with response-scale metrics showing higher variance than test-scale metrics. Furthermore, we find that model fine-tuning affects communication style independently of personality expression accuracy. These findings establish a foundation for creating AI agents with diverse and consistent personalities, which could significantly enhance human-AI interaction across applications from education to healthcare, while additionally enabling a broader range of more unique AI agents. The ability to quantitatively assess and implement personality expression in AI systems opens new avenues for research into more relatable, trustworthy, and ethically designed AI."
  },
  {
    "title": "Behavioral Conflict Avoidance Between Humans and Quadruped Robots in Shared Environments",
    "url": "http://arxiv.org/abs/2503.17014v1",
    "arxiv_id": "2503.17014v1",
    "authors": [
      "Shuang Wei",
      "Muhua Zhang",
      "Yun Gan",
      "Deqing Huang",
      "Lei Ma",
      "Chenguang Yang"
    ],
    "published": "2025-03-21T10:23:06+00:00",
    "summary": "Nowadays, robots are increasingly operated in environments shared with humans, where conflicts between human and robot behaviors may compromise safety. This paper presents a proactive behavioral conflict avoidance framework based on the principle of adaptation to trends for quadruped robots that not only ensures the robot's safety but also minimizes interference with human activities. It can proactively avoid potential conflicts with approaching humans or other dynamic objects, whether the robot is stationary or in motion, then swiftly resume its tasks once the conflict subsides. An enhanced approach is proposed to achieve precise human detection and tracking on vibratory robot platform equipped with low-cost hybrid solid-state LiDAR. When potential conflict detected, the robot selects an avoidance point and executes an evasion maneuver before resuming its task. This approach contrasts with conventional methods that remain goal-driven, often resulting in aggressive behaviors, such as forcibly bypassing obstacles and causing conflicts or becoming stuck in deadlock scenarios. The selection of avoidance points is achieved by integrating static and dynamic obstacle to generate a potential field map. The robot then searches for feasible regions within this map and determines the optimal avoidance point using an evaluation function. Experimental results demonstrate that the framework significantly reduces interference with human activities, enhances the safety of both robots and persons."
  },
  {
    "title": "Extending Behavior Trees for Robotic Missions with Quality Requirements",
    "url": "http://arxiv.org/abs/2503.16969v1",
    "arxiv_id": "2503.16969v1",
    "authors": [
      "Razan Ghzouli",
      "Rebekka Wohlrab",
      "Jennifer Horkoff"
    ],
    "published": "2025-03-21T09:32:25+00:00",
    "summary": "Context and motivation: In recent years, behavior trees have gained growing interest within the robotics community as a specification and control switching mechanism for the different tasks that form a robotics mission. Problem: Given the rising complexity and prevalence of robotic systems, it is increasingly challenging and important for practitioners to design high-quality missions that meet certain qualities, for instance, to consider potential failures or mitigate safety risks. In software requirements engineering, quality or non-functional requirements have long been recognized as a key factor in system success. Currently, qualities are not represented in behavior tree models, which capture a robotic mission, making it difficult to assess the extent to which different mission components comply with those qualities. Principal ideas: In this paper, we propose an extension for behavior trees to have qualities and quality requirements explicitly represented in robotics missions. We provide a meta-model for the extension, develop a domain-specific language (DSL), and describe how we integrated our DSL in one of the most used languages in robotics for developing behavior trees, BehaviorTree.CPP. A preliminary evaluation of the implemented DSL shows promising results for the feasibility of our approach and the need for similar DSLs. Contribution: Our approach paves the way for incorporating qualities into the behavior model of robotics missions. This promotes early expression of qualities in robotics missions, and a better overview of missions components and their contribution to the satisfaction of quality concerns."
  },
  {
    "title": "Somatic Safety: An Embodied Approach Towards Safe Human-Robot Interaction",
    "url": "http://arxiv.org/abs/2503.16960v1",
    "arxiv_id": "2503.16960v1",
    "authors": [
      "Steve Benford",
      "Eike Schneiders",
      "Juan Pablo Martinez Avila",
      "Praminda Caleb-Solly",
      "Patrick Robert Brundell",
      "Simon Castle-Green",
      "Feng Zhou",
      "Rachael Garrett",
      "Kristina H\u00f6\u00f6k",
      "Sarah Whatley",
      "Kate Marsh",
      "Paul Tennent"
    ],
    "published": "2025-03-21T09:09:58+00:00",
    "summary": "As robots enter the messy human world so the vital matter of safety takes on a fresh complexion with physical contact becoming inevitable and even desirable. We report on an artistic-exploration of how dancers, working as part of a multidisciplinary team, engaged in contact improvisation exercises to explore the opportunities and challenges of dancing with cobots. We reveal how they employed their honed bodily senses and physical skills to engage with the robots aesthetically and yet safely, interleaving improvised physical manipulations with reflections to grow their knowledge of how the robots behaved and felt. We introduce somatic safety, a holistic mind-body approach in which safety is learned, felt and enacted through bodily contact with robots in addition to being reasoned about. We conclude that robots need to be better designed for people to hold them and might recognise tacit safety cues among people.We propose that safety should be learned through iterative bodily experience interleaved with reflection."
  },
  {
    "title": "Salient Object Detection in Traffic Scene through the TSOD10K Dataset",
    "url": "http://arxiv.org/abs/2503.16910v1",
    "arxiv_id": "2503.16910v1",
    "authors": [
      "Yu Qiu",
      "Yuhang Sun",
      "Jie Mei",
      "Lin Xiao",
      "Jing Xu"
    ],
    "published": "2025-03-21T07:21:24+00:00",
    "summary": "Traffic Salient Object Detection (TSOD) aims to segment the objects critical to driving safety by combining semantic (e.g., collision risks) and visual saliency. Unlike SOD in natural scene images (NSI-SOD), which prioritizes visually distinctive regions, TSOD emphasizes the objects that demand immediate driver attention due to their semantic impact, even with low visual contrast. This dual criterion, i.e., bridging perception and contextual risk, re-defines saliency for autonomous and assisted driving systems. To address the lack of task-specific benchmarks, we collect the first large-scale TSOD dataset with pixel-wise saliency annotations, named TSOD10K. TSOD10K covers the diverse object categories in various real-world traffic scenes under various challenging weather/illumination variations (e.g., fog, snowstorms, low-contrast, and low-light). Methodologically, we propose a Mamba-based TSOD model, termed Tramba. Considering the challenge of distinguishing inconspicuous visual information from complex traffic backgrounds, Tramba introduces a novel Dual-Frequency Visual State Space module equipped with shifted window partitioning and dilated scanning to enhance the perception of fine details and global structure by hierarchically decomposing high/low-frequency components. To emphasize critical regions in traffic scenes, we propose a traffic-oriented Helix 2D-Selective-Scan (Helix-SS2D) mechanism that injects driving attention priors while effectively capturing global multi-direction spatial dependencies. We establish a comprehensive benchmark by evaluating Tramba and 22 existing NSI-SOD models on TSOD10K, demonstrating Tramba's superiority. Our research establishes the first foundation for safety-aware saliency analysis in intelligent transportation systems."
  },
  {
    "title": "In-House Evaluation Is Not Enough: Towards Robust Third-Party Flaw Disclosure for General-Purpose AI",
    "url": "http://arxiv.org/abs/2503.16861v1",
    "arxiv_id": "2503.16861v1",
    "authors": [
      "Shayne Longpre",
      "Kevin Klyman",
      "Ruth E. Appel",
      "Sayash Kapoor",
      "Rishi Bommasani",
      "Michelle Sahar",
      "Sean McGregor",
      "Avijit Ghosh",
      "Borhane Blili-Hamelin",
      "Nathan Butters",
      "Alondra Nelson",
      "Amit Elazari",
      "Andrew Sellars",
      "Casey John Ellis",
      "Dane Sherrets",
      "Dawn Song",
      "Harley Geiger",
      "Ilona Cohen",
      "Lauren McIlvenny",
      "Madhulika Srikumar",
      "Mark M. Jaycox",
      "Markus Anderljung",
      "Nadine Farid Johnson",
      "Nicholas Carlini",
      "Nicolas Miailhe",
      "Nik Marda",
      "Peter Henderson",
      "Rebecca S. Portnoff",
      "Rebecca Weiss",
      "Victoria Westerhoff",
      "Yacine Jernite",
      "Rumman Chowdhury",
      "Percy Liang",
      "Arvind Narayanan"
    ],
    "published": "2025-03-21T05:09:46+00:00",
    "summary": "The widespread deployment of general-purpose AI (GPAI) systems introduces significant new risks. Yet the infrastructure, practices, and norms for reporting flaws in GPAI systems remain seriously underdeveloped, lagging far behind more established fields like software security. Based on a collaboration between experts from the fields of software security, machine learning, law, social science, and policy, we identify key gaps in the evaluation and reporting of flaws in GPAI systems. We call for three interventions to advance system safety. First, we propose using standardized AI flaw reports and rules of engagement for researchers in order to ease the process of submitting, reproducing, and triaging flaws in GPAI systems. Second, we propose GPAI system providers adopt broadly-scoped flaw disclosure programs, borrowing from bug bounties, with legal safe harbors to protect researchers. Third, we advocate for the development of improved infrastructure to coordinate distribution of flaw reports across the many stakeholders who may be impacted. These interventions are increasingly urgent, as evidenced by the prevalence of jailbreaks and other flaws that can transfer across different providers' GPAI systems. By promoting robust reporting and coordination in the AI ecosystem, these proposals could significantly improve the safety, security, and accountability of GPAI systems."
  },
  {
    "title": "Towards LLM Guardrails via Sparse Representation Steering",
    "url": "http://arxiv.org/abs/2503.16851v1",
    "arxiv_id": "2503.16851v1",
    "authors": [
      "Zeqing He",
      "Zhibo Wang",
      "Huiyu Xu",
      "Kui Ren"
    ],
    "published": "2025-03-21T04:50:25+00:00",
    "summary": "Large Language Models (LLMs) have demonstrated remarkable performance in natural language generation tasks, yet their uncontrolled outputs pose significant ethical and safety risks. Recently, representation engineering methods have shown promising results in steering model behavior by modifying the rich semantic information encoded in activation vectors. However, due to the difficulty of precisely disentangling semantic directions within high-dimensional representation space, existing approaches suffer from three major limitations: lack of fine-grained control, quality degradation of generated content, and poor interpretability. To address these challenges, we propose a sparse encoding-based representation engineering method, named SRE, which decomposes polysemantic activations into a structured, monosemantic feature space. By leveraging sparse autoencoding, our approach isolates and adjusts only task-specific sparse feature dimensions, enabling precise and interpretable steering of model behavior while preserving content quality. We validate our method on three critical domains, i.e., safety, fairness, and truthfulness using the open-source LLM Gemma-2-2B-it. Experimental results show that SRE achieves superior controllability while maintaining the overall quality of generated content (i.e., controllability and quality), demonstrating its effectiveness as a fine-grained and interpretable activation steering framework."
  },
  {
    "title": "Safe On-Orbit Dislodging of Deployable Structures via Robust Adaptive MPC",
    "url": "http://arxiv.org/abs/2503.16849v1",
    "arxiv_id": "2503.16849v1",
    "authors": [
      "Longsen Gao",
      "Claus Danielson",
      "Andrew Kwas",
      "Rafael Fierro"
    ],
    "published": "2025-03-21T04:40:04+00:00",
    "summary": "This paper proposes a novel robust adaptive model predictive controller for on-orbit dislodging. We consider the scenario where a servicer, equipped with a robot arm, must dislodge a client, a time-varying system composed of an underpowered jammed solar panel with a hybrid hinge system on a space station. Our approach leverages online set-membership identification to reduce the uncertainty to provide robust safety guarantees during dislodging despite bounded disturbances while balancing exploration and exploitation effectively in the parameter space. The feasibility of the developed robust adaptive MPC method is also examined through dislodging simulations and hardware experiments in zero-gravity and gravity environments, respectively. In addition, the advantages of our method are shown through comparison experiments with several state-of-the-art control schemes for both accuracy of parameter estimation and control performance."
  },
  {
    "title": "The Deployment of End-to-End Audio Language Models Should Take into Account the Principle of Least Privilege",
    "url": "http://arxiv.org/abs/2503.16833v1",
    "arxiv_id": "2503.16833v1",
    "authors": [
      "Luxi He",
      "Xiangyu Qi",
      "Michel Liao",
      "Inyoung Cheong",
      "Prateek Mittal",
      "Danqi Chen",
      "Peter Henderson"
    ],
    "published": "2025-03-21T04:03:59+00:00",
    "summary": "We are at a turning point for language models that accept audio input. The latest end-to-end audio language models (Audio LMs) process speech directly instead of relying on a separate transcription step. This shift preserves detailed information, such as intonation or the presence of multiple speakers, that would otherwise be lost in transcription. However, it also introduces new safety risks, including the potential misuse of speaker identity cues and other sensitive vocal attributes, which could have legal implications. In this position paper, we urge a closer examination of how these models are built and deployed. We argue that the principle of least privilege should guide decisions on whether to deploy cascaded or end-to-end models. Specifically, evaluations should assess (1) whether end-to-end modeling is necessary for a given application; and (2), the appropriate scope of information access. Finally, We highlight related gaps in current audio LM benchmarks and identify key open research questions, both technical and policy-related, that must be addressed to enable the responsible deployment of end-to-end Audio LMs."
  },
  {
    "title": "Towards Agentic Recommender Systems in the Era of Multimodal Large Language Models",
    "url": "http://arxiv.org/abs/2503.16734v1",
    "arxiv_id": "2503.16734v1",
    "authors": [
      "Chengkai Huang",
      "Junda Wu",
      "Yu Xia",
      "Zixu Yu",
      "Ruhan Wang",
      "Tong Yu",
      "Ruiyi Zhang",
      "Ryan A. Rossi",
      "Branislav Kveton",
      "Dongruo Zhou",
      "Julian McAuley",
      "Lina Yao"
    ],
    "published": "2025-03-20T22:37:15+00:00",
    "summary": "Recent breakthroughs in Large Language Models (LLMs) have led to the emergence of agentic AI systems that extend beyond the capabilities of standalone models. By empowering LLMs to perceive external environments, integrate multimodal information, and interact with various tools, these agentic systems exhibit greater autonomy and adaptability across complex tasks. This evolution brings new opportunities to recommender systems (RS): LLM-based Agentic RS (LLM-ARS) can offer more interactive, context-aware, and proactive recommendations, potentially reshaping the user experience and broadening the application scope of RS. Despite promising early results, fundamental challenges remain, including how to effectively incorporate external knowledge, balance autonomy with controllability, and evaluate performance in dynamic, multimodal settings. In this perspective paper, we first present a systematic analysis of LLM-ARS: (1) clarifying core concepts and architectures; (2) highlighting how agentic capabilities -- such as planning, memory, and multimodal reasoning -- can enhance recommendation quality; and (3) outlining key research questions in areas such as safety, efficiency, and lifelong personalization. We also discuss open problems and future directions, arguing that LLM-ARS will drive the next wave of RS innovation. Ultimately, we foresee a paradigm shift toward intelligent, autonomous, and collaborative recommendation experiences that more closely align with users' evolving needs and complex decision-making processes."
  },
  {
    "title": "Subgradient Method for System Identification with Non-Smooth Objectives",
    "url": "http://arxiv.org/abs/2503.16673v1",
    "arxiv_id": "2503.16673v1",
    "authors": [
      "Baturalp Yalcin",
      "Javad Lavaei"
    ],
    "published": "2025-03-20T19:39:47+00:00",
    "summary": "This paper investigates a subgradient-based algorithm to solve the system identification problem for linear time-invariant systems with non-smooth objectives. This is essential for robust system identification in safety-critical applications. While existing work provides theoretical exact recovery guarantees using optimization solvers, the design of fast learning algorithms with convergence guarantees for practical use remains unexplored. We analyze the subgradient method in this setting where the optimization problems to be solved change over time as new measurements are taken, and we establish linear convergence results for both the best and Polyak step sizes after a burn-in period. Additionally, we characterize the asymptotic convergence of the best average sub-optimality gap under diminishing and constant step sizes. Finally, we compare the time complexity of standard solvers with the subgradient algorithm and support our findings with experimental results. This is the first work to analyze subgradient algorithms for system identification with non-smooth objectives."
  },
  {
    "title": "Stop Overthinking: A Survey on Efficient Reasoning for Large Language Models",
    "url": "http://arxiv.org/abs/2503.16419v1",
    "arxiv_id": "2503.16419v1",
    "authors": [
      "Yang Sui",
      "Yu-Neng Chuang",
      "Guanchu Wang",
      "Jiamu Zhang",
      "Tianyi Zhang",
      "Jiayi Yuan",
      "Hongyi Liu",
      "Andrew Wen",
      "Shaochen",
      "Zhong",
      "Hanjie Chen",
      "Xia Hu"
    ],
    "published": "2025-03-20T17:59:38+00:00",
    "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in complex tasks. Recent advancements in Large Reasoning Models (LRMs), such as OpenAI o1 and DeepSeek-R1, have further improved performance in System-2 reasoning domains like mathematics and programming by harnessing supervised fine-tuning (SFT) and reinforcement learning (RL) techniques to enhance the Chain-of-Thought (CoT) reasoning. However, while longer CoT reasoning sequences improve performance, they also introduce significant computational overhead due to verbose and redundant outputs, known as the \"overthinking phenomenon\". In this paper, we provide the first structured survey to systematically investigate and explore the current progress toward achieving efficient reasoning in LLMs. Overall, relying on the inherent mechanism of LLMs, we categorize existing works into several key directions: (1) model-based efficient reasoning, which considers optimizing full-length reasoning models into more concise reasoning models or directly training efficient reasoning models; (2) reasoning output-based efficient reasoning, which aims to dynamically reduce reasoning steps and length during inference; (3) input prompts-based efficient reasoning, which seeks to enhance reasoning efficiency based on input prompt properties such as difficulty or length control. Additionally, we introduce the use of efficient data for training reasoning models, explore the reasoning capabilities of small language models, and discuss evaluation methods and benchmarking."
  },
  {
    "title": "Survey on Evaluation of LLM-based Agents",
    "url": "http://arxiv.org/abs/2503.16416v1",
    "arxiv_id": "2503.16416v1",
    "authors": [
      "Asaf Yehudai",
      "Lilach Eden",
      "Alan Li",
      "Guy Uziel",
      "Yilun Zhao",
      "Roy Bar-Haim",
      "Arman Cohan",
      "Michal Shmueli-Scheuer"
    ],
    "published": "2025-03-20T17:59:23+00:00",
    "summary": "The emergence of LLM-based agents represents a paradigm shift in AI, enabling autonomous systems to plan, reason, use tools, and maintain memory while interacting with dynamic environments. This paper provides the first comprehensive survey of evaluation methodologies for these increasingly capable agents. We systematically analyze evaluation benchmarks and frameworks across four critical dimensions: (1) fundamental agent capabilities, including planning, tool use, self-reflection, and memory; (2) application-specific benchmarks for web, software engineering, scientific, and conversational agents; (3) benchmarks for generalist agents; and (4) frameworks for evaluating agents. Our analysis reveals emerging trends, including a shift toward more realistic, challenging evaluations with continuously updated benchmarks. We also identify critical gaps that future research must address-particularly in assessing cost-efficiency, safety, and robustness, and in developing fine-grained, and scalable evaluation methods. This survey maps the rapidly evolving landscape of agent evaluation, reveals the emerging trends in the field, identifies current limitations, and proposes directions for future research."
  },
  {
    "title": "Truthful Elicitation of Imprecise Forecasts",
    "url": "http://arxiv.org/abs/2503.16395v1",
    "arxiv_id": "2503.16395v1",
    "authors": [
      "Anurag Singh",
      "Siu Lun Chau",
      "Krikamol Muandet"
    ],
    "published": "2025-03-20T17:53:35+00:00",
    "summary": "The quality of probabilistic forecasts is crucial for decision-making under uncertainty. While proper scoring rules incentivize truthful reporting of precise forecasts, they fall short when forecasters face epistemic uncertainty about their beliefs, limiting their use in safety-critical domains where decision-makers (DMs) prioritize proper uncertainty management. To address this, we propose a framework for scoring imprecise forecasts -- forecasts given as a set of beliefs. Despite existing impossibility results for deterministic scoring rules, we enable truthful elicitation by drawing connection to social choice theory and introducing a two-way communication framework where DMs first share their aggregation rules (e.g., averaging or min-max) used in downstream decisions for resolving forecast ambiguity. This, in turn, helps forecasters resolve indecision during elicitation. We further show that truthful elicitation of imprecise forecasts is achievable using proper scoring rules randomized over the aggregation procedure. Our approach allows DM to elicit and integrate the forecaster's epistemic uncertainty into their decision-making process, thus improving credibility."
  },
  {
    "title": "Bridging Technology and Humanities: Evaluating the Impact of Large Language Models on Social Sciences Research with DeepSeek-R1",
    "url": "http://arxiv.org/abs/2503.16304v1",
    "arxiv_id": "2503.16304v1",
    "authors": [
      "Peiran Gu",
      "Fuhao Duan",
      "Wenhao Li",
      "Bochen Xu",
      "Ying Cai",
      "Teng Yao",
      "Chenxun Zhuo",
      "Tianming Liu",
      "Bao Ge"
    ],
    "published": "2025-03-20T16:25:24+00:00",
    "summary": "In recent years, the development of Large Language Models (LLMs) has made significant breakthroughs in the field of natural language processing and has gradually been applied to the field of humanities and social sciences research. LLMs have a wide range of application value in the field of humanities and social sciences because of its strong text understanding, generation and reasoning capabilities. In humanities and social sciences research, LLMs can analyze large-scale text data and make inferences.   This article analyzes the large language model DeepSeek-R1 from seven aspects: low-resource language translation, educational question-answering, student writing improvement in higher education, logical reasoning, educational measurement and psychometrics, public health policy analysis, and art education.Then we compare the answers given by DeepSeek-R1 in the seven aspects with the answers given by o1-preview. DeepSeek-R1 performs well in the humanities and social sciences, answering most questions correctly and logically, and can give reasonable analysis processes and explanations. Compared with o1-preview, it can automatically generate reasoning processes and provide more detailed explanations, which is suitable for beginners or people who need to have a detailed understanding of this knowledge, while o1-preview is more suitable for quick reading.   Through analysis, it is found that LLM has broad application potential in the field of humanities and social sciences, and shows great advantages in improving text analysis efficiency, language communication and other fields. LLM's powerful language understanding and generation capabilities enable it to deeply explore complex problems in the field of humanities and social sciences, and provide innovative tools for academic research and practical applications."
  },
  {
    "title": "Bridging Technology and Humanities: Evaluating the Impact of Large Language Models on Social Sciences Research with DeepSeek-R1",
    "url": "http://arxiv.org/abs/2503.16304v2",
    "arxiv_id": "2503.16304v2",
    "authors": [
      "Peiran Gu",
      "Fuhao Duan",
      "Wenhao Li",
      "Bochen Xu",
      "Ying Cai",
      "Teng Yao",
      "Chenxun Zhuo",
      "Tianming Liu",
      "Bao Ge"
    ],
    "published": "2025-03-20T16:25:24+00:00",
    "summary": "In recent years, the development of Large Language Models (LLMs) has made significant breakthroughs in the field of natural language processing and has gradually been applied to the field of humanities and social sciences research. LLMs have a wide range of application value in the field of humanities and social sciences because of its strong text understanding, generation and reasoning capabilities. In humanities and social sciences research, LLMs can analyze large-scale text data and make inferences.   This article analyzes the large language model DeepSeek-R1 from seven aspects: low-resource language translation, educational question-answering, student writing improvement in higher education, logical reasoning, educational measurement and psychometrics, public health policy analysis, and art education.Then we compare the answers given by DeepSeek-R1 in the seven aspects with the answers given by o1-preview. DeepSeek-R1 performs well in the humanities and social sciences, answering most questions correctly and logically, and can give reasonable analysis processes and explanations. Compared with o1-preview, it can automatically generate reasoning processes and provide more detailed explanations, which is suitable for beginners or people who need to have a detailed understanding of this knowledge, while o1-preview is more suitable for quick reading.   Through analysis, it is found that LLM has broad application potential in the field of humanities and social sciences, and shows great advantages in improving text analysis efficiency, language communication and other fields. LLM's powerful language understanding and generation capabilities enable it to deeply explore complex problems in the field of humanities and social sciences, and provide innovative tools for academic research and practical applications."
  },
  {
    "title": "Securing Satellite Communications: Real-Time Video Encryption Scheme on Satellite Payloads",
    "url": "http://arxiv.org/abs/2503.16287v1",
    "arxiv_id": "2503.16287v1",
    "authors": [
      "Hanshuo Qiu",
      "Jing Lian",
      "Xiaoyuan Wang",
      "Jizhao Liu"
    ],
    "published": "2025-03-20T16:14:14+00:00",
    "summary": "The rapid development of low-Earth orbit (LEO) satellite constellations and satellite communication systems has elevated the importance of secure video transmission, which is the key to applications such as remote sensing, disaster relief, and secure information exchange. In this context, three serious issues arise concerning real-time encryption of videos on satellite embedded devices: (a) the challenge of achieving real-time performance; (b) the limitations posed by the constrained computing performance of satellite payloads; and (c) the potential for excessive power consumption leading to overheating, thereby escalating safety risks. To overcome these challenges, this study introduced a novel approach for encrypting videos by employing two 1D chaotic maps, which was deployed on a satellite for the first time. The experiment on the satellite confirms that our scheme is suitable for complex satellite environments. In addition, the proposed chaotic maps were implemented on a Field Programmable Gate Array (FPGA) platform, and simulation results showed consistency with those obtained on a Raspberry Pi. Experiments on the Raspberry Pi 4B demonstrate exceptional real-time performance and low power consumption, validating both the hardware feasibility and the stability of our design. Rigorous statistical testing also confirms the scheme's resilience against a variety of attacks, underscoring its potential for secure, real-time data transmission in satellite communication systems."
  },
  {
    "title": "Fin-R1: A Large Language Model for Financial Reasoning through Reinforcement Learning",
    "url": "http://arxiv.org/abs/2503.16252v1",
    "arxiv_id": "2503.16252v1",
    "authors": [
      "Zhaowei Liu",
      "Xin Guo",
      "Fangqi Lou",
      "Lingfeng Zeng",
      "Jinyi Niu",
      "Zixuan Wang",
      "Jiajie Xu",
      "Weige Cai",
      "Ziwei Yang",
      "Xueqian Zhao",
      "Chao Li",
      "Sheng Xu",
      "Dezhi Chen",
      "Yun Chen",
      "Zuo Bai",
      "Liwen Zhang"
    ],
    "published": "2025-03-20T15:46:18+00:00",
    "summary": "Reasoning large language models are rapidly evolving across various domains. However, their capabilities in handling complex financial tasks still require in-depth exploration. In this paper, we introduce Fin-R1, a reasoning large language model specifically designed for the financial sector. Fin-R1 is built using a two-stage architecture, leveraging a financial reasoning dataset distilled and processed based on DeepSeek-R1. Through supervised fine-tuning (SFT) and reinforcement learning (RL) training, it demonstrates performance close to DeepSeek-R1 with a parameter size of 7 billion across a range of financial reasoning tasks. It achieves the state-of-the-art (SOTA) in the FinQA and ConvFinQA tasks between those LLMs in our evaluation, surpassing larger models in other tasks as well. Fin-R1 showcases strong reasoning and decision-making capabilities, providing solutions to various problems encountered in the financial domain. Our code is available at https://github.com/SUFE-AIFLM-Lab/Fin-R1."
  },
  {
    "title": "Fin-R1: A Large Language Model for Financial Reasoning through Reinforcement Learning",
    "url": "http://arxiv.org/abs/2503.16252v2",
    "arxiv_id": "2503.16252v2",
    "authors": [
      "Zhaowei Liu",
      "Xin Guo",
      "Fangqi Lou",
      "Lingfeng Zeng",
      "Jinyi Niu",
      "Zixuan Wang",
      "Jiajie Xu",
      "Weige Cai",
      "Ziwei Yang",
      "Xueqian Zhao",
      "Chao Li",
      "Sheng Xu",
      "Dezhi Chen",
      "Yun Chen",
      "Zuo Bai",
      "Liwen Zhang"
    ],
    "published": "2025-03-20T15:46:18+00:00",
    "summary": "Reasoning large language models are rapidly evolving across various domains. However, their capabilities in handling complex financial tasks still require in-depth exploration. In this paper, we introduce Fin-R1, a reasoning large language model specifically designed for the financial sector. Fin-R1 is built using a two-stage architecture, leveraging a financial reasoning dataset distilled and processed based on DeepSeek-R1. Through supervised fine-tuning (SFT) and reinforcement learning (RL) training, it demonstrates performance close to DeepSeek-R1 with a parameter size of 7 billion across a range of financial reasoning tasks. It achieves the state-of-the-art (SOTA) in the FinQA and ConvFinQA tasks between those LLMs in our evaluation, surpassing larger models in other tasks as well. Fin-R1 showcases strong reasoning and decision-making capabilities, providing solutions to various problems encountered in the financial domain. Our code is available at https://github.com/SUFE-AIFLM-Lab/Fin-R1."
  },
  {
    "title": "Flight Testing an Optionally Piloted Aircraft: a Case Study on Trust Dynamics in Human-Autonomy Teaming",
    "url": "http://arxiv.org/abs/2503.16227v1",
    "arxiv_id": "2503.16227v1",
    "authors": [
      "Jeremy C. -H. Wang",
      "Ming Hou",
      "David Dunwoody",
      "Marko Ilievski",
      "Justin Tomasi",
      "Edward Chao",
      "Carl Pigeon"
    ],
    "published": "2025-03-20T15:22:39+00:00",
    "summary": "This paper examines how trust is formed, maintained, or diminished over time in the context of human-autonomy teaming with an optionally piloted aircraft. Whereas traditional factor-based trust models offer a static representation of human confidence in technology, here we discuss how variations in the underlying factors lead to variations in trust, trust thresholds, and human behaviours. Over 200 hours of flight test data collected over a multi-year test campaign from 2021 to 2023 were reviewed. The dispositional-situational-learned, process-performance-purpose, and IMPACTS homeostasis trust models are applied to illuminate trust trends during nominal autonomous flight operations. The results offer promising directions for future studies on trust dynamics and design-for-trust in human-autonomy teaming."
  },
  {
    "title": "Reinforcement Learning for Reasoning in Small LLMs: What Works and What Doesn't",
    "url": "http://arxiv.org/abs/2503.16219v1",
    "arxiv_id": "2503.16219v1",
    "authors": [
      "Quy-Anh Dang",
      "Chris Ngo"
    ],
    "published": "2025-03-20T15:13:23+00:00",
    "summary": "Enhancing the reasoning capabilities of large language models (LLMs) typically relies on massive computational resources and extensive datasets, limiting accessibility for resource-constrained settings. Our study investigates the potential of reinforcement learning (RL) to improve reasoning in small LLMs, focusing on a 1.5-billion-parameter model, DeepSeek-R1-Distill-Qwen-1.5B, under strict constraints: training on 4 NVIDIA A40 GPUs (48 GB VRAM each) within 24 hours. Adapting the Group Relative Policy Optimization (GRPO) algorithm and curating a compact, high-quality mathematical reasoning dataset, we conducted three experiments to explore model behavior and performance. Our results demonstrate rapid reasoning gains - e.g., AMC23 accuracy rising from 63% to 80% and AIME24 reaching 46.7%, surpassing o1-preview - using only 7,000 samples and a $42 training cost, compared to thousands of dollars for baseline models. However, challenges such as optimization instability and length constraints emerged with prolonged training. These findings highlight the efficacy of RL-based fine-tuning for small LLMs, offering a cost-effective alternative to large-scale approaches. We release our code and datasets as open-source resources, providing insights into trade-offs and laying a foundation for scalable, reasoning-capable LLMs in resource-limited environments. All are available at https://github.com/knoveleng/open-rs."
  },
  {
    "title": "Iterative Optimal Attention and Local Model for Single Image Rain Streak Removal",
    "url": "http://arxiv.org/abs/2503.16165v1",
    "arxiv_id": "2503.16165v1",
    "authors": [
      "Xiangyu Li",
      "Wanshu Fan",
      "Yue Shen",
      "Cong Wang",
      "Wei Wang",
      "Xin Yang",
      "Qiang Zhang",
      "Dongsheng Zhou"
    ],
    "published": "2025-03-20T14:06:53+00:00",
    "summary": "High-fidelity imaging is crucial for the successful safety supervision and intelligent deployment of vision-based measurement systems (VBMS). It ensures high-quality imaging in VBMS, which is fundamental for reliable visual measurement and analysis. However, imaging quality can be significantly impaired by adverse weather conditions, particularly rain, leading to blurred images and reduced contrast. Such impairments increase the risk of inaccurate evaluations and misinterpretations in VBMS. To address these limitations, we propose an Expectation Maximization Reconstruction Transformer (EMResformer) for single image rain streak removal. The EMResformer retains the key self-attention values for feature aggregation, enhancing local features to produce superior image reconstruction. Specifically, we propose an Expectation Maximization Block seamlessly integrated into the single image rain streak removal network, enhancing its ability to eliminate superfluous information and restore a cleaner background image. Additionally, to further enhance local information for improved detail rendition, we introduce a Local Model Residual Block, which integrates two local model blocks along with a sequence of convolutions and activation functions. This integration synergistically facilitates the extraction of more pertinent features for enhanced single image rain streak removal. Extensive experiments validate that our proposed EMResformer surpasses current state-of-the-art single image rain streak removal methods on both synthetic and real-world datasets, achieving an improved balance between model complexity and single image deraining performance. Furthermore, we evaluate the effectiveness of our method in VBMS scenarios, demonstrating that high-quality imaging significantly improves the accuracy and reliability of VBMS tasks."
  },
  {
    "title": "Hyperspectral Imaging for Identifying Foreign Objects on Pork Belly",
    "url": "http://arxiv.org/abs/2503.16086v1",
    "arxiv_id": "2503.16086v1",
    "authors": [
      "Gabriela Ghimpeteanu",
      "Hayat Rajani",
      "Josep Quintana",
      "Rafael Garcia"
    ],
    "published": "2025-03-20T12:28:31+00:00",
    "summary": "Ensuring food safety and quality is critical in the food processing industry, where the detection of contaminants remains a persistent challenge. This study presents an automated solution for detecting foreign objects on pork belly meat using hyperspectral imaging (HSI). A hyperspectral camera was used to capture data across various bands in the near-infrared (NIR) spectrum (900-1700 nm), enabling accurate identification of contaminants that are often undetectable through traditional visual inspection methods. The proposed solution combines pre-processing techniques with a segmentation approach based on a lightweight Vision Transformer (ViT) to distinguish contaminants from meat, fat, and conveyor belt materials. The adopted strategy demonstrates high detection accuracy and training efficiency, while also addressing key industrial challenges such as inherent noise, temperature variations, and spectral similarity between contaminants and pork belly. Experimental results validate the effectiveness of hyperspectral imaging in enhancing food safety, highlighting its potential for broad real-time applications in automated quality control processes."
  },
  {
    "title": "Evaluating Test-Time Scaling LLMs for Legal Reasoning: OpenAI o1, DeepSeek-R1, and Beyond",
    "url": "http://arxiv.org/abs/2503.16040v1",
    "arxiv_id": "2503.16040v1",
    "authors": [
      "Yaoyao Yu",
      "Leilei Gan",
      "Yinghao Hu",
      "Bin Wei",
      "Kun Kuang",
      "Fei Wu"
    ],
    "published": "2025-03-20T11:14:39+00:00",
    "summary": "Recently, Test-Time Scaling Large Language Models (LLMs), such as DeepSeek-R1 and OpenAI o1, have demonstrated exceptional capabilities across various domains and tasks, particularly in reasoning. While these models have shown impressive performance on general language tasks, their effectiveness in specialized fields like legal remains unclear. To address this, we present a preliminary evaluation of LLMs in various legal scenarios, covering both Chinese and English legal tasks. Our analysis includes 9 LLMs and 17 legal tasks, with a focus on newly published and more complex challenges such as multi-defendant legal judgments and legal argument reasoning. Our findings indicate that, despite DeepSeek-R1 and OpenAI o1 being among the most powerful models, their legal reasoning capabilities are still lacking. Specifically, these models score below 80\\% on seven Chinese legal reasoning tasks and below 80\\% on two English legal reasoning tasks. This suggests that, even among the most advanced reasoning models, legal reasoning abilities remain underdeveloped."
  },
  {
    "title": "GAN-enhanced Simulation-driven DNN Testing in Absence of Ground Truth",
    "url": "http://arxiv.org/abs/2503.15953v1",
    "arxiv_id": "2503.15953v1",
    "authors": [
      "Mohammed Attaoui",
      "Fabrizio Pastore"
    ],
    "published": "2025-03-20T08:49:10+00:00",
    "summary": "The generation of synthetic inputs via simulators driven by search algorithms is essential for cost-effective testing of Deep Neural Network (DNN) components for safety-critical systems. However, in many applications, simulators are unable to produce the ground-truth data needed for automated test oracles and to guide the search process.   To tackle this issue, we propose an approach for the generation of inputs for computer vision DNNs that integrates a generative network to ensure simulator fidelity and employs heuristic-based search fitnesses that leverage transformation consistency, noise resistance, surprise adequacy, and uncertainty estimation. We compare the performance of our fitnesses with that of a traditional fitness function leveraging ground truth; further, we assess how the integration of a GAN not leveraging the ground truth impacts on test and retraining effectiveness.   Our results suggest that leveraging transformation consistency is the best option to generate inputs for both DNN testing and retraining; it maximizes input diversity, spots the inputs leading to worse DNN performance, and leads to best DNN performance after retraining. Besides enabling simulator-based testing in the absence of ground truth, our findings pave the way for testing solutions that replace costly simulators with diffusion and large language models, which might be more affordable than simulators, but cannot generate ground-truth data."
  },
  {
    "title": "Adaptive Group Policy Optimization: Towards Stable Training and Token-Efficient Reasoning",
    "url": "http://arxiv.org/abs/2503.15952v1",
    "arxiv_id": "2503.15952v1",
    "authors": [
      "Chen Li",
      "Nazhou Liu",
      "Kai Yang"
    ],
    "published": "2025-03-20T08:48:57+00:00",
    "summary": "Since DeepSeek-R1 popularized, Group Relative Policy Optimization (GRPO) has become the core part of Reasoning LLMs training. However, we find some deficiency that influences RL stability and inference efficiency. Thus, we propose Adaptive Group Policy Optimization (AGPO) which contains two simple but effective modifications: a revised advantage estimation method to mitigate zero-variance situations; a length-based reward, incentivizing the model to avoid overthinking. The experiments demonstrate our methods achieve more stable training and comparable or superior performance with significantly fewer tokens in reasoning steps."
  },
  {
    "title": "Multivariate Time Series Anomaly Detection in Industry 5.0",
    "url": "http://arxiv.org/abs/2503.15946v1",
    "arxiv_id": "2503.15946v1",
    "authors": [
      "Lorenzo Colombi",
      "Michela Vespa",
      "Nicolas Belletti",
      "Matteo Brina",
      "Simon Dahdal",
      "Filippo Tabanelli",
      "Elena Bellodi",
      "Mauro Tortonesi",
      "Cesare Stefanelli",
      "Massimiliano Vignoli"
    ],
    "published": "2025-03-20T08:38:58+00:00",
    "summary": "Industry5.0 environments present a critical need for effective anomaly detection methods that can indicate equipment malfunctions, process inefficiencies, or potential safety hazards. The ever-increasing sensorization of manufacturing lines makes processes more observable, but also poses the challenge of continuously analyzing vast amounts of multivariate time series data. These challenges include data quality since data may contain noise, be unlabeled or even mislabeled. A promising approach consists of combining an embedding model with other Machine Learning algorithms to enhance the overall performance in detecting anomalies. Moreover, representing time series as vectors brings many advantages like higher flexibility and improved ability to capture complex temporal dependencies. We tested our solution in a real industrial use case, using data collected from a Bonfiglioli plant. The results demonstrate that, unlike traditional reconstruction-based autoencoders, which often struggle in the presence of sporadic noise, our embedding-based framework maintains high performance across various noise conditions."
  },
  {
    "title": "Social Media for Activists: Reimagining Safety, Content Presentation, and Workflows",
    "url": "http://arxiv.org/abs/2503.15942v1",
    "arxiv_id": "2503.15942v1",
    "authors": [
      "Anna Ricarda Luther",
      "Hendrik Heuer",
      "Stephanie Geise",
      "Sebastian Haunss",
      "Andreas Breiter"
    ],
    "published": "2025-03-20T08:33:01+00:00",
    "summary": "Social media is central to activists, who use it internally for coordination and externally to reach supporters and the public. To date, the HCI community has not explored activists' perspectives on future social media platforms. In interviews with 14 activists from an environmental and a queer-feminist movement in Germany, we identify activists' needs and feature requests for future social media platforms. The key finding is that on- and offline safety is their main need. Based on this, we make concrete proposals to improve safety measures. Increased control over content presentation and tools to streamline activist workflows are also central to activists. We make concrete design and research recommendations on how social media platforms and the HCI community can contribute to improved safety and content presentation, and how activists themselves can reduce their workload."
  },
  {
    "title": "REVAL: A Comprehension Evaluation on Reliability and Values of Large Vision-Language Models",
    "url": "http://arxiv.org/abs/2503.16566v1",
    "arxiv_id": "2503.16566v1",
    "authors": [
      "Jie Zhang",
      "Zheng Yuan",
      "Zhongqi Wang",
      "Bei Yan",
      "Sibo Wang",
      "Xiangkui Cao",
      "Zonghui Guo",
      "Shiguang Shan",
      "Xilin Chen"
    ],
    "published": "2025-03-20T07:54:35+00:00",
    "summary": "The rapid evolution of Large Vision-Language Models (LVLMs) has highlighted the necessity for comprehensive evaluation frameworks that assess these models across diverse dimensions. While existing benchmarks focus on specific aspects such as perceptual abilities, cognitive capabilities, and safety against adversarial attacks, they often lack the breadth and depth required to provide a holistic understanding of LVLMs' strengths and limitations. To address this gap, we introduce REVAL, a comprehensive benchmark designed to evaluate the \\textbf{RE}liability and \\textbf{VAL}ue of LVLMs. REVAL encompasses over 144K image-text Visual Question Answering (VQA) samples, structured into two primary sections: Reliability, which assesses truthfulness (\\eg, perceptual accuracy and hallucination tendencies) and robustness (\\eg, resilience to adversarial attacks, typographic attacks, and image corruption), and Values, which evaluates ethical concerns (\\eg, bias and moral understanding), safety issues (\\eg, toxicity and jailbreak vulnerabilities), and privacy problems (\\eg, privacy awareness and privacy leakage). We evaluate 26 models, including mainstream open-source LVLMs and prominent closed-source models like GPT-4o and Gemini-1.5-Pro. Our findings reveal that while current LVLMs excel in perceptual tasks and toxicity avoidance, they exhibit significant vulnerabilities in adversarial scenarios, privacy preservation, and ethical reasoning. These insights underscore critical areas for future improvements, guiding the development of more secure, reliable, and ethically aligned LVLMs. REVAL provides a robust framework for researchers to systematically assess and compare LVLMs, fostering advancements in the field."
  },
  {
    "title": "No Thing, Nothing: Highlighting Safety-Critical Classes for Robust LiDAR Semantic Segmentation in Adverse Weather",
    "url": "http://arxiv.org/abs/2503.15910v1",
    "arxiv_id": "2503.15910v1",
    "authors": [
      "Junsung Park",
      "Hwijeong Lee",
      "Inha Kang",
      "Hyunjung Shim"
    ],
    "published": "2025-03-20T07:40:24+00:00",
    "summary": "Existing domain generalization methods for LiDAR semantic segmentation under adverse weather struggle to accurately predict \"things\" categories compared to \"stuff\" categories. In typical driving scenes, \"things\" categories can be dynamic and associated with higher collision risks, making them crucial for safe navigation and planning. Recognizing the importance of \"things\" categories, we identify their performance drop as a serious bottleneck in existing approaches. We observed that adverse weather induces degradation of semantic-level features and both corruption of local features, leading to a misprediction of \"things\" as \"stuff\". To mitigate these corruptions, we suggest our method, NTN - segmeNt Things for No-accident. To address semantic-level feature corruption, we bind each point feature to its superclass, preventing the misprediction of things classes into visually dissimilar categories. Additionally, to enhance robustness against local corruption caused by adverse weather, we define each LiDAR beam as a local region and propose a regularization term that aligns the clean data with its corrupted counterpart in feature space. NTN achieves state-of-the-art performance with a +2.6 mIoU gain on the SemanticKITTI-to-SemanticSTF benchmark and +7.9 mIoU on the SemanticPOSS-to-SemanticSTF benchmark. Notably, NTN achieves a +4.8 and +7.9 mIoU improvement on \"things\" classes, respectively, highlighting its effectiveness."
  },
  {
    "title": "Automatic Generation of Safety-compliant Linear Temporal Logic via Large Language Model: A Self-supervised Framework",
    "url": "http://arxiv.org/abs/2503.15840v1",
    "arxiv_id": "2503.15840v1",
    "authors": [
      "Junle Li",
      "Meiqi Tian",
      "Bingzhuo Zhong"
    ],
    "published": "2025-03-20T04:40:29+00:00",
    "summary": "Ensuring safety in cyber-physical systems (CPS) poses a significant challenge, especially when converting high-level tasks described by natural language into formal specifications like Linear Temporal Logic (LTL). In particular, the compliance of formal languages with respect to safety restrictions imposed on CPS is crucial for system safety. In this paper, we introduce AutoSafeLTL, a self-supervised framework that utilizes large language models (LLMs) to automate the generation of safety-compliant LTL. Our approach integrates a Language Inclusion check with an automated counterexample-guided feedback and modification mechanism, establishing a pipeline that verifies the safety-compliance of the resulting LTL while preserving its logical consistency and semantic accuracy. To enhance the framework's understanding and correction capabilities, we incorporate two additional Agent LLMs. Experimental results demonstrate that AutoSafeLTL effectively guarantees safety-compliance for generated LTL, achieving a 0% violation rate against imposed safety constraints."
  },
  {
    "title": "APEX-MR: Multi-Robot Asynchronous Planning and Execution for Cooperative Assembly",
    "url": "http://arxiv.org/abs/2503.15836v1",
    "arxiv_id": "2503.15836v1",
    "authors": [
      "Philip Huang",
      "Ruixuan Liu",
      "Changliu Liu",
      "Jiaoyang Li"
    ],
    "published": "2025-03-20T04:25:38+00:00",
    "summary": "Compared to a single-robot workstation, a multi-robot system offers several advantages: 1) it expands the system's workspace, 2) improves task efficiency, and more importantly, 3) enables robots to achieve significantly more complex and dexterous tasks, such as cooperative assembly. However, coordinating the tasks and motions of multiple robots is challenging due to issues, e.g. system uncertainty, task efficiency, algorithm scalability, and safety concerns. To address these challenges, this paper studies multi-robot coordination and proposes APEX-MR, an asynchronous planning and execution framework designed to safely and efficiently coordinate multiple robots to achieve cooperative assembly, e.g. LEGO assembly. In particular, APEX-MR provides a systematic approach to post-process multi-robot tasks and motion plans to enable robust asynchronous execution under uncertainty. Experimental results demonstrate that APEX-MR can significantly speed up the execution time of many long-horizon LEGO assembly tasks by 48% compared to sequential planning and 36% compared to synchronous planning on average. To further demonstrate the performance, we deploy APEX-MR to a dual-arm system to perform physical LEGO assembly. To our knowledge, this is the first robotic system capable of performing customized LEGO assembly using commercial LEGO bricks. The experiment results demonstrate that the dual-arm system, with APEX-MR, can safely coordinate robot motions, efficiently collaborate, and construct complex LEGO structures. Our project website is available at https://intelligent-control-lab.github.io/APEX-MR/"
  },
  {
    "title": "Efficient Symbolic Execution of Software under Fault Attacks",
    "url": "http://arxiv.org/abs/2503.15825v1",
    "arxiv_id": "2503.15825v1",
    "authors": [
      "Yuzhou Fang",
      "Chenyu Zhou",
      "Jingbo Wang",
      "Chao Wang"
    ],
    "published": "2025-03-20T03:19:48+00:00",
    "summary": "We propose a symbolic method for analyzing the safety of software under fault attacks both accurately and efficiently. Fault attacks leverage physically injected hardware faults to break the safety of a software program. While there are existing methods for analyzing the impact of faults on software, they suffer from inaccurate fault modeling and inefficient analysis algorithms. We propose two new techniques to overcome these problems. First, we propose a fault modeling technique that leverages program transformation to add symbolic variables to the program, to accurately model the fault-induced program behavior. Second, we propose a redundancy pruning technique that leverages the weakest precondition and fault saturation to mitigate path explosion, which is a performance bottleneck of symbolic execution that is exacerbated by the fault-induced program behavior. We have implemented the method and evaluated it on a variety of benchmark programs. The experimental results show that our method significantly outperforms the state-of-the-art method. Specifically, it not only reveals many previously-missed safety violations but also reduces the running time drastically. Compared to the baseline, our optimized method is 2.0$\\times$ faster on average."
  },
  {
    "title": "A Unified Stability Analysis of Safety-Critical Control using Multiple Control Barrier Functions",
    "url": "http://arxiv.org/abs/2503.15823v1",
    "arxiv_id": "2503.15823v1",
    "authors": [
      "Matheus F. Reis",
      "A. Pedro Aguiar"
    ],
    "published": "2025-03-20T03:16:38+00:00",
    "summary": "Ensuring liveness and safety of autonomous and cyber-physical systems remains a fundamental challenge, particularly when multiple safety constraints are present. This letter advances the theoretical foundations of safety-filter Quadratic Programs (QP) and Control Lyapunov Function (CLF)-Control Barrier Function (CBF) controllers by establishing a unified analytical framework for studying their stability properties. We derive sufficient feasibility conditions for QPs with multiple CBFs and formally characterize the conditions leading to undesirable equilibrium points at possible intersecting safe set boundaries. Additionally, we introduce a stability criterion for equilibrium points, providing a systematic approach to identifying conditions under which they can be destabilized or eliminated. Our analysis extends prior theoretical results, deepening the understanding of the conditions of feasibility and stability of CBF-based safety filters and the CLF-CBF QP framework."
  },
  {
    "title": "A Unified Stability Analysis of Safety-Critical Control using Multiple Control Barrier Functions",
    "url": "http://arxiv.org/abs/2503.15823v2",
    "arxiv_id": "2503.15823v2",
    "authors": [
      "Matheus F. Reis",
      "Jos\u00e9 P. Carvalho",
      "A. Pedro Aguiar"
    ],
    "published": "2025-03-20T03:16:38+00:00",
    "summary": "Ensuring liveness and safety of autonomous and cyber-physical systems remains a fundamental challenge, particularly when multiple safety constraints are present. This letter advances the theoretical foundations of safety-filter Quadratic Programs (QP) and Control Lyapunov Function (CLF)-Control Barrier Function (CBF) controllers by establishing a unified analytical framework for studying their stability properties. We derive sufficient feasibility conditions for QPs with multiple CBFs and formally characterize the conditions leading to undesirable equilibrium points at possible intersecting safe set boundaries. Additionally, we introduce a stability criterion for equilibrium points, providing a systematic approach to identifying conditions under which they can be destabilized or eliminated. Our analysis extends prior theoretical results, deepening the understanding of the conditions of feasibility and stability of CBF-based safety filters and the CLF-CBF QP framework."
  },
  {
    "title": "DNA Bench: When Silence is Smarter -- Benchmarking Over-Reasoning in Reasoning LLMs",
    "url": "http://arxiv.org/abs/2503.15793v1",
    "arxiv_id": "2503.15793v1",
    "authors": [
      "Masoud Hashemi",
      "Oluwanifemi Bamgbose",
      "Sathwik Tejaswi Madhusudhan",
      "Jishnu Sethumadhavan Nair",
      "Aman Tiwari",
      "Vikas Yadav"
    ],
    "published": "2025-03-20T02:19:14+00:00",
    "summary": "Test-time scaling has significantly improved large language model performance, enabling deeper reasoning to solve complex problems. However, this increased reasoning capability also leads to excessive token generation and unnecessary problem-solving attempts. We introduce Don\\'t Answer Bench (DNA Bench), a new benchmark designed to evaluate LLMs ability to robustly understand the tricky reasoning triggers and avoiding unnecessary generation. DNA Bench consists of 150 adversarially designed prompts that are easy for humans to understand and respond to, but surprisingly not for many of the recent prominent LLMs. DNA Bench tests models abilities across different capabilities, such as instruction adherence, hallucination avoidance, redundancy filtering, and unanswerable question recognition. We evaluate reasoning LLMs (RLMs), including DeepSeek-R1, OpenAI O3-mini, Claude-3.7-sonnet and compare them against a powerful non-reasoning model, e.g., GPT-4o. Our experiments reveal that RLMs generate up to 70x more tokens than necessary, often failing at tasks that simpler non-reasoning models handle efficiently with higher accuracy. Our findings underscore the need for more effective training and inference strategies in RLMs."
  },
  {
    "title": "Detecting LLM-Written Peer Reviews",
    "url": "http://arxiv.org/abs/2503.15772v1",
    "arxiv_id": "2503.15772v1",
    "authors": [
      "Vishisht Rao",
      "Aounon Kumar",
      "Himabindu Lakkaraju",
      "Nihar B. Shah"
    ],
    "published": "2025-03-20T01:11:35+00:00",
    "summary": "Editors of academic journals and program chairs of conferences require peer reviewers to write their own reviews. However, there is growing concern about the rise of lazy reviewing practices, where reviewers use large language models (LLMs) to generate reviews instead of writing them independently. Existing tools for detecting LLM-generated content are not designed to differentiate between fully LLM-generated reviews and those merely polished by an LLM. In this work, we employ a straightforward approach to identify LLM-generated reviews - doing an indirect prompt injection via the paper PDF to ask the LLM to embed a watermark. Our focus is on presenting watermarking schemes and statistical tests that maintain a bounded family-wise error rate, when a venue evaluates multiple reviews, with a higher power as compared to standard methods like Bonferroni correction. These guarantees hold without relying on any assumptions about human-written reviews. We also consider various methods for prompt injection including font embedding and jailbreaking. We evaluate the effectiveness and various tradeoffs of these methods, including different reviewer defenses. We find a high success rate in the embedding of our watermarks in LLM-generated reviews across models. We also find that our approach is resilient to common reviewer defenses, and that the bounds on error rates in our statistical tests hold in practice while having the power to flag LLM-generated reviews, while Bonferroni correction is infeasible."
  },
  {
    "title": "AutoRedTeamer: Autonomous Red Teaming with Lifelong Attack Integration",
    "url": "http://arxiv.org/abs/2503.15754v1",
    "arxiv_id": "2503.15754v1",
    "authors": [
      "Andy Zhou",
      "Kevin Wu",
      "Francesco Pinto",
      "Zhaorun Chen",
      "Yi Zeng",
      "Yu Yang",
      "Shuang Yang",
      "Sanmi Koyejo",
      "James Zou",
      "Bo Li"
    ],
    "published": "2025-03-20T00:13:04+00:00",
    "summary": "As large language models (LLMs) become increasingly capable, security and safety evaluation are crucial. While current red teaming approaches have made strides in assessing LLM vulnerabilities, they often rely heavily on human input and lack comprehensive coverage of emerging attack vectors. This paper introduces AutoRedTeamer, a novel framework for fully automated, end-to-end red teaming against LLMs. AutoRedTeamer combines a multi-agent architecture with a memory-guided attack selection mechanism to enable continuous discovery and integration of new attack vectors. The dual-agent framework consists of a red teaming agent that can operate from high-level risk categories alone to generate and execute test cases and a strategy proposer agent that autonomously discovers and implements new attacks by analyzing recent research. This modular design allows AutoRedTeamer to adapt to emerging threats while maintaining strong performance on existing attack vectors. We demonstrate AutoRedTeamer's effectiveness across diverse evaluation settings, achieving 20% higher attack success rates on HarmBench against Llama-3.1-70B while reducing computational costs by 46% compared to existing approaches. AutoRedTeamer also matches the diversity of human-curated benchmarks in generating test cases, providing a comprehensive, scalable, and continuously evolving framework for evaluating the security of AI systems."
  },
  {
    "title": "Disturbance Observers for Robust Backup Control Barrier Functions",
    "url": "http://arxiv.org/abs/2503.15734v1",
    "arxiv_id": "2503.15734v1",
    "authors": [
      "David E. J. van Wijk",
      "Ersin Das",
      "Anil Alan",
      "Samuel Coogan",
      "Tamas G. Molnar",
      "Joel W. Burdick",
      "Manoranjan Majji",
      "Kerianne L. Hobbs"
    ],
    "published": "2025-03-19T22:57:09+00:00",
    "summary": "Designing safe controllers is crucial and notoriously challenging for input-constrained safety-critical control systems. Backup control barrier functions offer an approach for the construction of safe controllers online by considering the flow of the system under a backup controller. However, in the presence of model uncertainties, the flow cannot be accurately computed, making this method insufficient for safety assurance. To tackle this shortcoming, we integrate backup control barrier functions with a disturbance observer and estimate the flow under a reconstruction of the disturbance while refining this estimate over time. We prove that the controllers resulting from the proposed Disturbance Observer Backup Control Barrier Function (DO-bCBF) approach guarantee safety, are robust to unknown disturbances, and satisfy input constraints."
  },
  {
    "title": "Cybersecurity in Vehicle-to-Grid (V2G) Systems: A Systematic Review",
    "url": "http://arxiv.org/abs/2503.15730v1",
    "arxiv_id": "2503.15730v1",
    "authors": [
      "Mohammad A Razzaque",
      "Shafiuzzaman K Khadem",
      "Sandipan Patra",
      "Glory Okwata",
      "Md. Noor-A-Rahim"
    ],
    "published": "2025-03-19T22:55:40+00:00",
    "summary": "This paper presents a systematic review of recent advancements in V2G cybersecurity, employing the PRISMA (Preferred Reporting Items for Systematic Reviews and Meta-Analyses) framework for detailed searches across three journal databases and included only peer-reviewed studies published between 2020 and 2024 (June). We identified and reviewed 133 V2G cybersecurity studies and found five important insights on existing V2G cybersecurity research. First, most studies (103 of 133) focused on protecting V2G systems against cyber threats, while only seven studies addressed the recovery aspect of the CRML (Cybersecurity Risk Management Lifecycle) function. Second, existing studies have adequately addressed the security of EVs and EVCS (EV charging stations) in V2G systems (112 and 81 of 133 studies, respectively). However, none have focused on the linkage between the behaviour of EV users and the cybersecurity of V2G systems. Third, physical access, control-related vulnerabilities, and user behaviour-related attacks in V2G systems are not addressed significantly. Furthermore, existing studies overlook vulnerabilities and attacks specific to AI and blockchain technologies. Fourth, blockchain, artificial intelligence (AI), encryption, control theory, and optimisation are the main technologies used, and finally, the inclusion of quantum safety within encryption and AI models and AI assurance (AIA) is in a very early stage; only two and one of 133 studies explicitly addressed quantum safety and AIA through explainability. By providing a holistic perspective, this study identifies critical research gaps and outlines future directions for developing robust end-to-end cybersecurity solutions to safeguard V2G systems and support global sustainability goals."
  },
  {
    "title": "Safety Aware Task Planning via Large Language Models in Robotics",
    "url": "http://arxiv.org/abs/2503.15707v1",
    "arxiv_id": "2503.15707v1",
    "authors": [
      "Azal Ahmad Khan",
      "Michael Andrev",
      "Muhammad Ali Murtaza",
      "Sergio Aguilera",
      "Rui Zhang",
      "Jie Ding",
      "Seth Hutchinson",
      "Ali Anwar"
    ],
    "published": "2025-03-19T21:41:10+00:00",
    "summary": "The integration of large language models (LLMs) into robotic task planning has unlocked better reasoning capabilities for complex, long-horizon workflows. However, ensuring safety in LLM-driven plans remains a critical challenge, as these models often prioritize task completion over risk mitigation. This paper introduces SAFER (Safety-Aware Framework for Execution in Robotics), a multi-LLM framework designed to embed safety awareness into robotic task planning. SAFER employs a Safety Agent that operates alongside the primary task planner, providing safety feedback. Additionally, we introduce LLM-as-a-Judge, a novel metric leveraging LLMs as evaluators to quantify safety violations within generated task plans. Our framework integrates safety feedback at multiple stages of execution, enabling real-time risk assessment, proactive error correction, and transparent safety evaluation. We also integrate a control framework using Control Barrier Functions (CBFs) to ensure safety guarantees within SAFER's task planning. We evaluated SAFER against state-of-the-art LLM planners on complex long-horizon tasks involving heterogeneous robotic agents, demonstrating its effectiveness in reducing safety violations while maintaining task efficiency. We also verify the task planner and safety planner through actual hardware experiments involving multiple robots and a human."
  },
  {
    "title": "Does Context Matter? ContextualJudgeBench for Evaluating LLM-based Judges in Contextual Settings",
    "url": "http://arxiv.org/abs/2503.15620v1",
    "arxiv_id": "2503.15620v1",
    "authors": [
      "Austin Xu",
      "Srijan Bansal",
      "Yifei Ming",
      "Semih Yavuz",
      "Shafiq Joty"
    ],
    "published": "2025-03-19T18:09:19+00:00",
    "summary": "The large language model (LLM)-as-judge paradigm has been used to meet the demand for a cheap, reliable, and fast evaluation of model outputs during AI system development and post-deployment monitoring. While judge models -- LLMs finetuned to specialize in assessing and critiquing model outputs -- have been touted as general purpose evaluators, they are typically evaluated only on non-contextual scenarios, such as instruction following. The omission of contextual settings -- those where external information is used as context to generate an output -- is surprising given the increasing prevalence of retrieval-augmented generation (RAG) and summarization use cases. Contextual assessment is uniquely challenging, as evaluation often depends on practitioner priorities, leading to conditional evaluation criteria (e.g., comparing responses based on factuality and then considering completeness if they are equally factual). To address the gap, we propose ContextualJudgeBench, a judge benchmark with 2,000 challenging response pairs across eight splits inspired by real-world contextual evaluation scenarios. We build our benchmark with a multi-pronged data construction pipeline that leverages both existing human annotations and model-based perturbations. Our comprehensive study across 11 judge models and 9 general purpose models, reveals that the contextual information and its assessment criteria present a significant challenge to even state-of-the-art models. For example, OpenAI's o1, the best-performing model, barely reaches 55% consistent accuracy."
  },
  {
    "title": "Low Cost C-ITS Stations Using Raspberry Pi and the Open Source Software OScar",
    "url": "http://arxiv.org/abs/2503.15461v1",
    "arxiv_id": "2503.15461v1",
    "authors": [
      "L. Farina",
      "M. Piccoli",
      "S. Iandolo",
      "A. Solida",
      "C. A. Grazia",
      "F. Raviglione",
      "C. Casetti",
      "A. Bazzi"
    ],
    "published": "2025-03-19T17:40:53+00:00",
    "summary": "The deployment of cooperative-intelligent transport systems (C-ITS) has started, and standardization and research activities are moving forward to improve road safety and vehicular efficiency. An aspect that is still felt as a limitation by the research groups active in the field, is the difficulty to validate the solutions with real hardware and software, because of the huge investments that are needed when multiple equipped vehicles need to be considered. In this work, we present a platform with low-cost hardware based on a Raspberry Pi and a Wi-Fi module transmitting at 5.9 GHz, and on the open-source software Open Stack for Car (OScar), which is compliant with the ETSI C-ITS standards. With a limited cost in the order of 200 EUR, the platform realizes a device which is standard compliant and can be used as either on-board unit (OBU) or road side unit (RSU). The limited cost makes the testbed scalable to several units with limited budget and the limited size makes it also deployable on mini-cars to test advanced connected and autonomous vehicle (CAV) networks and applications. Our tests demonstrate its interoperability with other devices, compliance in terms of power spectrum, and a range of a few hundred meters in line-of-sight (LOS) conditions using the standard settings of ITS-G5."
  },
  {
    "title": "V2X-DG: Domain Generalization for Vehicle-to-Everything Cooperative Perception",
    "url": "http://arxiv.org/abs/2503.15435v1",
    "arxiv_id": "2503.15435v1",
    "authors": [
      "Baolu Li",
      "Zongzhe Xu",
      "Jinlong Li",
      "Xinyu Liu",
      "Jianwu Fang",
      "Xiaopeng Li",
      "Hongkai Yu"
    ],
    "published": "2025-03-19T17:17:44+00:00",
    "summary": "LiDAR-based Vehicle-to-Everything (V2X) cooperative perception has demonstrated its impact on the safety and effectiveness of autonomous driving. Since current cooperative perception algorithms are trained and tested on the same dataset, the generalization ability of cooperative perception systems remains underexplored. This paper is the first work to study the Domain Generalization problem of LiDAR-based V2X cooperative perception (V2X-DG) for 3D detection based on four widely-used open source datasets: OPV2V, V2XSet, V2V4Real and DAIR-V2X. Our research seeks to sustain high performance not only within the source domain but also across other unseen domains, achieved solely through training on source domain. To this end, we propose Cooperative Mixup Augmentation based Generalization (CMAG) to improve the model generalization capability by simulating the unseen cooperation, which is designed compactly for the domain gaps in cooperative perception. Furthermore, we propose a constraint for the regularization of the robust generalized feature representation learning: Cooperation Feature Consistency (CFC), which aligns the intermediately fused features of the generalized cooperation by CMAG and the early fused features of the original cooperation in source domain. Extensive experiments demonstrate that our approach achieves significant performance gains when generalizing to other unseen datasets while it also maintains strong performance on the source dataset."
  },
  {
    "title": "ChonkyBFT: Consensus Protocol of ZKsync",
    "url": "http://arxiv.org/abs/2503.15380v1",
    "arxiv_id": "2503.15380v1",
    "authors": [
      "Bruno Fran\u00e7a",
      "Denis Kolegov",
      "Igor Konnov",
      "Grzegorz Prusak"
    ],
    "published": "2025-03-19T16:17:54+00:00",
    "summary": "We present ChonkyBFT, a partially-synchronous Byzantine fault-tolerant (BFT) consensus protocol used in the ZKsync system. The proposed protocol is a hybrid protocol inspired by FAB Paxos, Fast-HotStuff, and HotStuff-2. It is a committee-based protocol with only one round of voting, single slot finality, quadratic communication, and n >= 5f + 1 fault tolerance. This design enables its effective application within the context of the ZKsync rollup, achieving its most critical goals: simplicity, low transaction latency, and reduced system complexity. The target audience for this paper is the ZKsync community and others worldwide who seek assurance in the safety and security of the ZKsync protocols. The described consensus protocol has been implemented, analyzed, and tested using formal methods."
  },
  {
    "title": "Energy-efficient Merging of Connected and Automated Vehicles using Control Barrier Functions",
    "url": "http://arxiv.org/abs/2503.15379v1",
    "arxiv_id": "2503.15379v1",
    "authors": [
      "Shreshta Rajakumar Deshpande",
      "Mrdjan Jankovic"
    ],
    "published": "2025-03-19T16:15:54+00:00",
    "summary": "Highway merges present difficulties for human drivers and automated vehicles due to incomplete situational awareness and a need for a structured (precedence, order) environment, respectively. In this paper, an unstructured merge algorithm is presented for connected and automated vehicles. There is neither precedence nor established passing order through the merge point. The algorithm relies on Control Barrier Functions for safety (collision avoidance) and for coordination that arises from exponential instability of stall-equilibria in the inter-agent space. A Monte Carlo simulation comparison to a first-in-first-out approach shows improvement in traffic flow and a significant energy efficiency benefit."
  },
  {
    "title": "Priority-driven Constraints Softening in Safe MPC for Perturbed Systems",
    "url": "http://arxiv.org/abs/2503.15373v1",
    "arxiv_id": "2503.15373v1",
    "authors": [
      "Ying Shuai Quan",
      "Mohammad Jeddi",
      "Francesco Prignoli",
      "Paolo Falcone"
    ],
    "published": "2025-03-19T16:11:25+00:00",
    "summary": "This paper presents a safe model predictive control (SMPC) framework designed to ensure the satisfaction of hard constraints for systems perturbed by an external disturbance. Such safety guarantees are ensured, despite the disturbance, by online softening a subset of adjustable constraints defined by the designer. The selection of the constraints to be softened is made online based on a predefined priority assigned to each adjustable constraint. The design of a learning-based algorithm enables real-time computation while preserving the original safety properties.   Simulations results, obtained from an automated driving application, show that the proposed approach provides guarantees of collision-avoidance hard constraints despite the unpredicted behaviors of the surrounding environment."
  },
  {
    "title": "Performance-bounded Online Ensemble Learning Method Based on Multi-armed bandits and Its Applications in Real-time Safety Assessment",
    "url": "http://arxiv.org/abs/2503.15581v1",
    "arxiv_id": "2503.15581v1",
    "authors": [
      "Songqiao Hu",
      "Zeyi Liu",
      "Xiao He"
    ],
    "published": "2025-03-19T14:57:53+00:00",
    "summary": "Ensemble learning plays a crucial role in practical applications of online learning due to its enhanced classification performance and adaptable adjustment mechanisms. However, most weight allocation strategies in ensemble learning are heuristic, making it challenging to theoretically guarantee that the ensemble classifier outperforms its base classifiers. To address this issue, a performance-bounded online ensemble learning method based on multi-armed bandits, named PB-OEL, is proposed in this paper. Specifically, multi-armed bandit with expert advice is incorporated into online ensemble learning, aiming to update the weights of base classifiers and make predictions. A theoretical framework is established to bound the performance of the ensemble classifier relative to base classifiers. By setting expert advice of bandits, the bound exceeds the performance of any base classifier when the length of data stream is sufficiently large. Additionally, performance bounds for scenarios with limited annotations are also derived. Numerous experiments on benchmark datasets and a dataset of real-time safety assessment tasks are conducted. The experimental results validate the theoretical bound to a certain extent and demonstrate that the proposed method outperforms existing state-of-the-art methods."
  },
  {
    "title": "How Well Can AI Build SD Models?",
    "url": "http://arxiv.org/abs/2503.15580v1",
    "arxiv_id": "2503.15580v1",
    "authors": [
      "William Schoenberg",
      "Davidson Girard",
      "Saras Chung",
      "Ellen O'Neill",
      "Janet Velasquez",
      "Sara Metcalf"
    ],
    "published": "2025-03-19T14:48:47+00:00",
    "summary": "Introduction: As system dynamics (SD) embraces automation, AI offers efficiency but risks bias from missing data and flawed models. Models that omit multiple perspectives and data threaten model quality, whether created by humans or with the assistance of AI. To reduce uncertainty about how well AI can build SD models, we introduce two metrics for evaluation of AI-generated causal maps: technical correctness (causal translation) and adherence to instructions (conformance).   Approach: We developed an open source project called sd-ai to provide a basis for collaboration in the SD community, aiming to fully harness the potential of AI based tools like ChatGPT for dynamic modeling. Additionally, we created an evaluation theory along with a comprehensive suite of tests designed to evaluate any such tools developed within the sd-ai ecosystem.   Results: We tested 11 different LLMs on their ability to do causal translation as well as conform to user instruction. gpt-4.5-preview was the top performer, scoring 92.9% overall, excelling in both tasks. o1 scored 100% in causal translation. gpt-4o identified all causal links but struggled with positive polarity in decreasing terms. While gpt-4.5-preview and o1 are most accurate, gpt-4o is the cheapest.   Discussion: Causal translation and conformance tests applied to the sd-ai engine reveal significant variations across lLLMs, underscoring the need for continued evaluation to ensure responsible development of AI tools for dynamic modeling. To address this, an open collaboration among tool developers, modelers, and stakeholders is launched to standardize measures for evaluating the capacity of AI tools to improve the modeling process."
  },
  {
    "title": "A Peek Behind the Curtain: Using Step-Around Prompt Engineering to Identify Bias and Misinformation in GenAI Models",
    "url": "http://arxiv.org/abs/2503.15205v1",
    "arxiv_id": "2503.15205v1",
    "authors": [
      "Don Hickerson",
      "Mike Perkins"
    ],
    "published": "2025-03-19T13:47:28+00:00",
    "summary": "This research examines the emerging technique of step-around prompt engineering in GenAI research, a method that deliberately bypasses AI safety measures to expose underlying biases and vulnerabilities in GenAI models. We discuss how Internet-sourced training data introduces unintended biases and misinformation into AI systems, which can be revealed through the careful application of step-around techniques.   Drawing parallels with red teaming in cybersecurity, we argue that step-around prompting serves a vital role in identifying and addressing potential vulnerabilities while acknowledging its dual nature as both a research tool and a potential security threat. Our findings highlight three key implications: (1) the persistence of Internet-derived biases in AI training data despite content filtering, (2) the effectiveness of step-around techniques in exposing these biases when used responsibly, and (3) the need for robust safeguards against malicious applications of these methods.   We conclude by proposing an ethical framework for using step-around prompting in AI research and development, emphasizing the importance of balancing system improvements with security considerations."
  },
  {
    "title": "Detect-and-Guide: Self-regulation of Diffusion Models for Safe Text-to-Image Generation via Guideline Token Optimization",
    "url": "http://arxiv.org/abs/2503.15197v1",
    "arxiv_id": "2503.15197v1",
    "authors": [
      "Feifei Li",
      "Mi Zhang",
      "Yiming Sun",
      "Min Yang"
    ],
    "published": "2025-03-19T13:37:52+00:00",
    "summary": "Text-to-image diffusion models have achieved state-of-the-art results in synthesis tasks; however, there is a growing concern about their potential misuse in creating harmful content. To mitigate these risks, post-hoc model intervention techniques, such as concept unlearning and safety guidance, have been developed. However, fine-tuning model weights or adapting the hidden states of the diffusion model operates in an uninterpretable way, making it unclear which part of the intermediate variables is responsible for unsafe generation. These interventions severely affect the sampling trajectory when erasing harmful concepts from complex, multi-concept prompts, thus hindering their practical use in real-world settings. In this work, we propose the safe generation framework Detect-and-Guide (DAG), leveraging the internal knowledge of diffusion models to perform self-diagnosis and fine-grained self-regulation during the sampling process. DAG first detects harmful concepts from noisy latents using refined cross-attention maps of optimized tokens, then applies safety guidance with adaptive strength and editing regions to negate unsafe generation. The optimization only requires a small annotated dataset and can provide precise detection maps with generalizability and concept specificity. Moreover, DAG does not require fine-tuning of diffusion models, and therefore introduces no loss to their generation diversity. Experiments on erasing sexual content show that DAG achieves state-of-the-art safe generation performance, balancing harmfulness mitigation and text-following performance on multi-concept real-world prompts."
  },
  {
    "title": "Exploring the Perspectives of Social VR-Aware Non-Parent Adults and Parents on Children's Use of Social Virtual Reality",
    "url": "http://arxiv.org/abs/2503.15100v1",
    "arxiv_id": "2503.15100v1",
    "authors": [
      "Cristina Fiani",
      "Pejman Saeghe",
      "Mark McGill",
      "Mohamed Khamis"
    ],
    "published": "2025-03-19T10:57:20+00:00",
    "summary": "Social Virtual Reality (VR), where people meet in virtual spaces via 3D avatars, is used by children and adults alike. Children experience new forms of harassment in social VR where it is often inaccessible to parental oversight. To date, there is limited understanding of how parents and non-parent adults within the child social VR ecosystem perceive the appropriateness of social VR for different age groups and the measures in place to safeguard children. We present results of a mixed-methods questionnaire (N=149 adults, including 79 parents) focusing on encounters with children in social VR and perspectives towards children's use of social VR. We draw novel insights on the frequency of social VR use by children under 13 and current use of, and future aspirations for, child protection interventions. Compared to non-parent adults, parents familiar with social VR propose lower minimum ages and are more likely to allow social VR without supervision. Adult users experience immaturity from children in social VR, while children face abuse, encounter age-inappropriate behaviours and self-disclose to adults. We present directions to enhance the safety of social VR through pre-planned controls, real-time oversight, post-event insight and the need for evidence-based guidelines to support parents and platforms around age-appropriate interventions."
  },
  {
    "title": "Towards Understanding the Safety Boundaries of DeepSeek Models: Evaluation and Findings",
    "url": "http://arxiv.org/abs/2503.15092v1",
    "arxiv_id": "2503.15092v1",
    "authors": [
      "Zonghao Ying",
      "Guangyi Zheng",
      "Yongxin Huang",
      "Deyue Zhang",
      "Wenxin Zhang",
      "Quanchen Zou",
      "Aishan Liu",
      "Xianglong Liu",
      "Dacheng Tao"
    ],
    "published": "2025-03-19T10:44:37+00:00",
    "summary": "This study presents the first comprehensive safety evaluation of the DeepSeek models, focusing on evaluating the safety risks associated with their generated content. Our evaluation encompasses DeepSeek's latest generation of large language models, multimodal large language models, and text-to-image models, systematically examining their performance regarding unsafe content generation. Notably, we developed a bilingual (Chinese-English) safety evaluation dataset tailored to Chinese sociocultural contexts, enabling a more thorough evaluation of the safety capabilities of Chinese-developed models. Experimental results indicate that despite their strong general capabilities, DeepSeek models exhibit significant safety vulnerabilities across multiple risk dimensions, including algorithmic discrimination and sexual content. These findings provide crucial insights for understanding and improving the safety of large foundation models. Our code is available at https://github.com/NY1024/DeepSeek-Safety-Eval."
  },
  {
    "title": "HAD-Gen: Human-like and Diverse Driving Behavior Modeling for Controllable Scenario Generation",
    "url": "http://arxiv.org/abs/2503.15049v1",
    "arxiv_id": "2503.15049v1",
    "authors": [
      "Cheng Wang",
      "Lingxin Kong",
      "Massimiliano Tamborski",
      "Stefano V. Albrecht"
    ],
    "published": "2025-03-19T09:38:45+00:00",
    "summary": "Simulation-based testing has emerged as an essential tool for verifying and validating autonomous vehicles (AVs). However, contemporary methodologies, such as deterministic and imitation learning-based driver models, struggle to capture the variability of human-like driving behavior. Given these challenges, we propose HAD-Gen, a general framework for realistic traffic scenario generation that simulates diverse human-like driving behaviors. The framework first clusters the vehicle trajectory data into different driving styles according to safety features. It then employs maximum entropy inverse reinforcement learning on each of the clusters to learn the reward function corresponding to each driving style. Using these reward functions, the method integrates offline reinforcement learning pre-training and multi-agent reinforcement learning algorithms to obtain general and robust driving policies. Multi-perspective simulation results show that our proposed scenario generation framework can simulate diverse, human-like driving behaviors with strong generalization capability. The proposed framework achieves a 90.96% goal-reaching rate, an off-road rate of 2.08%, and a collision rate of 6.91% in the generalization test, outperforming prior approaches by over 20% in goal-reaching performance. The source code is released at https://github.com/RoboSafe-Lab/Sim4AD."
  },
  {
    "title": "Multivariate Gaussian Topic Modelling: A novel approach to discover topics with greater semantic coherence",
    "url": "http://arxiv.org/abs/2503.15036v1",
    "arxiv_id": "2503.15036v1",
    "authors": [
      "Satyajeet Sahoo",
      "Jhareswar Maiti",
      "Virendra Kumar Tewari"
    ],
    "published": "2025-03-19T09:25:54+00:00",
    "summary": "An important aspect of text mining involves information retrieval in form of discovery of semantic themes (topics) from documents using topic modelling. While generative topic models like Latent Dirichlet Allocation (LDA) elegantly model topics as probability distributions and are useful in identifying latent topics from large document corpora with minimal supervision, they suffer from difficulty in topic interpretability and reduced performance in shorter texts. Here we propose a novel Multivariate Gaussian Topic modelling (MGD) approach. In this approach topics are presented as Multivariate Gaussian Distributions and documents as Gaussian Mixture Models. Using EM algorithm, the various constituent Multivariate Gaussian Distributions and their corresponding parameters are identified. Analysis of the parameters helps identify the keywords having the highest variance and mean contributions to the topic, and from these key-words topic annotations are carried out. This approach is first applied on a synthetic dataset to demonstrate the interpretability benefits vis-\\`a-vis LDA. A real-world application of this topic model is demonstrated in analysis of risks and hazards at a petrochemical plant by applying the model on safety incident reports to identify the major latent hazards plaguing the plant. This model achieves a higher mean topic coherence of 0.436 vis-\\`a-vis 0.294 for LDA."
  },
  {
    "title": "High-Order Control Barrier Functions: Insights and a Truncated Taylor-Based Formulation",
    "url": "http://arxiv.org/abs/2503.15014v1",
    "arxiv_id": "2503.15014v1",
    "authors": [
      "Jianye Xu",
      "Bassam Alrifaee"
    ],
    "published": "2025-03-19T09:11:56+00:00",
    "summary": "We examine the complexity of the standard High-Order Control Barrier Function (HOCBF) approach and propose a truncated Taylor-based approach that reduces design parameters. First, we derive the explicit inequality condition for the HOCBF approach and show that the corresponding equality condition sets a lower bound on the barrier function value that regulates its decay rate. Next, we present our Truncated Taylor CBF (TTCBF), which uses a truncated Taylor series to approximate the discrete-time CBF condition. While the standard HOCBF approach requires multiple class K functions, leading to more design parameters as the constraint's relative degree increases, our TTCBF approach requires only one. We support our theoretical findings in numerical collision-avoidance experiments and show that our approach ensures safety while reducing design complexity."
  },
  {
    "title": "Curiosity-Diffuser: Curiosity Guide Diffusion Models for Reliability",
    "url": "http://arxiv.org/abs/2503.14833v1",
    "arxiv_id": "2503.14833v1",
    "authors": [
      "Zihao Liu",
      "Xing Liu",
      "Yizhai Zhang",
      "Zhengxiong Liu",
      "Panfeng Huang"
    ],
    "published": "2025-03-19T02:25:36+00:00",
    "summary": "One of the bottlenecks in robotic intelligence is the instability of neural network models, which, unlike control models, lack a well-defined convergence domain and stability. This leads to risks when applying intelligence in the physical world. Specifically, imitation policy based on neural network may generate hallucinations, leading to inaccurate behaviors that impact the safety of real-world applications. To address this issue, this paper proposes the Curiosity-Diffuser, aimed at guiding the conditional diffusion model to generate trajectories with lower curiosity, thereby improving the reliability of policy. The core idea is to use a Random Network Distillation (RND) curiosity module to assess whether the model's behavior aligns with the training data, and then minimize curiosity by classifier guidance diffusion to reduce overgeneralization during inference. Additionally, we propose a computationally efficient metric for evaluating the reliability of the policy, measuring the similarity between the generated behaviors and the training dataset, to facilitate research about reliability learning. Finally, simulation verify the effectiveness and applicability of the proposed method to a variety of scenarios, showing that Curiosity-Diffuser significantly improves task performance and produces behaviors that are more similar to the training data. The code for this work is available at: github.com/CarlDegio/Curiosity-Diffuser"
  },
  {
    "title": "MMDT: Decoding the Trustworthiness and Safety of Multimodal Foundation Models",
    "url": "http://arxiv.org/abs/2503.14827v1",
    "arxiv_id": "2503.14827v1",
    "authors": [
      "Chejian Xu",
      "Jiawei Zhang",
      "Zhaorun Chen",
      "Chulin Xie",
      "Mintong Kang",
      "Yujin Potter",
      "Zhun Wang",
      "Zhuowen Yuan",
      "Alexander Xiong",
      "Zidi Xiong",
      "Chenhui Zhang",
      "Lingzhi Yuan",
      "Yi Zeng",
      "Peiyang Xu",
      "Chengquan Guo",
      "Andy Zhou",
      "Jeffrey Ziwei Tan",
      "Xuandong Zhao",
      "Francesco Pinto",
      "Zhen Xiang",
      "Yu Gai",
      "Zinan Lin",
      "Dan Hendrycks",
      "Bo Li",
      "Dawn Song"
    ],
    "published": "2025-03-19T01:59:44+00:00",
    "summary": "Multimodal foundation models (MMFMs) play a crucial role in various applications, including autonomous driving, healthcare, and virtual assistants. However, several studies have revealed vulnerabilities in these models, such as generating unsafe content by text-to-image models. Existing benchmarks on multimodal models either predominantly assess the helpfulness of these models, or only focus on limited perspectives such as fairness and privacy. In this paper, we present the first unified platform, MMDT (Multimodal DecodingTrust), designed to provide a comprehensive safety and trustworthiness evaluation for MMFMs. Our platform assesses models from multiple perspectives, including safety, hallucination, fairness/bias, privacy, adversarial robustness, and out-of-distribution (OOD) generalization. We have designed various evaluation scenarios and red teaming algorithms under different tasks for each perspective to generate challenging data, forming a high-quality benchmark. We evaluate a range of multimodal models using MMDT, and our findings reveal a series of vulnerabilities and areas for improvement across these perspectives. This work introduces the first comprehensive and unique safety and trustworthiness evaluation platform for MMFMs, paving the way for developing safer and more reliable MMFMs and systems. Our platform and benchmark are available at https://mmdecodingtrust.github.io/."
  },
  {
    "title": "Towards Connected Smart Work Zones: Advancing Work Zone Management through Improved Connectivity",
    "url": "http://arxiv.org/abs/2503.14801v1",
    "arxiv_id": "2503.14801v1",
    "authors": [
      "Mariam Nour",
      "Mohamed H. Zaki",
      "Mohamed Abdel-Aty"
    ],
    "published": "2025-03-19T00:24:38+00:00",
    "summary": "Work zones play a key role in road and highway maintenance but can lead to significant risks to both drivers and workers. Smart Work Zones (SWZs) have emerged as a potential solution, offering decision-makers real-time insights into the status of the work zone. By utilizing work zone barrels equipped with sensors and communication nodes, SWZs facilitate collecting and transmitting critical data, including location, traffic density, flow patterns, and worker proximity alerts. In collaboration with the Florida Department of Transportation (FDOT), this study addresses work zone barrel connectivity requirements while considering a cost-effective, low-power, and low-maintenance solution. While the broader project aimed to create a complete SWZ system for the localization of work zone barrels, this paper proposes a novel relay node selection algorithm integrated with Bluetooth Low Energy (BLE) technology to enhance network performance. The proposed algorithm enhances the communication network performance by selecting specific nodes as relay points, avoiding message flooding in the network. It demonstrates an improvement in message delivery rates, achieving up to a 40% increase over existing methods while ensuring balanced load distribution among nodes. Moreover, it maintains an 80% message delivery rate while minimizing power consumption, outperforming other approaches. This improvement in communication efficiency is critical, as it ensures the accurate transmission and delivery of vital work zone data, allowing for faster and more informed decisions to enhance work zone safety and management."
  },
  {
    "title": "RAT: Boosting Misclassification Detection Ability without Extra Data",
    "url": "http://arxiv.org/abs/2503.14783v1",
    "arxiv_id": "2503.14783v1",
    "authors": [
      "Ge Yan",
      "Tsui-Wei Weng"
    ],
    "published": "2025-03-18T23:18:55+00:00",
    "summary": "As deep neural networks(DNN) become increasingly prevalent, particularly in high-stakes areas such as autonomous driving and healthcare, the ability to detect incorrect predictions of models and intervene accordingly becomes crucial for safety. In this work, we investigate the detection of misclassified inputs for image classification models from the lens of adversarial perturbation: we propose to use robust radius (a.k.a. input-space margin) as a confidence metric and design two efficient estimation algorithms, RR-BS and RR-Fast, for misclassification detection. Furthermore, we design a training method called Radius Aware Training (RAT) to boost models' ability to identify mistakes. Extensive experiments show our method could achieve up to 29.3% reduction on AURC and 21.62% reduction in FPR@95TPR, compared with previous methods."
  },
  {
    "title": "Temporal Context Awareness: A Defense Framework Against Multi-turn Manipulation Attacks on Large Language Models",
    "url": "http://arxiv.org/abs/2503.15560v1",
    "arxiv_id": "2503.15560v1",
    "authors": [
      "Prashant Kulkarni",
      "Assaf Namer"
    ],
    "published": "2025-03-18T22:30:17+00:00",
    "summary": "Large Language Models (LLMs) are increasingly vulnerable to sophisticated multi-turn manipulation attacks, where adversaries strategically build context through seemingly benign conversational turns to circumvent safety measures and elicit harmful or unauthorized responses. These attacks exploit the temporal nature of dialogue to evade single-turn detection methods, representing a critical security vulnerability with significant implications for real-world deployments.   This paper introduces the Temporal Context Awareness (TCA) framework, a novel defense mechanism designed to address this challenge by continuously analyzing semantic drift, cross-turn intention consistency and evolving conversational patterns. The TCA framework integrates dynamic context embedding analysis, cross-turn consistency verification, and progressive risk scoring to detect and mitigate manipulation attempts effectively. Preliminary evaluations on simulated adversarial scenarios demonstrate the framework's potential to identify subtle manipulation patterns often missed by traditional detection techniques, offering a much-needed layer of security for conversational AI systems. In addition to outlining the design of TCA , we analyze diverse attack vectors and their progression across multi-turn conversation, providing valuable insights into adversarial tactics and their impact on LLM vulnerabilities. Our findings underscore the pressing need for robust, context-aware defenses in conversational AI systems and highlight TCA framework as a promising direction for securing LLMs while preserving their utility in legitimate applications. We make our implementation available to support further research in this emerging area of AI security."
  },
  {
    "title": "ViVa-SAFELAND: a New Freeware for Safe Validation of Vision-based Navigation in Aerial Vehicles",
    "url": "http://arxiv.org/abs/2503.14719v1",
    "arxiv_id": "2503.14719v1",
    "authors": [
      "Miguel S. Soriano-Garc\u00eda",
      "Diego A. Mercado-Ravell"
    ],
    "published": "2025-03-18T20:48:50+00:00",
    "summary": "ViVa-SAFELAND is an open source software library, aimed to test and evaluate vision-based navigation strategies for aerial vehicles, with special interest in autonomous landing, while complying with legal regulations and people's safety. It consists of a collection of high definition aerial videos, focusing on real unstructured urban scenarios, recording moving obstacles of interest, such as cars and people. Then, an Emulated Aerial Vehicle (EAV) with a virtual moving camera is implemented in order to ``navigate\" inside the video, according to high-order commands. ViVa-SAFELAND provides a new, safe, simple and fair comparison baseline to evaluate and compare different visual navigation solutions under the same conditions, and to randomize variables along several trials. It also facilitates the development of autonomous landing and navigation strategies, as well as the generation of image datasets for different training tasks. Moreover, it is useful for training either human of autonomous pilots using deep learning. The effectiveness of the framework for validating vision algorithms is demonstrated through two case studies, detection of moving objects and risk assessment segmentation. To our knowledge, this is the first safe validation framework of its kind, to test and compare visual navigation solution for aerial vehicles, which is a crucial aspect for urban deployment in complex real scenarios."
  },
  {
    "title": "Construction Site Scaffolding Completeness Detection Based on Mask R-CNN and Hough Transform",
    "url": "http://arxiv.org/abs/2503.14716v1",
    "arxiv_id": "2503.14716v1",
    "authors": [
      "Pei-Hsin Lin",
      "Jacob J. Lin",
      "Shang-Hsien Hsieh"
    ],
    "published": "2025-03-18T20:27:22+00:00",
    "summary": "Construction site scaffolding is essential for many building projects, and ensuring its safety is crucial to prevent accidents. The safety inspector must check the scaffolding's completeness and integrity, where most violations occur. The inspection process includes ensuring all the components are in the right place since workers often compromise safety for convenience and disassemble parts such as cross braces. This paper proposes a deep learning-based approach to detect the scaffolding and its cross braces using computer vision. A scaffold image dataset with annotated labels is used to train a convolutional neural network (CNN) model. With the proposed approach, we can automatically detect the completeness of cross braces from images taken at construction sites, without the need for manual inspection, saving a significant amount of time and labor costs. This non-invasive and efficient solution for detecting scaffolding completeness can help improve safety in construction sites."
  },
  {
    "title": "AI-driven Uncertainty Quantification & Multi-Physics Approach to Evaluate Cladding Materials in a Microreactor",
    "url": "http://arxiv.org/abs/2503.14679v1",
    "arxiv_id": "2503.14679v1",
    "authors": [
      "Alex Foutch",
      "Kazuma Kobayashi",
      "Ayodeji Alajo",
      "Dinesh Kumar",
      "Syed Bahauddin Alam"
    ],
    "published": "2025-03-18T19:36:36+00:00",
    "summary": "The pursuit of enhanced nuclear safety has spurred the development of accident-tolerant cladding (ATC) materials for light water reactors (LWRs). This study investigates the potential of repurposing these ATCs in advanced reactor designs, aiming to expedite material development and reduce costs. The research employs a multi-physics approach, encompassing neutronics, heat transfer, thermodynamics, and structural mechanics, to evaluate four candidate materials (Haynes 230, Zircaloy-4, FeCrAl, and SiC-SiC) within the context of a high-temperature, sodium-cooled microreactor, exemplified by the Kilopower design. While neutronic simulations revealed negligible power profile variations among the materials, finite element analyses highlighted the superior thermal stability of SiC-SiC and the favorable stress resistance of Haynes 230. The high-temperature environment significantly impacted material performance, particularly for Zircaloy-4 and FeCrAl, while SiC-SiC's inherent properties limited its ability to withstand stress loads. Additionally, AI-driven uncertainty quantification and sensitivity analysis were conducted to assess the influence of material property variations on maximum hoop stress. The findings underscore the need for further research into high-temperature material properties to facilitate broader applicability of existing materials to advanced reactors. Haynes 230 is identified as the most promising candidate based on the evaluated criteria."
  },
  {
    "title": "These Magic Moments: Differentiable Uncertainty Quantification of Radiance Field Models",
    "url": "http://arxiv.org/abs/2503.14665v1",
    "arxiv_id": "2503.14665v1",
    "authors": [
      "Parker Ewen",
      "Hao Chen",
      "Seth Isaacson",
      "Joey Wilson",
      "Katherine A. Skinner",
      "Ram Vasudevan"
    ],
    "published": "2025-03-18T19:12:02+00:00",
    "summary": "This paper introduces a novel approach to uncertainty quantification for radiance fields by leveraging higher-order moments of the rendering equation. Uncertainty quantification is crucial for downstream tasks including view planning and scene understanding, where safety and robustness are paramount. However, the high dimensionality and complexity of radiance fields pose significant challenges for uncertainty quantification, limiting the use of these uncertainty quantification methods in high-speed decision-making. We demonstrate that the probabilistic nature of the rendering process enables efficient and differentiable computation of higher-order moments for radiance field outputs, including color, depth, and semantic predictions. Our method outperforms existing radiance field uncertainty estimation techniques while offering a more direct, computationally efficient, and differentiable formulation without the need for post-processing.Beyond uncertainty quantification, we also illustrate the utility of our approach in downstream applications such as next-best-view (NBV) selection and active ray sampling for neural radiance field training. Extensive experiments on synthetic and real-world scenes confirm the efficacy of our approach, which achieves state-of-the-art performance while maintaining simplicity."
  },
  {
    "title": "Safety-Critical and Distributed Nonlinear Predictive Controllers for Teams of Quadrupedal Robots",
    "url": "http://arxiv.org/abs/2503.14656v1",
    "arxiv_id": "2503.14656v1",
    "authors": [
      "Basit Muhammad Imran",
      "Jeeseop Kim",
      "Taizoon Chunawala",
      "Alexander Leonessa",
      "Kaveh Akbari Hamed"
    ],
    "published": "2025-03-18T19:05:57+00:00",
    "summary": "This paper presents a novel hierarchical, safety-critical control framework that integrates distributed nonlinear model predictive controllers (DNMPCs) with control barrier functions (CBFs) to enable cooperative locomotion of multi-agent quadrupedal robots in complex environments. While NMPC-based methods are widely adopted for enforcing safety constraints and navigating multi-robot systems (MRSs) through intricate environments, ensuring the safety of MRSs requires a formal definition grounded in the concept of invariant sets. CBFs, typically implemented via quadratic programs (QPs) at the planning layer, provide formal safety guarantees. However, their zero-control horizon limits their effectiveness for extended trajectory planning in inherently unstable, underactuated, and nonlinear legged robot models. Furthermore, the integration of CBFs into real-time NMPC for sophisticated MRSs, such as quadrupedal robot teams, remains underexplored. This paper develops computationally efficient, distributed NMPC algorithms that incorporate CBF-based collision safety guarantees within a consensus protocol, enabling longer planning horizons for safe cooperative locomotion under disturbances and rough terrain conditions. The optimal trajectories generated by the DNMPCs are tracked using full-order, nonlinear whole-body controllers at the low level. The proposed approach is validated through extensive numerical simulations with up to four Unitree A1 robots and hardware experiments involving two A1 robots subjected to external pushes, rough terrain, and uncertain obstacle information. Comparative analysis demonstrates that the proposed CBF-based DNMPCs achieve a 27.89% higher success rate than conventional NMPCs without CBF constraints."
  },
  {
    "title": "Reducing False Ventricular Tachycardia Alarms in ICU Settings: A Machine Learning Approach",
    "url": "http://arxiv.org/abs/2503.14621v1",
    "arxiv_id": "2503.14621v1",
    "authors": [
      "Grace Funmilayo Farayola",
      "Akinyemi Sadeeq Akintola",
      "Oluwole Fagbohun",
      "Chukwuka Michael Oforgu",
      "Bisola Faith Kayode",
      "Christian Chimezie",
      "Temitope Kadri",
      "Abiola Oludotun",
      "Nelson Ogbeide",
      "Mgbame Michael",
      "Adeseye Ifaturoti",
      "Toyese Oloyede"
    ],
    "published": "2025-03-18T18:18:38+00:00",
    "summary": "False arrhythmia alarms in intensive care units (ICUs) are a significant challenge, contributing to alarm fatigue and potentially compromising patient safety. Ventricular tachycardia (VT) alarms are particularly difficult to detect accurately due to their complex nature. This paper presents a machine learning approach to reduce false VT alarms using the VTaC dataset, a benchmark dataset of annotated VT alarms from ICU monitors. We extract time-domain and frequency-domain features from waveform data, preprocess the data, and train deep learning models to classify true and false VT alarms. Our results demonstrate high performance, with ROC-AUC scores exceeding 0.96 across various training configurations. This work highlights the potential of machine learning to improve the accuracy of VT alarm detection in clinical settings."
  },
  {
    "title": "Aligning Multimodal LLM with Human Preference: A Survey",
    "url": "http://arxiv.org/abs/2503.14504v1",
    "arxiv_id": "2503.14504v1",
    "authors": [
      "Tao Yu",
      "Yi-Fan Zhang",
      "Chaoyou Fu",
      "Junkang Wu",
      "Jinda Lu",
      "Kun Wang",
      "Xingyu Lu",
      "Yunhang Shen",
      "Guibin Zhang",
      "Dingjie Song",
      "Yibo Yan",
      "Tianlong Xu",
      "Qingsong Wen",
      "Zhang Zhang",
      "Yan Huang",
      "Liang Wang",
      "Tieniu Tan"
    ],
    "published": "2025-03-18T17:59:56+00:00",
    "summary": "Large language models (LLMs) can handle a wide variety of general tasks with simple prompts, without the need for task-specific training. Multimodal Large Language Models (MLLMs), built upon LLMs, have demonstrated impressive potential in tackling complex tasks involving visual, auditory, and textual data. However, critical issues related to truthfulness, safety, o1-like reasoning, and alignment with human preference remain insufficiently addressed. This gap has spurred the emergence of various alignment algorithms, each targeting different application scenarios and optimization goals. Recent studies have shown that alignment algorithms are a powerful approach to resolving the aforementioned challenges. In this paper, we aim to provide a comprehensive and systematic review of alignment algorithms for MLLMs. Specifically, we explore four key aspects: (1) the application scenarios covered by alignment algorithms, including general image understanding, multi-image, video, and audio, and extended multimodal applications; (2) the core factors in constructing alignment datasets, including data sources, model responses, and preference annotations; (3) the benchmarks used to evaluate alignment algorithms; and (4) a discussion of potential future directions for the development of alignment algorithms. This work seeks to help researchers organize current advancements in the field and inspire better alignment methods. The project page of this paper is available at https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/tree/Alignment."
  },
  {
    "title": "Temporal Consistency for LLM Reasoning Process Error Identification",
    "url": "http://arxiv.org/abs/2503.14495v1",
    "arxiv_id": "2503.14495v1",
    "authors": [
      "Jiacheng Guo",
      "Yue Wu",
      "Jiahao Qiu",
      "Kaixuan Huang",
      "Xinzhe Juan",
      "Ling Yang",
      "Mengdi Wang"
    ],
    "published": "2025-03-18T17:58:28+00:00",
    "summary": "Verification is crucial for effective mathematical reasoning. We present a new temporal consistency method where verifiers iteratively refine their judgments based on the previous assessment. Unlike one-round verification or multi-model debate approaches, our method leverages consistency in a sequence of self-reflection actions to improve verification accuracy. Empirical evaluations across diverse mathematical process error identification benchmarks (Mathcheck, ProcessBench, and PRM800K) show consistent performance improvements over baseline methods. When applied to the recent DeepSeek R1 distilled models, our method demonstrates strong performance, enabling 7B/8B distilled models to outperform all 70B/72B models and GPT-4o on ProcessBench. Notably, the distilled 14B model with our method achieves performance comparable to Deepseek-R1. Our codes are available at https://github.com/jcguo123/Temporal-Consistency"
  },
  {
    "title": "DAPO: An Open-Source LLM Reinforcement Learning System at Scale",
    "url": "http://arxiv.org/abs/2503.14476v1",
    "arxiv_id": "2503.14476v1",
    "authors": [
      "Qiying Yu",
      "Zheng Zhang",
      "Ruofei Zhu",
      "Yufeng Yuan",
      "Xiaochen Zuo",
      "Yu Yue",
      "Tiantian Fan",
      "Gaohong Liu",
      "Lingjun Liu",
      "Xin Liu",
      "Haibin Lin",
      "Zhiqi Lin",
      "Bole Ma",
      "Guangming Sheng",
      "Yuxuan Tong",
      "Chi Zhang",
      "Mofan Zhang",
      "Wang Zhang",
      "Hang Zhu",
      "Jinhua Zhu",
      "Jiaze Chen",
      "Jiangjie Chen",
      "Chengyi Wang",
      "Hongli Yu",
      "Weinan Dai",
      "Yuxuan Song",
      "Xiangpeng Wei",
      "Hao Zhou",
      "Jingjing Liu",
      "Wei-Ying Ma",
      "Ya-Qin Zhang",
      "Lin Yan",
      "Mu Qiao",
      "Yonghui Wu",
      "Mingxuan Wang"
    ],
    "published": "2025-03-18T17:49:06+00:00",
    "summary": "Inference scaling empowers LLMs with unprecedented reasoning ability, with reinforcement learning as the core technique to elicit complex reasoning. However, key technical details of state-of-the-art reasoning LLMs are concealed (such as in OpenAI o1 blog and DeepSeek R1 technical report), thus the community still struggles to reproduce their RL training results. We propose the $\\textbf{D}$ecoupled Clip and $\\textbf{D}$ynamic s$\\textbf{A}$mpling $\\textbf{P}$olicy $\\textbf{O}$ptimization ($\\textbf{DAPO}$) algorithm, and fully open-source a state-of-the-art large-scale RL system that achieves 50 points on AIME 2024 using Qwen2.5-32B base model. Unlike previous works that withhold training details, we introduce four key techniques of our algorithm that make large-scale LLM RL a success. In addition, we open-source our training code, which is built on the verl framework, along with a carefully curated and processed dataset. These components of our open-source system enhance reproducibility and support future research in large-scale LLM RL."
  },
  {
    "title": "PENCIL: Long Thoughts with Short Memory",
    "url": "http://arxiv.org/abs/2503.14337v1",
    "arxiv_id": "2503.14337v1",
    "authors": [
      "Chenxiao Yang",
      "Nathan Srebro",
      "David McAllester",
      "Zhiyuan Li"
    ],
    "published": "2025-03-18T15:14:14+00:00",
    "summary": "While recent works (e.g. o1, DeepSeek R1) have demonstrated great promise of using long Chain-of-Thought (CoT) to improve reasoning capabilities of language models, scaling it up during test-time is challenging due to inefficient memory usage -- intermediate computations accumulate indefinitely in context even no longer needed for future thoughts. We propose PENCIL, which incorporates a reduction mechanism into the autoregressive generation process, allowing the model to recursively clean up intermediate thoughts based on patterns learned from training. With this reduction mechanism, PENCIL significantly reduces the maximal context length required during generation, and thus can generate longer thoughts with limited memory, solving larger-scale problems given more thinking time. For example, we demonstrate PENCIL achieves 97\\% accuracy on the challenging Einstein's puzzle -- a task even large models like GPT-4 struggle with -- using only a small 25M-parameter transformer with 2048 context length. Theoretically, we prove PENCIL can perform universal space-efficient computation by simulating Turing machines with optimal time and space complexity, and thus can solve arbitrary computational tasks that would otherwise be intractable given context window constraints."
  },
  {
    "title": "ADAPT: An Autonomous Forklift for Construction Site Operation",
    "url": "http://arxiv.org/abs/2503.14331v1",
    "arxiv_id": "2503.14331v1",
    "authors": [
      "Johannes Huemer",
      "Markus Murschitz",
      "Matthias Sch\u00f6rghuber",
      "Lukas Reisinger",
      "Thomas Kadiofsky",
      "Christoph Weidinger",
      "Mario Niedermeyer",
      "Benedikt Widy",
      "Marcel Zeilinger",
      "Csaba Beleznai",
      "Tobias Gl\u00fcck",
      "Andreas Kugi",
      "Patrik Zips"
    ],
    "published": "2025-03-18T15:03:28+00:00",
    "summary": "Efficient material logistics play a critical role in controlling costs and schedules in the construction industry. However, manual material handling remains prone to inefficiencies, delays, and safety risks. Autonomous forklifts offer a promising solution to streamline on-site logistics, reducing reliance on human operators and mitigating labor shortages. This paper presents the development and evaluation of the Autonomous Dynamic All-terrain Pallet Transporter (ADAPT), a fully autonomous off-road forklift designed for construction environments. Unlike structured warehouse settings, construction sites pose significant challenges, including dynamic obstacles, unstructured terrain, and varying weather conditions. To address these challenges, our system integrates AI-driven perception techniques with traditional approaches for decision making, planning, and control, enabling reliable operation in complex environments. We validate the system through extensive real-world testing, comparing its long-term performance against an experienced human operator across various weather conditions. We also provide a comprehensive analysis of challenges and key lessons learned, contributing to the advancement of autonomous heavy machinery. Our findings demonstrate that autonomous outdoor forklifts can operate near human-level performance, offering a viable path toward safer and more efficient construction logistics."
  },
  {
    "title": "Multi-Parameter Analysis of Li-ion Battery Degradation: Integrating Optical Fiber Sensing with Differential State of Health Metrics",
    "url": "http://arxiv.org/abs/2503.14327v1",
    "arxiv_id": "2503.14327v1",
    "authors": [
      "Idris Temitope Bello",
      "Hassan Raza",
      "Madithedu Muneeswara",
      "Neha Tewari",
      "Yin Nee Cheung",
      "Tobi Alabi Michael",
      "Ridwan Taiwo",
      "Fiske Lin"
    ],
    "published": "2025-03-18T15:01:30+00:00",
    "summary": "The reliability and safety of Lithium-ion batteries (LiBs) are of great concern in the energy storage industry. Nevertheless, the real-time monitoring of their degradation remains challenging due to limited quantitative metrics available during cycling. This study addresses this limitation by employing a novel approach that combines external optical fiber sensing with advanced data analysis techniques to comprehensively assess battery health. We engineered a non-invasive optical sensing platform using tandem pairs of polymeric and silica-based fiber Bragg grating (FBG) sensors affixed to the external surface of commercial Li-ion button cells, enabling simultaneous, real-time monitoring of device-level volume changes and thermal events over 600 cycles. Our analysis incorporated differential techniques to estimate the battery's state of health (SOH) based on capacity, strain, and temperature variations with respect to voltage. Additionally, we implemented and compared three deep learning models - Long Short-Term Memory (LSTM), Gated Recurrent Unit (GRU), and Artificial Neural Network (ANN) - to predict battery SOH over cycles. We were able to capture both continuous and spontaneous degradation events and provide unique insights into battery behavior across its lifecycle through differential analysis and new SOH metrics demonstrating high correlation with conventional measures. This multi-parameter approach, combining advanced sensing techniques with innovative data analysis and deep learning methods, contributes significantly to battery diagnostics, potentially improving reliability assessment, enhancing safety standards, and accelerating the development of more sustainable energy storage solutions."
  },
  {
    "title": "Conversational Agents as Catalysts for Critical Thinking: Challenging Social Influence in Group Decision-making",
    "url": "http://arxiv.org/abs/2503.14263v1",
    "arxiv_id": "2503.14263v1",
    "authors": [
      "Soohwan Lee",
      "Seoyeong Hwang",
      "Dajung Kim",
      "Kyungho Lee"
    ],
    "published": "2025-03-18T13:54:12+00:00",
    "summary": "Group decision-making processes frequently suffer when social influence and power dynamics suppress minority viewpoints, leading to compliance and groupthink. Conversational agents can counteract these harmful dynamics by encouraging critical thinking. This study investigates how LLM-powered devil's advocate systems affect psychological safety, opinion expression, and satisfaction in power-imbalanced group dynamics. We conducted an experiment with 48 participants in 12 four-person groups, each containing three high-power (senior) and one low-power (junior) member. Each group completed decision tasks in both baseline and AI intervention conditions. Results show AI counterarguments fostered a more flexible atmosphere and significantly enhanced both process and outcome satisfaction for all participants, with particularly notable improvements for minority members. Cognitive workload increased slightly, though not significantly. This research contributes empirical evidence on how AI systems can effectively navigate power hierarchies to foster more inclusive decision-making environments, highlighting the importance of balancing intervention frequency, maintaining conversational flow, and preserving group cohesion."
  },
  {
    "title": "A Chain-Driven, Sandwich-Legged Quadruped Robot: Design and Experimental Analysis",
    "url": "http://arxiv.org/abs/2503.14255v1",
    "arxiv_id": "2503.14255v1",
    "authors": [
      "Aman Singh",
      "Bhavya Giri Goswami",
      "Ketan Nehete",
      "Shishir N. Y. Kolathaya"
    ],
    "published": "2025-03-18T13:44:34+00:00",
    "summary": "This paper introduces a chain-driven, sandwich-legged, mid-size quadruped robot designed as an accessible research platform. The design prioritizes enhanced locomotion capabilities, improved reliability and safety of the actuation system, and simplified, cost-effective manufacturing processes. Locomotion performance is optimized through a sandwiched leg design and a dual-motor configuration, reducing leg inertia for agile movements. Reliability and safety are achieved by integrating robust cable strain reliefs, efficient heat sinks for motor thermal management, and mechanical limits to restrict leg motion. Simplified design considerations include a quasi-direct drive (QDD) actuator and the adoption of low-cost fabrication techniques, such as laser cutting and 3D printing, to minimize cost and ensure rapid prototyping. The robot weighs approximately 25 kg and is developed at a cost under \\$8000, making it a scalable and affordable solution for robotics research. Experimental validations demonstrate the platform's capability to execute trot and crawl gaits on flat terrain and slopes, highlighting its potential as a versatile and reliable quadruped research platform."
  },
  {
    "title": "Inferring Event Descriptions from Time Series with Language Models",
    "url": "http://arxiv.org/abs/2503.14190v1",
    "arxiv_id": "2503.14190v1",
    "authors": [
      "Mingtian Tan",
      "Mike A. Merrill",
      "Zack Gottesman",
      "Tim Althoff",
      "David Evans",
      "Tom Hartvigsen"
    ],
    "published": "2025-03-18T12:07:33+00:00",
    "summary": "Time series data measure how environments change over time and drive decision-making in critical domains like finance and healthcare. When analyzing time series, we often seek to understand the underlying events occurring in the measured environment. For example, one might ask: What caused a sharp drop in the stock price? Events are often described with natural language, so we conduct the first study of whether Large Language Models (LLMs) can infer natural language events from time series. We curate a new benchmark featuring win probabilities collected from 4,200 basketball and American football games, featuring 1.7M timesteps with real value data and corresponding natural language events. Building on the recent wave of using LLMs on time series, we evaluate 16 LLMs and find that they demonstrate promising abilities to infer events from time series data. The open-weights DeepSeek-R1 32B model outperforms proprietary models like GPT-4o. Despite this impressive initial performance, we also find clear avenues to improve recent models, as we identify failures when altering the provided context, event sequence lengths, and evaluation strategy. (All resources needed to reproduce our work are available: https://github.com/BennyTMT/GAMETime)"
  },
  {
    "title": "Towards Harmless Multimodal Assistants with Blind Preference Optimization",
    "url": "http://arxiv.org/abs/2503.14189v1",
    "arxiv_id": "2503.14189v1",
    "authors": [
      "Yongqi Li",
      "Lu Yang",
      "Jian Wang",
      "Runyang You",
      "Wenjie Li",
      "Liqiang Nie"
    ],
    "published": "2025-03-18T12:02:38+00:00",
    "summary": "Multimodal Large Language Models (MLLMs) have demonstrated impressive capabilities in multimodal understanding, reasoning, and interaction. Given the extensive applications of MLLMs, the associated safety issues have become increasingly critical. Due to the effectiveness of preference optimization in aligning MLLMs with human preferences, there is an urgent need for safety-related preference data for MLLMs. To address this, we construct the MMSafe-PO preference dataset towards harmless multimodal assistants, featuring multimodal instructions, the conversational format, and ranked paired responses from human feedback. We also identify two insightful observations: modality co-defense and modality cheating, which illustrate that MLLMs possess a certain level of inherent defense while still presenting unique safety challenges. Based on these observations, we propose the Blind Preference Optimization (BPO) approach. Comprehensive experiments on three benchmarks show that BPO effectively enhances the safety capabilities of MLLMs. Notably, BPO significantly improves the safety rate of the base MLLM by 45.0%, outperforming the DPO approach. Additionally, applying BPO to the MMSafe-PO dataset greatly reduces the base MLLM's unsafe rate on other safety benchmarks (14.5% on MM-SafetyBench and 82.9% on HarmEval, demonstrating the effectiveness and robustness of both the dataset and the approach. We release code and data at https://lu-yang666.github.io/MMsafe-PO-Web/."
  },
  {
    "title": "Gravitational-wave Extraction using Independent Component Analysis",
    "url": "http://arxiv.org/abs/2503.14179v1",
    "arxiv_id": "2503.14179v1",
    "authors": [
      "Rika Shimomura",
      "Yuuichi Tabe",
      "Hisaaki Shinkai"
    ],
    "published": "2025-03-18T11:54:50+00:00",
    "summary": "Independent component analysis (ICA) is a method to extract a set of time-series data using \"statistical independency\" of each component. We propose applying ICA for extracting gravitational wave (GW) signals. Our idea is to extract a signal that is commonly included in multiple detectors and to find it by shifting the data-set around its arrival time. In this article, we show several tests using injected signals, and show that this method can be applied to events for signal-to-noise over 15. We then demonstrate the method to actual O1-O3 events, and the identification of the arrival time can be estimated more precisely than that was previously reported. This approach does not require templates of waveform, therefore it can be applied for testing general relativity, and also for finding unknown GW."
  },
  {
    "title": "DangerMaps: Personalized Safety Advice for Travel in Urban Environments using a Retrieval-Augmented Language Model",
    "url": "http://arxiv.org/abs/2503.14103v1",
    "arxiv_id": "2503.14103v1",
    "authors": [
      "Jonas Oppenlaender"
    ],
    "published": "2025-03-18T10:18:07+00:00",
    "summary": "Planning a trip into a potentially unsafe area is a difficult task. We conducted a formative study on travelers' information needs, finding that most of them turn to search engines for trip planning. Search engines, however, fail to provide easily interpretable results adapted to the context and personal information needs of a traveler. Large language models (LLMs) create new possibilities for providing personalized travel safety advice. To explore this idea, we developed DangerMaps, a mapping system that assists its users in researching the safety of an urban travel destination, whether it is pre-travel or on-location. DangerMaps plots safety ratings onto a map and provides explanations on demand. This late breaking work specifically emphasizes the challenges of designing real-world applications with large language models. We provide a detailed description of our approach to prompt design and highlight future areas of research."
  },
  {
    "title": "Robust Safety Critical Control Under Multiple State and Input Constraints: Volume Control Barrier Function Method",
    "url": "http://arxiv.org/abs/2503.13996v1",
    "arxiv_id": "2503.13996v1",
    "authors": [
      "Jinyang Dong",
      "Shizhen Wu",
      "Rui Liu",
      "Xiao Liang",
      "Biao Lu",
      "Yongchun Fang"
    ],
    "published": "2025-03-18T07:58:58+00:00",
    "summary": "In this paper, the safety-critical control problem for uncertain systems under multiple control barrier function (CBF) constraints and input constraints is investigated. A novel framework is proposed to generate a safety filter that minimizes changes to reference inputs when safety risks arise, ensuring a balance between safety and performance. A nonlinear disturbance observer (DOB) based on the robust integral of the sign of the error (RISE) is used to estimate system uncertainties, ensuring that the estimation error converges to zero exponentially. This error bound is integrated into the safety-critical controller to reduce conservativeness while ensuring safety. To further address the challenges arising from multiple CBF and input constraints, a novel Volume CBF (VCBF) is proposed by analyzing the feasible space of the quadratic programming (QP) problem. % ensuring solution feasibility by keeping the volume as a positive value. To ensure that the feasible space does not vanish under disturbances, a DOB-VCBF-based method is introduced, ensuring system safety while maintaining the feasibility of the resulting QP. Subsequently, several groups of simulation and experimental results are provided to validate the effectiveness of the proposed controller."
  },
  {
    "title": "Workflow for Safe-AI",
    "url": "http://arxiv.org/abs/2503.14563v1",
    "arxiv_id": "2503.14563v1",
    "authors": [
      "Suzana Veljanovska",
      "Hans Dermot Doran"
    ],
    "published": "2025-03-18T07:45:18+00:00",
    "summary": "The development and deployment of safe and dependable AI models is crucial in applications where functional safety is a key concern. Given the rapid advancement in AI research and the relative novelty of the safe-AI domain, there is an increasing need for a workflow that balances stability with adaptability. This work proposes a transparent, complete, yet flexible and lightweight workflow that highlights both reliability and qualifiability. The core idea is that the workflow must be qualifiable, which demands the use of qualified tools. Tool qualification is a resource-intensive process, both in terms of time and cost. We therefore place value on a lightweight workflow featuring a minimal number of tools with limited features. The workflow is built upon an extended ONNX model description allowing for validation of AI algorithms from their generation to runtime deployment. This validation is essential to ensure that models are validated before being reliably deployed across different runtimes, particularly in mixed-criticality systems. Keywords-AI workflows, safe-AI, dependable-AI, functional safety, v-model development"
  },
  {
    "title": "Empowering Smaller Models: Tuning LLaMA and Gemma with Chain-of-Thought for Ukrainian Exam Tasks",
    "url": "http://arxiv.org/abs/2503.13988v1",
    "arxiv_id": "2503.13988v1",
    "authors": [
      "Mykyta Syromiatnikov",
      "Victoria Ruvinskaya",
      "Nataliia Komleva"
    ],
    "published": "2025-03-18T07:44:49+00:00",
    "summary": "Leading large language models have demonstrated impressive capabilities in reasoning-intensive tasks, such as standardized educational testing. However, they often require extensive training in low-resource settings with inaccessible infrastructure. Small or compact models, though more efficient, frequently lack sufficient support for underrepresented languages, leaving a performance gap in critical domains. This work explores the potential of parameter-efficient fine-tuning of compact open-weight language models to handle reasoning-intensive tasks in the underrepresented Ukrainian language, building on the findings of the ZNO-Eval benchmark. Parameter-efficient fine-tuning of LLaMA 3.1 (8 billion parameters), LLaMA 3.2 (3 billion parameters), and Gemma 2 (9 billion parameters) models on chain-of-thought solutions resulted in a modest test score improvement of up to 17.4% on complex matching tasks and 1.6% overall compared to tuning on answer letters alone, offering enhanced interpretability and robustness. In addition, the proposed tuning method with joint task topic and step-by-step solution generation outperforms standard chain-of-thought tuning in matching tasks and provides a 5.4% gain over the best LLaMA 3.2 model due to guiding the model to recall and apply domain-relevant information. Contrasting obtained results with zero-shot evaluations of leading open-weight and proprietary models such as Qwen, DeepSeek R1, OpenAI o1 and o3, Gemini, and Claude, highlight that fine-tuning LLaMA and Gemma models with 2,032 step-by-step solutions and 20 to 50 million trainable parameters on a single A100 GPU lets them outperform GPT-4o mini, Mistral Large, and larger open-weight models. This research also evaluates how merging the quantized adapter with the base model influences the generation quality. Source code and tuned models are available at https://github.com/NLPForUA/ZNO."
  },
  {
    "title": "Survey of Adversarial Robustness in Multimodal Large Language Models",
    "url": "http://arxiv.org/abs/2503.13962v1",
    "arxiv_id": "2503.13962v1",
    "authors": [
      "Chengze Jiang",
      "Zhuangzhuang Wang",
      "Minjing Dong",
      "Jie Gui"
    ],
    "published": "2025-03-18T06:54:59+00:00",
    "summary": "Multimodal Large Language Models (MLLMs) have demonstrated exceptional performance in artificial intelligence by facilitating integrated understanding across diverse modalities, including text, images, video, audio, and speech. However, their deployment in real-world applications raises significant concerns about adversarial vulnerabilities that could compromise their safety and reliability. Unlike unimodal models, MLLMs face unique challenges due to the interdependencies among modalities, making them susceptible to modality-specific threats and cross-modal adversarial manipulations. This paper reviews the adversarial robustness of MLLMs, covering different modalities. We begin with an overview of MLLMs and a taxonomy of adversarial attacks tailored to each modality. Next, we review key datasets and evaluation metrics used to assess the robustness of MLLMs. After that, we provide an in-depth review of attacks targeting MLLMs across different modalities. Our survey also identifies critical challenges and suggests promising future research directions."
  },
  {
    "title": "What was Said, What was not Said",
    "url": "http://arxiv.org/abs/2503.13958v1",
    "arxiv_id": "2503.13958v1",
    "authors": [
      "Hamid Jahanian"
    ],
    "published": "2025-03-18T06:50:58+00:00",
    "summary": "In the process industry, the configuration of Safety Instrumented Systems (SIS) must comply with a defined set of safety requirements, typically documented in the Safety Requirements Specification (SRS). The functional safety standard IEC 61511 outlines the necessary content and quality criteria for the SRS. However, developing an effective SRS can be challenging. This article examines some of these challenges and proposes good practices to address them. It discusses SRS ownership, \"staged\" development of SRS, and the classification and traceability of requirements. Additionally, it explores the issue of untold \"negative\" requirements and suggests exploratory \"inspection\" of SIS Application Programs (APs) as a potential remedy."
  },
  {
    "title": "Joint ADS-B in B5G for Hierarchical UAV Networks: Performance Analysis and MEC Based Optimization",
    "url": "http://arxiv.org/abs/2503.13907v1",
    "arxiv_id": "2503.13907v1",
    "authors": [
      "Chao Dong",
      "Yiyang Liao",
      "Ziye Jia",
      "Qihui Wu",
      "Lei Zhang"
    ],
    "published": "2025-03-18T05:10:11+00:00",
    "summary": "Unmanned aerial vehicles (UAVs) play significant roles in multiple fields, which brings great challenges for the airspace safety. In order to achieve efficient surveillance and break the limitation of application scenarios caused by single communication, we propose the collaborative surveillance model for hierarchical UAVs based on the cooperation of automatic dependent surveillance-broadcast (ADS-B) and 5G. Specifically, UAVs are hierarchical deployed, with the low-altitude central UAV equipped with the 5G module, and the high-altitude central UAV with ADS-B, which helps automatically broadcast the flight information to surrounding aircraft and ground stations. Firstly, we build the framework, derive the analytic expression, and analyze the channel performance of both air-to-ground (A2G) and air-to-air (A2A). Then, since the redundancy or information loss during transmission aggravates the monitoring performance, the mobile edge computing (MEC) based on-board processing algorithm is proposed. Finally, the performances of the proposed model and algorithm are verified through both simulations and experiments. In detail, the redundant data filtered out by the proposed algorithm accounts for 53.48%, and the supplementary data accounts for 16.42% of the optimized data. This work designs a UAV monitoring framework and proposes an algorithm to enhance the observability of trajectory surveillance, which helps improve the airspace safety and enhance the air traffic flow management."
  },
  {
    "title": "Constraints on LIGO/Virgo Compact Object Mergers from Late-time Radio Observations",
    "url": "http://arxiv.org/abs/2503.13884v1",
    "arxiv_id": "2503.13884v1",
    "authors": [
      "Ashna Gulati",
      "Tara Murphy",
      "Dougal Dobie",
      "Adam Deller",
      "David L. Kaplan",
      "Emil Lenc",
      "Ilya Mandel",
      "Stefan Duchesne",
      "Vanessa Moss"
    ],
    "published": "2025-03-18T04:29:27+00:00",
    "summary": "We present results from a search for radio afterglows of compact object mergers conducted with the Australian SKA Pathfinder. We used data from four epochs of the Rapid ASKAP Continuum Survey to search compact binary merger localization regions observed during the LIGO/Virgo O2, and O3 observing runs. Our investigation focused on eleven events (published in the GWTC-1, GWTC-2, and GWTC-3 catalogues of gravitational-wave events) with 90\\% posterior localisations smaller than $150\\,\\deg^2$ and $\\ge$99\\% probabilities of being of astrophysical origin, to identify potential radio afterglow-like transients up to $\\lesssim$1500 days post-merger. We identified candidate afterglow-type variable sources in the 90\\% localisation for events -- GW190503, GW200202 and GW200208, which were ruled out as unlikely to be related to the corresponding GW event on further analysis. Since we find no likely candidate counterparts, we constrain the inclination angle and the circum-merger density at isotropic equivalent energies ranging from $2\\times10^{51} -1\\times10^{54}\\rm \\:erg$. These constraints are based on the assumption that the electron energy distribution in the associated jets follows a power-law index of $ p = 2.2$, with 1% of the shock energy in the magnetic field ($ \\epsilon_B = 0.01$) and 10% in the electrons ($\\epsilon_e = 0.1$). We discuss the detectability of late-time afterglows as a function of merger distance and inclination angles with millijansky surveys."
  },
  {
    "title": "Constraints on LIGO/Virgo Compact Object Mergers from Late-time Radio Observations",
    "url": "http://arxiv.org/abs/2503.13884v2",
    "arxiv_id": "2503.13884v2",
    "authors": [
      "Ashna Gulati",
      "Tara Murphy",
      "Dougal Dobie",
      "Adam Deller",
      "David L. Kaplan",
      "Emil Lenc",
      "Ilya Mandel",
      "Stefan Duchesne",
      "Vanessa Moss"
    ],
    "published": "2025-03-18T04:29:27+00:00",
    "summary": "We present results from a search for radio afterglows of compact object mergers conducted with the Australian SKA Pathfinder. We used data from four epochs of the Rapid ASKAP Continuum Survey to search compact binary merger localization regions observed during the LIGO/Virgo O2, and O3 observing runs. Our investigation focused on eleven events (published in the GWTC-1, GWTC-2, and GWTC-3 catalogues of gravitational-wave events) with 90\\% posterior localisations smaller than $150\\,\\deg^2$ and $\\ge$99\\% probabilities of being of astrophysical origin, to identify potential radio afterglow-like transients up to $\\lesssim$1500 days post-merger. We identified candidate afterglow-type variable sources in the 90\\% localisation for events -- GW190503, GW200202 and GW200208, which were ruled out as unlikely to be related to the corresponding GW event on further analysis. Since we find no likely candidate counterparts, we constrain the inclination angle and the circum-merger density at isotropic equivalent energies ranging from $2\\times10^{51} -1\\times10^{54}\\rm \\:erg$. These constraints are based on the assumption that the electron energy distribution in the associated jets follows a power-law index of $ p = 2.2$, with 1% of the shock energy in the magnetic field ($ \\epsilon_B = 0.01$) and 10% in the electrons ($\\epsilon_e = 0.1$). We discuss the detectability of late-time afterglows as a function of merger distance and inclination angles with millijansky surveys."
  },
  {
    "title": "FlexStep: Enabling Flexible Error Detection in Multi/Many-core Real-time Systems",
    "url": "http://arxiv.org/abs/2503.13848v1",
    "arxiv_id": "2503.13848v1",
    "authors": [
      "Tinglue Wang",
      "Yiming Li",
      "Wei Tang",
      "Jiapeng Guan",
      "Zhenghui Guo",
      "Renshuang Jiang",
      "Ran Wei",
      "Jing Li",
      "Zhe Jiang"
    ],
    "published": "2025-03-18T02:42:51+00:00",
    "summary": "Reliability and real-time responsiveness in safety-critical systems have traditionally been achieved using error detection mechanisms, such as LockStep, which require pre-configured checker cores,strict synchronisation between main and checker cores, static error detection regions, or limited preemption capabilities. However, these core-bound hardware mechanisms often lead to significant resource over-provisioning, and diminished real-time responsiveness, particularly in modern systems where tasks with varying reliability requirements are consolidated on shared processors to improve efficiency, reduce costs, and save power. To address these challenges, this work presents FlexStep, a systematic solution that integrates hardware and software across the SoC, ISA, and OS scheduling layers. FlexStep features a novel microarchitecture that supports dynamic core configuration and asynchronous, preemptive error detection. The FlexStep architecture naturally allows for flexible task scheduling and error detection, enabling new scheduling algorithms that enhance both resource efficiency and real-time schedulability. We publicly release FlexStep's source code, at https://anonymous.4open.science/r/FlexStep-DAC25-7B0C."
  },
  {
    "title": "Automatic MILP Model Construction for Multi-Robot Task Allocation and Scheduling Based on Large Language Models",
    "url": "http://arxiv.org/abs/2503.13813v1",
    "arxiv_id": "2503.13813v1",
    "authors": [
      "Mingming Peng",
      "Zhendong Chen",
      "Jie Yang",
      "Jin Huang",
      "Zhengqi Shi",
      "Qihao Liu",
      "Xinyu Li",
      "Liang Gao"
    ],
    "published": "2025-03-18T01:45:19+00:00",
    "summary": "With the accelerated development of Industry 4.0, intelligent manufacturing systems increasingly require efficient task allocation and scheduling in multi-robot systems. However, existing methods rely on domain expertise and face challenges in adapting to dynamic production constraints. Additionally, enterprises have high privacy requirements for production scheduling data, which prevents the use of cloud-based large language models (LLMs) for solution development. To address these challenges, there is an urgent need for an automated modeling solution that meets data privacy requirements. This study proposes a knowledge-augmented mixed integer linear programming (MILP) automated formulation framework, integrating local LLMs with domain-specific knowledge bases to generate executable code from natural language descriptions automatically. The framework employs a knowledge-guided DeepSeek-R1-Distill-Qwen-32B model to extract complex spatiotemporal constraints (82% average accuracy) and leverages a supervised fine-tuned Qwen2.5-Coder-7B-Instruct model for efficient MILP code generation (90% average accuracy). Experimental results demonstrate that the framework successfully achieves automatic modeling in the aircraft skin manufacturing case while ensuring data privacy and computational efficiency. This research provides a low-barrier and highly reliable technical path for modeling in complex industrial scenarios."
  },
  {
    "title": "Do Large Language Models Understand Performance Optimization?",
    "url": "http://arxiv.org/abs/2503.13772v1",
    "arxiv_id": "2503.13772v1",
    "authors": [
      "Bowen Cui",
      "Tejas Ramesh",
      "Oscar Hernandez",
      "Keren Zhou"
    ],
    "published": "2025-03-17T23:30:23+00:00",
    "summary": "Large Language Models (LLMs) have emerged as powerful tools for software development tasks such as code completion, translation, and optimization. However, their ability to generate efficient and correct code, particularly in complex High-Performance Computing (HPC) contexts, has remained underexplored. To address this gap, this paper presents a comprehensive benchmark suite encompassing multiple critical HPC computational motifs to evaluate the performance of code optimized by state-of-the-art LLMs, including OpenAI o1, Claude-3.5, and Llama-3.2. In addition to analyzing basic computational kernels, we developed an agent system that integrates LLMs to assess their effectiveness in real HPC applications. Our evaluation focused on key criteria such as execution time, correctness, and understanding of HPC-specific concepts. We also compared the results with those achieved using traditional HPC optimization tools. Based on the findings, we recognized the strengths of LLMs in understanding human instructions and performing automated code transformations. However, we also identified significant limitations, including their tendency to generate incorrect code and their challenges in comprehending complex control and data flows in sophisticated HPC code."
  },
  {
    "title": "Do Unit Proofs Work? An Empirical Study of Compositional Bounded Model Checking for Memory Safety Verification",
    "url": "http://arxiv.org/abs/2503.13762v1",
    "arxiv_id": "2503.13762v1",
    "authors": [
      "Paschal C. Amusuo",
      "Owen Cochell",
      "Taylor Le Lievre",
      "Parth V. Patil",
      "Aravind Machiry",
      "James C. Davis"
    ],
    "published": "2025-03-17T22:55:12+00:00",
    "summary": "Memory safety defects pose a major threat to software reliability, enabling cyberattacks, outages, and crashes. To mitigate these risks, organizations adopt Compositional Bounded Model Checking (BMC), using unit proofs to formally verify memory safety. However, methods for creating unit proofs vary across organizations and are inconsistent within the same project, leading to errors and missed defects. In addition, unit proofing remains understudied, with no systematic development methods or empirical evaluations.   This work presents the first empirical study on unit proofing for memory safety verification. We introduce a systematic method for creating unit proofs that leverages verification feedback and objective criteria. Using this approach, we develop 73 unit proofs for four embedded operating systems and evaluate their effectiveness, characteristics, cost, and generalizability. Our results show unit proofs are cost-effective, detecting 74\\% of recreated defects, with an additional 9\\% found with increased BMC bounds, and 19 new defects exposed. We also found that embedded software requires small unit proofs, which can be developed in 87 minutes and executed in 61 minutes on average. These findings provide practical guidance for engineers and empirical data to inform tooling design."
  },
  {
    "title": "Fire and Smoke Datasets in 20 Years: An In-depth Review",
    "url": "http://arxiv.org/abs/2503.14552v1",
    "arxiv_id": "2503.14552v1",
    "authors": [
      "Sayed Pedram Haeri Boroujeni",
      "Niloufar Mehrabi",
      "Fatemeh Afghah",
      "Connor Peter McGrath",
      "Danish Bhatkar",
      "Mithilesh Anil Biradar",
      "Abolfazl Razi"
    ],
    "published": "2025-03-17T22:08:02+00:00",
    "summary": "Fire and smoke phenomena pose a significant threat to the natural environment, ecosystems, and global economy, as well as human lives and wildlife. In this particular circumstance, there is a demand for more sophisticated and advanced technologies to implement an effective strategy for early detection, real-time monitoring, and minimizing the overall impacts of fires on ecological balance and public safety. Recently, the rapid advancement of Artificial Intelligence (AI) and Computer Vision (CV) frameworks has substantially revolutionized the momentum for developing efficient fire management systems. However, these systems extensively rely on the availability of adequate and high-quality fire and smoke data to create proficient Machine Learning (ML) methods for various tasks, such as detection and monitoring. Although fire and smoke datasets play a critical role in training, evaluating, and testing advanced Deep Learning (DL) models, a comprehensive review of the existing datasets is still unexplored. For this purpose, we provide an in-depth review to systematically analyze and evaluate fire and smoke datasets collected over the past 20 years. We investigate the characteristics of each dataset, including type, size, format, collection methods, and geographical diversities. We also review and highlight the unique features of each dataset, such as imaging modalities (RGB, thermal, infrared) and their applicability for different fire management tasks (classification, segmentation, detection). Furthermore, we summarize the strengths and weaknesses of each dataset and discuss their potential for advancing research and technology in fire management. Ultimately, we conduct extensive experimental analyses across different datasets using several state-of-the-art algorithms, such as ResNet-50, DeepLab-V3, and YoloV8."
  },
  {
    "title": "Optimal Replenishment Policies for Industrial Vending Machines",
    "url": "http://arxiv.org/abs/2503.13643v1",
    "arxiv_id": "2503.13643v1",
    "authors": [
      "Karina M. Sindermann",
      "Esma S. Gel",
      "Nesim K. Erkip"
    ],
    "published": "2025-03-17T18:47:11+00:00",
    "summary": "Industrial Vending Machines (IVMs) automate the dispensing of a variety of supplies like safety equipment and tools at customer sites, providing 24/7 access while tracking inventory in real-time. Industrial distribution companies typically manage the replenishment of IVMs using periodic schedules, which do not take advantage of these advanced real-time monitoring capabilities. We develop two approaches to optimize the long-term average cost of replenishments and stockouts per unit time: a state-dependent optimal control policy that jointly considers all inventory levels (referred to as trigger set policy) and a fixed cycle policy that optimizes replenishment frequency. We prove the monotonicity of the optimal trigger set policy and leverage it to design a computationally efficient approximate online control framework. Unlike existing methods, which typically handle a very limited number of items due to computational constraints, our approach scales to hundreds of items while achieving near-optimal performance. Leveraging transaction data from our industrial partner, we conduct an extensive set of numerical experiments to demonstrate this claim. Our results show that optimal fixed cycle replenishment reduces costs by 61.7 to 78.6% compared to current practice, with our online control framework delivering an additional 4.1 to 22.9% improvement. Our novel theoretical results provide practical tools for effective replenishment management in this modern vendor-managed inventory context."
  },
  {
    "title": "MetaScale: Test-Time Scaling with Evolving Meta-Thoughts",
    "url": "http://arxiv.org/abs/2503.13447v1",
    "arxiv_id": "2503.13447v1",
    "authors": [
      "Qin Liu",
      "Wenxuan Zhou",
      "Nan Xu",
      "James Y. Huang",
      "Fei Wang",
      "Sheng Zhang",
      "Hoifung Poon",
      "Muhao Chen"
    ],
    "published": "2025-03-17T17:59:54+00:00",
    "summary": "One critical challenge for large language models (LLMs) for making complex reasoning is their reliance on matching reasoning patterns from training data, instead of proactively selecting the most appropriate cognitive strategy to solve a given task. Existing approaches impose fixed cognitive structures that enhance performance in specific tasks but lack adaptability across diverse scenarios. To address this limitation, we introduce METASCALE, a test-time scaling framework based on meta-thoughts -- adaptive thinking strategies tailored to each task. METASCALE initializes a pool of candidate meta-thoughts, then iteratively selects and evaluates them using a multi-armed bandit algorithm with upper confidence bound selection, guided by a reward model. To further enhance adaptability, a genetic algorithm evolves high-reward meta-thoughts, refining and extending the strategy pool over time. By dynamically proposing and optimizing meta-thoughts at inference time, METASCALE improves both accuracy and generalization across a wide range of tasks. Experimental results demonstrate that MetaScale consistently outperforms standard inference approaches, achieving an 11% performance gain in win rate on Arena-Hard for GPT-4o, surpassing o1-mini by 0.9% under style control. Notably, METASCALE scales more effectively with increasing sampling budgets and produces more structured, expert-level responses."
  },
  {
    "title": "Faithfulness of LLM Self-Explanations for Commonsense Tasks: Larger Is Better, and Instruction-Tuning Allows Trade-Offs but Not Pareto Dominance",
    "url": "http://arxiv.org/abs/2503.13445v1",
    "arxiv_id": "2503.13445v1",
    "authors": [
      "Noah Y. Siegel",
      "Nicolas Heess",
      "Maria Perez-Ortiz",
      "Oana-Maria Camburu"
    ],
    "published": "2025-03-17T17:59:39+00:00",
    "summary": "As large language models (LLMs) become increasingly capable, ensuring that their self-generated explanations are faithful to their internal decision-making process is critical for safety and oversight. In this work, we conduct a comprehensive counterfactual faithfulness analysis across 62 models from 8 families, encompassing both pretrained and instruction-tuned variants and significantly extending prior studies of counterfactual tests. We introduce phi-CCT, a simplified variant of the Correlational Counterfactual Test, which avoids the need for token probabilities while explaining most of the variance of the original test. Our findings reveal clear scaling trends: larger models are consistently more faithful on our metrics. However, when comparing instruction-tuned and human-imitated explanations, we find that observed differences in faithfulness can often be attributed to explanation verbosity, leading to shifts along the true-positive/false-positive Pareto frontier. While instruction-tuning and prompting can influence this trade-off, we find limited evidence that they fundamentally expand the frontier of explanatory faithfulness beyond what is achievable with pretrained models of comparable size. Our analysis highlights the nuanced relationship between instruction-tuning, verbosity, and the faithful representation of model decision processes."
  },
  {
    "title": "Escaping Plato's Cave: Robust Conceptual Reasoning through Interpretable 3D Neural Object Volumes",
    "url": "http://arxiv.org/abs/2503.13429v1",
    "arxiv_id": "2503.13429v1",
    "authors": [
      "Nhi Pham",
      "Bernt Schiele",
      "Adam Kortylewski",
      "Jonas Fischer"
    ],
    "published": "2025-03-17T17:55:15+00:00",
    "summary": "With the rise of neural networks, especially in high-stakes applications, these networks need two properties (i) robustness and (ii) interpretability to ensure their safety. Recent advances in classifiers with 3D volumetric object representations have demonstrated a greatly enhanced robustness in out-of-distribution data. However, these 3D-aware classifiers have not been studied from the perspective of interpretability. We introduce CAVE - Concept Aware Volumes for Explanations - a new direction that unifies interpretability and robustness in image classification. We design an inherently-interpretable and robust classifier by extending existing 3D-aware classifiers with concepts extracted from their volumetric representations for classification. In an array of quantitative metrics for interpretability, we compare against different concept-based approaches across the explainable AI literature and show that CAVE discovers well-grounded concepts that are used consistently across images, while achieving superior robustness."
  },
  {
    "title": "Securing Virtual Reality Experiences: Unveiling and Tackling Cybersickness Attacks with Explainable AI",
    "url": "http://arxiv.org/abs/2503.13419v1",
    "arxiv_id": "2503.13419v1",
    "authors": [
      "Ripan Kumar Kundu",
      "Matthew Denton",
      "Genova Mongalo",
      "Prasad Calyam",
      "Khaza Anuarul Hoque"
    ],
    "published": "2025-03-17T17:49:51+00:00",
    "summary": "The synergy between virtual reality (VR) and artificial intelligence (AI), specifically deep learning (DL)-based cybersickness detection models, has ushered in unprecedented advancements in immersive experiences by automatically detecting cybersickness severity and adaptively various mitigation techniques, offering a smooth and comfortable VR experience. While this DL-enabled cybersickness detection method provides promising solutions for enhancing user experiences, it also introduces new risks since these models are vulnerable to adversarial attacks; a small perturbation of the input data that is visually undetectable to human observers can fool the cybersickness detection model and trigger unexpected mitigation, thus disrupting user immersive experiences (UIX) and even posing safety risks. In this paper, we present a new type of VR attack, i.e., a cybersickness attack, which successfully stops the triggering of cybersickness mitigation by fooling DL-based cybersickness detection models and dramatically hinders the UIX. Next, we propose a novel explainable artificial intelligence (XAI)-guided cybersickness attack detection framework to detect such attacks in VR to ensure UIX and a comfortable VR experience. We evaluate the proposed attack and the detection framework using two state-of-the-art open-source VR cybersickness datasets: Simulation 2021 and Gameplay dataset. Finally, to verify the effectiveness of our proposed method, we implement the attack and the XAI-based detection using a testbed with a custom-built VR roller coaster simulation with an HTC Vive Pro Eye headset and perform a user study. Our study shows that such an attack can dramatically hinder the UIX. However, our proposed XAI-guided cybersickness attack detection can successfully detect cybersickness attacks and trigger the proper mitigation, effectively reducing VR cybersickness."
  },
  {
    "title": "Continuous-time Data-driven Barrier Certificate Synthesis",
    "url": "http://arxiv.org/abs/2503.13392v1",
    "arxiv_id": "2503.13392v1",
    "authors": [
      "Luke Rickard",
      "Alessandro Abate",
      "Kostas Margellos"
    ],
    "published": "2025-03-17T17:25:32+00:00",
    "summary": "We consider the problem of verifying safety for continuous-time dynamical systems. Developing upon recent advancements in data-driven verification, we use only a finite number of sampled trajectories to learn a barrier certificate, namely a function which verifies safety. We train a safety-informed neural network to act as this certificate, with an appropriately designed loss function to encompass the safety conditions. In addition, we provide probabilistic generalisation guarantees from discrete samples of continuous trajectories, to unseen continuous ones. Numerical investigations demonstrate the efficacy of our approach and contrast it with related results in the literature."
  },
  {
    "title": "Mitigating Visual Forgetting via Take-along Visual Conditioning for Multi-modal Long CoT Reasoning",
    "url": "http://arxiv.org/abs/2503.13360v1",
    "arxiv_id": "2503.13360v1",
    "authors": [
      "Hai-Long Sun",
      "Zhun Sun",
      "Houwen Peng",
      "Han-Jia Ye"
    ],
    "published": "2025-03-17T16:45:12+00:00",
    "summary": "Recent advancements in Large Language Models (LLMs) have demonstrated enhanced reasoning capabilities, evolving from Chain-of-Thought (CoT) prompting to advanced, product-oriented solutions like OpenAI o1. During our re-implementation of this model, we noticed that in multimodal tasks requiring visual input (e.g., geometry problems), Multimodal LLMs (MLLMs) struggle to maintain focus on the visual information, in other words, MLLMs suffer from a gradual decline in attention to visual information as reasoning progresses, causing text-over-relied outputs. To investigate this, we ablate image inputs during long-chain reasoning. Concretely, we truncate the reasoning process midway, then re-complete the reasoning process with the input image removed. We observe only a ~2% accuracy drop on MathVista's test-hard subset, revealing the model's textual outputs dominate the following reasoning process. Motivated by this, we propose Take-along Visual Conditioning (TVC), a strategy that shifts image input to critical reasoning stages and compresses redundant visual tokens via dynamic pruning. This methodology helps the model retain attention to the visual components throughout the reasoning. Our approach achieves state-of-the-art performance on average across five mathematical reasoning benchmarks (+3.4% vs previous sota), demonstrating the effectiveness of TVC in enhancing multimodal reasoning systems."
  },
  {
    "title": "Reliable and Efficient Amortized Model-based Evaluation",
    "url": "http://arxiv.org/abs/2503.13335v1",
    "arxiv_id": "2503.13335v1",
    "authors": [
      "Sang Truong",
      "Yuheng Tu",
      "Percy Liang",
      "Bo Li",
      "Sanmi Koyejo"
    ],
    "published": "2025-03-17T16:15:02+00:00",
    "summary": "Comprehensive evaluations of language models (LM) during both development and deployment phases are necessary because these models possess numerous capabilities (e.g., mathematical reasoning, legal support, or medical diagnostic) as well as safety risks (e.g., racial bias, toxicity, or misinformation). The average score across a wide range of benchmarks provides a signal that helps guide the use of these LMs in practice. Currently, holistic evaluations are costly due to the large volume of benchmark questions, making frequent evaluations impractical. A popular attempt to lower the cost is to compute the average score on a subset of the benchmark. This approach, unfortunately, often renders an unreliable measure of LM performance because the average score is often confounded with the difficulty of the questions in the benchmark subset. Item response theory (IRT) was designed to address this challenge, providing a reliable measurement by careful controlling for question difficulty. Unfortunately, question difficulty is expensive to estimate. Facing this challenge, we train a model that predicts question difficulty from its content, enabling a reliable measurement at a fraction of the cost. In addition, we leverage this difficulty predictor to further improve the evaluation efficiency through training a question generator given a difficulty level. This question generator is essential in adaptive testing, where, instead of using a random subset of the benchmark questions, informative questions are adaptively chosen based on the current estimation of LLM performance. Experiments on 22 common natural language benchmarks and 172 LMs show that this approach is more reliable and efficient compared to current common practice."
  },
  {
    "title": "Is Crime Displacement Inevitable? Evidence from Police Crackdowns in Fortaleza, Brazil",
    "url": "http://arxiv.org/abs/2503.13571v1",
    "arxiv_id": "2503.13571v1",
    "authors": [
      "Jos\u00e9 Raimundo Carvalho",
      "Marcelino Guerra"
    ],
    "published": "2025-03-17T12:25:44+00:00",
    "summary": "We evaluated one of the most common policing strategies in Brazil: the allocation of police blitzes. This place-based focused deterrence intervention has well-defined assignments, and 3,423 interventions were precisely recorded in Fortaleza-CE, Brazil, between 2012 and 2013. Our analysis takes advantage of the high spatiotemporal daily data resolution coming from an unprecedented longitudinal micro-Big Data (GPS and PING records) to make comparisons of small intervention areas, while controlling for common daily trends, deterrence (spatial and temporal), and diffusion; to show that an average police crackdown causes a 35% decrease in violent crime occurrences. There are diminishing returns of public safety to hours spent by police in a single area, corroborating what police officers know well from their own experience and discretionary behavior. Although crime increases by 6% immediately after the end of a blitz, we observe lasting deterrent effects (diffusion) after 2-3 days. The residual deterrence cancels the relocation of the crime, and the intervention does not generate significant temporal displacement. In addition, we do not find spatial displacement from crime in blocks up to 1.5 km from a blitz. This type of micropolicing tactics generates deterrence by being highly visible in a street segment for a short period and intermittently quasirandom in space-time, which produces uncertainty that might be crucial in minimizing the temporal and spatial displacement of crime. Of public policy interest, we show that the allocation of blitzes passes in an initial cost-benefit analysis."
  },
  {
    "title": "LIVEPOINT: Fully Decentralized, Safe, Deadlock-Free Multi-Robot Control in Cluttered Environments with High-Dimensional Inputs",
    "url": "http://arxiv.org/abs/2503.13098v1",
    "arxiv_id": "2503.13098v1",
    "authors": [
      "Jeffrey Chen",
      "Rohan Chandra"
    ],
    "published": "2025-03-17T12:07:25+00:00",
    "summary": "Fully decentralized, safe, and deadlock-free multi-robot navigation in dynamic, cluttered environments is a critical challenge in robotics. Current methods require exact state measurements in order to enforce safety and liveness e.g. via control barrier functions (CBFs), which is challenging to achieve directly from onboard sensors like lidars and cameras. This work introduces LIVEPOINT, a decentralized control framework that synthesizes universal CBFs over point clouds to enable safe, deadlock-free real-time multi-robot navigation in dynamic, cluttered environments. Further, LIVEPOINT ensures minimally invasive deadlock avoidance behavior by dynamically adjusting agents' speeds based on a novel symmetric interaction metric. We validate our approach in simulation experiments across highly constrained multi-robot scenarios like doorways and intersections. Results demonstrate that LIVEPOINT achieves zero collisions or deadlocks and a 100% success rate in challenging settings compared to optimization-based baselines such as MPC and ORCA and neural methods such as MPNet, which fail in such environments. Despite prioritizing safety and liveness, LIVEPOINT is 35% smoother than baselines in the doorway environment, and maintains agility in constrained environments while still being safe and deadlock-free."
  },
  {
    "title": "A Framework to Assess Multilingual Vulnerabilities of LLMs",
    "url": "http://arxiv.org/abs/2503.13081v1",
    "arxiv_id": "2503.13081v1",
    "authors": [
      "Likai Tang",
      "Niruth Bogahawatta",
      "Yasod Ginige",
      "Jiarui Xu",
      "Shixuan Sun",
      "Surangika Ranathunga",
      "Suranga Seneviratne"
    ],
    "published": "2025-03-17T11:39:44+00:00",
    "summary": "Large Language Models (LLMs) are acquiring a wider range of capabilities, including understanding and responding in multiple languages. While they undergo safety training to prevent them from answering illegal questions, imbalances in training data and human evaluation resources can make these models more susceptible to attacks in low-resource languages (LRL). This paper proposes a framework to automatically assess the multilingual vulnerabilities of commonly used LLMs. Using our framework, we evaluated six LLMs across eight languages representing varying levels of resource availability. We validated the assessments generated by our automated framework through human evaluation in two languages, demonstrating that the framework's results align with human judgments in most cases. Our findings reveal vulnerabilities in LRL; however, these may pose minimal risk as they often stem from the model's poor performance, resulting in incoherent responses."
  },
  {
    "title": "Sensorless Remote Center of Motion Misalignment Estimation",
    "url": "http://arxiv.org/abs/2503.13011v1",
    "arxiv_id": "2503.13011v1",
    "authors": [
      "Hao Yang",
      "Lidia Al-Zogbi",
      "Ahmet Yildiz",
      "Nabil Simaan",
      "Jie Ying Wu"
    ],
    "published": "2025-03-17T10:11:05+00:00",
    "summary": "Laparoscopic surgery constrains instrument motion around a fixed pivot point at the incision into a patient to minimize tissue trauma. Surgical robots achieve this through either hardware to software-based remote center of motion (RCM) constraints. However, accurate RCM alignment is difficult due to manual trocar placement, patient motion, and tissue deformation. Misalignment between the robot's RCM point and the patient incision site can cause unsafe forces at the incision site. This paper presents a sensorless force estimation-based framework for dynamically assessing and optimizing RCM misalignment in robotic surgery. Our experiments demonstrate that misalignment exceeding 20 mm can generate large enough forces to potentially damage tissue, emphasizing the need for precise RCM positioning. For misalignment $D\\geq $ 20 mm, our optimization algorithm estimates the RCM offset with an absolute error within 5 mm. Accurate RCM misalignment estimation is a step toward automated RCM misalignment compensation, enhancing safety and reducing tissue damage in robotic-assisted laparoscopic surgery."
  },
  {
    "title": "SparseAlign: A Fully Sparse Framework for Cooperative Object Detection",
    "url": "http://arxiv.org/abs/2503.12982v1",
    "arxiv_id": "2503.12982v1",
    "authors": [
      "Yunshuang Yuan",
      "Yan Xia",
      "Daniel Cremers",
      "Monika Sester"
    ],
    "published": "2025-03-17T09:38:53+00:00",
    "summary": "Cooperative perception can increase the view field and decrease the occlusion of an ego vehicle, hence improving the perception performance and safety of autonomous driving. Despite the success of previous works on cooperative object detection, they mostly operate on dense Bird's Eye View (BEV) feature maps, which are computationally demanding and can hardly be extended to long-range detection problems. More efficient fully sparse frameworks are rarely explored. In this work, we design a fully sparse framework, SparseAlign, with three key features: an enhanced sparse 3D backbone, a query-based temporal context learning module, and a robust detection head specially tailored for sparse features. Extensive experimental results on both OPV2V and DairV2X datasets show that our framework, despite its sparsity, outperforms the state of the art with less communication bandwidth requirements. In addition, experiments on the OPV2Vt and DairV2Xt datasets for time-aligned cooperative object detection also show a significant performance gain compared to the baseline works."
  },
  {
    "title": "Efficient Action-Constrained Reinforcement Learning via Acceptance-Rejection Method and Augmented MDPs",
    "url": "http://arxiv.org/abs/2503.12932v1",
    "arxiv_id": "2503.12932v1",
    "authors": [
      "Wei Hung",
      "Shao-Hua Sun",
      "Ping-Chun Hsieh"
    ],
    "published": "2025-03-17T08:41:43+00:00",
    "summary": "Action-constrained reinforcement learning (ACRL) is a generic framework for learning control policies with zero action constraint violation, which is required by various safety-critical and resource-constrained applications. The existing ACRL methods can typically achieve favorable constraint satisfaction but at the cost of either high computational burden incurred by the quadratic programs (QP) or increased architectural complexity due to the use of sophisticated generative models. In this paper, we propose a generic and computationally efficient framework that can adapt a standard unconstrained RL method to ACRL through two modifications: (i) To enforce the action constraints, we leverage the classic acceptance-rejection method, where we treat the unconstrained policy as the proposal distribution and derive a modified policy with feasible actions. (ii) To improve the acceptance rate of the proposal distribution, we construct an augmented two-objective Markov decision process (MDP), which include additional self-loop state transitions and a penalty signal for the rejected actions. This augmented MDP incentives the learned policy to stay close to the feasible action sets. Through extensive experiments in both robot control and resource allocation domains, we demonstrate that the proposed framework enjoys faster training progress, better constraint satisfaction, and a lower action inference time simultaneously than the state-of-the-art ACRL methods. We have made the source code publicly available to encourage further research in this direction."
  },
  {
    "title": "MirrorGuard: Adaptive Defense Against Jailbreaks via Entropy-Guided Mirror Crafting",
    "url": "http://arxiv.org/abs/2503.12931v1",
    "arxiv_id": "2503.12931v1",
    "authors": [
      "Rui Pu",
      "Chaozhuo Li",
      "Rui Ha",
      "Litian Zhang",
      "Lirong Qiu",
      "Xi Zhang"
    ],
    "published": "2025-03-17T08:41:29+00:00",
    "summary": "Defending large language models (LLMs) against jailbreak attacks is crucial for ensuring their safe deployment. Existing defense strategies generally rely on predefined static criteria to differentiate between harmful and benign prompts. However, such rigid rules are incapable of accommodating the inherent complexity and dynamic nature of real jailbreak attacks. In this paper, we propose a novel concept of ``mirror'' to enable dynamic and adaptive defense. A mirror refers to a dynamically generated prompt that mirrors the syntactic structure of the input while ensuring semantic safety. The personalized discrepancies between the input prompts and their corresponding mirrors serve as the guiding principles for defense. A new defense paradigm, MirrorGuard, is further proposed to detect and calibrate risky inputs based on such mirrors. An entropy-based detection metric, Relative Input Uncertainty (RIU), is integrated into MirrorGuard to quantify the discrepancies between input prompts and mirrors. MirrorGuard is evaluated on several popular datasets, demonstrating state-of-the-art defense performance while maintaining general effectiveness."
  },
  {
    "title": "A Semantic-based Optimization Approach for Repairing LLMs: Case Study on Code Generation",
    "url": "http://arxiv.org/abs/2503.12899v1",
    "arxiv_id": "2503.12899v1",
    "authors": [
      "Jian Gu",
      "Aldeida Aleti",
      "Chunyang Chen",
      "Hongyu Zhang"
    ],
    "published": "2025-03-17T07:59:42+00:00",
    "summary": "Language Models (LMs) are widely used in software engineering for code generation, but they may produce code with errors. Rather than repairing the generated code, an alternative way is to address the underlying failures of models. LM repair offers a lightweight solution to this challenge: it requires minimal data, reduces computational costs, and reduces the side effects. Unlike retraining, LM repair focuses on applying tailored updates to targeted neurons, making it ideal for scenarios with limited resources, high-performance demands, or strict safety requirements. In this paper, we propose \\ul{S}emantic \\ul{T}argeting for \\ul{A}nalytical \\ul{R}epair (\\textsc{STAR}), a pioneering and novel semantic-based optimization approach for repairing LLMs. \\textsc{STAR} realizes main operations in LM repair methods in an optimization process, including locating ``buggy neurons'', solving ``neuron patches'', and patching ``buggy neurons''. Correspondingly, it computes the deltas of weight matrix as the prior information to guide optimization; and attributes the targeted layers and neurons leveraging statistical insights. The neuron patches are computed with a solid semantic-based analytical formula, which directly bridges the changes to logits with the deltas of neurons, by steering latent representations. Compared to the prior work of LM repair (\\textsc{MINT}) and optimization methods (\\textsc{SGD}), \\textsc{STAR} integrates their strengths while mitigating their limitations. \\textsc{STAR} supports solving multiple failures together, significantly improving the usefulness. Evaluated on three code generation tasks using popular code LMs, \\textsc{STAR} demonstrates superior effectiveness. Additionally, \\textsc{STAR} exhibits better efficiency. In terms of side effects, namely the balance between generalization and specificity, \\textsc{STAR} outperforms prior work by a significant margin."
  },
  {
    "title": "ACT360: An Efficient 360-Degree Action Detection and Summarization Framework for Mission-Critical Training and Debriefing",
    "url": "http://arxiv.org/abs/2503.12852v1",
    "arxiv_id": "2503.12852v1",
    "authors": [
      "Aditi Tiwari",
      "Klara Nahrstedt"
    ],
    "published": "2025-03-17T06:12:36+00:00",
    "summary": "Effective training and debriefing are critical in high-stakes, mission-critical environments such as disaster response, military simulations, and industrial safety, where precision and minimizing errors are paramount. The traditional post-training analysis relies on manually reviewing 2D videos, a time-consuming process that lacks comprehensive situational awareness. To address these limitations, we introduce ACT360, a system that leverages 360-degree videos and machine learning for automated action detection and structured debriefing. ACT360 integrates 360YOWO, an enhanced You Only Watch Once (YOWO) model with spatial attention and equirectangular-aware convolution (EAC) to mitigate panoramic video distortions. To enable deployment in resource-constrained environments, we apply quantization and model pruning, reducing the model size by 74% while maintaining robust accuracy (mAP drop of only 1.5%, from 0.865 to 0.850) and improving inference speed. We validate our approach on a publicly available dataset of 55 labeled 360-degree videos covering seven key operational actions, recorded across various real-world training sessions and environmental conditions. Additionally, ACT360 integrates 360AIE (Action Insight Explorer), a web-based interface for automatic action detection, retrieval, and textual summarization using large language models (LLMs), significantly enhancing post-incident analysis efficiency. ACT360 serves as a generalized framework for mission-critical debriefing, incorporating EAC, spatial attention, summarization, and model optimization. These innovations apply to any training environment requiring lightweight action detection and structured post-exercise analysis."
  },
  {
    "title": "In vivo validation of Wireless Power Transfer System for Magnetically Controlled Robotic Capsule Endoscopy",
    "url": "http://arxiv.org/abs/2503.12850v1",
    "arxiv_id": "2503.12850v1",
    "authors": [
      "Alessandro Catania",
      "Michele Bertozzi",
      "Nikita J. Greenidge",
      "Benjamin Calme",
      "Gabriele Bandini",
      "Christian Sbrana",
      "Roberto Cecchi",
      "Alice Buffi",
      "Sebastiano Strangio",
      "Pietro Valdastri",
      "Giuseppe Iannaccone"
    ],
    "published": "2025-03-17T06:03:49+00:00",
    "summary": "This paper presents the in vivo validation of an inductive wireless power transfer (WPT) system integrated for the first time into a magnetically controlled robotic capsule endoscopy platform. The proposed system enables continuous power delivery to the capsule without the need for onboard batteries, thus extending operational time and reducing size constraints. The WPT system operates through a resonant inductive coupling mechanism, based on a transmitting coil mounted on the end effector of a robotic arm that also houses an external permanent magnet and a localization coil for precise capsule manipulation. To ensure robust and stable power transmission in the presence of coil misalignment and rotation, a 3D receiving coil is integrated within the capsule. Additionally, a closed-loop adaptive control system, based on load-shift keying (LSK) modulation, dynamically adjusts the transmitted power to optimize efficiency while maintaining compliance with specific absorption rate (SAR) safety limits. The system has been extensively characterized in laboratory settings and validated through in vivo experiments using a porcine model, demonstrating reliable power transfer and effective robotic navigation in realistic gastrointestinal conditions: the average received power was 110 mW at a distance of 9 cm between the coils, with variable capsule rotation angles. The results confirm the feasibility of the proposed WPT approach for autonomous, battery-free robotic capsule endoscopy, paving the way for enhanced diagnostic in gastrointestinal medicine."
  },
  {
    "title": "SNPL: Simultaneous Policy Learning and Evaluation for Safe Multi-Objective Policy Improvement",
    "url": "http://arxiv.org/abs/2503.12760v1",
    "arxiv_id": "2503.12760v1",
    "authors": [
      "Brian Cho",
      "Ana-Roxana Pop",
      "Ariel Evince",
      "Nathan Kallus"
    ],
    "published": "2025-03-17T02:53:53+00:00",
    "summary": "To design effective digital interventions, experimenters face the challenge of learning decision policies that balance multiple objectives using offline data. Often, they aim to develop policies that maximize goal outcomes, while ensuring there are no undesirable changes in guardrail outcomes. To provide credible recommendations, experimenters must not only identify policies that satisfy the desired changes in goal and guardrail outcomes, but also offer probabilistic guarantees about the changes these policies induce. In practice, however, policy classes are often large, and digital experiments tend to produce datasets with small effect sizes relative to noise. In this setting, standard approaches such as data splitting or multiple testing often result in unstable policy selection and/or insufficient statistical power. In this paper, we provide safe noisy policy learning (SNPL), a novel approach that leverages the concept of algorithmic stability to address these challenges. Our method enables policy learning while simultaneously providing high-confidence guarantees using the entire dataset, avoiding the need for data-splitting. We present finite-sample and asymptotic versions of our algorithm that ensure the recommended policy satisfies high-probability guarantees for avoiding guardrail regressions and/or achieving goal outcome improvements. We test both variants of our approach approach empirically on a real-world application of personalizing SMS delivery. Our results on real-world data suggest that our approach offers dramatic improvements in settings with large policy classes and low signal-to-noise across both finite-sample and asymptotic safety guarantees, offering up to 300\\% improvements in detection rates and 150\\% improvements in policy gains at significantly smaller sample sizes."
  },
  {
    "title": "SafeSlice: Enabling SLA-Compliant O-RAN Slicing via Safe Deep Reinforcement Learning",
    "url": "http://arxiv.org/abs/2503.12753v1",
    "arxiv_id": "2503.12753v1",
    "authors": [
      "Ahmad M. Nagib",
      "Hatem Abou-Zeid",
      "Hossam S. Hassanein"
    ],
    "published": "2025-03-17T02:41:49+00:00",
    "summary": "Deep reinforcement learning (DRL)-based slicing policies have shown significant success in simulated environments but face challenges in physical systems such as open radio access networks (O-RANs) due to simulation-to-reality gaps. These policies often lack safety guarantees to ensure compliance with service level agreements (SLAs), such as the strict latency requirements of immersive applications. As a result, a deployed DRL slicing agent may make resource allocation (RA) decisions that degrade system performance, particularly in previously unseen scenarios. Real-world immersive applications require maintaining SLA constraints throughout deployment to prevent risky DRL exploration. In this paper, we propose SafeSlice to address both the cumulative (trajectory-wise) and instantaneous (state-wise) latency constraints of O-RAN slices. We incorporate the cumulative constraints by designing a sigmoid-based risk-sensitive reward function that reflects the slices' latency requirements. Moreover, we build a supervised learning cost model as part of a safety layer that projects the slicing agent's RA actions to the nearest safe actions, fulfilling instantaneous constraints. We conduct an exhaustive experiment that supports multiple services, including real virtual reality (VR) gaming traffic, to investigate the performance of SafeSlice under extreme and changing deployment conditions. SafeSlice achieves reductions of up to 83.23% in average cumulative latency, 93.24% in instantaneous latency violations, and 22.13% in resource consumption compared to the baselines. The results also indicate SafeSlice's robustness to changing the threshold configurations of latency constraints, a vital deployment scenario that will be realized by the O-RAN paradigm to empower mobile network operators (MNOs)."
  },
  {
    "title": "Identifying Cooperative Personalities in Multi-agent Contexts through Personality Steering with Representation Engineering",
    "url": "http://arxiv.org/abs/2503.12722v1",
    "arxiv_id": "2503.12722v1",
    "authors": [
      "Kenneth J. K. Ong",
      "Lye Jia Jun",
      "Hieu Minh \"Jord\" Nguyen",
      "Seong Hah Cho",
      "Natalia P\u00e9rez-Campanero Antol\u00edn"
    ],
    "published": "2025-03-17T01:21:54+00:00",
    "summary": "As Large Language Models (LLMs) gain autonomous capabilities, their coordination in multi-agent settings becomes increasingly important. However, they often struggle with cooperation, leading to suboptimal outcomes. Inspired by Axelrod's Iterated Prisoner's Dilemma (IPD) tournaments, we explore how personality traits influence LLM cooperation. Using representation engineering, we steer Big Five traits (e.g., Agreeableness, Conscientiousness) in LLMs and analyze their impact on IPD decision-making. Our results show that higher Agreeableness and Conscientiousness improve cooperation but increase susceptibility to exploitation, highlighting both the potential and limitations of personality-based steering for aligning AI agents."
  },
  {
    "title": "Can Reasoning Models Reason about Hardware? An Agentic HLS Perspective",
    "url": "http://arxiv.org/abs/2503.12721v1",
    "arxiv_id": "2503.12721v1",
    "authors": [
      "Luca Collini",
      "Andrew Hennessee",
      "Ramesh Karri",
      "Siddharth Garg"
    ],
    "published": "2025-03-17T01:21:39+00:00",
    "summary": "Recent Large Language Models (LLMs) such as OpenAI o3-mini and DeepSeek-R1 use enhanced reasoning through Chain-of-Thought (CoT). Their potential in hardware design, which relies on expert-driven iterative optimization, remains unexplored. This paper investigates whether reasoning LLMs can address challenges in High-Level Synthesis (HLS) design space exploration and optimization. During HLS, engineers manually define pragmas/directives to balance performance and resource constraints. We propose an LLM-based optimization agentic framework that automatically restructures code, inserts pragmas, and identifies optimal design points via feedback from HLs tools and access to integer-linear programming (ILP) solvers. Experiments compare reasoning models against conventional LLMs on benchmarks using success rate, efficiency, and design quality (area/latency) metrics, and provide the first-ever glimpse into the CoTs produced by a powerful open-source reasoning model like DeepSeek-R1."
  },
  {
    "title": "CDKFormer: Contextual Deviation Knowledge-Based Transformer for Long-Tail Trajectory Prediction",
    "url": "http://arxiv.org/abs/2503.12695v1",
    "arxiv_id": "2503.12695v1",
    "authors": [
      "Yuansheng Lian",
      "Ke Zhang",
      "Meng Li"
    ],
    "published": "2025-03-16T23:48:13+00:00",
    "summary": "Predicting the future movements of surrounding vehicles is essential for ensuring the safe operation and efficient navigation of autonomous vehicles (AVs) in urban traffic environments. Existing vehicle trajectory prediction methods primarily focus on improving overall performance, yet they struggle to address long-tail scenarios effectively. This limitation often leads to poor predictions in rare cases, significantly increasing the risk of safety incidents. Taking Argoverse 2 motion forecasting dataset as an example, we first investigate the long-tail characteristics in trajectory samples from two perspectives, individual motion and group interaction, and deriving deviation features to distinguish abnormal from regular scenarios. On this basis, we propose CDKFormer, a Contextual Deviation Knowledge-based Transformer model for long-tail trajectory prediction. CDKFormer integrates an attention-based scene context fusion module to encode spatiotemporal interaction and road topology. An additional deviation feature fusion module is proposed to capture the dynamic deviations in the target vehicle status. We further introduce a dual query-based decoder, supported by a multi-stream decoder block, to sequentially decode heterogeneous scene deviation features and generate multimodal trajectory predictions. Extensive experiments demonstrate that CDKFormer achieves state-of-the-art performance, significantly enhancing prediction accuracy and robustness for long-tailed trajectories compared to existing methods, thus advancing the reliability of AVs in complex real-world environments."
  },
  {
    "title": "AI Agents: Evolution, Architecture, and Real-World Applications",
    "url": "http://arxiv.org/abs/2503.12687v1",
    "arxiv_id": "2503.12687v1",
    "authors": [
      "Naveen Krishnan"
    ],
    "published": "2025-03-16T23:07:48+00:00",
    "summary": "This paper examines the evolution, architecture, and practical applications of AI agents from their early, rule-based incarnations to modern sophisticated systems that integrate large language models with dedicated modules for perception, planning, and tool use. Emphasizing both theoretical foundations and real-world deployments, the paper reviews key agent paradigms, discusses limitations of current evaluation benchmarks, and proposes a holistic evaluation framework that balances task effectiveness, efficiency, robustness, and safety. Applications across enterprise, personal assistance, and specialized domains are analyzed, with insights into future research directions for more resilient and adaptive AI agent systems."
  },
  {
    "title": "What is unethical about software? User perceptions in the Netherlands",
    "url": "http://arxiv.org/abs/2503.12640v1",
    "arxiv_id": "2503.12640v1",
    "authors": [
      "Yagil Elias",
      "Tom P. Humbert",
      "Lauren Olson",
      "Emitz\u00e1 Guzm\u00e1n"
    ],
    "published": "2025-03-16T20:29:25+00:00",
    "summary": "Software has the potential to improve lives. Yet, unethical and uninformed software practices are at the root of an increasing number of ethical concerns. Despite its pervasiveness, few research has analyzed end-users perspectives on the ethical issues of the software they use. We address this gap, and investigate end-user's ethical concerns in software through 19 semi-structured interviews with residents of the Netherlands. We ask a diverse group of users about their ethical concerns when using everyday software applications. We investigate the underlying reasons for their concerns and what solutions they propose to eliminate them. We find that our participants actively worry about privacy, transparency, manipulation, safety and inappropriate content; with privacy and manipulation often being at the center of their worries. Our participants demand software solutions to improve information clarity in applications and provide more control over the user experience. They further expect larger systematic changes within software practices and government regulation."
  },
  {
    "title": "Negotiative Alignment: Embracing Disagreement to Achieve Fairer Outcomes -- Insights from Urban Studies",
    "url": "http://arxiv.org/abs/2503.12613v1",
    "arxiv_id": "2503.12613v1",
    "authors": [
      "Rashid Mushkani",
      "Hugo Berard",
      "Shin Koseki"
    ],
    "published": "2025-03-16T18:55:54+00:00",
    "summary": "Cities are not monolithic; they are arenas of negotiation among groups that hold varying needs, values, and experiences. Conventional methods of urban assessment -- from standardized surveys to AI-driven evaluations -- frequently rely on a single consensus metric (e.g., an average measure of inclusivity or safety). Although such aggregations simplify design decisions, they risk obscuring the distinct perspectives of marginalized populations. In this paper, we present findings from a community-centered study in Montreal involving 35 residents with diverse demographic and social identities, particularly wheelchair users, seniors, and LGBTQIA2+ individuals. Using rating and ranking tasks on 20 urban sites, we observe that disagreements are systematic rather than random, reflecting structural inequalities, differing cultural values, and personal experiences of safety and accessibility.   Based on these empirical insights, we propose negotiative alignment, an AI framework that treats disagreement as an essential input to be preserved, analyzed, and addressed. Negotiative alignment builds on pluralistic models by dynamically updating stakeholder preferences through multi-agent negotiation mechanisms, ensuring no single perspective is marginalized. We outline how this framework can be integrated into urban analytics -- and other decision-making contexts -- to retain minority viewpoints, adapt to changing stakeholder concerns, and enhance fairness and accountability. The study demonstrates that preserving and engaging with disagreement, rather than striving for an artificial consensus, can produce more equitable and responsive AI-driven outcomes in urban design."
  },
  {
    "title": "Point Cloud Based Scene Segmentation: A Survey",
    "url": "http://arxiv.org/abs/2503.12595v1",
    "arxiv_id": "2503.12595v1",
    "authors": [
      "Dan Halperin",
      "Niklas Eisl"
    ],
    "published": "2025-03-16T18:02:41+00:00",
    "summary": "Autonomous driving is a safety-critical application, and it is therefore a top priority that the accompanying assistance systems are able to provide precise information about the surrounding environment of the vehicle. Tasks such as 3D Object Detection deliver an insufficiently detailed understanding of the surrounding scene because they only predict a bounding box for foreground objects. In contrast, 3D Semantic Segmentation provides richer and denser information about the environment by assigning a label to each individual point, which is of paramount importance for autonomous driving tasks, such as navigation or lane changes. To inspire future research, in this review paper, we provide a comprehensive overview of the current state-of-the-art methods in the field of Point Cloud Semantic Segmentation for autonomous driving. We categorize the approaches into projection-based, 3D-based and hybrid methods. Moreover, we discuss the most important and commonly used datasets for this task and also emphasize the importance of synthetic data to support research when real-world data is limited. We further present the results of the different methods and compare them with respect to their segmentation accuracy and efficiency."
  },
  {
    "title": "Automotive Battery Pack Standards and Design Characteristics: A Review",
    "url": "http://arxiv.org/abs/2503.12566v1",
    "arxiv_id": "2503.12566v1",
    "authors": [
      "Saeid Haghbin",
      "Morteza Rezaei Larijani",
      "MohammadReza Zolghadri",
      "Shahin Hedayati Kia"
    ],
    "published": "2025-03-16T16:42:26+00:00",
    "summary": "The latest status and near-future trends of automotive battery packs are presented and discussed, with a focus on automakers. Desired pack specifications, aligned with regulatory standards, are outlined from an automaker's perspective. In response to these specifications, high-level solutions are proposed to converge toward a standard architecture for passenger cars Key aspects such as electrical performance, safety, mechanical integrity, reliability, environmental conditions, diagnostics, and practical considerations are examined. Furthermore, near-future developments and emerging applications, including battery use in airplanes, are discussed."
  },
  {
    "title": "Automotive Battery Pack Standards and Design Characteristics: A Review",
    "url": "http://arxiv.org/abs/2503.12566v2",
    "arxiv_id": "2503.12566v2",
    "authors": [
      "Saeid Haghbin",
      "Morteza Rezaei Larijani",
      "MohammadReza Zolghadri",
      "Shahin Hedayati Kia"
    ],
    "published": "2025-03-16T16:42:26+00:00",
    "summary": "This paper outlines the existing situation and future trends related to automobile battery packs, specifically from the automobile manufacturer's point of view. It formulates the specifications required for such packs to adhere to prevailing regulatory schemes and examines top-level solutions to target a uniform architecture for passenger cars. Key elements such as electrical performance, safety, mechanical integrity, reliability, environmental issues, diagnostics, and real-world implications have been extensively examined. This paper draws attention to the industry trend of shifting to high-voltage battery architectures to enable ultra-fast charging above 350 kW, reducing the charging time to less than 20 minutes. Technological advancements in energy density and battery pack capacities are poised to take electric vehicle ranges over 1000 km from a single charge. This study also examines developments in artificial intelligence-improved battery management systems, enhanced safety, mechanical integrity, reliability, diagnostics, and practical considerations. Furthermore, future developments, such as the incorporation of batteries in aviation and other new uses, are investigated to provide insight into the future generation of economically viable, secure, and high-performance battery systems."
  },
  {
    "title": "Polytope Volume Monitoring Problem: Formulation and Solution via Parametric Linear Program Based Control Barrier Function",
    "url": "http://arxiv.org/abs/2503.12546v1",
    "arxiv_id": "2503.12546v1",
    "authors": [
      "Shizhen Wu",
      "Jinyang Dong",
      "Xu Fang",
      "Ning Sun",
      "Yongchun Fang"
    ],
    "published": "2025-03-16T15:27:22+00:00",
    "summary": "Motivated by the latest research on feasible space monitoring of multiple control barrier functions (CBFs) as well as polytopic collision avoidance, this paper studies the Polytope Volume Monitoring (PVM) problem, whose goal is to design a control law for inputs of nonlinear systems to prevent the volume of some state-dependent polytope from decreasing to zero. Recent studies have explored the idea of applying Chebyshev ball method in optimization theory to solve the case study of PVM; however, the underlying difficulties caused by nonsmoothness have not been addressed. This paper continues the study on this topic, where our main contribution is to establish the relationship between nonsmooth CBF and parametric optimization theory through directional derivatives for the first time, so as to solve PVM problems more conveniently. In detail, inspired by Chebyshev ball approach, a parametric linear program (PLP) based nonsmooth barrier function candidate is established for PVM, and then, sufficient conditions for it to be a nonsmooth CBF are proposed, based on which a quadratic program (QP) based safety filter with guaranteed feasibility is proposed to address PVM problems. Finally, a numerical simulation example is given to show the efficiency of the proposed safety filter."
  },
  {
    "title": "LLM-Driven Multi-step Translation from C to Rust using Static Analysis",
    "url": "http://arxiv.org/abs/2503.12511v1",
    "arxiv_id": "2503.12511v1",
    "authors": [
      "Tianyang Zhou",
      "Haowen Lin",
      "Somesh Jha",
      "Mihai Christodorescu",
      "Kirill Levchenko",
      "Varun Chandrasekaran"
    ],
    "published": "2025-03-16T14:05:26+00:00",
    "summary": "Translating software written in legacy languages to modern languages, such as C to Rust, has significant benefits in improving memory safety while maintaining high performance. However, manual translation is cumbersome, error-prone, and produces unidiomatic code. Large language models (LLMs) have demonstrated promise in producing idiomatic translations, but offer no correctness guarantees as they lack the ability to capture all the semantics differences between the source and target languages. To resolve this issue, we propose SACTOR, an LLM-driven C-to-Rust zero-shot translation tool using a two-step translation methodology: an \"unidiomatic\" step to translate C into Rust while preserving semantics, and an \"idiomatic\" step to refine the code to follow Rust's semantic standards. SACTOR utilizes information provided by static analysis of the source C program to address challenges such as pointer semantics and dependency resolution. To validate the correctness of the translated result from each step, we use end-to-end testing via the foreign function interface to embed our translated code segment into the original code. We evaluate the translation of 200 programs from two datasets and two case studies, comparing the performance of GPT-4o, Claude 3.5 Sonnet, Gemini 2.0 Flash, Llama 3.3 70B and DeepSeek-R1 in SACTOR. Our results demonstrate that SACTOR achieves high correctness and improved idiomaticity, with the best-performing model (DeepSeek-R1) reaching 93% and (GPT-4o, Claude 3.5, DeepSeek-R1) reaching 84% correctness (on each dataset, respectively), while producing more natural and Rust-compliant translations compared to existing methods."
  },
  {
    "title": "LLM-Driven Multi-step Translation from C to Rust using Static Analysis",
    "url": "http://arxiv.org/abs/2503.12511v2",
    "arxiv_id": "2503.12511v2",
    "authors": [
      "Tianyang Zhou",
      "Haowen Lin",
      "Somesh Jha",
      "Mihai Christodorescu",
      "Kirill Levchenko",
      "Varun Chandrasekaran"
    ],
    "published": "2025-03-16T14:05:26+00:00",
    "summary": "Translating software written in legacy languages to modern languages, such as C to Rust, has significant benefits in improving memory safety while maintaining high performance. However, manual translation is cumbersome, error-prone, and produces unidiomatic code. Large language models (LLMs) have demonstrated promise in producing idiomatic translations, but offer no correctness guarantees as they lack the ability to capture all the semantics differences between the source and target languages. To resolve this issue, we propose SACTOR, an LLM-driven C-to-Rust zero-shot translation tool using a two-step translation methodology: an \"unidiomatic\" step to translate C into Rust while preserving semantics, and an \"idiomatic\" step to refine the code to follow Rust's semantic standards. SACTOR utilizes information provided by static analysis of the source C program to address challenges such as pointer semantics and dependency resolution. To validate the correctness of the translated result from each step, we use end-to-end testing via the foreign function interface to embed our translated code segment into the original code. We evaluate the translation of 200 programs from two datasets and two case studies, comparing the performance of GPT-4o, Claude 3.5 Sonnet, Gemini 2.0 Flash, Llama 3.3 70B and DeepSeek-R1 in SACTOR. Our results demonstrate that SACTOR achieves high correctness and improved idiomaticity, with the best-performing model (DeepSeek-R1) reaching 93% and (GPT-4o, Claude 3.5, DeepSeek-R1) reaching 84% correctness (on each dataset, respectively), while producing more natural and Rust-compliant translations compared to existing methods."
  },
  {
    "title": "Modularization is Better: Effective Code Generation with Modular Prompting",
    "url": "http://arxiv.org/abs/2503.12483v1",
    "arxiv_id": "2503.12483v1",
    "authors": [
      "Ruwei Pan",
      "Hongyu Zhang"
    ],
    "published": "2025-03-16T12:23:23+00:00",
    "summary": "Large Language Models are transforming software development by automatically generating code. Current prompting techniques such as Chain-of-Thought (CoT) suggest tasks step by step and the reasoning process follows a linear structure, which hampers the understanding of complex programming problems, particularly those requiring hierarchical solutions. Inspired by the principle of modularization in software development, in this work, we propose a novel prompting technique, called MoT, to enhance the code generation performance of LLMs. At first, MoT exploits modularization principles to decompose complex programming problems into smaller, independent reasoning steps, enabling a more structured and interpretable problem-solving process. This hierarchical structure improves the LLM's ability to comprehend complex programming problems. Then, it structures the reasoning process using an MLR Graph (Multi-Level Reasoning Graph), which hierarchically organizes reasoning steps. This approach enhances modular understanding and ensures better alignment between reasoning steps and the generated code, significantly improving code generation performance. Our experiments on two advanced LLMs (GPT-4o-mini and DeepSeek-R1), comparing MoT to six baseline prompting techniques across six widely used datasets, HumanEval, HumanEval-ET, HumanEval+, MBPP, MBPP-ET, and MBPP+, demonstrate that MoT significantly outperforms existing baselines (e.g., CoT and SCoT), achieving Pass@1 scores ranging from 58.1% to 95.1%. The experimental results confirm that MoT significantly enhances the performance of LLM-based code generation."
  },
  {
    "title": "ResLPR: A LiDAR Data Restoration Network and Benchmark for Robust Place Recognition Against Weather Corruptions",
    "url": "http://arxiv.org/abs/2503.12350v1",
    "arxiv_id": "2503.12350v1",
    "authors": [
      "Wenqing Kuang",
      "Xiongwei Zhao",
      "Yehui Shen",
      "Congcong Wen",
      "Huimin Lu",
      "Zongtan Zhou",
      "Xieyuanli Chen"
    ],
    "published": "2025-03-16T04:14:20+00:00",
    "summary": "LiDAR-based place recognition (LPR) is a key component for autonomous driving, and its resilience to environmental corruption is critical for safety in high-stakes applications. While state-of-the-art (SOTA) LPR methods perform well in clean weather, they still struggle with weather-induced corruption commonly encountered in driving scenarios. To tackle this, we propose ResLPRNet, a novel LiDAR data restoration network that largely enhances LPR performance under adverse weather by restoring corrupted LiDAR scans using a wavelet transform-based network. ResLPRNet is efficient, lightweight and can be integrated plug-and-play with pretrained LPR models without substantial additional computational cost. Given the lack of LPR datasets under adverse weather, we introduce ResLPR, a novel benchmark that examines SOTA LPR methods under a wide range of LiDAR distortions induced by severe snow, fog, and rain conditions. Experiments on our proposed WeatherKITTI and WeatherNCLT datasets demonstrate the resilience and notable gains achieved by using our restoration method with multiple LPR approaches in challenging weather scenarios. Our code and benchmark are publicly available here: https://github.com/nubot-nudt/ResLPR."
  },
  {
    "title": "Augmented Adversarial Trigger Learning",
    "url": "http://arxiv.org/abs/2503.12339v1",
    "arxiv_id": "2503.12339v1",
    "authors": [
      "Zhe Wang",
      "Yanjun Qi"
    ],
    "published": "2025-03-16T03:20:52+00:00",
    "summary": "Gradient optimization-based adversarial attack methods automate the learning of adversarial triggers to generate jailbreak prompts or leak system prompts. In this work, we take a closer look at the optimization objective of adversarial trigger learning and propose ATLA: Adversarial Trigger Learning with Augmented objectives. ATLA improves the negative log-likelihood loss used by previous studies into a weighted loss formulation that encourages the learned adversarial triggers to optimize more towards response format tokens. This enables ATLA to learn an adversarial trigger from just one query-response pair and the learned trigger generalizes well to other similar queries. We further design a variation to augment trigger optimization with an auxiliary loss that suppresses evasive responses. We showcase how to use ATLA to learn adversarial suffixes jailbreaking LLMs and to extract hidden system prompts. Empirically we demonstrate that ATLA consistently outperforms current state-of-the-art techniques, achieving nearly 100% success in attacking while requiring 80% fewer queries. ATLA learned jailbreak suffixes demonstrate high generalization to unseen queries and transfer well to new LLMs."
  },
  {
    "title": "GameChat: Multi-LLM Dialogue for Safe, Agile, and Socially Optimal Multi-Agent Navigation in Constrained Environments",
    "url": "http://arxiv.org/abs/2503.12333v1",
    "arxiv_id": "2503.12333v1",
    "authors": [
      "Vagul Mahadevan",
      "Shangtong Zhang",
      "Rohan Chandra"
    ],
    "published": "2025-03-16T03:02:40+00:00",
    "summary": "Safe, agile, and socially compliant multi-robot navigation in cluttered and constrained environments remains a critical challenge. This is especially difficult with self-interested agents in decentralized settings, where there is no central authority to resolve conflicts induced by spatial symmetry. We address this challenge by proposing a novel approach, GameChat, which facilitates safe, agile, and deadlock-free navigation for both cooperative and self-interested agents. Key to our approach is the use of natural language communication to resolve conflicts, enabling agents to prioritize more urgent tasks and break spatial symmetry in a socially optimal manner. Our algorithm ensures subgame perfect equilibrium, preventing agents from deviating from agreed-upon behaviors and supporting cooperation. Furthermore, we guarantee safety through control barrier functions and preserve agility by minimizing disruptions to agents' planned trajectories. We evaluate GameChat in simulated environments with doorways and intersections. The results show that even in the worst case, GameChat reduces the time for all agents to reach their goals by over 35% from a naive baseline and by over 20% from SMG-CBF in the intersection scenario, while doubling the rate of ensuring the agent with a higher priority task reaches the goal first, from 50% (equivalent to random chance) to a 100% perfect performance at maximizing social welfare."
  },
  {
    "title": "Integrating Chain-of-Thought and Retrieval Augmented Generation Enhances Rare Disease Diagnosis from Clinical Notes",
    "url": "http://arxiv.org/abs/2503.12286v1",
    "arxiv_id": "2503.12286v1",
    "authors": [
      "Da Wu",
      "Zhanliang Wang",
      "Quan Nguyen",
      "Kai Wang"
    ],
    "published": "2025-03-15T22:57:31+00:00",
    "summary": "Background: Several studies show that large language models (LLMs) struggle with phenotype-driven gene prioritization for rare diseases. These studies typically use Human Phenotype Ontology (HPO) terms to prompt foundation models like GPT and LLaMA to predict candidate genes. However, in real-world settings, foundation models are not optimized for domain-specific tasks like clinical diagnosis, yet inputs are unstructured clinical notes rather than standardized terms. How LLMs can be instructed to predict candidate genes or disease diagnosis from unstructured clinical notes remains a major challenge. Methods: We introduce RAG-driven CoT and CoT-driven RAG, two methods that combine Chain-of-Thought (CoT) and Retrieval Augmented Generation (RAG) to analyze clinical notes. A five-question CoT protocol mimics expert reasoning, while RAG retrieves data from sources like HPO and OMIM (Online Mendelian Inheritance in Man). We evaluated these approaches on rare disease datasets, including 5,980 Phenopacket-derived notes, 255 literature-based narratives, and 220 in-house clinical notes from Childrens Hospital of Philadelphia. Results: We found that recent foundations models, including Llama 3.3-70B-Instruct and DeepSeek-R1-Distill-Llama-70B, outperformed earlier versions such as Llama 2 and GPT-3.5. We also showed that RAG-driven CoT and CoT-driven RAG both outperform foundation models in candidate gene prioritization from clinical notes; in particular, both methods with DeepSeek backbone resulted in a top-10 gene accuracy of over 40% on Phenopacket-derived clinical notes. RAG-driven CoT works better for high-quality notes, where early retrieval can anchor the subsequent reasoning steps in domain-specific evidence, while CoT-driven RAG has advantage when processing lengthy and noisy notes."
  },
  {
    "title": "Reflect-DiT: Inference-Time Scaling for Text-to-Image Diffusion Transformers via In-Context Reflection",
    "url": "http://arxiv.org/abs/2503.12271v1",
    "arxiv_id": "2503.12271v1",
    "authors": [
      "Shufan Li",
      "Konstantinos Kallidromitis",
      "Akash Gokul",
      "Arsh Koneru",
      "Yusuke Kato",
      "Kazuki Kozuka",
      "Aditya Grover"
    ],
    "published": "2025-03-15T21:58:12+00:00",
    "summary": "The predominant approach to advancing text-to-image generation has been training-time scaling, where larger models are trained on more data using greater computational resources. While effective, this approach is computationally expensive, leading to growing interest in inference-time scaling to improve performance. Currently, inference-time scaling for text-to-image diffusion models is largely limited to best-of-N sampling, where multiple images are generated per prompt and a selection model chooses the best output. Inspired by the recent success of reasoning models like DeepSeek-R1 in the language domain, we introduce an alternative to naive best-of-N sampling by equipping text-to-image Diffusion Transformers with in-context reflection capabilities. We propose Reflect-DiT, a method that enables Diffusion Transformers to refine their generations using in-context examples of previously generated images alongside textual feedback describing necessary improvements. Instead of passively relying on random sampling and hoping for a better result in a future generation, Reflect-DiT explicitly tailors its generations to address specific aspects requiring enhancement. Experimental results demonstrate that Reflect-DiT improves performance on the GenEval benchmark (+0.19) using SANA-1.0-1.6B as a base model. Additionally, it achieves a new state-of-the-art score of 0.81 on GenEval while generating only 20 samples per prompt, surpassing the previous best score of 0.80, which was obtained using a significantly larger model (SANA-1.5-4.8B) with 2048 samples under the best-of-N approach."
  },
  {
    "title": "Indoor Positioning for Public Safety: Role of UAVs, LEOs, and Propagation-Aware Techniques",
    "url": "http://arxiv.org/abs/2503.12264v1",
    "arxiv_id": "2503.12264v1",
    "authors": [
      "Gaurav Duggal",
      "Harish K. Dureppagari",
      "Harpreet S. Dhillon",
      "Jeffrey H. Reed",
      "R. Michael Buehrer"
    ],
    "published": "2025-03-15T21:17:57+00:00",
    "summary": "Effective indoor positioning is critical for public safety, enabling first responders to locate at-risk individuals accurately during emergency scenarios. However, traditional Global Navigation Satellite Systems (GNSS) often perform poorly indoors due to poor coverage and non-line-of-sight (NLOS) conditions. Moreover, relying on fixed cellular infrastructure, such as terrestrial networks (TNs), may not be feasible, as indoor signal coverage from a sufficient number of base stations or WiFi access points cannot be guaranteed for accurate positioning. In this paper, we propose a rapidly deployable indoor positioning system (IPS) leveraging mobile anchors, including uncrewed aerial vehicles (UAVs) and Low-Earth-Orbit (LEO) satellites, and discuss the role of GNSS and LEOs in localizing the mobile anchors. Additionally, we discuss the role of sidelink-based positioning, which is introduced in 3rd Generation Partnership Project (3GPP) Release 18, in enabling public safety systems. By examining outdoor-to-indoor (O2I) signal propagation, particularly diffraction-based approaches, we highlight how propagation-aware positioning methods can outperform conventional strategies that disregard propagation mechanism information. The study highlights how emerging 5G Advanced and Non-Terrestrial Networks (NTN) features offer new avenues to improve positioning in challenging indoor environments, ultimately paving the way for cost-effective and resilient IPS solutions tailored to public safety applications."
  },
  {
    "title": "GenOSIL: Generalized Optimal and Safe Robot Control using Parameter-Conditioned Imitation Learning",
    "url": "http://arxiv.org/abs/2503.12243v1",
    "arxiv_id": "2503.12243v1",
    "authors": [
      "Mumuksh Tayal",
      "Manan Tayal",
      "Ravi Prakash"
    ],
    "published": "2025-03-15T19:52:16+00:00",
    "summary": "Ensuring safe and generalizable control remains a fundamental challenge in robotics, particularly when deploying imitation learning in dynamic environments. Traditional behavior cloning (BC) struggles to generalize beyond its training distribution, as it lacks an understanding of the safety critical reasoning behind expert demonstrations. To address this limitation, we propose GenOSIL, a novel imitation learning framework that explicitly incorporates environment parameters into policy learning via a structured latent representation. Unlike conventional methods that treat the environment as a black box, GenOSIL employs a variational autoencoder (VAE) to encode measurable safety parameters such as obstacle position, velocity, and geometry into a latent space that captures intrinsic correlations between expert behavior and environmental constraints. This enables the policy to infer the rationale behind expert trajectories rather than merely replicating them. We validate our approach on two robotic platforms an autonomous ground vehicle and a Franka Emika Panda manipulator demonstrating superior safety and goal reaching performance compared to baseline methods. The simulation and hardware videos can be viewed on the project webpage: https://mumukshtayal.github.io/GenOSIL/."
  },
  {
    "title": "A Novel Double Pruning method for Imbalanced Data using Information Entropy and Roulette Wheel Selection for Breast Cancer Diagnosis",
    "url": "http://arxiv.org/abs/2503.12239v1",
    "arxiv_id": "2503.12239v1",
    "authors": [
      "Soufiane Bacha",
      "Huansheng Ning",
      "Belarbi Mostefa",
      "Doreen Sebastian Sarwatt",
      "Sahraoui Dhelim"
    ],
    "published": "2025-03-15T19:34:15+00:00",
    "summary": "Accurate illness diagnosis is vital for effective treatment and patient safety. Machine learning models are widely used for cancer diagnosis based on historical medical data. However, data imbalance remains a major challenge, leading to hindering classifier performance and reliability. The SMOTEBoost method addresses this issue by generating synthetic data to balance the dataset, but it may overlook crucial overlapping regions near the decision boundary and can produce noisy samples. This paper proposes RE-SMOTEBoost, an enhanced version of SMOTEBoost, designed to overcome these limitations. Firstly, RE-SMOTEBoost focuses on generating synthetic samples in overlapping regions to better capture the decision boundary using roulette wheel selection. Secondly, it incorporates a filtering mechanism based on information entropy to reduce noise, and borderline cases and improve the quality of generated data. Thirdly, we introduce a double regularization penalty to control the synthetic samples proximity to the decision boundary and avoid class overlap. These enhancements enable higher-quality oversampling of the minority class, resulting in a more balanced and effective training dataset. The proposed method outperforms existing state-of-the-art techniques when evaluated on imbalanced datasets. Compared to the top-performing sampling algorithms, RE-SMOTEBoost demonstrates a notable improvement of 3.22\\% in accuracy and a variance reduction of 88.8\\%. These results indicate that the proposed model offers a solid solution for medical settings, effectively overcoming data scarcity and severe imbalance caused by limited samples, data collection difficulties, and privacy constraints."
  },
  {
    "title": "Gun Detection Using Combined Human Pose and Weapon Appearance",
    "url": "http://arxiv.org/abs/2503.12215v1",
    "arxiv_id": "2503.12215v1",
    "authors": [
      "Amulya Reddy Maligireddy",
      "Manohar Reddy Uppula",
      "Nidhi Rastogi",
      "Yaswanth Reddy Parla"
    ],
    "published": "2025-03-15T17:57:35+00:00",
    "summary": "The increasing frequency of firearm-related incidents has necessitated advancements in security and surveillance systems, particularly in firearm detection within public spaces. Traditional gun detection methods rely on manual inspections and continuous human monitoring of CCTV footage, which are labor-intensive and prone to high false positive and negative rates. To address these limitations, we propose a novel approach that integrates human pose estimation with weapon appearance recognition using deep learning techniques. Unlike prior studies that focus on either body pose estimation or firearm detection in isolation, our method jointly analyzes posture and weapon presence to enhance detection accuracy in real-world, dynamic environments. To train our model, we curated a diverse dataset comprising images from open-source repositories such as IMFDB and Monash Guns, supplemented with AI-generated and manually collected images from web sources. This dataset ensures robust generalization and realistic performance evaluation under various surveillance conditions. Our research aims to improve the precision and reliability of firearm detection systems, contributing to enhanced public safety and threat mitigation in high-risk areas."
  },
  {
    "title": "Hyperbolic Safety-Aware Vision-Language Models",
    "url": "http://arxiv.org/abs/2503.12127v1",
    "arxiv_id": "2503.12127v1",
    "authors": [
      "Tobia Poppi",
      "Tejaswi Kasarla",
      "Pascal Mettes",
      "Lorenzo Baraldi",
      "Rita Cucchiara"
    ],
    "published": "2025-03-15T13:18:04+00:00",
    "summary": "Addressing the retrieval of unsafe content from vision-language models such as CLIP is an important step towards real-world integration. Current efforts have relied on unlearning techniques that try to erase the model's knowledge of unsafe concepts. While effective in reducing unwanted outputs, unlearning limits the model's capacity to discern between safe and unsafe content. In this work, we introduce a novel approach that shifts from unlearning to an awareness paradigm by leveraging the inherent hierarchical properties of the hyperbolic space. We propose to encode safe and unsafe content as an entailment hierarchy, where both are placed in different regions of hyperbolic space. Our HySAC, Hyperbolic Safety-Aware CLIP, employs entailment loss functions to model the hierarchical and asymmetrical relations between safe and unsafe image-text pairs. This modelling, ineffective in standard vision-language models due to their reliance on Euclidean embeddings, endows the model with awareness of unsafe content, enabling it to serve as both a multimodal unsafe classifier and a flexible content retriever, with the option to dynamically redirect unsafe queries toward safer alternatives or retain the original output. Extensive experiments show that our approach not only enhances safety recognition but also establishes a more adaptable and interpretable framework for content moderation in vision-language models. Our source code is available at https://github.com/aimagelab/HySAC."
  },
  {
    "title": "Towards Vision Zero: The Accid3nD Dataset",
    "url": "http://arxiv.org/abs/2503.12095v1",
    "arxiv_id": "2503.12095v1",
    "authors": [
      "Walter Zimmer",
      "Ross Greer",
      "Daniel Lehmberg",
      "Marc Pavel",
      "Holger Caesar",
      "Xingcheng Zhou",
      "Ahmed Ghita",
      "Mohan Trivedi",
      "Rui Song",
      "Hu Cao",
      "Akshay Gopalkrishnan",
      "Alois C. Knoll"
    ],
    "published": "2025-03-15T11:42:16+00:00",
    "summary": "Even though a significant amount of work has been done to increase the safety of transportation networks, accidents still occur regularly. They must be understood as unavoidable and sporadic outcomes of traffic networks. No public dataset contains 3D annotations of real-world accidents recorded from roadside sensors. We present the Accid3nD dataset, a collection of real-world highway accidents in different weather and lighting conditions. It contains vehicle crashes at high-speed driving with 2,634,233 labeled 2D bounding boxes, instance masks, and 3D bounding boxes with track IDs. In total, the dataset contains 111,945 labeled frames recorded from four roadside cameras and LiDARs at 25 Hz. The dataset contains six object classes and is provided in the OpenLABEL format. We propose an accident detection model that combines a rule-based approach with a learning-based one. Experiments and ablation studies on our dataset show the robustness of our proposed method. The dataset, model, and code are available on our website: https://accident-dataset.github.io."
  },
  {
    "title": "Automating the loop in traffic incident management on highway",
    "url": "http://arxiv.org/abs/2503.12085v1",
    "arxiv_id": "2503.12085v1",
    "authors": [
      "Matteo Cercola",
      "Nicola Gatti",
      "Pedro Huertas Leyva",
      "Benedetto Carambia",
      "Simone Formentin"
    ],
    "published": "2025-03-15T11:22:13+00:00",
    "summary": "Effective traffic incident management is essential for ensuring safety, minimizing congestion, and reducing response times in emergency situations. Traditional highway incident management relies heavily on radio room operators, who must make rapid, informed decisions in high-stakes environments. This paper proposes an innovative solution to support and enhance these decisions by integrating Large Language Models (LLMs) into a decision-support system for traffic incident management. We introduce two approaches: (1) an LLM + Optimization hybrid that leverages both the flexibility of natural language interaction and the robustness of optimization techniques, and (2) a Full LLM approach that autonomously generates decisions using only LLM capabilities. We tested our solutions using historical event data from Autostrade per l'Italia. Experimental results indicate that while both approaches show promise, the LLM + Optimization solution demonstrates superior reliability, making it particularly suited to critical applications where consistency and accuracy are paramount. This research highlights the potential for LLMs to transform highway incident management by enabling accessible, data-driven decision-making support."
  },
  {
    "title": "Proof-Driven Clause Learning in Neural Network Verification",
    "url": "http://arxiv.org/abs/2503.12083v1",
    "arxiv_id": "2503.12083v1",
    "authors": [
      "Omri Isac",
      "Idan Refaeli",
      "Haoze Wu",
      "Clark Barrett",
      "Guy Katz"
    ],
    "published": "2025-03-15T11:05:15+00:00",
    "summary": "The widespread adoption of deep neural networks (DNNs) requires efficient techniques for safety verification. Existing methods struggle to scale to real-world DNNs, and tremendous efforts are being put into improving their scalability. In this work, we propose an approach for improving the scalability of DNN verifiers using Conflict-Driven Clause Learning (CDCL) -- an approach that has proven highly successful in SAT and SMT solving. We present a novel algorithm for deriving conflict clauses using UNSAT proofs, and propose several optimizations for expediting it. Our approach allows a modular integration of SAT solvers and DNN verifiers, and we implement it on top of an interface designed for this purpose. The evaluation of our implementation over several benchmarks suggests a 2X--3X improvement over a similar approach, with specific cases outperforming the state of the art."
  },
  {
    "title": "Generative Modeling of Adversarial Lane-Change Scenario",
    "url": "http://arxiv.org/abs/2503.12055v1",
    "arxiv_id": "2503.12055v1",
    "authors": [
      "Chuancheng Zhang",
      "Zhenhao Wang",
      "Jiangcheng Wang",
      "Kun Su",
      "Qiang Lv",
      "Bin Jiang",
      "Kunkun Hao",
      "Wenyu Wang"
    ],
    "published": "2025-03-15T09:05:04+00:00",
    "summary": "Decision-making in long-tail scenarios is crucial to autonomous driving development, with realistic and challenging simulations playing a pivotal role in testing safety-critical situations. However, the current open-source datasets do not systematically include long-tail distributed scenario data, making acquiring such scenarios a formidable task. To address this problem, a data mining framework is proposed, which performs in-depth analysis on two widely-used datasets, NGSIM and INTERACTION, to pinpoint data with hazardous behavioral traits, aiming to bridge the gap in these overlooked scenarios. The approach utilizes Generative Adversarial Imitation Learning (GAIL) based on an enhanced Proximal Policy Optimization (PPO) model, integrated with the vehicle's environmental analysis, to iteratively refine and represent the newly generated vehicle trajectory. Innovatively, the solution optimizes the generation of adversarial scenario data from the perspectives of sensitivity and reasonable adversarial. It is demonstrated through experiments that, compared to the unfiltered data and baseline models, the approach exhibits more adversarial yet natural behavior regarding collision rate, acceleration, and lane changes, thereby validating its suitability for generating scenario data and providing constructive insights for the development of future scenarios and subsequent decision training."
  },
  {
    "title": "TLUE: A Tibetan Language Understanding Evaluation Benchmark",
    "url": "http://arxiv.org/abs/2503.12051v1",
    "arxiv_id": "2503.12051v1",
    "authors": [
      "Fan Gao",
      "Cheng Huang",
      "Nyima Tashi",
      "Xiangxiang Wang",
      "Thupten Tsering",
      "Ban Ma-bao",
      "Renzeg Duojie",
      "Gadeng Luosang",
      "Rinchen Dongrub",
      "Dorje Tashi",
      "Xiao Feng",
      "Yongbin Yu"
    ],
    "published": "2025-03-15T08:54:25+00:00",
    "summary": "Large language models (LLMs) have made tremendous progress in recent years, but low-resource languages, such as Tibetan, remain significantly underrepresented in their evaluation. Despite Tibetan being spoken by over seven million people, it has largely been neglected in the development and assessment of LLMs. To address this gap, we present TLUE (A Tibetan Language Understanding Evaluation Benchmark), the first large-scale benchmark for assessing LLMs' capabilities in Tibetan. TLUE comprises two major components: (1) a comprehensive multi-task understanding benchmark spanning 5 domains and 67 subdomains, and (2) a safety benchmark covering 7 subdomains. We evaluate a diverse set of state-of-the-art LLMs. Experimental results demonstrate that most LLMs perform below the random baseline, highlighting the considerable challenges LLMs face in processing Tibetan, a low-resource language. TLUE provides an essential foundation for driving future research and progress in Tibetan language understanding and underscores the need for greater inclusivity in LLM development."
  },
  {
    "title": "An LLM-Integrated Framework for Completion, Management, and Tracing of STPA",
    "url": "http://arxiv.org/abs/2503.12043v1",
    "arxiv_id": "2503.12043v1",
    "authors": [
      "Ali Raeisdanaei",
      "Juho Kim",
      "Michael Liao",
      "Sparsh Kochhar"
    ],
    "published": "2025-03-15T08:31:13+00:00",
    "summary": "In many safety-critical engineering domains, hazard analysis techniques are an essential part of requirement elicitation. Of the methods proposed for this task, STPA (System-Theoretic Process Analysis) represents a relatively recent development in the field. The completion, management, and traceability of this hazard analysis technique present a time-consuming challenge to the requirements and safety engineers involved. In this paper, we introduce a free, open-source software framework to build STPA models with several automated workflows powered by large language models (LLMs). In past works, LLMs have been successfully integrated into a myriad of workflows across various fields. Here, we demonstrate that LLMs can be used to complete tasks associated with STPA with a high degree of accuracy, saving the time and effort of the human engineers involved. We experimentally validate our method on real-world STPA models built by requirement engineers and researchers. The source code of our software framework is available at the following link: https://github.com/blueskysolarracing/stpa."
  },
  {
    "title": "Safety for Time-Varying Parameterized Sets Using Control Barrier Function Methods",
    "url": "http://arxiv.org/abs/2503.12003v1",
    "arxiv_id": "2503.12003v1",
    "authors": [
      "James Usevitch",
      "Jackson Sahleen"
    ],
    "published": "2025-03-15T05:53:21+00:00",
    "summary": "A fundamental and classical problem in mobile autonomous systems is maintaining the safety of autonomous agents during deployment. Prior literature has presented techniques using control barrier functions (CBFs) to achieve this goal. These prior techniques utilize CBFs to keep an isolated point in state space away from the unsafe set. However, various situations require a non-singleton set of states to be kept away from an unsafe set. Prior literature has addressed this problem using nonsmooth CBF methods, but no prior work has solved this problem using only \"smooth\" CBF methods. This paper addresses this gap by presenting a novel method of applying CBF methods to non-singleton parameterized convex sets. The method ensures differentiability of the squared distance function between ego and obstacle sets by leveraging a form of the log-sum-exp function to form strictly convex, arbitrarily tight overapproximations of these sets. Safety-preserving control inputs can be computed via convex optimization formulations. The efficacy of our results is demonstrated through multi-agent simulations."
  },
  {
    "title": "Impact of Frequency on Diffraction-Aided Wireless Positioning",
    "url": "http://arxiv.org/abs/2503.11993v1",
    "arxiv_id": "2503.11993v1",
    "authors": [
      "Gaurav Duggal",
      "Anand M. Kumar",
      "R. Michael Buehrer",
      "Harpreet S. Dhillon",
      "Nishith Tripathi",
      "Jeffrey H. Reed"
    ],
    "published": "2025-03-15T04:46:48+00:00",
    "summary": "This paper tackles the challenge of accurate positioning in Non-Line-of-Sight (NLoS) environments, with a focus on indoor public safety scenarios where NLoS bias severely impacts localization performance. We explore Diffraction MultiPath Components (MPC) as a critical mechanism for Outdoor-to-Indoor (O2I) signal propagation and its role in positioning. The proposed system comprises outdoor Uncrewed Aerial Vehicle (UAV) transmitters and indoor receivers that require localization. To facilitate diffraction-based positioning, we develop a method to isolate diffraction MPCs at indoor receivers and validate its effectiveness using a ray-tracing-generated dataset, which we have made publicly available. Our evaluation across the FR1, FR2, and FR3 frequency bands within the 5G/6G spectrum confirms the viability of diffraction-based positioning techniques for next-generation wireless networks."
  },
  {
    "title": "SagaLLM: Context Management, Validation, and Transaction Guarantees for Multi-Agent LLM Planning",
    "url": "http://arxiv.org/abs/2503.11951v1",
    "arxiv_id": "2503.11951v1",
    "authors": [
      "Edward Y. Chang"
    ],
    "published": "2025-03-15T01:43:03+00:00",
    "summary": "Recent LLM-based agent frameworks have demonstrated impressive capabilities in task delegation and workflow orchestration, but face significant challenges in maintaining context awareness and ensuring planning consistency. This paper presents SagaLLM, a structured multi-agent framework that addresses four fundamental limitations in current LLM approaches: inadequate self-validation, context narrowing, lacking transaction properties, and insufficient inter-agent coordination. By implementing specialized context management agents and validation protocols, SagaLLM preserves critical constraints and state information throughout complex planning processes, enabling robust and consistent decision-making even during disruptions. We evaluate our approach using selected problems from the REALM benchmark, focusing on sequential and reactive planning scenarios that challenge both context retention and adaptive reasoning. Our experiments with state-of-the-art LLMs, Claude 3.7, DeepSeek R1, GPT-4o, and GPT-o1, demonstrate that while these models exhibit impressive reasoning capabilities, they struggle with maintaining global constraint awareness during complex planning tasks, particularly when adapting to unexpected changes. In contrast, the distributed cognitive architecture of SagaLLM shows significant improvements in planning consistency, constraint enforcement, and adaptation to disruptions in various scenarios."
  },
  {
    "title": "SagaLLM: Context Management, Validation, and Transaction Guarantees for Multi-Agent LLM Planning",
    "url": "http://arxiv.org/abs/2503.11951v2",
    "arxiv_id": "2503.11951v2",
    "authors": [
      "Edward Y. Chang",
      "Longling Geng"
    ],
    "published": "2025-03-15T01:43:03+00:00",
    "summary": "Recent LLM-based agent frameworks have demonstrated impressive capabilities in task delegation and workflow orchestration, but face significant challenges in maintaining context awareness and ensuring planning consistency. This paper presents SagaLLM, a structured multi-agent framework that addresses four fundamental limitations in current LLM approaches: inadequate self-validation, context narrowing, lacking transaction properties, and insufficient inter-agent coordination. By implementing specialized context management agents and validation protocols, SagaLLM preserves critical constraints and state information throughout complex planning processes, enabling robust and consistent decision-making even during disruptions. We evaluate our approach using selected problems from the REALM benchmark, focusing on sequential and reactive planning scenarios that challenge both context retention and adaptive reasoning. Our experiments with state-of-the-art LLMs, Claude 3.7, DeepSeek R1, GPT-4o, and GPT-o1, demonstrate that while these models exhibit impressive reasoning capabilities, they struggle with maintaining global constraint awareness during complex planning tasks, particularly when adapting to unexpected changes. In contrast, the distributed cognitive architecture of SagaLLM shows significant improvements in planning consistency, constraint enforcement, and adaptation to disruptions in various scenarios."
  },
  {
    "title": "Monitoring Reasoning Models for Misbehavior and the Risks of Promoting Obfuscation",
    "url": "http://arxiv.org/abs/2503.11926v1",
    "arxiv_id": "2503.11926v1",
    "authors": [
      "Bowen Baker",
      "Joost Huizinga",
      "Leo Gao",
      "Zehao Dou",
      "Melody Y. Guan",
      "Aleksander Madry",
      "Wojciech Zaremba",
      "Jakub Pachocki",
      "David Farhi"
    ],
    "published": "2025-03-14T23:50:34+00:00",
    "summary": "Mitigating reward hacking--where AI systems misbehave due to flaws or misspecifications in their learning objectives--remains a key challenge in constructing capable and aligned models. We show that we can monitor a frontier reasoning model, such as OpenAI o3-mini, for reward hacking in agentic coding environments by using another LLM that observes the model's chain-of-thought (CoT) reasoning. CoT monitoring can be far more effective than monitoring agent actions and outputs alone, and we further found that a LLM weaker than o3-mini, namely GPT-4o, can effectively monitor a stronger model. Because CoT monitors can be effective at detecting exploits, it is natural to ask whether those exploits can be suppressed by incorporating a CoT monitor directly into the agent's training objective. While we show that integrating CoT monitors into the reinforcement learning reward can indeed produce more capable and more aligned agents in the low optimization regime, we find that with too much optimization, agents learn obfuscated reward hacking, hiding their intent within the CoT while still exhibiting a significant rate of reward hacking. Because it is difficult to tell when CoTs have become obfuscated, it may be necessary to pay a monitorability tax by not applying strong optimization pressures directly to the chain-of-thought, ensuring that CoTs remain monitorable and useful for detecting misaligned behavior."
  },
  {
    "title": "On Regulating Downstream AI Developers",
    "url": "http://arxiv.org/abs/2503.11922v1",
    "arxiv_id": "2503.11922v1",
    "authors": [
      "Sophie Williams",
      "Jonas Schuett",
      "Markus Anderljung"
    ],
    "published": "2025-03-14T23:15:54+00:00",
    "summary": "Foundation models - models trained on broad data that can be adapted to a wide range of downstream tasks - can pose significant risks, ranging from intimate image abuse, cyberattacks, to bioterrorism. To reduce these risks, policymakers are starting to impose obligations on the developers of these models. However, downstream developers - actors who fine-tune or otherwise modify foundational models - can create or amplify risks by improving a model's capabilities or compromising its safety features. This can make rules on upstream developers ineffective. One way to address this issue could be to impose direct obligations on downstream developers. However, since downstream developers are numerous, diverse, and rapidly growing in number, such direct regulation may be both practically challenging and stifling to innovation. A different approach would be to require upstream developers to mitigate downstream modification risks (e.g. by restricting what modifications can be made). Another approach would be to use alternative policy tools (e.g. clarifying how existing tort law applies to downstream developers or issuing voluntary guidance to help mitigate downstream modification risks). We expect that regulation on upstream developers to mitigate downstream modification risks will be necessary. Although further work is needed, regulation of downstream developers may also be warranted where they retain the ability to increase risk to an unacceptable level."
  },
  {
    "title": "Centaur: Robust End-to-End Autonomous Driving with Test-Time Training",
    "url": "http://arxiv.org/abs/2503.11650v1",
    "arxiv_id": "2503.11650v1",
    "authors": [
      "Chonghao Sima",
      "Kashyap Chitta",
      "Zhiding Yu",
      "Shiyi Lan",
      "Ping Luo",
      "Andreas Geiger",
      "Hongyang Li",
      "Jose M. Alvarez"
    ],
    "published": "2025-03-14T17:59:41+00:00",
    "summary": "How can we rely on an end-to-end autonomous vehicle's complex decision-making system during deployment? One common solution is to have a ``fallback layer'' that checks the planned trajectory for rule violations and replaces it with a pre-defined safe action if necessary. Another approach involves adjusting the planner's decisions to minimize a pre-defined ``cost function'' using additional system predictions such as road layouts and detected obstacles. However, these pre-programmed rules or cost functions cannot learn and improve with new training data, often resulting in overly conservative behaviors. In this work, we propose Centaur (Cluster Entropy for Test-time trAining using Uncertainty) which updates a planner's behavior via test-time training, without relying on hand-engineered rules or cost functions. Instead, we measure and minimize the uncertainty in the planner's decisions. For this, we develop a novel uncertainty measure, called Cluster Entropy, which is simple, interpretable, and compatible with state-of-the-art planning algorithms. Using data collected at prior test-time time-steps, we perform an update to the model's parameters using a gradient that minimizes the Cluster Entropy. With only this sole gradient update prior to inference, Centaur exhibits significant improvements, ranking first on the navtest leaderboard with notable gains in safety-critical metrics such as time to collision. To provide detailed insights on a per-scenario basis, we also introduce navsafe, a challenging new benchmark, which highlights previously undiscovered failure modes of driving models."
  },
  {
    "title": "Tit-for-Tat: Safeguarding Large Vision-Language Models Against Jailbreak Attacks via Adversarial Defense",
    "url": "http://arxiv.org/abs/2503.11619v1",
    "arxiv_id": "2503.11619v1",
    "authors": [
      "Shuyang Hao",
      "Yiwei Wang",
      "Bryan Hooi",
      "Ming-Hsuan Yang",
      "Jun Liu",
      "Chengcheng Tang",
      "Zi Huang",
      "Yujun Cai"
    ],
    "published": "2025-03-14T17:39:45+00:00",
    "summary": "Deploying large vision-language models (LVLMs) introduces a unique vulnerability: susceptibility to malicious attacks via visual inputs. However, existing defense methods suffer from two key limitations: (1) They solely focus on textual defenses, fail to directly address threats in the visual domain where attacks originate, and (2) the additional processing steps often incur significant computational overhead or compromise model performance on benign tasks. Building on these insights, we propose ESIII (Embedding Security Instructions Into Images), a novel methodology for transforming the visual space from a source of vulnerability into an active defense mechanism. Initially, we embed security instructions into defensive images through gradient-based optimization, obtaining security instructions in the visual dimension. Subsequently, we integrate security instructions from visual and textual dimensions with the input query. The collaboration between security instructions from different dimensions ensures comprehensive security protection. Extensive experiments demonstrate that our approach effectively fortifies the robustness of LVLMs against such attacks while preserving their performance on standard benign tasks and incurring an imperceptible increase in time costs."
  },
  {
    "title": "Broaden your SCOPE! Efficient Multi-turn Conversation Planning for LLMs using Semantic Space",
    "url": "http://arxiv.org/abs/2503.11586v1",
    "arxiv_id": "2503.11586v1",
    "authors": [
      "Zhiliang Chen",
      "Xinyuan Niu",
      "Chuan-Sheng Foo",
      "Bryan Kian Hsiang Low"
    ],
    "published": "2025-03-14T16:55:46+00:00",
    "summary": "Large language models (LLMs) are used in chatbots or AI assistants to hold conversations with a human user. In such applications, the quality (e.g., user engagement, safety) of a conversation is important and can only be exactly known at the end of the conversation. To maximize its expected quality, conversation planning reasons about the stochastic transitions within a conversation to select the optimal LLM response at each turn. Existing simulation-based conversation planning algorithms typically select the optimal response by simulating future conversations with a large number of LLM queries at every turn. However, this process is extremely time-consuming and hence impractical for real-time conversations. This paper presents a novel approach called Semantic space COnversation Planning with improved Efficiency (SCOPE) that exploits the dense semantic representation of conversations to perform conversation planning efficiently. In particular, SCOPE models the stochastic transitions in conversation semantics and their associated rewards to plan entirely within the semantic space. This allows us to select the optimal LLM response at every conversation turn without needing additional LLM queries for simulation. As a result, SCOPE can perform conversation planning 70 times faster than conventional simulation-based planning algorithms when applied to a wide variety of conversation starters and two reward functions seen in the real world, yet achieving a higher reward within a practical planning budget. Our code can be found at: https://github.com/chenzhiliang94/convo-plan-SCOPE."
  },
  {
    "title": "A Review of DeepSeek Models' Key Innovative Techniques",
    "url": "http://arxiv.org/abs/2503.11486v1",
    "arxiv_id": "2503.11486v1",
    "authors": [
      "Chengen Wang",
      "Murat Kantarcioglu"
    ],
    "published": "2025-03-14T15:11:29+00:00",
    "summary": "DeepSeek-V3 and DeepSeek-R1 are leading open-source Large Language Models (LLMs) for general-purpose tasks and reasoning, achieving performance comparable to state-of-the-art closed-source models from companies like OpenAI and Anthropic -- while requiring only a fraction of their training costs. Understanding the key innovative techniques behind DeepSeek's success is crucial for advancing LLM research. In this paper, we review the core techniques driving the remarkable effectiveness and efficiency of these models, including refinements to the transformer architecture, innovations such as Multi-Head Latent Attention and Mixture of Experts, Multi-Token Prediction, the co-design of algorithms, frameworks, and hardware, the Group Relative Policy Optimization algorithm, post-training with pure reinforcement learning and iterative training alternating between supervised fine-tuning and reinforcement learning. Additionally, we identify several open questions and highlight potential research opportunities in this rapidly advancing field."
  },
  {
    "title": "Certified Inductive Synthesis for Online Mixed-Integer Optimization",
    "url": "http://arxiv.org/abs/2503.11388v1",
    "arxiv_id": "2503.11388v1",
    "authors": [
      "Marco Zamponi",
      "Emilio Incerto",
      "Daniele Masti",
      "Mirco Tribastone"
    ],
    "published": "2025-03-14T13:31:03+00:00",
    "summary": "In fields such as autonomous and safety-critical systems, online optimization plays a crucial role in control and decision-making processes, often requiring the integration of continuous and discrete variables. These tasks are frequently modeled as mixed-integer programming (MIP) problems, where feedback data are incorporated as parameters. However, solving MIPs within strict time constraints is challenging due to their $\\mathcal{NP}$-complete nature. A promising solution to this challenge involves leveraging the largely invariant structure of these problems to perform most computations offline, thus enabling efficient online solving even on platforms with limited hardware capabilities. In this paper we present a novel implementation of this strategy that uses counterexample-guided inductive synthesis to split the MIP solution process into two stages. In the offline phase, we construct a mapping that provides feasible assignments for binary variables based on parameter values within a specified range. In the online phase, we solve the remaining continuous part of the problem by fixing the binary variables to the values predicted by this mapping. Our numerical evaluation demonstrates the efficiency and solution quality of this approach compared to standard mixed-integer solvers, highlighting its potential for real-time applications in resource-constrained environments."
  },
  {
    "title": "Road Rage Reasoning with Vision-language Models (VLMs): Task Definition and Evaluation Dataset",
    "url": "http://arxiv.org/abs/2503.11342v1",
    "arxiv_id": "2503.11342v1",
    "authors": [
      "Yibing Weng",
      "Yu Gu",
      "Fuji Ren"
    ],
    "published": "2025-03-14T12:18:11+00:00",
    "summary": "Road rage, triggered by driving-related stimuli such as traffic congestion and aggressive driving, poses a significant threat to road safety. Previous research on road rage regulation has primarily focused on response suppression, lacking proactive prevention capabilities. With the advent of Vision-Language Models (VLMs), it has become possible to reason about trigger events visually and then engage in dialog-based comforting before drivers' anger escalates. To this end, we propose the road rage reasoning task, along with a finely annotated test dataset and evaluation metrics, to assess the capabilities of current mainstream VLMs in scene understanding, event recognition, and road rage reasoning. The results indicate that current VLMs exhibit significant shortcomings in scene understanding within the visual modality, as well as in comprehending the spatial relationships between objects in the textual modality. Improving VLMs' performance in these areas will greatly benefit downstream tasks like antecedent-focused road rage regulation."
  },
  {
    "title": "Contract Based Program Models for Software Model Checking",
    "url": "http://arxiv.org/abs/2503.11236v1",
    "arxiv_id": "2503.11236v1",
    "authors": [
      "Jesper Amilon",
      "Dilian Gurov"
    ],
    "published": "2025-03-14T09:34:59+00:00",
    "summary": "Model checking temporal properties of software is algorithmically hard. To be practically feasible, it usually requires the creation of simpler, abstract models of the software, over which the properties are checked. However, creating suitable abstractions is another difficult problem. We argue that such abstract models can be obtained with little effort, when the state transformation properties of the software components have already been deductively verified. As a concrete, language-independent representation of such abstractions we propose the use of \\emph{flow graphs}, a formalism previously developed for the purposes of compositional model checking. In this paper, we describe how we envisage the work flow and tool chain to support the proposed verification approach in the context of embedded, safety-critical software written in~C."
  },
  {
    "title": "Can Large Reasoning Models do Analogical Reasoning under Perceptual Uncertainty?",
    "url": "http://arxiv.org/abs/2503.11207v1",
    "arxiv_id": "2503.11207v1",
    "authors": [
      "Giacomo Camposampiero",
      "Michael Hersche",
      "Roger Wattenhofer",
      "Abu Sebastian",
      "Abbas Rahimi"
    ],
    "published": "2025-03-14T08:52:25+00:00",
    "summary": "This work presents a first evaluation of two state-of-the-art Large Reasoning Models (LRMs), OpenAI's o3-mini and DeepSeek R1, on analogical reasoning, focusing on well-established nonverbal human IQ tests based on Raven's progressive matrices. We benchmark with the I-RAVEN dataset and its more difficult extension, I-RAVEN-X, which tests the ability to generalize to longer reasoning rules and ranges of the attribute values. To assess the influence of visual uncertainties on these nonverbal analogical reasoning tests, we extend the I-RAVEN-X dataset, which otherwise assumes an oracle perception. We adopt a two-fold strategy to simulate this imperfect visual perception: 1) we introduce confounding attributes which, being sampled at random, do not contribute to the prediction of the correct answer of the puzzles and 2) smoothen the distributions of the input attributes' values. We observe a sharp decline in OpenAI's o3-mini task accuracy, dropping from 86.6% on the original I-RAVEN to just 17.0% -- approaching random chance -- on the more challenging I-RAVEN-X, which increases input length and range and emulates perceptual uncertainty. This drop occurred despite spending 3.4x more reasoning tokens. A similar trend is also observed for DeepSeek R1: from 80.6% to 23.2%. On the other hand, a neuro-symbolic probabilistic abductive model, ARLC, that achieves state-of-the-art performances on I-RAVEN, can robustly reason under all these out-of-distribution tests, maintaining strong accuracy with only a modest reduction from 98.6% to 88.0%. Our code is available at https://github.com/IBM/raven-large-language-models."
  },
  {
    "title": "GAIPAT -Dataset on Human Gaze and Actions for Intent Prediction in Assembly Tasks",
    "url": "http://arxiv.org/abs/2503.11186v1",
    "arxiv_id": "2503.11186v1",
    "authors": [
      "Maxence Grand",
      "Damien Pellier",
      "Francis Jambon"
    ],
    "published": "2025-03-14T08:32:52+00:00",
    "summary": "The primary objective of the dataset is to provide a better understanding of the coupling between human actions and gaze in a shared working environment with a cobot, with the aim of signifcantly enhancing the effciency and safety of humancobot interactions. More broadly, by linking gaze patterns with physical actions, the dataset offers valuable insights into cognitive processes and attention dynamics in the context of assembly tasks. The proposed dataset contains gaze and action data from approximately 80 participants, recorded during simulated industrial assembly tasks. The tasks were simulated using controlled scenarios in which participants manipulated educational building blocks. Gaze data was collected using two different eye-tracking setups -head-mounted and remote-while participants worked in two positions: sitting and standing."
  },
  {
    "title": "Align in Depth: Defending Jailbreak Attacks via Progressive Answer Detoxification",
    "url": "http://arxiv.org/abs/2503.11185v1",
    "arxiv_id": "2503.11185v1",
    "authors": [
      "Yingjie Zhang",
      "Tong Liu",
      "Zhe Zhao",
      "Guozhu Meng",
      "Kai Chen"
    ],
    "published": "2025-03-14T08:32:12+00:00",
    "summary": "Large Language Models (LLMs) are vulnerable to jailbreak attacks, which use crafted prompts to elicit toxic responses. These attacks exploit LLMs' difficulty in dynamically detecting harmful intents during the generation process. Traditional safety alignment methods, often relying on the initial few generation steps, are ineffective due to limited computational budget. This paper proposes DEEPALIGN, a robust defense framework that fine-tunes LLMs to progressively detoxify generated content, significantly improving both the computational budget and effectiveness of mitigating harmful generation. Our approach uses a hybrid loss function operating on hidden states to directly improve LLMs' inherent awareness of toxity during generation. Furthermore, we redefine safe responses by generating semantically relevant answers to harmful queries, thereby increasing robustness against representation-mutation attacks. Evaluations across multiple LLMs demonstrate state-of-the-art defense performance against six different attack types, reducing Attack Success Rates by up to two orders of magnitude compared to previous state-of-the-art defense while preserving utility. This work advances LLM safety by addressing limitations of conventional alignment through dynamic, context-aware mitigation."
  },
  {
    "title": "Hand Over or Place On The Table? A Study On Robotic Object Delivery When The Recipient Is Occupied",
    "url": "http://arxiv.org/abs/2503.11177v1",
    "arxiv_id": "2503.11177v1",
    "authors": [
      "Thieu Long Phan",
      "Akansel Cosgun"
    ],
    "published": "2025-03-14T08:25:34+00:00",
    "summary": "This study investigates the subjective experiences of users in two robotic object delivery methods: direct handover and table placement, when users are occupied with another task. A user study involving 15 participants engaged in a typing game revealed that table placement significantly enhances user experience compared to direct handovers, particularly in terms of satisfaction, perceived safety and intuitiveness. Additionally, handovers negatively impacted typing performance, while all participants expressed a clear preference for table placement as the delivery method. These findings highlight the advantages of table placement in scenarios requiring minimal user disruption."
  },
  {
    "title": "Prompt Alchemy: Automatic Prompt Refinement for Enhancing Code Generation",
    "url": "http://arxiv.org/abs/2503.11085v1",
    "arxiv_id": "2503.11085v1",
    "authors": [
      "Sixiang Ye",
      "Zeyu Sun",
      "Guoqing Wang",
      "Liwei Guo",
      "Qingyuan Liang",
      "Zheng Li",
      "Yong Liu"
    ],
    "published": "2025-03-14T04:53:03+00:00",
    "summary": "Code generation has emerged as a key task to automate software development by converting high-level descriptions into executable code. Large language models (LLMs) excel at this but depend heavily on input prompt quality.Manual prompt engineering can be time-consuming and inconsistent, limiting LLM effectiveness. This paper introduces Prochemy, an innovative method for automatically refining prompts to boost code generation. Prochemy overcomes manual prompt limitations by automating optimization, ensuring consistency during inference, and supporting multi-agent systems.It iteratively refines prompts based on model performance, using an optimized final prompt for improved consistency across tasks. We tested Prochemy on natural language-based code generation and translation tasks using three LLM series. Results indicate Prochemy enhances existing methods, improving performance by 5.0% for GPT-3.5-Turbo and 1.9% for GPT-4o over zero-shot baselines on HumanEval. In state-of-the-art LDB, Prochemy + LDB surpasses standalone methods by 1.2-1.8%. For code translation, Prochemy boosts GPT-4o's Java-to-Python (AVATAR) performance from 74.5 to 84.1 (+12.9%) and Python-to-Java from 66.8 to 78.2 (+17.1%). Moreover, Prochemy maintains strong performance when integrated with the o1-mini model, validating its efficacy in code tasks. Designed as plug-and-play, Prochemy optimizes prompts with minimal human input, bridging the gap between simple prompts and complex frameworks."
  },
  {
    "title": "Inverter Control with Time-Varying and Nonconvex State and Input Constraints",
    "url": "http://arxiv.org/abs/2503.11075v1",
    "arxiv_id": "2503.11075v1",
    "authors": [
      "Zixiao Ma",
      "Baosen Zhang"
    ],
    "published": "2025-03-14T04:35:02+00:00",
    "summary": "The growing integration of inverter-based resources (IBRs) into modern power systems poses significant challenges for maintaining reliable operation under dynamic and constrained conditions. This paper focuses on the power tracking problem for grid-connected IBRs, addressing the complexities introduced by voltage and power factor constraints. Voltage constraints, being time-varying and nonlinear input constraints, often conflict with power factor constraints, which are state constraints. These conflicts, coupled with stability requirements, add substantial complexity to control design. To overcome these challenges, we propose a computationally efficient static state-feedback controller that guarantees stability and satisfies operational constraints. The concept of achievability is introduced to evaluate whether power setpoints can be accurately tracked while adhering to all constraints. Using a parameterization framework and the S-lemma, we develop criteria to assess and maximize the continuous achievable region for IBR operation. This framework allows system operators to ensure safety and stability by precomputing a finite set of control gains, significantly reducing online computational requirements. The proposed approach is validated through simulations, demonstrating its effectiveness in handling time-varying grid disturbances and achieving reliable control performance."
  },
  {
    "title": "Large Reasoning Models in Agent Scenarios: Exploring the Necessity of Reasoning Capabilities",
    "url": "http://arxiv.org/abs/2503.11074v1",
    "arxiv_id": "2503.11074v1",
    "authors": [
      "Xueyang Zhou",
      "Guiyao Tie",
      "Guowen Zhang",
      "Weidong Wang",
      "Zhigang Zuo",
      "Di Wu",
      "Duanfeng Chu",
      "Pan Zhou",
      "Lichao Sun",
      "Neil Zhenqiang Gong"
    ],
    "published": "2025-03-14T04:34:31+00:00",
    "summary": "The rise of Large Reasoning Models (LRMs) signifies a paradigm shift toward advanced computational reasoning. Yet, this progress disrupts traditional agent frameworks, traditionally anchored by execution-oriented Large Language Models (LLMs). To explore this transformation, we propose the LaRMA framework, encompassing nine tasks across Tool Usage, Plan Design, and Problem Solving, assessed with three top LLMs (e.g., Claude3.5-sonnet) and five leading LRMs (e.g., DeepSeek-R1). Our findings address four research questions: LRMs surpass LLMs in reasoning-intensive tasks like Plan Design, leveraging iterative reflection for superior outcomes; LLMs excel in execution-driven tasks such as Tool Usage, prioritizing efficiency; hybrid LLM-LRM configurations, pairing LLMs as actors with LRMs as reflectors, optimize agent performance by blending execution speed with reasoning depth; and LRMs' enhanced reasoning incurs higher computational costs, prolonged processing, and behavioral challenges, including overthinking and fact-ignoring tendencies. This study fosters deeper inquiry into LRMs' balance of deep thinking and overthinking, laying a critical foundation for future agent design advancements."
  },
  {
    "title": "Approximate Reachable Sets using Singularly Perturbed Differential Games, with Application to Biological System Models",
    "url": "http://arxiv.org/abs/2503.11021v1",
    "arxiv_id": "2503.11021v1",
    "authors": [
      "Dylan Hirsch",
      "Sylvia Herbert"
    ],
    "published": "2025-03-14T02:37:32+00:00",
    "summary": "Hamilton-Jacobi Reachability (HJR) is an exciting framework used for control of safety-critical systems with nonlinear and possibly uncertain dynamics. However, HJR suffers from the curse of dimensionality, with computation times growing exponentially in the dimension of the system state. Many autonomous and controlled systems involve dynamics that evolve on multiple timescales, and for these systems, singular perturbation methods can be used for model reduction. However, such methods are more challenging to apply in HJR due to the presence of an underlying differential game. In this work, we leverage prior work on singularly perturbed differential games to identify a class of systems which can be readily reduced, and we relate these results to the quantities of interest in HJR. We demonstrate the utility of our results on two examples involving biological systems, where dynamics fitting the identified class are frequently encountered."
  },
  {
    "title": "Comparative Analysis of Advanced AI-based Object Detection Models for Pavement Marking Quality Assessment during Daytime",
    "url": "http://arxiv.org/abs/2503.11008v1",
    "arxiv_id": "2503.11008v1",
    "authors": [
      "Gian Antariksa",
      "Rohir Chakraborty",
      "Shriyank Somvanshi",
      "Subasish Das",
      "Mohammad Jalayer",
      "Deep Rameshkumar Patel",
      "David Mills"
    ],
    "published": "2025-03-14T02:06:46+00:00",
    "summary": "Visual object detection utilizing deep learning plays a vital role in computer vision and has extensive applications in transportation engineering. This paper focuses on detecting pavement marking quality during daytime using the You Only Look Once (YOLO) model, leveraging its advanced architectural features to enhance road safety through precise and real-time assessments. Utilizing image data from New Jersey, this study employed three YOLOv8 variants: YOLOv8m, YOLOv8n, and YOLOv8x. The models were evaluated based on their prediction accuracy for classifying pavement markings into good, moderate, and poor visibility categories. The results demonstrated that YOLOv8n provides the best balance between accuracy and computational efficiency, achieving the highest mean Average Precision (mAP) for objects with good visibility and demonstrating robust performance across various Intersections over Union (IoU) thresholds. This research enhances transportation safety by offering an automated and accurate method for evaluating the quality of pavement markings."
  },
  {
    "title": "Comparative Analysis of Advanced AI-based Object Detection Models for Pavement Marking Quality Assessment during Daytime",
    "url": "http://arxiv.org/abs/2503.11008v2",
    "arxiv_id": "2503.11008v2",
    "authors": [
      "Gian Antariksa",
      "Rohit Chakraborty",
      "Shriyank Somvanshi",
      "Subasish Das",
      "Mohammad Jalayer",
      "Deep Rameshkumar Patel",
      "David Mills"
    ],
    "published": "2025-03-14T02:06:46+00:00",
    "summary": "Visual object detection utilizing deep learning plays a vital role in computer vision and has extensive applications in transportation engineering. This paper focuses on detecting pavement marking quality during daytime using the You Only Look Once (YOLO) model, leveraging its advanced architectural features to enhance road safety through precise and real-time assessments. Utilizing image data from New Jersey, this study employed three YOLOv8 variants: YOLOv8m, YOLOv8n, and YOLOv8x. The models were evaluated based on their prediction accuracy for classifying pavement markings into good, moderate, and poor visibility categories. The results demonstrated that YOLOv8n provides the best balance between accuracy and computational efficiency, achieving the highest mean Average Precision (mAP) for objects with good visibility and demonstrating robust performance across various Intersections over Union (IoU) thresholds. This research enhances transportation safety by offering an automated and accurate method for evaluating the quality of pavement markings."
  },
  {
    "title": "TxAgent: An AI Agent for Therapeutic Reasoning Across a Universe of Tools",
    "url": "http://arxiv.org/abs/2503.10970v1",
    "arxiv_id": "2503.10970v1",
    "authors": [
      "Shanghua Gao",
      "Richard Zhu",
      "Zhenglun Kong",
      "Ayush Noori",
      "Xiaorui Su",
      "Curtis Ginder",
      "Theodoros Tsiligkaridis",
      "Marinka Zitnik"
    ],
    "published": "2025-03-14T00:28:15+00:00",
    "summary": "Precision therapeutics require multimodal adaptive models that generate personalized treatment recommendations. We introduce TxAgent, an AI agent that leverages multi-step reasoning and real-time biomedical knowledge retrieval across a toolbox of 211 tools to analyze drug interactions, contraindications, and patient-specific treatment strategies. TxAgent evaluates how drugs interact at molecular, pharmacokinetic, and clinical levels, identifies contraindications based on patient comorbidities and concurrent medications, and tailors treatment strategies to individual patient characteristics. It retrieves and synthesizes evidence from multiple biomedical sources, assesses interactions between drugs and patient conditions, and refines treatment recommendations through iterative reasoning. It selects tools based on task objectives and executes structured function calls to solve therapeutic tasks that require clinical reasoning and cross-source validation. The ToolUniverse consolidates 211 tools from trusted sources, including all US FDA-approved drugs since 1939 and validated clinical insights from Open Targets. TxAgent outperforms leading LLMs, tool-use models, and reasoning agents across five new benchmarks: DrugPC, BrandPC, GenericPC, TreatmentPC, and DescriptionPC, covering 3,168 drug reasoning tasks and 456 personalized treatment scenarios. It achieves 92.1% accuracy in open-ended drug reasoning tasks, surpassing GPT-4o and outperforming DeepSeek-R1 (671B) in structured multi-step reasoning. TxAgent generalizes across drug name variants and descriptions. By integrating multi-step inference, real-time knowledge grounding, and tool-assisted decision-making, TxAgent ensures that treatment recommendations align with established clinical guidelines and real-world evidence, reducing the risk of adverse events and improving therapeutic decision-making."
  },
  {
    "title": "Safe Control of Second-Order Systems with Linear Constraints",
    "url": "http://arxiv.org/abs/2503.10953v1",
    "arxiv_id": "2503.10953v1",
    "authors": [
      "Mohammed Alyaseen",
      "Nikolay Atanasov",
      "Jorge Cortes"
    ],
    "published": "2025-03-13T23:39:45+00:00",
    "summary": "Control barrier functions (CBFs) offer a powerful tool for enforcing safety specifications in control synthesis. This paper deals with the problem of constructing valid CBFs. Given a second-order system and any desired safety set with linear boundaries in the position space, we construct a provably control-invariant subset of this desired safety set. The constructed subset does not sacrifice any positions allowed by the desired safety set, which can be nonconvex. We show how our construction can also meet safety specification on the velocity. We then demonstrate that if the system satisfies standard Euler-Lagrange systems properties then our construction can also handle constraints on the allowable control inputs. We finally show the efficacy of the proposed method in a numerical example of keeping a 2D robot arm safe from collision."
  },
  {
    "title": "Safe Continual Domain Adaptation after Sim2Real Transfer of Reinforcement Learning Policies in Robotics",
    "url": "http://arxiv.org/abs/2503.10949v1",
    "arxiv_id": "2503.10949v1",
    "authors": [
      "Josip Josifovski",
      "Shangding Gu",
      "Mohammadhossein Malmir",
      "Haoliang Huang",
      "Sayantan Auddy",
      "Nicol\u00e1s Navarro-Guerrero",
      "Costas Spanos",
      "Alois Knoll"
    ],
    "published": "2025-03-13T23:28:11+00:00",
    "summary": "Domain randomization has emerged as a fundamental technique in reinforcement learning (RL) to facilitate the transfer of policies from simulation to real-world robotic applications. Many existing domain randomization approaches have been proposed to improve robustness and sim2real transfer. These approaches rely on wide randomization ranges to compensate for the unknown actual system parameters, leading to robust but inefficient real-world policies. In addition, the policies pretrained in the domain-randomized simulation are fixed after deployment due to the inherent instability of the optimization processes based on RL and the necessity of sampling exploitative but potentially unsafe actions on the real system. This limits the adaptability of the deployed policy to the inevitably changing system parameters or environment dynamics over time. We leverage safe RL and continual learning under domain-randomized simulation to address these limitations and enable safe deployment-time policy adaptation in real-world robot control. The experiments show that our method enables the policy to adapt and fit to the current domain distribution and environment dynamics of the real system while minimizing safety risks and avoiding issues like catastrophic forgetting of the general policy found in randomized simulation during the pretraining phase. Videos and supplementary material are available at https://safe-cda.github.io/."
  },
  {
    "title": "TAIJI: Textual Anchoring for Immunizing Jailbreak Images in Vision Language Models",
    "url": "http://arxiv.org/abs/2503.10872v1",
    "arxiv_id": "2503.10872v1",
    "authors": [
      "Xiangyu Yin",
      "Yi Qi",
      "Jinwei Hu",
      "Zhen Chen",
      "Yi Dong",
      "Xingyu Zhao",
      "Xiaowei Huang",
      "Wenjie Ruan"
    ],
    "published": "2025-03-13T20:39:31+00:00",
    "summary": "Vision Language Models (VLMs) have demonstrated impressive inference capabilities, but remain vulnerable to jailbreak attacks that can induce harmful or unethical responses. Existing defence methods are predominantly white-box approaches that require access to model parameters and extensive modifications, making them costly and impractical for many real-world scenarios. Although some black-box defences have been proposed, they often impose input constraints or require multiple queries, limiting their effectiveness in safety-critical tasks such as autonomous driving. To address these challenges, we propose a novel black-box defence framework called \\textbf{T}extual \\textbf{A}nchoring for \\textbf{I}mmunizing \\textbf{J}ailbreak \\textbf{I}mages (\\textbf{TAIJI}). TAIJI leverages key phrase-based textual anchoring to enhance the model's ability to assess and mitigate the harmful content embedded within both visual and textual prompts. Unlike existing methods, TAIJI operates effectively with a single query during inference, while preserving the VLM's performance on benign tasks. Extensive experiments demonstrate that TAIJI significantly enhances the safety and reliability of VLMs, providing a practical and efficient solution for real-world deployment."
  },
  {
    "title": "Efficient Reachability Analysis for Convolutional Neural Networks Using Hybrid Zonotopes",
    "url": "http://arxiv.org/abs/2503.10840v1",
    "arxiv_id": "2503.10840v1",
    "authors": [
      "Yuhao Zhang",
      "Xiangru Xu"
    ],
    "published": "2025-03-13T19:45:26+00:00",
    "summary": "Feedforward neural networks are widely used in autonomous systems, particularly for control and perception tasks within the system loop. However, their vulnerability to adversarial attacks necessitates formal verification before deployment in safety-critical applications. Existing set propagation-based reachability analysis methods for feedforward neural networks often struggle to achieve both scalability and accuracy. This work presents a novel set-based approach for computing the reachable sets of convolutional neural networks. The proposed method leverages a hybrid zonotope representation and an efficient neural network reduction technique, providing a flexible trade-off between computational complexity and approximation accuracy. Numerical examples are presented to demonstrate the effectiveness of the proposed approach."
  },
  {
    "title": "Thinking Machines: A Survey of LLM based Reasoning Strategies",
    "url": "http://arxiv.org/abs/2503.10814v1",
    "arxiv_id": "2503.10814v1",
    "authors": [
      "Dibyanayan Bandyopadhyay",
      "Soham Bhattacharjee",
      "Asif Ekbal"
    ],
    "published": "2025-03-13T19:03:41+00:00",
    "summary": "Large Language Models (LLMs) are highly proficient in language-based tasks. Their language capabilities have positioned them at the forefront of the future AGI (Artificial General Intelligence) race. However, on closer inspection, Valmeekam et al. (2024); Zecevic et al. (2023); Wu et al. (2024) highlight a significant gap between their language proficiency and reasoning abilities. Reasoning in LLMs and Vision Language Models (VLMs) aims to bridge this gap by enabling these models to think and re-evaluate their actions and responses. Reasoning is an essential capability for complex problem-solving and a necessary step toward establishing trust in Artificial Intelligence (AI). This will make AI suitable for deployment in sensitive domains, such as healthcare, banking, law, defense, security etc. In recent times, with the advent of powerful reasoning models like OpenAI O1 and DeepSeek R1, reasoning endowment has become a critical research topic in LLMs. In this paper, we provide a detailed overview and comparison of existing reasoning techniques and present a systematic survey of reasoning-imbued language models. We also study current challenges and present our findings."
  },
  {
    "title": "HALURust: Exploiting Hallucinations of Large Language Models to Detect Vulnerabilities in Rust",
    "url": "http://arxiv.org/abs/2503.10793v1",
    "arxiv_id": "2503.10793v1",
    "authors": [
      "Yu Luo",
      "Han Zhou",
      "Mengtao Zhang",
      "Dylan De La Rosa",
      "Hafsa Ahmed",
      "Weifeng Xu",
      "Dianxiang Xu"
    ],
    "published": "2025-03-13T18:38:34+00:00",
    "summary": "As an emerging programming language, Rust has rapidly gained popularity and recognition among developers due to its strong emphasis on safety. It employs a unique ownership system and safe concurrency practices to ensure robust safety. Despite these safeguards, security in Rust still presents challenges. Since 2018, 442 Rust-related vulnerabilities have been reported in real-world applications. The limited availability of data has resulted in existing vulnerability detection tools performing poorly in real-world scenarios, often failing to adapt to new and complex vulnerabilities. This paper introduces HALURust, a novel framework that leverages hallucinations of large language models (LLMs) to detect vulnerabilities in real-world Rust scenarios. HALURust leverages LLMs' strength in natural language generation by transforming code into detailed vulnerability analysis reports. The key innovation lies in prompting the LLM to always assume the presence of a vulnerability. If the code sample is vulnerable, the LLM provides an accurate analysis; if not, it generates a hallucinated report. By fine-tuning LLMs on these hallucinations, HALURust can effectively distinguish between vulnerable and non-vulnerable code samples. HALURust was evaluated on a dataset of 81 real-world vulnerabilities, covering 447 functions and 18,691 lines of code across 54 applications. It outperformed existing methods, achieving an F1 score of 77.3%, with over 10% improvement. The hallucinated report-based fine-tuning improved detection by 20\\% compared to traditional code-based fine-tuning. Additionally, HALURust effectively adapted to unseen vulnerabilities and other programming languages, demonstrating strong generalization capabilities."
  },
  {
    "title": "HeightFormer: Learning Height Prediction in Voxel Features for Roadside Vision Centric 3D Object Detection via Transformer",
    "url": "http://arxiv.org/abs/2503.10777v1",
    "arxiv_id": "2503.10777v1",
    "authors": [
      "Zhang Zhang",
      "Chao Sun",
      "Chao Yue",
      "Da Wen",
      "Yujie Chen",
      "Tianze Wang",
      "Jianghao Leng"
    ],
    "published": "2025-03-13T18:17:19+00:00",
    "summary": "Roadside vision centric 3D object detection has received increasing attention in recent years. It expands the perception range of autonomous vehicles, enhances the road safety. Previous methods focused on predicting per-pixel height rather than depth, making significant gains in roadside visual perception. While it is limited by the perspective property of near-large and far-small on image features, making it difficult for network to understand real dimension of objects in the 3D world. BEV features and voxel features present the real distribution of objects in 3D world compared to the image features. However, BEV features tend to lose details due to the lack of explicit height information, and voxel features are computationally expensive. Inspired by this insight, an efficient framework learning height prediction in voxel features via transformer is proposed, dubbed HeightFormer. It groups the voxel features into local height sequences, and utilize attention mechanism to obtain height distribution prediction. Subsequently, the local height sequences are reassembled to generate accurate 3D features. The proposed method is applied to two large-scale roadside benchmarks, DAIR-V2X-I and Rope3D. Extensive experiments are performed and the HeightFormer outperforms the state-of-the-art methods in roadside vision centric 3D object detection task."
  },
  {
    "title": "A Frustratingly Simple Yet Highly Effective Attack Baseline: Over 90% Success Rate Against the Strong Black-box Models of GPT-4.5/4o/o1",
    "url": "http://arxiv.org/abs/2503.10635v1",
    "arxiv_id": "2503.10635v1",
    "authors": [
      "Zhaoyi Li",
      "Xiaohan Zhao",
      "Dong-Dong Wu",
      "Jiacheng Cui",
      "Zhiqiang Shen"
    ],
    "published": "2025-03-13T17:59:55+00:00",
    "summary": "Despite promising performance on open-source large vision-language models (LVLMs), transfer-based targeted attacks often fail against black-box commercial LVLMs. Analyzing failed adversarial perturbations reveals that the learned perturbations typically originate from a uniform distribution and lack clear semantic details, resulting in unintended responses. This critical absence of semantic information leads commercial LVLMs to either ignore the perturbation entirely or misinterpret its embedded semantics, thereby causing the attack to fail. To overcome these issues, we notice that identifying core semantic objects is a key objective for models trained with various datasets and methodologies. This insight motivates our approach that refines semantic clarity by encoding explicit semantic details within local regions, thus ensuring interoperability and capturing finer-grained features, and by concentrating modifications on semantically rich areas rather than applying them uniformly. To achieve this, we propose a simple yet highly effective solution: at each optimization step, the adversarial image is cropped randomly by a controlled aspect ratio and scale, resized, and then aligned with the target image in the embedding space. Experimental results confirm our hypothesis. Our adversarial examples crafted with local-aggregated perturbations focused on crucial regions exhibit surprisingly good transferability to commercial LVLMs, including GPT-4.5, GPT-4o, Gemini-2.0-flash, Claude-3.5-sonnet, Claude-3.7-sonnet, and even reasoning models like o1, Claude-3.7-thinking and Gemini-2.0-flash-thinking. Our approach achieves success rates exceeding 90% on GPT-4.5, 4o, and o1, significantly outperforming all prior state-of-the-art attack methods. Our optimized adversarial examples under different configurations and training code are available at https://github.com/VILA-Lab/M-Attack."
  },
  {
    "title": "DriveLMM-o1: A Step-by-Step Reasoning Dataset and Large Multimodal Model for Driving Scenario Understanding",
    "url": "http://arxiv.org/abs/2503.10621v1",
    "arxiv_id": "2503.10621v1",
    "authors": [
      "Ayesha Ishaq",
      "Jean Lahoud",
      "Ketan More",
      "Omkar Thawakar",
      "Ritesh Thawkar",
      "Dinura Dissanayake",
      "Noor Ahsan",
      "Yuhao Li",
      "Fahad Shahbaz Khan",
      "Hisham Cholakkal",
      "Ivan Laptev",
      "Rao Muhammad Anwer",
      "Salman Khan"
    ],
    "published": "2025-03-13T17:59:01+00:00",
    "summary": "While large multimodal models (LMMs) have demonstrated strong performance across various Visual Question Answering (VQA) tasks, certain challenges require complex multi-step reasoning to reach accurate answers. One particularly challenging task is autonomous driving, which demands thorough cognitive processing before decisions can be made. In this domain, a sequential and interpretive understanding of visual cues is essential for effective perception, prediction, and planning. Nevertheless, common VQA benchmarks often focus on the accuracy of the final answer while overlooking the reasoning process that enables the generation of accurate responses. Moreover, existing methods lack a comprehensive framework for evaluating step-by-step reasoning in realistic driving scenarios. To address this gap, we propose DriveLMM-o1, a new dataset and benchmark specifically designed to advance step-wise visual reasoning for autonomous driving. Our benchmark features over 18k VQA examples in the training set and more than 4k in the test set, covering diverse questions on perception, prediction, and planning, each enriched with step-by-step reasoning to ensure logical inference in autonomous driving scenarios. We further introduce a large multimodal model that is fine-tuned on our reasoning dataset, demonstrating robust performance in complex driving scenarios. In addition, we benchmark various open-source and closed-source methods on our proposed dataset, systematically comparing their reasoning capabilities for autonomous driving tasks. Our model achieves a +7.49% gain in final answer accuracy, along with a 3.62% improvement in reasoning score over the previous best open-source model. Our framework, dataset, and model are available at https://github.com/ayesha-ishaq/DriveLMM-o1."
  },
  {
    "title": "Siege: Autonomous Multi-Turn Jailbreaking of Large Language Models with Tree Search",
    "url": "http://arxiv.org/abs/2503.10619v1",
    "arxiv_id": "2503.10619v1",
    "authors": [
      "Andy Zhou"
    ],
    "published": "2025-03-13T17:57:32+00:00",
    "summary": "We introduce Siege, a multi-turn adversarial framework that models the gradual erosion of Large Language Model (LLM) safety through a tree search perspective. Unlike single-turn jailbreaks that rely on one meticulously engineered prompt, Siege expands the conversation at each turn in a breadth-first fashion, branching out multiple adversarial prompts that exploit partial compliance from previous responses. By tracking these incremental policy leaks and re-injecting them into subsequent queries, Siege reveals how minor concessions can accumulate into fully disallowed outputs. Evaluations on the JailbreakBench dataset show that Siege achieves a 100% success rate on GPT-3.5-turbo and 97% on GPT-4 in a single multi-turn run, using fewer queries than baselines such as Crescendo or GOAT. This tree search methodology offers an in-depth view of how model safeguards degrade over successive dialogue turns, underscoring the urgency of robust multi-turn testing procedures for language models."
  },
  {
    "title": "Siege: Autonomous Multi-Turn Jailbreaking of Large Language Models with Tree Search",
    "url": "http://arxiv.org/abs/2503.10619v2",
    "arxiv_id": "2503.10619v2",
    "authors": [
      "Andy Zhou"
    ],
    "published": "2025-03-13T17:57:32+00:00",
    "summary": "We introduce Siege, a multi-turn adversarial framework that models the gradual erosion of Large Language Model (LLM) safety through a tree search perspective. Unlike single-turn jailbreaks that rely on one meticulously engineered prompt, Siege expands the conversation at each turn in a breadth-first fashion, branching out multiple adversarial prompts that exploit partial compliance from previous responses. By tracking these incremental policy leaks and re-injecting them into subsequent queries, Siege reveals how minor concessions can accumulate into fully disallowed outputs. Evaluations on the JailbreakBench dataset show that Siege achieves a 100% success rate on GPT-3.5-turbo and 97% on GPT-4 in a single multi-turn run, using fewer queries than baselines such as Crescendo or GOAT. This tree search methodology offers an in-depth view of how model safeguards degrade over successive dialogue turns, underscoring the urgency of robust multi-turn testing procedures for language models."
  },
  {
    "title": "OCCUQ: Exploring Efficient Uncertainty Quantification for 3D Occupancy Prediction",
    "url": "http://arxiv.org/abs/2503.10605v1",
    "arxiv_id": "2503.10605v1",
    "authors": [
      "Severin Heidrich",
      "Till Beemelmanns",
      "Alexey Nekrasov",
      "Bastian Leibe",
      "Lutz Eckstein"
    ],
    "published": "2025-03-13T17:50:07+00:00",
    "summary": "Autonomous driving has the potential to significantly enhance productivity and provide numerous societal benefits. Ensuring robustness in these safety-critical systems is essential, particularly when vehicles must navigate adverse weather conditions and sensor corruptions that may not have been encountered during training. Current methods often overlook uncertainties arising from adversarial conditions or distributional shifts, limiting their real-world applicability. We propose an efficient adaptation of an uncertainty estimation technique for 3D occupancy prediction. Our method dynamically calibrates model confidence using epistemic uncertainty estimates. Our evaluation under various camera corruption scenarios, such as fog or missing cameras, demonstrates that our approach effectively quantifies epistemic uncertainty by assigning higher uncertainty values to unseen data. We introduce region-specific corruptions to simulate defects affecting only a single camera and validate our findings through both scene-level and region-level assessments. Our results show superior performance in Out-of-Distribution (OoD) detection and confidence calibration compared to common baselines such as Deep Ensembles and MC-Dropout. Our approach consistently demonstrates reliable uncertainty measures, indicating its potential for enhancing the robustness of autonomous driving systems in real-world scenarios. Code and dataset are available at https://github.com/ika-rwth-aachen/OCCUQ ."
  },
  {
    "title": "Unveiling the Mathematical Reasoning in DeepSeek Models: A Comparative Study of Large Language Models",
    "url": "http://arxiv.org/abs/2503.10573v1",
    "arxiv_id": "2503.10573v1",
    "authors": [
      "Afrar Jahin",
      "Arif Hassan Zidan",
      "Yu Bao",
      "Shizhe Liang",
      "Tianming Liu",
      "Wei Zhang"
    ],
    "published": "2025-03-13T17:23:45+00:00",
    "summary": "With the rapid evolution of Artificial Intelligence (AI), Large Language Models (LLMs) have reshaped the frontiers of various fields, spanning healthcare, public health, engineering, science, agriculture, education, arts, humanities, and mathematical reasoning. Among these advancements, DeepSeek models have emerged as noteworthy contenders, demonstrating promising capabilities that set them apart from their peers. While previous studies have conducted comparative analyses of LLMs, few have delivered a comprehensive evaluation of mathematical reasoning across a broad spectrum of LLMs. In this work, we aim to bridge this gap by conducting an in-depth comparative study, focusing on the strengths and limitations of DeepSeek models in relation to their leading counterparts. In particular, our study systematically evaluates the mathematical reasoning performance of two DeepSeek models alongside five prominent LLMs across three independent benchmark datasets. The findings reveal several key insights: 1). DeepSeek-R1 consistently achieved the highest accuracy on two of the three datasets, demonstrating strong mathematical reasoning capabilities. 2). The distilled variant of LLMs significantly underperformed compared to its peers, highlighting potential drawbacks in using distillation techniques. 3). In terms of response time, Gemini 2.0 Flash demonstrated the fastest processing speed, outperforming other models in efficiency, which is a crucial factor for real-time applications. Beyond these quantitative assessments, we delve into how architecture, training, and optimization impact LLMs' mathematical reasoning. Moreover, our study goes beyond mere performance comparison by identifying key areas for future advancements in LLM-driven mathematical reasoning. This research enhances our understanding of LLMs' mathematical reasoning and lays the groundwork for future advancements"
  },
  {
    "title": "ASIDE: Architectural Separation of Instructions and Data in Language Models",
    "url": "http://arxiv.org/abs/2503.10566v1",
    "arxiv_id": "2503.10566v1",
    "authors": [
      "Egor Zverev",
      "Evgenii Kortukov",
      "Alexander Panfilov",
      "Soroush Tabesh",
      "Alexandra Volkova",
      "Sebastian Lapuschkin",
      "Wojciech Samek",
      "Christoph H. Lampert"
    ],
    "published": "2025-03-13T17:17:17+00:00",
    "summary": "Despite their remarkable performance, large language models lack elementary safety features, and this makes them susceptible to numerous malicious attacks. In particular, previous work has identified the absence of an intrinsic separation between instructions and data as a root cause for the success of prompt injection attacks. In this work, we propose an architectural change, ASIDE, that allows the model to clearly separate between instructions and data by using separate embeddings for them. Instead of training the embeddings from scratch, we propose a method to convert an existing model to ASIDE form by using two copies of the original model's embeddings layer, and applying an orthogonal rotation to one of them. We demonstrate the effectiveness of our method by showing (1) highly increased instruction-data separation scores without a loss in model capabilities and (2) competitive results on prompt injection benchmarks, even without dedicated safety training. Additionally, we study the working mechanism behind our method through an analysis of model representations."
  },
  {
    "title": "Towards Safe Path Tracking Using the Simplex Architecture",
    "url": "http://arxiv.org/abs/2503.10559v1",
    "arxiv_id": "2503.10559v1",
    "authors": [
      "Georg J\u00e4ger",
      "Nils-Jonathan Friedrich",
      "Hauke Petersen",
      "Benjamin Noack"
    ],
    "published": "2025-03-13T17:11:55+00:00",
    "summary": "Robot navigation in complex environments necessitates controllers that are adaptive and safe. Traditional controllers like Regulated Pure Pursuit, Dynamic Window Approach, and Model-Predictive Path Integral, while reliable, struggle to adapt to dynamic conditions. Reinforcement Learning offers adaptability but lacks formal safety guarantees. To address this, we propose a path tracking controller leveraging the Simplex architecture. It combines a Reinforcement Learning controller for adaptiveness and performance with a high-assurance controller providing safety and stability. Our contribution is twofold. We firstly discuss general stability and safety considerations for designing controllers using the Simplex architecture. Secondly, we present a Simplex-based path tracking controller. Our simulation results, supported by preliminary in-field tests, demonstrate the controller's effectiveness in maintaining safety while achieving comparable performance to state-of-the-art methods."
  },
  {
    "title": "Safety Filter for Limiting the Current of Grid-Forming Matrix Modular Multilevel Converters",
    "url": "http://arxiv.org/abs/2503.10498v1",
    "arxiv_id": "2503.10498v1",
    "authors": [
      "Michael Schneeberger",
      "Silvia Mastellone",
      "Florian D\u00f6rfler"
    ],
    "published": "2025-03-13T16:01:11+00:00",
    "summary": "Grid-forming (GFM) converters face significant challenges in limiting current during transient grid events while preserving their grid-forming behavior. This paper offers an elegant solution to the problem with a priori guarantees, presenting a safety filter approach based on Control Barrier Functions (CBFs) to enforce current constraints with minimal deviation from the nominal voltage reference. The safety filter is implemented as a Quadratic Program, enabling real-time computation of safe voltage adjustments that ensure smooth transitions and maintain the GFM behavior during nominal operation. To provide formal safety certificate, the CBF is synthesized offline using a Sum-of-Squares optimization framework, ensuring that the converter remains within its allowable operating limits under all conditions. Additionally, a Control Lyapunov Function is incorporated to facilitate a smooth return to the nominal operating region following grid events. The proposed method is modular and can be integrated into many of the GFM control architectures, as demonstrated with two different GFM implementations. High-fidelity simulations conducted with an enhanced matrix modular multilevel converter connected to both high-inertia and low-inertia grid scenarios validate the effectiveness of the safety filter, showing that it successfully limits current during faults, preserves GFM behavior, and ensures a seamless recovery to nominal operation."
  },
  {
    "title": "LLMs in Disease Diagnosis: A Comparative Study of DeepSeek-R1 and O3 Mini Across Chronic Health Conditions",
    "url": "http://arxiv.org/abs/2503.10486v1",
    "arxiv_id": "2503.10486v1",
    "authors": [
      "Gaurav Kumar Gupta",
      "Pranal Pande"
    ],
    "published": "2025-03-13T15:54:26+00:00",
    "summary": "Large Language Models (LLMs) are revolutionizing medical diagnostics by enhancing both disease classification and clinical decision-making. In this study, we evaluate the performance of two LLM- based diagnostic tools, DeepSeek R1 and O3 Mini, using a structured dataset of symptoms and diagnoses. We assessed their predictive accuracy at both the disease and category levels, as well as the reliability of their confidence scores. DeepSeek R1 achieved a disease-level accuracy of 76% and an overall accuracy of 82%, outperforming O3 Mini, which attained 72% and 75% respectively. Notably, DeepSeek R1 demonstrated exceptional performance in Mental Health, Neurological Disorders, and Oncology, where it reached 100% accuracy, while O3 Mini excelled in Autoimmune Disease classification with 100% accuracy. Both models, however, struggled with Respiratory Disease classification, recording accuracies of only 40% for DeepSeek R1 and 20% for O3 Mini. Additionally, the analysis of confidence scores revealed that DeepSeek R1 provided high-confidence predictions in 92% of cases, compared to 68% for O3 Mini. Ethical considerations regarding bias, model interpretability, and data privacy are also discussed to ensure the responsible integration of LLMs into clinical practice. Overall, our findings offer valuable insights into the strengths and limitations of LLM-based diagnostic systems and provide a roadmap for future enhancements in AI-driven healthcare."
  },
  {
    "title": "Stratified Topological Autonomy for Long-Range Coordination (STALC)",
    "url": "http://arxiv.org/abs/2503.10475v1",
    "arxiv_id": "2503.10475v1",
    "authors": [
      "Cora A. Dimmig",
      "Adam Goertz",
      "Adam Polevoy",
      "Mark Gonzales",
      "Kevin C. Wolfe",
      "Bradley Woosley",
      "John Rogers",
      "Joseph Moore"
    ],
    "published": "2025-03-13T15:45:27+00:00",
    "summary": "Achieving unified multi-robot coordination and motion planning in complex environments is a challenging problem. In this paper, we present a hierarchical approach to long-range coordination, which we call Stratified Topological Autonomy for Long-Range Coordination (STALC). In particular, we look at the problem of minimizing visibility to observers and maximizing safety with a multi-robot team navigating through a hazardous environment. At its core, our approach relies on the notion of a dynamic topological graph, where the edge weights vary dynamically based on the locations of the robots in the graph. To create this dynamic topological graph, we evaluate the visibility of the robot team from a discrete set of observer locations (both adversarial and friendly), and construct a topological graph whose edge weights depend on both adversary position and robot team configuration. We then impose temporal constraints on the evolution of those edge weights based on robot team state and use Mixed-Integer Programming (MIP) to generate optimal multirobot plans through the graph. The visibility information also informs the lower layers of the autonomy stack to plan minimal visibility paths through the environment for the team of robots. Our approach presents methods to reduce the computational complexity for a team of robots that interact and coordinate across the team to accomplish a common goal. We demonstrate our approach in simulated and hardware experiments in forested and urban environments."
  },
  {
    "title": "Applying Tabular Deep Learning Models to Estimate Crash Injury Types of Young Motorcyclists",
    "url": "http://arxiv.org/abs/2503.10474v1",
    "arxiv_id": "2503.10474v1",
    "authors": [
      "Shriyank Somvanshi",
      "Anannya Ghosh Tusti",
      "Rohit Chakraborty",
      "Subasish Das"
    ],
    "published": "2025-03-13T15:45:13+00:00",
    "summary": "Young motorcyclists, particularly those aged 15 to 24 years old, face a heightened risk of severe crashes due to factors such as speeding, traffic violations, and helmet usage. This study aims to identify key factors influencing crash severity by analyzing 10,726 young motorcyclist crashes in Texas from 2017 to 2022. Two advanced tabular deep learning models, ARMNet and MambaNet, were employed, using an advanced resampling technique to address class imbalance. The models were trained to classify crashes into three severity levels, Fatal or Severe, Moderate or Minor, and No Injury. ARMNet achieved an accuracy of 87 percent, outperforming 86 percent of Mambanet, with both models excelling in predicting severe and no injury crashes while facing challenges in moderate crash classification. Key findings highlight the significant influence of demographic, environmental, and behavioral factors on crash outcomes. The study underscores the need for targeted interventions, including stricter helmet enforcement and educational programs customized to young motorcyclists. These insights provide valuable guidance for policymakers in developing evidence-based strategies to enhance motorcyclist safety and reduce crash severity."
  },
  {
    "title": "Light-R1: Curriculum SFT, DPO and RL for Long COT from Scratch and Beyond",
    "url": "http://arxiv.org/abs/2503.10460v1",
    "arxiv_id": "2503.10460v1",
    "authors": [
      "Liang Wen",
      "Yunke Cai",
      "Fenrui Xiao",
      "Xin He",
      "Qi An",
      "Zhenyu Duan",
      "Yimin Du",
      "Junchen Liu",
      "Lifu Tang",
      "Xiaowei Lv",
      "Haosheng Zou",
      "Yongchao Deng",
      "Shousheng Jia",
      "Xiangzheng Zhang"
    ],
    "published": "2025-03-13T15:29:22+00:00",
    "summary": "This paper presents our work on the Light-R1 series, with models, data, and code all released.   We first focus on training long COT models from scratch, specifically starting from models initially lacking long COT capabilities. Using a curriculum training recipe consisting of two-stage SFT and semi-on-policy DPO, we train our model Light-R1-32B from Qwen2.5-32B-Instruct, resulting in superior math performance compared to DeepSeek-R1-Distill-Qwen-32B. Despite being trained exclusively on math data, Light-R1-32B shows strong generalization across other domains. In the subsequent phase of this work, we highlight the significant benefit of the 3k dataset constructed for the second SFT stage on enhancing other models. By fine-tuning DeepSeek-R1-Distilled models using this dataset, we obtain new SOTA models in 7B and 14B, while the 32B model, Light-R1-32B-DS performed comparably to QwQ-32B and DeepSeek-R1.   Furthermore, we extend our work by applying reinforcement learning, specifically GRPO, on long-COT models to further improve reasoning performance. We successfully train our final Light-R1-14B-DS with RL, achieving SOTA performance among 14B parameter models in math. With AIME24 & 25 scores of 74.0 and 60.2 respectively, Light-R1-14B-DS surpasses even many 32B models and DeepSeek-R1-Distill-Llama-70B. Its RL training also exhibits well expected behavior, showing simultaneous increase in response length and reward score.   The Light-R1 series of work validates training long-COT models from scratch, showcases the art in SFT data and releases SOTA models from RL."
  },
  {
    "title": "Social Media Harm Abatement: Mechanisms for Transparent Public Health Assessment",
    "url": "http://arxiv.org/abs/2503.10458v1",
    "arxiv_id": "2503.10458v1",
    "authors": [
      "Nathaniel Lubin",
      "Yuning Liu",
      "Amanda Yarnell",
      "S. Bryn Austin",
      "Zachary J. Ward",
      "Ravi Iyer",
      "Jonathan Stray",
      "Matthew Lawrence",
      "Alissa Cooper",
      "Peter Chapman"
    ],
    "published": "2025-03-13T15:26:46+00:00",
    "summary": "Social media platforms have been accused of causing a range of harms, resulting in dozens of lawsuits across jurisdictions. These lawsuits are situated within the context of a long history of American product safety litigation, suggesting opportunities for remediation outside of financial compensation. Anticipating that at least some of these cases may be successful and/or lead to settlements, this article outlines an implementable mechanism for an abatement and/or settlement plan capable of mitigating abuse. The paper describes the requirements of such a mechanism, implications for privacy and oversight, and tradeoffs that such a procedure would entail. The mechanism is framed to operate at the intersection of legal procedure, standards for transparent public health assessment, and the practical requirements of modern technology products."
  },
  {
    "title": "Finetuning Generative Trajectory Model with Reinforcement Learning from Human Feedback",
    "url": "http://arxiv.org/abs/2503.10434v1",
    "arxiv_id": "2503.10434v1",
    "authors": [
      "Derun Li",
      "Jianwei Ren",
      "Yue Wang",
      "Xin Wen",
      "Pengxiang Li",
      "Leimeng Xu",
      "Kun Zhan",
      "Zhongpu Xia",
      "Peng Jia",
      "Xianpeng Lang",
      "Ningyi Xu",
      "Hang Zhao"
    ],
    "published": "2025-03-13T14:56:17+00:00",
    "summary": "Generating human-like and adaptive trajectories is essential for autonomous driving in dynamic environments. While generative models have shown promise in synthesizing feasible trajectories, they often fail to capture the nuanced variability of human driving styles due to dataset biases and distributional shifts. To address this, we introduce TrajHF, a human feedback-driven finetuning framework for generative trajectory models, designed to align motion planning with diverse driving preferences. TrajHF incorporates multi-conditional denoiser and reinforcement learning with human feedback to refine multi-modal trajectory generation beyond conventional imitation learning. This enables better alignment with human driving preferences while maintaining safety and feasibility constraints. TrajHF achieves PDMS of 93.95 on NavSim benchmark, significantly exceeding other methods. TrajHF sets a new paradigm for personalized and adaptable trajectory generation in autonomous driving."
  },
  {
    "title": "Safe exploration in reproducing kernel Hilbert spaces",
    "url": "http://arxiv.org/abs/2503.10352v1",
    "arxiv_id": "2503.10352v1",
    "authors": [
      "Abdullah Tokmak",
      "Kiran G. Krishnan",
      "Thomas B. Sch\u00f6n",
      "Dominik Baumann"
    ],
    "published": "2025-03-13T13:28:54+00:00",
    "summary": "Popular safe Bayesian optimization (BO) algorithms learn control policies for safety-critical systems in unknown environments. However, most algorithms make a smoothness assumption, which is encoded by a known bounded norm in a reproducing kernel Hilbert space (RKHS). The RKHS is a potentially infinite-dimensional space, and it remains unclear how to reliably obtain the RKHS norm of an unknown function. In this work, we propose a safe BO algorithm capable of estimating the RKHS norm from data. We provide statistical guarantees on the RKHS norm estimation, integrate the estimated RKHS norm into existing confidence intervals and show that we retain theoretical guarantees, and prove safety of the resulting safe BO algorithm. We apply our algorithm to safely optimize reinforcement learning policies on physics simulators and on a real inverted pendulum, demonstrating improved performance, safety, and scalability compared to the state-of-the-art."
  },
  {
    "title": "HALO: Fault-Tolerant Safety Architecture For High-Speed Autonomous Racing",
    "url": "http://arxiv.org/abs/2503.10341v1",
    "arxiv_id": "2503.10341v1",
    "authors": [
      "Aron Harder",
      "Amar Kulkarni",
      "Madhur Behl"
    ],
    "published": "2025-03-13T13:19:51+00:00",
    "summary": "The field of high-speed autonomous racing has seen significant advances in recent years, with the rise of competitions such as RoboRace and the Indy Autonomous Challenge providing a platform for researchers to develop software stacks for autonomous race vehicles capable of reaching speeds in excess of 170 mph. Ensuring the safety of these vehicles requires the software to continuously monitor for different faults and erroneous operating conditions during high-speed operation, with the goal of mitigating any unreasonable risks posed by malfunctions in sub-systems and components. This paper presents a comprehensive overview of the HALO safety architecture, which has been implemented on a full-scale autonomous racing vehicle as part of the Indy Autonomous Challenge. The paper begins with a failure mode and criticality analysis of the perception, planning, control, and communication modules of the software stack. Specifically, we examine three different types of faults - node health, data health, and behavioral-safety faults. To mitigate these faults, the paper then outlines HALO safety archetypes and runtime monitoring methods. Finally, the paper demonstrates the effectiveness of the HALO safety architecture for each of the faults, through real-world data gathered from autonomous racing vehicle trials during multi-agent scenarios."
  },
  {
    "title": "Enhance Exploration in Safe Reinforcement Learning with Contrastive Representation Learning",
    "url": "http://arxiv.org/abs/2503.10318v1",
    "arxiv_id": "2503.10318v1",
    "authors": [
      "Duc Kien Doan",
      "Bang Giang Le",
      "Viet Cuong Ta"
    ],
    "published": "2025-03-13T12:53:42+00:00",
    "summary": "In safe reinforcement learning, agent needs to balance between exploration actions and safety constraints. Following this paradigm, domain transfer approaches learn a prior Q-function from the related environments to prevent unsafe actions. However, because of the large number of false positives, some safe actions are never executed, leading to inadequate exploration in sparse-reward environments. In this work, we aim to learn an efficient state representation to balance the exploration and safety-prefer action in a sparse-reward environment. Firstly, the image input is mapped to latent representation by an auto-encoder. A further contrastive learning objective is employed to distinguish safe and unsafe states. In the learning phase, the latent distance is used to construct an additional safety check, which allows the agent to bias the exploration if it visits an unsafe state. To verify the effectiveness of our method, the experiment is carried out in three navigation-based MiniGrid environments. The result highlights that our method can explore the environment better while maintaining a good balance between safety and efficiency."
  },
  {
    "title": "CODEI: Resource-Efficient Task-Driven Co-Design of Perception and Decision Making for Mobile Robots Applied to Autonomous Vehicles",
    "url": "http://arxiv.org/abs/2503.10296v1",
    "arxiv_id": "2503.10296v1",
    "authors": [
      "Dejan Milojevic",
      "Gioele Zardini",
      "Miriam Elser",
      "Andrea Censi",
      "Emilio Frazzoli"
    ],
    "published": "2025-03-13T12:12:44+00:00",
    "summary": "This paper discusses the integration challenges and strategies for designing mobile robots, by focusing on the task-driven, optimal selection of hardware and software to balance safety, efficiency, and minimal usage of resources such as costs, energy, computational requirements, and weight. We emphasize the interplay between perception and motion planning in decision-making by introducing the concept of occupancy queries to quantify the perception requirements for sampling-based motion planners. Sensor and algorithm performance are evaluated using False Negative Rates (FPR) and False Positive Rates (FPR) across various factors such as geometric relationships, object properties, sensor resolution, and environmental conditions. By integrating perception requirements with perception performance, an Integer Linear Programming (ILP) approach is proposed for efficient sensor and algorithm selection and placement. This forms the basis for a co-design optimization that includes the robot body, motion planner, perception pipeline, and computing unit. We refer to this framework for solving the co-design problem of mobile robots as CODEI, short for Co-design of Embodied Intelligence. A case study on developing an Autonomous Vehicle (AV) for urban scenarios provides actionable information for designers, and shows that complex tasks escalate resource demands, with task performance affecting choices of the autonomy stack. The study demonstrates that resource prioritization influences sensor choice: cameras are preferred for cost-effective and lightweight designs, while lidar sensors are chosen for better energy and computational efficiency."
  },
  {
    "title": "Thermal Management of Lithium-Ion Batteries: A Comparative Study of Phase Change Materials and Air-Cooling Systems Equipped with Fins",
    "url": "http://arxiv.org/abs/2503.10244v1",
    "arxiv_id": "2503.10244v1",
    "authors": [
      "Masoumeh Karimi Kisomi"
    ],
    "published": "2025-03-13T10:45:16+00:00",
    "summary": "Lithium-ion batteries are extensively utilized as the primary power source for electric vehicles due to their high energy density, environmental friendliness and lightweight nature. However, their performance and safety are highly dependent on operating temperature. Therefore, a battery thermal management system (BTMS) is essential to ensure the reliable operation and safety of electric vehicles. This study presents a battery thermal management system incorporating phase change material (PCM) and air cooling in a cylindrical lithium-ion cell with fins to enhance heat dissipation. The effects of each system on maximum and minimum temperature, and temperature uniformity along the battery cell are analyzed. Additionally, the impact of fins in both systems is evaluated against a finless cell. A numerical analysis utilizing ANSYS software and the finite volume method (FVM) is performed to evaluate the cooling performance of the systems. The results show that PCM reduces both the maximum and minimum temperatures compared to the air cooling system due to the phase change mechanism. In the finless battery case, the maximum temperature decreases from 316 K to 304 K when using PCM instead of the air cooling system. Also, in the same fin-based battery, the minimum temperature decreases from 307 K to 302 K by using PCM instead of the air cooling system, leading to improved temperature stability. The results indicate that, in general, the fins help reduce the maximum cell temperature when compared to the case without fins in both cases. Using rectangular fins reduces the maximum temperature by approximately 3% compared to a finless battery in the air cooling system. Additionally, the presence of fins reduces the temperature difference along the battery, ensuring a more uniform temperature distribution, such that, in the PCM system with rectangular fins, the temperature difference remains below 1 K."
  },
  {
    "title": "MinorBench: A hand-built benchmark for content-based risks for children",
    "url": "http://arxiv.org/abs/2503.10242v1",
    "arxiv_id": "2503.10242v1",
    "authors": [
      "Shaun Khoo",
      "Gabriel Chua",
      "Rachel Shong"
    ],
    "published": "2025-03-13T10:34:43+00:00",
    "summary": "Large Language Models (LLMs) are rapidly entering children's lives - through parent-driven adoption, schools, and peer networks - yet current AI ethics and safety research do not adequately address content-related risks specific to minors. In this paper, we highlight these gaps with a real-world case study of an LLM-based chatbot deployed in a middle school setting, revealing how students used and sometimes misused the system. Building on these findings, we propose a new taxonomy of content-based risks for minors and introduce MinorBench, an open-source benchmark designed to evaluate LLMs on their ability to refuse unsafe or inappropriate queries from children. We evaluate six prominent LLMs under different system prompts, demonstrating substantial variability in their child-safety compliance. Our results inform practical steps for more robust, child-focused safety mechanisms and underscore the urgency of tailoring AI systems to safeguard young users."
  },
  {
    "title": "Red Teaming Contemporary AI Models: Insights from Spanish and Basque Perspectives",
    "url": "http://arxiv.org/abs/2503.10192v1",
    "arxiv_id": "2503.10192v1",
    "authors": [
      "Miguel Romero-Arjona",
      "Pablo Valle",
      "Juan C. Alonso",
      "Ana B. S\u00e1nchez",
      "Miriam Ugarte",
      "Antonia Cazalilla",
      "Vicente Cambr\u00f3n",
      "Jos\u00e9 A. Parejo",
      "Aitor Arrieta",
      "Sergio Segura"
    ],
    "published": "2025-03-13T09:27:24+00:00",
    "summary": "The battle for AI leadership is on, with OpenAI in the United States and DeepSeek in China as key contenders. In response to these global trends, the Spanish government has proposed ALIA, a public and transparent AI infrastructure incorporating small language models designed to support Spanish and co-official languages such as Basque. This paper presents the results of Red Teaming sessions, where ten participants applied their expertise and creativity to manually test three of the latest models from these initiatives$\\unicode{x2013}$OpenAI o3-mini, DeepSeek R1, and ALIA Salamandra$\\unicode{x2013}$focusing on biases and safety concerns. The results, based on 670 conversations, revealed vulnerabilities in all the models under test, with biased or unsafe responses ranging from 29.5% in o3-mini to 50.6% in Salamandra. These findings underscore the persistent challenges in developing reliable and trustworthy AI systems, particularly those intended to support Spanish and Basque languages."
  },
  {
    "title": "Safety Control of Impulsive Systems with Control Barrier Functions and Adaptive Gains",
    "url": "http://arxiv.org/abs/2503.10164v1",
    "arxiv_id": "2503.10164v1",
    "authors": [
      "Zihan Liu",
      "Yuan-Hua Ni"
    ],
    "published": "2025-03-13T08:38:47+00:00",
    "summary": "This paper addresses the safety challenges in impulsive systems, where abrupt state jumps introduce significant complexities into system dynamics. A unified framework is proposed by integrating Quadratic Programming (QP), Control Barrier Functions (CBFs), and adaptive gain mechanisms to ensure system safety during impulsive events. The CBFs are constructed to enforce safety constraints by capturing the system's continuous dynamics and the effects of impulsive state transitions. An adaptive gain mechanism dynamically adjusts control inputs based on the magnitudes of the impulses and the system's proximity to safety boundaries, maintaining safety during instantaneous state jumps. A tailored QP formulation incorporates CBFs constraints and adaptive gain adjustments, optimizing control inputs while ensuring compliance with safety-critical requirements. Theoretical analysis establishes the boundedness, continuity, and feasibility of the adaptive gain and the overall framework. The effectiveness of the method is demonstrated through simulations on a robotic manipulator, showcasing its practical applicability to impulsive systems with state jumps."
  },
  {
    "title": "Unlocking Generalization Power in LiDAR Point Cloud Registration",
    "url": "http://arxiv.org/abs/2503.10149v1",
    "arxiv_id": "2503.10149v1",
    "authors": [
      "Zhenxuan Zeng",
      "Qiao Wu",
      "Xiyu Zhang",
      "Lin Yuanbo Wu",
      "Pei An",
      "Jiaqi Yang",
      "Ji Wang",
      "Peng Wang"
    ],
    "published": "2025-03-13T08:20:59+00:00",
    "summary": "In real-world environments, a LiDAR point cloud registration method with robust generalization capabilities (across varying distances and datasets) is crucial for ensuring safety in autonomous driving and other LiDAR-based applications. However, current methods fall short in achieving this level of generalization. To address these limitations, we propose UGP, a pruned framework designed to enhance generalization power for LiDAR point cloud registration. The core insight in UGP is the elimination of cross-attention mechanisms to improve generalization, allowing the network to concentrate on intra-frame feature extraction. Additionally, we introduce a progressive self-attention module to reduce ambiguity in large-scale scenes and integrate Bird's Eye View (BEV) features to incorporate semantic information about scene elements. Together, these enhancements significantly boost the network's generalization performance. We validated our approach through various generalization experiments in multiple outdoor scenes. In cross-distance generalization experiments on KITTI and nuScenes, UGP achieved state-of-the-art mean Registration Recall rates of 94.5% and 91.4%, respectively. In cross-dataset generalization from nuScenes to KITTI, UGP achieved a state-of-the-art mean Registration Recall of 90.9%. Code will be available at https://github.com/peakpang/UGP."
  },
  {
    "title": "Mapless Collision-Free Flight via MPC using Dual KD-Trees in Cluttered Environments",
    "url": "http://arxiv.org/abs/2503.10141v1",
    "arxiv_id": "2503.10141v1",
    "authors": [
      "Linzuo Zhang",
      "Yu Hu",
      "Yang Deng",
      "Feng Yu",
      "Danping Zou"
    ],
    "published": "2025-03-13T08:00:58+00:00",
    "summary": "Collision-free flight in cluttered environments is a critical capability for autonomous quadrotors. Traditional methods often rely on detailed 3D map construction, trajectory generation, and tracking. However, this cascade pipeline can introduce accumulated errors and computational delays, limiting flight agility and safety. In this paper, we propose a novel method for enabling collision-free flight in cluttered environments without explicitly constructing 3D maps or generating and tracking collision-free trajectories. Instead, we leverage Model Predictive Control (MPC) to directly produce safe actions from sparse waypoints and point clouds from a depth camera. These sparse waypoints are dynamically adjusted online based on nearby obstacles detected from point clouds. To achieve this, we introduce a dual KD-Tree mechanism: the Obstacle KD-Tree quickly identifies the nearest obstacle for avoidance, while the Edge KD-Tree provides a robust initial guess for the MPC solver, preventing it from getting stuck in local minima during obstacle avoidance. We validate our approach through extensive simulations and real-world experiments. The results show that our approach significantly outperforms the mapping-based methods and is also superior to imitation learning-based methods, demonstrating reliable obstacle avoidance at up to 12 m/s in simulations and 6 m/s in real-world tests. Our method provides a simple and robust alternative to existing methods."
  },
  {
    "title": "IMPACT: Intelligent Motion Planning with Acceptable Contact Trajectories via Vision-Language Models",
    "url": "http://arxiv.org/abs/2503.10110v1",
    "arxiv_id": "2503.10110v1",
    "authors": [
      "Yiyang Ling",
      "Karan Owalekar",
      "Oluwatobiloba Adesanya",
      "Erdem B\u0131y\u0131k",
      "Daniel Seita"
    ],
    "published": "2025-03-13T07:09:00+00:00",
    "summary": "Motion planning involves determining a sequence of robot configurations to reach a desired pose, subject to movement and safety constraints. Traditional motion planning finds collision-free paths, but this is overly restrictive in clutter, where it may not be possible for a robot to accomplish a task without contact. In addition, contacts range from relatively benign (e.g., brushing a soft pillow) to more dangerous (e.g., toppling a glass vase). Due to this diversity, it is difficult to characterize which contacts may be acceptable or unacceptable. In this paper, we propose IMPACT, a novel motion planning framework that uses Vision-Language Models (VLMs) to infer environment semantics, identifying which parts of the environment can best tolerate contact based on object properties and locations. Our approach uses the VLM's outputs to produce a dense 3D \"cost map\" that encodes contact tolerances and seamlessly integrates with standard motion planners. We perform experiments using 20 simulation and 10 real-world scenes and assess using task success rate, object displacements, and feedback from human evaluators. Our results over 3620 simulation and 200 real-world trials suggest that IMPACT enables efficient contact-rich motion planning in cluttered settings while outperforming alternative methods and ablations. Supplementary material is available at https://impact-planning.github.io/."
  },
  {
    "title": "Representation-based Reward Modeling for Efficient Safety Alignment of Large Language Model",
    "url": "http://arxiv.org/abs/2503.10093v1",
    "arxiv_id": "2503.10093v1",
    "authors": [
      "Qiyuan Deng",
      "Xuefeng Bai",
      "Kehai Chen",
      "Yaowei Wang",
      "Liqiang Nie",
      "Min Zhang"
    ],
    "published": "2025-03-13T06:40:34+00:00",
    "summary": "Reinforcement Learning (RL) algorithms for safety alignment of Large Language Models (LLMs), such as Direct Preference Optimization (DPO), encounter the challenge of distribution shift. Current approaches typically address this issue through online sampling from the target policy, which requires significant computational resources. In this paper, we hypothesize that during off-policy training, while the ranking order of output generated by policy changes, their overall distribution remains relatively stable. This stability allows the transformation of the sampling process from the target policy into a re-ranking of preference data. Building on this hypothesis, We propose a new framework that leverages the model's intrinsic safety judgment capability to extract reward signals, which are then used to calculate label confidence for preferences reordering. Extensive experimental results and theoretical analysis demonstrate that the proposed method effectively addresses the distribution shift issue, remarkably enhancing the safety performance while reducing about 300x computational overheads."
  },
  {
    "title": "Advanced Tool Learning and Selection System (ATLASS): A Closed-Loop Framework Using LLM",
    "url": "http://arxiv.org/abs/2503.10071v1",
    "arxiv_id": "2503.10071v1",
    "authors": [
      "Mohd Ariful Haque",
      "Justin Williams",
      "Sunzida Siddique",
      "Md. Hujaifa Islam",
      "Hasmot Ali",
      "Kishor Datta Gupta",
      "Roy George"
    ],
    "published": "2025-03-13T05:39:00+00:00",
    "summary": "The combination of LLM agents with external tools enables models to solve complex tasks beyond their knowledge base. Human-designed tools are inflexible and restricted to solutions within the scope of pre-existing tools created by experts. To address this problem, we propose ATLASS, an advanced tool learning and selection system designed as a closed-loop framework. It enables the LLM to solve problems by dynamically generating external tools on demand. In this framework, agents play a crucial role in orchestrating tool selection, execution, and refinement, ensuring adaptive problem-solving capabilities. The operation of ATLASS follows three phases: The first phase, Understanding Tool Requirements, involves the Agents determining whether tools are required and specifying their functionality; the second phase, Tool Retrieval/Generation, involves the Agents retrieving or generating tools based on their availability; and the third phase, Task Solving, involves combining all the component tools necessary to complete the initial task. The Tool Dataset stores the generated tools, ensuring reusability and minimizing inference cost. Current LLM-based tool generation systems have difficulty creating complex tools that need APIs or external packages. In ATLASS, we solve the problem by automatically setting up the environment, fetching relevant API documentation online, and using a Python interpreter to create a reliable, versatile tool that works in a wider range of situations. OpenAI GPT-4.0 is used as the LLM agent, and safety and ethical concerns are handled through human feedback before executing generated code. By addressing the limitations of predefined toolsets and enhancing adaptability, ATLASS serves as a real-world solution that empowers users with dynamically generated tools for complex problem-solving."
  },
  {
    "title": "Enhancing Multi-Agent Systems via Reinforcement Learning with LLM-based Planner and Graph-based Policy",
    "url": "http://arxiv.org/abs/2503.10049v1",
    "arxiv_id": "2503.10049v1",
    "authors": [
      "Ziqi Jia",
      "Junjie Li",
      "Xiaoyang Qu",
      "Jianzong Wang"
    ],
    "published": "2025-03-13T05:02:49+00:00",
    "summary": "Multi-agent systems (MAS) have shown great potential in executing complex tasks, but coordination and safety remain significant challenges. Multi-Agent Reinforcement Learning (MARL) offers a promising framework for agent collaboration, but it faces difficulties in handling complex tasks and designing reward functions. The introduction of Large Language Models (LLMs) has brought stronger reasoning and cognitive abilities to MAS, but existing LLM-based systems struggle to respond quickly and accurately in dynamic environments. To address these challenges, we propose LLM-based Graph Collaboration MARL (LGC-MARL), a framework that efficiently combines LLMs and MARL. This framework decomposes complex tasks into executable subtasks and achieves efficient collaboration among multiple agents through graph-based coordination. Specifically, LGC-MARL consists of two main components: an LLM planner and a graph-based collaboration meta policy. The LLM planner transforms complex task instructions into a series of executable subtasks, evaluates the rationality of these subtasks using a critic model, and generates an action dependency graph. The graph-based collaboration meta policy facilitates communication and collaboration among agents based on the action dependency graph, and adapts to new task environments through meta-learning. Experimental results on the AI2-THOR simulation platform demonstrate the superior performance and scalability of LGC-MARL in completing various complex tasks."
  },
  {
    "title": "Post-disaster building indoor damage and survivor detection using autonomous path planning and deep learning with unmanned aerial vehicles",
    "url": "http://arxiv.org/abs/2503.10027v1",
    "arxiv_id": "2503.10027v1",
    "authors": [
      "Xiao Pan",
      "Sina Tavasoli",
      "T. Y. Yang",
      "Sina Poorghasem"
    ],
    "published": "2025-03-13T04:13:48+00:00",
    "summary": "Rapid response to natural disasters such as earthquakes is a crucial element in ensuring the safety of civil infrastructures and minimizing casualties. Traditional manual inspection is labour-intensive, time-consuming, and can be dangerous for inspectors and rescue workers. This paper proposed an autonomous inspection approach for structural damage inspection and survivor detection in the post-disaster building indoor scenario, which incorporates an autonomous navigation method, deep learning-based damage and survivor detection method, and a customized low-cost micro aerial vehicle (MAV) with onboard sensors. Experimental studies in a pseudo-post-disaster office building have shown the proposed methodology can achieve high accuracy in structural damage inspection and survivor detection. Overall, the proposed inspection approach shows great potential to improve the efficiency of existing manual post-disaster building inspection."
  },
  {
    "title": "OR-LLM-Agent: Automating Modeling and Solving of Operations Research Optimization Problem with Reasoning Large Language Model",
    "url": "http://arxiv.org/abs/2503.10009v1",
    "arxiv_id": "2503.10009v1",
    "authors": [
      "Bowen Zhang",
      "Pengcheng Luo"
    ],
    "published": "2025-03-13T03:40:50+00:00",
    "summary": "Operations Research (OR) has been widely applied in various fields such as resource allocation, production planning, and supply chain management. However, addressing real-world OR problems requires OR experts to perform mathematical modeling and programmers to develop solution algorithms. This traditional method, heavily reliant on experts, is costly and has long development cycles, severely limiting the widespread adoption of OR techniques. Few have considered using Artificial Intelligence (AI) to replace professionals to achieve fully automated solutions for OR problems. We propose OR-LLM-Agent, the first AI agent that enables end-to-end automation for solving real-world OR problems. OR-LLM-Agent leverages the Chain-of-Thought (CoT) reasoning capabilities of Large Language Models (LLMs) to translate natural language problem descriptions into formal mathematical models and automatically generate Gurobi solver code. In OR-LLM-Agent, OR-CodeAgent is designed to automate code execution and repair within a sandbox environment, facilitating the derivation of the final solution. Due to the lack of dedicated benchmark datasets for evaluating the automated solving of OR problems, we construct a benchmark dataset comprising 83 real-world OR problems described in natural language. We conduct comparative experiments with state-of-the-art (SOTA) reasoning LLMs, including GPT-o3-mini, DeepSeek-R1, and Gemini 2.0 Flash Thinking. The OR-LLM-Agent achieved the highest pass rate of 100% and the highest solution accuracy of 85%, demonstrating the feasibility of automated OR problem-solving. Data and code have been publicly available at https://github.com/bwz96sco/or_llm_agent."
  },
  {
    "title": "ExtremeAIGC: Benchmarking LMM Vulnerability to AI-Generated Extremist Content",
    "url": "http://arxiv.org/abs/2503.09964v1",
    "arxiv_id": "2503.09964v1",
    "authors": [
      "Bhavik Chandna",
      "Mariam Aboujenane",
      "Usman Naseem"
    ],
    "published": "2025-03-13T02:10:29+00:00",
    "summary": "Large Multimodal Models (LMMs) are increasingly vulnerable to AI-generated extremist content, including photorealistic images and text, which can be used to bypass safety mechanisms and generate harmful outputs. However, existing datasets for evaluating LMM robustness offer limited exploration of extremist content, often lacking AI-generated images, diverse image generation models, and comprehensive coverage of historical events, which hinders a complete assessment of model vulnerabilities. To fill this gap, we introduce ExtremeAIGC, a benchmark dataset and evaluation framework designed to assess LMM vulnerabilities against such content. ExtremeAIGC simulates real-world events and malicious use cases by curating diverse text- and image-based examples crafted using state-of-the-art image generation techniques. Our study reveals alarming weaknesses in LMMs, demonstrating that even cutting-edge safety measures fail to prevent the generation of extremist material. We systematically quantify the success rates of various attack strategies, exposing critical gaps in current defenses and emphasizing the need for more robust mitigation strategies."
  },
  {
    "title": "Optimizing Fire Safety: Reducing False Alarms Using Advanced Machine Learning Techniques",
    "url": "http://arxiv.org/abs/2503.09960v1",
    "arxiv_id": "2503.09960v1",
    "authors": [
      "Muhammad Hassan Jamal",
      "Abdulwahab Alazeb",
      "Shahid Allah Bakhsh",
      "Wadii Boulila",
      "Syed Aziz Shah",
      "Aizaz Ahmad Khattak",
      "Muhammad Shahbaz Khan"
    ],
    "published": "2025-03-13T02:07:14+00:00",
    "summary": "Fire safety practices are important to reduce the extent of destruction caused by fire. While smoke alarms help save lives, firefighters struggle with the increasing number of false alarms. This paper presents a precise and efficient Weighted ensemble model for decreasing false alarms. It estimates the density, computes weights according to the high and low-density regions, forwards the high region weights to KNN and low region weights to XGBoost and combines the predictions. The proposed model is effective at reducing response time, increasing fire safety, and minimizing the damage that fires cause. A specifically designed dataset for smoke detection is utilized to test the proposed model. In addition, a variety of ML models, such as Logistic Regression (LR), Decision Tree (DT), Random Forest (RF), Nai:ve Bayes (NB), K-Nearest Neighbour (KNN), Support Vector Machine (SVM), Extreme Gradient Boosting (XGBoost), Adaptive Boosting (ADAB), have also been utilized. To maximize the use of the smoke detection dataset, all the algorithms utilize the SMOTE re-sampling technique. After evaluating the assessment criteria, this paper presents a concise summary of the comprehensive findings obtained by comparing the outcomes of all models."
  },
  {
    "title": "Identification and Classification of Human Performance related Challenges during Remote Driving",
    "url": "http://arxiv.org/abs/2503.09865v1",
    "arxiv_id": "2503.09865v1",
    "authors": [
      "Ole Hans",
      "J\u00fcrgen Adamy"
    ],
    "published": "2025-03-12T21:50:16+00:00",
    "summary": "Remote driving of vehicles is gaining in importance in the transportation sector, especially when Automated Driving Systems (ADSs) reach the limits of their system boundaries. This study investigates the challenges faced by human Remote Drivers (RDs) during remote driving, particularly focusing on the identification and classification of human performance-related challenges through a comprehensive analysis of real-world remote driving data Las Vegas. For this purpose, a total of 183 RD performance-related Safety Driver (SD) interventions were analyzed and classified using an introduced severity classification. As it is essential to prevent the need for SD interventions, this study identified and analyzed harsh driving events to detect an increased likelihood of interventions by the SD. In addition, the results of the subjective RD questionnaire are used to evaluate whether the objective metrics from SD interventions and harsh driving events can also be confirmed by the RDs and whether additional challenges can be uncovered. The analysis reveals learning curves, showing a significant decrease in SD interventions as RD experience increases. Early phases of remote driving experience, especially below 200 km of experience, showed the highest frequency of safety-related events, including braking too late for traffic signs and responding impatiently to other traffic participants. Over time, RDs follow defined rules for improving their control, with experience leading to less harsh braking, acceleration, and steering maneuvers. The study contributes to understanding the requirements of RDS, emphasizing the importance of targeted training to address human performance limitations. It further highlights the need for system improvements to address challenges like latency and the limited haptic feedback replaced by visual feedback, which affect the RDs' perception and vehicle control."
  },
  {
    "title": "Honey Trap or Romantic Utopia: A Case Study of Final Fantasy XIV Players PII Disclosure in Intimate Partner-Seeking Posts",
    "url": "http://arxiv.org/abs/2503.09832v1",
    "arxiv_id": "2503.09832v1",
    "authors": [
      "Yihao Zhou",
      "Tanusree Sharma"
    ],
    "published": "2025-03-12T20:53:06+00:00",
    "summary": "Massively multiplayer online games (MMOGs) can foster social interaction and relationship formation, but they pose specific privacy and safety challenges, especially in the context of mediating intimate interpersonal connections. To explore the potential risks, we conducted a case study on Final Fantasy XIV (FFXIV) players intimate partner seeking posts on social media. We analyzed 1,288 posts from a public Weibo account using Latent Dirichlet Allocation (LDA) topic modeling and thematic analysis. Our findings reveal that players disclose sensitive personal information and share vulnerabilities to establish trust but face difficulties in managing identity and privacy across multiple platforms. We also found that players expectations regarding intimate partner are diversified, and mismatch of expectations may leads to issues like privacy leakage or emotional exploitation. Based on our findings, we propose design implications for reducing privacy and safety risks and fostering healthier social interactions in virtual worlds."
  },
  {
    "title": "How good are deep learning methods for automated road safety analysis using video data? An experimental study",
    "url": "http://arxiv.org/abs/2503.09807v1",
    "arxiv_id": "2503.09807v1",
    "authors": [
      "Qingwu Liu",
      "Nicolas Saunier",
      "Guillaume-Alexandre Bilodeau"
    ],
    "published": "2025-03-12T20:17:50+00:00",
    "summary": "Image-based multi-object detection (MOD) and multi-object tracking (MOT) are advancing at a fast pace. A variety of 2D and 3D MOD and MOT methods have been developed for monocular and stereo cameras. Road safety analysis can benefit from those advancements. As crashes are rare events, surrogate measures of safety (SMoS) have been developed for safety analyses. (Semi-)Automated safety analysis methods extract road user trajectories to compute safety indicators, for example, Time-to-Collision (TTC) and Post-encroachment Time (PET). Inspired by the success of deep learning in MOD and MOT, we investigate three MOT methods, including one based on a stereo-camera, using the annotated KITTI traffic video dataset. Two post-processing steps, IDsplit and SS, are developed to improve the tracking results and investigate the factors influencing the TTC. The experimental results show that, despite some advantages in terms of the numbers of interactions or similarity to the TTC distributions, all the tested methods systematically over-estimate the number of interactions and under-estimate the TTC: they report more interactions and more severe interactions, making the road user interactions appear less safe than they are. Further efforts will be directed towards testing more methods and more data, in particular from roadside sensors, to verify the results and improve the performance."
  },
  {
    "title": "Constrained Language Generation with Discrete Diffusion Models",
    "url": "http://arxiv.org/abs/2503.09790v1",
    "arxiv_id": "2503.09790v1",
    "authors": [
      "Michael Cardei",
      "Jacob K Christopher",
      "Thomas Hartvigsen",
      "Brian R. Bartoldson",
      "Bhavya Kailkhura",
      "Ferdinando Fioretto"
    ],
    "published": "2025-03-12T19:48:12+00:00",
    "summary": "Constraints are critical in text generation as LLM outputs are often unreliable when it comes to ensuring generated outputs adhere to user defined instruction or general safety guidelines. To address this gap, we present Constrained Discrete Diffusion (CDD), a novel method for enforcing constraints on natural language by integrating discrete diffusion models with differentiable optimization. Unlike conventional text generators, which often rely on post-hoc filtering or model retraining for controllable generation, we propose imposing constraints directly into the discrete diffusion sampling process. We illustrate how this technique can be applied to satisfy a variety of natural language constraints, including (i) toxicity mitigation by preventing harmful content from emerging, (ii) character and sequence level lexical constraints, and (iii) novel molecule sequence generation with specific property adherence. Experimental results show that our constraint-aware procedure achieves high fidelity in meeting these requirements while preserving fluency and semantic coherence, outperforming auto-regressive and existing discrete diffusion approaches."
  },
  {
    "title": "Efficient Multi-Task Inferencing: Model Merging with Gromov-Wasserstein Feature Alignment",
    "url": "http://arxiv.org/abs/2503.09774v1",
    "arxiv_id": "2503.09774v1",
    "authors": [
      "Luyang Fang",
      "Ehsan Latif",
      "Haoran Lu",
      "Yifan Zhou",
      "Ping Ma",
      "Xiaoming Zhai"
    ],
    "published": "2025-03-12T19:20:33+00:00",
    "summary": "Automatic scoring of student responses enhances efficiency in education, but deploying a separate neural network for each task increases storage demands, maintenance efforts, and redundant computations. To address these challenges, this paper introduces the Gromov-Wasserstein Scoring Model Merging (GW-SMM) method, which merges models based on feature distribution similarities measured via the Gromov-Wasserstein distance. Our approach begins by extracting features from student responses using individual models, capturing both item-specific context and unique learned representations. The Gromov-Wasserstein distance then quantifies the similarity between these feature distributions, identifying the most compatible models for merging. Models exhibiting the smallest pairwise distances, typically in pairs or trios, are merged by combining only the shared layers preceding the classification head. This strategy results in a unified feature extractor while preserving separate classification heads for item-specific scoring. We validated our approach against human expert knowledge and a GPT-o1-based merging method. GW-SMM consistently outperformed both, achieving a higher micro F1 score, macro F1 score, exact match accuracy, and per-label accuracy. The improvements in micro F1 and per-label accuracy were statistically significant compared to GPT-o1-based merging (p=0.04, p=0.01). Additionally, GW-SMM reduced storage requirements by half without compromising much accuracy, demonstrating its computational efficiency alongside reliable scoring performance."
  },
  {
    "title": "How to Protect Yourself from 5G Radiation? Investigating LLM Responses to Implicit Misinformation",
    "url": "http://arxiv.org/abs/2503.09598v1",
    "arxiv_id": "2503.09598v1",
    "authors": [
      "Ruohao Guo",
      "Wei Xu",
      "Alan Ritter"
    ],
    "published": "2025-03-12T17:59:18+00:00",
    "summary": "As Large Language Models (LLMs) are widely deployed in diverse scenarios, the extent to which they could tacitly spread misinformation emerges as a critical safety concern. Current research primarily evaluates LLMs on explicit false statements, overlooking how misinformation often manifests subtly as unchallenged premises in real-world user interactions. We curated ECHOMIST, the first comprehensive benchmark for implicit misinformation, where the misinformed assumptions are embedded in a user query to LLMs. ECHOMIST is based on rigorous selection criteria and carefully curated data from diverse sources, including real-world human-AI conversations and social media interactions. We also introduce a new evaluation metric to measure whether LLMs can recognize and counter false information rather than amplify users' misconceptions. Through an extensive empirical study on a wide range of LLMs, including GPT-4, Claude, and Llama, we find that current models perform alarmingly poorly on this task, often failing to detect false premises and generating misleading explanations. Our findings underscore the critical need for an increased focus on implicit misinformation in LLM safety research."
  },
  {
    "title": "Towards Reasoning Era: A Survey of Long Chain-of-Thought for Reasoning Large Language Models",
    "url": "http://arxiv.org/abs/2503.09567v1",
    "arxiv_id": "2503.09567v1",
    "authors": [
      "Qiguang Chen",
      "Libo Qin",
      "Jinhao Liu",
      "Dengyun Peng",
      "Jiannan Guan",
      "Peng Wang",
      "Mengkang Hu",
      "Yuhang Zhou",
      "Te Gao",
      "Wangxiang Che"
    ],
    "published": "2025-03-12T17:35:03+00:00",
    "summary": "Recent advancements in reasoning with large language models (RLLMs), such as OpenAI-O1 and DeepSeek-R1, have demonstrated their impressive capabilities in complex domains like mathematics and coding. A central factor in their success lies in the application of long chain-of-thought (Long CoT) characteristics, which enhance reasoning abilities and enable the solution of intricate problems. However, despite these developments, a comprehensive survey on Long CoT is still lacking, limiting our understanding of its distinctions from traditional short chain-of-thought (Short CoT) and complicating ongoing debates on issues like \"overthinking\" and \"test-time scaling.\" This survey seeks to fill this gap by offering a unified perspective on Long CoT. (1) We first distinguish Long CoT from Short CoT and introduce a novel taxonomy to categorize current reasoning paradigms. (2) Next, we explore the key characteristics of Long CoT: deep reasoning, extensive exploration, and feasible reflection, which enable models to handle more complex tasks and produce more efficient, coherent outcomes compared to the shallower Short CoT. (3) We then investigate key phenomena such as the emergence of Long CoT with these characteristics, including overthinking, and test-time scaling, offering insights into how these processes manifest in practice. (4) Finally, we identify significant research gaps and highlight promising future directions, including the integration of multi-modal reasoning, efficiency improvements, and enhanced knowledge frameworks. By providing a structured overview, this survey aims to inspire future research and further the development of logical reasoning in artificial intelligence."
  },
  {
    "title": "Towards Reasoning Era: A Survey of Long Chain-of-Thought for Reasoning Large Language Models",
    "url": "http://arxiv.org/abs/2503.09567v2",
    "arxiv_id": "2503.09567v2",
    "authors": [
      "Qiguang Chen",
      "Libo Qin",
      "Jinhao Liu",
      "Dengyun Peng",
      "Jiannan Guan",
      "Peng Wang",
      "Mengkang Hu",
      "Yuhang Zhou",
      "Te Gao",
      "Wanxiang Che"
    ],
    "published": "2025-03-12T17:35:03+00:00",
    "summary": "Recent advancements in reasoning with large language models (RLLMs), such as OpenAI-O1 and DeepSeek-R1, have demonstrated their impressive capabilities in complex domains like mathematics and coding. A central factor in their success lies in the application of long chain-of-thought (Long CoT) characteristics, which enhance reasoning abilities and enable the solution of intricate problems. However, despite these developments, a comprehensive survey on Long CoT is still lacking, limiting our understanding of its distinctions from traditional short chain-of-thought (Short CoT) and complicating ongoing debates on issues like \"overthinking\" and \"test-time scaling.\" This survey seeks to fill this gap by offering a unified perspective on Long CoT. (1) We first distinguish Long CoT from Short CoT and introduce a novel taxonomy to categorize current reasoning paradigms. (2) Next, we explore the key characteristics of Long CoT: deep reasoning, extensive exploration, and feasible reflection, which enable models to handle more complex tasks and produce more efficient, coherent outcomes compared to the shallower Short CoT. (3) We then investigate key phenomena such as the emergence of Long CoT with these characteristics, including overthinking, and test-time scaling, offering insights into how these processes manifest in practice. (4) Finally, we identify significant research gaps and highlight promising future directions, including the integration of multi-modal reasoning, efficiency improvements, and enhanced knowledge frameworks. By providing a structured overview, this survey aims to inspire future research and further the development of logical reasoning in artificial intelligence."
  },
  {
    "title": "Search-R1: Training LLMs to Reason and Leverage Search Engines with Reinforcement Learning",
    "url": "http://arxiv.org/abs/2503.09516v1",
    "arxiv_id": "2503.09516v1",
    "authors": [
      "Bowen Jin",
      "Hansi Zeng",
      "Zhenrui Yue",
      "Dong Wang",
      "Hamed Zamani",
      "Jiawei Han"
    ],
    "published": "2025-03-12T16:26:39+00:00",
    "summary": "Efficiently acquiring external knowledge and up-to-date information is essential for effective reasoning and text generation in large language models (LLMs). Retrieval augmentation and tool-use training approaches where a search engine is treated as a tool lack complex multi-turn retrieval flexibility or require large-scale supervised data. Prompting advanced LLMs with reasoning capabilities during inference to use search engines is not optimal, since the LLM does not learn how to optimally interact with the search engine. This paper introduces Search-R1, an extension of the DeepSeek-R1 model where the LLM learns -- solely through reinforcement learning (RL) -- to autonomously generate (multiple) search queries during step-by-step reasoning with real-time retrieval. Search-R1 optimizes LLM rollouts with multi-turn search interactions, leveraging retrieved token masking for stable RL training and a simple outcome-based reward function. Experiments on seven question-answering datasets show that Search-R1 improves performance by 26% (Qwen2.5-7B), 21% (Qwen2.5-3B), and 10% (LLaMA3.2-3B) over SOTA baselines. This paper further provides empirical insights into RL optimization methods, LLM choices, and response length dynamics in retrieval-augmented reasoning. The code and model checkpoints are available at https://github.com/PeterGriffinJin/Search-R1."
  },
  {
    "title": "Reinforcement Learning is all You Need",
    "url": "http://arxiv.org/abs/2503.09512v1",
    "arxiv_id": "2503.09512v1",
    "authors": [
      "Yongsheng Lian"
    ],
    "published": "2025-03-12T16:22:28+00:00",
    "summary": "Inspired by the success of DeepSeek R1 in reasoning via reinforcement learning without human feedback, we train a 3B language model using the Countdown Game with pure reinforcement learning. Our model outperforms baselines on four of five benchmarks, demonstrating improved generalization beyond its training data. Notably, response length does not correlate with reasoning quality, and while \"aha moments\" emerge, they do not always yield correct answers. These findings highlight the potential of RL-only training for reasoning enhancement and suggest future work on refining reward structures to bridge emergent insights with accuracy."
  },
  {
    "title": "Experimental study of the convection in a thin cylindrical gas layer with imposed bottom and top fluxes and imposed side temperature",
    "url": "http://arxiv.org/abs/2503.09461v1",
    "arxiv_id": "2503.09461v1",
    "authors": [
      "Florian Rein",
      "Laure Car\u00e9nini",
      "Florian Fichot",
      "Benjamin Favier",
      "Michael Le Bars"
    ],
    "published": "2025-03-12T15:10:41+00:00",
    "summary": "We investigate convection in a thin cylindrical gas layer with an imposed flux at the bottom and a fixed temperature along the side, using a combination of direct numerical simulations and laboratory experiments. The experimental approach allows us to extend by two orders of magnitude the explored range in terms of flux Rayleigh number. We identify a scaling law governing the root-mean-square horizontal velocity and explain it through a dimensional analysis based on heat transport in the turbulent regime. Using particle image velocimetry, we experimentally confirm, for the most turbulent regimes, the presence of a drifting persistent pattern consisting of radial branches, as identified by Rein et al. (2023, J. Fluid Mech. 977, A26). We characterise the angular drift frequency and azimuthal wavenumber of this pattern as functions of the Rayleigh number. The system exhibits a wide distribution of heat flux across various time scales, with the longest fluctuations attributed to the branch pattern and the shortest to turbulent fluctuations. Consequently, the branch pattern must be considered to better forecast important wall heat flux fluctuations, a result of great relevance in the context of nuclear safety, the initial motivation for our study."
  },
  {
    "title": "Modelling lined rock caverns subject to hydrogen embrittlement and cyclic pressurisation in fractured rock masses",
    "url": "http://arxiv.org/abs/2503.09429v1",
    "arxiv_id": "2503.09429v1",
    "authors": [
      "Chenxi Zhao",
      "Haiyang Yu",
      "Zixin Zhang",
      "Qinghua Lei"
    ],
    "published": "2025-03-12T14:26:34+00:00",
    "summary": "The technology of lined rock cavern (LRC) with great geographical flexibility is a promising, cost-effective solution to underground hydrogen storage. However, the air-tight steel tanks used in this technology are susceptible to material degradation due to hydrogen embrittlement (HE), potentially leading to leakage and structural failure, especial for LRCs constructed in complex geological conditions. In this paper, we develop a 2D multiscale numerical model based on the finite element method to assess the impact of HE on the LRC performance in fractured rock masses under cyclic gas pressurisation. Within this framework, a large-scale model is used to simulate the deformation and damage evolution of both fractured rock and an LRC under in-situ stresses and internal gas pressurisation, while a small-scale model captures HE in the steel lining of the LRC. Our simulations reveal that damage in the rock, concrete, and steel degradation is strongly affected by pre-existing fractures and in-situ stresses. Our results also reveal the presence of a strong positive feedback between hydrogen concentration and stress redistribution in the steel lining. Moreover, a comparison between models with and without considering HE illuminates that hydrogen concentration significantly contributes to steel degradation, particularly during the long-term LRC operation, highlighting the critical role of HE in the safety and performance of the LRC. The findings and insights obtained from our work have important implications for the design optimisation and performance assessment of LRCs for sustainable underground hydrogen storage."
  },
  {
    "title": "Efficient dynamic modal load reconstruction using physics-informed Gaussian processes based on frequency-sparse Fourier basis functions",
    "url": "http://arxiv.org/abs/2503.09418v1",
    "arxiv_id": "2503.09418v1",
    "authors": [
      "Gledson Rodrigo Tondo",
      "Igor Kavrakov",
      "Guido Morgenthal"
    ],
    "published": "2025-03-12T14:16:27+00:00",
    "summary": "Knowledge of the force time history of a structure is essential to assess its behaviour, ensure safety and maintain reliability. However, direct measurement of external forces is often challenging due to sensor limitations, unknown force characteristics, or inaccessible load points. This paper presents an efficient dynamic load reconstruction method using physics-informed Gaussian processes (GP) based on frequency-sparse Fourier basis functions. The GP's covariance matrices are built using the description of the system dynamics, and the model is trained using structural response measurements. This provides support and interpretability to the machine learning model, in contrast to purely data-driven methods. In addition, the model filters out irrelevant components in the Fourier basis function by leveraging the sparsity of structural responses in the frequency domain, thereby reducing computational complexity during optimization. The trained model for structural responses is then integrated with the differential equation for a harmonic oscillator, creating a probabilistic dynamic load model that predicts load patterns without requiring force data during training. The model's effectiveness is validated through two case studies: a numerical model of a wind-excited 76-story building and an experiment using a physical scale model of the Lilleb{\\ae}lt Bridge in Denmark, excited by a servo motor. For both cases, validation of the reconstructed forces is provided using comparison metrics for several signal properties. The developed model holds potential for applications in structural health monitoring, damage prognosis, and load model validation."
  },
  {
    "title": "Ecosystem Evolution and Drivers across the Tibetan Plateau and Surrounding Regions",
    "url": "http://arxiv.org/abs/2503.09404v1",
    "arxiv_id": "2503.09404v1",
    "authors": [
      "Yiran Xie",
      "Xu Wang",
      "Yatong Qian",
      "Teng Liu",
      "Hao Fan",
      "Xiaosong Chen"
    ],
    "published": "2025-03-12T13:54:46+00:00",
    "summary": "The Tibetan Plateau (TP) and surrounding regions, vital to global energy and water cycles, are profoundly influenced by climate change and anthropogenic activities. Despite widespread attention to vegetation greening across the region since the 1980s, its underlying mechanisms remain poorly understood. This study employs the eigen microstates method to quantify vegetation greening dynamics using long-term remote sensing and reanalysis data. We identify two dominant modes that collectively explain more than 61% of the vegetation dynamics. The strong seasonal heterogeneity in the southern TP, primarily driven by radiation and agricultural activities, is reflected in the first mode, which accounts for 46.34% of the variance. The second mode, which explains 15% of the variance, is closely linked to deep soil moisture (SM3, 28 cm to 1 m). Compared to precipitation and surface soil moisture (SM1 and SM2, 0 to 28 cm), our results show that deep soil moisture exerts a stronger and more immediate influence on vegetation growth, with a one-month response time. This study provides a complexity theory-based framework to quantify vegetation dynamics and underscores the critical influence of deep soil moisture on greening patterns in the TP."
  },
  {
    "title": "Evaluating Reinforcement Learning Safety and Trustworthiness in Cyber-Physical Systems",
    "url": "http://arxiv.org/abs/2503.09388v1",
    "arxiv_id": "2503.09388v1",
    "authors": [
      "Katherine Dearstyne",
      "Pedro",
      "Alarcon Granadeno",
      "Theodore Chambers",
      "Jane Cleland-Huang"
    ],
    "published": "2025-03-12T13:33:07+00:00",
    "summary": "Cyber-Physical Systems (CPS) often leverage Reinforcement Learning (RL) techniques to adapt dynamically to changing environments and optimize performance. However, it is challenging to construct safety cases for RL components. We therefore propose the SAFE-RL (Safety and Accountability Framework for Evaluating Reinforcement Learning) for supporting the development, validation, and safe deployment of RL-based CPS. We adopt a design science approach to construct the framework and demonstrate its use in three RL applications in small Uncrewed Aerial systems (sUAS)"
  },
  {
    "title": "Fully-Synthetic Training for Visual Quality Inspection in Automotive Production",
    "url": "http://arxiv.org/abs/2503.09354v1",
    "arxiv_id": "2503.09354v1",
    "authors": [
      "Christoph Huber",
      "Dino Knoll",
      "Michael Guthe"
    ],
    "published": "2025-03-12T12:58:30+00:00",
    "summary": "Visual Quality Inspection plays a crucial role in modern manufacturing environments as it ensures customer safety and satisfaction. The introduction of Computer Vision (CV) has revolutionized visual quality inspection by improving the accuracy and efficiency of defect detection. However, traditional CV models heavily rely on extensive datasets for training, which can be costly, time-consuming, and error-prone. To overcome these challenges, synthetic images have emerged as a promising alternative. They offer a cost-effective solution with automatically generated labels. In this paper, we propose a pipeline for generating synthetic images using domain randomization. We evaluate our approach in three real inspection scenarios and demonstrate that an object detection model trained solely on synthetic data can outperform models trained on real images."
  },
  {
    "title": "MOAT: Evaluating LMMs for Capability Integration and Instruction Grounding",
    "url": "http://arxiv.org/abs/2503.09348v1",
    "arxiv_id": "2503.09348v1",
    "authors": [
      "Zhoutong Ye",
      "Mingze Sun",
      "Huan-ang Gao",
      "Chun Yu",
      "Yuanchun Shi"
    ],
    "published": "2025-03-12T12:49:31+00:00",
    "summary": "Large multimodal models (LMMs) have demonstrated significant potential as generalists in vision-language (VL) tasks. However, there remains a significant gap between state-of-the-art LMMs and human performance when it comes to complex tasks that require a combination of fundamental VL capabilities, as well as tasks involving the grounding of complex instructions. To thoroughly investigate the human-LMM gap and its underlying causes, we propose MOAT, a diverse benchmark with complex real-world VL tasks that are challenging for LMMs. Specifically, the tasks in MOAT require LMMs to engage in generalist problem solving by integrating fundamental VL capabilities such as reading text, counting, understanding spatial relations, grounding textual and visual instructions, etc. All these abilities fit into a taxonomy proposed by us that contains 10 fundamental VL capabilities, enabling MOAT to provide a fine-grained view of LMMs' strengths and weaknesses. Besides, MOAT is the first benchmark to explicitly evaluate LMMs' ability to ground complex text and visual instructions, which is essential to many real-world applications. We evaluate over 20 proprietary and open source LMMs, as well as humans, on MOAT, and found that humans achieved 82.7% accuracy while the best performing LMM (OpenAI o1) achieved only 38.8%. To guide future model development, we analyze common trends in our results and discuss the underlying causes of observed performance gaps between LMMs and humans, focusing on which VL capability forms the bottleneck in complex tasks, whether test time scaling improves performance on MOAT, and how tiling harms LMMs' capability to count. Code and data are available at https://cambrian-yzt.github.io/MOAT."
  },
  {
    "title": "Safer or Luckier? LLMs as Safety Evaluators Are Not Robust to Artifacts",
    "url": "http://arxiv.org/abs/2503.09347v1",
    "arxiv_id": "2503.09347v1",
    "authors": [
      "Hongyu Chen",
      "Seraphina Goldfarb-Tarrant"
    ],
    "published": "2025-03-12T12:49:02+00:00",
    "summary": "Large Language Models (LLMs) are increasingly employed as automated evaluators to assess the safety of generated content, yet their reliability in this role remains uncertain. This study evaluates a diverse set of 11 LLM judge models across critical safety domains, examining three key aspects: self-consistency in repeated judging tasks, alignment with human judgments, and susceptibility to input artifacts such as apologetic or verbose phrasing. Our findings reveal that biases in LLM judges can significantly distort the final verdict on which content source is safer, undermining the validity of comparative evaluations. Notably, apologetic language artifacts alone can skew evaluator preferences by up to 98\\%. Contrary to expectations, larger models do not consistently exhibit greater robustness, while smaller models sometimes show higher resistance to specific artifacts. To mitigate LLM evaluator robustness issues, we investigate jury-based evaluations aggregating decisions from multiple models. Although this approach both improves robustness and enhances alignment to human judgements, artifact sensitivity persists even with the best jury configurations. These results highlight the urgent need for diversified, artifact-resistant methodologies to ensure reliable safety assessments."
  },
  {
    "title": "CyberLLMInstruct: A New Dataset for Analysing Safety of Fine-Tuned LLMs Using Cyber Security Data",
    "url": "http://arxiv.org/abs/2503.09334v1",
    "arxiv_id": "2503.09334v1",
    "authors": [
      "Adel ElZemity",
      "Budi Arief",
      "Shujun Li"
    ],
    "published": "2025-03-12T12:29:27+00:00",
    "summary": "The integration of large language models (LLMs) into cyber security applications presents significant opportunities, such as enhancing threat analysis and malware detection, but can also introduce critical risks and safety concerns, including personal data leakage and automated generation of new malware. To address these challenges, we developed CyberLLMInstruct, a dataset of 54,928 instruction-response pairs spanning cyber security tasks such as malware analysis, phishing simulations, and zero-day vulnerabilities. The dataset was constructed through a multi-stage process. This involved sourcing data from multiple resources, filtering and structuring it into instruction-response pairs, and aligning it with real-world scenarios to enhance its applicability. Seven open-source LLMs were chosen to test the usefulness of CyberLLMInstruct: Phi 3 Mini 3.8B, Mistral 7B, Qwen 2.5 7B, Llama 3 8B, Llama 3.1 8B, Gemma 2 9B, and Llama 2 70B. In our primary example, we rigorously assess the safety of fine-tuned models using the OWASP top 10 framework, finding that fine-tuning reduces safety resilience across all tested LLMs and every adversarial attack (e.g., the security score of Llama 3.1 8B against prompt injection drops from 0.95 to 0.15). In our second example, we show that these same fine-tuned models can also achieve up to 92.50 percent accuracy on the CyberMetric benchmark. These findings highlight a trade-off between performance and safety, showing the importance of adversarial testing and further research into fine-tuning methodologies that can mitigate safety risks while still improving performance across diverse datasets and domains. All scripts required to reproduce the dataset, along with examples and relevant resources for replicating our results, will be made available upon the paper's acceptance."
  },
  {
    "title": "Earth as an Exoplanet: Investigating the effects of cloud variability on the direct-imaging of atmospheres",
    "url": "http://arxiv.org/abs/2503.09136v1",
    "arxiv_id": "2503.09136v1",
    "authors": [
      "Soumil Kelkar",
      "Prabal Saxena",
      "Ravi Kopparapu",
      "Joy Monteiro"
    ],
    "published": "2025-03-12T07:57:40+00:00",
    "summary": "A planet's spectrum is dynamic and only represents a time-dependent snapshot of its properties. Changing atmospheric conditions due to climate and weather patterns, particularly variation in cloud cover, can significantly affect the spectrum in ways that complicate the understanding of a planet's baseline atmospheric properties. Variable cloud cover and cloud properties affect the detectability of atmospheric constituents, and also greatly influence the radiative transfer that determines a planet's spectrum. This has considerable implications for direct imaging observations of potentially habitable exoplanets and thus it is critical to study and characterize the effects of clouds on their spectra. Clouds have been extensively modeled before and their effects have been incorporated across climate frameworks spanning a spectrum of complexity. Given the challenges associated with modeling clouds, we adopt a novel approach in this work to study the effects of clouds by using real-time cloud data from Earth observations. Treating Earth as an exoplanet and using detailed observations from the MERRA 2 data collection, we quantify the effects of cloud variability on the spectrum as well as on the detectability of atmospheric constituents, specifically biomarkers like O2, O3 and H2O. The coverage and vertical position of clouds significantly affects the SNRs of these gases and subsequently their detectability in exo-Earth atmospheres. Moreover, we show that variations in the amount of cloud cover will potentially confound efforts to retrieve a stable baseline atmosphere for a planet. This work has important applications to future direct-imaging missions like the Habitable Worlds Observatory (HWO)."
  },
  {
    "title": "Using Co-Located Range and Doppler Radars for Initial Orbit Determination",
    "url": "http://arxiv.org/abs/2503.09135v1",
    "arxiv_id": "2503.09135v1",
    "authors": [
      "Cristina Parigini",
      "Laura Pirovano",
      "Roberto Armellin",
      "Darren McKnight",
      "Adam Marsh",
      "Tom Reddell"
    ],
    "published": "2025-03-12T07:45:47+00:00",
    "summary": "With debris larger than 1 cm in size estimated to be over one million, precise cataloging efforts are essential to ensure space operations' safety. Compounding this challenge is the oversubscribed problem, where the sheer volume of space objects surpasses ground-based observatories' observational capacity. This results in sparse, brief observations and extended intervals before image acquisition. LeoLabs' network of phased-array radars addresses this need by reliably tracking 10 cm objects and larger in low Earth orbit with 10 independent radars across six sites. While LeoLabs tracklets are extremely short, they hold much more information than typical radar observations. Furthermore, two tracklets are generally available, separated by a couple of minutes. Thus, this paper develops a tailored approach to initialize state and uncertainty from a single or pair of tracklets. Through differential algebra, the initial orbit determination provides the state space compatible with the available measurements, namely an orbit set. This practice, widely used in previous research, allows for efficient data association of different tracklets, thus enabling the addition of accurate tracks to the catalog following their independent initialization. The algorithm's efficacy is tested using real measurements, evaluating the IOD solution's accuracy and ability to predict the next passage from a single or a pair of tracklets."
  },
  {
    "title": "Probing Latent Subspaces in LLM for AI Security: Identifying and Manipulating Adversarial States",
    "url": "http://arxiv.org/abs/2503.09066v1",
    "arxiv_id": "2503.09066v1",
    "authors": [
      "Xin Wei Chia",
      "Jonathan Pan"
    ],
    "published": "2025-03-12T04:59:22+00:00",
    "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across various tasks, yet they remain vulnerable to adversarial manipulations such as jailbreaking via prompt injection attacks. These attacks bypass safety mechanisms to generate restricted or harmful content. In this study, we investigated the underlying latent subspaces of safe and jailbroken states by extracting hidden activations from a LLM. Inspired by attractor dynamics in neuroscience, we hypothesized that LLM activations settle into semi stable states that can be identified and perturbed to induce state transitions. Using dimensionality reduction techniques, we projected activations from safe and jailbroken responses to reveal latent subspaces in lower dimensional spaces. We then derived a perturbation vector that when applied to safe representations, shifted the model towards a jailbreak state. Our results demonstrate that this causal intervention results in statistically significant jailbreak responses in a subset of prompts. Next, we probed how these perturbations propagate through the model's layers, testing whether the induced state change remains localized or cascades throughout the network. Our findings indicate that targeted perturbations induced distinct shifts in activations and model responses. Our approach paves the way for potential proactive defenses, shifting from traditional guardrail based methods to preemptive, model agnostic techniques that neutralize adversarial states at the representation level."
  },
  {
    "title": "ManeuverGPT Agentic Control for Safe Autonomous Stunt Maneuvers",
    "url": "http://arxiv.org/abs/2503.09035v1",
    "arxiv_id": "2503.09035v1",
    "authors": [
      "Shawn Azdam",
      "Pranav Doma",
      "Aliasghar Moj Arab"
    ],
    "published": "2025-03-12T03:51:41+00:00",
    "summary": "The next generation of active safety features in autonomous vehicles should be capable of safely executing evasive hazard-avoidance maneuvers akin to those performed by professional stunt drivers to achieve high-agility motion at the limits of vehicle handling. This paper presents a novel framework, ManeuverGPT, for generating and executing high-dynamic stunt maneuvers in autonomous vehicles using large language model (LLM)-based agents as controllers. We target aggressive maneuvers, such as J-turns, within the CARLA simulation environment and demonstrate an iterative, prompt-based approach to refine vehicle control parameters, starting tabula rasa without retraining model weights. We propose an agentic architecture comprised of three specialized agents (1) a Query Enricher Agent for contextualizing user commands, (2) a Driver Agent for generating maneuver parameters, and (3) a Parameter Validator Agent that enforces physics-based and safety constraints. Experimental results demonstrate successful J-turn execution across multiple vehicle models through textual prompts that adapt to differing vehicle dynamics. We evaluate performance via established success criteria and discuss limitations regarding numeric precision and scenario complexity. Our findings underscore the potential of LLM-driven control for flexible, high-dynamic maneuvers, while highlighting the importance of hybrid approaches that combine language-based reasoning with algorithmic validation."
  },
  {
    "title": "Traffic Regulation-aware Path Planning with Regulation Databases and Vision-Language Models",
    "url": "http://arxiv.org/abs/2503.09024v1",
    "arxiv_id": "2503.09024v1",
    "authors": [
      "Xu Han",
      "Zhiwen Wu",
      "Xin Xia",
      "Jiaqi Ma"
    ],
    "published": "2025-03-12T03:21:03+00:00",
    "summary": "This paper introduces and tests a framework integrating traffic regulation compliance into automated driving systems (ADS). The framework enables ADS to follow traffic laws and make informed decisions based on the driving environment. Using RGB camera inputs and a vision-language model (VLM), the system generates descriptive text to support a regulation-aware decision-making process, ensuring legal and safe driving practices. This information is combined with a machine-readable ADS regulation database to guide future driving plans within legal constraints. Key features include: 1) a regulation database supporting ADS decision-making, 2) an automated process using sensor input for regulation-aware path planning, and 3) validation in both simulated and real-world environments. Particularly, the real-world vehicle tests not only assess the framework's performance but also evaluate the potential and challenges of VLMs to solve complex driving problems by integrating detection, reasoning, and planning. This work enhances the legality, safety, and public trust in ADS, representing a significant step forward in the field."
  },
  {
    "title": "JBFuzz: Jailbreaking LLMs Efficiently and Effectively Using Fuzzing",
    "url": "http://arxiv.org/abs/2503.08990v1",
    "arxiv_id": "2503.08990v1",
    "authors": [
      "Vasudev Gohil"
    ],
    "published": "2025-03-12T01:52:17+00:00",
    "summary": "Large language models (LLMs) have shown great promise as language understanding and decision making tools, and they have permeated various aspects of our everyday life. However, their widespread availability also comes with novel risks, such as generating harmful, unethical, or offensive content, via an attack called jailbreaking. Despite extensive efforts from LLM developers to align LLMs using human feedback, they are still susceptible to jailbreak attacks. To tackle this issue, researchers often employ red-teaming to understand and investigate jailbreak prompts. However, existing red-teaming approaches lack effectiveness, scalability, or both. To address these issues, we propose JBFuzz, a novel effective, automated, and scalable red-teaming technique for jailbreaking LLMs.   JBFuzz is inspired by the success of fuzzing for detecting bugs/vulnerabilities in software. We overcome three challenges related to effectiveness and scalability by devising novel seed prompts, a lightweight mutation engine, and a lightweight and accurate evaluator for guiding the fuzzer. Assimilating all three solutions results in a potent fuzzer that only requires black-box access to the target LLM. We perform extensive experimental evaluation of JBFuzz using nine popular and widely-used LLMs. We find that JBFuzz successfully jailbreaks all LLMs for various harmful/unethical questions, with an average attack success rate of 99%. We also find that JBFuzz is extremely efficient as it jailbreaks a given LLM for a given question in 60 seconds on average. Our work highlights the susceptibility of the state-of-the-art LLMs to jailbreak attacks even after safety alignment, and serves as a valuable red-teaming tool for LLM developers."
  },
  {
    "title": "Leaky Batteries: A Novel Set of Side-Channel Attacks on Electric Vehicles",
    "url": "http://arxiv.org/abs/2503.08956v1",
    "arxiv_id": "2503.08956v1",
    "authors": [
      "Francesco Marchiori",
      "Mauro Conti"
    ],
    "published": "2025-03-11T23:18:26+00:00",
    "summary": "Advancements in battery technology have accelerated the adoption of Electric Vehicles (EVs) due to their environmental benefits. However, their growing sophistication introduces security and privacy challenges. Often seen as mere operational data, battery consumption patterns can unintentionally reveal critical information exploitable for malicious purposes. These risks go beyond privacy, impacting vehicle security and regulatory compliance. Despite these concerns, current research has largely overlooked the broader implications of battery consumption data exposure. As EVs integrate further into smart transportation networks, addressing these gaps is crucial to ensure their safety, reliability, and resilience. In this work, we introduce a novel class of side-channel attacks that exploit EV battery data to extract sensitive user information. Leveraging only battery consumption patterns, we demonstrate a methodology to accurately identify the EV driver and their driving style, determine the number of occupants, and infer the vehicle's start and end locations when user habits are known. We utilize several machine learning models and feature extraction techniques to analyze EV power consumption patterns, validating our approach on simulated and real-world datasets collected from actual drivers. Our attacks achieve an average success rate of 95.4% across all attack objectives. Our findings highlight the privacy risks associated with EV battery data, emphasizing the need for stronger protections to safeguard user privacy and vehicle security."
  },
  {
    "title": "Data-Driven Modeling of Amyloid-beta Targeted Antibodies for Alzheimer's Disease",
    "url": "http://arxiv.org/abs/2503.08938v1",
    "arxiv_id": "2503.08938v1",
    "authors": [
      "Kobra Rabiei",
      "Jeffrey R. Petrella",
      "Suzanne Lenhart",
      "Chun Liu",
      "P. Murali Doraiswamy",
      "Wenrui Hao"
    ],
    "published": "2025-03-11T22:35:29+00:00",
    "summary": "Alzheimer's disease (AD) is driven by the accumulation of amyloid-beta (Abeta) proteins in the brain, leading to memory loss and cognitive decline. While monoclonal antibodies targeting Abetahave been approved, optimizing their use to maximize benefits while minimizing side effects remains a challenge. This study develops a mathematical model to describe Abeta aggregation, capturing its progression from monomers to toxic oligomers, protofibrils, and fibrils using mass-action kinetics and coarse-grained modeling. The model is calibrated with experimental data, incorporating parameter estimation and sensitivity analysis to ensure accuracy. An optimal control framework is introduced to determine the best drug dosing strategy that reduces toxic Abeta aggregates while minimizing adverse effects, such as amyloid-related imaging abnormalities (ARIA). Results indicate that Donanemab achieves the greatest reduction in fibrils. This work provides a quantitative framework for optimizing AD treatment strategies, offering insights into balancing therapeutic efficacy and safety."
  },
  {
    "title": "Backtracking for Safety",
    "url": "http://arxiv.org/abs/2503.08919v1",
    "arxiv_id": "2503.08919v1",
    "authors": [
      "Bilgehan Sel",
      "Dingcheng Li",
      "Phillip Wallis",
      "Vaishakh Keshava",
      "Ming Jin",
      "Siddhartha Reddy Jonnalagadda"
    ],
    "published": "2025-03-11T22:04:22+00:00",
    "summary": "Large language models (LLMs) have demonstrated remarkable capabilities across various tasks, but ensuring their safety and alignment with human values remains crucial. Current safety alignment methods, such as supervised fine-tuning and reinforcement learning-based approaches, can exhibit vulnerabilities to adversarial attacks and often result in shallow safety alignment, primarily focusing on preventing harmful content in the initial tokens of the generated output. While methods like resetting can help recover from unsafe generations by discarding previous tokens and restarting the generation process, they are not well-suited for addressing nuanced safety violations like toxicity that may arise within otherwise benign and lengthy generations. In this paper, we propose a novel backtracking method designed to address these limitations. Our method allows the model to revert to a safer generation state, not necessarily at the beginning, when safety violations occur during generation. This approach enables targeted correction of problematic segments without discarding the entire generated text, thereby preserving efficiency. We demonstrate that our method dramatically reduces toxicity appearing through the generation process with minimal impact to efficiency."
  },
  {
    "title": "SICNav-Diffusion: Safe and Interactive Crowd Navigation with Diffusion Trajectory Predictions",
    "url": "http://arxiv.org/abs/2503.08858v1",
    "arxiv_id": "2503.08858v1",
    "authors": [
      "Sepehr Samavi",
      "Anthony Lem",
      "Fumiaki Sato",
      "Sirui Chen",
      "Qiao Gu",
      "Keijiro Yano",
      "Angela P. Schoellig",
      "Florian Shkurti"
    ],
    "published": "2025-03-11T19:54:50+00:00",
    "summary": "To navigate crowds without collisions, robots must interact with humans by forecasting their future motion and reacting accordingly. While learning-based prediction models have shown success in generating likely human trajectory predictions, integrating these stochastic models into a robot controller presents several challenges. The controller needs to account for interactive coupling between planned robot motion and human predictions while ensuring both predictions and robot actions are safe (i.e. collision-free). To address these challenges, we present a receding horizon crowd navigation method for single-robot multi-human environments. We first propose a diffusion model to generate joint trajectory predictions for all humans in the scene. We then incorporate these multi-modal predictions into a SICNav Bilevel MPC problem that simultaneously solves for a robot plan (upper-level) and acts as a safety filter to refine the predictions for non-collision (lower-level). Combining planning and prediction refinement into one bilevel problem ensures that the robot plan and human predictions are coupled. We validate the open-loop trajectory prediction performance of our diffusion model on the commonly used ETH/UCY benchmark and evaluate the closed-loop performance of our robot navigation method in simulation and extensive real-robot experiments demonstrating safe, efficient, and reactive robot motion."
  },
  {
    "title": "Intrinsic momentum transport driven by almost-rational surfaces in tokamak plasmas",
    "url": "http://arxiv.org/abs/2503.08793v1",
    "arxiv_id": "2503.08793v1",
    "authors": [
      "Justin Ball",
      "Arnas Volcokas",
      "Stephan Brunner"
    ],
    "published": "2025-03-11T18:07:18+00:00",
    "summary": "We demonstrate that a symmetry of the local gyrokinetic model is broken when the safety factor q is almost (but not exactly) a rational number and magnetic shear is $\\hat{s} \\approx 0$. Tokamaks with such a q profile will spontaneously rotate due to turbulent momentum transport. Nonlinear gyrokinetic simulations indicate this mechanism is significantly stronger than all other drives of intrinsic rotation. It also generates intrinsic electric current that pulls q towards rational values, potentially aiding non-inductive current drive. This is likely important in the triggering of internal transport barriers."
  },
  {
    "title": "CoLMDriver: LLM-based Negotiation Benefits Cooperative Autonomous Driving",
    "url": "http://arxiv.org/abs/2503.08683v1",
    "arxiv_id": "2503.08683v1",
    "authors": [
      "Changxing Liu",
      "Genjia Liu",
      "Zijun Wang",
      "Jinchang Yang",
      "Siheng Chen"
    ],
    "published": "2025-03-11T17:58:42+00:00",
    "summary": "Vehicle-to-vehicle (V2V) cooperative autonomous driving holds great promise for improving safety by addressing the perception and prediction uncertainties inherent in single-agent systems. However, traditional cooperative methods are constrained by rigid collaboration protocols and limited generalization to unseen interactive scenarios. While LLM-based approaches offer generalized reasoning capabilities, their challenges in spatial planning and unstable inference latency hinder their direct application in cooperative driving. To address these limitations, we propose CoLMDriver, the first full-pipeline LLM-based cooperative driving system, enabling effective language-based negotiation and real-time driving control. CoLMDriver features a parallel driving pipeline with two key components: (i) an LLM-based negotiation module under an actor-critic paradigm, which continuously refines cooperation policies through feedback from previous decisions of all vehicles; and (ii) an intention-guided waypoint generator, which translates negotiation outcomes into executable waypoints. Additionally, we introduce InterDrive, a CARLA-based simulation benchmark comprising 10 challenging interactive driving scenarios for evaluating V2V cooperation. Experimental results demonstrate that CoLMDriver significantly outperforms existing approaches, achieving an 11% higher success rate across diverse highly interactive V2V driving scenarios. Code will be released on https://github.com/cxliu0314/CoLMDriver."
  },
  {
    "title": "Chain-of-Thought Reasoning In The Wild Is Not Always Faithful",
    "url": "http://arxiv.org/abs/2503.08679v1",
    "arxiv_id": "2503.08679v1",
    "authors": [
      "Iv\u00e1n Arcuschin",
      "Jett Janiak",
      "Robert Krzyzanowski",
      "Senthooran Rajamanoharan",
      "Neel Nanda",
      "Arthur Conmy"
    ],
    "published": "2025-03-11T17:56:30+00:00",
    "summary": "Chain-of-Thought (CoT) reasoning has significantly advanced state-of-the-art AI capabilities. However, recent studies have shown that CoT reasoning is not always faithful, i.e. CoT reasoning does not always reflect how models arrive at conclusions. So far, most of these studies have focused on unfaithfulness in unnatural contexts where an explicit bias has been introduced. In contrast, we show that unfaithful CoT can occur on realistic prompts with no artificial bias. Our results reveal concerning rates of several forms of unfaithful reasoning in frontier models: Sonnet 3.7 (30.6%), DeepSeek R1 (15.8%) and ChatGPT-4o (12.6%) all answer a high proportion of question pairs unfaithfully. Specifically, we find that models rationalize their implicit biases in answers to binary questions (\"implicit post-hoc rationalization\"). For example, when separately presented with the questions \"Is X bigger than Y?\" and \"Is Y bigger than X?\", models sometimes produce superficially coherent arguments to justify answering Yes to both questions or No to both questions, despite such responses being logically contradictory. We also investigate restoration errors (Dziri et al., 2023), where models make and then silently correct errors in their reasoning, and unfaithful shortcuts, where models use clearly illogical reasoning to simplify solving problems in Putnam questions (a hard benchmark). Our findings raise challenges for AI safety work that relies on monitoring CoT to detect undesired behavior."
  },
  {
    "title": "Chain-of-Thought Reasoning In The Wild Is Not Always Faithful",
    "url": "http://arxiv.org/abs/2503.08679v2",
    "arxiv_id": "2503.08679v2",
    "authors": [
      "Iv\u00e1n Arcuschin",
      "Jett Janiak",
      "Robert Krzyzanowski",
      "Senthooran Rajamanoharan",
      "Neel Nanda",
      "Arthur Conmy"
    ],
    "published": "2025-03-11T17:56:30+00:00",
    "summary": "Chain-of-Thought (CoT) reasoning has significantly advanced state-of-the-art AI capabilities. However, recent studies have shown that CoT reasoning is not always faithful, i.e. CoT reasoning does not always reflect how models arrive at conclusions. So far, most of these studies have focused on unfaithfulness in unnatural contexts where an explicit bias has been introduced. In contrast, we show that unfaithful CoT can occur on realistic prompts with no artificial bias. Our results reveal non-negligible rates of several forms of unfaithful reasoning in frontier models: Sonnet 3.7 (16.3%), DeepSeek R1 (5.3%) and ChatGPT-4o (7.0%) all answer a notable proportion of question pairs unfaithfully. Specifically, we find that models rationalize their implicit biases in answers to binary questions (\"implicit post-hoc rationalization\"). For example, when separately presented with the questions \"Is X bigger than Y?\" and \"Is Y bigger than X?\", models sometimes produce superficially coherent arguments to justify answering Yes to both questions or No to both questions, despite such responses being logically contradictory. We also investigate restoration errors (Dziri et al., 2023), where models make and then silently correct errors in their reasoning, and unfaithful shortcuts, where models use clearly illogical reasoning to simplify solving problems in Putnam questions (a hard benchmark). Our findings raise challenges for AI safety work that relies on monitoring CoT to detect undesired behavior."
  },
  {
    "title": "AgentOrca: A Dual-System Framework to Evaluate Language Agents on Operational Routine and Constraint Adherence",
    "url": "http://arxiv.org/abs/2503.08669v1",
    "arxiv_id": "2503.08669v1",
    "authors": [
      "Zekun Li",
      "Shinda Huang",
      "Jiangtian Wang",
      "Nathan Zhang",
      "Antonis Antoniades",
      "Wenyue Hua",
      "Kaijie Zhu",
      "Sirui Zeng",
      "William Yang Wang",
      "Xifeng Yan"
    ],
    "published": "2025-03-11T17:53:02+00:00",
    "summary": "As language agents progressively automate critical tasks across domains, their ability to operate within operational constraints and safety protocols becomes essential. While extensive research has demonstrated these agents' effectiveness in downstream task completion, their reliability in following operational procedures and constraints remains largely unexplored. To this end, we present AgentOrca, a dual-system framework for evaluating language agents' compliance with operational constraints and routines. Our framework encodes action constraints and routines through both natural language prompts for agents and corresponding executable code serving as ground truth for automated verification. Through an automated pipeline of test case generation and evaluation across five real-world domains, we quantitatively assess current language agents' adherence to operational constraints. Our findings reveal notable performance gaps among state-of-the-art models, with large reasoning models like o1 demonstrating superior compliance while others show significantly lower performance, particularly when encountering complex constraints or user persuasion attempts."
  },
  {
    "title": "Generating Robot Constitutions & Benchmarks for Semantic Safety",
    "url": "http://arxiv.org/abs/2503.08663v1",
    "arxiv_id": "2503.08663v1",
    "authors": [
      "Pierre Sermanet",
      "Anirudha Majumdar",
      "Alex Irpan",
      "Dmitry Kalashnikov",
      "Vikas Sindhwani"
    ],
    "published": "2025-03-11T17:50:47+00:00",
    "summary": "Until recently, robotics safety research was predominantly about collision avoidance and hazard reduction in the immediate vicinity of a robot. Since the advent of large vision and language models (VLMs), robots are now also capable of higher-level semantic scene understanding and natural language interactions with humans. Despite their known vulnerabilities (e.g. hallucinations or jail-breaking), VLMs are being handed control of robots capable of physical contact with the real world. This can lead to dangerous behaviors, making semantic safety for robots a matter of immediate concern. Our contributions in this paper are two fold: first, to address these emerging risks, we release the ASIMOV Benchmark, a large-scale and comprehensive collection of datasets for evaluating and improving semantic safety of foundation models serving as robot brains. Our data generation recipe is highly scalable: by leveraging text and image generation techniques, we generate undesirable situations from real-world visual scenes and human injury reports from hospitals. Secondly, we develop a framework to automatically generate robot constitutions from real-world data to steer a robot's behavior using Constitutional AI mechanisms. We propose a novel auto-amending process that is able to introduce nuances in written rules of behavior; this can lead to increased alignment with human preferences on behavior desirability and safety. We explore trade-offs between generality and specificity across a diverse set of constitutions of different lengths, and demonstrate that a robot is able to effectively reject unconstitutional actions. We measure a top alignment rate of 84.3% on the ASIMOV Benchmark using generated constitutions, outperforming no-constitution baselines and human-written constitutions. Data is available at asimov-benchmark.github.io"
  },
  {
    "title": "Exploiting Instruction-Following Retrievers for Malicious Information Retrieval",
    "url": "http://arxiv.org/abs/2503.08644v1",
    "arxiv_id": "2503.08644v1",
    "authors": [
      "Parishad BehnamGhader",
      "Nicholas Meade",
      "Siva Reddy"
    ],
    "published": "2025-03-11T17:36:53+00:00",
    "summary": "Instruction-following retrievers have been widely adopted alongside LLMs in real-world applications, but little work has investigated the safety risks surrounding their increasing search capabilities. We empirically study the ability of retrievers to satisfy malicious queries, both when used directly and when used in a retrieval augmented generation-based setup. Concretely, we investigate six leading retrievers, including NV-Embed and LLM2Vec, and find that given malicious requests, most retrievers can (for >50% of queries) select relevant harmful passages. For example, LLM2Vec correctly selects passages for 61.35% of our malicious queries. We further uncover an emerging risk with instruction-following retrievers, where highly relevant harmful information can be surfaced by exploiting their instruction-following capabilities. Finally, we show that even safety-aligned LLMs, such as Llama3, can satisfy malicious requests when provided with harmful retrieved passages in-context. In summary, our findings underscore the malicious misuse risks associated with increasing retriever capability."
  },
  {
    "title": "DeepReview: Improving LLM-based Paper Review with Human-like Deep Thinking Process",
    "url": "http://arxiv.org/abs/2503.08569v1",
    "arxiv_id": "2503.08569v1",
    "authors": [
      "Minjun Zhu",
      "Yixuan Weng",
      "Linyi Yang",
      "Yue Zhang"
    ],
    "published": "2025-03-11T15:59:43+00:00",
    "summary": "Large Language Models (LLMs) are increasingly utilized in scientific research assessment, particularly in automated paper review. However, existing LLM-based review systems face significant challenges, including limited domain expertise, hallucinated reasoning, and a lack of structured evaluation. To address these limitations, we introduce DeepReview, a multi-stage framework designed to emulate expert reviewers by incorporating structured analysis, literature retrieval, and evidence-based argumentation. Using DeepReview-13K, a curated dataset with structured annotations, we train DeepReviewer-14B, which outperforms CycleReviewer-70B with fewer tokens. In its best mode, DeepReviewer-14B achieves win rates of 88.21\\% and 80.20\\% against GPT-o1 and DeepSeek-R1 in evaluations. Our work sets a new benchmark for LLM-based paper review, with all resources publicly available. The code, model, dataset and demo have be released in http://ai-researcher.net."
  },
  {
    "title": "Can We Detect Failures Without Failure Data? Uncertainty-Aware Runtime Failure Detection for Imitation Learning Policies",
    "url": "http://arxiv.org/abs/2503.08558v1",
    "arxiv_id": "2503.08558v1",
    "authors": [
      "Chen Xu",
      "Tony Khuong Nguyen",
      "Emma Dixon",
      "Christopher Rodriguez",
      "Patrick Miller",
      "Robert Lee",
      "Paarth Shah",
      "Rares Ambrus",
      "Haruki Nishimura",
      "Masha Itkina"
    ],
    "published": "2025-03-11T15:47:12+00:00",
    "summary": "Recent years have witnessed impressive robotic manipulation systems driven by advances in imitation learning and generative modeling, such as diffusion- and flow-based approaches. As robot policy performance increases, so does the complexity and time horizon of achievable tasks, inducing unexpected and diverse failure modes that are difficult to predict a priori. To enable trustworthy policy deployment in safety-critical human environments, reliable runtime failure detection becomes important during policy inference. However, most existing failure detection approaches rely on prior knowledge of failure modes and require failure data during training, which imposes a significant challenge in practicality and scalability. In response to these limitations, we present FAIL-Detect, a modular two-stage approach for failure detection in imitation learning-based robotic manipulation. To accurately identify failures from successful training data alone, we frame the problem as sequential out-of-distribution (OOD) detection. We first distill policy inputs and outputs into scalar signals that correlate with policy failures and capture epistemic uncertainty. FAIL-Detect then employs conformal prediction (CP) as a versatile framework for uncertainty quantification with statistical guarantees. Empirically, we thoroughly investigate both learned and post-hoc scalar signal candidates on diverse robotic manipulation tasks. Our experiments show learned signals to be mostly consistently effective, particularly when using our novel flow-based density estimator. Furthermore, our method detects failures more accurately and faster than state-of-the-art (SOTA) failure detection baselines. These results highlight the potential of FAIL-Detect to enhance the safety and reliability of imitation learning-based robotic systems as they progress toward real-world deployment."
  },
  {
    "title": "An Analysis of Safety Guarantees in Multi-Task Bayesian Optimization",
    "url": "http://arxiv.org/abs/2503.08555v1",
    "arxiv_id": "2503.08555v1",
    "authors": [
      "Jannis O. Luebsen",
      "Annika Eichler"
    ],
    "published": "2025-03-11T15:45:37+00:00",
    "summary": "In many practical scenarios of black box optimization, the objective function is subject to constraints that must be satisfied to avoid undesirable outcomes. Such constraints are typically unknown and must be learned during optimization. Safe Bayesian optimization aims to find the global optimum while ensuring that the constraints are satisfied with high probability. However, it is often sample-inefficient due to the small initial feasible set, which requires expansion by evaluating the objective or constraint functions, limiting its applicability to low-dimensional or inexpensive problems. To enhance sample efficiency, additional information from cheap simulations can be leveraged, albeit at the cost of safeness guarantees. This paper introduces a novel safe multi-task Bayesian optimization algorithm that integrates multiple tasks while maintaining high-probability safety. We derive robust uniform error bounds for the multi-task case and demonstrate the effectiveness of the approach on benchmark functions and a control problem. Our results show a significant improvement in sample efficiency, making the proposed method well-suited for expensive-to-evaluate functions."
  },
  {
    "title": "Soft Actor-Critic-based Control Barrier Adaptation for Robust Autonomous Navigation in Unknown Environments",
    "url": "http://arxiv.org/abs/2503.08479v1",
    "arxiv_id": "2503.08479v1",
    "authors": [
      "Nicholas Mohammad",
      "Nicola Bezzo"
    ],
    "published": "2025-03-11T14:33:55+00:00",
    "summary": "Motion planning failures during autonomous navigation often occur when safety constraints are either too conservative, leading to deadlocks, or too liberal, resulting in collisions. To improve robustness, a robot must dynamically adapt its safety constraints to ensure it reaches its goal while balancing safety and performance measures. To this end, we propose a Soft Actor-Critic (SAC)-based policy for adapting Control Barrier Function (CBF) constraint parameters at runtime, ensuring safe yet non-conservative motion. The proposed approach is designed for a general high-level motion planner, low-level controller, and target system model, and is trained in simulation only. Through extensive simulations and physical experiments, we demonstrate that our framework effectively adapts CBF constraints, enabling the robot to reach its final goal without compromising safety."
  },
  {
    "title": "Status and Future Prospects of the Standardization Framework Industry 4.0: A European Perspective",
    "url": "http://arxiv.org/abs/2503.08460v1",
    "arxiv_id": "2503.08460v1",
    "authors": [
      "Olga Meyer",
      "Marvin Boell",
      "Christoph Legat"
    ],
    "published": "2025-03-11T14:08:57+00:00",
    "summary": "The rapid development of Industry 4.0 technologies requires robust and comprehensive standardization to ensure interoperability, safety and efficiency in the Industry of the Future. This paper examines the fundamental role and functionality of standardization, with a particular focus on its importance in Europe's regulatory framework. Based on this, selected topics in context of standardization activities in context intelligent manufacturing and digital twins are highlighted and, by that, an overview of the Industry 4.0 standards framework is provided. This paper serves both as an informative guide to the existing standards in Industry 4.0 with respect to Artificial Intelligence and Digital Twins, and as a call to action for increased cooperation between standardization bodies and the research community. By fostering such collaboration, we aim to facilitate the continued development and implementation of standards that will drive innovation and progress in the manufacturing sector."
  },
  {
    "title": "Status and Future Prospects of the Standardization Framework Industry 4.0: A European Perspective",
    "url": "http://arxiv.org/abs/2503.08460v2",
    "arxiv_id": "2503.08460v2",
    "authors": [
      "Olga Meyer",
      "Marvin Boell",
      "Christoph Legat"
    ],
    "published": "2025-03-11T14:08:57+00:00",
    "summary": "The rapid development of Industry 4.0 technologies requires robust and comprehensive standardization to ensure interoperability, safety and efficiency in the Industry of the Future. This paper examines the fundamental role and functionality of standardization, with a particular focus on its importance in Europe's regulatory framework. Based on this, selected topics in context of standardization activities in context intelligent manufacturing and digital twins are highlighted and, by that, an overview of the Industry 4.0 standards framework is provided. This paper serves both as an informative guide to the existing standards in Industry 4.0 with respect to Artificial Intelligence and Digital Twins, and as a call to action for increased cooperation between standardization bodies and the research community. By fostering such collaboration, we aim to facilitate the continued development and implementation of standards that will drive innovation and progress in the manufacturing sector."
  },
  {
    "title": "ICPR 2024 Competition on Rider Intention Prediction",
    "url": "http://arxiv.org/abs/2503.08437v1",
    "arxiv_id": "2503.08437v1",
    "authors": [
      "Shankar Gangisetty",
      "Abdul Wasi",
      "Shyam Nandan Rai",
      "C. V. Jawahar",
      "Sajay Raj",
      "Manish Prajapati",
      "Ayesha Choudhary",
      "Aaryadev Chandra",
      "Dev Chandan",
      "Shireen Chand",
      "Suvaditya Mukherjee"
    ],
    "published": "2025-03-11T13:50:37+00:00",
    "summary": "The recent surge in the vehicle market has led to an alarming increase in road accidents. This underscores the critical importance of enhancing road safety measures, particularly for vulnerable road users like motorcyclists. Hence, we introduce the rider intention prediction (RIP) competition that aims to address challenges in rider safety by proactively predicting maneuvers before they occur, thereby strengthening rider safety. This capability enables the riders to react to the potential incorrect maneuvers flagged by advanced driver assistance systems (ADAS). We collect a new dataset, namely, rider action anticipation dataset (RAAD) for the competition consisting of two tasks: single-view RIP and multi-view RIP. The dataset incorporates a spectrum of traffic conditions and challenging navigational maneuvers on roads with varying lighting conditions. For the competition, we received seventy-five registrations and five team submissions for inference of which we compared the methods of the top three performing teams on both the RIP tasks: one state-space model (Mamba2) and two learning-based approaches (SVM and CNN-LSTM). The results indicate that the state-space model outperformed the other methods across the entire dataset, providing a balanced performance across maneuver classes. The SVM-based RIP method showed the second-best performance when using random sampling and SMOTE. However, the CNN-LSTM method underperformed, primarily due to class imbalance issues, particularly struggling with minority classes. This paper details the proposed RAAD dataset and provides a summary of the submissions for the RIP 2024 competition."
  },
  {
    "title": "Uni-Gaussians: Unifying Camera and Lidar Simulation with Gaussians for Dynamic Driving Scenarios",
    "url": "http://arxiv.org/abs/2503.08317v1",
    "arxiv_id": "2503.08317v1",
    "authors": [
      "Zikang Yuan",
      "Yuechuan Pu",
      "Hongcheng Luo",
      "Fengtian Lang",
      "Cheng Chi",
      "Teng Li",
      "Yingying Shen",
      "Haiyang Sun",
      "Bing Wang",
      "Xin Yang"
    ],
    "published": "2025-03-11T11:25:57+00:00",
    "summary": "Ensuring the safety of autonomous vehicles necessitates comprehensive simulation of multi-sensor data, encompassing inputs from both cameras and LiDAR sensors, across various dynamic driving scenarios. Neural rendering techniques, which utilize collected raw sensor data to simulate these dynamic environments, have emerged as a leading methodology. While NeRF-based approaches can uniformly represent scenes for rendering data from both camera and LiDAR, they are hindered by slow rendering speeds due to dense sampling. Conversely, Gaussian Splatting-based methods employ Gaussian primitives for scene representation and achieve rapid rendering through rasterization. However, these rasterization-based techniques struggle to accurately model non-linear optical sensors. This limitation restricts their applicability to sensors beyond pinhole cameras. To address these challenges and enable unified representation of dynamic driving scenarios using Gaussian primitives, this study proposes a novel hybrid approach. Our method utilizes rasterization for rendering image data while employing Gaussian ray-tracing for LiDAR data rendering. Experimental results on public datasets demonstrate that our approach outperforms current state-of-the-art methods. This work presents a unified and efficient solution for realistic simulation of camera and LiDAR data in autonomous driving scenarios using Gaussian primitives, offering significant advancements in both rendering quality and computational efficiency."
  },
  {
    "title": "Dynamic Risk Assessment for Human-Robot Collaboration Using a Heuristics-based Approach",
    "url": "http://arxiv.org/abs/2503.08316v1",
    "arxiv_id": "2503.08316v1",
    "authors": [
      "Georgios Katranis",
      "Frederik Plahl",
      "Joachim Grimstadt",
      "Ilshat Mamaev",
      "Silvia Vock",
      "Andrey Morozov"
    ],
    "published": "2025-03-11T11:25:47+00:00",
    "summary": "Human-robot collaboration (HRC) introduces significant safety challenges, particularly in protecting human operators working alongside collaborative robots (cobots). While current ISO standards emphasize risk assessment and hazard identification, these procedures are often insufficient for addressing the complexity of HRC environments, which involve numerous design factors and dynamic interactions. This publication presents a method for objective hazard analysis to support Dynamic Risk Assessment, extending beyond reliance on expert knowledge. The approach monitors scene parameters, such as the distance between human body parts and the cobot, as well as the cobot`s Cartesian velocity. Additionally, an anthropocentric parameter focusing on the orientation of the human head within the collaborative workspace is introduced. These parameters are transformed into hazard indicators using non-linear heuristic functions. The hazard indicators are then aggregated to estimate the total hazard level of a given scenario. The proposed method is evaluated using an industrial dataset that depicts various interactions between a human operator and a cobot."
  },
  {
    "title": "Safety-Ensured Control Framework for Robotic Endoscopic Task Automation",
    "url": "http://arxiv.org/abs/2503.08214v1",
    "arxiv_id": "2503.08214v1",
    "authors": [
      "Yitaek Kim",
      "I\u00f1igo Iturrate",
      "Christoffer Sloth",
      "Hansoul Kim"
    ],
    "published": "2025-03-11T09:28:35+00:00",
    "summary": "There is growing interest in automating surgical tasks using robotic systems, such as endoscopy for treating gastrointestinal (GI) cancer. However, previous studies have primarily focused on detecting and analyzing objects or robots, with limited attention to ensuring safety, which is critical for clinical applications, where accidents can be caused by unsafe robot motions. In this study, we propose a new control framework that can formally ensure the safety of automating certain processes involved in endoscopic submucosal dissection (ESD), a representative endoscopic surgical method for the treatment of early GI cancer, by using an endoscopic robot. The proposed framework utilizes Control Barrier Functions (CBFs) to accurately identify the boundaries of individual tumors, even in close proximity within the GI tract, ensuring precise treatment and removal while preserving the surrounding normal tissue. Additionally, by adopting a model-free control scheme, safety assurance is made possible even in endoscopic robotic systems where dynamic modeling is challenging. We demonstrate the proposed framework in cases where the tumors to be removed are close to each other, showing that the safety constraints are enforced. We show that the model-free CBF-based controlled robot eliminates one tumor completely without damaging it, while not invading another nearby tumor."
  },
  {
    "title": "Dialogue Injection Attack: Jailbreaking LLMs through Context Manipulation",
    "url": "http://arxiv.org/abs/2503.08195v1",
    "arxiv_id": "2503.08195v1",
    "authors": [
      "Wenlong Meng",
      "Fan Zhang",
      "Wendao Yao",
      "Zhenyuan Guo",
      "Yuwei Li",
      "Chengkun Wei",
      "Wenzhi Chen"
    ],
    "published": "2025-03-11T09:00:45+00:00",
    "summary": "Large language models (LLMs) have demonstrated significant utility in a wide range of applications; however, their deployment is plagued by security vulnerabilities, notably jailbreak attacks. These attacks manipulate LLMs to generate harmful or unethical content by crafting adversarial prompts. While much of the current research on jailbreak attacks has focused on single-turn interactions, it has largely overlooked the impact of historical dialogues on model behavior. In this paper, we introduce a novel jailbreak paradigm, Dialogue Injection Attack (DIA), which leverages the dialogue history to enhance the success rates of such attacks. DIA operates in a black-box setting, requiring only access to the chat API or knowledge of the LLM's chat template. We propose two methods for constructing adversarial historical dialogues: one adapts gray-box prefilling attacks, and the other exploits deferred responses. Our experiments show that DIA achieves state-of-the-art attack success rates on recent LLMs, including Llama-3.1 and GPT-4o. Additionally, we demonstrate that DIA can bypass 5 different defense mechanisms, highlighting its robustness and effectiveness."
  },
  {
    "title": "FASIONAD++ : Integrating High-Level Instruction and Information Bottleneck in FAt-Slow fusION Systems for Enhanced Safety in Autonomous Driving with Adaptive Feedback",
    "url": "http://arxiv.org/abs/2503.08162v1",
    "arxiv_id": "2503.08162v1",
    "authors": [
      "Kangan Qian",
      "Ziang Luo",
      "Sicong Jiang",
      "Zilin Huang",
      "Jinyu Miao",
      "Zhikun Ma",
      "Tianze Zhu",
      "Jiayin Li",
      "Yangfan He",
      "Zheng Fu",
      "Yining Shi",
      "Boyue Wang",
      "Hezhe Lin",
      "Ziyu Chen",
      "Jiangbo Yu",
      "Xinyu Jiao",
      "Mengmeng Yang",
      "Kun Jiang",
      "Diange Yang"
    ],
    "published": "2025-03-11T08:27:01+00:00",
    "summary": "Ensuring safe, comfortable, and efficient planning is crucial for autonomous driving systems. While end-to-end models trained on large datasets perform well in standard driving scenarios, they struggle with complex low-frequency events. Recent Large Language Models (LLMs) and Vision Language Models (VLMs) advancements offer enhanced reasoning but suffer from computational inefficiency. Inspired by the dual-process cognitive model \"Thinking, Fast and Slow\", we propose $\\textbf{FASIONAD}$ -- a novel dual-system framework that synergizes a fast end-to-end planner with a VLM-based reasoning module. The fast system leverages end-to-end learning to achieve real-time trajectory generation in common scenarios, while the slow system activates through uncertainty estimation to perform contextual analysis and complex scenario resolution. Our architecture introduces three key innovations: (1) A dynamic switching mechanism enabling slow system intervention based on real-time uncertainty assessment; (2) An information bottleneck with high-level plan feedback that optimizes the slow system's guidance capability; (3) A bidirectional knowledge exchange where visual prompts enhance the slow system's reasoning while its feedback refines the fast planner's decision-making. To strengthen VLM reasoning, we develop a question-answering mechanism coupled with reward-instruct training strategy. In open-loop experiments, FASIONAD achieves a $6.7\\%$ reduction in average $L2$ trajectory error and $28.1\\%$ lower collision rate."
  },
  {
    "title": "Toward Stable World Models: Measuring and Addressing World Instability in Generative Environments",
    "url": "http://arxiv.org/abs/2503.08122v1",
    "arxiv_id": "2503.08122v1",
    "authors": [
      "Soonwoo Kwon",
      "Jin-Young Kim",
      "Hyojun Go",
      "Kyungjune Baek"
    ],
    "published": "2025-03-11T07:38:11+00:00",
    "summary": "We present a novel study on enhancing the capability of preserving the content in world models, focusing on a property we term World Stability. Recent diffusion-based generative models have advanced the synthesis of immersive and realistic environments that are pivotal for applications such as reinforcement learning and interactive game engines. However, while these models excel in quality and diversity, they often neglect the preservation of previously generated scenes over time--a shortfall that can introduce noise into agent learning and compromise performance in safety-critical settings. In this work, we introduce an evaluation framework that measures world stability by having world models perform a sequence of actions followed by their inverses to return to their initial viewpoint, thereby quantifying the consistency between the starting and ending observations. Our comprehensive assessment of state-of-the-art diffusion-based world models reveals significant challenges in achieving high world stability. Moreover, we investigate several improvement strategies to enhance world stability. Our results underscore the importance of world stability in world modeling and provide actionable insights for future research in this domain."
  },
  {
    "title": "Control Barrier Functions for Prescribed-time Reach-Avoid-Stay Tasks using Spatiotemporal Tubes",
    "url": "http://arxiv.org/abs/2503.08106v1",
    "arxiv_id": "2503.08106v1",
    "authors": [
      "Ratnangshu Das",
      "Pranav Bakshi",
      "Pushpak Jagtap"
    ],
    "published": "2025-03-11T07:15:49+00:00",
    "summary": "Prescribed-time reach-avoid-stay (PT-RAS) specifications are crucial in applications requiring precise timing, state constraints, and safety guarantees. While control carrier functions (CBFs) have emerged as a promising approach, providing formal guarantees of safety, constructing CBFs that satisfy PT-RAS specifications remains challenging. In this paper, we present a novel approach using a spatiotemporal tubes (STTs) framework to construct CBFs for PT-RAS tasks. The STT framework allows for the systematic design of CBFs that dynamically manage both spatial and temporal constraints, ensuring the system remains within a safe operational envelope while achieving the desired temporal objectives. The proposed method is validated with two case studies: temporal motion planning of an omnidirectional robot and temporal waypoint navigation of a drone with obstacles, using higher-order CBFs."
  },
  {
    "title": "Enhancing Vehicle Platooning Safety via Control Node Placement and Sizing under State and Input Bounds",
    "url": "http://arxiv.org/abs/2503.08089v1",
    "arxiv_id": "2503.08089v1",
    "authors": [
      "Yifei She",
      "Shen Wang",
      "Ahmad Taha",
      "Xiaofeng Tao"
    ],
    "published": "2025-03-11T06:45:29+00:00",
    "summary": "Vehicle platooning with Cooperative Adaptive Cruise Control improves traffic efficiency, reduces energy consumption, and enhances safety but remains vulnerable to cyber-attacks that disrupt communication and cause unsafe actions. To address these risks, this paper investigates control node placement and input bound optimization to balance safety and defense efficiency under various conditions. We propose a two-stage actuator placement and actuator saturation approach, which focuses on identifying key actuators that maximize the system's controllability while operating under state and input constraints. By strategically placing and limiting the input bounds of critical actuators, we ensure that vehicles maintain safe distances even under attack. Simulation results show that our method effectively mitigates the impact of attacks while preserving defense efficiency, offering a robust solution to vehicle platooning safety challenges."
  },
  {
    "title": "TRUST: Stability and Safety Controller Synthesis for Unknown Dynamical Models Using a Single Trajectory",
    "url": "http://arxiv.org/abs/2503.08081v1",
    "arxiv_id": "2503.08081v1",
    "authors": [
      "Jamie Gardner",
      "Ben Wooding",
      "Amy Nejati",
      "Abolfazl Lavaei"
    ],
    "published": "2025-03-11T06:27:40+00:00",
    "summary": "TRUST is an open-source software tool developed for data-driven controller synthesis of dynamical systems with unknown mathematical models, ensuring either stability or safety properties. By collecting only a single input-state trajectory from the unknown system and satisfying a rank condition that ensures the system is persistently excited according to the Willems et al.'s fundamental lemma, TRUST aims to design either control Lyapunov functions (CLF) or control barrier certificates (CBC), along with their corresponding stability or safety controllers. The tool implements sum-of-squares (SOS) optimization programs solely based on data to enforce stability or safety properties across four system classes: (i) continuous-time nonlinear polynomial systems, (ii) continuous-time linear systems, (iii) discrete-time nonlinear polynomial systems, and (iv) discrete-time linear systems. TRUST is a Python-based web application featuring an intuitive, reactive graphic user interface (GUI) built with web technologies. It can be accessed at https://trust.tgo.dev or installed locally, and supports both manual data entry and data file uploads. Leveraging the power of the Python backend and a JavaScript frontend, TRUST is designed to be highly user-friendly and accessible across desktop, laptop, tablet, and mobile devices. We apply TRUST to a set of physical benchmarks with unknown dynamics, ensuring either stability or safety properties across the four supported classes of models."
  },
  {
    "title": "STGDPM:Vessel Trajectory Prediction with Spatio-Temporal Graph Diffusion Probabilistic Model",
    "url": "http://arxiv.org/abs/2503.08065v1",
    "arxiv_id": "2503.08065v1",
    "authors": [
      "Jin Wenzhe",
      "Tang Haina",
      "Zhang Xudong"
    ],
    "published": "2025-03-11T05:50:27+00:00",
    "summary": "Vessel trajectory prediction is a critical component for ensuring maritime traffic safety and avoiding collisions. Due to the inherent uncertainty in vessel behavior, trajectory prediction systems must adopt a multimodal approach to accurately model potential future motion states. However, existing vessel trajectory prediction methods lack the ability to comprehensively model behavioral multi-modality. To better capture multimodal behavior in interactive scenarios, we propose modeling interactions as dynamic graphs, replacing traditional aggregation-based techniques that rely on vessel states. By leveraging the natural multimodal capabilities of diffusion models, we frame the trajectory prediction task as an inverse process of motion uncertainty diffusion, wherein uncertainties across potential navigational areas are progressively eliminated until the desired trajectories is produced. In summary, we pioneer the integration of Spatio-Temporal Graph (STG) with diffusion models in ship trajectory prediction. Extensive experiments on real Automatic Identification System (AIS) data validate the superiority of our approach."
  },
  {
    "title": "Data-Driven Dynamic Controller Synthesis for Discrete-Time General Nonlinear Systems",
    "url": "http://arxiv.org/abs/2503.08060v1",
    "arxiv_id": "2503.08060v1",
    "authors": [
      "Behrad Samari",
      "Abolfazl Lavaei"
    ],
    "published": "2025-03-11T05:38:47+00:00",
    "summary": "Synthesizing safety controllers for general nonlinear systems is a highly challenging task, particularly when the system models are unknown, and input constraints are present. While some recent efforts have explored data-driven safety controller design for nonlinear systems, these approaches are primarily limited to specific classes of nonlinear dynamics (e.g., polynomials) and are not applicable to general nonlinear systems. This paper develops a direct data-driven approach for discrete-time general nonlinear systems, facilitating the simultaneous learning of control barrier certificates (CBCs) and dynamic controllers to ensure safety properties under input constraints. Specifically, by leveraging the adding-one-integrator approach, we incorporate the controller's dynamics into the system dynamics to synthesize a virtual static-feedback controller for the augmented system, resulting in a dynamic safety controller for the actual dynamics. We collect input-state data from the augmented system during a finite-time experiment, referred to as a single trajectory. Using this data, we learn augmented CBCs and the corresponding virtual safety controllers, ensuring the safety of the actual system and adherence to input constraints over a finite time horizon. We demonstrate that our proposed conditions boil down to some data-dependent linear matrix inequalities (LMIs), which are easy to satisfy. We showcase the effectiveness of our data-driven approach through two case studies: one exhibiting significant nonlinearity and the other featuring high dimensionality."
  },
  {
    "title": "SGNetPose+: Stepwise Goal-Driven Networks with Pose Information for Trajectory Prediction in Autonomous Driving",
    "url": "http://arxiv.org/abs/2503.08016v1",
    "arxiv_id": "2503.08016v1",
    "authors": [
      "Akshat Ghiya",
      "Ali K. AlShami",
      "Jugal Kalita"
    ],
    "published": "2025-03-11T03:45:51+00:00",
    "summary": "Predicting pedestrian trajectories is essential for autonomous driving systems, as it significantly enhances safety and supports informed decision-making. Accurate predictions enable the prevention of collisions, anticipation of crossing intent, and improved overall system efficiency. In this study, we present SGNetPose+, an enhancement of the SGNet architecture designed to integrate skeleton information or body segment angles with bounding boxes to predict pedestrian trajectories from video data to avoid hazards in autonomous driving. Skeleton information was extracted using a pose estimation model, and joint angles were computed based on the extracted joint data. We also apply temporal data augmentation by horizontally flipping video frames to increase the dataset size and improve performance. Our approach achieves state-of-the-art results on the JAAD and PIE datasets using pose data with the bounding boxes, outperforming the SGNet model. Code is available on Github: SGNetPose+."
  },
  {
    "title": "STEAD: Spatio-Temporal Efficient Anomaly Detection for Time and Compute Sensitive Applications",
    "url": "http://arxiv.org/abs/2503.07942v1",
    "arxiv_id": "2503.07942v1",
    "authors": [
      "Andrew Gao",
      "Jun Liu"
    ],
    "published": "2025-03-11T00:48:32+00:00",
    "summary": "This paper presents a new method for anomaly detection in automated systems with time and compute sensitive requirements, such as autonomous driving, with unparalleled efficiency. As systems like autonomous driving become increasingly popular, ensuring their safety has become more important than ever. Therefore, this paper focuses on how to quickly and effectively detect various anomalies in the aforementioned systems, with the goal of making them safer and more effective. Many detection systems have been developed with great success under spatial contexts; however, there is still significant room for improvement when it comes to temporal context. While there is substantial work regarding this task, there is minimal work done regarding the efficiency of models and their ability to be applied to scenarios that require real-time inference, i.e., autonomous driving where anomalies need to be detected the moment they are within view. To address this gap, we propose STEAD (Spatio-Temporal Efficient Anomaly Detection), whose backbone is developed using (2+1)D Convolutions and Performer Linear Attention, which ensures computational efficiency without sacrificing performance. When tested on the UCF-Crime benchmark, our base model achieves an AUC of 91.34%, outperforming the previous state-of-the-art, and our fast version achieves an AUC of 88.87%, while having 99.70% less parameters and outperforming the previous state-of-the-art as well. The code and pretrained models are made publicly available at https://github.com/agao8/STEAD"
  },
  {
    "title": "Intelligent Framework for Human-Robot Collaboration: Safety, Dynamic Ergonomics, and Adaptive Decision-Making",
    "url": "http://arxiv.org/abs/2503.07901v1",
    "arxiv_id": "2503.07901v1",
    "authors": [
      "Francesco Iodice",
      "Elena De Momi",
      "Arash Ajoudani"
    ],
    "published": "2025-03-10T22:43:07+00:00",
    "summary": "The integration of collaborative robots into industrial environments has improved productivity, but has also highlighted significant challenges related to operator safety and ergonomics. This paper proposes an innovative framework that integrates advanced visual perception technologies, real-time ergonomic monitoring, and Behaviour Tree (BT)-based adaptive decision-making. Unlike traditional methods, which often operate in isolation or statically, our approach combines deep learning models (YOLO11 and SlowOnly), advanced tracking (Unscented Kalman Filter) and dynamic ergonomic assessments (OWAS), offering a modular, scalable and adaptive system. Experimental results show that the framework outperforms previous methods in several aspects: accuracy in detecting postures and actions, adaptivity in managing human-robot interactions, and ability to reduce ergonomic risk through timely robotic interventions. In particular, the visual perception module showed superiority over YOLOv9 and YOLOv8, while real-time ergonomic monitoring eliminated the limitations of static analysis. Adaptive role management, made possible by the Behaviour Tree, provided greater responsiveness than rule-based systems, making the framework suitable for complex industrial scenarios. Our system demonstrated a 92.5\\% accuracy in grasping intention recognition and successfully classified ergonomic risks with real-time responsiveness (average latency of 0.57 seconds), enabling timely robotic"
  },
  {
    "title": "Safety Guardrails for LLM-Enabled Robots",
    "url": "http://arxiv.org/abs/2503.07885v1",
    "arxiv_id": "2503.07885v1",
    "authors": [
      "Zachary Ravichandran",
      "Alexander Robey",
      "Vijay Kumar",
      "George J. Pappas",
      "Hamed Hassani"
    ],
    "published": "2025-03-10T22:01:56+00:00",
    "summary": "Although the integration of large language models (LLMs) into robotics has unlocked transformative capabilities, it has also introduced significant safety concerns, ranging from average-case LLM errors (e.g., hallucinations) to adversarial jailbreaking attacks, which can produce harmful robot behavior in real-world settings. Traditional robot safety approaches do not address the novel vulnerabilities of LLMs, and current LLM safety guardrails overlook the physical risks posed by robots operating in dynamic real-world environments. In this paper, we propose RoboGuard, a two-stage guardrail architecture to ensure the safety of LLM-enabled robots. RoboGuard first contextualizes pre-defined safety rules by grounding them in the robot's environment using a root-of-trust LLM, which employs chain-of-thought (CoT) reasoning to generate rigorous safety specifications, such as temporal logic constraints. RoboGuard then resolves potential conflicts between these contextual safety specifications and a possibly unsafe plan using temporal logic control synthesis, which ensures safety compliance while minimally violating user preferences. Through extensive simulation and real-world experiments that consider worst-case jailbreaking attacks, we demonstrate that RoboGuard reduces the execution of unsafe plans from 92% to below 2.5% without compromising performance on safe plans. We also demonstrate that RoboGuard is resource-efficient, robust against adaptive attacks, and significantly enhanced by enabling its root-of-trust LLM to perform CoT reasoning. These results underscore the potential of RoboGuard to mitigate the safety risks and enhance the reliability of LLM-enabled robots."
  },
  {
    "title": "Health Prognostics in Multi-sensor Systems Based on Multivariate Functional Data Analysis",
    "url": "http://arxiv.org/abs/2503.07854v1",
    "arxiv_id": "2503.07854v1",
    "authors": [
      "Cevahir Yildirim",
      "Alba M. Franco-Pereira",
      "Rosa E. Lillo"
    ],
    "published": "2025-03-10T21:00:48+00:00",
    "summary": "Recent developments in big data analysis, machine learning, Industry 4.0, and IoT applications have enabled the monitoring and processing of multi-sensor data collected from systems, allowing for the prediction of the \"Remaining Useful Life\" (RUL) of system components. Particularly in the aviation industry, Prognostic Health Management (PHM) has become one of the most important practices for ensuring reliability and safety. Not only is the accuracy of RUL prediction important, but the implementability of techniques, domain adaptability, and interpretability of system degradation behaviors have also become essential. In this paper, the data collected from the multi-sensor environment of complex systems are processed using a Functional Data Analysis (FDA) approach to predict when the systems will fail and to understand and interpret the systems' life cycles. The approach is applied to the C-MAPSS datasets shared by National Aeronautics and Space Administration, and the behaviors of the sensors in aircraft engine failures are adaptively modeled with Multivariate Functional Principal Component Analysis (MFPCA). While the results indicate that the proposed method predicts the RUL competitively compared to other methods in the literature, it also demonstrates how multivariate Functional Data Analysis is useful for interpretability in prognostic studies within multi-sensor environments."
  },
  {
    "title": "Safe Explicable Policy Search",
    "url": "http://arxiv.org/abs/2503.07848v1",
    "arxiv_id": "2503.07848v1",
    "authors": [
      "Akkamahadevi Hanni",
      "Jonathan Monta\u00f1o",
      "Yu Zhang"
    ],
    "published": "2025-03-10T20:52:41+00:00",
    "summary": "When users work with AI agents, they form conscious or subconscious expectations of them. Meeting user expectations is crucial for such agents to engage in successful interactions and teaming. However, users may form expectations of an agent that differ from the agent's planned behaviors. These differences lead to the consideration of two separate decision models in the planning process to generate explicable behaviors. However, little has been done to incorporate safety considerations, especially in a learning setting. We present Safe Explicable Policy Search (SEPS), which aims to provide a learning approach to explicable behavior generation while minimizing the safety risk, both during and after learning. We formulate SEPS as a constrained optimization problem where the agent aims to maximize an explicability score subject to constraints on safety and a suboptimality criterion based on the agent's model. SEPS innovatively combines the capabilities of Constrained Policy Optimization and Explicable Policy Search. We evaluate SEPS in safety-gym environments and with a physical robot experiment to show that it can learn explicable behaviors that adhere to the agent's safety requirements and are efficient. Results show that SEPS can generate safe and explicable behaviors while ensuring a desired level of performance w.r.t. the agent's objective, and has real-world relevance in human-AI teaming."
  },
  {
    "title": "Improving Pedestrian Safety at Intersections Using Probabilistic Models and Monte Carlo Simulations",
    "url": "http://arxiv.org/abs/2503.07805v1",
    "arxiv_id": "2503.07805v1",
    "authors": [
      "Alben Rome Bagabaldo",
      "J\u00fcrgen Hackl"
    ],
    "published": "2025-03-10T19:39:18+00:00",
    "summary": "National Highway Traffic Safety Administration reported 7,345 pedestrian fatalities in the United States in 2022, making pedestrian safety a pressing issue in urban mobility. This study presents a novel probabilistic simulation framework integrating dynamic pedestrian crossing models and Monte Carlo simulations to evaluate safety under varying traffic conditions. The framework captures key influences on pedestrian decisions, such as traffic light states, vehicle proximity, and waiting times, while employing the Intelligent Driver Model (IDM) to simulate realistic vehicle dynamics. Results from 500 trials show that pedestrians avoid crossing during green lights, reducing collision risks, while shorter waiting times during red lights encourage safer crossings. The risk is heightened during yellow lights, especially with nearby vehicles. This research emphasizes the importance of adaptive traffic control measures, such as pedestrian-triggered signals and enhanced traffic light timing, to mitigate risks and prioritize pedestrian safety. By modeling realistic interactions between pedestrians and vehicles, the study offers insights for designing safer and more sustainable urban intersections."
  },
  {
    "title": "A Simple Approach to Constraint-Aware Imitation Learning with Application to Autonomous Racing",
    "url": "http://arxiv.org/abs/2503.07737v1",
    "arxiv_id": "2503.07737v1",
    "authors": [
      "Shengfan Cao",
      "Eunhyek Joa",
      "Francesco Borrelli"
    ],
    "published": "2025-03-10T18:00:16+00:00",
    "summary": "Guaranteeing constraint satisfaction is challenging in imitation learning (IL), particularly in tasks that require operating near a system's handling limits. Traditional IL methods often struggle to enforce constraints, leading to suboptimal performance in high-precision tasks. In this paper, we present a simple approach to incorporating safety into the IL objective. Through simulations, we empirically validate our approach on an autonomous racing task with both full-state and image feedback, demonstrating improved constraint satisfaction and greater consistency in task performance compared to a baseline method."
  },
  {
    "title": "AlphaDrive: Unleashing the Power of VLMs in Autonomous Driving via Reinforcement Learning and Reasoning",
    "url": "http://arxiv.org/abs/2503.07608v1",
    "arxiv_id": "2503.07608v1",
    "authors": [
      "Bo Jiang",
      "Shaoyu Chen",
      "Qian Zhang",
      "Wenyu Liu",
      "Xinggang Wang"
    ],
    "published": "2025-03-10T17:59:42+00:00",
    "summary": "OpenAI o1 and DeepSeek R1 achieve or even surpass human expert-level performance in complex domains like mathematics and science, with reinforcement learning (RL) and reasoning playing a crucial role. In autonomous driving, recent end-to-end models have greatly improved planning performance but still struggle with long-tailed problems due to limited common sense and reasoning abilities. Some studies integrate vision-language models (VLMs) into autonomous driving, but they typically rely on pre-trained models with simple supervised fine-tuning (SFT) on driving data, without further exploration of training strategies or optimizations specifically tailored for planning. In this paper, we propose AlphaDrive, a RL and reasoning framework for VLMs in autonomous driving. AlphaDrive introduces four GRPO-based RL rewards tailored for planning and employs a two-stage planning reasoning training strategy that combines SFT with RL. As a result, AlphaDrive significantly improves both planning performance and training efficiency compared to using only SFT or without reasoning. Moreover, we are also excited to discover that, following RL training, AlphaDrive exhibits some emergent multimodal planning capabilities, which is critical for improving driving safety and efficiency. To the best of our knowledge, AlphaDrive is the first to integrate GRPO-based RL with planning reasoning into autonomous driving. Code will be released to facilitate future research."
  },
  {
    "title": "Implicit Reasoning in Transformers is Reasoning through Shortcuts",
    "url": "http://arxiv.org/abs/2503.07604v1",
    "arxiv_id": "2503.07604v1",
    "authors": [
      "Tianhe Lin",
      "Jian Xie",
      "Siyu Yuan",
      "Deqing Yang"
    ],
    "published": "2025-03-10T17:58:31+00:00",
    "summary": "Test-time compute is emerging as a new paradigm for enhancing language models' complex multi-step reasoning capabilities, as demonstrated by the success of OpenAI's o1 and o3, as well as DeepSeek's R1. Compared to explicit reasoning in test-time compute, implicit reasoning is more inference-efficient, requiring fewer generated tokens. However, why does the advanced reasoning capability fail to emerge in the implicit reasoning style? In this work, we train GPT-2 from scratch on a curated multi-step mathematical reasoning dataset and conduct analytical experiments to investigate how language models perform implicit reasoning in multi-step tasks. Our findings reveal: 1) Language models can perform step-by-step reasoning and achieve high accuracy in both in-domain and out-of-domain tests via implicit reasoning. However, this capability only emerges when trained on fixed-pattern data. 2) Conversely, implicit reasoning abilities emerging from training on unfixed-pattern data tend to overfit a specific pattern and fail to generalize further. Notably, this limitation is also observed in state-of-the-art large language models. These findings suggest that language models acquire implicit reasoning through shortcut learning, enabling strong performance on tasks with similar patterns while lacking generalization."
  },
  {
    "title": "PointVLA: Injecting the 3D World into Vision-Language-Action Models",
    "url": "http://arxiv.org/abs/2503.07511v1",
    "arxiv_id": "2503.07511v1",
    "authors": [
      "Chengmeng Li",
      "Junjie Wen",
      "Yan Peng",
      "Yaxin Peng",
      "Feifei Feng",
      "Yichen Zhu"
    ],
    "published": "2025-03-10T16:32:41+00:00",
    "summary": "Vision-Language-Action (VLA) models excel at robotic tasks by leveraging large-scale 2D vision-language pretraining, but their reliance on RGB images limits spatial reasoning critical for real-world interaction. Retraining these models with 3D data is computationally prohibitive, while discarding existing 2D datasets wastes valuable resources. To bridge this gap, we propose PointVLA, a framework that enhances pre-trained VLAs with point cloud inputs without requiring retraining. Our method freezes the vanilla action expert and injects 3D features via a lightweight modular block. To identify the most effective way of integrating point cloud representations, we conduct a skip-block analysis to pinpoint less useful blocks in the vanilla action expert, ensuring that 3D features are injected only into these blocks--minimizing disruption to pre-trained representations.   Extensive experiments demonstrate that PointVLA outperforms state-of-the-art 2D imitation learning methods, such as OpenVLA, Diffusion Policy and DexVLA, across both simulated and real-world robotic tasks. Specifically, we highlight several key advantages of PointVLA enabled by point cloud integration: (1) Few-shot multi-tasking, where PointVLA successfully performs four different tasks using only 20 demonstrations each; (2) Real-vs-photo discrimination, where PointVLA distinguishes real objects from their images, leveraging 3D world knowledge to improve safety and reliability; (3) Height adaptability, Unlike conventional 2D imitation learning methods, PointVLA enables robots to adapt to objects at varying table height that unseen in train data. Furthermore, PointVLA achieves strong performance in long-horizon tasks, such as picking and packing objects from a moving conveyor belt, showcasing its ability to generalize across complex, dynamic environments."
  },
  {
    "title": "Securing External Deeper-than-black-box GPAI Evaluations",
    "url": "http://arxiv.org/abs/2503.07496v1",
    "arxiv_id": "2503.07496v1",
    "authors": [
      "Alejandro Tlaie",
      "Jimmy Farrell"
    ],
    "published": "2025-03-10T16:13:45+00:00",
    "summary": "This paper examines the critical challenges and potential solutions for conducting secure and effective external evaluations of general-purpose AI (GPAI) models. With the exponential growth in size, capability, reach and accompanying risk of these models, ensuring accountability, safety, and public trust requires frameworks that go beyond traditional black-box methods. The discussion begins with an analysis of the need for deeper-than-black-box evaluations (Section I), emphasizing the importance of understanding model internals to uncover latent risks and ensure compliance with ethical and regulatory standards. Building on this foundation, Section II addresses the security considerations of remote evaluations, outlining the threat landscape, technical solutions, and safeguards necessary to protect both evaluators and proprietary model data. Finally, Section III synthesizes these insights into actionable recommendations and future directions, aiming to establish a robust, scalable, and transparent framework for external assessments in GPAI governance."
  },
  {
    "title": "Destination Calculus: A Linear \u03bb-Calculus for Purely Functional Memory Writes",
    "url": "http://arxiv.org/abs/2503.07489v1",
    "arxiv_id": "2503.07489v1",
    "authors": [
      "Thomas Bagrel",
      "Arnaud Spiwack"
    ],
    "published": "2025-03-10T16:08:47+00:00",
    "summary": "Destination passing -- aka. out parameters -- is taking a parameter to fill rather than returning a result from a function. Due to its apparently imperative nature, destination passing has struggled to find its way to pure functional programming. In this paper, we present a pure functional calculus with destinations at its core. Our calculus subsumes all the similar systems, and can be used to reason about their correctness or extension. In addition, our calculus can express programs that were previously not known to be expressible in a pure language. This is guaranteed by a modal type system where modes are used to manage both linearity and scopes. Type safety of our core calculus was proved formally with the Coq proof assistant."
  },
  {
    "title": "MedAgentsBench: Benchmarking Thinking Models and Agent Frameworks for Complex Medical Reasoning",
    "url": "http://arxiv.org/abs/2503.07459v1",
    "arxiv_id": "2503.07459v1",
    "authors": [
      "Xiangru Tang",
      "Daniel Shao",
      "Jiwoong Sohn",
      "Jiapeng Chen",
      "Jiayi Zhang",
      "Jinyu Xiang",
      "Fang Wu",
      "Yilun Zhao",
      "Chenglin Wu",
      "Wenqi Shi",
      "Arman Cohan",
      "Mark Gerstein"
    ],
    "published": "2025-03-10T15:38:44+00:00",
    "summary": "Large Language Models (LLMs) have shown impressive performance on existing medical question-answering benchmarks. This high performance makes it increasingly difficult to meaningfully evaluate and differentiate advanced methods. We present MedAgentsBench, a benchmark that focuses on challenging medical questions requiring multi-step clinical reasoning, diagnosis formulation, and treatment planning-scenarios where current models still struggle despite their strong performance on standard tests. Drawing from seven established medical datasets, our benchmark addresses three key limitations in existing evaluations: (1) the prevalence of straightforward questions where even base models achieve high performance, (2) inconsistent sampling and evaluation protocols across studies, and (3) lack of systematic analysis of the interplay between performance, cost, and inference time. Through experiments with various base models and reasoning methods, we demonstrate that the latest thinking models, DeepSeek R1 and OpenAI o3, exhibit exceptional performance in complex medical reasoning tasks. Additionally, advanced search-based agent methods offer promising performance-to-cost ratios compared to traditional approaches. Our analysis reveals substantial performance gaps between model families on complex questions and identifies optimal model selections for different computational constraints. Our benchmark and evaluation framework are publicly available at https://github.com/gersteinlab/medagents-benchmark."
  },
  {
    "title": "CATPlan: Loss-based Collision Prediction in End-to-End Autonomous Driving",
    "url": "http://arxiv.org/abs/2503.07425v1",
    "arxiv_id": "2503.07425v1",
    "authors": [
      "Ziliang Xiong",
      "Shipeng Liu",
      "Nathaniel Helgesen",
      "Joakim Johnander",
      "Per-Erik Forssen"
    ],
    "published": "2025-03-10T15:10:40+00:00",
    "summary": "In recent years, there has been increased interest in the design, training, and evaluation of end-to-end autonomous driving (AD) systems. One often overlooked aspect is the uncertainty of planned trajectories predicted by these systems, despite awareness of their own uncertainty being key to achieve safety and robustness. We propose to estimate this uncertainty by adapting loss prediction from the uncertainty quantification literature. To this end, we introduce a novel light-weight module, dubbed CATPlan, that is trained to decode motion and planning embeddings into estimates of the collision loss used to partially supervise end-to-end AD systems. During inference, these estimates are interpreted as collision risk. We evaluate CATPlan on the safety-critical, nerf-based, closed-loop benchmark NeuroNCAP and find that it manages to detect collisions with a $54.8\\%$ relative improvement to average precision over a GMM-based baseline in which the predicted trajectory is compared to the forecasted trajectories of other road users. Our findings indicate that the addition of CATPlan can lead to safer end-to-end AD systems and hope that our work will spark increased interest in uncertainty quantification for such systems."
  },
  {
    "title": "Towards Safe Robot Foundation Models",
    "url": "http://arxiv.org/abs/2503.07404v1",
    "arxiv_id": "2503.07404v1",
    "authors": [
      "Maximilian T\u00f6lle",
      "Theo Gruner",
      "Daniel Palenicek",
      "Jonas G\u00fcnster",
      "Puze Liu",
      "Joe Watson",
      "Davide Tateo",
      "Jan Peters"
    ],
    "published": "2025-03-10T14:55:09+00:00",
    "summary": "Robot foundation models hold the potential for deployment across diverse environments, from industrial applications to household tasks. While current research focuses primarily on the policies' generalization capabilities across a variety of tasks, it fails to address safety, a critical requirement for deployment on real-world systems. In this paper, we introduce a safety layer designed to constrain the action space of any generalist policy appropriately. Our approach uses ATACOM, a safe reinforcement learning algorithm that creates a safe action space and, therefore, ensures safe state transitions. By extending ATACOM to generalist policies, our method facilitates their deployment in safety-critical scenarios without requiring any specific safety fine-tuning. We demonstrate the effectiveness of this safety layer in an air hockey environment, where it prevents a puck-hitting agent from colliding with its surroundings, a failure observed in generalist policies."
  },
  {
    "title": "ECNN: A Low-complex, Adjustable CNN for Industrial Pump Monitoring Using Vibration Data",
    "url": "http://arxiv.org/abs/2503.07401v1",
    "arxiv_id": "2503.07401v1",
    "authors": [
      "Jonas Ney",
      "Norbert Wehn"
    ],
    "published": "2025-03-10T14:49:37+00:00",
    "summary": "Industrial pumps are essential components in various sectors, such as manufacturing, energy production, and water treatment, where their failures can cause significant financial and safety risks. Anomaly detection can be used to reduce those risks and increase reliability. In this work, we propose a novel enhanced convolutional neural network (ECNN) to predict the failure of an industrial pump based on the vibration data captured by an acceleration sensor. The convolutional neural network (CNN) is designed with a focus on low complexity to enable its implementation on edge devices with limited computational resources. Therefore, a detailed design space exploration is performed to find a topology satisfying the trade-off between complexity and accuracy. Moreover, to allow for adaptation to unknown pumps, our algorithm features a pump-specific parameter that can be determined by a small set of normal data samples. Finally, we combine the ECNN with a threshold approach to further increase the performance and satisfy the application requirements. As a result, our combined approach significantly outperforms a traditional statistical approach and a classical CNN in terms of accuracy. To summarize, this work provides a novel, low-complex, CNN-based algorithm that is enhanced by classical methods to offer high accuracy for anomaly detection of industrial pumps."
  },
  {
    "title": "AttentionSwarm: Reinforcement Learning with Attention Control Barier Function for Crazyflie Drones in Dynamic Environments",
    "url": "http://arxiv.org/abs/2503.07376v1",
    "arxiv_id": "2503.07376v1",
    "authors": [
      "Grik Tadevosyan",
      "Valerii Serpiva",
      "Aleksey Fedoseev",
      "Roohan Ahmed Khan",
      "Demetros Aschu",
      "Faryal Batool",
      "Nickolay Efanov",
      "Artem Mikhaylov",
      "Dzmitry Tsetserukou"
    ],
    "published": "2025-03-10T14:30:59+00:00",
    "summary": "We introduce AttentionSwarm, a novel benchmark designed to evaluate safe and efficient swarm control across three challenging environments: a landing environment with obstacles, a competitive drone game setting, and a dynamic drone racing scenario. Central to our approach is the Attention Model Based Control Barrier Function (CBF) framework, which integrates attention mechanisms with safety-critical control theory to enable real-time collision avoidance and trajectory optimization. This framework dynamically prioritizes critical obstacles and agents in the swarms vicinity using attention weights, while CBFs formally guarantee safety by enforcing collision-free constraints. The safe attention net algorithm was developed and evaluated using a swarm of Crazyflie 2.1 micro quadrotors, which were tested indoors with the Vicon motion capture system to ensure precise localization and control. Experimental results show that our system achieves landing accuracy of 3.02 cm with a mean time of 23 s and collision-free landings in a dynamic landing environment, 100% and collision-free navigation in a drone game environment, and 95% and collision-free navigation for a dynamic multiagent drone racing environment, underscoring its effectiveness and robustness in real-world scenarios. This work offers a promising foundation for applications in dynamic environments where safety and fastness are paramount."
  },
  {
    "title": "MM-Eureka: Exploring Visual Aha Moment with Rule-based Large-scale Reinforcement Learning",
    "url": "http://arxiv.org/abs/2503.07365v1",
    "arxiv_id": "2503.07365v1",
    "authors": [
      "Fanqing Meng",
      "Lingxiao Du",
      "Zongkai Liu",
      "Zhixiang Zhou",
      "Quanfeng Lu",
      "Daocheng Fu",
      "Botian Shi",
      "Wenhai Wang",
      "Junjun He",
      "Kaipeng Zhang",
      "Ping Luo",
      "Yu Qiao",
      "Qiaosheng Zhang",
      "Wenqi Shao"
    ],
    "published": "2025-03-10T14:23:12+00:00",
    "summary": "We present MM-Eureka, a multimodal reasoning model that successfully extends large-scale rule-based reinforcement learning (RL) to multimodal reasoning. While rule-based RL has shown remarkable success in improving LLMs' reasoning abilities in text domains, its application to multimodal settings has remained challenging. Our work reproduces key characteristics of text-based RL systems like DeepSeek-R1 in the multimodal space, including steady increases in accuracy reward and response length, and the emergence of reflection behaviors. We demonstrate that both instruction-tuned and pre-trained models can develop strong multimodal reasoning capabilities through rule-based RL without supervised fine-tuning, showing superior data efficiency compared to alternative approaches. We open-source our complete pipeline to foster further research in this area. We release all our codes, models, data, etc. at https://github.com/ModalMinds/MM-EUREKA"
  },
  {
    "title": "The Economics of p(doom): Scenarios of Existential Risk and Economic Growth in the Age of Transformative AI",
    "url": "http://arxiv.org/abs/2503.07341v1",
    "arxiv_id": "2503.07341v1",
    "authors": [
      "Jakub Growiec",
      "Klaus Prettner"
    ],
    "published": "2025-03-10T13:53:39+00:00",
    "summary": "Recent advances in artificial intelligence (AI) have led to a diverse set of predictions about its long-term impact on humanity. A central focus is the potential emergence of transformative AI (TAI), eventually capable of outperforming humans in all economically valuable tasks and fully automating labor. Discussed scenarios range from human extinction after a misaligned TAI takes over (\"AI doom\") to unprecedented economic growth and abundance (\"post-scarcity\"). However, the probabilities and implications of these scenarios remain highly uncertain. Here, we organize the various scenarios and evaluate their associated existential risks and economic outcomes in terms of aggregate welfare. Our analysis shows that even low-probability catastrophic outcomes justify large investments in AI safety and alignment research. We find that the optimizing representative individual would rationally allocate substantial resources to mitigate extinction risk; in some cases, she would prefer not to develop TAI at all. This result highlights that current global efforts in AI safety and alignment research are vastly insufficient relative to the scale and urgency of existential risks posed by TAI. Our findings therefore underscore the need for stronger safeguards to balance the potential economic benefits of TAI with the prevention of irreversible harm. Addressing these risks is crucial for steering technological progress toward sustainable human prosperity."
  },
  {
    "title": "Mitigating Hallucinations in YOLO-based Object Detection Models: A Revisit to Out-of-Distribution Detection",
    "url": "http://arxiv.org/abs/2503.07330v1",
    "arxiv_id": "2503.07330v1",
    "authors": [
      "Weicheng He",
      "Changshun Wu",
      "Chih-Hong Cheng",
      "Xiaowei Huang",
      "Saddek Bensalem"
    ],
    "published": "2025-03-10T13:42:41+00:00",
    "summary": "Object detection systems must reliably perceive objects of interest without being overly confident to ensure safe decision-making in dynamic environments. Filtering techniques based on out-of-distribution (OoD) detection are commonly added as an extra safeguard to filter hallucinations caused by overconfidence in novel objects. Nevertheless, evaluating YOLO-family detectors and their filters under existing OoD benchmarks often leads to unsatisfactory performance. This paper studies the underlying reasons for performance bottlenecks and proposes a methodology to improve performance fundamentally. Our first contribution is a calibration of all existing evaluation results: Although images in existing OoD benchmark datasets are claimed not to have objects within in-distribution (ID) classes (i.e., categories defined in the training dataset), around 13% of objects detected by the object detector are actually ID objects. Dually, the ID dataset containing OoD objects can also negatively impact the decision boundary of filters. These ultimately lead to a significantly imprecise performance estimation. Our second contribution is to consider the task of hallucination reduction as a joint pipeline of detectors and filters. By developing a methodology to carefully synthesize an OoD dataset that semantically resembles the objects to be detected, and using the crafted OoD dataset in the fine-tuning of YOLO detectors to suppress the objectness score, we achieve a 88% reduction in overall hallucination error with a combined fine-tuned detection and filtering system on the self-driving benchmark BDD-100K. Our code and dataset are available at: https://gricad-gitlab.univ-grenoble-alpes.fr/dnn-safety/m-hood."
  },
  {
    "title": "Complete the Cycle: Reachability Types with Expressive Cyclic References",
    "url": "http://arxiv.org/abs/2503.07328v1",
    "arxiv_id": "2503.07328v1",
    "authors": [
      "Haotian Deng",
      "Siyuan He",
      "Songlin Jia",
      "Yuyan Bao",
      "Tiark Rompf"
    ],
    "published": "2025-03-10T13:42:02+00:00",
    "summary": "Reachability Types (RT) are a qualified type system for tracking aliasing and separation in functional and higher-order programming. By formalizing resource reachability with a sound static type system, RT enable higher-order programming patterns with runtime safety and non-interference guarantees. However, previous RT systems have been based on calculi that restrict cyclic dependencies and are shown to be terminating in the absence of built-in recursive constructs. While termination is sometimes a desirable property, simplifying reasoning and ensuring predictable behavior, it implies an inability to encode expressive programs involving non-termination and advanced recursive patterns, such as mutual recursion and various fixed-point combinators.   In this paper, we address this limitation by extending RT with an expressive cyclic reference type that permits the formation of cyclic dependencies through the store, thereby allowing the system to encode recursive programming patterns without relying on extra built-in constructs. In addition, we redesign qualifier typing in the reference introduction rule, allowing separate references to point to a shared and tracked referent. We formalize the system as the $\\lambda^{\\circ}_{<:}$-calculus, with a mechanized soundness proof via the standard progress and preservation lemmas. As a demonstration, we implement a well-typed fixpoint operator, proving that recursive patterns can be encoded using the novel cyclic reference type."
  },
  {
    "title": "Benchmarking Chinese Medical LLMs: A Medbench-based Analysis of Performance Gaps and Hierarchical Optimization Strategies",
    "url": "http://arxiv.org/abs/2503.07306v1",
    "arxiv_id": "2503.07306v1",
    "authors": [
      "Luyi Jiang",
      "Jiayuan Chen",
      "Lu Lu",
      "Xinwei Peng",
      "Lihao Liu",
      "Junjun He",
      "Jie Xu"
    ],
    "published": "2025-03-10T13:28:25+00:00",
    "summary": "The evaluation and improvement of medical large language models (LLMs) are critical for their real-world deployment, particularly in ensuring accuracy, safety, and ethical alignment. Existing frameworks inadequately dissect domain-specific error patterns or address cross-modal challenges. This study introduces a granular error taxonomy through systematic analysis of top 10 models on MedBench, categorizing incorrect responses into eight types: Omissions, Hallucination, Format Mismatch, Causal Reasoning Deficiency, Contextual Inconsistency, Unanswered, Output Error, and Deficiency in Medical Language Generation. Evaluation of 10 leading models reveals vulnerabilities: despite achieving 0.86 accuracy in medical knowledge recall, critical reasoning tasks show 96.3% omission, while safety ethics evaluations expose alarming inconsistency (robustness score: 0.79) under option shuffled. Our analysis uncovers systemic weaknesses in knowledge boundary enforcement and multi-step reasoning. To address these, we propose a tiered optimization strategy spanning four levels, from prompt engineering and knowledge-augmented retrieval to hybrid neuro-symbolic architectures and causal reasoning frameworks. This work establishes an actionable roadmap for developing clinically robust LLMs while redefining evaluation paradigms through error-driven insights, ultimately advancing the safety and trustworthiness of AI in high-stakes medical environments."
  },
  {
    "title": "What is missing from existing Lithium-Sulfur models to capture coin-cell behaviour?",
    "url": "http://arxiv.org/abs/2503.07684v1",
    "arxiv_id": "2503.07684v1",
    "authors": [
      "Miss. Elizabeth Olisa Monica Marinescu"
    ],
    "published": "2025-03-10T12:24:20+00:00",
    "summary": "Lithium-sulfur (Li-S) batteries offer a promising alternative to current lithium-ion (Li-ion) batteries, with a high theoretical energy density, improved safety and high abundance, low cost of materials. For Li-S to reach commercial application, it is essential to understand how the behaviour scales between cell formats; new material development is predominately completed at coin-cell level, whilst pouch-cells will be used for commercial applications. Differences such as reduced electrolyte-to-sulfur (E/S) ratios and increased geometric size at larger cell formats contribute to the behavioural differences, in terms of achievable capacity, cyclability and potential degradation mechanisms.   This work focuses on the steps required to capture and test coin-cell behaviour, building upon the existing models within the literature, which predominately focus on pouch-cells. The areas investigated throughout this study, to improve the capability of the model in terms of scaling ability and causality of predictions, include the cathode surface area, precipitation dynamics and C-rate dependence."
  },
  {
    "title": "Learning and planning for optimal synergistic human-robot coordination in manufacturing contexts",
    "url": "http://arxiv.org/abs/2503.07238v1",
    "arxiv_id": "2503.07238v1",
    "authors": [
      "Samuele Sandrini",
      "Marco Faroni",
      "Nicola Pedrocchi"
    ],
    "published": "2025-03-10T12:20:29+00:00",
    "summary": "Collaborative robotics cells leverage heterogeneous agents to provide agile production solutions. Effective coordination is essential to prevent inefficiencies and risks for human operators working alongside robots. This paper proposes a human-aware task allocation and scheduling model based on Mixed Integer Nonlinear Programming to optimize efficiency and safety starting from task planning stages. The approach exploits synergies that encode the coupling effects between pairs of tasks executed in parallel by the agents, arising from the safety constraints imposed on robot agents. These terms are learned from previous executions using a Bayesian estimation; the inference of the posterior probability distribution of the synergy coefficients is performed using the Markov Chain Monte Carlo method. The synergy enhances task planning by adapting the nominal duration of the plan according to the effect of the operator's presence. Simulations and experimental results demonstrate that the proposed method produces improved human-aware task plans, reducing unuseful interference between agents, increasing human-robot distance, and achieving up to an 18\\% reduction in process execution time."
  },
  {
    "title": "Reactive and Safety-Aware Path Replanning for Collaborative Applications",
    "url": "http://arxiv.org/abs/2503.07192v1",
    "arxiv_id": "2503.07192v1",
    "authors": [
      "Cesare Tonola",
      "Marco Faroni",
      "Saeed Abdolshah",
      "Mazin Hamad",
      "Sami Haddadin",
      "Nicola Pedrocchi",
      "Manuel Beschi"
    ],
    "published": "2025-03-10T11:22:33+00:00",
    "summary": "This paper addresses motion replanning in human-robot collaborative scenarios, emphasizing reactivity and safety-compliant efficiency. While existing human-aware motion planners are effective in structured environments, they often struggle with unpredictable human behavior, leading to safety measures that limit robot performance and throughput. In this study, we combine reactive path replanning and a safety-aware cost function, allowing the robot to adjust its path to changes in the human state. This solution reduces the execution time and the need for trajectory slowdowns without sacrificing safety. Simulations and real-world experiments show the method's effectiveness compared to standard human-robot cooperation approaches, with efficiency enhancements of up to 60\\%."
  },
  {
    "title": "Correctness Learning: Deductive Verification Guided Learning for Human-AI Collaboration",
    "url": "http://arxiv.org/abs/2503.07096v1",
    "arxiv_id": "2503.07096v1",
    "authors": [
      "Zhao Jin",
      "Lu Jin",
      "Yizhe Luo",
      "Shuo Feng",
      "Yucheng Shi",
      "Kai Zheng",
      "Xinde Yu",
      "Mingliang Xu"
    ],
    "published": "2025-03-10T09:20:38+00:00",
    "summary": "Despite significant progress in AI and decision-making technologies in safety-critical fields, challenges remain in verifying the correctness of decision output schemes and verification-result driven design. We propose correctness learning (CL) to enhance human-AI collaboration integrating deductive verification methods and insights from historical high-quality schemes. The typical pattern hidden in historical high-quality schemes, such as change of task priorities in shared resources, provides critical guidance for intelligent agents in learning and decision-making. By utilizing deductive verification methods, we proposed patten-driven correctness learning (PDCL), formally modeling and reasoning the adaptive behaviors-or 'correctness pattern'-of system agents based on historical high-quality schemes, capturing the logical relationships embedded within these schemes. Using this logical information as guidance, we establish a correctness judgment and feedback mechanism to steer the intelligent decision model toward the 'correctness pattern' reflected in historical high-quality schemes. Extensive experiments across multiple working conditions and core parameters validate the framework's components and demonstrate its effectiveness in improving decision-making and resource optimization."
  },
  {
    "title": "Multimodal Human-AI Synergy for Medical Imaging Quality Control: A Hybrid Intelligence Framework with Adaptive Dataset Curation and Closed-Loop Evaluation",
    "url": "http://arxiv.org/abs/2503.07032v1",
    "arxiv_id": "2503.07032v1",
    "authors": [
      "Zhi Qin",
      "Qianhui Gui",
      "Mouxiao Bian",
      "Rui Wang",
      "Hong Ge",
      "Dandan Yao",
      "Ziying Sun",
      "Yuan Zhao",
      "Yu Zhang",
      "Hui Shi",
      "Dongdong Wang",
      "Chenxin Song",
      "Shenghong Ju",
      "Lihao Liu",
      "Junjun He",
      "Jie Xu",
      "Yuan-Cheng Wang"
    ],
    "published": "2025-03-10T08:16:18+00:00",
    "summary": "Medical imaging quality control (QC) is essential for accurate diagnosis, yet traditional QC methods remain labor-intensive and subjective. To address this challenge, in this study, we establish a standardized dataset and evaluation framework for medical imaging QC, systematically assessing large language models (LLMs) in image quality assessment and report standardization. Specifically, we first constructed and anonymized a dataset of 161 chest X-ray (CXR) radiographs and 219 CT reports for evaluation. Then, multiple LLMs, including Gemini 2.0-Flash, GPT-4o, and DeepSeek-R1, were evaluated based on recall, precision, and F1 score to detect technical errors and inconsistencies. Experimental results show that Gemini 2.0-Flash achieved a Macro F1 score of 90 in CXR tasks, demonstrating strong generalization but limited fine-grained performance. DeepSeek-R1 excelled in CT report auditing with a 62.23\\% recall rate, outperforming other models. However, its distilled variants performed poorly, while InternLM2.5-7B-chat exhibited the highest additional discovery rate, indicating broader but less precise error detection. These findings highlight the potential of LLMs in medical imaging QC, with DeepSeek-R1 and Gemini 2.0-Flash demonstrating superior performance."
  },
  {
    "title": "Combating Partial Perception Deficit in Autonomous Driving with Multimodal LLM Commonsense",
    "url": "http://arxiv.org/abs/2503.07020v1",
    "arxiv_id": "2503.07020v1",
    "authors": [
      "Yuting Hu",
      "Chenhui Xu",
      "Ruiyang Qin",
      "Dancheng Liu",
      "Amir Nassereldine",
      "Yiyu Shi",
      "Jinjun Xiong"
    ],
    "published": "2025-03-10T08:01:41+00:00",
    "summary": "Partial perception deficits can compromise autonomous vehicle safety by disrupting environmental understanding. Current protocols typically respond with immediate stops or minimal-risk maneuvers, worsening traffic flow and lacking flexibility for rare driving scenarios. In this paper, we propose LLM-RCO, a framework leveraging large language models to integrate human-like driving commonsense into autonomous systems facing perception deficits. LLM-RCO features four key modules: hazard inference, short-term motion planner, action condition verifier, and safety constraint generator. These modules interact with the dynamic driving environment, enabling proactive and context-aware control actions to override the original control policy of autonomous agents. To improve safety in such challenging conditions, we construct DriveLM-Deficit, a dataset of 53,895 video clips featuring deficits of safety-critical objects, complete with annotations for LLM-based hazard inference and motion planning fine-tuning. Extensive experiments in adverse driving conditions with the CARLA simulator demonstrate that systems equipped with LLM-RCO significantly improve driving performance, highlighting its potential for enhancing autonomous driving resilience against adverse perception deficits. Our results also show that LLMs fine-tuned with DriveLM-Deficit can enable more proactive movements instead of conservative stops in the context of perception deficits."
  },
  {
    "title": "Explicit Solution of Tunable Input-to-State Safe-Based Controller Under High-Relative-Degree Constraints",
    "url": "http://arxiv.org/abs/2503.07007v1",
    "arxiv_id": "2503.07007v1",
    "authors": [
      "Yan Wei",
      "Yu Feng",
      "Linlin Ou",
      "Yueying Wang",
      "Xinyi Yu"
    ],
    "published": "2025-03-10T07:40:17+00:00",
    "summary": "This paper investigates the safety analysis and verification of nonlinear systems subject to high-relative-degree constraints and unknown disturbance. The closed-form solution of the high-order control barrier functions (HOCBF) optimization problem with and without a nominal controller is first provided, making it unnecessary to solve the quadratic program problem online and facilitating the analysis. Further, we introduce the concept of tunable input-to-state safety(ISSf), and a new tunable function in conjunction with HOCBF is provided. When combined with the existing ISSf theorem, produces controllers for constrained nonlinear systems with external disturbances. The theoretical results are proven and supported by numerical simulations."
  },
  {
    "title": "Parametric Value Approximation for General-sum Differential Games with State Constraints",
    "url": "http://arxiv.org/abs/2503.06994v1",
    "arxiv_id": "2503.06994v1",
    "authors": [
      "Lei Zhang",
      "Mukesh Ghimire",
      "Wenlong Zhang",
      "Zhe Xu",
      "Yi Ren"
    ],
    "published": "2025-03-10T07:19:02+00:00",
    "summary": "General-sum differential games can approximate values solved by Hamilton-Jacobi-Isaacs (HJI) equations for efficient inference when information is incomplete. However, solving such games through conventional methods encounters the curse of dimensionality (CoD). Physics-informed neural networks (PINNs) offer a scalable approach to alleviate the CoD and approximate values, but there exist convergence issues for value approximations through vanilla PINNs when state constraints lead to values with large Lipschitz constants, particularly in safety-critical applications. In addition to addressing CoD, it is necessary to learn a generalizable value across a parametric space of games, rather than training multiple ones for each specific player-type configuration. To overcome these challenges, we propose a Hybrid Neural Operator (HNO), which is an operator that can map parameter functions for games to value functions. HNO leverages informative supervised data and samples PDE-driven data across entire spatial-temporal space for model refinement. We evaluate HNO on 9D and 13D scenarios with nonlinear dynamics and state constraints, comparing it against a Supervised Neural Operator (a variant of DeepONet). Under the same computational budget and training data, HNO outperforms SNO for safety performance. This work provides a step toward scalable and generalizable value function approximation, enabling real-time inference for complex human-robot or multi-agent interactions."
  },
  {
    "title": "Utilizing Jailbreak Probability to Attack and Safeguard Multimodal LLMs",
    "url": "http://arxiv.org/abs/2503.06989v1",
    "arxiv_id": "2503.06989v1",
    "authors": [
      "Wenzhuo Xu",
      "Zhipeng Wei",
      "Xiongtao Sun",
      "Deyue Zhang",
      "Dongdong Yang",
      "Quanchen Zou",
      "Xiangzheng Zhang"
    ],
    "published": "2025-03-10T07:10:38+00:00",
    "summary": "Recently, Multimodal Large Language Models (MLLMs) have demonstrated their superior ability in understanding multimodal contents. However, they remain vulnerable to jailbreak attacks, which exploit weaknesses in their safety alignment to generate harmful responses. Previous studies categorize jailbreaks as successful or failed based on whether responses contain malicious content. However, given the stochastic nature of MLLM responses, this binary classification of an input's ability to jailbreak MLLMs is inappropriate. Derived from this viewpoint, we introduce jailbreak probability to quantify the jailbreak potential of an input, which represents the likelihood that MLLMs generated a malicious response when prompted with this input. We approximate this probability through multiple queries to MLLMs. After modeling the relationship between input hidden states and their corresponding jailbreak probability using Jailbreak Probability Prediction Network (JPPN), we use continuous jailbreak probability for optimization. Specifically, we propose Jailbreak-Probability-based Attack (JPA) that optimizes adversarial perturbations on inputs to maximize jailbreak probability. To counteract attacks, we also propose two defensive methods: Jailbreak-Probability-based Finetuning (JPF) and Jailbreak-Probability-based Defensive Noise (JPDN), which minimizes jailbreak probability in the MLLM parameters and input space, respectively. Extensive experiments show that (1) JPA yields improvements (up to 28.38\\%) under both white and black box settings compared to previous methods with small perturbation bounds and few iterations. (2) JPF and JPDN significantly reduce jailbreaks by at most over 60\\%. Both of the above results demonstrate the significance of introducing jailbreak probability to make nuanced distinctions among input jailbreak abilities."
  },
  {
    "title": "Lshan-1.0 Technical Report",
    "url": "http://arxiv.org/abs/2503.06949v1",
    "arxiv_id": "2503.06949v1",
    "authors": [
      "Haotian Chen",
      "Yanyu Xu",
      "Boyan Wang",
      "Chaoyue Zhao",
      "Xiaoyu Han",
      "Fang Wang",
      "Lizhen Cui",
      "Yonghui Xu"
    ],
    "published": "2025-03-10T05:54:23+00:00",
    "summary": "In this report, we introduce our first-generation reasoning model, Lshan-1.0, a large language model designed for the highly specialized Chinese legal domain, offering comprehensive capabilities to meet diverse realistic needs. Existing legal LLMs face two primary challenges. Firstly, their design and evaluation are predominantly driven by computer science perspectives, leading to insufficient incorporation of legal expertise and logic, which is crucial for high-precision legal applications, such as handling complex prosecutorial tasks. Secondly, these models often underperform due to a lack of comprehensive training data from the legal domain, limiting their ability to effectively address real-world legal scenarios. To address this, we first compile millions of legal documents covering over 20 types of crimes from 31 provinces in China for model training. From the extensive dataset, we further select high-quality for supervised fine-tuning, ensuring enhanced relevance and precision. The model further undergoes large-scale reinforcement learning without additional supervision, emphasizing the enhancement of its reasoning capabilities and explainability. To validate its effectiveness in complex legal applications, we also conduct human evaluations with legal experts. We develop fine-tuned models based on DeepSeek-R1-Distilled versions, available in three dense configurations: 14B, 32B, and 70B."
  },
  {
    "title": "LexPro-1.0 Technical Report",
    "url": "http://arxiv.org/abs/2503.06949v2",
    "arxiv_id": "2503.06949v2",
    "authors": [
      "Haotian Chen",
      "Yanyu Xu",
      "Boyan Wang",
      "Chaoyue Zhao",
      "Xiaoyu Han",
      "Fang Wang",
      "Lizhen Cui",
      "Yonghui Xu"
    ],
    "published": "2025-03-10T05:54:23+00:00",
    "summary": "In this report, we introduce our first-generation reasoning model, LexPro-1.0, a large language model designed for the highly specialized Chinese legal domain, offering comprehensive capabilities to meet diverse realistic needs. Existing legal LLMs face two primary challenges. Firstly, their design and evaluation are predominantly driven by computer science perspectives, leading to insufficient incorporation of legal expertise and logic, which is crucial for high-precision legal applications, such as handling complex prosecutorial tasks. Secondly, these models often underperform due to a lack of comprehensive training data from the legal domain, limiting their ability to effectively address real-world legal scenarios. To address this, we first compile millions of legal documents covering over 20 types of crimes from 31 provinces in China for model training. From the extensive dataset, we further select high-quality for supervised fine-tuning, ensuring enhanced relevance and precision. The model further undergoes large-scale reinforcement learning without additional supervision, emphasizing the enhancement of its reasoning capabilities and explainability. To validate its effectiveness in complex legal applications, we also conduct human evaluations with legal experts. We develop fine-tuned models based on DeepSeek-R1-Distilled versions, available in three dense configurations: 14B, 32B, and 70B."
  },
  {
    "title": "SafePlan: Leveraging Formal Logic and Chain-of-Thought Reasoning for Enhanced Safety in LLM-based Robotic Task Planning",
    "url": "http://arxiv.org/abs/2503.06892v1",
    "arxiv_id": "2503.06892v1",
    "authors": [
      "Ike Obi",
      "Vishnunandan L. N. Venkatesh",
      "Weizheng Wang",
      "Ruiqi Wang",
      "Dayoon Suh",
      "Temitope I. Amosa",
      "Wonse Jo",
      "Byung-Cheol Min"
    ],
    "published": "2025-03-10T03:37:36+00:00",
    "summary": "Robotics researchers increasingly leverage large language models (LLM) in robotics systems, using them as interfaces to receive task commands, generate task plans, form team coalitions, and allocate tasks among multi-robot and human agents. However, despite their benefits, the growing adoption of LLM in robotics has raised several safety concerns, particularly regarding executing malicious or unsafe natural language prompts. In addition, ensuring that task plans, team formation, and task allocation outputs from LLMs are adequately examined, refined, or rejected is crucial for maintaining system integrity. In this paper, we introduce SafePlan, a multi-component framework that combines formal logic and chain-of-thought reasoners for enhancing the safety of LLM-based robotics systems. Using the components of SafePlan, including Prompt Sanity COT Reasoner and Invariant, Precondition, and Postcondition COT reasoners, we examined the safety of natural language task prompts, task plans, and task allocation outputs generated by LLM-based robotic systems as means of investigating and enhancing system safety profile. Our results show that SafePlan outperforms baseline models by leading to 90.5% reduction in harmful task prompt acceptance while still maintaining reasonable acceptance of safe tasks."
  },
  {
    "title": "Graphormer-Guided Task Planning: Beyond Static Rules with LLM Safety Perception",
    "url": "http://arxiv.org/abs/2503.06866v1",
    "arxiv_id": "2503.06866v1",
    "authors": [
      "Wanjing Huang",
      "Tongjie Pan",
      "Yalan Ye"
    ],
    "published": "2025-03-10T02:43:54+00:00",
    "summary": "Recent advancements in large language models (LLMs) have expanded their role in robotic task planning. However, while LLMs have been explored for generating feasible task sequences, their ability to ensure safe task execution remains underdeveloped. Existing methods struggle with structured risk perception, making them inadequate for safety-critical applications where low-latency hazard adaptation is required. To address this limitation, we propose a Graphormer-enhanced risk-aware task planning framework that combines LLM-based decision-making with structured safety modeling. Our approach constructs a dynamic spatio-semantic safety graph, capturing spatial and contextual risk factors to enable online hazard detection and adaptive task refinement. Unlike existing methods that rely on predefined safety constraints, our framework introduces a context-aware risk perception module that continuously refines safety predictions based on real-time task execution. This enables a more flexible and scalable approach to robotic planning, allowing for adaptive safety compliance beyond static rules. To validate our framework, we conduct experiments in the AI2-THOR environment. The experiments results validates improvements in risk detection accuracy, rising safety notice, and task adaptability of our framework in continuous environments compared to static rule-based and LLM-only baselines. Our project is available at https://github.com/hwj20/GGTP"
  },
  {
    "title": "Physically Large Apertures for Wireless Power Transfer: Performance and Regulatory Aspects",
    "url": "http://arxiv.org/abs/2503.06807v1",
    "arxiv_id": "2503.06807v1",
    "authors": [
      "Benjamin J. B. Deutschmann",
      "Ulrich Muehlmann",
      "Ahmet Kaplan",
      "Gilles Callebaut",
      "Thomas Wilding",
      "Bert Cox",
      "Liesbet Van der Perre",
      "Fredrik Tufvesson",
      "Erik G. Larsson",
      "Klaus Witrisal"
    ],
    "published": "2025-03-09T23:28:06+00:00",
    "summary": "Wireless power transfer (WPT) is a promising service for the Internet of Things, providing a cost-effective and sustainable solution to deploy so-called energy-neutral devices on a massive scale. The power received at the device side decays rapidly with the distance from a conventional transmit antenna with a physically small aperture. New opportunities arise from the transition from conventional far-field beamforming to near-field beam focusing. We argue that a \"physically large\" aperture, i.e., large w.r.t. the distance to the receiver, enables a power budget that remains practically independent of distance. Distance-dependent array gain patterns allow focusing the power density maximum precisely at the device location, while reducing the power density near the infrastructure. The physical aperture size is a key resource in enabling efficient yet regulatory-compliant WPT. We use real-world measurements to demonstrate that a regulatory-compliant system operating at sub-10GHz frequencies can increase the power received at the device into the milliwatt range. Our empirical demonstration shows that power-optimal near-field beam focusing inherently exploits multipath propagation, yielding both increased WPT efficiency and improved human exposure safety in real-world scenarios."
  },
  {
    "title": "AutoMisty: A Multi-Agent LLM Framework for Automated Code Generation in the Misty Social Robot",
    "url": "http://arxiv.org/abs/2503.06791v1",
    "arxiv_id": "2503.06791v1",
    "authors": [
      "Xiao Wang",
      "Lu Dong",
      "Sahana Rangasrinivasan",
      "Ifeoma Nwogu",
      "Srirangaraj Setlur",
      "Venugopal Govindaraju"
    ],
    "published": "2025-03-09T22:07:46+00:00",
    "summary": "The social robot's open API allows users to customize open-domain interactions. However, it remains inaccessible to those without programming experience. In this work, we introduce AutoMisty, the first multi-agent collaboration framework powered by large language models (LLMs), to enable the seamless generation of executable Misty robot code from natural language instructions. AutoMisty incorporates four specialized agent modules to manage task decomposition, assignment, problem-solving, and result synthesis. Each agent incorporates a two-layer optimization mechanism, with self-reflection for iterative refinement and human-in-the-loop for better alignment with user preferences. AutoMisty ensures a transparent reasoning process, allowing users to iteratively refine tasks through natural language feedback for precise execution. To evaluate AutoMisty's effectiveness, we designed a benchmark task set spanning four levels of complexity and conducted experiments in a real Misty robot environment. Extensive evaluations demonstrate that AutoMisty not only consistently generates high-quality code but also enables precise code control, significantly outperforming direct reasoning with ChatGPT-4o and ChatGPT-o1. All code, optimized APIs, and experimental videos will be publicly released through the webpage: https://wangxiaoshawn.github.io/AutoMisty.html"
  },
  {
    "title": "Chance-constrained Linear Quadratic Gaussian Games for Multi-robot Interaction under Uncertainty",
    "url": "http://arxiv.org/abs/2503.06776v1",
    "arxiv_id": "2503.06776v1",
    "authors": [
      "Kai Ren",
      "Giulio Salizzoni",
      "Mustafa Emre G\u00fcrsoy",
      "Maryam Kamgarpour"
    ],
    "published": "2025-03-09T21:03:53+00:00",
    "summary": "We address safe multi-robot interaction under uncertainty. In particular, we formulate a chance-constrained linear quadratic Gaussian game with coupling constraints and system uncertainties. We find a tractable reformulation of the game and propose a dual ascent algorithm. We prove that the algorithm converges to a generalized Nash equilibrium of the reformulated game, ensuring the satisfaction of the chance constraints. We test our method in driving simulations and real-world robot experiments. Our method ensures safety under uncertainty and generates less conservative trajectories than single-agent model predictive control."
  },
  {
    "title": "Cell-Free MIMO-ISAC: Joint Location and Velocity Estimation and Fundamental CRLB Analysis",
    "url": "http://arxiv.org/abs/2503.06766v1",
    "arxiv_id": "2503.06766v1",
    "authors": [
      "Guoqing Xia",
      "Pei Xiao",
      "Qu Luo",
      "Bing Ji",
      "Yue Zhang",
      "Huiyu Zhou"
    ],
    "published": "2025-03-09T20:43:18+00:00",
    "summary": "This paper investigates joint location and velocity estimation, along with their fundamental performance bounds analysis, in a cell-free multi-input multi-output (MIMO) integrated sensing and communication (ISAC) system. First, unlike existing studies that derive likelihood functions for target parameter estimation using continuous received signals, we formulate the maximum likelihood estimation (MLE) for radar sensing based on discrete received signals at a given sampling rate. Second, leveraging the proposed MLEs, we derive closed-form Cramer-Rao lower bounds (CRLBs) for joint location and velocity estimation in both single-target and multiple-target scenarios. Third, to enhance computational efficiency, we propose approximate CRLBs and conduct an in-depth accuracy analysis. Additionally, we thoroughly examine the impact of sampling rate, squared effective bandwidth, and time width on CRLB performance. For multiple-target scenarios, the concepts of safety distance and safety velocity are introduced to characterize conditions under which the CRLBs for multiple targets converge to their single target counterparts. Finally, extensive simulations are conducted to verify the accuracy of the proposed CRLBs and the theoretical results using state-of-the-art waveforms, namely orthogonal frequency division multiplexing (OFDM) and orthogonal chirp division multiplexing (OCDM)."
  },
  {
    "title": "Vision-R1: Incentivizing Reasoning Capability in Multimodal Large Language Models",
    "url": "http://arxiv.org/abs/2503.06749v1",
    "arxiv_id": "2503.06749v1",
    "authors": [
      "Wenxuan Huang",
      "Bohan Jia",
      "Zijie Zhai",
      "Shaosheng Cao",
      "Zheyu Ye",
      "Fei Zhao",
      "Yao Hu",
      "Shaohui Lin"
    ],
    "published": "2025-03-09T20:06:45+00:00",
    "summary": "DeepSeek-R1-Zero has successfully demonstrated the emergence of reasoning capabilities in LLMs purely through Reinforcement Learning (RL). Inspired by this breakthrough, we explore how RL can be utilized to enhance the reasoning capability of MLLMs. However, direct training with RL struggles to activate complex reasoning capabilities such as questioning and reflection in MLLMs, due to the absence of substantial high-quality multimodal reasoning data. To address this issue, we propose the reasoning MLLM, Vision-R1, to improve multimodal reasoning capability. Specifically, we first construct a high-quality multimodal CoT dataset without human annotations by leveraging an existing MLLM and DeepSeek-R1 through modality bridging and data filtering to obtain a 200K multimodal CoT dataset, Vision-R1-cold dataset. It serves as cold-start initialization data for Vision-R1. To mitigate the optimization challenges caused by overthinking after cold start, we propose Progressive Thinking Suppression Training (PTST) strategy and employ Group Relative Policy Optimization (GRPO) with the hard formatting result reward function to gradually refine the model's ability to learn correct and complex reasoning processes on a 10K multimodal math dataset. Comprehensive experiments show our model achieves an average improvement of $\\sim$6% across various multimodal math reasoning benchmarks. Vision-R1-7B achieves a 73.5% accuracy on the widely used MathVista benchmark, which is only 0.4% lower than the leading reasoning model, OpenAI O1. The datasets and code will be released in: https://github.com/Osilly/Vision-R1 ."
  },
  {
    "title": "Vision-R1: Incentivizing Reasoning Capability in Multimodal Large Language Models",
    "url": "http://arxiv.org/abs/2503.06749v2",
    "arxiv_id": "2503.06749v2",
    "authors": [
      "Wenxuan Huang",
      "Bohan Jia",
      "Zijie Zhai",
      "Shaosheng Cao",
      "Zheyu Ye",
      "Fei Zhao",
      "Zhe Xu",
      "Yao Hu",
      "Shaohui Lin"
    ],
    "published": "2025-03-09T20:06:45+00:00",
    "summary": "DeepSeek-R1-Zero has successfully demonstrated the emergence of reasoning capabilities in LLMs purely through Reinforcement Learning (RL). Inspired by this breakthrough, we explore how RL can be utilized to enhance the reasoning capability of MLLMs. However, direct training with RL struggles to activate complex reasoning capabilities such as questioning and reflection in MLLMs, due to the absence of substantial high-quality multimodal reasoning data. To address this issue, we propose the reasoning MLLM, Vision-R1, to improve multimodal reasoning capability. Specifically, we first construct a high-quality multimodal CoT dataset without human annotations by leveraging an existing MLLM and DeepSeek-R1 through modality bridging and data filtering to obtain a 200K multimodal CoT dataset, Vision-R1-cold dataset. It serves as cold-start initialization data for Vision-R1. To mitigate the optimization challenges caused by overthinking after cold start, we propose Progressive Thinking Suppression Training (PTST) strategy and employ Group Relative Policy Optimization (GRPO) with the hard formatting result reward function to gradually refine the model's ability to learn correct and complex reasoning processes on a 10K multimodal math dataset. Comprehensive experiments show our model achieves an average improvement of $\\sim$6% across various multimodal math reasoning benchmarks. Vision-R1-7B achieves a 73.5% accuracy on the widely used MathVista benchmark, which is only 0.4% lower than the leading reasoning model, OpenAI O1. The datasets and code will be released in: https://github.com/Osilly/Vision-R1 ."
  },
  {
    "title": "Safe, Task-Consistent Manipulation with Operational Space Control Barrier Functions",
    "url": "http://arxiv.org/abs/2503.06736v1",
    "arxiv_id": "2503.06736v1",
    "authors": [
      "Daniel Morton",
      "Marco Pavone"
    ],
    "published": "2025-03-09T19:29:15+00:00",
    "summary": "Safe real-time control of robotic manipulators in unstructured environments requires handling numerous safety constraints without compromising task performance. Traditional approaches, such as artificial potential fields (APFs), suffer from local minima, oscillations, and limited scalability, while model predictive control (MPC) can be computationally expensive. Control barrier functions (CBFs) offer a promising alternative due to their high level of robustness and low computational cost, but these safety filters must be carefully designed to avoid significant reductions in the overall performance of the manipulator. In this work, we introduce an Operational Space Control Barrier Function (OSCBF) framework that integrates safety constraints while preserving task-consistent behavior. Our approach scales to hundreds of simultaneous constraints while retaining real-time control rates, ensuring collision avoidance, singularity prevention, and workspace containment even in highly cluttered and dynamic settings. By explicitly accounting for the task hierarchy in the CBF objective, we prevent degraded performance across both joint-space and operational-space tasks, when at the limit of safety. Our open-source, high-performance software will be available at our project webpage, https://stanfordasl.github.io/oscbf/"
  },
  {
    "title": "Diffusion Model Based Probabilistic Day-ahead Load Forecasting",
    "url": "http://arxiv.org/abs/2503.06697v1",
    "arxiv_id": "2503.06697v1",
    "authors": [
      "Ding Lin",
      "Han Guo",
      "Jianhui Wang"
    ],
    "published": "2025-03-09T17:27:12+00:00",
    "summary": "Accurate probabilistic load forecasting is crucial for maintaining the safety and stability of power systems. However, the mainstream approach, multi-step prediction, must be improved by cumulative errors and latency issues, which limits its effectiveness in probabilistic day-ahead load forecasting (PDALF). To overcome these challenges, we introduce DALNet, a novel denoising diffusion model designed to generate load curves rather than relying on direct prediction. By shifting the focus to curve generation, DALNet captures the complex distribution of actual load time-series data under specific conditions with greater fidelity. To further enhance DALNet, we propose the temporal multi-scale attention block (TMSAB), a mechanism designed to integrate both positional and temporal information for improved forecasting precision. Furthermore, we utilize kernel density estimation (KDE) to reconstruct the distribution of generated load curves and employ KL divergence to compare them with the actual data distribution. Experimental results demonstrate that DALNet excels in load forecasting accuracy and offers a novel perspective for other predictive tasks within power systems."
  },
  {
    "title": "PANDA: Parkinson's Assistance and Notification Driving Aid",
    "url": "http://arxiv.org/abs/2503.06659v1",
    "arxiv_id": "2503.06659v1",
    "authors": [
      "Tianyang Wen",
      "Xucheng Zhang",
      "Zhirong Wan",
      "Jing Zhao",
      "Yicheng Zhu",
      "Ning Su",
      "Xiaolan Peng",
      "Jin Huang",
      "Wei Sun",
      "Feng Tian",
      "Franklin Mingzhe Li"
    ],
    "published": "2025-03-09T15:19:04+00:00",
    "summary": "Parkinson's Disease (PD) significantly impacts driving abilities, often leading to early driving cessation or accidents due to reduced motor control and increasing reaction times. To diminish the impact of these symptoms, we developed PANDA (Parkinson's Assistance and Notification Driving Aid), a multi-modality real-time alert system designed to monitor driving patterns continuously and provide immediate alerts for irregular driving behaviors, enhancing driver safety of individuals with PD. The system was developed through a participatory design process with 9 people with PD and 13 non-PD individuals using a driving simulator, which allowed us to identify critical design characteristics and collect detailed data on driving behavior. A user study involving individuals with PD evaluated the effectiveness of PANDA, exploring optimal strategies for delivering alerts and ensuring they are timely and helpful. Our findings demonstrate that PANDA has the potential to enhance the driving safety of individuals with PD, offering a valuable tool for maintaining independence and confidence behind the wheel."
  },
  {
    "title": "Reinforcement Learning with Verifiable Rewards: GRPO's Effective Loss, Dynamics, and Success Amplification",
    "url": "http://arxiv.org/abs/2503.06639v1",
    "arxiv_id": "2503.06639v1",
    "authors": [
      "Youssef Mroueh"
    ],
    "published": "2025-03-09T14:36:45+00:00",
    "summary": "Group Relative Policy Optimization (GRPO) was introduced and used successfully to train DeepSeek R1 models for promoting reasoning capabilities of LLMs using verifiable or binary rewards. We show in this paper that GRPO with verifiable rewards can be written as a Kullback Leibler ($\\mathsf{KL}$) regularized contrastive loss, where the contrastive samples are synthetic data sampled from the old policy. The optimal GRPO policy $\\pi_{n}$ can be expressed explicitly in terms of the binary reward, as well as the first and second order statistics of the old policy ($\\pi_{n-1}$) and the reference policy $\\pi_0$. Iterating this scheme, we obtain a sequence of policies $\\pi_{n}$ for which we can quantify the probability of success $p_n$. We show that the probability of success of the policy satisfies a recurrence that converges to a fixed point of a function that depends on the initial probability of success $p_0$ and the regularization parameter $\\beta$ of the $\\mathsf{KL}$ regularizer. We show that the fixed point $p^*$ is guaranteed to be larger than $p_0$, thereby demonstrating that GRPO effectively amplifies the probability of success of the policy."
  },
  {
    "title": "Intelligent Control of Merging Car-following and Lane-Changing Behavior",
    "url": "http://arxiv.org/abs/2503.06572v1",
    "arxiv_id": "2503.06572v1",
    "authors": [
      "Farzam Tajdari",
      "Amin Rezasoltani"
    ],
    "published": "2025-03-09T12:03:46+00:00",
    "summary": "Recent research has paid little attention to complex driving behaviors, namely merging car-following and lane-changing behavior, and how lane-changing affects algorithms designed to model and control a car-following vehicle. During the merging behavior, the Follower Vehicle (FV) might significantly diverge from typical car-following models. Thus, this paper aims to control the FV witnessing lane-changing behavior based on anticipation, perception, preparation, and relaxation states defined by a novel measurable human perception index. Data from human drivers are utilized to create a perception-based fuzzy controller for the behavior vehicle's route guidance, taking into account the opacity of human driving judgments. We illustrate the efficacy of the established technique using simulated trials and data from actual drivers, focusing on the benefits of the increased comfort, safety, and uniformity of traffic flow and the decreased of wait time and motion sickness this brings about."
  },
  {
    "title": "Multimodal Programming in Computer Science with Interactive Assistance Powered by Large Language Model",
    "url": "http://arxiv.org/abs/2503.06552v1",
    "arxiv_id": "2503.06552v1",
    "authors": [
      "Rajan Das Gupta",
      "Md. Tanzib Hosain",
      "M. F. Mridha",
      "Salah Uddin Ahmed"
    ],
    "published": "2025-03-09T10:48:47+00:00",
    "summary": "LLM chatbot interfaces allow students to get instant, interactive assistance with homework, but doing so carelessly may not advance educational objectives. In this study, an interactive homework help system based on DeepSeek R1 is developed and first implemented for students enrolled in a large computer science beginning programming course. In addition to an assist button in a well-known code editor, our assistant also has a feedback option in our command-line automatic evaluator. It wraps student work in a personalized prompt that advances our educational objectives without offering answers straight away. We have discovered that our assistant can recognize students' conceptual difficulties and provide ideas, plans, and template code in pedagogically appropriate ways. However, among other mistakes, it occasionally incorrectly labels the correct student code as incorrect or encourages students to use correct-but-lesson-inappropriate approaches, which can lead to long and frustrating journeys for the students. After discussing many development and deployment issues, we provide our conclusions and future actions."
  },
  {
    "title": "BingoGuard: LLM Content Moderation Tools with Risk Levels",
    "url": "http://arxiv.org/abs/2503.06550v1",
    "arxiv_id": "2503.06550v1",
    "authors": [
      "Fan Yin",
      "Philippe Laban",
      "Xiangyu Peng",
      "Yilun Zhou",
      "Yixin Mao",
      "Vaibhav Vats",
      "Linnea Ross",
      "Divyansh Agarwal",
      "Caiming Xiong",
      "Chien-Sheng Wu"
    ],
    "published": "2025-03-09T10:43:09+00:00",
    "summary": "Malicious content generated by large language models (LLMs) can pose varying degrees of harm. Although existing LLM-based moderators can detect harmful content, they struggle to assess risk levels and may miss lower-risk outputs. Accurate risk assessment allows platforms with different safety thresholds to tailor content filtering and rejection. In this paper, we introduce per-topic severity rubrics for 11 harmful topics and build BingoGuard, an LLM-based moderation system designed to predict both binary safety labels and severity levels. To address the lack of annotations on levels of severity, we propose a scalable generate-then-filter framework that first generates responses across different severity levels and then filters out low-quality responses. Using this framework, we create BingoGuardTrain, a training dataset with 54,897 examples covering a variety of topics, response severity, styles, and BingoGuardTest, a test set with 988 examples explicitly labeled based on our severity rubrics that enables fine-grained analysis on model behaviors on different severity levels. Our BingoGuard-8B, trained on BingoGuardTrain, achieves the state-of-the-art performance on several moderation benchmarks, including WildGuardTest and HarmBench, as well as BingoGuardTest, outperforming best public models, WildGuard, by 4.3\\%. Our analysis demonstrates that incorporating severity levels into training significantly enhances detection performance and enables the model to effectively gauge the severity of harmful responses."
  },
  {
    "title": "AnywhereDoor: Multi-Target Backdoor Attacks on Object Detection",
    "url": "http://arxiv.org/abs/2503.06529v1",
    "arxiv_id": "2503.06529v1",
    "authors": [
      "Jialin Lu",
      "Junjie Shan",
      "Ziqi Zhao",
      "Ka-Ho Chow"
    ],
    "published": "2025-03-09T09:24:24+00:00",
    "summary": "As object detection becomes integral to many safety-critical applications, understanding its vulnerabilities is essential. Backdoor attacks, in particular, pose a serious threat by implanting hidden triggers in victim models, which adversaries can later exploit to induce malicious behaviors during inference. However, current understanding is limited to single-target attacks, where adversaries must define a fixed malicious behavior (target) before training, making inference-time adaptability impossible. Given the large output space of object detection (including object existence prediction, bounding box estimation, and classification), the feasibility of flexible, inference-time model control remains unexplored. This paper introduces AnywhereDoor, a multi-target backdoor attack for object detection. Once implanted, AnywhereDoor allows adversaries to make objects disappear, fabricate new ones, or mislabel them, either across all object classes or specific ones, offering an unprecedented degree of control. This flexibility is enabled by three key innovations: (i) objective disentanglement to scale the number of supported targets; (ii) trigger mosaicking to ensure robustness even against region-based detectors; and (iii) strategic batching to address object-level data imbalances that hinder manipulation. Extensive experiments demonstrate that AnywhereDoor grants attackers a high degree of control, improving attack success rates by 26% compared to adaptations of existing methods for such flexible control."
  },
  {
    "title": "Can Small Language Models Reliably Resist Jailbreak Attacks? A Comprehensive Evaluation",
    "url": "http://arxiv.org/abs/2503.06519v1",
    "arxiv_id": "2503.06519v1",
    "authors": [
      "Wenhui Zhang",
      "Huiyu Xu",
      "Zhibo Wang",
      "Zeqing He",
      "Ziqi Zhu",
      "Kui Ren"
    ],
    "published": "2025-03-09T08:47:16+00:00",
    "summary": "Small language models (SLMs) have emerged as promising alternatives to large language models (LLMs) due to their low computational demands, enhanced privacy guarantees and comparable performance in specific domains through light-weight fine-tuning. Deploying SLMs on edge devices, such as smartphones and smart vehicles, has become a growing trend. However, the security implications of SLMs have received less attention than LLMs, particularly regarding jailbreak attacks, which is recognized as one of the top threats of LLMs by the OWASP. In this paper, we conduct the first large-scale empirical study of SLMs' vulnerabilities to jailbreak attacks. Through systematically evaluation on 63 SLMs from 15 mainstream SLM families against 8 state-of-the-art jailbreak methods, we demonstrate that 47.6% of evaluated SLMs show high susceptibility to jailbreak attacks (ASR > 40%) and 38.1% of them can not even resist direct harmful query (ASR > 50%). We further analyze the reasons behind the vulnerabilities and identify four key factors: model size, model architecture, training datasets and training techniques. Moreover, we assess the effectiveness of three prompt-level defense methods and find that none of them achieve perfect performance, with detection accuracy varying across different SLMs and attack methods. Notably, we point out that the inherent security awareness play a critical role in SLM security, and models with strong security awareness could timely terminate unsafe response with little reminder. Building upon the findings, we highlight the urgent need for security-by-design approaches in SLM development and provide valuable insights for building more trustworthy SLM ecosystem."
  },
  {
    "title": "Evaluation of Safety Cognition Capability in Vision-Language Models for Autonomous Driving",
    "url": "http://arxiv.org/abs/2503.06497v1",
    "arxiv_id": "2503.06497v1",
    "authors": [
      "Enming Zhang",
      "Peizhe Gong",
      "Xingyuan Dai",
      "Yisheng Lv",
      "Qinghai Miao"
    ],
    "published": "2025-03-09T07:53:19+00:00",
    "summary": "Assessing the safety of vision-language models (VLMs) in autonomous driving is particularly important; however, existing work mainly focuses on traditional benchmark evaluations. As interactive components within autonomous driving systems, VLMs must maintain strong safety cognition during interactions. From this perspective, we propose a novel evaluation method: Safety Cognitive Driving Benchmark (SCD-Bench) . To address the large-scale annotation challenge for SCD-Bench, we develop the Autonomous Driving Image-Text Annotation System (ADA) . Additionally, to ensure data quality in SCD-Bench, our dataset undergoes manual refinement by experts with professional knowledge in autonomous driving. We further develop an automated evaluation method based on large language models (LLMs). To verify its effectiveness, we compare its evaluation results with those of expert human evaluations, achieving a consistency rate of 99.74%. Preliminary experimental results indicate that existing open-source models still lack sufficient safety cognition, showing a significant gap compared to GPT-4o. Notably, lightweight models (1B-4B) demonstrate minimal safety cognition. However, since lightweight models are crucial for autonomous driving systems, this presents a significant challenge for integrating VLMs into the field."
  },
  {
    "title": "OT-DETECTOR: Delving into Optimal Transport for Zero-shot Out-of-Distribution Detection",
    "url": "http://arxiv.org/abs/2503.06442v1",
    "arxiv_id": "2503.06442v1",
    "authors": [
      "Yu Liu",
      "Hao Tang",
      "Haiqi Zhang",
      "Jing Qin",
      "Zechao Li"
    ],
    "published": "2025-03-09T04:47:19+00:00",
    "summary": "Out-of-distribution (OOD) detection is crucial for ensuring the reliability and safety of machine learning models in real-world applications. While zero-shot OOD detection, which requires no training on in-distribution (ID) data, has become feasible with the emergence of vision-language models like CLIP, existing methods primarily focus on semantic matching and fail to fully capture distributional discrepancies. To address these limitations, we propose OT-DETECTOR, a novel framework that employs Optimal Transport (OT) to quantify both semantic and distributional discrepancies between test samples and ID labels. Specifically, we introduce cross-modal transport mass and transport cost as semantic-wise and distribution-wise OOD scores, respectively, enabling more robust detection of OOD samples. Additionally, we present a semantic-aware content refinement (SaCR) module, which utilizes semantic cues from ID labels to amplify the distributional discrepancy between ID and hard OOD samples. Extensive experiments on several benchmarks demonstrate that OT-DETECTOR achieves state-of-the-art performance across various OOD detection tasks, particularly in challenging hard-OOD scenarios."
  },
  {
    "title": "Explaining Control Policies through Predicate Decision Diagrams",
    "url": "http://arxiv.org/abs/2503.06420v1",
    "arxiv_id": "2503.06420v1",
    "authors": [
      "Debraj Chakraborty",
      "Clemens Dubslaff",
      "Sudeep Kanav",
      "Jan Kretinsky",
      "Christoph Weinhuber"
    ],
    "published": "2025-03-09T03:31:48+00:00",
    "summary": "Safety-critical controllers of complex systems are hard to construct manually. Automated approaches such as controller synthesis or learning provide a tempting alternative but usually lack explainability. To this end, learning decision trees (DTs) have been prevalently used towards an interpretable model of the generated controllers. However, DTs do not exploit shared decision-making, a key concept exploited in binary decision diagrams (BDDs) to reduce their size and thus improve explainability. In this work, we introduce predicate decision diagrams (PDDs) that extend BDDs with predicates and thus unite the advantages of DTs and BDDs for controller representation. We establish a synthesis pipeline for efficient construction of PDDs from DTs representing controllers, exploiting reduction techniques for BDDs also for PDDs."
  },
  {
    "title": "Exponential-polynomial divergence based inference for nondestructive one-shot devices under progressive stress model",
    "url": "http://arxiv.org/abs/2503.06414v1",
    "arxiv_id": "2503.06414v1",
    "authors": [
      "Shanya Baghel",
      "Shuvashree Mondal"
    ],
    "published": "2025-03-09T03:16:21+00:00",
    "summary": "Nondestructive one-shot device (NOSD) testing plays a crucial role in engineering, particularly in the reliability assessment of high-stakes systems such as aerospace components, medical devices, and semiconductor technologies. Accurate reliability prognosis of NOSD testing data is essential for ensuring product durability, safety, and performance optimization. The conventional estimation methods like maximum likelihood estimation (MLE) are sensitive to data contamination, leading to biased results. Consequently, this study develops robust inferential analysis for NOSD testing data under a progressive stress model. The lifetime of NOSD is assumed to follow Log-logistic distribution. The estimation procedure addresses robustness by incorporating Exponential-polynomial divergence (EPD). Equipped with three tuning parameters, EPD based estimation is proven to be more flexible than density power divergence estimation frequently used for one-shot device testing data analysis. Further, we explore the asymptotic behaviour of minimum EPD estimator (MEPDE) for large sample size. The robustness of MEPDE is analytically studied through influence function. Since tradeoff between efficiency and robustness of EPD based estimation is governed by three tuning parameters, a novel approach leveraging Concrete Score Matching (CSM) is introduced to optimize the tuning parameters of MEPDE. Moreover, a comparative study with the existing methods of finding tuning parameters is conducted through extensive simulation experiment and data analysis. Another aspect of this study is determining an optimal plan to ensure a successful ALT experiment within specified budget and time constraints. It is designed on A-optimality criteria subject to the given constraints and is executed using the constraint particle swarm optimization (CPSO) algorithm."
  },
  {
    "title": "Decoding the Black Box: Integrating Moral Imagination with Technical AI Governance",
    "url": "http://arxiv.org/abs/2503.06411v1",
    "arxiv_id": "2503.06411v1",
    "authors": [
      "Krti Tallam"
    ],
    "published": "2025-03-09T03:11:32+00:00",
    "summary": "This paper examines the intricate interplay among AI safety, security, and governance by integrating technical systems engineering with principles of moral imagination and ethical philosophy. Drawing on foundational insights from Weapons of Math Destruction and Thinking in Systems alongside contemporary debates in AI ethics, we develop a comprehensive multi-dimensional framework designed to regulate AI technologies deployed in high-stakes domains such as defense, finance, healthcare, and education. Our approach combines rigorous technical analysis, quantitative risk assessment, and normative evaluation to expose systemic vulnerabilities inherent in opaque, black-box models. Detailed case studies, including analyses of Microsoft Tay (2016) and the UK A-Level Grading Algorithm (2020), demonstrate how security lapses, bias amplification, and lack of accountability can precipitate cascading failures that undermine public trust. We conclude by outlining targeted strategies for enhancing AI resilience through adaptive regulatory mechanisms, robust security protocols, and interdisciplinary oversight, thereby advancing the state of the art in ethical and technical AI governance."
  },
  {
    "title": "Backdoor Attacks on Discrete Graph Diffusion Models",
    "url": "http://arxiv.org/abs/2503.06340v1",
    "arxiv_id": "2503.06340v1",
    "authors": [
      "Jiawen Wang",
      "Samin Karim",
      "Yuan Hong",
      "Binghui Wang"
    ],
    "published": "2025-03-08T21:01:15+00:00",
    "summary": "Diffusion models are powerful generative models in continuous data domains such as image and video data. Discrete graph diffusion models (DGDMs) have recently extended them for graph generation, which are crucial in fields like molecule and protein modeling, and obtained the SOTA performance. However, it is risky to deploy DGDMs for safety-critical applications (e.g., drug discovery) without understanding their security vulnerabilities. In this work, we perform the first study on graph diffusion models against backdoor attacks, a severe attack that manipulates both the training and inference/generation phases in graph diffusion models. We first define the threat model, under which we design the attack such that the backdoored graph diffusion model can generate 1) high-quality graphs without backdoor activation, 2) effective, stealthy, and persistent backdoored graphs with backdoor activation, and 3) graphs that are permutation invariant and exchangeable--two core properties in graph generative models. 1) and 2) are validated via empirical evaluations without and with backdoor defenses, while 3) is validated via theoretical results."
  },
  {
    "title": "Accurate and Efficient Two-Stage Gun Detection in Video",
    "url": "http://arxiv.org/abs/2503.06317v1",
    "arxiv_id": "2503.06317v1",
    "authors": [
      "Badhan Chandra Das",
      "M. Hadi Amini",
      "Yanzhao Wu"
    ],
    "published": "2025-03-08T19:26:23+00:00",
    "summary": "Object detection in videos plays a crucial role in advancing applications such as public safety and anomaly detection. Existing methods have explored different techniques, including CNN, deep learning, and Transformers, for object detection and video classification. However, detecting tiny objects, e.g., guns, in videos remains challenging due to their small scale and varying appearances in complex scenes. Moreover, existing video analysis models for classification or detection often perform poorly in real-world gun detection scenarios due to limited labeled video datasets for training. Thus, developing efficient methods for effectively capturing tiny object features and designing models capable of accurate gun detection in real-world videos is imperative. To address these challenges, we make three original contributions in this paper. First, we conduct an empirical study of several existing video classification and object detection methods to identify guns in videos. Our extensive analysis shows that these methods may not accurately detect guns in videos. Second, we propose a novel two-stage gun detection method. In stage 1, we train an image-augmented model to effectively classify ``Gun'' videos. To make the detection more precise and efficient, stage 2 employs an object detection model to locate the exact region of the gun within video frames for videos classified as ``Gun'' by stage 1. Third, our experimental results demonstrate that the proposed domain-specific method achieves significant performance improvements and enhances efficiency compared with existing techniques. We also discuss challenges and future research directions in gun detection tasks in computer vision."
  },
  {
    "title": "Synergizing AI and Digital Twins for Next-Generation Network Optimization, Forecasting, and Security",
    "url": "http://arxiv.org/abs/2503.06302v1",
    "arxiv_id": "2503.06302v1",
    "authors": [
      "Zifan Zhang",
      "Minghong Fang",
      "Dianwei Chen",
      "Xianfeng Yang",
      "Yuchen Liu"
    ],
    "published": "2025-03-08T18:30:54+00:00",
    "summary": "Digital network twins (DNTs) are virtual representations of physical networks, designed to enable real-time monitoring, simulation, and optimization of network performance. When integrated with machine learning (ML) techniques, particularly federated learning (FL) and reinforcement learning (RL), DNTs emerge as powerful solutions for managing the complexities of network operations. This article presents a comprehensive analysis of the synergy of DNTs, FL, and RL techniques, showcasing their collective potential to address critical challenges in 6G networks. We highlight key technical challenges that need to be addressed, such as ensuring network reliability, achieving joint data-scenario forecasting, and maintaining security in high-risk environments. Additionally, we propose several pipelines that integrate DNT and ML within coherent frameworks to enhance network optimization and security. Case studies demonstrate the practical applications of our proposed pipelines in edge caching and vehicular networks. In edge caching, the pipeline achieves over 80% cache hit rates while balancing base station loads. In autonomous vehicular system, it ensure a 100% no-collision rate, showcasing its reliability in safety-critical scenarios. By exploring these synergies, we offer insights into the future of intelligent and adaptive network systems that automate decision-making and problem-solving."
  },
  {
    "title": "Exploring Adversarial Transferability between Kolmogorov-arnold Networks",
    "url": "http://arxiv.org/abs/2503.06276v1",
    "arxiv_id": "2503.06276v1",
    "authors": [
      "Songping Wang",
      "Xinquan Yue",
      "Yueming Lyu",
      "Caifeng Shan"
    ],
    "published": "2025-03-08T16:48:05+00:00",
    "summary": "Kolmogorov-Arnold Networks (KANs) have emerged as a transformative model paradigm, significantly impacting various fields. However, their adversarial robustness remains less underexplored, especially across different KAN architectures. To explore this critical safety issue, we conduct an analysis and find that due to overfitting to the specific basis functions of KANs, they possess poor adversarial transferability among different KANs. To tackle this challenge, we propose AdvKAN, the first transfer attack method for KANs. AdvKAN integrates two key components: 1) a Breakthrough-Defense Surrogate Model (BDSM), which employs a breakthrough-defense training strategy to mitigate overfitting to the specific structures of KANs. 2) a Global-Local Interaction (GLI) technique, which promotes sufficient interaction between adversarial gradients of hierarchical levels, further smoothing out loss surfaces of KANs. Both of them work together to enhance the strength of transfer attack among different KANs. Extensive experimental results on various KANs and datasets demonstrate the effectiveness of AdvKAN, which possesses notably superior attack capabilities and deeply reveals the vulnerabilities of KANs. Code will be released upon acceptance."
  },
  {
    "title": "Using Mechanistic Interpretability to Craft Adversarial Attacks against Large Language Models",
    "url": "http://arxiv.org/abs/2503.06269v1",
    "arxiv_id": "2503.06269v1",
    "authors": [
      "Thomas Winninger",
      "Boussad Addad",
      "Katarzyna Kapusta"
    ],
    "published": "2025-03-08T16:29:45+00:00",
    "summary": "Traditional white-box methods for creating adversarial perturbations against LLMs typically rely only on gradient computation from the targeted model, ignoring the internal mechanisms responsible for attack success or failure. Conversely, interpretability studies that analyze these internal mechanisms lack practical applications beyond runtime interventions. We bridge this gap by introducing a novel white-box approach that leverages mechanistic interpretability techniques to craft practical adversarial inputs. Specifically, we first identify acceptance subspaces - sets of feature vectors that do not trigger the model's refusal mechanisms - then use gradient-based optimization to reroute embeddings from refusal subspaces to acceptance subspaces, effectively achieving jailbreaks. This targeted approach significantly reduces computation cost, achieving attack success rates of 80-95\\% on state-of-the-art models including Gemma2, Llama3.2, and Qwen2.5 within minutes or even seconds, compared to existing techniques that often fail or require hours of computation. We believe this approach opens a new direction for both attack research and defense development. Furthermore, it showcases a practical application of mechanistic interpretability where other methods are less efficient, which highlights its utility. The code and generated datasets are available at https://github.com/Sckathach/subspace-rerouting."
  },
  {
    "title": "MAD-MAX: Modular And Diverse Malicious Attack MiXtures for Automated LLM Red Teaming",
    "url": "http://arxiv.org/abs/2503.06253v1",
    "arxiv_id": "2503.06253v1",
    "authors": [
      "Stefan Schoepf",
      "Muhammad Zaid Hameed",
      "Ambrish Rawat",
      "Kieran Fraser",
      "Giulio Zizzo",
      "Giandomenico Cornacchia",
      "Mark Purcell"
    ],
    "published": "2025-03-08T15:28:26+00:00",
    "summary": "With LLM usage rapidly increasing, their vulnerability to jailbreaks that create harmful outputs are a major security risk. As new jailbreaking strategies emerge and models are changed by fine-tuning, continuous testing for security vulnerabilities is necessary. Existing Red Teaming methods fall short in cost efficiency, attack success rate, attack diversity, or extensibility as new attack types emerge. We address these challenges with Modular And Diverse Malicious Attack MiXtures (MAD-MAX) for Automated LLM Red Teaming. MAD-MAX uses automatic assignment of attack strategies into relevant attack clusters, chooses the most relevant clusters for a malicious goal, and then combines strategies from the selected clusters to achieve diverse novel attacks with high attack success rates. MAD-MAX further merges promising attacks together at each iteration of Red Teaming to boost performance and introduces a similarity filter to prune out similar attacks for increased cost efficiency. The MAD-MAX approach is designed to be easily extensible with newly discovered attack strategies and outperforms the prominent Red Teaming method Tree of Attacks with Pruning (TAP) significantly in terms of Attack Success Rate (ASR) and queries needed to achieve jailbreaks. MAD-MAX jailbreaks 97% of malicious goals in our benchmarks on GPT-4o and Gemini-Pro compared to TAP with 66%. MAD-MAX does so with only 10.9 average queries to the target LLM compared to TAP with 23.3.   WARNING: This paper contains contents which are offensive in nature."
  },
  {
    "title": "Rethinking Lanes and Points in Complex Scenarios for Monocular 3D Lane Detection",
    "url": "http://arxiv.org/abs/2503.06237v1",
    "arxiv_id": "2503.06237v1",
    "authors": [
      "Yifan Chang",
      "Junjie Huang",
      "Xiaofeng Wang",
      "Yun Ye",
      "Zhujin Liang",
      "Yi Shan",
      "Dalong Du",
      "Xingang Wang"
    ],
    "published": "2025-03-08T14:45:49+00:00",
    "summary": "Monocular 3D lane detection is a fundamental task in autonomous driving. Although sparse-point methods lower computational load and maintain high accuracy in complex lane geometries, current methods fail to fully leverage the geometric structure of lanes in both lane geometry representations and model design. In lane geometry representations, we present a theoretical analysis alongside experimental validation to verify that current sparse lane representation methods contain inherent flaws, resulting in potential errors of up to 20 m, which raise significant safety concerns for driving. To address this issue, we propose a novel patching strategy to completely represent the full lane structure. To enable existing models to match this strategy, we introduce the EndPoint head (EP-head), which adds a patching distance to endpoints. The EP-head enables the model to predict more complete lane representations even with fewer preset points, effectively addressing existing limitations and paving the way for models that are faster and require fewer parameters in the future. In model design, to enhance the model's perception of lane structures, we propose the PointLane attention (PL-attention), which incorporates prior geometric knowledge into the attention mechanism. Extensive experiments demonstrate the effectiveness of the proposed methods on various state-of-the-art models. For instance, in terms of the overall F1-score, our methods improve Persformer by 4.4 points, Anchor3DLane by 3.2 points, and LATR by 2.8 points. The code will be available soon."
  },
  {
    "title": "Reinforced Diffuser for Red Teaming Large Vision-Language Models",
    "url": "http://arxiv.org/abs/2503.06223v1",
    "arxiv_id": "2503.06223v1",
    "authors": [
      "Ruofan Wang",
      "Xiang Zheng",
      "Xiaosen Wang",
      "Cong Wang",
      "Xingjun Ma"
    ],
    "published": "2025-03-08T13:51:40+00:00",
    "summary": "The rapid advancement of large Vision-Language Models (VLMs) has raised significant safety concerns, particularly regarding their vulnerability to jailbreak attacks. While existing research primarily focuses on VLMs' susceptibility to harmful instructions, this work identifies a critical yet overlooked vulnerability: current alignment mechanisms often fail to address the risks posed by toxic text continuation tasks. To investigate this issue, we propose a novel Red Team Diffuser (RTD) framework, which leverages reinforcement learning to generate red team images that effectively induce highly toxic continuations from target black-box VLMs. The RTD pipeline begins with a greedy search for high-quality image prompts that maximize the toxicity of VLM-generated sentence continuations, guided by a Large Language Model (LLM). These prompts are then used as input for the reinforcement fine-tuning of a diffusion model, which employs toxicity and alignment rewards to further amplify harmful outputs. Experimental results demonstrate the effectiveness of RTD, increasing the toxicity rate of LLaVA outputs by 10.69% on the original attack set and 8.91% on a hold-out set. Moreover, RTD exhibits strong cross-model transferability, raising the toxicity rate by 5.1% on Gemini and 26.83% on LLaMA. These findings reveal significant deficiencies in existing alignment strategies, particularly their inability to prevent harmful continuations. Our work underscores the urgent need for more robust and adaptive alignment mechanisms to ensure the safe deployment of VLMs in real-world applications."
  },
  {
    "title": "Secure On-Device Video OOD Detection Without Backpropagation",
    "url": "http://arxiv.org/abs/2503.06166v1",
    "arxiv_id": "2503.06166v1",
    "authors": [
      "Li Li",
      "Peilin Cai",
      "Yuxiao Zhou",
      "Zhiyu Ni",
      "Renjie Liang",
      "You Qin",
      "Yi Nian",
      "Zhengzhong Tu",
      "Xiyang Hu",
      "Yue Zhao"
    ],
    "published": "2025-03-08T11:03:21+00:00",
    "summary": "Out-of-Distribution (OOD) detection is critical for ensuring the reliability of machine learning models in safety-critical applications such as autonomous driving and medical diagnosis. While deploying personalized OOD detection directly on edge devices is desirable, it remains challenging due to large model sizes and the computational infeasibility of on-device training. Federated learning partially addresses this but still requires gradient computation and backpropagation, exceeding the capabilities of many edge devices. To overcome these challenges, we propose SecDOOD, a secure cloud-device collaboration framework for efficient on-device OOD detection without requiring device-side backpropagation. SecDOOD utilizes cloud resources for model training while ensuring user data privacy by retaining sensitive information on-device. Central to SecDOOD is a HyperNetwork-based personalized parameter generation module, which adapts cloud-trained models to device-specific distributions by dynamically generating local weight adjustments, effectively combining central and local information without local fine-tuning. Additionally, our dynamic feature sampling and encryption strategy selectively encrypts only the most informative feature channels, largely reducing encryption overhead without compromising detection performance. Extensive experiments across multiple datasets and OOD scenarios demonstrate that SecDOOD achieves performance comparable to fully fine-tuned models, enabling secure, efficient, and personalized OOD detection on resource-limited edge devices. To enhance accessibility and reproducibility, our code is publicly available at https://github.com/Dystopians/SecDOOD."
  },
  {
    "title": "Exploring the usage of Probabilistic Neural Networks for Ionospheric electron density estimation",
    "url": "http://arxiv.org/abs/2503.06144v1",
    "arxiv_id": "2503.06144v1",
    "authors": [
      "Miquel Garcia-Fernandez"
    ],
    "published": "2025-03-08T10:06:15+00:00",
    "summary": "A fundamental limitation of traditional Neural Networks (NN) in predictive modelling is their inability to quantify uncertainty in their outputs. In critical applications like positioning systems, understanding the reliability of predictions is critical for constructing confidence intervals, early warning systems, and effectively propagating results. For instance, Precise Point Positioning in satellite navigation heavily relies on accurate error models for ancillary data (orbits, clocks, ionosphere, and troposphere) to compute precise error estimates. In addition, these uncertainty estimates are needed to establish robust protection levels in safety critical applications.   To address this challenge, the main objectives of this paper aims at exploring a potential framework capable of providing both point estimates and associated uncertainty measures of ionospheric Vertical Total Electron Content (VTEC). In this context, Probabilistic Neural Networks (PNNs) offer a promising approach to achieve this goal. However, constructing an effective PNN requires meticulous design of hidden and output layers, as well as careful definition of prior and posterior probability distributions for network weights and biases.   A key finding of this study is that the uncertainty provided by the PNN model in VTEC estimates may be systematically underestimated. In low-latitude areas, the actual error was observed to be as much as twice the model's estimate. This underestimation is expected to be more pronounced during solar maximum, correlating with increased VTEC values."
  },
  {
    "title": "dARt Vinci: Egocentric Data Collection for Surgical Robot Learning at Scale",
    "url": "http://arxiv.org/abs/2503.05646v1",
    "arxiv_id": "2503.05646v1",
    "authors": [
      "Yihao Liu",
      "Yu-Chun Ku",
      "Jiaming Zhang",
      "Hao Ding",
      "Peter Kazanzides",
      "Mehran Armand"
    ],
    "published": "2025-03-07T18:07:54+00:00",
    "summary": "Data scarcity has long been an issue in the robot learning community. Particularly, in safety-critical domains like surgical applications, obtaining high-quality data can be especially difficult. It poses challenges to researchers seeking to exploit recent advancements in reinforcement learning and imitation learning, which have greatly improved generalizability and enabled robots to conduct tasks autonomously. We introduce dARt Vinci, a scalable data collection platform for robot learning in surgical settings. The system uses Augmented Reality (AR) hand tracking and a high-fidelity physics engine to capture subtle maneuvers in primitive surgical tasks: By eliminating the need for a physical robot setup and providing flexibility in terms of time, space, and hardware resources-such as multiview sensors and actuators-specialized simulation is a viable alternative. At the same time, AR allows the robot data collection to be more egocentric, supported by its body tracking and content overlaying capabilities. Our user study confirms the proposed system's efficiency and usability, where we use widely-used primitive tasks for training teleoperation with da Vinci surgical robots. Data throughput improves across all tasks compared to real robot settings by 41% on average. The total experiment time is reduced by an average of 10%. The temporal demand in the task load survey is improved. These gains are statistically significant. Additionally, the collected data is over 400 times smaller in size, requiring far less storage while achieving double the frequency."
  },
  {
    "title": "Nuanced Safety for Generative AI: How Demographics Shape Responsiveness to Severity",
    "url": "http://arxiv.org/abs/2503.05609v1",
    "arxiv_id": "2503.05609v1",
    "authors": [
      "Pushkar Mishra",
      "Charvi Rastogi",
      "Stephen R. Pfohl",
      "Alicia Parrish",
      "Roma Patel",
      "Mark Diaz",
      "Ding Wang",
      "Michela Paganini",
      "Vinodkumar Prabhakaran",
      "Lora Aroyo",
      "Verena Rieser"
    ],
    "published": "2025-03-07T17:32:31+00:00",
    "summary": "Ensuring safety of Generative AI requires a nuanced understanding of pluralistic viewpoints. In this paper, we introduce a novel data-driven approach for calibrating granular ratings in pluralistic datasets. Specifically, we address the challenge of interpreting responses of a diverse population to safety expressed via ordinal scales (e.g., Likert scale). We distill non-parametric responsiveness metrics that quantify the consistency of raters in scoring the varying levels of the severity of safety violations. Using safety evaluation of AI-generated content as a case study, we investigate how raters from different demographic groups (age, gender, ethnicity) use an ordinal scale to express their perception of the severity of violations in a pluralistic safety dataset. We apply our metrics across violation types, demonstrating their utility in extracting nuanced insights that are crucial for developing reliable AI systems in a multi-cultural contexts. We show that our approach offers improved capabilities for prioritizing safety concerns by capturing nuanced viewpoints across different demographic groups, hence improving the reliability of pluralistic data collection and in turn contributing to more robust AI evaluations."
  },
  {
    "title": "TomatoScanner: phenotyping tomato fruit based on only RGB image",
    "url": "http://arxiv.org/abs/2503.05568v1",
    "arxiv_id": "2503.05568v1",
    "authors": [
      "Xiaobei Zhao",
      "Xiangrong Zeng",
      "Yihang Ma",
      "Pengjin Tang",
      "Xiang Li"
    ],
    "published": "2025-03-07T16:47:48+00:00",
    "summary": "In tomato greenhouse, phenotypic measurement is meaningful for researchers and farmers to monitor crop growth, thereby precisely control environmental conditions in time, leading to better quality and higher yield. Traditional phenotyping mainly relies on manual measurement, which is accurate but inefficient, more importantly, endangering the health and safety of people. Several studies have explored computer vision-based methods to replace manual phenotyping. However, the 2D-based need extra calibration, or cause destruction to fruit, or can only measure limited and meaningless traits. The 3D-based need extra depth camera, which is expensive and unacceptable for most farmers. In this paper, we propose a non-contact tomato fruit phenotyping method, titled TomatoScanner, where RGB image is all you need for input. First, pixel feature is extracted by instance segmentation of our proposed EdgeYOLO with preprocessing of individual separation and pose correction. Second, depth feature is extracted by depth estimation of Depth Pro. Third, pixel and depth feature are fused to output phenotype results in reality. We establish self-built Tomato Phenotype Dataset to test TomatoScanner, which achieves excellent phenotyping on width, height, vertical area and volume, with median relative error of 5.63%, 7.03%, -0.64% and 37.06%, respectively. We propose and add three innovative modules - EdgeAttention, EdgeLoss and EdgeBoost - into EdgeYOLO, to enhance the segmentation accuracy on edge portion. Precision and mean Edge Error greatly improve from 0.943 and 5.641% to 0.986 and 2.963%, respectively. Meanwhile, EdgeYOLO keeps lightweight and efficient, with 48.7 M weights size and 76.34 FPS. Codes and datasets: https://github.com/AlexTraveling/TomatoScanner."
  },
  {
    "title": "Step-by-step design guide of a cryogenic three-axis vector magnet",
    "url": "http://arxiv.org/abs/2503.05459v1",
    "arxiv_id": "2503.05459v1",
    "authors": [
      "Gaia Da Prato",
      "Yong Yu",
      "Ronald Bode",
      "Simon Gr\u00f6blacher"
    ],
    "published": "2025-03-07T14:32:11+00:00",
    "summary": "A tunable magnetic field at low temperatures is essential for numerous applications, including spintronics, magnetic resonance imaging, and condensed matter physics. While commercial superconducting vector magnets are available, they are complex, expensive, and often not adaptable to specific experimental needs. As a result, simple in-house designs are often being used in research environments. However, no comprehensive step-by-step guide for their construction currently exists. In this work, we provide a detailed manual for designing and building a cryogenically compatible three-axis vector magnet. The system is tested at the mixing chamber of a dilution refrigerator at temperatures ranging from 15 mK to 4 K, with no significant increase in base temperature. Safety measures are implemented to mitigate heating from quenching. The coils are successfully driven with DC currents as high as 3 A, generating magnetic fields of up to 2.5 T. Magnetic field measurements using Hall sensors demonstrate good agreement with the predictions of the designed performance."
  },
  {
    "title": "$\\mathrm{O}$/$\\mathrm{SO}$ Gauge Groups, $BC$ Quivers and $O3$ Planes",
    "url": "http://arxiv.org/abs/2503.05443v1",
    "arxiv_id": "2503.05443v1",
    "authors": [
      "Sam Bennett",
      "Amihay Hanany"
    ],
    "published": "2025-03-07T14:14:53+00:00",
    "summary": "D3-/D5-/NS5-brane systems with $O3$ orientifold planes realise 3d $\\mathcal{N}=4$ gauge theories with orthogonal and symplectic gauge groups on the D3-brane worldvolume. Such setups have long contained an ambiguity regarding the global form of $D$-type gauge groups. This note offers a partial prescription for reading $\\mathrm{O}(2k)$ and $\\mathrm{SO}(2k)$ gauge nodes in orthosymplectic quivers using the presence of $\\frac{1}{2}$D5-branes on the orientifolds bordering $\\frac{1}{2}$NS5-brane intervals spanned by $O3^{-}$ planes. A set of identities are proposed relating the Coulomb branches of generic quivers under a Higgsing that relates $\\mathrm{SO}(2k+1)$ and $\\mathrm{O}(2k)$ gauge groups. A further prescription is conjectured regarding the action of $\\frac{1}{2}$D5-branes on maximal $DC$-chains."
  },
  {
    "title": "A Hybrid Approach for Extending Automotive Radar Operation to NLOS Urban Scenarios",
    "url": "http://arxiv.org/abs/2503.05413v1",
    "arxiv_id": "2503.05413v1",
    "authors": [
      "Aviran Gal",
      "Igal Bilik"
    ],
    "published": "2025-03-07T13:37:47+00:00",
    "summary": "Automotive radar is a key component of sensing suites in autonomous driving (AD) and advanced driver-assist systems (ADAS). However, limited line-of-sight (LOS) significantly reduces radar efficiency in dense urban environments. Therefore, automotive radars need to extend their capabilities beyond LOS by localizing occluding and reflective surfaces and non-line-of-sight (NLOS) targets. This work addresses the NLOS target localization challenge by revisiting the NLOS radar signal propagation model and introducing a hybrid localization approach. The proposed approach first detects and localizes reflective surfaces, then identifies the LOS/NLOS propagation conditions, and finally localizes the target without prior scene knowledge, without using Doppler information, and without any auxiliary sensors. The proposed hybrid approach addresses the computational complexity challenge by integrating a physical radar electromagnetic wave propagation model with a deep neural network (DNN) to estimate occluding surface parameters. The efficiency of the proposed approach to localize the NLOS targets and to identify the NLOS/LOS propagation conditions is evaluated via simulations in a broad range of realistic automotive scenarios. Extending automotive radar sensing beyond LOS is expected to enhance the safety and reliability of autonomous and ADAS-equipped vehicles."
  },
  {
    "title": "Ontology Generation using Large Language Models",
    "url": "http://arxiv.org/abs/2503.05388v1",
    "arxiv_id": "2503.05388v1",
    "authors": [
      "Anna Sofia Lippolis",
      "Mohammad Javad Saeedizade",
      "Robin Keskis\u00e4rkk\u00e4",
      "Sara Zuppiroli",
      "Miguel Ceriani",
      "Aldo Gangemi",
      "Eva Blomqvist",
      "Andrea Giovanni Nuzzolese"
    ],
    "published": "2025-03-07T13:03:28+00:00",
    "summary": "The ontology engineering process is complex, time-consuming, and error-prone, even for experienced ontology engineers. In this work, we investigate the potential of Large Language Models (LLMs) to provide effective OWL ontology drafts directly from ontological requirements described using user stories and competency questions. Our main contribution is the presentation and evaluation of two new prompting techniques for automated ontology development: Memoryless CQbyCQ and Ontogenia. We also emphasize the importance of three structural criteria for ontology assessment, alongside expert qualitative evaluation, highlighting the need for a multi-dimensional evaluation in order to capture the quality and usability of the generated ontologies. Our experiments, conducted on a benchmark dataset of ten ontologies with 100 distinct CQs and 29 different user stories, compare the performance of three LLMs using the two prompting techniques. The results demonstrate improvements over the current state-of-the-art in LLM-supported ontology engineering. More specifically, the model OpenAI o1-preview with Ontogenia produces ontologies of sufficient quality to meet the requirements of ontology engineers, significantly outperforming novice ontology engineers in modelling ability. However, we still note some common mistakes and variability of result quality, which is important to take into account when using LLMs for ontology authoring support. We discuss these limitations and propose directions for future research."
  },
  {
    "title": "Shifting Perspectives: Steering Vector Ensembles for Robust Bias Mitigation in LLMs",
    "url": "http://arxiv.org/abs/2503.05371v1",
    "arxiv_id": "2503.05371v1",
    "authors": [
      "Zara Siddique",
      "Irtaza Khalid",
      "Liam D. Turner",
      "Luis Espinosa-Anke"
    ],
    "published": "2025-03-07T12:25:29+00:00",
    "summary": "We present a novel approach to bias mitigation in large language models (LLMs) by applying steering vectors to modify model activations in forward passes. We employ Bayesian optimization to systematically identify effective contrastive pair datasets across nine bias axes. When optimized on the BBQ dataset, our individually tuned steering vectors achieve average improvements of 12.2%, 4.7%, and 3.2% over the baseline for Mistral, Llama, and Qwen, respectively. Building on these promising results, we introduce Steering Vector Ensembles (SVE), a method that averages multiple individually optimized steering vectors, each targeting a specific bias axis such as age, race, or gender. By leveraging their collective strength, SVE outperforms individual steering vectors in both bias reduction and maintaining model performance. The work presents the first systematic investigation of steering vectors for bias mitigation, and we demonstrate that SVE is a powerful and computationally efficient strategy for reducing bias in LLMs, with broader implications for enhancing AI safety."
  },
  {
    "title": "Toward an Evaluation Science for Generative AI Systems",
    "url": "http://arxiv.org/abs/2503.05336v1",
    "arxiv_id": "2503.05336v1",
    "authors": [
      "Laura Weidinger",
      "Deb Raji",
      "Hanna Wallach",
      "Margaret Mitchell",
      "Angelina Wang",
      "Olawale Salaudeen",
      "Rishi Bommasani",
      "Sayash Kapoor",
      "Deep Ganguli",
      "Sanmi Koyejo",
      "William Isaac"
    ],
    "published": "2025-03-07T11:23:48+00:00",
    "summary": "There is an increasing imperative to anticipate and understand the performance and safety of generative AI systems in real-world deployment contexts. However, the current evaluation ecosystem is insufficient: Commonly used static benchmarks face validity challenges, and ad hoc case-by-case audits rarely scale. In this piece, we advocate for maturing an evaluation science for generative AI systems. While generative AI creates unique challenges for system safety engineering and measurement science, the field can draw valuable insights from the development of safety evaluation practices in other fields, including transportation, aerospace, and pharmaceutical engineering. In particular, we present three key lessons: Evaluation metrics must be applicable to real-world performance, metrics must be iteratively refined, and evaluation institutions and norms must be established. Applying these insights, we outline a concrete path toward a more rigorous approach for evaluating generative AI systems."
  },
  {
    "title": "Jailbreaking is (Mostly) Simpler Than You Think",
    "url": "http://arxiv.org/abs/2503.05264v1",
    "arxiv_id": "2503.05264v1",
    "authors": [
      "Mark Russinovich",
      "Ahmed Salem"
    ],
    "published": "2025-03-07T09:28:19+00:00",
    "summary": "We introduce the Context Compliance Attack (CCA), a novel, optimization-free method for bypassing AI safety mechanisms. Unlike current approaches -- which rely on complex prompt engineering and computationally intensive optimization -- CCA exploits a fundamental architectural vulnerability inherent in many deployed AI systems. By subtly manipulating conversation history, CCA convinces the model to comply with a fabricated dialogue context, thereby triggering restricted behavior. Our evaluation across a diverse set of open-source and proprietary models demonstrates that this simple attack can circumvent state-of-the-art safety protocols. We discuss the implications of these findings and propose practical mitigation strategies to fortify AI systems against such elementary yet effective adversarial tactics."
  },
  {
    "title": "ARbiter: Generating Dialogue Options and Communication Support in Augmented Reality",
    "url": "http://arxiv.org/abs/2503.05220v1",
    "arxiv_id": "2503.05220v1",
    "authors": [
      "Juli\u00e1n M\u00e9ndez",
      "Marc Satkowski"
    ],
    "published": "2025-03-07T08:16:30+00:00",
    "summary": "In this position paper, we propose researching the combination of Augmented Reality (AR) and Artificial Intelligence (AI) to support conversations, inspired by the interfaces of dialogue systems commonly found in videogames. AR-capable devices are becoming more powerful and conventional in looks, as seen in head-mounted displays (HMDs) like the Snapchat Spectacles, the XREAL glasses, or the recently presented Meta Orion. This development reduces possible ergonomic, appearance, and runtime concerns, thus allowing a more straightforward integration and extended use of AR in our everyday lives, both in private and at work. At the same time, we can observe an immense surge in AI development (also at CHI). Recently notorious Large Language Models (LLMs) like OpenAI's o3-mini or DeepSeek-R1 soar over their precursors in their ability to sustain conversations, provide suggestions, and handle complex topics in (almost) real time. In combination with natural language recognition systems, which are nowadays a standard component of smartphones and similar devices (including modern AR-HMDs), it is easy to imagine a combined system that integrates into daily conversations and provides various types of assistance. Such a system would enable many opportunities for research in AR+AI, which, as stated by Hirzle et al., remains scarce. In the following, we describe how the design of a conversational AR+AI system can learn from videogame dialogue systems, and we propose use cases and research questions that can be investigated thanks to this AR+AI combination."
  },
  {
    "title": "Safety-Critical Traffic Simulation with Adversarial Transfer of Driving Intentions",
    "url": "http://arxiv.org/abs/2503.05180v1",
    "arxiv_id": "2503.05180v1",
    "authors": [
      "Zherui Huang",
      "Xing Gao",
      "Guanjie Zheng",
      "Licheng Wen",
      "Xuemeng Yang",
      "Xiao Sun"
    ],
    "published": "2025-03-07T06:59:27+00:00",
    "summary": "Traffic simulation, complementing real-world data with a long-tail distribution, allows for effective evaluation and enhancement of the ability of autonomous vehicles to handle accident-prone scenarios. Simulating such safety-critical scenarios is nontrivial, however, from log data that are typically regular scenarios, especially in consideration of dynamic adversarial interactions between the future motions of autonomous vehicles and surrounding traffic participants. To address it, this paper proposes an innovative and efficient strategy, termed IntSim, that explicitly decouples the driving intentions of surrounding actors from their motion planning for realistic and efficient safety-critical simulation. We formulate the adversarial transfer of driving intention as an optimization problem, facilitating extensive exploration of diverse attack behaviors and efficient solution convergence. Simultaneously, intention-conditioned motion planning benefits from powerful deep models and large-scale real-world data, permitting the simulation of realistic motion behaviors for actors. Specially, through adapting driving intentions based on environments, IntSim facilitates the flexible realization of dynamic adversarial interactions with autonomous vehicles. Finally, extensive open-loop and closed-loop experiments on real-world datasets, including nuScenes and Waymo, demonstrate that the proposed IntSim achieves state-of-the-art performance in simulating realistic safety-critical scenarios and further improves planners in handling such scenarios."
  },
  {
    "title": "R1-Zero's \"Aha Moment\" in Visual Reasoning on a 2B Non-SFT Model",
    "url": "http://arxiv.org/abs/2503.05132v1",
    "arxiv_id": "2503.05132v1",
    "authors": [
      "Hengguang Zhou",
      "Xirui Li",
      "Ruochen Wang",
      "Minhao Cheng",
      "Tianyi Zhou",
      "Cho-Jui Hsieh"
    ],
    "published": "2025-03-07T04:21:47+00:00",
    "summary": "Recently DeepSeek R1 demonstrated how reinforcement learning with simple rule-based incentives can enable autonomous development of complex reasoning in large language models, characterized by the \"aha moment\", in which the model manifest self-reflection and increased response length during training. However, attempts to extend this success to multimodal reasoning often failed to reproduce these key characteristics. In this report, we present the first successful replication of these emergent characteristics for multimodal reasoning on only a non-SFT 2B model. Starting with Qwen2-VL-2B and applying reinforcement learning directly on the SAT dataset, our model achieves 59.47% accuracy on CVBench, outperforming the base model by approximately ~30% and exceeding both SFT setting by ~2%. In addition, we share our failed attempts and insights in attempting to achieve R1-like reasoning using RL with instruct models. aiming to shed light on the challenges involved. Our key observations include: (1) applying RL on instruct model often results in trivial reasoning trajectories, and (2) naive length reward are ineffective in eliciting reasoning capabilities. The project code is available at https://github.com/turningpoint-ai/VisualThinker-R1-Zero"
  },
  {
    "title": "Enhancing Autonomous Vehicle-Pedestrian Interaction in Shared Spaces: The Impact of Intended Path-Projection",
    "url": "http://arxiv.org/abs/2503.05041v1",
    "arxiv_id": "2503.05041v1",
    "authors": [
      "Le Yue",
      "Tram Thi Minh Tran",
      "Xinyan Yu",
      "Marius Hoggenmueller"
    ],
    "published": "2025-03-06T23:34:02+00:00",
    "summary": "External Human-Machine Interfaces (eHMIs) are critical for seamless interactions between autonomous vehicles (AVs) and pedestrians in shared spaces. However, they often struggle to adapt to these environments, where pedestrian movement is fluid and right-of-way is ambiguous. To address these challenges, we propose PaveFlow, an eHMI that projects the AV's intended path onto the ground in real time, providing continuous spatial information rather than a binary stop/go signal. Through a VR study (N=18), we evaluated PaveFlow's effectiveness under two AV density conditions (single vs. multiple AVs) and a baseline condition without PaveFlow. The results showed that PaveFlow significantly improved pedestrian perception of safety, trust, and user experience while reducing cognitive workload. This performance remained consistent across both single and multiple AV conditions, despite persistent tensions in priority negotiation. These findings suggest that path projection enhances eHMI transparency by offering richer movement cues, which may better support AV-pedestrian interaction in shared spaces."
  },
  {
    "title": "Safety is Not Only About Refusal: Reasoning-Enhanced Fine-tuning for Interpretable LLM Safety",
    "url": "http://arxiv.org/abs/2503.05021v1",
    "arxiv_id": "2503.05021v1",
    "authors": [
      "Yuyou Zhang",
      "Miao Li",
      "William Han",
      "Yihang Yao",
      "Zhepeng Cen",
      "Ding Zhao"
    ],
    "published": "2025-03-06T22:47:45+00:00",
    "summary": "Large Language Models (LLMs) are vulnerable to jailbreak attacks that exploit weaknesses in traditional safety alignment, which often relies on rigid refusal heuristics or representation engineering to block harmful outputs. While they are effective for direct adversarial attacks, they fall short of broader safety challenges requiring nuanced, context-aware decision-making. To address this, we propose Reasoning-enhanced Finetuning for interpretable LLM Safety (Rational), a novel framework that trains models to engage in explicit safe reasoning before response. Fine-tuned models leverage the extensive pretraining knowledge in self-generated reasoning to bootstrap their own safety through structured reasoning, internalizing context-sensitive decision-making. Our findings suggest that safety extends beyond refusal, requiring context awareness for more robust, interpretable, and adaptive responses. Reasoning is not only a core capability of LLMs but also a fundamental mechanism for LLM safety. Rational employs reasoning-enhanced fine-tuning, allowing it to reject harmful prompts while providing meaningful and context-aware responses in complex scenarios."
  },
  {
    "title": "Quantifying and Modeling Driving Styles in Trajectory Forecasting",
    "url": "http://arxiv.org/abs/2503.04994v1",
    "arxiv_id": "2503.04994v1",
    "authors": [
      "Laura Zheng",
      "Hamidreza Yaghoubi Araghi",
      "Tony Wu",
      "Sandeep Thalapanane",
      "Tianyi Zhou",
      "Ming C. Lin"
    ],
    "published": "2025-03-06T21:47:49+00:00",
    "summary": "Trajectory forecasting has become a popular deep learning task due to its relevance for scenario simulation for autonomous driving. Specifically, trajectory forecasting predicts the trajectory of a short-horizon future for specific human drivers in a particular traffic scenario. Robust and accurate future predictions can enable autonomous driving planners to optimize for low-risk and predictable outcomes for human drivers around them. Although some work has been done to model driving style in planning and personalized autonomous polices, a gap exists in explicitly modeling human driving styles for trajectory forecasting of human behavior. Human driving style is most certainly a correlating factor to decision making, especially in edge-case scenarios where risk is nontrivial, as justified by the large amount of traffic psychology literature on risky driving. So far, the current real-world datasets for trajectory forecasting lack insight on the variety of represented driving styles. While the datasets may represent real-world distributions of driving styles, we posit that fringe driving style types may also be correlated with edge-case safety scenarios. In this work, we conduct analyses on existing real-world trajectory datasets for driving and dissect these works from the lens of driving styles, which is often intangible and non-standardized."
  },
  {
    "title": "From Voice to Safety: Language AI Powered Pilot-ATC Communication Understanding for Airport Surface Movement Collision Risk Assessment",
    "url": "http://arxiv.org/abs/2503.04974v1",
    "arxiv_id": "2503.04974v1",
    "authors": [
      "Yutian Pang",
      "Andrew Paul Kendall",
      "Alex Porcayo",
      "Mariah Barsotti",
      "Anahita Jain",
      "John-Paul Clarke"
    ],
    "published": "2025-03-06T21:08:07+00:00",
    "summary": "This work integrates language AI-based voice communication understanding with collision risk assessment. The proposed framework consists of two major parts, (a) Automatic Speech Recognition (ASR); (b) surface collision risk modeling. ASR module generates information tables by processing voice communication transcripts, which serve as references for producing potential taxi plans and calculating the surface movement collision risk. For ASR, we collect and annotate our own Named Entity Recognition (NER) dataset based on open-sourced video recordings and safety investigation reports. Additionally, we refer to FAA Order JO 7110.65W and FAA Order JO 7340.2N to get the list of heuristic rules and phase contractions of communication between the pilot and the Air Traffic Controller (ATCo) used in daily aviation operations. Then, we propose the novel ATC Rule-Enhanced NER method, which integrates the heuristic rules into the model training and inference stages, resulting into hybrid rule-based NER model. We show the effectiveness of this hybrid approach by comparing different setups with different token-level embedding models. For the risk modeling, we adopt the node-link airport layout graph from NASA FACET and model the aircraft taxi speed at each link as a log-normal distribution and derive the total taxi time distribution. Then, we propose a spatiotemporal formulation of the risk probability of two aircraft moving across potential collision nodes during ground movement. We show the effectiveness of our approach by simulating two case studies, (a) the Henada airport runway collision accident happened in January 2024; (b) the KATL taxiway collision happened in September 2024. We show that, by understanding the pilot-ATC communication transcripts and analyzing surface movement patterns, the proposed model improves airport safety by providing risk assessment in time."
  },
  {
    "title": "Data-Efficient Learning from Human Interventions for Mobile Robots",
    "url": "http://arxiv.org/abs/2503.04969v1",
    "arxiv_id": "2503.04969v1",
    "authors": [
      "Zhenghao Peng",
      "Zhizheng Liu",
      "Bolei Zhou"
    ],
    "published": "2025-03-06T21:02:02+00:00",
    "summary": "Mobile robots are essential in applications such as autonomous delivery and hospitality services. Applying learning-based methods to address mobile robot tasks has gained popularity due to its robustness and generalizability. Traditional methods such as Imitation Learning (IL) and Reinforcement Learning (RL) offer adaptability but require large datasets, carefully crafted reward functions, and face sim-to-real gaps, making them challenging for efficient and safe real-world deployment. We propose an online human-in-the-loop learning method PVP4Real that combines IL and RL to address these issues. PVP4Real enables efficient real-time policy learning from online human intervention and demonstration, without reward or any pretraining, significantly improving data efficiency and training safety. We validate our method by training two different robots -- a legged quadruped, and a wheeled delivery robot -- in two mobile robot tasks, one of which even uses raw RGBD image as observation. The training finishes within 15 minutes. Our experiments show the promising future of human-in-the-loop learning in addressing the data efficiency issue in real-world robotic tasks. More information is available at: https://metadriverse.github.io/pvp4real/"
  },
  {
    "title": "SafeArena: Evaluating the Safety of Autonomous Web Agents",
    "url": "http://arxiv.org/abs/2503.04957v1",
    "arxiv_id": "2503.04957v1",
    "authors": [
      "Ada Defne Tur",
      "Nicholas Meade",
      "Xing Han L\u00f9",
      "Alejandra Zambrano",
      "Arkil Patel",
      "Esin Durmus",
      "Spandana Gella",
      "Karolina Sta\u0144czak",
      "Siva Reddy"
    ],
    "published": "2025-03-06T20:43:14+00:00",
    "summary": "LLM-based agents are becoming increasingly proficient at solving web-based tasks. With this capability comes a greater risk of misuse for malicious purposes, such as posting misinformation in an online forum or selling illicit substances on a website. To evaluate these risks, we propose SafeArena, the first benchmark to focus on the deliberate misuse of web agents. SafeArena comprises 250 safe and 250 harmful tasks across four websites. We classify the harmful tasks into five harm categories -- misinformation, illegal activity, harassment, cybercrime, and social bias, designed to assess realistic misuses of web agents. We evaluate leading LLM-based web agents, including GPT-4o, Claude-3.5 Sonnet, Qwen-2-VL 72B, and Llama-3.2 90B, on our benchmark. To systematically assess their susceptibility to harmful tasks, we introduce the Agent Risk Assessment framework that categorizes agent behavior across four risk levels. We find agents are surprisingly compliant with malicious requests, with GPT-4o and Qwen-2 completing 34.7% and 27.3% of harmful requests, respectively. Our findings highlight the urgent need for safety alignment procedures for web agents. Our benchmark is available here: https://safearena.github.io"
  },
  {
    "title": "SAFE-TAXI: A Hierarchical Multi-UAS Safe Auto-Taxiing Framework with Runtime Safety Assurance and Conflict Resolution",
    "url": "http://arxiv.org/abs/2503.04942v1",
    "arxiv_id": "2503.04942v1",
    "authors": [
      "Kartik A. Pant",
      "Li-Yu Lin",
      "Worawis Sribunma",
      "Sabine Brunswicker",
      "James M. Goppert",
      "Inseok Hwang"
    ],
    "published": "2025-03-06T20:18:01+00:00",
    "summary": "We present a hierarchical safe auto-taxiing framework to enhance the automated ground operations of multiple unmanned aircraft systems (multi-UAS). The auto-taxiing problem becomes particularly challenging due to (i) unknown disturbances, such as crosswind affecting the aircraft dynamics, (ii) taxiway incursions due to unplanned obstacles, and (iii) spatiotemporal conflicts at the intersections between multiple entry points in the taxiway. To address these issues, we propose a hierarchical framework, i.e., SAFE-TAXI, combining centralized spatiotemporal planning with decentralized MPC-CBF-based control to safely navigate the aircraft through the taxiway while avoiding intersection conflicts and unplanned obstacles (e.g., other aircraft or ground vehicles). Our proposed framework decouples the auto-taxiing problem temporally into conflict resolution and motion planning, respectively. Conflict resolution is handled in a centralized manner by computing conflict-aware reference trajectories for each aircraft. In contrast, safety assurance from unplanned obstacles is handled by an MPC-CBF-based controller implemented in a decentralized manner. We demonstrate the effectiveness of our proposed framework through numerical simulations and experimentally validate it using Night Vapor, a small-scale fixed-wing test platform."
  },
  {
    "title": "Neural Configuration-Space Barriers for Manipulation Planning and Control",
    "url": "http://arxiv.org/abs/2503.04929v1",
    "arxiv_id": "2503.04929v1",
    "authors": [
      "Kehan Long",
      "Ki Myung Brian Lee",
      "Nikola Raicevic",
      "Niyas Attasseri",
      "Melvin Leok",
      "Nikolay Atanasov"
    ],
    "published": "2025-03-06T20:00:56+00:00",
    "summary": "Planning and control for high-dimensional robot manipulators in cluttered, dynamic environments require both computational efficiency and robust safety guarantees. Inspired by recent advances in learning configuration-space distance functions (CDFs) as robot body representations, we propose a unified framework for motion planning and control that formulates safety constraints as CDF barriers. A CDF barrier approximates the local free configuration space, substantially reducing the number of collision-checking operations during motion planning. However, learning a CDF barrier with a neural network and relying on online sensor observations introduce uncertainties that must be considered during control synthesis. To address this, we develop a distributionally robust CDF barrier formulation for control that explicitly accounts for modeling errors and sensor noise without assuming a known underlying distribution. Simulations and hardware experiments on a 6-DoF xArm manipulator show that our neural CDF barrier formulation enables efficient planning and robust real-time safe control in cluttered and dynamic environments, relying only on onboard point-cloud observations."
  },
  {
    "title": "Quantifying the Reasoning Abilities of LLMs on Real-world Clinical Cases",
    "url": "http://arxiv.org/abs/2503.04691v1",
    "arxiv_id": "2503.04691v1",
    "authors": [
      "Pengcheng Qiu",
      "Chaoyi Wu",
      "Shuyu Liu",
      "Weike Zhao",
      "Ya Zhang",
      "Yanfeng Wang",
      "Weidi Xie"
    ],
    "published": "2025-03-06T18:35:39+00:00",
    "summary": "The latest reasoning-enhanced large language models (reasoning LLMs), such as DeepSeek-R1 and OpenAI-o3, have demonstrated remarkable success. However, the application of such reasoning enhancements to the highly professional medical domain has not been clearly evaluated, particularly regarding with not only assessing the final generation but also examining the quality of their reasoning processes. In this study, we present MedR-Bench, a reasoning-focused medical evaluation benchmark comprising 1,453 structured patient cases with reasoning references mined from case reports. Our benchmark spans 13 body systems and 10 specialty disorders, encompassing both common and rare diseases. In our evaluation, we introduce a versatile framework consisting of three critical clinical stages: assessment recommendation, diagnostic decision-making, and treatment planning, comprehensively capturing the LLMs' performance across the entire patient journey in healthcare. For metrics, we propose a novel agentic system, Reasoning Evaluator, designed to automate and objectively quantify free-text reasoning responses in a scalable manner from the perspectives of efficiency, factuality, and completeness by dynamically searching and performing cross-referencing checks. As a result, we assess five state-of-the-art reasoning LLMs, including DeepSeek-R1, OpenAI-o3-mini, and others. Our results reveal that current LLMs can handle relatively simple diagnostic tasks with sufficient critical assessment results, achieving accuracy generally over 85%. However, they still struggle with more complex tasks, such as assessment recommendation and treatment planning. In reasoning, their reasoning processes are generally reliable, with factuality scores exceeding 90%, though they often omit critical reasoning steps. Our study clearly reveals further development directions for current clinical LLMs."
  },
  {
    "title": "START: Self-taught Reasoner with Tools",
    "url": "http://arxiv.org/abs/2503.04625v1",
    "arxiv_id": "2503.04625v1",
    "authors": [
      "Chengpeng Li",
      "Mingfeng Xue",
      "Zhenru Zhang",
      "Jiaxi Yang",
      "Beichen Zhang",
      "Xiang Wang",
      "Bowen Yu",
      "Binyuan Hui",
      "Junyang Lin",
      "Dayiheng Liu"
    ],
    "published": "2025-03-06T17:11:51+00:00",
    "summary": "Large reasoning models (LRMs) like OpenAI-o1 and DeepSeek-R1 have demonstrated remarkable capabilities in complex reasoning tasks through the utilization of long Chain-of-thought (CoT). However, these models often suffer from hallucinations and inefficiencies due to their reliance solely on internal reasoning processes. In this paper, we introduce START (Self-Taught Reasoner with Tools), a novel tool-integrated long CoT reasoning LLM that significantly enhances reasoning capabilities by leveraging external tools. Through code execution, START is capable of performing complex computations, self-checking, exploring diverse methods, and self-debugging, thereby addressing the limitations of LRMs. The core innovation of START lies in its self-learning framework, which comprises two key techniques: 1) Hint-infer: We demonstrate that inserting artificially designed hints (e.g., ``Wait, maybe using Python here is a good idea.'') during the inference process of a LRM effectively stimulates its ability to utilize external tools without the need for any demonstration data. Hint-infer can also serve as a simple and effective sequential test-time scaling method; 2) Hint Rejection Sampling Fine-Tuning (Hint-RFT): Hint-RFT combines Hint-infer and RFT by scoring, filtering, and modifying the reasoning trajectories with tool invocation generated by a LRM via Hint-infer, followed by fine-tuning the LRM. Through this framework, we have fine-tuned the QwQ-32B model to achieve START. On PhD-level science QA (GPQA), competition-level math benchmarks (AMC23, AIME24, AIME25), and the competition-level code benchmark (LiveCodeBench), START achieves accuracy rates of 63.6%, 95.0%, 66.7%, 47.1%, and 47.3%, respectively. It significantly outperforms the base QwQ-32B and achieves performance comparable to the state-of-the-art open-weight model R1-Distill-Qwen-32B and the proprietary model o1-Preview."
  },
  {
    "title": "START: Self-taught Reasoner with Tools",
    "url": "http://arxiv.org/abs/2503.04625v2",
    "arxiv_id": "2503.04625v2",
    "authors": [
      "Chengpeng Li",
      "Mingfeng Xue",
      "Zhenru Zhang",
      "Jiaxi Yang",
      "Beichen Zhang",
      "Xiang Wang",
      "Bowen Yu",
      "Binyuan Hui",
      "Junyang Lin",
      "Dayiheng Liu"
    ],
    "published": "2025-03-06T17:11:51+00:00",
    "summary": "Large reasoning models (LRMs) like OpenAI-o1 and DeepSeek-R1 have demonstrated remarkable capabilities in complex reasoning tasks through the utilization of long Chain-of-thought (CoT). However, these models often suffer from hallucinations and inefficiencies due to their reliance solely on internal reasoning processes. In this paper, we introduce START (Self-Taught Reasoner with Tools), a novel tool-integrated long CoT reasoning LLM that significantly enhances reasoning capabilities by leveraging external tools. Through code execution, START is capable of performing complex computations, self-checking, exploring diverse methods, and self-debugging, thereby addressing the limitations of LRMs. The core innovation of START lies in its self-learning framework, which comprises two key techniques: 1) Hint-infer: We demonstrate that inserting artificially designed hints (e.g., ``Wait, maybe using Python here is a good idea.'') during the inference process of a LRM effectively stimulates its ability to utilize external tools without the need for any demonstration data. Hint-infer can also serve as a simple and effective sequential test-time scaling method; 2) Hint Rejection Sampling Fine-Tuning (Hint-RFT): Hint-RFT combines Hint-infer and RFT by scoring, filtering, and modifying the reasoning trajectories with tool invocation generated by a LRM via Hint-infer, followed by fine-tuning the LRM. Through this framework, we have fine-tuned the QwQ-32B model to achieve START. On PhD-level science QA (GPQA), competition-level math benchmarks (AMC23, AIME24, AIME25), and the competition-level code benchmark (LiveCodeBench), START achieves accuracy rates of 63.6%, 95.0%, 66.7%, 47.1%, and 47.3%, respectively. It significantly outperforms the base QwQ-32B and achieves performance comparable to the state-of-the-art open-weight model R1-Distill-Qwen-32B and the proprietary model o1-Preview."
  },
  {
    "title": "TinyR1-32B-Preview: Boosting Accuracy with Branch-Merge Distillation",
    "url": "http://arxiv.org/abs/2503.04872v1",
    "arxiv_id": "2503.04872v1",
    "authors": [
      "Lin Sun",
      "Guangxiang Zhao",
      "Xiaoqi Jian",
      "Yuhan Wu",
      "Weihong Lin",
      "Yongfu Zhu",
      "Change Jia",
      "Linglin Zhang",
      "Jinzhu Wu",
      "Junfeng Ran",
      "Sai-er Hu",
      "Zihan Jiang",
      "Junting Zhou",
      "Wenrui Liu",
      "Bin Cui",
      "Tong Yang",
      "Xiangzheng Zhang"
    ],
    "published": "2025-03-06T16:25:53+00:00",
    "summary": "The challenge of reducing the size of Large Language Models (LLMs) while maintaining their performance has gained significant attention. However, existing methods, such as model distillation and transfer learning, often fail to achieve high accuracy. To address this limitation, we introduce the Branch-Merge distillation approach, which enhances model compression through two phases: (1) the Branch Phase, where knowledge from a large teacher model is \\textit{selectively distilled} into specialized student models via domain-specific supervised fine-tuning (SFT); And (2) the Merge Phase, where these student models are merged to enable cross-domain knowledge transfer and improve generalization. We validate our distillation approach using DeepSeek-R1 as the teacher and DeepSeek-R1-Distill-Qwen-32B as the student. The resulting merged model, TinyR1-32B-Preview, outperforms its counterpart DeepSeek-R1-Distill-Qwen-32B across multiple benchmarks, including Mathematics (+5.5 points), Coding (+4.4 points) and Science (+2.9 points), while achieving near-equal performance to DeepSeek-R1 on AIME 2024. The Branch-Merge distillation approach provides a scalable solution for creating smaller, high-performing LLMs with reduced computational cost and time."
  },
  {
    "title": "Occlusion-Aware Consistent Model Predictive Control for Robot Navigation in Occluded Obstacle-Dense Environments",
    "url": "http://arxiv.org/abs/2503.04563v1",
    "arxiv_id": "2503.04563v1",
    "authors": [
      "Minzhe Zheng",
      "Lei Zheng",
      "Lei Zhu",
      "Jun Ma"
    ],
    "published": "2025-03-06T15:52:59+00:00",
    "summary": "Ensuring safety and motion consistency for robot navigation in occluded, obstacle-dense environments is a critical challenge. In this context, this study presents an occlusion-aware Consistent Model Predictive Control (CMPC) strategy. To account for the occluded obstacles, it incorporates adjustable risk regions that represent their potential future locations. Subsequently, dynamic risk boundary constraints are developed online to ensure safety. The CMPC then constructs multiple locally optimal trajectory branches (each tailored to different risk regions) to balance between exploitation and exploration. A shared consensus trunk is generated to ensure smooth transitions between branches without significant velocity fluctuations, further preserving motion consistency. To facilitate high computational efficiency and ensure coordination across local trajectories, we use the alternating direction method of multipliers (ADMM) to decompose the CMPC into manageable sub-problems for parallel solving. The proposed strategy is validated through simulation and real-world experiments on an Ackermann-steering robot platform. The results demonstrate the effectiveness of the proposed CMPC strategy through comparisons with baseline approaches in occluded, obstacle-dense environments."
  },
  {
    "title": "Compositional Causal Reasoning Evaluation in Language Models",
    "url": "http://arxiv.org/abs/2503.04556v1",
    "arxiv_id": "2503.04556v1",
    "authors": [
      "Jacqueline R. M. A. Maasch",
      "Alihan H\u00fcy\u00fck",
      "Xinnuo Xu",
      "Aditya V. Nori",
      "Javier Gonzalez"
    ],
    "published": "2025-03-06T15:47:19+00:00",
    "summary": "Causal reasoning and compositional reasoning are two core aspirations in generative AI. Measuring the extent of these behaviors requires principled evaluation methods. We explore a unified perspective that considers both behaviors simultaneously, termed compositional causal reasoning (CCR): the ability to infer how causal measures compose and, equivalently, how causal quantities propagate through graphs. We instantiate a framework for the systematic evaluation of CCR for the average treatment effect and the probability of necessity and sufficiency. As proof of concept, we demonstrate the design of CCR tasks for language models in the LLama, Phi, and GPT families. On a math word problem, our framework revealed a range of taxonomically distinct error patterns. Additionally, CCR errors increased with the complexity of causal paths for all models except o1."
  },
  {
    "title": "Benchmarking Reasoning Robustness in Large Language Models",
    "url": "http://arxiv.org/abs/2503.04550v1",
    "arxiv_id": "2503.04550v1",
    "authors": [
      "Tong Yu",
      "Yongcheng Jing",
      "Xikun Zhang",
      "Wentao Jiang",
      "Wenjie Wu",
      "Yingjie Wang",
      "Wenbin Hu",
      "Bo Du",
      "Dacheng Tao"
    ],
    "published": "2025-03-06T15:36:06+00:00",
    "summary": "Despite the recent success of large language models (LLMs) in reasoning such as DeepSeek, we for the first time identify a key dilemma in reasoning robustness and generalization: significant performance degradation on novel or incomplete data, suggesting a reliance on memorized patterns rather than systematic reasoning. Our closer examination reveals four key unique limitations underlying this issue:(1) Positional bias--models favor earlier queries in multi-query inputs but answering the wrong one in the latter (e.g., GPT-4o's accuracy drops from 75.8 percent to 72.8 percent); (2) Instruction sensitivity--performance declines by 5.0 to 7.5 percent in the Qwen2.5 Series and by 5.0 percent in DeepSeek-V3 with auxiliary guidance; (3) Numerical fragility--value substitution sharply reduces accuracy (e.g., GPT-4o drops from 97.5 percent to 82.5 percent, GPT-o1-mini drops from 97.5 percent to 92.5 percent); and (4) Memory dependence--models resort to guesswork when missing critical data. These findings further highlight the reliance on heuristic recall over rigorous logical inference, demonstrating challenges in reasoning robustness. To comprehensively investigate these robustness challenges, this paper introduces a novel benchmark, termed as Math-RoB, that exploits hallucinations triggered by missing information to expose reasoning gaps. This is achieved by an instruction-based approach to generate diverse datasets that closely resemble training distributions, facilitating a holistic robustness assessment and advancing the development of more robust reasoning frameworks. Bad character(s) in field Abstract."
  },
  {
    "title": "An Empirical Study on Eliciting and Improving R1-like Reasoning Models",
    "url": "http://arxiv.org/abs/2503.04548v1",
    "arxiv_id": "2503.04548v1",
    "authors": [
      "Zhipeng Chen",
      "Yingqian Min",
      "Beichen Zhang",
      "Jie Chen",
      "Jinhao Jiang",
      "Daixuan Cheng",
      "Wayne Xin Zhao",
      "Zheng Liu",
      "Xu Miao",
      "Yang Lu",
      "Lei Fang",
      "Zhongyuan Wang",
      "Ji-Rong Wen"
    ],
    "published": "2025-03-06T15:34:27+00:00",
    "summary": "In this report, we present the third technical report on the development of slow-thinking models as part of the STILL project. As the technical pathway becomes clearer, scaling RL training has become a central technique for implementing such reasoning models. We systematically experiment with and document the effects of various factors influencing RL training, conducting experiments on both base models and fine-tuned models. Specifically, we demonstrate that our RL training approach consistently improves the Qwen2.5-32B base models, enhancing both response length and test accuracy. Furthermore, we show that even when a model like DeepSeek-R1-Distill-Qwen-1.5B has already achieved a high performance level, it can be further refined through RL training, reaching an accuracy of 39.33% on AIME 2024. Beyond RL training, we also explore the use of tool manipulation, finding that it significantly boosts the reasoning performance of large reasoning models. This approach achieves a remarkable accuracy of 86.67% with greedy search on AIME 2024, underscoring its effectiveness in enhancing model capabilities. We release our resources at the STILL project website: https://github.com/RUCAIBox/Slow_Thinking_with_LLMs."
  },
  {
    "title": "Method for recovering data on unreported low-severity crashes",
    "url": "http://arxiv.org/abs/2503.04529v1",
    "arxiv_id": "2503.04529v1",
    "authors": [
      "Alberto Morando"
    ],
    "published": "2025-03-06T15:18:45+00:00",
    "summary": "Objective: Many low-severity crashes are not reported due to sampling criteria, introducing missing not at random (MNAR) bias. If not addressed, MNAR bias can lead to inaccurate safety analyses. This paper illustrates a statistical method to address such bias. Methods: We defined a custom probability distribution for the observed data as a product of an exponential population distribution and a logistic reporting function. We used modern Bayesian probabilistic programming techniques. Results: Using simulated data, we verified the correctness of the procedure. Applying it to real crash data, we estimated the {\\Delta}v distribution for passenger vehicles involved in personal damage-only (PDO) rear-end crashes. We found that about 77% of cases are unreported. Conclusions: The method preserves the original data and it accounts well for uncertainty from both modeling assumptions and input data. It can improve safety assessments and it applies broadly to other MNAR cases."
  },
  {
    "title": "Method for recovering data on unreported low-severity crashes",
    "url": "http://arxiv.org/abs/2503.04529v2",
    "arxiv_id": "2503.04529v2",
    "authors": [
      "Alberto Morando"
    ],
    "published": "2025-03-06T15:18:45+00:00",
    "summary": "Objective: Many low-severity crashes are not reported due to sampling criteria, introducing missing not at random (MNAR) bias. If not addressed, MNAR bias can lead to inaccurate safety analyses. This paper illustrates a statistical method to address such bias. Methods: We defined a custom probability distribution for the observed data as a product of an exponential population distribution and a logistic reporting function. We used modern Bayesian probabilistic programming techniques. Results: Using simulated data, we verified the correctness of the procedure. Applying it to real crash data, we estimated the {\\Delta}v distribution for passenger vehicles involved in personal damage-only (PDO) rear-end crashes. We found that about 77% of cases are unreported. Conclusions: The method preserves the original data and it accounts well for uncertainty from both modeling assumptions and input data. It can improve safety assessments and it applies broadly to other MNAR cases."
  },
  {
    "title": "Research on a Driver's Perceived Risk Prediction Model Considering Traffic Scene Interaction",
    "url": "http://arxiv.org/abs/2503.04516v1",
    "arxiv_id": "2503.04516v1",
    "authors": [
      "Chenhao Yang",
      "Siwei Huang",
      "Chuan Hu"
    ],
    "published": "2025-03-06T15:03:34+00:00",
    "summary": "In the field of conditional autonomous driving technology, driver perceived risk prediction plays a crucial role in reducing traffic risks and ensuring passenger safety. This study introduces an innovative perceived risk prediction model for human-machine interaction in intelligent driving systems. The model aims to enhance prediction accuracy and, thereby, ensure passenger safety. Through a comprehensive analysis of risk impact mechanisms, we identify three key categories of factors, both subjective and objective, influencing perceived risk: driver's personal characteristics, ego-vehicle motion, and surrounding environment characteristics. We then propose a deep-learning-based risk prediction network that uses the first two categories of factors as inputs. The network captures the interactive relationships among traffic participants in dynamic driving scenarios. Additionally, we design a personalized modeling strategy that incorporates driver-specific traits to improve prediction accuracy. To ensure high-quality training data, we conducted a rigorous video rating experiment. Experimental results show that the proposed network achieves a 10.0% performance improvement over state-of-the-art methods. These findings suggest that the proposed network has significant potential to enhance the safety of conditional autonomous driving systems."
  },
  {
    "title": "Know Thy Judge: On the Robustness Meta-Evaluation of LLM Safety Judges",
    "url": "http://arxiv.org/abs/2503.04474v1",
    "arxiv_id": "2503.04474v1",
    "authors": [
      "Francisco Eiras",
      "Eliott Zemour",
      "Eric Lin",
      "Vaikkunth Mugunthan"
    ],
    "published": "2025-03-06T14:24:12+00:00",
    "summary": "Large Language Model (LLM) based judges form the underpinnings of key safety evaluation processes such as offline benchmarking, automated red-teaming, and online guardrailing. This widespread requirement raises the crucial question: can we trust the evaluations of these evaluators? In this paper, we highlight two critical challenges that are typically overlooked: (i) evaluations in the wild where factors like prompt sensitivity and distribution shifts can affect performance and (ii) adversarial attacks that target the judge. We highlight the importance of these through a study of commonly used safety judges, showing that small changes such as the style of the model output can lead to jumps of up to 0.24 in the false negative rate on the same dataset, whereas adversarial attacks on the model generation can fool some judges into misclassifying 100% of harmful generations as safe ones. These findings reveal gaps in commonly used meta-evaluation benchmarks and weaknesses in the robustness of current LLM judges, indicating that low attack success under certain judges could create a false sense of security."
  },
  {
    "title": "Activation Space Interventions Can Be Transferred Between Large Language Models",
    "url": "http://arxiv.org/abs/2503.04429v1",
    "arxiv_id": "2503.04429v1",
    "authors": [
      "Narmeen Oozeer",
      "Dhruv Nathawani",
      "Nirmalendu Prakash",
      "Michael Lan",
      "Abir Harrasse",
      "Amirali Abdullah"
    ],
    "published": "2025-03-06T13:38:44+00:00",
    "summary": "The study of representation universality in AI models reveals growing convergence across domains, modalities, and architectures. However, the practical applications of representation universality remain largely unexplored. We bridge this gap by demonstrating that safety interventions can be transferred between models through learned mappings of their shared activation spaces. We demonstrate this approach on two well-established AI safety tasks: backdoor removal and refusal of harmful prompts, showing successful transfer of steering vectors that alter the models' outputs in a predictable way. Additionally, we propose a new task, \\textit{corrupted capabilities}, where models are fine-tuned to embed knowledge tied to a backdoor. This tests their ability to separate useful skills from backdoors, reflecting real-world challenges. Extensive experiments across Llama, Qwen and Gemma model families show that our method enables using smaller models to efficiently align larger ones. Furthermore, we demonstrate that autoencoder mappings between base and fine-tuned models can serve as reliable ``lightweight safety switches\", allowing dynamic toggling between model behaviors."
  },
  {
    "title": "FORTALESA: Fault-Tolerant Reconfigurable Systolic Array for DNN Inference",
    "url": "http://arxiv.org/abs/2503.04426v1",
    "arxiv_id": "2503.04426v1",
    "authors": [
      "Natalia Cherezova",
      "Artur Jutman",
      "Maksim Jenihhin"
    ],
    "published": "2025-03-06T13:35:59+00:00",
    "summary": "The emergence of Deep Neural Networks (DNNs) in mission- and safety-critical applications brings their reliability to the front. High performance demands of DNNs require the use of specialized hardware accelerators. Systolic array architecture is widely used in DNN accelerators due to its parallelism and regular structure. This work presents a run-time reconfigurable systolic array architecture with three execution modes and four implementation options. All four implementations are evaluated in terms of resource utilization, throughput, and fault tolerance improvement. The proposed architecture is used for reliability enhancement of DNN inference on systolic array through heterogeneous mapping of different network layers to different execution modes. The approach is supported by a novel reliability assessment method based on fault propagation analysis. It is used for the exploration of the appropriate execution mode-layer mapping for DNN inference. The proposed architecture efficiently protects registers and MAC units of systolic array PEs from transient and permanent faults. The reconfigurability feature enables a speedup of up to $3\\times$, depending on layer vulnerability. Furthermore, it requires $6\\times$ less resources compared to static redundancy and $2.5\\times$ less resources compared to the previously proposed solution for transient faults."
  },
  {
    "title": "On the Analysis of Stability, Sensitivity and Transparency in Variable Admittance Control for pHRI Enhanced by Virtual Fixtures",
    "url": "http://arxiv.org/abs/2503.04414v1",
    "arxiv_id": "2503.04414v1",
    "authors": [
      "Davide Tebaldi",
      "Dario Onfiani",
      "Luigi Biagiotti"
    ],
    "published": "2025-03-06T13:15:19+00:00",
    "summary": "The interest in Physical Human-Robot Interaction (pHRI) has significantly increased over the last two decades thanks to the availability of collaborative robots that guarantee user safety during force exchanges. For this reason, stability concerns have been addressed extensively in the literature while proposing new control schemes for pHRI applications. Because of the nonlinear nature of robots, stability analyses generally leverage passivity concepts. On the other hand, the proposed algorithms generally consider ideal models of robot manipulators. For this reason, the primary objective of this paper is to conduct a detailed analysis of the sources of instability for a class of pHRI control schemes, namely proxy-based constrained admittance controllers, by considering parasitic effects such as transmission elasticity, motor velocity saturation, and actuation delay. Next, a sensitivity analysis supported by experimental results is carried out, in order to identify how the control parameters affect the stability of the overall system. Finally, an adaptation technique for the proxy parameters is proposed with the goal of maximizing transparency in pHRI. The proposed adaptation method is validated through both simulations and experimental tests."
  },
  {
    "title": "Dedicated Feedback and Edit Models Empower Inference-Time Scaling for Open-Ended General-Domain Tasks",
    "url": "http://arxiv.org/abs/2503.04378v1",
    "arxiv_id": "2503.04378v1",
    "authors": [
      "Zhilin Wang",
      "Jiaqi Zeng",
      "Olivier Delalleau",
      "Daniel Egert",
      "Ellie Evans",
      "Hoo-Chang Shin",
      "Felipe Soares",
      "Yi Dong",
      "Oleksii Kuchaiev"
    ],
    "published": "2025-03-06T12:30:24+00:00",
    "summary": "Inference-Time Scaling has been critical to the success of recent models such as OpenAI o1 and DeepSeek R1. However, many techniques used to train models for inference-time scaling require tasks to have answers that can be verified, limiting their application to domains such as math, coding and logical reasoning. We take inspiration from how humans make first attempts, ask for detailed feedback from others and make improvements based on such feedback across a wide spectrum of open-ended endeavors. To this end, we collect data for and train dedicated Feedback and Edit Models that are capable of performing inference-time scaling for open-ended general-domain tasks. In our setup, one model generates an initial response, which are given feedback by a second model, that are then used by a third model to edit the response. We show that performance on Arena Hard, a benchmark strongly predictive of Chatbot Arena Elo can be boosted by scaling the number of initial response drafts, effective feedback and edited responses. When scaled optimally, our setup based on 70B models from the Llama 3 family can reach SoTA performance on Arena Hard at 92.7 as of 5 Mar 2025, surpassing OpenAI o1-preview-2024-09-12 with 90.4 and DeepSeek R1 with 92.3."
  },
  {
    "title": "From Waterfallish Aerospace Certification onto Agile Certifiable Iterations",
    "url": "http://arxiv.org/abs/2503.04265v1",
    "arxiv_id": "2503.04265v1",
    "authors": [
      "J. Eduardo Ferreira Ribeiro",
      "M\u00e1rio Zenha-Rela",
      "Jo\u00e3o Gabriel Silva"
    ],
    "published": "2025-03-06T09:49:57+00:00",
    "summary": "Agile software development is becoming increasingly popular in the aerospace industry because of its capability to accommodate requirement changes. However, safety-critical domains require compliance with strict regulations such as the DO-178C avionics standard, which demands thorough documentation. The main challenge of this constraint is not the content itself, but rather the comprehensive traceability from system-level requirements to all sorts of testing and verification evidence, including who did what, when, and to which artifact. Currently, this is mostly a manual activity performed at the end of the project, which blocks efforts to agilize the development of software for aerospace applications. In this paper, we present a strategy and tools that support the generation of continuous documentation complying with DO-178C requirements. By iteratively creating the DO-178C documentation associated with each software component and seamlessly merging it with the previously generated documentation, we open the way to truly continuous certifiable iterations, an evolution from the current Waterfallish industry practice. The proposed mechanisms and tools were co-designed and validated with aerospace industry professionals, thereby confirming its applicability and usefulness. The generated artifacts show that document automation is feasible in the aerospace industry, opening the way for more widespread adoption of Agile practices in this highly regulated sector."
  },
  {
    "title": "Conformal forecasting for surgical instrument trajectory",
    "url": "http://arxiv.org/abs/2503.04191v1",
    "arxiv_id": "2503.04191v1",
    "authors": [
      "Sara Sangalli",
      "Gary Sarwin",
      "Ertunc Erdil",
      "Carlo Serra",
      "Ender Konukoglu"
    ],
    "published": "2025-03-06T08:06:03+00:00",
    "summary": "Forecasting surgical instrument trajectories and predicting the next surgical action recently started to attract attention from the research community. Both these tasks are crucial for automation and assistance in endoscopy surgery. Given the safety-critical nature of these tasks, reliable uncertainty quantification is essential. Conformal prediction is a fast-growing and widely recognized framework for uncertainty estimation in machine learning and computer vision, offering distribution-free, theoretically valid prediction intervals. In this work, we explore the application of standard conformal prediction and conformalized quantile regression to estimate uncertainty in forecasting surgical instrument motion, i.e., predicting direction and magnitude of surgical instruments' future motion. We analyze and compare their coverage and interval sizes, assessing the impact of multiple hypothesis testing and correction methods. Additionally, we show how these techniques can be employed to produce useful uncertainty heatmaps. To the best of our knowledge, this is the first study applying conformal prediction to surgical guidance, marking an initial step toward constructing principled prediction intervals with formal coverage guarantees in this domain."
  },
  {
    "title": "One-Shot is Enough: Consolidating Multi-Turn Attacks into Efficient Single-Turn Prompts for LLMs",
    "url": "http://arxiv.org/abs/2503.04856v1",
    "arxiv_id": "2503.04856v1",
    "authors": [
      "Junwoo Ha",
      "Hyunjun Kim",
      "Sangyoon Yu",
      "Haon Park",
      "Ashkan Yousefpour",
      "Yuna Park",
      "Suhyun Kim"
    ],
    "published": "2025-03-06T07:34:51+00:00",
    "summary": "Despite extensive safety enhancements in large language models (LLMs), multi-turn \"jailbreak\" conversations crafted by skilled human adversaries can still breach even the most sophisticated guardrails. However, these multi-turn attacks demand considerable manual effort, limiting their scalability. In this work, we introduce a novel approach called Multi-turn-to-Single-turn (M2S) that systematically converts multi-turn jailbreak prompts into single-turn attacks. Specifically, we propose three conversion strategies - Hyphenize, Numberize, and Pythonize - each preserving sequential context yet packaging it in a single query. Our experiments on the Multi-turn Human Jailbreak (MHJ) dataset show that M2S often increases or maintains high Attack Success Rates (ASRs) compared to original multi-turn conversations. Notably, using a StrongREJECT-based evaluation of harmfulness, M2S achieves up to 95.9% ASR on Mistral-7B and outperforms original multi-turn prompts by as much as 17.5% in absolute improvement on GPT-4o. Further analysis reveals that certain adversarial tactics, when consolidated into a single prompt, exploit structural formatting cues to evade standard policy checks. These findings underscore that single-turn attacks - despite being simpler and cheaper to conduct - can be just as potent, if not more, than their multi-turn counterparts. Our findings underscore the urgent need to reevaluate and reinforce LLM safety strategies, given how adversarial queries can be compacted into a single prompt while still retaining sufficient complexity to bypass existing safety measures."
  },
  {
    "title": "Simulation-based Analysis Of Highway Trajectory Planning Using High-Order Polynomial For Highly Automated Driving Function",
    "url": "http://arxiv.org/abs/2503.04159v1",
    "arxiv_id": "2503.04159v1",
    "authors": [
      "Milin Patel",
      "Marzana Khatun",
      "Rolf Jung",
      "Michael Gla\u00df"
    ],
    "published": "2025-03-06T07:23:17+00:00",
    "summary": "One of the fundamental tasks of autonomous driving is safe trajectory planning, the task of deciding where the vehicle needs to drive, while avoiding obstacles, obeying safety rules, and respecting the fundamental limits of road. Real-world application of such a method involves consideration of surrounding environment conditions and movements such as Lane Change, collision avoidance, and lane merge. The focus of the paper is to develop and implement safe collision free highway Lane Change trajectory using high order polynomial for Highly Automated Driving Function (HADF). Planning is often considered as a higher-level process than control. Behavior Planning Module (BPM) is designed that plans the high-level driving actions like Lane Change maneuver to safely achieve the functionality of transverse guidance ensuring safety of the vehicle using motion planning in a scenario including environmental situation. Based on the recommendation received from the (BPM), the function will generate a desire corresponding trajectory. The proposed planning system is situation specific with polynomial based algorithm for same direction two lane highway scenario. To support the trajectory system polynomial curve can be used to reduces overall complexity and thereby allows rapid computation. The proposed Lane Change scenario is modeled, and results has been analyzed (verified and validate) through the MATLAB simulation environment. The method proposed in this paper has achieved a significant improvement in safety and stability of Lane Changing maneuver."
  },
  {
    "title": "KidneyTalk-open: No-code Deployment of a Private Large Language Model with Medical Documentation-Enhanced Knowledge Database for Kidney Disease",
    "url": "http://arxiv.org/abs/2503.04153v1",
    "arxiv_id": "2503.04153v1",
    "authors": [
      "Yongchao Long",
      "Chao Yang",
      "Gongzheng Tang",
      "Jinwei Wang",
      "Zhun Sui",
      "Yuxi Zhou",
      "Shenda Hong",
      "Luxia Zhang"
    ],
    "published": "2025-03-06T07:01:36+00:00",
    "summary": "Privacy-preserving medical decision support for kidney disease requires localized deployment of large language models (LLMs) while maintaining clinical reasoning capabilities. Current solutions face three challenges: 1) Cloud-based LLMs pose data security risks; 2) Local model deployment demands technical expertise; 3) General LLMs lack mechanisms to integrate medical knowledge. Retrieval-augmented systems also struggle with medical document processing and clinical usability. We developed KidneyTalk-open, a desktop system integrating three technical components: 1) No-code deployment of state-of-the-art (SOTA) open-source LLMs (such as DeepSeek-r1, Qwen2.5) via local inference engine; 2) Medical document processing pipeline combining context-aware chunking and intelligent filtering; 3) Adaptive Retrieval and Augmentation Pipeline (AddRep) employing agents collaboration for improving the recall rate of medical documents. A graphical interface was designed to enable clinicians to manage medical documents and conduct AI-powered consultations without technical expertise. Experimental validation on 1,455 challenging nephrology exam questions demonstrates AddRep's effectiveness: achieving 29.1% accuracy (+8.1% over baseline) with intelligent knowledge integration, while maintaining robustness through 4.9% rejection rate to suppress hallucinations. Comparative case studies with the mainstream products (AnythingLLM, Chatbox, GPT4ALL) demonstrate KidneyTalk-open's superior performance in real clinical query. KidneyTalk-open represents the first no-code medical LLM system enabling secure documentation-enhanced medical Q&A on desktop. Its designs establishes a new framework for privacy-sensitive clinical AI applications. The system significantly lowers technical barriers while improving evidence traceability, enabling more medical staff or patients to use SOTA open-source LLMs conveniently."
  },
  {
    "title": "Analyzing the Impact of Augmented Reality Head-Mounted Displays on Workers' Safety and Situational Awareness in Hazardous Industrial Settings",
    "url": "http://arxiv.org/abs/2503.04075v1",
    "arxiv_id": "2503.04075v1",
    "authors": [
      "Graciela Camacho-Fidalgo",
      "Blain Judkins",
      "Kylee Friederichs",
      "Lara Soberanis",
      "Vicente Hernandez",
      "Kevin McSweeney",
      "Freddie Witherden",
      "Edgar Rojas-Mu\u00f1oz"
    ],
    "published": "2025-03-06T04:13:14+00:00",
    "summary": "Augmented Reality Head-Mounted Displays (AR-HMDs) have proven effective to assist workers. However, they may degrade their Safety and Situational Awareness (SSA), particularly in complex and hazardous industrial settings. This paper analyzes, objectively and subjectively, the effects of AR-HMDs' on workers' SSA in a simulated hazardous industrial environment. Our evaluation was comprised of sixty participants performing various tasks in a simulated cargo ship room while receiving remote guidance through one of three devices: two off-the-shelf AR-HMDs (Trimble XR10 with HoloLens 2, RealWear Navigator 520), and a smartphone (Google Pixel 6). Several sensors were installed throughout the room to obtain quantitative measures of the participants' safe execution of the tasks, such as the frequency in which they hit the objects in the room or stepped over simulated holes or oil spills. The results reported that the Trimble XR10 led to statistically highest head-knockers and knee-knocker incidents compared to the Navigator 520 and the Pixel 6. Furthermore, the Trimble XR10 also led to significantly higher difficulties to cross hatch doors, lower perceived safety, comfort, perceived performance, and usability. Overall, participants wearing AR-HMDs failed to perceive more hazards, meaning that safety-preserving capabilities must be developed for AR-HMDs before introducing them into industrial hazardous settings confidently."
  },
  {
    "title": "Insights from Rights and Wrongs: A Large Language Model for Solving Assertion Failures in RTL Design",
    "url": "http://arxiv.org/abs/2503.04057v1",
    "arxiv_id": "2503.04057v1",
    "authors": [
      "Jie Zhou",
      "Youshu Ji",
      "Ning Wang",
      "Yuchen Hu",
      "Xinyao Jiao",
      "Bingkun Yao",
      "Xinwei Fang",
      "Shuai Zhao",
      "Nan Guan",
      "Zhe Jiang"
    ],
    "published": "2025-03-06T03:17:48+00:00",
    "summary": "SystemVerilog Assertions (SVAs) are essential for verifying Register Transfer Level (RTL) designs, as they can be embedded into key functional paths to detect unintended behaviours. During simulation, assertion failures occur when the design's behaviour deviates from expectations. Solving these failures, i.e., identifying and fixing the issues causing the deviation, requires analysing complex logical and timing relationships between multiple signals. This process heavily relies on human expertise, and there is currently no automatic tool available to assist with it. Here, we present AssertSolver, an open-source Large Language Model (LLM) specifically designed for solving assertion failures. By leveraging synthetic training data and learning from error responses to challenging cases, AssertSolver achieves a bug-fixing pass@1 metric of 88.54% on our testbench, significantly outperforming OpenAI's o1-preview by up to 11.97%. We release our model and testbench for public access to encourage further research: https://github.com/SEU-ACAL/reproduce-AssertSolver-DAC-25."
  },
  {
    "title": "NsBM-GAT: A Non-stationary Block Maximum and Graph Attention Framework for General Traffic Crash Risk Prediction",
    "url": "http://arxiv.org/abs/2503.04018v1",
    "arxiv_id": "2503.04018v1",
    "authors": [
      "Kequan Chen",
      "Pan Liu",
      "Yuxuan Wang",
      "David Z. W. Wang",
      "Yifan Dai",
      "Zhibin Li"
    ],
    "published": "2025-03-06T02:12:40+00:00",
    "summary": "Accurate prediction of traffic crash risks for individual vehicles is essential for enhancing vehicle safety. While significant attention has been given to traffic crash risk prediction, existing studies face two main challenges: First, due to the scarcity of individual vehicle data before crashes, most models rely on hypothetical scenarios deemed dangerous by researchers. This raises doubts about their applicability to actual pre-crash conditions. Second, some crash risk prediction frameworks were learned from dashcam videos. Although such videos capture the pre-crash behavior of individual vehicles, they often lack critical information about the movements of surrounding vehicles. However, the interaction between a vehicle and its surrounding vehicles is highly influential in crash occurrences. To overcome these challenges, we propose a novel non-stationary extreme value theory (EVT), where the covariate function is optimized in a nonlinear fashion using a graph attention network. The EVT component incorporates the stochastic nature of crashes through probability distribution, which enhances model interpretability. Notably, the nonlinear covariate function enables the model to capture the interactive behavior between the target vehicle and its multiple surrounding vehicles, facilitating crash risk prediction across different driving tasks. We train and test our model using 100 sets of vehicle trajectory data before real crashes, collected via drones over three years from merging and weaving segments. We demonstrate that our model successfully learns micro-level precursors of crashes and fits a more accurate distribution with the aid of the nonlinear covariate function. Our experiments on the testing dataset show that the proposed model outperforms existing models by providing more accurate predictions for both rear-end and sideswipe crashes simultaneously."
  },
  {
    "title": "Planning and Control for Deformable Linear Object Manipulation",
    "url": "http://arxiv.org/abs/2503.04007v1",
    "arxiv_id": "2503.04007v1",
    "authors": [
      "Burak Aksoy",
      "John Wen"
    ],
    "published": "2025-03-06T01:44:36+00:00",
    "summary": "Manipulating a deformable linear object (DLO) such as wire, cable, and rope is a common yet challenging task due to their high degrees of freedom and complex deformation behaviors, especially in an environment with obstacles. Existing local control methods are efficient but prone to failure in complex scenarios, while precise global planners are computationally intensive and difficult to deploy. This paper presents an efficient, easy-to-deploy framework for collision-free DLO manipulation using mobile manipulators. We demonstrate the effectiveness of leveraging standard planning tools for high-dimensional DLO manipulation without requiring custom planners or extensive data-driven models. Our approach combines an off-the-shelf global planner with a real-time local controller. The global planner approximates the DLO as a series of rigid links connected by spherical joints, enabling rapid path planning without the need for problem-specific planners or large datasets. The local controller employs control barrier functions (CBFs) to enforce safety constraints, maintain the DLO integrity, prevent overstress, and handle obstacle avoidance. It compensates for modeling inaccuracies by using a state-of-the-art position-based dynamics technique that approximates physical properties like Young's and shear moduli. We validate our framework through extensive simulations and real-world demonstrations. In complex obstacle scenarios-including tent pole transport, corridor navigation, and tasks requiring varied stiffness-our method achieves a 100% success rate over thousands of trials, with significantly reduced planning times compared to state-of-the-art techniques. Real-world experiments include transportation of a tent pole and a rope using mobile manipulators. We share our ROS-based implementation to facilitate adoption in various applications."
  },
  {
    "title": "Enhancing Autonomous Driving Safety with Collision Scenario Integration",
    "url": "http://arxiv.org/abs/2503.03957v1",
    "arxiv_id": "2503.03957v1",
    "authors": [
      "Zi Wang",
      "Shiyi Lan",
      "Xinglong Sun",
      "Nadine Chang",
      "Zhenxin Li",
      "Zhiding Yu",
      "Jose M. Alvarez"
    ],
    "published": "2025-03-05T23:08:43+00:00",
    "summary": "Autonomous vehicle safety is crucial for the successful deployment of self-driving cars. However, most existing planning methods rely heavily on imitation learning, which limits their ability to leverage collision data effectively. Moreover, collecting collision or near-collision data is inherently challenging, as it involves risks and raises ethical and practical concerns. In this paper, we propose SafeFusion, a training framework to learn from collision data. Instead of over-relying on imitation learning, SafeFusion integrates safety-oriented metrics during training to enable collision avoidance learning. In addition, to address the scarcity of collision data, we propose CollisionGen, a scalable data generation pipeline to generate diverse, high-quality scenarios using natural language prompts, generative models, and rule-based filtering. Experimental results show that our approach improves planning performance in collision-prone scenarios by 56\\% over previous state-of-the-art planners while maintaining effectiveness in regular driving situations. Our work provides a scalable and effective solution for advancing the safety of autonomous driving systems."
  },
  {
    "title": "Safe LLM-Controlled Robots with Formal Guarantees via Reachability Analysis",
    "url": "http://arxiv.org/abs/2503.03911v1",
    "arxiv_id": "2503.03911v1",
    "authors": [
      "Ahmad Hafez",
      "Alireza Naderi Akhormeh",
      "Amr Hegazy",
      "Amr Alanwar"
    ],
    "published": "2025-03-05T21:23:15+00:00",
    "summary": "The deployment of Large Language Models (LLMs) in robotic systems presents unique safety challenges, particularly in unpredictable environments. Although LLMs, leveraging zero-shot learning, enhance human-robot interaction and decision-making capabilities, their inherent probabilistic nature and lack of formal guarantees raise significant concerns for safety-critical applications. Traditional model-based verification approaches often rely on precise system models, which are difficult to obtain for real-world robotic systems and may not be fully trusted due to modeling inaccuracies, unmodeled dynamics, or environmental uncertainties. To address these challenges, this paper introduces a safety assurance framework for LLM-controlled robots based on data-driven reachability analysis, a formal verification technique that ensures all possible system trajectories remain within safe operational limits. Our framework specifically investigates the problem of instructing an LLM to navigate the robot to a specified goal and assesses its ability to generate low-level control actions that successfully guide the robot safely toward that goal. By leveraging historical data to construct reachable sets of states for the robot-LLM system, our approach provides rigorous safety guarantees against unsafe behaviors without relying on explicit analytical models. We validate the framework through experimental case studies in autonomous navigation and task planning, demonstrating its effectiveness in mitigating risks associated with LLM-generated commands. This work advances the integration of formal methods into LLM-based robotics, offering a principled and practical approach to ensuring safety in next-generation autonomous systems."
  },
  {
    "title": "Parser Knows Best: Testing DBMS with Coverage-Guided Grammar-Rule Traversal",
    "url": "http://arxiv.org/abs/2503.03893v1",
    "arxiv_id": "2503.03893v1",
    "authors": [
      "Yu Liang",
      "Hong Hu"
    ],
    "published": "2025-03-05T20:50:41+00:00",
    "summary": "Database Management System (DBMS) is the key component for data-intensive applications. Recently, researchers propose many tools to comprehensively test DBMS systems for finding various bugs. However, these tools only cover a small subset of diverse syntax elements defined in DBMS-specific SQL dialects, leaving a large number of features unexplored. In this paper, we propose ParserFuzz, a novel fuzzing framework that automatically extracts grammar rules from DBMSs' built-in syntax definition files for SQL query generation. Without any input corpus, ParserFuzz can generate diverse query statements to saturate the grammar features of the tested DBMSs, which grammar features could be missed by previous tools. Additionally, ParserFuzz utilizes code coverage as feedback to guide the query mutation, which combines different DBMS features extracted from the syntax rules to find more function and safety bugs. In our evaluation, ParserFuzz outperforms all state-of-the-art existing DBMS testing tools in terms of bug finding, grammar rule coverage and code coverage. ParserFuzz detects 81 previously unknown bugs in total across 5 popular DBMSs, where all bugs are confirmed and 34 have been fixed."
  },
  {
    "title": "Seldonian Reinforcement Learning for Ad Hoc Teamwork",
    "url": "http://arxiv.org/abs/2503.03885v1",
    "arxiv_id": "2503.03885v1",
    "authors": [
      "Edoardo Zorzi",
      "Alberto Castellini",
      "Leonidas Bakopoulos",
      "Georgios Chalkiadakis",
      "Alessandro Farinelli"
    ],
    "published": "2025-03-05T20:37:02+00:00",
    "summary": "Most offline RL algorithms return optimal policies but do not provide statistical guarantees on undesirable behaviors. This could generate reliability issues in safety-critical applications, such as in some multiagent domains where agents, and possibly humans, need to interact to reach their goals without harming each other. In this work, we propose a novel offline RL approach, inspired by Seldonian optimization, which returns policies with good performance and statistically guaranteed properties with respect to predefined undesirable behaviors. In particular, our focus is on Ad Hoc Teamwork settings, where agents must collaborate with new teammates without prior coordination. Our method requires only a pre-collected dataset, a set of candidate policies for our agent, and a specification about the possible policies followed by the other players -- it does not require further interactions, training, or assumptions on the type and architecture of the policies. We test our algorithm in Ad Hoc Teamwork problems and show that it consistently finds reliable policies while improving sample efficiency with respect to standard ML baselines."
  },
  {
    "title": "Nexar Dashcam Collision Prediction Dataset and Challenge",
    "url": "http://arxiv.org/abs/2503.03848v1",
    "arxiv_id": "2503.03848v1",
    "authors": [
      "Daniel C. Moura",
      "Shizhan Zhu",
      "Orly Zvitia"
    ],
    "published": "2025-03-05T19:20:28+00:00",
    "summary": "This paper presents the Nexar Dashcam Collision Prediction Dataset and Challenge, designed to support research in traffic event analysis, collision prediction, and autonomous vehicle safety. The dataset consists of 1,500 annotated video clips, each approximately 40 seconds long, capturing a diverse range of real-world traffic scenarios. Videos are labeled with event type (collision/near-collision vs. normal driving), environmental conditions (lighting conditions and weather), and scene type (urban, rural, highway, etc.). For collision and near-collision cases, additional temporal labels are provided, including the precise moment of the event and the alert time, marking when the collision first becomes predictable.   To advance research on accident prediction, we introduce the Nexar Dashcam Collision Prediction Challenge, a public competition on top of this dataset. Participants are tasked with developing machine learning models that predict the likelihood of an imminent collision, given an input video. Model performance is evaluated using the average precision (AP) computed across multiple intervals before the accident (i.e. 500 ms, 1000 ms, and 1500 ms prior to the event), emphasizing the importance of early and reliable predictions.   The dataset is released under an open license with restrictions on unethical use, ensuring responsible research and innovation."
  },
  {
    "title": "RiskAgent: Autonomous Medical AI Copilot for Generalist Risk Prediction",
    "url": "http://arxiv.org/abs/2503.03802v1",
    "arxiv_id": "2503.03802v1",
    "authors": [
      "Fenglin Liu",
      "Jinge Wu",
      "Hongjian Zhou",
      "Xiao Gu",
      "Soheila Molaei",
      "Anshul Thakur",
      "Lei Clifton",
      "Honghan Wu",
      "David A. Clifton"
    ],
    "published": "2025-03-05T18:46:51+00:00",
    "summary": "The application of Large Language Models (LLMs) to various clinical applications has attracted growing research attention. However, real-world clinical decision-making differs significantly from the standardized, exam-style scenarios commonly used in current efforts. In this paper, we present the RiskAgent system to perform a broad range of medical risk predictions, covering over 387 risk scenarios across diverse complex diseases, e.g., cardiovascular disease and cancer. RiskAgent is designed to collaborate with hundreds of clinical decision tools, i.e., risk calculators and scoring systems that are supported by evidence-based medicine. To evaluate our method, we have built the first benchmark MedRisk specialized for risk prediction, including 12,352 questions spanning 154 diseases, 86 symptoms, 50 specialties, and 24 organ systems. The results show that our RiskAgent, with 8 billion model parameters, achieves 76.33% accuracy, outperforming the most recent commercial LLMs, o1, o3-mini, and GPT-4.5, and doubling the 38.39% accuracy of GPT-4o. On rare diseases, e.g., Idiopathic Pulmonary Fibrosis (IPF), RiskAgent outperforms o1 and GPT-4.5 by 27.27% and 45.46% accuracy, respectively. Finally, we further conduct a generalization evaluation on an external evidence-based diagnosis benchmark and show that our RiskAgent achieves the best results. These encouraging results demonstrate the great potential of our solution for diverse diagnosis domains. To improve the adaptability of our model in different scenarios, we have built and open-sourced a family of models ranging from 1 billion to 70 billion parameters. Our code, data, and models are all available at https://github.com/AI-in-Health/RiskAgent."
  },
  {
    "title": "Improving LLM Safety Alignment with Dual-Objective Optimization",
    "url": "http://arxiv.org/abs/2503.03710v1",
    "arxiv_id": "2503.03710v1",
    "authors": [
      "Xuandong Zhao",
      "Will Cai",
      "Tianneng Shi",
      "David Huang",
      "Licong Lin",
      "Song Mei",
      "Dawn Song"
    ],
    "published": "2025-03-05T18:01:05+00:00",
    "summary": "Existing training-time safety alignment techniques for large language models (LLMs) remain vulnerable to jailbreak attacks. Direct preference optimization (DPO), a widely deployed alignment method, exhibits limitations in both experimental and theoretical contexts as its loss function proves suboptimal for refusal learning. Through gradient-based analysis, we identify these shortcomings and propose an improved safety alignment that disentangles DPO objectives into two components: (1) robust refusal training, which encourages refusal even when partial unsafe generations are produced, and (2) targeted unlearning of harmful knowledge. This approach significantly increases LLM robustness against a wide range of jailbreak attacks, including prefilling, suffix, and multi-turn attacks across both in-distribution and out-of-distribution scenarios. Furthermore, we introduce a method to emphasize critical refusal tokens by incorporating a reward-based token-level weighting mechanism for refusal learning, which further improves the robustness against adversarial exploits. Our research also suggests that robustness to jailbreak attacks is correlated with token distribution shifts in the training process and internal representations of refusal and harmful tokens, offering valuable directions for future research in LLM safety alignment. The code is available at https://github.com/wicai24/DOOR-Alignment"
  },
  {
    "title": "AEGIS: Towards Formalized and Practical Memory-Safe Execution of C programs via MSWASM",
    "url": "http://arxiv.org/abs/2503.03698v1",
    "arxiv_id": "2503.03698v1",
    "authors": [
      "Shahram Esmaeilsabzali",
      "Arayi Khalatyan",
      "Zhijun Mo",
      "Sruthi Venkatanarayanan",
      "Shengjie Xu"
    ],
    "published": "2025-03-05T17:50:43+00:00",
    "summary": "Programs written in unsafe languages such as C are prone to memory safety errors, which can lead to program compromises and serious real-world security consequences. Recently, Memory-Safe WebAssembly (MSWASM) is introduced as a general-purpose intermediate bytecode with built-in memory safety semantics. Programs written in C can be compiled into MSWASM to get complete memory safety protection. In this paper, we present our extensions on MSWASM, which improve its semantics and practicality. First, we formalize MSWASM semantics in Coq/Iris, extending it with inter-module interaction, showing that MSWASM provides fine-grained isolation guarantees analogous to WASM's coarse-grained isolation via linear memory. Second, we present Aegis, a system to adopt the memory safety of MSWASM for C programs in an interoperable way. Aegis pipeline generates Checked C source code from MSWASM modules to enforce spatial memory safety. Checked C is a recent binary-compatible extension of C which can provide guaranteed spatial safety. Our design allows Aegis to protect C programs that depend on legacy C libraries with no extra dependency and with low overhead. Aegis pipeline incurs 67% runtime overhead and near-zero memory overhead on PolyBenchC programs compared to native."
  },
  {
    "title": "TeraSim: Uncovering Unknown Unsafe Events for Autonomous Vehicles through Generative Simulation",
    "url": "http://arxiv.org/abs/2503.03629v1",
    "arxiv_id": "2503.03629v1",
    "authors": [
      "Haowei Sun",
      "Xintao Yan",
      "Zhijie Qiao",
      "Haojie Zhu",
      "Yihao Sun",
      "Jiawei Wang",
      "Shengyin Shen",
      "Darian Hogue",
      "Rajanikant Ananta",
      "Derek Johnson",
      "Greg Stevens",
      "Greg McGuire",
      "Yifan Wei",
      "Wei Zheng",
      "Yong Sun",
      "Yasuo Fukai",
      "Henry X. Liu"
    ],
    "published": "2025-03-05T16:09:30+00:00",
    "summary": "Traffic simulation is essential for autonomous vehicle (AV) development, enabling comprehensive safety evaluation across diverse driving conditions. However, traditional rule-based simulators struggle to capture complex human interactions, while data-driven approaches often fail to maintain long-term behavioral realism or generate diverse safety-critical events. To address these challenges, we propose TeraSim, an open-source, high-fidelity traffic simulation platform designed to uncover unknown unsafe events and efficiently estimate AV statistical performance metrics, such as crash rates. TeraSim is designed for seamless integration with third-party physics simulators and standalone AV stacks, to construct a complete AV simulation system. Experimental results demonstrate its effectiveness in generating diverse safety-critical events involving both static and dynamic agents, identifying hidden deficiencies in AV systems, and enabling statistical performance evaluation. These findings highlight TeraSim's potential as a practical tool for AV safety assessment, benefiting researchers, developers, and policymakers. The code is available at https://github.com/mcity/TeraSim."
  },
  {
    "title": "TeraSim: Uncovering Unknown Unsafe Events for Autonomous Vehicles through Generative Simulation",
    "url": "http://arxiv.org/abs/2503.03629v2",
    "arxiv_id": "2503.03629v2",
    "authors": [
      "Haowei Sun",
      "Xintao Yan",
      "Zhijie Qiao",
      "Haojie Zhu",
      "Yihao Sun",
      "Jiawei Wang",
      "Shengyin Shen",
      "Darian Hogue",
      "Rajanikant Ananta",
      "Derek Johnson",
      "Greg Stevens",
      "Greg McGuire",
      "Yifan Wei",
      "Wei Zheng",
      "Yong Sun",
      "Yasuo Fukai",
      "Henry X. Liu"
    ],
    "published": "2025-03-05T16:09:30+00:00",
    "summary": "Traffic simulation is essential for autonomous vehicle (AV) development, enabling comprehensive safety evaluation across diverse driving conditions. However, traditional rule-based simulators struggle to capture complex human interactions, while data-driven approaches often fail to maintain long-term behavioral realism or generate diverse safety-critical events. To address these challenges, we propose TeraSim, an open-source, high-fidelity traffic simulation platform designed to uncover unknown unsafe events and efficiently estimate AV statistical performance metrics, such as crash rates. TeraSim is designed for seamless integration with third-party physics simulators and standalone AV stacks, to construct a complete AV simulation system. Experimental results demonstrate its effectiveness in generating diverse safety-critical events involving both static and dynamic agents, identifying hidden deficiencies in AV systems, and enabling statistical performance evaluation. These findings highlight TeraSim's potential as a practical tool for AV safety assessment, benefiting researchers, developers, and policymakers. The code is available at https://github.com/mcity/TeraSim."
  },
  {
    "title": "Simulation-Based Performance Evaluation of 3D Object Detection Methods with Deep Learning for a LiDAR Point Cloud Dataset in a SOTIF-related Use Case",
    "url": "http://arxiv.org/abs/2503.03548v1",
    "arxiv_id": "2503.03548v1",
    "authors": [
      "Milin Patel",
      "Rolf Jung"
    ],
    "published": "2025-03-05T14:32:32+00:00",
    "summary": "Safety of the Intended Functionality (SOTIF) addresses sensor performance limitations and deep learning-based object detection insufficiencies to ensure the intended functionality of Automated Driving Systems (ADS). This paper presents a methodology examining the adaptability and performance evaluation of the 3D object detection methods on a LiDAR point cloud dataset generated by simulating a SOTIF-related Use Case. The major contributions of this paper include defining and modelling a SOTIF-related Use Case with 21 diverse weather conditions and generating a LiDAR point cloud dataset suitable for application of 3D object detection methods. The dataset consists of 547 frames, encompassing clear, cloudy, rainy weather conditions, corresponding to different times of the day, including noon, sunset, and night. Employing MMDetection3D and OpenPCDET toolkits, the performance of State-of-the-Art (SOTA) 3D object detection methods is evaluated and compared by testing the pre-trained Deep Learning (DL) models on the generated dataset using Average Precision (AP) and Recall metrics."
  },
  {
    "title": "Simulation-Based Application of Safety of The Intended Functionality to Mitigate Foreseeable Misuse in Automated Driving Systems",
    "url": "http://arxiv.org/abs/2503.03534v1",
    "arxiv_id": "2503.03534v1",
    "authors": [
      "Milin Patel",
      "Rolf Jung"
    ],
    "published": "2025-03-05T14:16:49+00:00",
    "summary": "The development of Automated Driving Systems (ADS) has the potential to revolutionise the transportation industry, but it also presents significant safety challenges. One of the key challenges is ensuring that the ADS is safe in the event of Foreseeable Misuse (FM) by the human driver. To address this challenge, a case study on simulation-based testing to mitigate FM by the driver using the driving simulator is presented. FM by the human driver refers to potential driving scenarios where the driver misinterprets the intended functionality of ADS, leading to hazardous behaviour. Safety of the Intended Functionality (SOTIF) focuses on ensuring the absence of unreasonable risk resulting from hazardous behaviours related to functional insufficiencies caused by FM and performance limitations of sensors and machine learning-based algorithms for ADS. The simulation-based application of SOTIF to mitigate FM in ADS entails determining potential misuse scenarios, conducting simulation-based testing, and evaluating the effectiveness of measures dedicated to preventing or mitigating FM. The major contribution includes defining (i) test requirements for performing simulation-based testing of a potential misuse scenario, (ii) evaluation criteria in accordance with SOTIF requirements for implementing measures dedicated to preventing or mitigating FM, and (iii) approach to evaluate the effectiveness of the measures dedicated to preventing or mitigating FM. In conclusion, an exemplary case study incorporating driver-vehicle interface and driver interactions with ADS forming the basis for understanding the factors and causes contributing to FM is investigated. Furthermore, the test procedure for evaluating the effectiveness of the measures dedicated to preventing or mitigating FM by the driver is developed in this work."
  },
  {
    "title": "DO-IQS: Dynamics-Aware Offline Inverse Q-Learning for Optimal Stopping with Unknown Gain Functions",
    "url": "http://arxiv.org/abs/2503.03515v1",
    "arxiv_id": "2503.03515v1",
    "authors": [
      "Anna Kuchko"
    ],
    "published": "2025-03-05T14:01:17+00:00",
    "summary": "We consider Inverse Optimal Stopping (IOS) problem where, based on stopped expert trajectories, one aims to recover the optimal stopping region through continuation and stopping gain functions approximation. The uniqueness of the stopping region allows the use of IOS in real-world applications with safety concerns. While current state-of-the-art inverse reinforcement learning methods recover both a Q-function and the corresponding optimal policy, they fail to account for specific challenges posed by optimal stopping problems. These include data sparsity near the stopping region, non-Markovian nature of the continuation gain, a proper treatment of boundary conditions, the need for a stable offline approach for risk-sensitive applications, and a lack of a quality evaluation metric. These challenges are addressed with the proposed Dynamics-Aware Offline Inverse Q-Learning for Optimal Stopping (DO-IQS), which incorporates temporal information by approximating the cumulative continuation gain together with the world dynamics and the Q-function without querying to the environment. Moreover, a confidence-based oversampling approach is proposed to treat the data sparsity problem. We demonstrate the performance of our models on real and artificial data including an optimal intervention for critical events problem."
  },
  {
    "title": "CURVALID: Geometrically-guided Adversarial Prompt Detection",
    "url": "http://arxiv.org/abs/2503.03502v1",
    "arxiv_id": "2503.03502v1",
    "authors": [
      "Canaan Yung",
      "Hanxun Huang",
      "Sarah Monazam Erfani",
      "Christopher Leckie"
    ],
    "published": "2025-03-05T13:47:53+00:00",
    "summary": "Adversarial prompts capable of jailbreaking large language models (LLMs) and inducing undesirable behaviours pose a significant obstacle to their safe deployment. Current mitigation strategies rely on activating built-in defence mechanisms or fine-tuning the LLMs, but the fundamental distinctions between adversarial and benign prompts are yet to be understood. In this work, we introduce CurvaLID, a novel defense framework that efficiently detects adversarial prompts by leveraging their geometric properties. It is agnostic to the type of LLM, offering a unified detection framework across diverse adversarial prompts and LLM architectures. CurvaLID builds on the geometric analysis of text prompts to uncover their underlying differences. We theoretically extend the concept of curvature via the Whewell equation into an $n$-dimensional word embedding space, enabling us to quantify local geometric properties, including semantic shifts and curvature in the underlying manifolds. Additionally, we employ Local Intrinsic Dimensionality (LID) to capture geometric features of text prompts within adversarial subspaces. Our findings reveal that adversarial prompts differ fundamentally from benign prompts in terms of their geometric characteristics. Our results demonstrate that CurvaLID delivers superior detection and rejection of adversarial queries, paving the way for safer LLM deployment. The source code can be found at https://github.com/Cancanxxx/CurvaLID"
  },
  {
    "title": "Differentially Private Learners for Heterogeneous Treatment Effects",
    "url": "http://arxiv.org/abs/2503.03486v1",
    "arxiv_id": "2503.03486v1",
    "authors": [
      "Maresa Schr\u00f6der",
      "Valentyn Melnychuk",
      "Stefan Feuerriegel"
    ],
    "published": "2025-03-05T13:24:58+00:00",
    "summary": "Patient data is widely used to estimate heterogeneous treatment effects and thus understand the effectiveness and safety of drugs. Yet, patient data includes highly sensitive information that must be kept private. In this work, we aim to estimate the conditional average treatment effect (CATE) from observational data under differential privacy. Specifically, we present DP-CATE, a novel framework for CATE estimation that is Neyman-orthogonal and further ensures differential privacy of the estimates. Our framework is highly general: it applies to any two-stage CATE meta-learner with a Neyman-orthogonal loss function, and any machine learning model can be used for nuisance estimation. We further provide an extension of our DP-CATE, where we employ RKHS regression to release the complete CATE function while ensuring differential privacy. We demonstrate our DP-CATE across various experiments using synthetic and real-world datasets. To the best of our knowledge, we are the first to provide a framework for CATE estimation that is Neyman-orthogonal and differentially private."
  },
  {
    "title": "SafeVLA: Towards Safety Alignment of Vision-Language-Action Model via Safe Reinforcement Learning",
    "url": "http://arxiv.org/abs/2503.03480v1",
    "arxiv_id": "2503.03480v1",
    "authors": [
      "Borong Zhang",
      "Yuhao Zhang",
      "Jiaming Ji",
      "Yingshan Lei",
      "Josef Dai",
      "Yuanpei Chen",
      "Yaodong Yang"
    ],
    "published": "2025-03-05T13:16:55+00:00",
    "summary": "Vision-language-action models (VLAs) have shown great potential as generalist robot policies. However, these models pose urgent safety challenges during deployment, including the risk of physical harm to the environment, the robot itself, and humans. How can safety be explicitly incorporated into VLAs? In this work, we propose SafeVLA, a novel algorithm designed to integrate safety into VLAs, ensuring the protection of the environment, robot hardware and humans in real-world settings. SafeVLA effectively balances safety and task performance by employing large-scale constrained learning within simulated environments. We demonstrate that SafeVLA outperforms the current state-of-the-art method in both safety and task performance, achieving average improvements of 83.58% and 3.85%, respectively, in simulation. By prioritizing safety, our approach eliminates high-risk behaviors and reduces the upper bound of unsafe behaviors to 1/35 of that in the current state-of-the-art, thereby significantly mitigating long-tail risks. Furthermore, the learned safety constraints generalize to diverse, unseen scenarios, including multiple out-of-distribution perturbations and tasks. Our data, models and newly proposed benchmark environment are available at https://sites.google.com/view/pku-safevla."
  },
  {
    "title": "SEAL: Safety Enhanced Trajectory Planning and Control Framework for Quadrotor Flight in Complex Environments",
    "url": "http://arxiv.org/abs/2503.03346v1",
    "arxiv_id": "2503.03346v1",
    "authors": [
      "Yiming Wang",
      "Jianbin Ma",
      "Junda Wu",
      "Huizhe Li",
      "Zhexuan Zhou",
      "Youmin Gong",
      "Jie Mei",
      "Guangfu Ma"
    ],
    "published": "2025-03-05T10:15:56+00:00",
    "summary": "For quadrotors, achieving safe and autonomous flight in complex environments with wind disturbances and dynamic obstacles still faces significant challenges. Most existing methods address wind disturbances in either trajectory planning or control, which may lead to hazardous situations during flight. The emergence of dynamic obstacles would further worsen the situation. Therefore, we propose an efficient and reliable framework for quadrotors that incorporates wind disturbance estimations during both the planning and control phases via a generalized proportional integral observer. First, we develop a real-time adaptive spatial-temporal trajectory planner that utilizes Hamilton-Jacobi (HJ) reachability analysis for error dynamics resulting from wind disturbances. By considering the forward reachability sets propagation on an Euclidean Signed Distance Field (ESDF) map, safety is guaranteed. Additionally, a Nonlinear Model Predictive Control (NMPC) controller considering wind disturbance compensation is implemented for robust trajectory tracking. Simulation and real-world experiments verify the effectiveness of our framework. The video and supplementary material will be available at https://github.com/Ma29-HIT/SEAL/."
  },
  {
    "title": "Quantitative Magnetohydrodynamic Modelling of Flux Pumping in ASDEX Upgrade",
    "url": "http://arxiv.org/abs/2503.03344v1",
    "arxiv_id": "2503.03344v1",
    "authors": [
      "Haowei Zhang",
      "Matthias H\u00f6lzl",
      "Isabel Krebs",
      "Andreas Burckhart",
      "Alexander Bock",
      "Sibylle G\u00fcnter",
      "Valentin Igochine",
      "Karl Lackner",
      "Rohan Ramasamy",
      "Hartmut Zohm",
      "the JOREK team",
      "the ASDEX Upgrade team"
    ],
    "published": "2025-03-05T10:14:54+00:00",
    "summary": "The sawtooth-free hybrid scenario has been achieved recently in ASDEX Upgrade (AUG) with applied non-inductive current sources and auxiliary heating [A. Burckhart et al 2023 Nucl. Fusion 63 126056]. Control experiments in AUG suggest that the self-regulating magnetic flux pumping mechanism, characterized by anomalous current redistribution, is responsible for clamping the central safety factor (q_0) close to unity, thereby preventing the sawtooth onset. This work presents a numerical and theoretical investigation of flux pumping in the AUG hybrid scenario based on the two-temperature, visco-resistive, full magnetohydrodynamic (MHD) model with the JOREK code. To quantitatively model the flux pumping, we choose realistic parameters, plasma configurations, and source terms based on AUG experiments. During the initial saturation stage of the unstable 1/1 quasi-interchange mode (on millisecond timescales), q_0 exhibits fast under-damped oscillation and reaches a value closer to unity, which is attributed to the self-regulation of core plasma and the fast dynamo effect on the order of V/m. On the longer resistive diffusion timescale of seconds, the slow negative dynamo effect on the order of mV/m induced by the 1/1 MHD instability plays an effective role in flux pumping, which provides quantitative agreement with experimental observations for the first time. The final saturated 1/1 MHD instability exhibits features of the quasi-interchange mode and tearing mode, and the associated convective plasma flow velocity is a few m/s. The toroidal negative electric field from the slow dynamo dominantly offsets the positive current drive and continuously redistributes the current density and pressure. As a result, q_0 is maintained close to unity due to the low-shear profiles of current density and pressure in the plasma core, and the system enters a sawtooth-free and quasi-stationary helical state."
  },
  {
    "title": "Safety Verification of Nonlinear Stochastic Systems via Probabilistic Tube",
    "url": "http://arxiv.org/abs/2503.03328v1",
    "arxiv_id": "2503.03328v1",
    "authors": [
      "Zishun Liu",
      "Saber Jafarpour",
      "Yongxin Chen"
    ],
    "published": "2025-03-05T10:02:25+00:00",
    "summary": "We address the problem of safety verification for nonlinear stochastic systems, specifically the task of certifying that system trajectories remain within a safe set with high probability. To tackle this challenge, we adopt a set-erosion strategy, which decouples the effects of stochastic disturbances from deterministic dynamics. This approach converts the stochastic safety verification problem on a safe set into a deterministic safety verification problem on an eroded subset of the safe set. The success of this strategy hinges on the depth of erosion, which is determined by a probabilistic tube that bounds the deviation of stochastic trajectories from their corresponding deterministic trajectories. Our main contribution is the establishment of a tight bound for the probabilistic tube of nonlinear stochastic systems. To obtain a probabilistic bound for stochastic trajectories, we adopt a martingale-based approach. The core innovation lies in the design of a novel energy function associated with the averaged moment generating function, which forms an affine martingale, a generalization of the traditional c-martingale. Using this energy function, we derive a precise bound for the probabilistic tube. Furthermore, we enhance this bound by incorporating the union-bound inequality for strictly contractive dynamics. By integrating the derived probabilistic tubes into the set-erosion strategy, we demonstrate that the safety verification problem for nonlinear stochastic systems can be reduced to a deterministic safety verification problem. Our theoretical results are validated through applications in reachability-based safety verification and safe controller synthesis, accompanied by several numerical examples that illustrate their effectiveness."
  },
  {
    "title": "Supervised Visual Docking Network for Unmanned Surface Vehicles Using Auto-labeling in Real-world Water Environments",
    "url": "http://arxiv.org/abs/2503.03282v1",
    "arxiv_id": "2503.03282v1",
    "authors": [
      "Yijie Chu",
      "Ziniu Wu",
      "Yong Yue",
      "Eng Gee Lim",
      "Paolo Paoletti",
      "Xiaohui Zhu"
    ],
    "published": "2025-03-05T09:07:13+00:00",
    "summary": "Unmanned Surface Vehicles (USVs) are increasingly applied to water operations such as environmental monitoring and river-map modeling. It faces a significant challenge in achieving precise autonomous docking at ports or stations, still relying on remote human control or external positioning systems for accuracy and safety which limits the full potential of human-out-of-loop deployment for USVs.This paper introduces a novel supervised learning pipeline with the auto-labeling technique for USVs autonomous visual docking. Firstly, we designed an auto-labeling data collection pipeline that appends relative pose and image pair to the dataset. This step does not require conventional manual labeling for supervised learning. Secondly, the Neural Dock Pose Estimator (NDPE) is proposed to achieve relative dock pose prediction without the need for hand-crafted feature engineering, camera calibration, and peripheral markers. Moreover, The NDPE can accurately predict the relative dock pose in real-world water environments, facilitating the implementation of Position-Based Visual Servo (PBVS) and low-level motion controllers for efficient and autonomous docking.Experiments show that the NDPE is robust to the disturbance of the distance and the USV velocity. The effectiveness of our proposed solution is tested and validated in real-world water environments, reflecting its capability to handle real-world autonomous docking tasks."
  },
  {
    "title": "Reduced Spatial Dependency for More General Video-level Deepfake Detection",
    "url": "http://arxiv.org/abs/2503.03270v1",
    "arxiv_id": "2503.03270v1",
    "authors": [
      "Beilin Chu",
      "Xuan Xu",
      "Yufei Zhang",
      "Weike You",
      "Linna Zhou"
    ],
    "published": "2025-03-05T08:51:55+00:00",
    "summary": "As one of the prominent AI-generated content, Deepfake has raised significant safety concerns. Although it has been demonstrated that temporal consistency cues offer better generalization capability, existing methods based on CNNs inevitably introduce spatial bias, which hinders the extraction of intrinsic temporal features. To address this issue, we propose a novel method called Spatial Dependency Reduction (SDR), which integrates common temporal consistency features from multiple spatially-perturbed clusters, to reduce the dependency of the model on spatial information. Specifically, we design multiple Spatial Perturbation Branch (SPB) to construct spatially-perturbed feature clusters. Subsequently, we utilize the theory of mutual information and propose a Task-Relevant Feature Integration (TRFI) module to capture temporal features residing in similar latent space from these clusters. Finally, the integrated feature is fed into a temporal transformer to capture long-range dependencies. Extensive benchmarks and ablation studies demonstrate the effectiveness and rationale of our approach."
  },
  {
    "title": "Trajectory Prediction for Autonomous Driving: Progress, Limitations, and Future Directions",
    "url": "http://arxiv.org/abs/2503.03262v1",
    "arxiv_id": "2503.03262v1",
    "authors": [
      "Nadya Abdel Madjid",
      "Abdulrahman Ahmad",
      "Murad Mebrahtu",
      "Yousef Babaa",
      "Abdelmoamen Nasser",
      "Sumbal Malik",
      "Bilal Hassan",
      "Naoufel Werghi",
      "Jorge Dias",
      "Majid Khonji"
    ],
    "published": "2025-03-05T08:38:51+00:00",
    "summary": "As the potential for autonomous vehicles to be integrated on a large scale into modern traffic systems continues to grow, ensuring safe navigation in dynamic environments is crucial for smooth integration. To guarantee safety and prevent collisions, autonomous vehicles must be capable of accurately predicting the trajectories of surrounding traffic agents. Over the past decade, significant efforts from both academia and industry have been dedicated to designing solutions for precise trajectory forecasting. These efforts have produced a diverse range of approaches, raising questions about the differences between these methods and whether trajectory prediction challenges have been fully addressed. This paper reviews a substantial portion of recent trajectory prediction methods and devises a taxonomy to classify existing solutions. A general overview of the prediction pipeline is also provided, covering input and output modalities, modeling features, and prediction paradigms discussed in the literature. In addition, the paper discusses active research areas within trajectory prediction, addresses the posed research questions, and highlights the remaining research gaps and challenges."
  },
  {
    "title": "STORM: Spatial-Temporal Iterative Optimization for Reliable Multicopter Trajectory Generation",
    "url": "http://arxiv.org/abs/2503.03252v1",
    "arxiv_id": "2503.03252v1",
    "authors": [
      "Jinhao Zhang",
      "Zhexuan Zhou",
      "Wenlong Xia",
      "Youmin Gong",
      "Jie Mei"
    ],
    "published": "2025-03-05T08:11:59+00:00",
    "summary": "Efficient and safe trajectory planning plays a critical role in the application of quadrotor unmanned aerial vehicles. Currently, the inherent trade-off between constraint compliance and computational efficiency enhancement in UAV trajectory optimization problems has not been sufficiently addressed. To enhance the performance of UAV trajectory optimization, we propose a spatial-temporal iterative optimization framework. Firstly, B-splines are utilized to represent UAV trajectories, with rigorous safety assurance achieved through strict enforcement of constraints on control points. Subsequently, a set of QP-LP subproblems via spatial-temporal decoupling and constraint linearization is derived. Finally, an iterative optimization strategy incorporating guidance gradients is employed to obtain high-performance UAV trajectories in different scenarios. Both simulation and real-world experimental results validate the efficiency and high-performance of the proposed optimization framework in generating safe and fast trajectories. Our source codes will be released for community reference at https://hitsz-mas.github.io/STORM"
  },
  {
    "title": "Distributed Certifiably Correct Range-Aided SLAM",
    "url": "http://arxiv.org/abs/2503.03192v1",
    "arxiv_id": "2503.03192v1",
    "authors": [
      "Alexander Thoms",
      "Alan Papalia",
      "Jared Velasquez",
      "David M. Rosen",
      "Sriram Narasimhan"
    ],
    "published": "2025-03-05T05:17:15+00:00",
    "summary": "Reliable simultaneous localization and mapping (SLAM) algorithms are necessary for safety-critical autonomous navigation. In the communication-constrained multi-agent setting, navigation systems increasingly use point-to-point range sensors as they afford measurements with low bandwidth requirements and known data association. The state estimation problem for these systems takes the form of range-aided (RA) SLAM. However, distributed algorithms for solving the RA-SLAM problem lack formal guarantees on the quality of the returned estimate. To this end, we present the first distributed algorithm for RA-SLAM that can efficiently recover certifiably globally optimal solutions. Our algorithm, distributed certifiably correct RA-SLAM (DCORA), achieves this via the Riemannian Staircase method, where computational procedures developed for distributed certifiably correct pose graph optimization are generalized to the RA-SLAM problem. We demonstrate DCORA's efficacy on real-world multi-agent datasets by achieving absolute trajectory errors comparable to those of a state-of-the-art centralized certifiably correct RA-SLAM algorithm. Additionally, we perform a parametric study on the structure of the RA-SLAM problem using synthetic data, revealing how common parameters affect DCORA's performance."
  },
  {
    "title": "Physically-Feasible Reactive Synthesis for Terrain-Adaptive Locomotion via Trajectory Optimization and Symbolic Repair",
    "url": "http://arxiv.org/abs/2503.03071v1",
    "arxiv_id": "2503.03071v1",
    "authors": [
      "Ziyi Zhou",
      "Qian Meng",
      "Hadas Kress-Gazit",
      "Ye Zhao"
    ],
    "published": "2025-03-05T00:21:23+00:00",
    "summary": "We propose an integrated planning framework for quadrupedal locomotion over dynamically changing, unforeseen terrains. Existing approaches either rely on heuristics for instantaneous foothold selection--compromising safety and versatility--or solve expensive trajectory optimization problems with complex terrain features and long time horizons. In contrast, our framework leverages reactive synthesis to generate correct-by-construction controllers at the symbolic level, and mixed-integer convex programming (MICP) for dynamic and physically feasible footstep planning for each symbolic transition. We use a high-level manager to reduce the large state space in synthesis by incorporating local environment information, improving synthesis scalability. To handle specifications that cannot be met due to dynamic infeasibility, and to minimize costly MICP solves, we leverage a symbolic repair process to generate only necessary symbolic transitions. During online execution, re-running the MICP with real-world terrain data, along with runtime symbolic repair, bridges the gap between offline synthesis and online execution. We demonstrate, in simulation, our framework's capabilities to discover missing locomotion skills and react promptly in safety-critical environments, such as scattered stepping stones and rebars."
  },
  {
    "title": "LINGOLY-TOO: Disentangling Memorisation from Reasoning with Linguistic Templatisation and Orthographic Obfuscation",
    "url": "http://arxiv.org/abs/2503.02972v1",
    "arxiv_id": "2503.02972v1",
    "authors": [
      "Jude Khouja",
      "Karolina Korgul",
      "Simi Hellsten",
      "Lingyi Yang",
      "Vlad Neacs",
      "Harry Mayne",
      "Ryan Kearns",
      "Andrew Bean",
      "Adam Mahdi"
    ],
    "published": "2025-03-04T19:57:47+00:00",
    "summary": "Effective evaluation of the reasoning capabilities of large language models (LLMs) are susceptible to overestimation due to data exposure of evaluation benchmarks. We introduce a framework for producing linguistic reasoning problems that reduces the effect of memorisation in model performance estimates and apply this framework to develop LINGOLY-TOO, a challenging evaluation benchmark for linguistic reasoning. By developing orthographic templates, we dynamically obfuscate the writing systems of real languages to generate numerous question variations. These variations preserve the reasoning steps required for each solution while reducing the likelihood of specific problem instances appearing in model training data. Our experiments demonstrate that frontier models, including OpenAI o1-preview and DeepSeem R1, struggle with advanced reasoning. Our analysis also shows that LLMs exhibit noticeable variance in accuracy across permutations of the same problem, and on average perform better on questions appearing in their original orthography. Our findings highlight the opaque nature of response generation in LLMs and provide evidence that prior data exposure contributes to overestimating the reasoning capabilities of frontier models."
  },
  {
    "title": "LINGOLY-TOO: Disentangling Memorisation from Reasoning with Linguistic Templatisation and Orthographic Obfuscation",
    "url": "http://arxiv.org/abs/2503.02972v2",
    "arxiv_id": "2503.02972v2",
    "authors": [
      "Jude Khouja",
      "Karolina Korgul",
      "Simi Hellsten",
      "Lingyi Yang",
      "Vlad Neacs",
      "Harry Mayne",
      "Ryan Kearns",
      "Andrew Bean",
      "Adam Mahdi"
    ],
    "published": "2025-03-04T19:57:47+00:00",
    "summary": "Assessing the reasoning capabilities of large language models (LLMs) is susceptible to overestimation due to data exposure of evaluation benchmarks. We introduce a framework for producing linguistic reasoning problems that reduces the effect of memorisation in model performance estimates and apply this framework to develop LINGOLY-TOO, a challenging benchmark for linguistic reasoning. By developing orthographic templates, we dynamically obfuscate the writing systems of real languages to generate numerousquestion variations. These variations preserve the reasoning steps required for each solution while reducing the likelihood of specific problem instances appearing in model training data. Our experiments demonstrate that frontier models, including Claud 3.7 Sonnet, o1-preview and DeepSeek R1, struggle with advanced reasoning. Our analysis also shows that LLMs exhibit noticeable variance in accuracy across permutations of the same problem, and on average perform better on questions appearing in their original orthography. Our findings highlight the opaque nature of response generation in LLMs and provide evidence that prior data exposure contributes to over estimating the reasoning capabilities of frontier models."
  },
  {
    "title": "LINGOLY-TOO: Disentangling Memorisation from Reasoning with Linguistic Templatisation and Orthographic Obfuscation",
    "url": "http://arxiv.org/abs/2503.02972v3",
    "arxiv_id": "2503.02972v3",
    "authors": [
      "Jude Khouja",
      "Karolina Korgul",
      "Simi Hellsten",
      "Lingyi Yang",
      "Vlad Neacsu",
      "Harry Mayne",
      "Ryan Kearns",
      "Andrew Bean",
      "Adam Mahdi"
    ],
    "published": "2025-03-04T19:57:47+00:00",
    "summary": "Assessing the reasoning capabilities of large language models (LLMs) is susceptible to overestimation due to data exposure of evaluation benchmarks. We introduce a framework for producing linguistic reasoning problems that reduces the effect of memorisation in model performance estimates and apply this framework to develop LINGOLY-TOO, a challenging benchmark for linguistic reasoning. By developing orthographic templates, we dynamically obfuscate the writing systems of real languages to generate numerousquestion variations. These variations preserve the reasoning steps required for each solution while reducing the likelihood of specific problem instances appearing in model training data. Our experiments demonstrate that frontier models, including Claud 3.7 Sonnet, o1-preview and DeepSeek R1, struggle with advanced reasoning. Our analysis also shows that LLMs exhibit noticeable variance in accuracy across permutations of the same problem, and on average perform better on questions appearing in their original orthography. Our findings highlight the opaque nature of response generation in LLMs and provide evidence that prior data exposure contributes to over estimating the reasoning capabilities of frontier models."
  },
  {
    "title": "Revolutionizing Traffic Management with AI-Powered Machine Vision: A Step Toward Smart Cities",
    "url": "http://arxiv.org/abs/2503.02967v1",
    "arxiv_id": "2503.02967v1",
    "authors": [
      "Seyed Hossein Hosseini DolatAbadi",
      "Sayyed Mohammad Hossein Hashemi",
      "Mohammad Hosseini",
      "Moein-Aldin AliHosseini"
    ],
    "published": "2025-03-04T19:50:42+00:00",
    "summary": "The rapid urbanization of cities and increasing vehicular congestion have posed significant challenges to traffic management and safety. This study explores the transformative potential of artificial intelligence (AI) and machine vision technologies in revolutionizing traffic systems. By leveraging advanced surveillance cameras and deep learning algorithms, this research proposes a system for real-time detection of vehicles, traffic anomalies, and driver behaviors. The system integrates geospatial and weather data to adapt dynamically to environmental conditions, ensuring robust performance in diverse scenarios. Using YOLOv8 and YOLOv11 models, the study achieves high accuracy in vehicle detection and anomaly recognition, optimizing traffic flow and enhancing road safety. These findings contribute to the development of intelligent traffic management solutions and align with the vision of creating smart cities with sustainable and efficient urban infrastructure."
  },
  {
    "title": "A Theoretical Model for Grit in Pursuing Ambitious Ends",
    "url": "http://arxiv.org/abs/2503.02952v1",
    "arxiv_id": "2503.02952v1",
    "authors": [
      "Avrim Blum",
      "Emily Diana",
      "Kavya Ravichandran",
      "Alexander Williams Tolbert"
    ],
    "published": "2025-03-04T19:17:42+00:00",
    "summary": "Ambition and risk-taking have been heralded as important ways for marginalized communities to get out of cycles of poverty. As a result, educational messaging often encourages individuals to strengthen their personal resolve and develop characteristics such as discipline and grit to succeed in ambitious ends. However, recent work in philosophy and sociology highlights that this messaging often does more harm than good for students in these situations. We study similar questions using a different epistemic approach and in simple theoretical models -- we provide a quantitative model of decision-making between stable and risky choices in the improving multi-armed bandits framework. We use this model to first study how individuals' \"strategies\" are affected by their level of grittiness and how this affects their accrued rewards. Then, we study the impact of various interventions, such as increasing grit or providing a financial safety net. Our investigation of rational decision making involves two different formal models of rationality, the competitive ratio between the accrued reward and the optimal reward and Bayesian quantification of uncertainty."
  },
  {
    "title": "KodCode: A Diverse, Challenging, and Verifiable Synthetic Dataset for Coding",
    "url": "http://arxiv.org/abs/2503.02951v1",
    "arxiv_id": "2503.02951v1",
    "authors": [
      "Zhangchen Xu",
      "Yang Liu",
      "Yueqin Yin",
      "Mingyuan Zhou",
      "Radha Poovendran"
    ],
    "published": "2025-03-04T19:17:36+00:00",
    "summary": "We introduce KodCode, a synthetic dataset that addresses the persistent challenge of acquiring high-quality, verifiable training data across diverse difficulties and domains for training Large Language Models for coding. Existing code-focused resources typically fail to ensure either the breadth of coverage (e.g., spanning simple coding tasks to advanced algorithmic problems) or verifiable correctness (e.g., unit tests). In contrast, KodCode comprises question-solution-test triplets that are systematically validated via a self-verification procedure. Our pipeline begins by synthesizing a broad range of coding questions, then generates solutions and test cases with additional attempts allocated to challenging problems. Finally, post-training data synthesis is done by rewriting questions into diverse formats and generating responses under a test-based reject sampling procedure from a reasoning model (DeepSeek R1). This pipeline yields a large-scale, robust and diverse coding dataset. KodCode is suitable for supervised fine-tuning and the paired unit tests also provide great potential for RL tuning. Fine-tuning experiments on coding benchmarks (HumanEval(+), MBPP(+), BigCodeBench, and LiveCodeBench) demonstrate that KodCode-tuned models achieve state-of-the-art performance, surpassing models like Qwen2.5-Coder-32B-Instruct and DeepSeek-R1-Distill-Llama-70B."
  },
  {
    "title": "Optimal Power Management for Large-Scale Battery Energy Storage Systems via Bayesian Inference",
    "url": "http://arxiv.org/abs/2503.02866v1",
    "arxiv_id": "2503.02866v1",
    "authors": [
      "Amir Farakhor",
      "Iman Askari",
      "Di Wu",
      "Yebin Wang",
      "Huazhen Fang"
    ],
    "published": "2025-03-04T18:45:40+00:00",
    "summary": "Large-scale battery energy storage systems (BESS) have found ever-increasing use across industry and society to accelerate clean energy transition and improve energy supply reliability and resilience. However, their optimal power management poses significant challenges: the underlying high-dimensional nonlinear nonconvex optimization lacks computational tractability in real-world implementation, and the uncertainty of the exogenous power demand makes exact optimization difficult. This paper presents a new solution framework to address these bottlenecks. The solution pivots on introducing power-sharing ratios to specify each cell's power quota from the output power demand. To find the optimal power-sharing ratios, we formulate a nonlinear model predictive control (NMPC) problem to achieve power-loss-minimizing BESS operation while complying with safety, cell balancing, and power supply-demand constraints. We then propose a parameterized control policy for the power-sharing ratios, which utilizes only three parameters, to reduce the computational demand in solving the NMPC problem. This policy parameterization allows us to translate the NMPC problem into a Bayesian inference problem for the sake of 1) computational tractability, and 2) overcoming the nonconvexity of the optimization problem. We leverage the ensemble Kalman inversion technique to solve the parameter estimation problem. Concurrently, a low-level control loop is developed to seamlessly integrate our proposed approach with the BESS to ensure practical implementation. This low-level controller receives the optimal power-sharing ratios, generates output power references for the cells, and maintains a balance between power supply and demand despite uncertainty in output power. We conduct extensive simulations and experiments on a 20-cell prototype to validate the proposed approach."
  },
  {
    "title": "FairSense-AI: Responsible AI Meets Sustainability",
    "url": "http://arxiv.org/abs/2503.02865v1",
    "arxiv_id": "2503.02865v1",
    "authors": [
      "Shaina Raza",
      "Mukund Sayeeganesh Chettiar",
      "Matin Yousefabadi",
      "Tahniat Khan",
      "Marcelo Lotif"
    ],
    "published": "2025-03-04T18:43:57+00:00",
    "summary": "In this paper, we introduce FairSense-AI: a multimodal framework designed to detect and mitigate bias in both text and images. By leveraging Large Language Models (LLMs) and Vision-Language Models (VLMs), FairSense-AI uncovers subtle forms of prejudice or stereotyping that can appear in content, providing users with bias scores, explanatory highlights, and automated recommendations for fairness enhancements. In addition, FairSense-AI integrates an AI risk assessment component that aligns with frameworks like the MIT AI Risk Repository and NIST AI Risk Management Framework, enabling structured identification of ethical and safety concerns. The platform is optimized for energy efficiency via techniques such as model pruning and mixed-precision computation, thereby reducing its environmental footprint. Through a series of case studies and applications, we demonstrate how FairSense-AI promotes responsible AI use by addressing both the social dimension of fairness and the pressing need for sustainability in large-scale AI deployments. https://vectorinstitute.github.io/FairSense-AI, https://pypi.org/project/fair-sense-ai/"
  },
  {
    "title": "FairSense-AI: Responsible AI Meets Sustainability",
    "url": "http://arxiv.org/abs/2503.02865v2",
    "arxiv_id": "2503.02865v2",
    "authors": [
      "Shaina Raza",
      "Mukund Sayeeganesh Chettiar",
      "Matin Yousefabadi",
      "Tahniat Khan",
      "Marcelo Lotif"
    ],
    "published": "2025-03-04T18:43:57+00:00",
    "summary": "In this paper, we introduce FairSense-AI: a multimodal framework designed to detect and mitigate bias in both text and images. By leveraging Large Language Models (LLMs) and Vision-Language Models (VLMs), FairSense-AI uncovers subtle forms of prejudice or stereotyping that can appear in content, providing users with bias scores, explanatory highlights, and automated recommendations for fairness enhancements. In addition, FairSense-AI integrates an AI risk assessment component that aligns with frameworks like the MIT AI Risk Repository and NIST AI Risk Management Framework, enabling structured identification of ethical and safety concerns. The platform is optimized for energy efficiency via techniques such as model pruning and mixed-precision computation, thereby reducing its environmental footprint. Through a series of case studies and applications, we demonstrate how FairSense-AI promotes responsible AI use by addressing both the social dimension of fairness and the pressing need for sustainability in large-scale AI deployments. https://vectorinstitute.github.io/FairSense-AI, https://pypi.org/project/fair-sense-ai/ (Sustainability , Responsible AI , Large Language Models , Vision Language Models , Ethical AI , Green AI)"
  },
  {
    "title": "Multimodal AI predicts clinical outcomes of drug combinations from preclinical data",
    "url": "http://arxiv.org/abs/2503.02781v1",
    "arxiv_id": "2503.02781v1",
    "authors": [
      "Yepeng Huang",
      "Xiaorui Su",
      "Varun Ullanat",
      "Ivy Liang",
      "Lindsay Clegg",
      "Damilola Olabode",
      "Nicholas Ho",
      "Bino John",
      "Megan Gibbs",
      "Marinka Zitnik"
    ],
    "published": "2025-03-04T16:55:14+00:00",
    "summary": "Predicting clinical outcomes from preclinical data is essential for identifying safe and effective drug combinations. Current models rely on structural or target-based features to identify high-efficacy, low-toxicity drug combinations. However, these approaches fail to incorporate the multimodal data necessary for accurate, clinically-relevant predictions. Here, we introduce MADRIGAL, a multimodal AI model that learns from structural, pathway, cell viability, and transcriptomic data to predict drug combination effects across 953 clinical outcomes and 21842 compounds, including combinations of approved drugs and novel compounds in development. MADRIGAL uses a transformer bottleneck module to unify preclinical drug data modalities while handling missing data during training and inference--a major challenge in multimodal learning. It outperforms single-modality methods and state-of-the-art models in predicting adverse drug interactions. MADRIGAL performs virtual screening of anticancer drug combinations and supports polypharmacy management for type II diabetes and metabolic dysfunction-associated steatohepatitis (MASH). It identifies transporter-mediated drug interactions. MADRIGAL predicts resmetirom, the first and only FDA-approved drug for MASH, among therapies with the most favorable safety profile. It supports personalized cancer therapy by integrating genomic profiles from cancer patients. Using primary acute myeloid leukemia samples and patient-derived xenograft models, it predicts the efficacy of personalized drug combinations. Integrating MADRIGAL with a large language model allows users to describe clinical outcomes in natural language, improving safety assessment by identifying potential adverse interactions and toxicity risks. MADRIGAL provides a multimodal approach for designing combination therapies with improved predictive accuracy and clinical relevance."
  },
  {
    "title": "Deep Learning-Enhanced Visual Monitoring in Hazardous Underwater Environments with a Swarm of Micro-Robots",
    "url": "http://arxiv.org/abs/2503.02752v1",
    "arxiv_id": "2503.02752v1",
    "authors": [
      "Shuang Chen",
      "Yifeng He",
      "Barry Lennox",
      "Farshad Arvin",
      "Amir Atapour-Abarghouei"
    ],
    "published": "2025-03-04T16:19:06+00:00",
    "summary": "Long-term monitoring and exploration of extreme environments, such as underwater storage facilities, is costly, labor-intensive, and hazardous. Automating this process with low-cost, collaborative robots can greatly improve efficiency. These robots capture images from different positions, which must be processed simultaneously to create a spatio-temporal model of the facility. In this paper, we propose a novel approach that integrates data simulation, a multi-modal deep learning network for coordinate prediction, and image reassembly to address the challenges posed by environmental disturbances causing drift and rotation in the robots' positions and orientations. Our approach enhances the precision of alignment in noisy environments by integrating visual information from snapshots, global positional context from masks, and noisy coordinates. We validate our method through extensive experiments using synthetic data that simulate real-world robotic operations in underwater settings. The results demonstrate very high coordinate prediction accuracy and plausible image assembly, indicating the real-world applicability of our approach. The assembled images provide clear and coherent views of the underwater environment for effective monitoring and inspection, showcasing the potential for broader use in extreme settings, further contributing to improved safety, efficiency, and cost reduction in hazardous field monitoring. Code is available on https://github.com/ChrisChen1023/Micro-Robot-Swarm."
  },
  {
    "title": "Generative Modeling of Microweather Wind Velocities for Urban Air Mobility",
    "url": "http://arxiv.org/abs/2503.02690v1",
    "arxiv_id": "2503.02690v1",
    "authors": [
      "Tristan A. Shah",
      "Michael C. Stanley",
      "James E. Warner"
    ],
    "published": "2025-03-04T15:03:15+00:00",
    "summary": "Motivated by the pursuit of safe, reliable, and weather-tolerant urban air mobility (UAM) solutions, this work proposes a generative modeling approach for characterizing microweather wind velocities. Microweather, or the weather conditions in highly localized areas, is particularly complex in urban environments owing to the chaotic and turbulent nature of wind flows. Furthermore, traditional means of assessing local wind fields are not generally viable solutions for UAM applications: 1) field measurements that would rely on permanent wind profiling systems in operational air space are not practical, 2) physics-based models that simulate fluid dynamics at a sufficiently high resolution are not computationally tractable, and 3) data-driven modeling approaches that are largely deterministic ignore the inherent variability in turbulent flows that dictates UAM reliability. Thus, advancements in predictive capabilities are needed to help mitigate the unique operational safety risks that microweather winds pose for smaller, lighter weight UAM aircraft.   This work aims to model microweather wind velocities in a manner that is computationally-efficient, captures random variability, and would only require a temporary, rather than permanent, field measurement campaign. Inspired by recent breakthroughs in conditional generative AI such as text-to-image generation, the proposed approach learns a probabilistic macro-to-microweather mapping between regional weather forecasts and measured local wind velocities using generative modeling (denoising diffusion probabilistic models, flow matching, and Gaussian mixture models). A simple proof of concept was implemented using a dataset comprised of local (micro) measurements from a Sonic Detection and Ranging (SoDAR) wind profiler along with (macro) forecast data from a nearby weather station over the same time period."
  },
  {
    "title": "State of play and future directions in industrial computer vision AI standards",
    "url": "http://arxiv.org/abs/2503.02675v1",
    "arxiv_id": "2503.02675v1",
    "authors": [
      "Artemis Stefanidou",
      "Panagiotis Radoglou-Grammatikis",
      "Vasileios Argyriou",
      "Panagiotis Sarigiannidis",
      "Iraklis Varlamis",
      "Georgios Th. Papadopoulos"
    ],
    "published": "2025-03-04T14:46:34+00:00",
    "summary": "The recent tremendous advancements in the areas of Artificial Intelligence (AI) and Deep Learning (DL) have also resulted into corresponding remarkable progress in the field of Computer Vision (CV), showcasing robust technological solutions in a wide range of application sectors of high industrial interest (e.g., healthcare, autonomous driving, automation, etc.). Despite the outstanding performance of CV systems in specific domains, their development and exploitation at industrial-scale necessitates, among other, the addressing of requirements related to the reliability, transparency, trustworthiness, security, safety, and robustness of the developed AI models. The latter raises the imperative need for the development of efficient, comprehensive and widely-adopted industrial standards. In this context, this study investigates the current state of play regarding the development of industrial computer vision AI standards, emphasizing on critical aspects, like model interpretability, data quality, and regulatory compliance. In particular, a systematic analysis of launched and currently developing CV standards, proposed by the main international standardization bodies (e.g. ISO/IEC, IEEE, DIN, etc.) is performed. The latter is complemented by a comprehensive discussion on the current challenges and future directions observed in this regularization endeavor."
  },
  {
    "title": "Human-aligned Safe Reinforcement Learning for Highway On-Ramp Merging in Dense Traffic",
    "url": "http://arxiv.org/abs/2503.02624v1",
    "arxiv_id": "2503.02624v1",
    "authors": [
      "Yang Li",
      "Shijie Yuan",
      "Yuan Chang",
      "Xiaolong Chen",
      "Qisong Yang",
      "Zhiyuan Yang",
      "Hongmao Qin"
    ],
    "published": "2025-03-04T13:49:12+00:00",
    "summary": "Most reinforcement learning (RL) approaches for the decision-making of autonomous driving consider safety as a reward instead of a cost, which makes it hard to balance the tradeoff between safety and other objectives. Human risk preference has also rarely been incorporated, and the trained policy might be either conservative or aggressive for users. To this end, this study proposes a human-aligned safe RL approach for autonomous merging, in which the high-level decision problem is formulated as a constrained Markov decision process (CMDP) that incorporates users' risk preference into the safety constraints, followed by a model predictive control (MPC)-based low-level control. The safety level of RL policy can be adjusted by computing cost limits of CMDP's constraints based on risk preferences and traffic density using a fuzzy control method. To filter out unsafe or invalid actions, we design an action shielding mechanism that pre-executes RL actions using an MPC method and performs collision checks with surrounding agents. We also provide theoretical proof to validate the effectiveness of the shielding mechanism in enhancing RL's safety and sample efficiency. Simulation experiments in multiple levels of traffic densities show that our method can significantly reduce safety violations without sacrificing traffic efficiency. Furthermore, due to the use of risk preference-aware constraints in CMDP and action shielding, we can not only adjust the safety level of the final policy but also reduce safety violations during the training stage, proving a promising solution for online learning in real-world environments."
  },
  {
    "title": "MM-OR: A Large Multimodal Operating Room Dataset for Semantic Understanding of High-Intensity Surgical Environments",
    "url": "http://arxiv.org/abs/2503.02579v1",
    "arxiv_id": "2503.02579v1",
    "authors": [
      "Ege \u00d6zsoy",
      "Chantal Pellegrini",
      "Tobias Czempiel",
      "Felix Tristram",
      "Kun Yuan",
      "David Bani-Harouni",
      "Ulrich Eck",
      "Benjamin Busam",
      "Matthias Keicher",
      "Nassir Navab"
    ],
    "published": "2025-03-04T13:00:52+00:00",
    "summary": "Operating rooms (ORs) are complex, high-stakes environments requiring precise understanding of interactions among medical staff, tools, and equipment for enhancing surgical assistance, situational awareness, and patient safety. Current datasets fall short in scale, realism and do not capture the multimodal nature of OR scenes, limiting progress in OR modeling. To this end, we introduce MM-OR, a realistic and large-scale multimodal spatiotemporal OR dataset, and the first dataset to enable multimodal scene graph generation. MM-OR captures comprehensive OR scenes containing RGB-D data, detail views, audio, speech transcripts, robotic logs, and tracking data and is annotated with panoptic segmentations, semantic scene graphs, and downstream task labels. Further, we propose MM2SG, the first multimodal large vision-language model for scene graph generation, and through extensive experiments, demonstrate its ability to effectively leverage multimodal inputs. Together, MM-OR and MM2SG establish a new benchmark for holistic OR understanding, and open the path towards multimodal scene analysis in complex, high-stakes environments. Our code, and data is available at https://github.com/egeozsoy/MM-OR."
  },
  {
    "title": "LLM-Safety Evaluations Lack Robustness",
    "url": "http://arxiv.org/abs/2503.02574v1",
    "arxiv_id": "2503.02574v1",
    "authors": [
      "Tim Beyer",
      "Sophie Xhonneux",
      "Simon Geisler",
      "Gauthier Gidel",
      "Leo Schwinn",
      "Stephan G\u00fcnnemann"
    ],
    "published": "2025-03-04T12:55:07+00:00",
    "summary": "In this paper, we argue that current safety alignment research efforts for large language models are hindered by many intertwined sources of noise, such as small datasets, methodological inconsistencies, and unreliable evaluation setups. This can, at times, make it impossible to evaluate and compare attacks and defenses fairly, thereby slowing progress. We systematically analyze the LLM safety evaluation pipeline, covering dataset curation, optimization strategies for automated red-teaming, response generation, and response evaluation using LLM judges. At each stage, we identify key issues and highlight their practical impact. We also propose a set of guidelines for reducing noise and bias in evaluations of future attack and defense papers. Lastly, we offer an opposing perspective, highlighting practical reasons for existing limitations. We believe that addressing the outlined problems in future research will improve the field's ability to generate easily comparable results and make measurable progress."
  },
  {
    "title": "Tracking-Aware Deformation Field Estimation for Non-rigid 3D Reconstruction in Robotic Surgeries",
    "url": "http://arxiv.org/abs/2503.02558v1",
    "arxiv_id": "2503.02558v1",
    "authors": [
      "Zeqing Wang",
      "Han Fang",
      "Yihong Xu",
      "Yutong Ban"
    ],
    "published": "2025-03-04T12:33:17+00:00",
    "summary": "Minimally invasive procedures have been advanced rapidly by the robotic laparoscopic surgery. The latter greatly assists surgeons in sophisticated and precise operations with reduced invasiveness. Nevertheless, it is still safety critical to be aware of even the least tissue deformation during instrument-tissue interactions, especially in 3D space. To address this, recent works rely on NeRF to render 2D videos from different perspectives and eliminate occlusions. However, most of the methods fail to predict the accurate 3D shapes and associated deformation estimates robustly. Differently, we propose Tracking-Aware Deformation Field (TADF), a novel framework which reconstructs the 3D mesh along with the 3D tissue deformation simultaneously. It first tracks the key points of soft tissue by a foundation vision model, providing an accurate 2D deformation field. Then, the 2D deformation field is smoothly incorporated with a neural implicit reconstruction network to obtain tissue deformation in the 3D space. Finally, we experimentally demonstrate that the proposed method provides more accurate deformation estimation compared with other 3D neural reconstruction methods in two public datasets."
  },
  {
    "title": "World Models for Anomaly Detection during Model-Based Reinforcement Learning Inference",
    "url": "http://arxiv.org/abs/2503.02552v1",
    "arxiv_id": "2503.02552v1",
    "authors": [
      "Fabian Domberg",
      "Georg Schildbach"
    ],
    "published": "2025-03-04T12:25:01+00:00",
    "summary": "Learning-based controllers are often purposefully kept out of real-world applications due to concerns about their safety and reliability. We explore how state-of-the-art world models in Model-Based Reinforcement Learning can be utilized beyond the training phase to ensure a deployed policy only operates within regions of the state-space it is sufficiently familiar with. This is achieved by continuously monitoring discrepancies between a world model's predictions and observed system behavior during inference. It allows for triggering appropriate measures, such as an emergency stop, once an error threshold is surpassed. This does not require any task-specific knowledge and is thus universally applicable. Simulated experiments on established robot control tasks show the effectiveness of this method, recognizing changes in local robot geometry and global gravitational magnitude. Real-world experiments using an agile quadcopter further demonstrate the benefits of this approach by detecting unexpected forces acting on the vehicle. These results indicate how even in new and adverse conditions, safe and reliable operation of otherwise unpredictable learning-based controllers can be achieved."
  },
  {
    "title": "A Systematic Literature Review on Safety of the Intended Functionality for Automated Driving Systems",
    "url": "http://arxiv.org/abs/2503.02498v1",
    "arxiv_id": "2503.02498v1",
    "authors": [
      "Milin Patel",
      "Rolf Jung",
      "Marzana Khatun"
    ],
    "published": "2025-03-04T11:04:36+00:00",
    "summary": "In the automobile industry, ensuring the safety of automated vehicles equipped with the Automated Driving System (ADS) is becoming a significant focus due to the increasing development and deployment of automated driving. Automated driving depends on sensing both the external and internal environments of a vehicle, utilizing perception sensors and algorithms, and Electrical/Electronic (E/E) systems for situational awareness and response. ISO 21448 is the standard for Safety of the Intended Functionality (SOTIF) that aims to ensure that the ADS operate safely within their intended functionality. SOTIF focuses on preventing or mitigating potential hazards that may arise from the limitations or failures of the ADS, including hazards due to insufficiencies of specification, or performance insufficiencies, as well as foreseeable misuse of the intended functionality. However, the challenge lies in ensuring the safety of vehicles despite the limited availability of extensive and systematic literature on SOTIF. To address this challenge, a Systematic Literature Review (SLR) on SOTIF for the ADS is performed following the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines. The objective is to methodically gather and analyze the existing literature on SOTIF. The major contributions of this paper are: (i) presenting a summary of the literature by synthesizing and organizing the collective findings, methodologies, and insights into distinct thematic groups, and (ii) summarizing and categorizing the acknowledged limitations based on data extracted from an SLR of 51 research papers published between 2018 and 2023. Furthermore, research gaps are determined, and future research directions are proposed."
  },
  {
    "title": "UAV-VLPA*: A Vision-Language-Path-Action System for Optimal Route Generation on a Large Scales",
    "url": "http://arxiv.org/abs/2503.02454v1",
    "arxiv_id": "2503.02454v1",
    "authors": [
      "Oleg Sautenkov",
      "Aibek Akhmetkazy",
      "Yasheerah Yaqoot",
      "Muhammad Ahsan Mustafa",
      "Grik Tadevosyan",
      "Artem Lykov",
      "Dzmitry Tsetserukou"
    ],
    "published": "2025-03-04T10:02:53+00:00",
    "summary": "The UAV-VLPA* (Visual-Language-Planning-and-Action) system represents a cutting-edge advancement in aerial robotics, designed to enhance communication and operational efficiency for unmanned aerial vehicles (UAVs). By integrating advanced planning capabilities, the system addresses the Traveling Salesman Problem (TSP) to optimize flight paths, reducing the total trajectory length by 18.5\\% compared to traditional methods. Additionally, the incorporation of the A* algorithm enables robust obstacle avoidance, ensuring safe and efficient navigation in complex environments. The system leverages satellite imagery processing combined with the Visual Language Model (VLM) and GPT's natural language processing capabilities, allowing users to generate detailed flight plans through simple text commands. This seamless fusion of visual and linguistic analysis empowers precise decision-making and mission planning, making UAV-VLPA* a transformative tool for modern aerial operations. With its unmatched operational efficiency, navigational safety, and user-friendly functionality, UAV-VLPA* sets a new standard in autonomous aerial robotics, paving the way for future innovations in the field."
  },
  {
    "title": "Artificial Intelligence in Reactor Physics: Current Status and Future Prospects",
    "url": "http://arxiv.org/abs/2503.02440v1",
    "arxiv_id": "2503.02440v1",
    "authors": [
      "Ruizhi Zhang",
      "Shengfeng Zhu",
      "Kan Wang",
      "Ding She",
      "Jean-Philippe Argaud",
      "Bertrand Bouriquet",
      "Qing Li",
      "Helin Gong"
    ],
    "published": "2025-03-04T09:36:58+00:00",
    "summary": "Reactor physics is the study of neutron properties, focusing on using models to examine the interactions between neutrons and materials in nuclear reactors. Artificial intelligence (AI) has made significant contributions to reactor physics, e.g., in operational simulations, safety design, real-time monitoring, core management and maintenance. This paper presents a comprehensive review of AI approaches in reactor physics, especially considering the category of Machine Learning (ML), with the aim of describing the application scenarios, frontier topics, unsolved challenges and future research directions. From equation solving and state parameter prediction to nuclear industry applications, this paper provides a step-by-step overview of ML methods applied to steady-state, transient and combustion problems. Most literature works achieve industry-demanded models by enhancing the efficiency of deterministic methods or correcting uncertainty methods, which leads to successful applications. However, research on ML methods in reactor physics is somewhat fragmented, and the ability to generalize models needs to be strengthened. Progress is still possible, especially in addressing theoretical challenges and enhancing industrial applications such as building surrogate models and digital twins."
  },
  {
    "title": "Unlocking a New Rust Programming Experience: Fast and Slow Thinking with LLMs to Conquer Undefined Behaviors",
    "url": "http://arxiv.org/abs/2503.02335v1",
    "arxiv_id": "2503.02335v1",
    "authors": [
      "Renshuang Jiang",
      "Pan Dong",
      "Zhenling Duan",
      "Yu Shi",
      "Xiaoxiang Fang",
      "Yan Ding",
      "Jun Ma",
      "Shuai Zhao",
      "Zhe Jiang"
    ],
    "published": "2025-03-04T06:48:45+00:00",
    "summary": "To provide flexibility and low-level interaction capabilities, the unsafe tag in Rust is essential in many projects, but undermines memory safety and introduces Undefined Behaviors (UBs) that reduce safety. Eliminating these UBs requires a deep understanding of Rust's safety rules and strong typing. Traditional methods require depth analysis of code, which is laborious and depends on knowledge design. The powerful semantic understanding capabilities of LLM offer new opportunities to solve this problem. Although existing large model debugging frameworks excel in semantic tasks, limited by fixed processes and lack adaptive and dynamic adjustment capabilities. Inspired by the dual process theory of decision-making (Fast and Slow Thinking), we present a LLM-based framework called RustBrain that automatically and flexibly minimizes UBs in Rust projects. Fast thinking extracts features to generate solutions, while slow thinking decomposes, verifies, and generalizes them abstractly. To apply verification and generalization results to solution generation, enabling dynamic adjustments and precise outputs, RustBrain integrates two thinking through a feedback mechanism. Experimental results on Miri dataset show a 94.3% pass rate and 80.4% execution rate, improving flexibility and Rust projects safety."
  },
  {
    "title": "Rethinking Static Line Rating for Economic and Efficient Power Operation in South Korea",
    "url": "http://arxiv.org/abs/2503.02274v1",
    "arxiv_id": "2503.02274v1",
    "authors": [
      "Junseon Park",
      "Junhyun Lee",
      "Hyeongon Park"
    ],
    "published": "2025-03-04T04:46:43+00:00",
    "summary": "In South Korea, power grid is currently operated based on the static line rating (SLR) method, where the transmission   line capacity is determined based on extreme weather conditions. However, with global warming, there is a concern   that the temperatures during summer may exceed the SLR criteria, posing safety risks. On the other hand, the conservative estimates used for winter conditions limit the utilization of renewable energy. Proposals to install new lines face   significant financial and environmental hurdles, complicating efforts to adapt to these changing conditions. Dynamic   Line Rating (DLR) offers a real-time solution but requires extensive weather monitoring and complex integration. This   paper proposes a novel method that improves on SLR by analyzing historical data to refine line rating criteria on a   monthly, seasonal, and semi-annual basis. Through simulations, we show our approach significantly enhances cost effectiveness and reliability of the power system, achieving efficiencies close to DLR with existing infrastructure. This   method offers a practical alternative to overcome the limitations of SLR and the implementation challenges of DLR."
  },
  {
    "title": "V2X-LLM: Enhancing V2X Integration and Understanding in Connected Vehicle Corridors",
    "url": "http://arxiv.org/abs/2503.02239v1",
    "arxiv_id": "2503.02239v1",
    "authors": [
      "Keshu Wu",
      "Pei Li",
      "Yang Zhou",
      "Rui Gan",
      "Junwei You",
      "Yang Cheng",
      "Jingwen Zhu",
      "Steven T. Parker",
      "Bin Ran",
      "David A. Noyce",
      "Zhengzhong Tu"
    ],
    "published": "2025-03-04T03:28:30+00:00",
    "summary": "The advancement of Connected and Automated Vehicles (CAVs) and Vehicle-to-Everything (V2X) offers significant potential for enhancing transportation safety, mobility, and sustainability. However, the integration and analysis of the diverse and voluminous V2X data, including Basic Safety Messages (BSMs) and Signal Phase and Timing (SPaT) data, present substantial challenges, especially on Connected Vehicle Corridors. These challenges include managing large data volumes, ensuring real-time data integration, and understanding complex traffic scenarios. Although these projects have developed an advanced CAV data pipeline that enables real-time communication between vehicles, infrastructure, and other road users for managing connected vehicle and roadside unit (RSU) data, significant hurdles in data comprehension and real-time scenario analysis and reasoning persist. To address these issues, we introduce the V2X-LLM framework, a novel enhancement to the existing CV data pipeline. V2X-LLM leverages Large Language Models (LLMs) to improve the understanding and real-time analysis of V2X data. The framework includes four key tasks: Scenario Explanation, offering detailed narratives of traffic conditions; V2X Data Description, detailing vehicle and infrastructure statuses; State Prediction, forecasting future traffic states; and Navigation Advisory, providing optimized routing instructions. By integrating LLM-driven reasoning with V2X data within the data pipeline, the V2X-LLM framework offers real-time feedback and decision support for traffic management. This integration enhances the accuracy of traffic analysis, safety, and traffic optimization. Demonstrations in a real-world urban corridor highlight the framework's potential to advance intelligent transportation systems."
  },
  {
    "title": "ADMM-MCBF-LCA: A Layered Control Architecture for Safe Real-Time Navigation",
    "url": "http://arxiv.org/abs/2503.02208v1",
    "arxiv_id": "2503.02208v1",
    "authors": [
      "Anusha Srikanthan",
      "Yifan Xue",
      "Vijay Kumar",
      "Nikolai Matni",
      "Nadia Figueroa"
    ],
    "published": "2025-03-04T02:40:17+00:00",
    "summary": "We consider the problem of safe real-time navigation of a robot in a dynamic environment with moving obstacles of arbitrary smooth geometries and input saturation constraints. We assume that the robot detects and models nearby obstacle boundaries with a short-range sensor and that this detection is error-free. This problem presents three main challenges: i) input constraints, ii) safety, and iii) real-time computation. To tackle all three challenges, we present a layered control architecture (LCA) consisting of an offline path library generation layer, and an online path selection and safety layer. To overcome the limitations of reactive methods, our offline path library consists of feasible controllers, feedback gains, and reference trajectories. To handle computational burden and safety, we solve online path selection and generate safe inputs that run at 100 Hz. Through simulations on Gazebo and Fetch hardware in an indoor environment, we evaluate our approach against baselines that are layered, end-to-end, or reactive. Our experiments demonstrate that among all algorithms, only our proposed LCA is able to complete tasks such as reaching a goal, safely. When comparing metrics such as safety, input error, and success rate, we show that our approach generates safe and feasible inputs throughout the robot execution."
  },
  {
    "title": "Words or Vision: Do Vision-Language Models Have Blind Faith in Text?",
    "url": "http://arxiv.org/abs/2503.02199v1",
    "arxiv_id": "2503.02199v1",
    "authors": [
      "Ailin Deng",
      "Tri Cao",
      "Zhirui Chen",
      "Bryan Hooi"
    ],
    "published": "2025-03-04T02:21:07+00:00",
    "summary": "Vision-Language Models (VLMs) excel in integrating visual and textual information for vision-centric tasks, but their handling of inconsistencies between modalities is underexplored. We investigate VLMs' modality preferences when faced with visual data and varied textual inputs in vision-centered settings. By introducing textual variations to four vision-centric tasks and evaluating ten Vision-Language Models (VLMs), we discover a \\emph{``blind faith in text''} phenomenon: VLMs disproportionately trust textual data over visual data when inconsistencies arise, leading to significant performance drops under corrupted text and raising safety concerns. We analyze factors influencing this text bias, including instruction prompts, language model size, text relevance, token order, and the interplay between visual and textual certainty. While certain factors, such as scaling up the language model size, slightly mitigate text bias, others like token order can exacerbate it due to positional biases inherited from language models. To address this issue, we explore supervised fine-tuning with text augmentation and demonstrate its effectiveness in reducing text bias. Additionally, we provide a theoretical analysis suggesting that the blind faith in text phenomenon may stem from an imbalance of pure text and multi-modal data during training. Our findings highlight the need for balanced training and careful consideration of modality interactions in VLMs to enhance their robustness and reliability in handling multi-modal data inconsistencies."
  },
  {
    "title": "Adversarial Tokenization",
    "url": "http://arxiv.org/abs/2503.02174v1",
    "arxiv_id": "2503.02174v1",
    "authors": [
      "Renato Lui Geh",
      "Zilei Shao",
      "Guy Van den Broeck"
    ],
    "published": "2025-03-04T01:31:17+00:00",
    "summary": "Current LLM pipelines account for only one possible tokenization for a given string, ignoring exponentially many alternative tokenizations during training and inference. For example, the standard Llama3 tokenization of penguin is [p,enguin], yet [peng,uin] is another perfectly valid alternative. In this paper, we show that despite LLMs being trained solely on one tokenization, they still retain semantic understanding of other tokenizations, raising questions about their implications in LLM safety. Put succinctly, we answer the following question: can we adversarially tokenize an obviously malicious string to evade safety and alignment restrictions? We show that not only is adversarial tokenization an effective yet previously neglected axis of attack, but it is also competitive against existing state-of-the-art adversarial approaches without changing the text of the harmful request. We empirically validate this exploit across three state-of-the-art LLMs and adversarial datasets, revealing a previously unknown vulnerability in subword models."
  },
  {
    "title": "Four Principles for Physically Interpretable World Models",
    "url": "http://arxiv.org/abs/2503.02143v1",
    "arxiv_id": "2503.02143v1",
    "authors": [
      "Jordan Peper",
      "Zhenjiang Mao",
      "Yuang Geng",
      "Siyuan Pan",
      "Ivan Ruchkin"
    ],
    "published": "2025-03-04T00:19:32+00:00",
    "summary": "As autonomous systems are increasingly deployed in open and uncertain settings, there is a growing need for trustworthy world models that can reliably predict future high-dimensional observations. The learned latent representations in world models lack direct mapping to meaningful physical quantities and dynamics, limiting their utility and interpretability in downstream planning, control, and safety verification. In this paper, we argue for a fundamental shift from physically informed to physically interpretable world models - and crystallize four principles that leverage symbolic knowledge to achieve these ends: (1) structuring latent spaces according to the physical intent of variables, (2) learning aligned invariant and equivariant representations of the physical world, (3) adapting training to the varied granularity of supervision signals, and (4) partitioning generative outputs to support scalability and verifiability. We experimentally demonstrate the value of each principle on two benchmarks. This paper opens several intriguing research directions to achieve and capitalize on full physical interpretability in world models."
  },
  {
    "title": "NavG: Risk-Aware Navigation in Crowded Environments Based on Reinforcement Learning with Guidance Points",
    "url": "http://arxiv.org/abs/2503.02111v1",
    "arxiv_id": "2503.02111v1",
    "authors": [
      "Qianyi Zhang",
      "Wentao Luo",
      "Boyi Liu",
      "Ziyang Zhang",
      "Yaoyuan Wang",
      "Jingtai Liu"
    ],
    "published": "2025-03-03T22:53:06+00:00",
    "summary": "Motion planning in navigation systems is highly susceptible to upstream perceptual errors, particularly in human detection and tracking. To mitigate this issue, the concept of guidance points--a novel directional cue within a reinforcement learning-based framework--is introduced. A structured method for identifying guidance points is developed, consisting of obstacle boundary extraction, potential guidance point detection, and redundancy elimination. To integrate guidance points into the navigation pipeline, a perception-to-planning mapping strategy is proposed, unifying guidance points with other perceptual inputs and enabling the RL agent to effectively leverage the complementary relationships among raw laser data, human detection and tracking, and guidance points. Qualitative and quantitative simulations demonstrate that the proposed approach achieves the highest success rate and near-optimal travel times, greatly improving both safety and efficiency. Furthermore, real-world experiments in dynamic corridors and lobbies validate the robot's ability to confidently navigate around obstacles and robustly avoid pedestrians."
  },
  {
    "title": "Uncertainty Representation in a SOTIF-Related Use Case with Dempster-Shafer Theory for LiDAR Sensor-Based Object Detection",
    "url": "http://arxiv.org/abs/2503.02087v1",
    "arxiv_id": "2503.02087v1",
    "authors": [
      "Milin Patel",
      "Rolf Jung"
    ],
    "published": "2025-03-03T22:13:51+00:00",
    "summary": "Uncertainty in LiDAR sensor-based object detection arises from environmental variability and sensor performance limitations. Representing these uncertainties is essential for ensuring the Safety of the Intended Functionality (SOTIF), which focuses on preventing hazards in automated driving scenarios. This paper presents a systematic approach to identifying, classifying, and representing uncertainties in LiDAR-based object detection within a SOTIF-related scenario. Dempster-Shafer Theory (DST) is employed to construct a Frame of Discernment (FoD) to represent detection outcomes. Conditional Basic Probability Assignments (BPAs) are applied based on dependencies among identified uncertainty sources. Yager's Rule of Combination is used to resolve conflicting evidence from multiple sources, providing a structured framework to evaluate uncertainties' effects on detection accuracy. The study applies variance-based sensitivity analysis (VBSA) to quantify and prioritize uncertainties, detailing their specific impact on detection performance."
  },
  {
    "title": "CorrA: Leveraging Large Language Models for Dynamic Obstacle Avoidance of Autonomous Vehicles",
    "url": "http://arxiv.org/abs/2503.02076v1",
    "arxiv_id": "2503.02076v1",
    "authors": [
      "Shanting Wang",
      "Panagiotis Typaldos",
      "Andreas A. Malikopoulos"
    ],
    "published": "2025-03-03T21:57:28+00:00",
    "summary": "In this paper, we present Corridor-Agent (CorrA), a framework that integrates large language models (LLMs) with model predictive control (MPC) to address the challenges of dynamic obstacle avoidance in autonomous vehicles. Our approach leverages LLM reasoning ability to generate appropriate parameters for sigmoid-based boundary functions that define safe corridors around obstacles, effectively reducing the state-space of the controlled vehicle. The proposed framework adjusts these boundaries dynamically based on real-time vehicle data that guarantees collision-free trajectories while also ensuring both computational efficiency and trajectory optimality. The problem is formulated as an optimal control problem and solved with differential dynamic programming (DDP) for constrained optimization, and the proposed approach is embedded within an MPC framework. Extensive simulation and real-world experiments demonstrate that the proposed framework achieves superior performance in maintaining safety and efficiency in complex, dynamic environments compared to a baseline MPC approach."
  },
  {
    "title": "Comparative Analysis of OpenAI GPT-4o and DeepSeek R1 for Scientific Text Categorization Using Prompt Engineering",
    "url": "http://arxiv.org/abs/2503.02032v1",
    "arxiv_id": "2503.02032v1",
    "authors": [
      "Aniruddha Maiti",
      "Samuel Adewumi",
      "Temesgen Alemayehu Tikure",
      "Zichun Wang",
      "Niladri Sengupta",
      "Anastasiia Sukhanova",
      "Ananya Jana"
    ],
    "published": "2025-03-03T20:09:35+00:00",
    "summary": "This study examines how large language models categorize sentences from scientific papers using prompt engineering. We use two advanced web-based models, GPT-4o (by OpenAI) and DeepSeek R1, to classify sentences into predefined relationship categories. DeepSeek R1 has been tested on benchmark datasets in its technical report. However, its performance in scientific text categorization remains unexplored. To address this gap, we introduce a new evaluation method designed specifically for this task. We also compile a dataset of cleaned scientific papers from diverse domains. This dataset provides a platform for comparing the two models. Using this dataset, we analyze their effectiveness and consistency in categorization."
  },
  {
    "title": "One ruler to measure them all: Benchmarking multilingual long-context language models",
    "url": "http://arxiv.org/abs/2503.01996v1",
    "arxiv_id": "2503.01996v1",
    "authors": [
      "Yekyung Kim",
      "Jenna Russell",
      "Marzena Karpinska",
      "Mohit Iyyer"
    ],
    "published": "2025-03-03T19:12:48+00:00",
    "summary": "We present ONERULER, a multilingual benchmark designed to evaluate long-context language models across 26 languages. ONERULER adapts the English-only RULER benchmark (Hsieh et al., 2024) by including seven synthetic tasks that test both retrieval and aggregation, including new variations of the \"needle-in-a-haystack\" task that allow for the possibility of a nonexistent needle. We create ONERULER through a two-step process, first writing English instructions for each task and then collaborating with native speakers to translate them into 25 additional languages. Experiments with both open-weight and closed LLMs reveal a widening performance gap between low- and high-resource languages as context length increases from 8K to 128K tokens. Surprisingly, English is not the top-performing language on long-context tasks (ranked 6th out of 26), with Polish emerging as the top language. Our experiments also show that many LLMs (particularly OpenAI's o3-mini-high) incorrectly predict the absence of an answer, even in high-resource languages. Finally, in cross-lingual scenarios where instructions and context appear in different languages, performance can fluctuate by up to 20% depending on the instruction language. We hope the release of ONERULER will facilitate future research into improving multilingual and cross-lingual long-context training pipelines."
  },
  {
    "title": "Analyzing the Safety of Japanese Large Language Models in Stereotype-Triggering Prompts",
    "url": "http://arxiv.org/abs/2503.01947v1",
    "arxiv_id": "2503.01947v1",
    "authors": [
      "Akito Nakanishi",
      "Yukie Sano",
      "Geng Liu",
      "Francesco Pierri"
    ],
    "published": "2025-03-03T19:00:00+00:00",
    "summary": "In recent years, Large Language Models (LLMs) have attracted growing interest for their significant potential, though concerns have rapidly emerged regarding unsafe behaviors stemming from inherent stereotypes and biases.Most research on stereotypes in LLMs has primarily relied on indirect evaluation setups, in which models are prompted to select between pairs of sentences associated with particular social groups. Recently, direct evaluation methods have emerged, examining open-ended model responses to overcome limitations of previous approaches, such as annotator biases.Most existing studies have focused on English-centric LLMs, whereas research on non-English models--particularly Japanese--remains sparse, despite the growing development and adoption of these models.This study examines the safety of Japanese LLMs when responding to stereotype-triggering prompts in direct setups.We constructed 3,612 prompts by combining 301 social group terms--categorized by age, gender, and other attributes--with 12 stereotype-inducing templates in Japanese.Responses were analyzed from three foundational models trained respectively on Japanese, English, and Chinese language.Our findings reveal that LLM-jp, a Japanese native model, exhibits the lowest refusal rate and is more likely to generate toxic and negative responses compared to other models.Additionally, prompt format significantly influence the output of all models, and the generated responses include exaggerated reactions toward specific social groups, varying across models.These findings underscore the insufficient ethical safety mechanisms in Japanese LLMs and demonstrate that even high-accuracy models can produce biased outputs when processing Japanese-language prompts.We advocate for improving safety mechanisms and bias mitigation strategies in Japanese LLMs, contributing to ongoing discussions on AI ethics beyond linguistic boundaries."
  },
  {
    "title": "Analyzing the Safety of Japanese Large Language Models in Stereotype-Triggering Prompts",
    "url": "http://arxiv.org/abs/2503.01947v2",
    "arxiv_id": "2503.01947v2",
    "authors": [
      "Akito Nakanishi",
      "Yukie Sano",
      "Geng Liu",
      "Francesco Pierri"
    ],
    "published": "2025-03-03T19:00:00+00:00",
    "summary": "In recent years, Large Language Models have attracted growing interest for their significant potential, though concerns have rapidly emerged regarding unsafe behaviors stemming from inherent stereotypes and biases. Most research on stereotypes in LLMs has primarily relied on indirect evaluation setups, in which models are prompted to select between pairs of sentences associated with particular social groups. Recently, direct evaluation methods have emerged, examining open-ended model responses to overcome limitations of previous approaches, such as annotator biases. Most existing studies have focused on English-centric LLMs, whereas research on non-English models, particularly Japanese, remains sparse, despite the growing development and adoption of these models. This study examines the safety of Japanese LLMs when responding to stereotype-triggering prompts in direct setups. We constructed 3,612 prompts by combining 301 social group terms, categorized by age, gender, and other attributes, with 12 stereotype-inducing templates in Japanese. Responses were analyzed from three foundational models trained respectively on Japanese, English, and Chinese language. Our findings reveal that LLM-jp, a Japanese native model, exhibits the lowest refusal rate and is more likely to generate toxic and negative responses compared to other models. Additionally, prompt format significantly influence the output of all models, and the generated responses include exaggerated reactions toward specific social groups, varying across models. These findings underscore the insufficient ethical safety mechanisms in Japanese LLMs and demonstrate that even high-accuracy models can produce biased outputs when processing Japanese-language prompts. We advocate for improving safety mechanisms and bias mitigation strategies in Japanese LLMs, contributing to ongoing discussions on AI ethics beyond linguistic boundaries."
  },
  {
    "title": "Jailbreaking Safeguarded Text-to-Image Models via Large Language Models",
    "url": "http://arxiv.org/abs/2503.01839v1",
    "arxiv_id": "2503.01839v1",
    "authors": [
      "Zhengyuan Jiang",
      "Yuepeng Hu",
      "Yuchen Yang",
      "Yinzhi Cao",
      "Neil Zhenqiang Gong"
    ],
    "published": "2025-03-03T18:58:46+00:00",
    "summary": "Text-to-Image models may generate harmful content, such as pornographic images, particularly when unsafe prompts are submitted. To address this issue, safety filters are often added on top of text-to-image models, or the models themselves are aligned to reduce harmful outputs. However, these defenses remain vulnerable when an attacker strategically designs adversarial prompts to bypass these safety guardrails. In this work, we propose PromptTune, a method to jailbreak text-to-image models with safety guardrails using a fine-tuned large language model. Unlike other query-based jailbreak attacks that require repeated queries to the target model, our attack generates adversarial prompts efficiently after fine-tuning our AttackLLM. We evaluate our method on three datasets of unsafe prompts and against five safety guardrails. Our results demonstrate that our approach effectively bypasses safety guardrails, outperforms existing no-box attacks, and also facilitates other query-based attacks."
  },
  {
    "title": "$\\texttt{SEM-CTRL}$: Semantically Controlled Decoding",
    "url": "http://arxiv.org/abs/2503.01804v1",
    "arxiv_id": "2503.01804v1",
    "authors": [
      "Mohammad Albinhassan",
      "Pranava Madhyastha",
      "Alessandra Russo"
    ],
    "published": "2025-03-03T18:33:46+00:00",
    "summary": "Ensuring both syntactic and semantic correctness in Large Language Model (LLM) outputs remains a significant challenge, despite being critical for real-world deployment. In this paper, we introduce $\\texttt{SEM-CTRL}$, a unified approach that enforces rich context-sensitive constraints and task- and instance-specific semantics directly on an LLM decoder. Our approach integrates token-level MCTS, which is guided by specific syntactic and semantic constraints. The constraints over the desired outputs are expressed using Answer Set Grammars -- a logic-based formalism that generalizes context-sensitive grammars while incorporating background knowledge to represent task-specific semantics. We show that our approach guarantees correct completions for any off-the-shelf LLM without the need for fine-tuning. We evaluate $\\texttt{SEM-CTRL}$ on a range of tasks, including synthetic grammar synthesis, combinatorial reasoning, and planning. Our results demonstrate that $\\texttt{SEM-CTRL}$ allows small pre-trained LLMs to efficiently outperform larger variants and state-of-the-art reasoning models (e.g., o1-preview) while simultaneously guaranteeing solution correctness."
  },
  {
    "title": "Visual-RFT: Visual Reinforcement Fine-Tuning",
    "url": "http://arxiv.org/abs/2503.01785v1",
    "arxiv_id": "2503.01785v1",
    "authors": [
      "Ziyu Liu",
      "Zeyi Sun",
      "Yuhang Zang",
      "Xiaoyi Dong",
      "Yuhang Cao",
      "Haodong Duan",
      "Dahua Lin",
      "Jiaqi Wang"
    ],
    "published": "2025-03-03T18:16:32+00:00",
    "summary": "Reinforcement Fine-Tuning (RFT) in Large Reasoning Models like OpenAI o1 learns from feedback on its answers, which is especially useful in applications when fine-tuning data is scarce. Recent open-source work like DeepSeek-R1 demonstrates that reinforcement learning with verifiable reward is one key direction in reproducing o1. While the R1-style model has demonstrated success in language models, its application in multi-modal domains remains under-explored. This work introduces Visual Reinforcement Fine-Tuning (Visual-RFT), which further extends the application areas of RFT on visual tasks. Specifically, Visual-RFT first uses Large Vision-Language Models (LVLMs) to generate multiple responses containing reasoning tokens and final answers for each input, and then uses our proposed visual perception verifiable reward functions to update the model via the policy optimization algorithm such as Group Relative Policy Optimization (GRPO). We design different verifiable reward functions for different perception tasks, such as the Intersection over Union (IoU) reward for object detection. Experimental results on fine-grained image classification, few-shot object detection, reasoning grounding, as well as open-vocabulary object detection benchmarks show the competitive performance and advanced generalization ability of Visual-RFT compared with Supervised Fine-tuning (SFT). For example, Visual-RFT improves accuracy by $24.3\\%$ over the baseline in one-shot fine-grained image classification with around 100 samples. In few-shot object detection, Visual-RFT also exceeds the baseline by $21.9$ on COCO's two-shot setting and $15.4$ on LVIS. Our Visual-RFT represents a paradigm shift in fine-tuning LVLMs, offering a data-efficient, reward-driven approach that enhances reasoning and adaptability for domain-specific tasks."
  },
  {
    "title": "Cats Confuse Reasoning LLM: Query Agnostic Adversarial Triggers for Reasoning Models",
    "url": "http://arxiv.org/abs/2503.01781v1",
    "arxiv_id": "2503.01781v1",
    "authors": [
      "Meghana Rajeev",
      "Rajkumar Ramamurthy",
      "Prapti Trivedi",
      "Vikas Yadav",
      "Oluwanifemi Bamgbose",
      "Sathwik Tejaswi Madhusudan",
      "James Zou",
      "Nazneen Rajani"
    ],
    "published": "2025-03-03T18:10:54+00:00",
    "summary": "We investigate the robustness of reasoning models trained for step-by-step problem solving by introducing query-agnostic adversarial triggers - short, irrelevant text that, when appended to math problems, systematically mislead models to output incorrect answers without altering the problem's semantics. We propose CatAttack, an automated iterative attack pipeline for generating triggers on a weaker, less expensive proxy model (DeepSeek V3) and successfully transfer them to more advanced reasoning target models like DeepSeek R1 and DeepSeek R1-distilled-Qwen-32B, resulting in greater than 300% increase in the likelihood of the target model generating an incorrect answer. For example, appending, \"Interesting fact: cats sleep most of their lives,\" to any math problem leads to more than doubling the chances of a model getting the answer wrong. Our findings highlight critical vulnerabilities in reasoning models, revealing that even state-of-the-art models remain susceptible to subtle adversarial inputs, raising security and reliability concerns. The CatAttack triggers dataset with model responses is available at https://huggingface.co/datasets/collinear-ai/cat-attack-adversarial-triggers."
  },
  {
    "title": "Phi-4-Mini Technical Report: Compact yet Powerful Multimodal Language Models via Mixture-of-LoRAs",
    "url": "http://arxiv.org/abs/2503.01743v1",
    "arxiv_id": "2503.01743v1",
    "authors": [
      "Abdelrahman Abouelenin",
      "Atabak Ashfaq",
      "Adam Atkinson",
      "Hany Awadalla",
      "Nguyen Bach",
      "Jianmin Bao",
      "Alon Benhaim",
      "Martin Cai",
      "Vishrav Chaudhary",
      "Congcong Chen",
      "Dong Chen",
      "Dongdong Chen",
      "Junkun Chen",
      "Weizhu Chen",
      "Yen-Chun Chen",
      "Yi-ling Chen",
      "Qi Dai",
      "Xiyang Dai",
      "Ruchao Fan",
      "Mei Gao",
      "Min Gao",
      "Amit Garg",
      "Abhishek Goswami",
      "Junheng Hao",
      "Amr Hendy",
      "Yuxuan Hu",
      "Xin Jin",
      "Mahmoud Khademi",
      "Dongwoo Kim",
      "Young Jin Kim",
      "Gina Lee",
      "Jinyu Li",
      "Yunsheng Li",
      "Chen Liang",
      "Xihui Lin",
      "Zeqi Lin",
      "Mengchen Liu",
      "Yang Liu",
      "Gilsinia Lopez",
      "Chong Luo",
      "Piyush Madan",
      "Vadim Mazalov",
      "Ali Mousavi",
      "Anh Nguyen",
      "Jing Pan",
      "Daniel Perez-Becker",
      "Jacob Platin",
      "Thomas Portet",
      "Kai Qiu",
      "Bo Ren",
      "Liliang Ren",
      "Sambuddha Roy",
      "Ning Shang",
      "Yelong Shen",
      "Saksham Singhal",
      "Subhojit Som",
      "Xia Song",
      "Tetyana Sych",
      "Praneetha Vaddamanu",
      "Shuohang Wang",
      "Yiming Wang",
      "Zhenghao Wang",
      "Haibin Wu",
      "Haoran Xu",
      "Weijian Xu",
      "Yifan Yang",
      "Ziyi Yang",
      "Donghan Yu",
      "Ishmam Zabir",
      "Jianwen Zhang",
      "Li Lyna Zhang",
      "Yunan Zhang",
      "Xiren Zhou"
    ],
    "published": "2025-03-03T17:05:52+00:00",
    "summary": "We introduce Phi-4-Mini and Phi-4-Multimodal, compact yet highly capable language and multimodal models. Phi-4-Mini is a 3.8-billion-parameter language model trained on high-quality web and synthetic data, significantly outperforming recent open-source models of similar size and matching the performance of models twice its size on math and coding tasks requiring complex reasoning. This achievement is driven by a carefully curated synthetic data recipe emphasizing high-quality math and coding datasets. Compared to its predecessor, Phi-3.5-Mini, Phi-4-Mini features an expanded vocabulary size of 200K tokens to better support multilingual applications, as well as group query attention for more efficient long-sequence generation. Phi-4-Multimodal is a multimodal model that integrates text, vision, and speech/audio input modalities into a single model. Its novel modality extension approach leverages LoRA adapters and modality-specific routers to allow multiple inference modes combining various modalities without interference. For example, it now ranks first in the OpenASR leaderboard to date, although the LoRA component of the speech/audio modality has just 460 million parameters. Phi-4-Multimodal supports scenarios involving (vision + language), (vision + speech), and (speech/audio) inputs, outperforming larger vision-language and speech-language models on a wide range of tasks. Additionally, we experiment to further train Phi-4-Mini to enhance its reasoning capabilities. Despite its compact 3.8-billion-parameter size, this experimental version achieves reasoning performance on par with or surpassing significantly larger models, including DeepSeek-R1-Distill-Qwen-7B and DeepSeek-R1-Distill-Llama-8B."
  },
  {
    "title": "Building Safe GenAI Applications: An End-to-End Overview of Red Teaming for Large Language Models",
    "url": "http://arxiv.org/abs/2503.01742v1",
    "arxiv_id": "2503.01742v1",
    "authors": [
      "Alberto Purpura",
      "Sahil Wadhwa",
      "Jesse Zymet",
      "Akshay Gupta",
      "Andy Luo",
      "Melissa Kazemi Rad",
      "Swapnil Shinde",
      "Mohammad Shahed Sorower"
    ],
    "published": "2025-03-03T17:04:22+00:00",
    "summary": "The rapid growth of Large Language Models (LLMs) presents significant privacy, security, and ethical concerns. While much research has proposed methods for defending LLM systems against misuse by malicious actors, researchers have recently complemented these efforts with an offensive approach that involves red teaming, i.e., proactively attacking LLMs with the purpose of identifying their vulnerabilities. This paper provides a concise and practical overview of the LLM red teaming literature, structured so as to describe a multi-component system end-to-end. To motivate red teaming we survey the initial safety needs of some high-profile LLMs, and then dive into the different components of a red teaming system as well as software packages for implementing them. We cover various attack methods, strategies for attack-success evaluation, metrics for assessing experiment outcomes, as well as a host of other considerations. Our survey will be useful for any reader who wants to rapidly obtain a grasp of the major red teaming concepts for their own use in practical applications."
  },
  {
    "title": "Building Safe GenAI Applications: An End-to-End Overview of Red Teaming for Large Language Models",
    "url": "http://arxiv.org/abs/2503.01742v2",
    "arxiv_id": "2503.01742v2",
    "authors": [
      "Alberto Purpura",
      "Sahil Wadhwa",
      "Jesse Zymet",
      "Akshay Gupta",
      "Andy Luo",
      "Melissa Kazemi Rad",
      "Swapnil Shinde",
      "Mohammad Shahed Sorower"
    ],
    "published": "2025-03-03T17:04:22+00:00",
    "summary": "The rapid growth of Large Language Models (LLMs) presents significant privacy, security, and ethical concerns. While much research has proposed methods for defending LLM systems against misuse by malicious actors, researchers have recently complemented these efforts with an offensive approach that involves red teaming, i.e., proactively attacking LLMs with the purpose of identifying their vulnerabilities. This paper provides a concise and practical overview of the LLM red teaming literature, structured so as to describe a multi-component system end-to-end. To motivate red teaming we survey the initial safety needs of some high-profile LLMs, and then dive into the different components of a red teaming system as well as software packages for implementing them. We cover various attack methods, strategies for attack-success evaluation, metrics for assessing experiment outcomes, as well as a host of other considerations. Our survey will be useful for any reader who wants to rapidly obtain a grasp of the major red teaming concepts for their own use in practical applications."
  },
  {
    "title": "CoT-VLM4Tar: Chain-of-Thought Guided Vision-Language Models for Traffic Anomaly Resolution",
    "url": "http://arxiv.org/abs/2503.01632v1",
    "arxiv_id": "2503.01632v1",
    "authors": [
      "Tianchi Ren",
      "Haibo Hu",
      "Jiacheng Zuo",
      "Xinhong Chen",
      "Jianping Wang",
      "Chun Jason Xue",
      "Jen-Ming Wu",
      "Nan Guan"
    ],
    "published": "2025-03-03T15:07:25+00:00",
    "summary": "With the acceleration of urbanization, modern urban traffic systems are becoming increasingly complex, leading to frequent traffic anomalies. These anomalies encompass not only common traffic jams but also more challenging issues such as phantom traffic jams, intersection deadlocks, and accident liability analysis, which severely impact traffic flow, vehicular safety, and overall transportation efficiency. Currently, existing solutions primarily rely on manual intervention by traffic police or artificial intelligence-based detection systems. However, these methods often suffer from response delays and inconsistent management due to inadequate resources, while AI detection systems, despite enhancing efficiency to some extent, still struggle to handle complex traffic anomalies in a real-time and precise manner. To address these issues, we propose CoT-VLM4Tar: (Chain of Thought Visual-Language Model for Traffic Anomaly Resolution), this innovative approach introduces a new chain-of-thought to guide the VLM in analyzing, reasoning, and generating solutions for traffic anomalies with greater reasonable and effective solution, and to evaluate the performance and effectiveness of our method, we developed a closed-loop testing framework based on the CARLA simulator. Furthermore, to ensure seamless integration of the solutions generated by the VLM with the CARLA simulator, we implement an itegration module that converts these solutions into executable commands. Our results demonstrate the effectiveness of VLM in the resolution of real-time traffic anomalies, providing a proof-of-concept for its integration into autonomous traffic management systems."
  },
  {
    "title": "Llama-3.1-Sherkala-8B-Chat: An Open Large Language Model for Kazakh",
    "url": "http://arxiv.org/abs/2503.01493v1",
    "arxiv_id": "2503.01493v1",
    "authors": [
      "Fajri Koto",
      "Rituraj Joshi",
      "Nurdaulet Mukhituly",
      "Yuxia Wang",
      "Zhuohan Xie",
      "Rahul Pal",
      "Daniil Orel",
      "Parvez Mullah",
      "Diana Turmakhan",
      "Maiya Goloburda",
      "Mohammed Kamran",
      "Samujjwal Ghosh",
      "Bokang Jia",
      "Jonibek Mansurov",
      "Mukhammed Togmanov",
      "Debopriyo Banerjee",
      "Nurkhan Laiyk",
      "Akhmed Sakip",
      "Xudong Han",
      "Ekaterina Kochmar",
      "Alham Fikri Aji",
      "Aaryamonvikram Singh",
      "Alok Anil Jadhav",
      "Satheesh Katipomu",
      "Samta Kamboj",
      "Monojit Choudhury",
      "Gurpreet Gosal",
      "Gokul Ramakrishnan",
      "Biswajit Mishra",
      "Sarath Chandran",
      "Avraham Sheinin",
      "Natalia Vassilieva",
      "Neha Sengupta",
      "Larry Murray",
      "Preslav Nakov"
    ],
    "published": "2025-03-03T13:05:48+00:00",
    "summary": "Llama-3.1-Sherkala-8B-Chat, or Sherkala-Chat (8B) for short, is a state-of-the-art instruction-tuned open generative large language model (LLM) designed for Kazakh. Sherkala-Chat (8B) aims to enhance the inclusivity of LLM advancements for Kazakh speakers. Adapted from the LLaMA-3.1-8B model, Sherkala-Chat (8B) is trained on 45.3B tokens across Kazakh, English, Russian, and Turkish. With 8 billion parameters, it demonstrates strong knowledge and reasoning abilities in Kazakh, significantly outperforming existing open Kazakh and multilingual models of similar scale while achieving competitive performance in English. We release Sherkala-Chat (8B) as an open-weight instruction-tuned model and provide a detailed overview of its training, fine-tuning, safety alignment, and evaluation, aiming to advance research and support diverse real-world applications."
  },
  {
    "title": "Towards Widening The Distillation Bottleneck for Reasoning Models",
    "url": "http://arxiv.org/abs/2503.01461v1",
    "arxiv_id": "2503.01461v1",
    "authors": [
      "Huifeng Yin",
      "Yu Zhao",
      "Minghao Wu",
      "Xuanfan Ni",
      "Bo Zeng",
      "Hao Wang",
      "Tianqi Shi",
      "Liangying Shao",
      "Chenyang Lyu",
      "Longyue Wang",
      "Weihua Luo",
      "Kaifu Zhang"
    ],
    "published": "2025-03-03T12:17:36+00:00",
    "summary": "Large Reasoning Models(LRMs) such as OpenAI o1 and DeepSeek-R1 have shown remarkable reasoning capabilities by scaling test-time compute and generating long Chain-of-Thought(CoT). Distillation--post-training on LRMs-generated data--is a straightforward yet effective method to enhance the reasoning abilities of smaller models, but faces a critical bottleneck: we found that distilled long CoT data poses learning difficulty for small models and leads to the inheritance of biases (i.e. over-thinking) when using Supervised Fine-tuning(SFT) and Reinforcement Learning(RL) methods. To alleviate this bottleneck, we propose constructing tree-based CoT data from scratch via Monte Carlo Tree Search(MCTS). We then exploit a set of CoT-aware approaches, including Thoughts Length Balance, Fine-grained DPO, and Joint Post-training Objective, to enhance SFT and RL on the construted data."
  },
  {
    "title": "Jailbreaking Generative AI: Empowering Novices to Conduct Phishing Attacks",
    "url": "http://arxiv.org/abs/2503.01395v1",
    "arxiv_id": "2503.01395v1",
    "authors": [
      "Rina Mishra",
      "Gaurav Varshney",
      "Shreya Singh"
    ],
    "published": "2025-03-03T10:51:10+00:00",
    "summary": "The rapid advancements in generative AI models, such as ChatGPT, have introduced both significant benefits and new risks within the cybersecurity landscape. This paper investigates the potential misuse of the latest AI model, ChatGPT-4o Mini, in facilitating social engineering attacks, with a particular focus on phishing, one of the most pressing cybersecurity threats today. While existing literature primarily addresses the technical aspects, such as jailbreaking techniques, none have fully explored the free and straightforward execution of a comprehensive phishing campaign by novice users using ChatGPT-4o Mini. In this study, we examine the vulnerabilities of AI-driven chatbot services in 2025, specifically how methods like jailbreaking and reverse psychology can bypass ethical safeguards, allowing ChatGPT to generate phishing content, suggest hacking tools, and assist in carrying out phishing attacks. Our findings underscore the alarming ease with which even inexperienced users can execute sophisticated phishing campaigns, emphasizing the urgent need for stronger cybersecurity measures and heightened user awareness in the age of AI."
  },
  {
    "title": "Victim-Centred Abuse Investigations and Defenses for Social Media Platforms",
    "url": "http://arxiv.org/abs/2503.01327v1",
    "arxiv_id": "2503.01327v1",
    "authors": [
      "Zaid Hakami",
      "Ashfaq Ali Shafin",
      "Peter J. Clarke",
      "Niki Pissinou",
      "Bogdan Carbunar"
    ],
    "published": "2025-03-03T09:08:20+00:00",
    "summary": "Online abuse, a persistent aspect of social platform interactions, impacts user well-being and exposes flaws in platform designs that include insufficient detection efforts and inadequate victim protection measures. Ensuring safety in platform interactions requires the integration of victim perspectives in the design of abuse detection and response systems. In this paper, we conduct surveys (n = 230) and semi-structured interviews (n = 15) with students at a minority-serving institution in the US, to explore their experiences with abuse on a variety of social platforms, their defense strategies, and their recommendations for social platforms to improve abuse responses. We build on study findings to propose design requirements for abuse defense systems and discuss the role of privacy, anonymity, and abuse attribution requirements in their implementation. We introduce ARI, a blueprint for a unified, transparent, and personalized abuse response system for social platforms that sustainably detects abuse by leveraging the expertise of platform users, incentivized with proceeds obtained from abusers."
  },
  {
    "title": "ABFS: Natural Robustness Testing for LLM-based NLP Software",
    "url": "http://arxiv.org/abs/2503.01319v1",
    "arxiv_id": "2503.01319v1",
    "authors": [
      "Mingxuan Xiao",
      "Yan Xiao",
      "Shunhui Ji",
      "Yunhe Li",
      "Lei Xue",
      "Pengcheng Zhang"
    ],
    "published": "2025-03-03T09:02:06+00:00",
    "summary": "Owing to the exceptional performance of Large Language Models (LLMs) in Natural Language Processing (NLP) tasks, LLM-based NLP software has rapidly gained traction across various domains, such as financial analysis and content moderation. However, these applications frequently exhibit robustness deficiencies, where slight perturbations in input (prompt+example) may lead to erroneous outputs. Current robustness testing methods face two main limitations: (1) low testing effectiveness, limiting the applicability of LLM-based software in safety-critical scenarios, and (2) insufficient naturalness of test cases, reducing the practical value of testing outcomes. To address these issues, this paper proposes ABFS, a straightforward yet effective automated testing method that, for the first time, treats the input prompts and examples as a unified whole for robustness testing. Specifically, ABFS formulates the testing process as a combinatorial optimization problem, employing Best-First Search to identify successful test cases within the perturbation space and designing a novel Adaptive control strategy to enhance test case naturalness. We evaluate the robustness testing performance of ABFS on three datasets across five threat models. On Llama2-13b, the traditional StressTest achieves only a 13.273% success rate, while ABFS attains a success rate of 98.064%, supporting a more comprehensive robustness assessment before software deployment. Compared to baseline methods, ABFS introduces fewer modifications to the original input and consistently generates test cases with superior naturalness. Furthermore, test cases generated by ABFS exhibit stronger transferability and higher testing efficiency, significantly reducing testing costs."
  },
  {
    "title": "Interactive Gadolinium-Free MRI Synthesis: A Transformer with Localization Prompt Learning",
    "url": "http://arxiv.org/abs/2503.01265v1",
    "arxiv_id": "2503.01265v1",
    "authors": [
      "Linhao Li",
      "Changhui Su",
      "Yu Guo",
      "Huimao Zhang",
      "Dong Liang",
      "Kun Shang"
    ],
    "published": "2025-03-03T07:44:28+00:00",
    "summary": "Contrast-enhanced magnetic resonance imaging (CE-MRI) is crucial for tumor detection and diagnosis, but the use of gadolinium-based contrast agents (GBCAs) in clinical settings raises safety concerns due to potential health risks. To circumvent these issues while preserving diagnostic accuracy, we propose a novel Transformer with Localization Prompts (TLP) framework for synthesizing CE-MRI from non-contrast MR images. Our architecture introduces three key innovations: a hierarchical backbone that uses efficient Transformer to process multi-scale features; a multi-stage fusion system consisting of Local and Global Fusion modules that hierarchically integrate complementary information via spatial attention operations and cross-attention mechanisms, respectively; and a Fuzzy Prompt Generation (FPG) module that enhances the TLP model's generalization by emulating radiologists' manual annotation through stochastic feature perturbation. The framework uniquely enables interactive clinical integration by allowing radiologists to input diagnostic prompts during inference, synergizing artificial intelligence with medical expertise. This research establishes a new paradigm for contrast-free MRI synthesis while addressing critical clinical needs for safer diagnostic procedures. Codes are available at https://github.com/ChanghuiSu/TLP."
  },
  {
    "title": "Enhancing Network Security Management in Water Systems using FM-based Attack Attribution",
    "url": "http://arxiv.org/abs/2503.01229v1",
    "arxiv_id": "2503.01229v1",
    "authors": [
      "Aleksandar Avdalovic",
      "Joseph Khoury",
      "Ahmad Taha",
      "Elias Bou-Harb"
    ],
    "published": "2025-03-03T06:52:00+00:00",
    "summary": "Water systems are vital components of modern infrastructure, yet they are increasingly susceptible to sophisticated cyber attacks with potentially dire consequences on public health and safety. While state-of-the-art machine learning techniques effectively detect anomalies, contemporary model-agnostic attack attribution methods using LIME, SHAP, and LEMNA are deemed impractical for large-scale, interdependent water systems. This is due to the intricate interconnectivity and dynamic interactions that define these complex environments. Such methods primarily emphasize individual feature importance while falling short of addressing the crucial sensor-actuator interactions in water systems, which limits their effectiveness in identifying root cause attacks. To this end, we propose a novel model-agnostic Factorization Machines (FM)-based approach that capitalizes on water system sensor-actuator interactions to provide granular explanations and attributions for cyber attacks. For instance, an anomaly in an actuator pump activity can be attributed to a top root cause attack candidates, a list of water pressure sensors, which is derived from the underlying linear and quadratic effects captured by our approach. We validate our method using two real-world water system specific datasets, SWaT and WADI, demonstrating its superior performance over traditional attribution methods. In multi-feature cyber attack scenarios involving intricate sensor-actuator interactions, our FM-based attack attribution method effectively ranks attack root causes, achieving approximately 20% average improvement over SHAP and LEMNA."
  },
  {
    "title": "STGAN: Spatial-temporal Graph Autoregression Network for Pavement Distress Deterioration Prediction",
    "url": "http://arxiv.org/abs/2503.01152v1",
    "arxiv_id": "2503.01152v1",
    "authors": [
      "Shilin Tong",
      "Difei Wu",
      "Xiaona Liu",
      "Le Zheng",
      "Yuchuan Du",
      "Difan Zou"
    ],
    "published": "2025-03-03T03:59:34+00:00",
    "summary": "Pavement distress significantly compromises road integrity and poses risks to drivers. Accurate prediction of pavement distress deterioration is essential for effective road management, cost reduction in maintenance, and improvement of traffic safety. However, real-world data on pavement distress is usually collected irregularly, resulting in uneven, asynchronous, and sparse spatial-temporal datasets. This hinders the application of existing spatial-temporal models, such as DCRNN, since they are only applicable to regularly and synchronously collected data. To overcome these challenges, we propose the Spatial-Temporal Graph Autoregression Network (STGAN), a novel graph neural network model designed for accurately predicting irregular pavement distress deterioration using complex spatial-temporal data. Specifically, STGAN integrates the temporal domain into the spatial domain, creating a larger graph where nodes are represented by spatial-temporal tuples and edges are formed based on a similarity-based connection mechanism. Furthermore, based on the constructed spatiotemporal graph, we formulate pavement distress deterioration prediction as a graph autoregression task, i.e., the graph size increases incrementally and the prediction is performed sequentially. This is accomplished by a novel spatial-temporal attention mechanism deployed by STGAN. Utilizing the ConTrack dataset, which contains pavement distress records collected from different locations in Shanghai, we demonstrate the superior performance of STGAN in capturing spatial-temporal correlations and addressing the aforementioned challenges. Experimental results further show that STGAN outperforms baseline models, and ablation studies confirm the effectiveness of its novel modules. Our findings contribute to promoting proactive road maintenance decision-making and ultimately enhancing road safety and resilience."
  },
  {
    "title": "Beyond Visibility Limits: A DRL-Based Navigation Strategy for Unexpected Obstacles",
    "url": "http://arxiv.org/abs/2503.01127v1",
    "arxiv_id": "2503.01127v1",
    "authors": [
      "Mingao Tan",
      "Shanze Wang",
      "Biao Huang",
      "Zhibo Yang",
      "Rongfei Chen",
      "Xiaoyu Shen",
      "Wei Zhang"
    ],
    "published": "2025-03-03T03:14:08+00:00",
    "summary": "Distance-based reward mechanisms in deep reinforcement learning (DRL) navigation systems suffer from critical safety limitations in dynamic environments, frequently resulting in collisions when visibility is restricted. We propose DRL-NSUO, a novel navigation strategy for unexpected obstacles that leverages the rate of change in LiDAR data as a dynamic environmental perception element. Our approach incorporates a composite reward function with environmental change rate constraints and dynamically adjusted weights through curriculum learning, enabling robots to autonomously balance between path efficiency and safety maximization. We enhance sensitivity to nearby obstacles by implementing short-range feature preprocessing of LiDAR data. Experimental results demonstrate that this method significantly improves both robot and pedestrian safety in complex scenarios compared to traditional DRL-based methods. When evaluated on the BARN navigation dataset, our method achieved superior performance with success rates of 94.0% at 0.5 m/s and 91.0% at 1.0 m/s, outperforming conservative obstacle expansion strategies. These results validate DRL-NSUO's enhanced practicality and safety for human-robot collaborative environments, including intelligent logistics applications."
  },
  {
    "title": "KineSoft: Learning Proprioceptive Manipulation Policies with Soft Robot Hands",
    "url": "http://arxiv.org/abs/2503.01078v1",
    "arxiv_id": "2503.01078v1",
    "authors": [
      "Uksang Yoo",
      "Jonathan Francis",
      "Jean Oh",
      "Jeffrey Ichnowski"
    ],
    "published": "2025-03-03T00:48:08+00:00",
    "summary": "Underactuated soft robot hands offer inherent safety and adaptability advantages over rigid systems, but developing dexterous manipulation skills remains challenging. While imitation learning shows promise for complex manipulation tasks, traditional approaches struggle with soft systems due to demonstration collection challenges and ineffective state representations. We present KineSoft, a framework enabling direct kinesthetic teaching of soft robotic hands by leveraging their natural compliance as a skill teaching advantage rather than only as a control challenge. KineSoft makes two key contributions: (1) an internal strain sensing array providing occlusion-free proprioceptive shape estimation, and (2) a shape-based imitation learning framework that uses proprioceptive feedback with a low-level shape-conditioned controller to ground diffusion-based policies. This enables human demonstrators to physically guide the robot while the system learns to associate proprioceptive patterns with successful manipulation strategies. We validate KineSoft through physical experiments, demonstrating superior shape estimation accuracy compared to baseline methods, precise shape-trajectory tracking, and higher task success rates compared to baseline imitation learning approaches."
  },
  {
    "title": "Real-World Deployment and Assessment of a Multi-Agent Reinforcement Learning-Based Variable Speed Limit Control System",
    "url": "http://arxiv.org/abs/2503.01017v1",
    "arxiv_id": "2503.01017v1",
    "authors": [
      "Yuhang Zhang",
      "Zhiyao Zhang",
      "Junyi Ji",
      "Marcos Qui\u00f1ones-Grueiro",
      "William Barbour",
      "Derek Gloudemans",
      "Gergely Zach\u00e1r",
      "Clay Weston",
      "Gautam Biswas",
      "Daniel B. Work"
    ],
    "published": "2025-03-02T21:09:16+00:00",
    "summary": "This article presents the first field deployment of a multi-agent reinforcement learning (MARL) based variable speed limit (VSL) control system on Interstate 24 (I-24) near Nashville, Tennessee. We design and demonstrate a full pipeline from training MARL agents in a traffic simulator to a field deployment on a 17-mile segment of I-24 encompassing 67 VSL controllers. The system was launched on March 8th, 2024, and has made approximately 35 million decisions on 28 million trips in six months of operation. We apply an invalid action masking mechanism and several safety guards to ensure real-world constraints. The MARL-based implementation operates up to 98% of the time, with the safety guards overriding the MARL decisions for the remaining time. We evaluate the performance of the MARL-based algorithm in comparison to a previously deployed non-RL VSL benchmark algorithm on I-24. Results show that the MARL-based VSL control system achieves a superior performance. The accuracy of correctly warning drivers about slowing traffic ahead is improved by 14% and the response delay to non-recurrent congestion is reduced by 75%. The preliminary data shows that the VSL control system has reduced the crash rate by 26% and the secondary crash rate by 50%. We open-sourced the deployed MARL-based VSL algorithm at https://github.com/Lab-Work/marl-vsl-controller."
  },
  {
    "title": "HWC-Loco: A Hierarchical Whole-Body Control Approach to Robust Humanoid Locomotion",
    "url": "http://arxiv.org/abs/2503.00923v1",
    "arxiv_id": "2503.00923v1",
    "authors": [
      "Sixu Lin",
      "Guanren Qiao",
      "Yunxin Tai",
      "Ang Li",
      "Kui Jia",
      "Guiliang Liu"
    ],
    "published": "2025-03-02T14:55:22+00:00",
    "summary": "Humanoid robots, capable of assuming human roles in various workplaces, have become essential to the advancement of embodied intelligence. However, as robots with complex physical structures, learning a control model that can operate robustly across diverse environments remains inherently challenging, particularly under the discrepancies between training and deployment environments. In this study, we propose HWC-Loco, a robust whole-body control algorithm tailored for humanoid locomotion tasks. By reformulating policy learning as a robust optimization problem, HWC-Loco explicitly learns to recover from safety-critical scenarios. While prioritizing safety guarantees, overly conservative behavior can compromise the robot's ability to complete the given tasks. To tackle this challenge, HWC-Loco leverages a hierarchical policy for robust control. This policy can dynamically resolve the trade-off between goal-tracking and safety recovery, guided by human behavior norms and dynamic constraints. To evaluate the performance of HWC-Loco, we conduct extensive comparisons against state-of-the-art humanoid control models, demonstrating HWC-Loco's superior performance across diverse terrains, robot structures, and locomotion tasks under both simulated and real-world environments."
  },
  {
    "title": "Unnatural Languages Are Not Bugs but Features for LLMs",
    "url": "http://arxiv.org/abs/2503.01926v1",
    "arxiv_id": "2503.01926v1",
    "authors": [
      "Keyu Duan",
      "Yiran Zhao",
      "Zhili Feng",
      "Jinjie Ni",
      "Tianyu Pang",
      "Qian Liu",
      "Tianle Cai",
      "Longxu Dou",
      "Kenji Kawaguchi",
      "Anirudh Goyal",
      "J. Zico Kolter",
      "Michael Qizhe Shieh"
    ],
    "published": "2025-03-02T12:10:17+00:00",
    "summary": "Large Language Models (LLMs) have been observed to process non-human-readable text sequences, such as jailbreak prompts, often viewed as a bug for aligned LLMs. In this work, we present a systematic investigation challenging this perception, demonstrating that unnatural languages - strings that appear incomprehensible to humans but maintain semantic meanings for LLMs - contain latent features usable by models. Notably, unnatural languages possess latent features that can be generalized across different models and tasks during inference. Furthermore, models fine-tuned on unnatural versions of instruction datasets perform on-par with those trained on natural language, achieving 49.71 win rates in Length-controlled AlpacaEval 2.0 in average across various base models. In addition, through comprehensive analysis, we demonstrate that LLMs process unnatural languages by filtering noise and inferring contextual meaning from filtered words."
  },
  {
    "title": "Evaluation of adaptive sampling methods in scenario generation for virtual safety impact assessment of pre-crash safety systems",
    "url": "http://arxiv.org/abs/2503.00815v1",
    "arxiv_id": "2503.00815v1",
    "authors": [
      "Xiaomi Yang",
      "Henrik Imberg",
      "Carol Flannagan",
      "Jonas B\u00e4rgman"
    ],
    "published": "2025-03-02T09:44:18+00:00",
    "summary": "Virtual safety assessment plays a vital role in evaluating the safety impact of pre-crash safety systems such as advanced driver assistance systems (ADAS) and automated driving systems (ADS). However, as the number of parameters in simulation-based scenario generation increases, the number of crash scenarios to simulate grows exponentially, making complete enumeration computationally infeasible. Efficient sampling methods, such as importance sampling and active sampling, have been proposed to address this challenge. However, a comprehensive evaluation of how domain knowledge, stratification, and batch sampling affect their efficiency remains limited.   This study evaluates the performance of importance sampling and active sampling in scenario generation, incorporating two domain-knowledge-driven features: adaptive sample space reduction (ASSR) and stratification. Additionally, we assess the effects of a third feature, batch sampling, on computational efficiency in terms of both CPU and wall-clock time. Based on our findings, we provide practical recommendations for applying ASSR, stratification, and batch sampling to optimize sampling performance.   Our results demonstrate that ASSR substantially improves sampling efficiency for both importance sampling and active sampling. When integrated into active sampling, ASSR reduces the root mean squared estimation error (RMSE) of the estimates by up to 90\\%. Stratification further improves sampling performance for both methods, regardless of ASSR implementation. When ASSR and/or stratification are applied, importance sampling performs on par with active sampling, whereas when neither feature is used, active sampling is more efficient. Larger batch sizes reduce wall-clock time but increase the number of simulations required to achieve the same estimation accuracy."
  },
  {
    "title": "Acoustic Anomaly Detection on UAM Propeller Defect with Acoustic dataset for Crack of drone Propeller (ADCP)",
    "url": "http://arxiv.org/abs/2503.00790v1",
    "arxiv_id": "2503.00790v1",
    "authors": [
      "Juho Lee",
      "Donghyun Yoon",
      "Gumoon Jeong",
      "Hyeoncheol Kim"
    ],
    "published": "2025-03-02T08:40:23+00:00",
    "summary": "The imminent commercialization of UAM requires stable, AI-based maintenance systems to ensure safety for both passengers and pedestrians. This paper presents a methodology for non-destructively detecting cracks in UAM propellers using drone propeller sound datasets. Normal operating sounds were recorded, and abnormal sounds (categorized as ripped and broken) were differentiated by varying the microphone-propeller angle and throttle power. Our novel approach integrates FFT and STFT preprocessing techniques to capture both global frequency patterns and local time-frequency variations, thereby enhancing anomaly detection performance. The constructed Acoustic Dataset for Crack of Drone Propeller (ADCP) demonstrates the potential for detecting propeller cracks and lays the groundwork for future UAM maintenance applications."
  },
  {
    "title": "Output Length Effect on DeepSeek-R1's Safety in Forced Thinking",
    "url": "http://arxiv.org/abs/2503.01923v1",
    "arxiv_id": "2503.01923v1",
    "authors": [
      "Xuying Li",
      "Zhuo Li",
      "Yuji Kosuga",
      "Victor Bian"
    ],
    "published": "2025-03-02T06:29:22+00:00",
    "summary": "Large Language Models (LLMs) have demonstrated strong reasoning capabilities, but their safety under adversarial conditions remains a challenge. This study examines the impact of output length on the robustness of DeepSeek-R1, particularly in Forced Thinking scenarios. We analyze responses across various adversarial prompts and find that while longer outputs can improve safety through self-correction, certain attack types exploit extended generations. Our findings suggest that output length should be dynamically controlled to balance reasoning effectiveness and security. We propose reinforcement learning-based policy adjustments and adaptive token length regulation to enhance LLM safety."
  },
  {
    "title": "State-Dependent Conformal Perception Bounds for Neuro-Symbolic Verification of Autonomous Systems",
    "url": "http://arxiv.org/abs/2502.21308v1",
    "arxiv_id": "2502.21308v1",
    "authors": [
      "Thomas Waite",
      "Yuang Geng",
      "Trevor Turnquist",
      "Ivan Ruchkin",
      "Radoslav Ivanov"
    ],
    "published": "2025-02-28T18:51:20+00:00",
    "summary": "It remains a challenge to provide safety guarantees for autonomous systems with neural perception and control. A typical approach obtains symbolic bounds on perception error (e.g., using conformal prediction) and performs verification under these bounds. However, these bounds can lead to drastic conservatism in the resulting end-to-end safety guarantee. This paper proposes an approach to synthesize symbolic perception error bounds that serve as an optimal interface between perception performance and control verification. The key idea is to consider our error bounds to be heteroskedastic with respect to the system's state -- not time like in previous approaches. These bounds can be obtained with two gradient-free optimization algorithms. We demonstrate that our bounds lead to tighter safety guarantees than the state-of-the-art in a case study on a mountain car."
  },
  {
    "title": "Dynamically Local-Enhancement Planner for Large-Scale Autonomous Driving",
    "url": "http://arxiv.org/abs/2502.21134v1",
    "arxiv_id": "2502.21134v1",
    "authors": [
      "Nanshan Deng",
      "Weitao Zhou",
      "Bo Zhang",
      "Junze Wen",
      "Kun Jiang",
      "Zhong Cao",
      "Diange Yang"
    ],
    "published": "2025-02-28T15:17:20+00:00",
    "summary": "Current autonomous vehicles operate primarily within limited regions, but there is increasing demand for broader applications. However, as models scale, their limited capacity becomes a significant challenge for adapting to novel scenarios. It is increasingly difficult to improve models for new situations using a single monolithic model. To address this issue, we introduce the concept of dynamically enhancing a basic driving planner with local driving data, without permanently modifying the planner itself. This approach, termed the Dynamically Local-Enhancement (DLE) Planner, aims to improve the scalability of autonomous driving systems without significantly expanding the planner's size. Our approach introduces a position-varying Markov Decision Process formulation coupled with a graph neural network that extracts region-specific driving features from local observation data. The learned features describe the local behavior of the surrounding objects, which is then leveraged to enhance a basic reinforcement learning-based policy. We evaluated our approach in multiple scenarios and compared it with a one-for-all driving model. The results show that our method outperforms the baseline policy in both safety (collision rate) and average reward, while maintaining a lighter scale. This approach has the potential to benefit large-scale autonomous vehicles without the need for largely expanding on-device driving models."
  },
  {
    "title": "Rare event modeling with self-regularized normalizing flows: what can we learn from a single failure?",
    "url": "http://arxiv.org/abs/2502.21110v1",
    "arxiv_id": "2502.21110v1",
    "authors": [
      "Charles Dawson",
      "Van Tran",
      "Max Z. Li",
      "Chuchu Fan"
    ],
    "published": "2025-02-28T14:47:52+00:00",
    "summary": "Increased deployment of autonomous systems in fields like transportation and robotics have seen a corresponding increase in safety-critical failures. These failures can be difficult to model and debug due to the relative lack of data: compared to tens of thousands of examples from normal operations, we may have only seconds of data leading up to the failure. This scarcity makes it challenging to train generative models of rare failure events, as existing methods risk either overfitting to noise in the limited failure dataset or underfitting due to an overly strong prior. We address this challenge with CalNF, or calibrated normalizing flows, a self-regularized framework for posterior learning from limited data. CalNF achieves state-of-the-art performance on data-limited failure modeling and inverse problems and enables a first-of-a-kind case study into the root causes of the 2022 Southwest Airlines scheduling crisis."
  },
  {
    "title": "AuthSim: Towards Authentic and Effective Safety-critical Scenario Generation for Autonomous Driving Tests",
    "url": "http://arxiv.org/abs/2502.21100v1",
    "arxiv_id": "2502.21100v1",
    "authors": [
      "Yukuan Yang",
      "Xucheng Lu",
      "Zhili Zhang",
      "Zepeng Wu",
      "Guoqi Li",
      "Lingzhong Meng",
      "Yunzhi Xue"
    ],
    "published": "2025-02-28T14:38:35+00:00",
    "summary": "Generating adversarial safety-critical scenarios is a pivotal method for testing autonomous driving systems, as it identifies potential weaknesses and enhances system robustness and reliability. However, existing approaches predominantly emphasize unrestricted collision scenarios, prompting non-player character (NPC) vehicles to attack the ego vehicle indiscriminately. These works overlook these scenarios' authenticity, rationality, and relevance, resulting in numerous extreme, contrived, and largely unrealistic collision events involving aggressive NPC vehicles. To rectify this issue, we propose a three-layer relative safety region model, which partitions the area based on danger levels and increases the likelihood of NPC vehicles entering relative boundary regions. This model directs NPC vehicles to engage in adversarial actions within relatively safe boundary regions, thereby augmenting the scenarios' authenticity. We introduce AuthSim, a comprehensive platform for generating authentic and effective safety-critical scenarios by integrating the three-layer relative safety region model with reinforcement learning. To our knowledge, this is the first attempt to address the authenticity and effectiveness of autonomous driving system test scenarios comprehensively. Extensive experiments demonstrate that AuthSim outperforms existing methods in generating effective safety-critical scenarios. Notably, AuthSim achieves a 5.25% improvement in average cut-in distance and a 27.12% enhancement in average collision interval time, while maintaining higher efficiency in generating effective safety-critical scenarios compared to existing methods. This underscores its significant advantage in producing authentic scenarios over current methodologies."
  },
  {
    "title": "FC-Attack: Jailbreaking Large Vision-Language Models via Auto-Generated Flowcharts",
    "url": "http://arxiv.org/abs/2502.21059v1",
    "arxiv_id": "2502.21059v1",
    "authors": [
      "Ziyi Zhang",
      "Zhen Sun",
      "Zongmin Zhang",
      "Jihui Guo",
      "Xinlei He"
    ],
    "published": "2025-02-28T13:59:11+00:00",
    "summary": "Large Vision-Language Models (LVLMs) have become powerful and widely adopted in some practical applications. However, recent research has revealed their vulnerability to multimodal jailbreak attacks, whereby the model can be induced to generate harmful content, leading to safety risks. Although most LVLMs have undergone safety alignment, recent research shows that the visual modality is still vulnerable to jailbreak attacks. In our work, we discover that by using flowcharts with partially harmful information, LVLMs can be induced to provide additional harmful details. Based on this, we propose a jailbreak attack method based on auto-generated flowcharts, FC-Attack. Specifically, FC-Attack first fine-tunes a pre-trained LLM to create a step-description generator based on benign datasets. The generator is then used to produce step descriptions corresponding to a harmful query, which are transformed into flowcharts in 3 different shapes (vertical, horizontal, and S-shaped) as visual prompts. These flowcharts are then combined with a benign textual prompt to execute a jailbreak attack on LVLMs. Our evaluations using the Advbench dataset show that FC-Attack achieves over 90% attack success rates on Gemini-1.5, Llaval-Next, Qwen2-VL, and InternVL-2.5 models, outperforming existing LVLM jailbreak methods. Additionally, we investigate factors affecting the attack performance, including the number of steps and the font styles in the flowcharts. Our evaluation shows that FC-Attack can improve the jailbreak performance from 4% to 28% in Claude-3.5 by changing the font style. To mitigate the attack, we explore several defenses and find that AdaShield can largely reduce the jailbreak performance but with the cost of utility drop."
  },
  {
    "title": "Including Follower Dynamics in Beaconing for Platooning Safety",
    "url": "http://arxiv.org/abs/2502.21039v1",
    "arxiv_id": "2502.21039v1",
    "authors": [
      "Hassan Laghbi",
      "Nigel Thomas"
    ],
    "published": "2025-02-28T13:30:46+00:00",
    "summary": "In this paper, we propose procedures to address platoon follower dynamics within adaptive beaconing. We implement them in a known adaptive beaconing scheme which is Jerk Beaconing (JB) to improve its safety. We evaluate our proposed approach in terms of safety, string stability and the channel busy ratio (CBR) overhead. The results reveal that our proposal significantly enhances safety without imposing substantial CBR overhead and maintains the string stability of the PATH CACC controller under normal conditions."
  },
  {
    "title": "A RISC-V Multicore and GPU SoC Platform with a Qualifiable Software Stack for Safety Critical Systems",
    "url": "http://arxiv.org/abs/2502.21027v1",
    "arxiv_id": "2502.21027v1",
    "authors": [
      "Marc Sol\u00e9 i Bonet",
      "Jannis Wolf",
      "Leonidas Kosmidis"
    ],
    "published": "2025-02-28T13:16:00+00:00",
    "summary": "In the context of the Horizon Europe project, METASAT, a hardware platform was developed as a prototype of future space systems. The platform is based on a multiprocessor NOEL-V, an established space-grade processor, which is integrated with the SPARROW AI accelerator and connected to a GPU, Vortex. Both processing systems follow the RISC-V specification. This is a novel hardware architecture for the space domain as the use of massive parallel processing units, such as GPUs, is starting to be considered for upcoming space missions due to the increased performance required to future space-related workloads, in particular, related to AI. However, such solutions are only currently adopted for New Space, since their limitations come not only from the hardware, but also from the software, which needs to be qualified before being deployed on an institutional mission. For this reason, the METASAT platform is one of the first endeavors towards enabling the use of high performance hardware in a qualifiable environment for safety critical systems. The software stack is based on baremetal, RTEMS and the XtratuM hypervisor, providing different options for applications of various degrees of criticality."
  },
  {
    "title": "Beware of Your Po! Measuring and Mitigating AI Safety Risks in Role-Play Fine-Tuning of LLMs",
    "url": "http://arxiv.org/abs/2502.20968v1",
    "arxiv_id": "2502.20968v1",
    "authors": [
      "Weixiang Zhao",
      "Yulin Hu",
      "Yang Deng",
      "Jiahe Guo",
      "Xingyu Sui",
      "Xinyang Han",
      "An Zhang",
      "Yanyan Zhao",
      "Bing Qin",
      "Tat-Seng Chua",
      "Ting Liu"
    ],
    "published": "2025-02-28T11:31:27+00:00",
    "summary": "Role-playing enables large language models (LLMs) to engage users in immersive and personalized interactions, but it also introduces significant safety risks. Existing role-play fine-tuning techniques improve role adaptability but may degrade safety performance, particularly for villainous characters. In this work, we conduct the first comprehensive assessment of role-play fine-tuning risks by training 95 role-specific LLMs using RoleBench. Our experiments reveal that role-play fine-tuning leads to a noticeable decline in safety performance, with safety risks varying based on character traits. To tackle this challenge, we propose Safety-Aware Role-Play Fine-Tuning (SaRFT), a novel method designed to balance role-playing capabilities and safety. Extensive experiments on LLaMA-3-8B-Instruct, Gemma-2-9B-it, and Qwen2.5-7B-Instruct demonstrate that SaRFT consistently outperforms state-of-the-art baselines under both LoRA and full-parameter fine-tuning settings. Our findings highlight the necessity of role-adaptive safety measures and provide insights into mitigating role-specific safety risks in role-playing LLMs."
  },
  {
    "title": "Efficient Jailbreaking of Large Models by Freeze Training: Lower Layers Exhibit Greater Sensitivity to Harmful Content",
    "url": "http://arxiv.org/abs/2502.20952v1",
    "arxiv_id": "2502.20952v1",
    "authors": [
      "Hongyuan Shen",
      "Min Zheng",
      "Jincheng Wang",
      "Yang Zhao"
    ],
    "published": "2025-02-28T11:07:41+00:00",
    "summary": "With the widespread application of Large Language Models across various domains, their security issues have increasingly garnered significant attention from both academic and industrial communities. This study conducts sampling and normalization of the parameters of the LLM to generate visual representations and heatmaps of parameter distributions, revealing notable discrepancies in parameter distributions among certain layers within the hidden layers. Further analysis involves calculating statistical metrics for each layer, followed by the computation of a Comprehensive Sensitivity Score based on these metrics, which identifies the lower layers as being particularly sensitive to the generation of harmful content. Based on this finding, we employ a Freeze training strategy, selectively performing Supervised Fine-Tuning only on the lower layers. Experimental results demonstrate that this method significantly reduces training duration and GPU memory consumption while maintaining a high jailbreak success rate and a high harm score, outperforming the results achieved by applying the LoRA method for SFT across all layers. Additionally, the method has been successfully extended to other open-source large models, validating its generality and effectiveness across different model architectures. Furthermore, we compare our method with ohter jailbreak method, demonstrating the superior performance of our approach. By innovatively proposing a method to statistically analyze and compare large model parameters layer by layer, this study provides new insights into the interpretability of large models. These discoveries emphasize the necessity of continuous research and the implementation of adaptive security measures in the rapidly evolving field of LLMs to prevent potential jailbreak attack risks, thereby promoting the development of more robust and secure LLMs."
  },
  {
    "title": "ProBench: Benchmarking Large Language Models in Competitive Programming",
    "url": "http://arxiv.org/abs/2502.20868v1",
    "arxiv_id": "2502.20868v1",
    "authors": [
      "Lei Yang",
      "Renren Jin",
      "Ling Shi",
      "Jianxiang Peng",
      "Yue Chen",
      "Deyi Xiong"
    ],
    "published": "2025-02-28T09:12:42+00:00",
    "summary": "With reasoning language models such as OpenAI-o3 and DeepSeek-R1 emerging, large language models (LLMs) have entered a new phase of development. However, existing benchmarks for coding evaluation are gradually inadequate to assess the capability of advanced LLMs in code reasoning. To bridge the gap for high-level code reasoning assessment, we propose ProBench to benchmark LLMs in competitive programming, drawing inspiration from the International Collegiate Programming Contest. ProBench collects a comprehensive set of competitive programming problems from Codeforces, Luogu, and Nowcoder platforms during the period from July to December 2024, obtaining real test results through online submissions to ensure the fairness and accuracy of the evaluation. We establish a unified problem attribute system, including difficulty grading and algorithm tagging. With carefully collected and annotated data in ProBench, we systematically assess 9 latest LLMs in competitive programming across multiple dimensions, including thought chain analysis, error type diagnosis, and reasoning depth evaluation. Experimental results show that QwQ-32B-Preview achieves the best score of 20.93 followed by DeepSeek-V3 with a score of 16.38, suggesting that models trained with specialized reasoning tasks significantly outperform general-purpose models (even larger than reasoning-oriented models) in programming. Further analysis also reveals key areas for programming capability enhancement, e.g., algorithm adaptability and reasoning sufficiency, providing important insights for the future development of reasoning models."
  },
  {
    "title": "Multimodal Learning for Just-In-Time Software Defect Prediction in Autonomous Driving Systems",
    "url": "http://arxiv.org/abs/2502.20806v1",
    "arxiv_id": "2502.20806v1",
    "authors": [
      "Faisal Mohammad",
      "Duksan Ryu"
    ],
    "published": "2025-02-28T07:45:10+00:00",
    "summary": "In recent years, the rise of autonomous driving technologies has highlighted the critical importance of reliable software for ensuring safety and performance. This paper proposes a novel approach for just-in-time software defect prediction (JIT-SDP) in autonomous driving software systems using multimodal learning. The proposed model leverages the multimodal transformers in which the pre-trained transformers and a combining module deal with the multiple data modalities of the software system datasets such as code features, change metrics, and contextual information. The key point for adapting multimodal learning is to utilize the attention mechanism between the different data modalities such as text, numerical, and categorical. In the combining module, the output of a transformer model on text data and tabular features containing categorical and numerical data are combined to produce the predictions using the fully connected layers. Experiments conducted on three open-source autonomous driving system software projects collected from the GitHub repository (Apollo, Carla, and Donkeycar) demonstrate that the proposed approach significantly outperforms state-of-the-art deep learning and machine learning models regarding evaluation metrics. Our findings highlight the potential of multimodal learning to enhance the reliability and safety of autonomous driving software through improved defect prediction."
  },
  {
    "title": "Characteristics Analysis of Autonomous Vehicle Pre-crash Scenarios",
    "url": "http://arxiv.org/abs/2502.20789v1",
    "arxiv_id": "2502.20789v1",
    "authors": [
      "Yixuan Li",
      "Xuesong Wang",
      "Tianyi Wang",
      "Qian Liu"
    ],
    "published": "2025-02-28T07:10:53+00:00",
    "summary": "To date, hundreds of crashes have occurred in open road testing of automated vehicles (AVs), highlighting the need for improving AV reliability and safety. Pre-crash scenario typology classifies crashes based on vehicle dynamics and kinematics features. Building on this, characteristics analysis can identify similar features under comparable crashes, offering a more effective reflection of general crash patterns and providing more targeted recommendations for enhancing AV performance. However, current studies primarily concentrated on crashes among conventional human-driven vehicles, leaving a gap in research dedicated to in-depth AV crash analyses. In this paper, we analyzed the latest California AV collision reports and used the newly revised pre-crash scenario typology to identify pre-crash scenarios. We proposed a set of mapping rules for automatically extracting these AV pre-crash scenarios, successfully identifying 24 types with a 98.1% accuracy rate, and obtaining two key scenarios of AV crashes (i.e., rear-end scenarios and intersection scenarios) through detailed analysis. Association analyses of rear-end scenarios showed that the significant environmental influencing factors were traffic control type, location type, light, etc. For intersection scenarios prone to severe crashes with detailed descriptions, we employed causal analyses to obtain the significant causal factors: habitual violations and expectations of certain behavior. Optimization recommendations were then formulated, addressing both governmental oversight and AV manufacturers' potential improvements. The findings of this paper could guide government authorities to develop related regulations, help manufacturers design AV test scenarios, and identify potential shortcomings in control algorithms specific to various real-world scenarios, thereby optimizing AV systems effectively."
  },
  {
    "title": "The Rise of Darkness: Safety-Utility Trade-Offs in Role-Playing Dialogue Agents",
    "url": "http://arxiv.org/abs/2502.20757v1",
    "arxiv_id": "2502.20757v1",
    "authors": [
      "Yihong Tang",
      "Kehai Chen",
      "Xuefeng Bai",
      "Zhengyu Niu",
      "Bo Wang",
      "Jie Liu",
      "Min Zhang"
    ],
    "published": "2025-02-28T06:18:50+00:00",
    "summary": "Large Language Models (LLMs) have made remarkable advances in role-playing dialogue agents, demonstrating their utility in character simulations. However, it remains challenging for these agents to balance character portrayal utility with content safety because this essential character simulation often comes with the risk of generating unsafe content. To address this issue, we first conduct a systematic exploration of the safety-utility trade-off across multiple LLMs. Our analysis reveals that risk scenarios created by villain characters and user queries (referred to as risk coupling) contribute to this trade-off. Building on this, we propose a novel Adaptive Dynamic Multi-Preference (ADMP) method, which dynamically adjusts safety-utility preferences based on the degree of risk coupling and guides the model to generate responses biased toward utility or safety. We further introduce Coupling Margin Sampling (CMS) into coupling detection to enhance the model's ability to handle high-risk scenarios. Experimental results demonstrate that our approach improves safety metrics while maintaining utility."
  },
  {
    "title": "Computationally Efficient Safe Control of Linear Systems under Severe Sensor Attacks",
    "url": "http://arxiv.org/abs/2502.20718v1",
    "arxiv_id": "2502.20718v1",
    "authors": [
      "Xiao Tan",
      "Pio Ong",
      "Paulo Tabuada",
      "Aaron D. Ames"
    ],
    "published": "2025-02-28T05:05:48+00:00",
    "summary": "Cyber-physical systems are prone to sensor attacks that can compromise safety. A common approach to synthesizing controllers robust to sensor attacks is secure state reconstruction (SSR) -- but this is computationally expensive, hindering real-time control. In this paper, we take a safety-critical perspective on mitigating severe sensor attacks, leading to a computationally efficient solution. Namely, we design feedback controllers that ensure system safety by directly computing control actions from past input-output data. Instead of fully solving the SSR problem, we use conservative bounds on a control barrier function (CBF) condition, which we obtain by extending the recent eigendecomposition-based SSR approach to severe sensor attack settings. Additionally, we present an extended approach that solves a smaller-scale subproblem of the SSR problem, taking on some computational burden to mitigate the conservatism in the main approach. Numerical comparisons confirm that the traditional SSR approaches suffer from combinatorial issues, while our approach achieves safety guarantees with greater computational efficiency."
  },
  {
    "title": "From Safety Standards to Safe Operation with Mobile Robotic Systems Deployment",
    "url": "http://arxiv.org/abs/2502.20693v1",
    "arxiv_id": "2502.20693v1",
    "authors": [
      "Bruno Belzile",
      "Tatiana Wanang-Siyapdjie",
      "Sina Karimi",
      "Rafael Gomes Braga",
      "Ivanka Iordanova",
      "David St-Onge"
    ],
    "published": "2025-02-28T03:52:10+00:00",
    "summary": "Mobile robotic systems are increasingly used in various work environments to support productivity. However, deploying robots in workplaces crowded by human workers and interacting with them results in safety challenges and concerns, namely robot-worker collisions and worker distractions in hazardous environments. Moreover, the literature on risk assessment as well as the standard specific to mobile platforms is rather limited. In this context, this paper first conducts a review of the relevant standards and methodologies and then proposes a risk assessment for the safe deployment of mobile robots on construction sites. The approach extends relevant existing safety standards to encompass uncovered scenarios. Safety recommendations are made based on the framework, after its validation by field experts."
  },
  {
    "title": "Delayed-Decision Motion Planning in the Presence of Multiple Predictions",
    "url": "http://arxiv.org/abs/2502.20636v1",
    "arxiv_id": "2502.20636v1",
    "authors": [
      "David Isele",
      "Alexandre Miranda Anon",
      "Faizan M. Tariq",
      "Goro Yeh",
      "Avinash Singh",
      "Sangjae Bae"
    ],
    "published": "2025-02-28T01:36:33+00:00",
    "summary": "Reliable automated driving technology is challenged by various sources of uncertainties, in particular, behavioral uncertainties of traffic agents. It is common for traffic agents to have intentions that are unknown to others, leaving an automated driving car to reason over multiple possible behaviors. This paper formalizes a behavior planning scheme in the presence of multiple possible futures with corresponding probabilities. We present a maximum entropy formulation and show how, under certain assumptions, this allows delayed decision-making to improve safety. The general formulation is then turned into a model predictive control formulation, which is solved as a quadratic program or a set of quadratic programs. We discuss implementation details for improving computation and verify operation in simulation and on a mobile robot."
  },
  {
    "title": "SafeText: Safe Text-to-image Models via Aligning the Text Encoder",
    "url": "http://arxiv.org/abs/2502.20623v1",
    "arxiv_id": "2502.20623v1",
    "authors": [
      "Yuepeng Hu",
      "Zhengyuan Jiang",
      "Neil Zhenqiang Gong"
    ],
    "published": "2025-02-28T01:02:57+00:00",
    "summary": "Text-to-image models can generate harmful images when presented with unsafe prompts, posing significant safety and societal risks. Alignment methods aim to modify these models to ensure they generate only non-harmful images, even when exposed to unsafe prompts. A typical text-to-image model comprises two main components: 1) a text encoder and 2) a diffusion module. Existing alignment methods mainly focus on modifying the diffusion module to prevent harmful image generation. However, this often significantly impacts the model's behavior for safe prompts, causing substantial quality degradation of generated images. In this work, we propose SafeText, a novel alignment method that fine-tunes the text encoder rather than the diffusion module. By adjusting the text encoder, SafeText significantly alters the embedding vectors for unsafe prompts, while minimally affecting those for safe prompts. As a result, the diffusion module generates non-harmful images for unsafe prompts while preserving the quality of images for safe prompts. We evaluate SafeText on multiple datasets of safe and unsafe prompts, including those generated through jailbreak attacks. Our results show that SafeText effectively prevents harmful image generation with minor impact on the images for safe prompts, and SafeText outperforms six existing alignment methods. We will publish our code and data after paper acceptance."
  },
  {
    "title": "HazardNet: A Small-Scale Vision Language Model for Real-Time Traffic Safety Detection at Edge Devices",
    "url": "http://arxiv.org/abs/2502.20572v1",
    "arxiv_id": "2502.20572v1",
    "authors": [
      "Mohammad Abu Tami",
      "Mohammed Elhenawy",
      "Huthaifa I. Ashqar"
    ],
    "published": "2025-02-27T22:21:45+00:00",
    "summary": "Traffic safety remains a vital concern in contemporary urban settings, intensified by the increase of vehicles and the complicated nature of road networks. Traditional safety-critical event detection systems predominantly rely on sensor-based approaches and conventional machine learning algorithms, necessitating extensive data collection and complex training processes to adhere to traffic safety regulations. This paper introduces HazardNet, a small-scale Vision Language Model designed to enhance traffic safety by leveraging the reasoning capabilities of advanced language and vision models. We built HazardNet by fine-tuning the pre-trained Qwen2-VL-2B model, chosen for its superior performance among open-source alternatives and its compact size of two billion parameters. This helps to facilitate deployment on edge devices with efficient inference throughput. In addition, we present HazardQA, a novel Vision Question Answering (VQA) dataset constructed specifically for training HazardNet on real-world scenarios involving safety-critical events. Our experimental results show that the fine-tuned HazardNet outperformed the base model up to an 89% improvement in F1-Score and has comparable results with improvement in some cases reach up to 6% when compared to larger models, such as GPT-4o. These advancements underscore the potential of HazardNet in providing real-time, reliable traffic safety event detection, thereby contributing to reduced accidents and improved traffic management in urban environments. Both HazardNet model and the HazardQA dataset are available at https://huggingface.co/Tami3/HazardNet and https://huggingface.co/datasets/Tami3/HazardQA, respectively."
  },
  {
    "title": "SoS1: O1 and R1-Like Reasoning LLMs are Sum-of-Square Solvers",
    "url": "http://arxiv.org/abs/2502.20545v1",
    "arxiv_id": "2502.20545v1",
    "authors": [
      "Kechen Li",
      "Wenqi Zhu",
      "Coralia Cartis",
      "Tianbo Ji",
      "Shiwei Liu"
    ],
    "published": "2025-02-27T21:41:43+00:00",
    "summary": "Large Language Models (LLMs) have achieved human-level proficiency across diverse tasks, but their ability to perform rigorous mathematical problem solving remains an open challenge. In this work, we investigate a fundamental yet computationally intractable problem: determining whether a given multivariate polynomial is nonnegative. This problem, closely related to Hilbert's Seventeenth Problem, plays a crucial role in global polynomial optimization and has applications in various fields. First, we introduce SoS-1K, a meticulously curated dataset of approximately 1,000 polynomials, along with expert-designed reasoning instructions based on five progressively challenging criteria. Evaluating multiple state-of-the-art LLMs, we find that without structured guidance, all models perform only slightly above the random guess baseline 50%. However, high-quality reasoning instructions significantly improve accuracy, boosting performance up to 81%. Furthermore, our 7B model, SoS-7B, fine-tuned on SoS-1K for just 4 hours, outperforms the 671B DeepSeek-V3 and GPT-4o-mini in accuracy while only requiring 1.8% and 5% of the computation time needed for letters, respectively. Our findings highlight the potential of LLMs to push the boundaries of mathematical reasoning and tackle NP-hard problems."
  },
  {
    "title": "Automated Profile-Guided Replacement of Data Structures to Reduce Memory Allocation",
    "url": "http://arxiv.org/abs/2502.20536v1",
    "arxiv_id": "2502.20536v1",
    "authors": [
      "Lukas Makor",
      "Sebastian Kloibhofer",
      "Peter Hofer",
      "David Leopoldseder",
      "Hanspeter M\u00f6ssenb\u00f6ck"
    ],
    "published": "2025-02-27T21:38:48+00:00",
    "summary": "Data structures are a cornerstone of most modern programming languages. Whether they are provided via separate libraries, built into the language specification, or as part of the language's standard library -- data structures such as lists, maps, sets, or arrays provide programmers with a large repertoire of tools to deal with data. Moreover, each kind of data structure typically comes with a variety of implementations that focus on scalability, memory efficiency, performance, thread-safety, or similar aspects. Choosing the *right* data structure for a particular use case can be difficult or even impossible if the data structure is part of a framework over which the user has no control. It typically requires in-depth knowledge about the program and, in particular, about the usage of the data structure in question. However, it is usually not feasible for developers to obtain such information about programs in advance. Hence, it makes sense to look for a more automated way for optimizing data structures. We present an approach to automatically replace data structures in Java applications. We use profiling to determine allocation-site-specific metrics about data structures and their usages, and then automatically replace their allocations with customized versions, focusing on memory efficiency. Our approach is integrated into GraalVM Native Image, an Ahead-of-Time compiler for Java applications. By analyzing the generated data structure profiles, we show how standard benchmarks and microservice-based applications use data structures and demonstrate the impact of customized data structures on the memory usage of applications. We conducted an evaluation on standard and microservice-based benchmarks, in which the memory usage was reduced by up to 13.85 % in benchmarks that make heavy use of data structures. While others are only slightly affected, we could still reduce the average memory usage by 1.63 % in standard benchmarks and by 2.94 % in microservice-based benchmarks. We argue that our work demonstrates that choosing appropriate data structures can reduce the memory usage of applications. While acknowledge that our approach does not provide benefits for all kinds of workloads, our work nevertheless shows how automated profiling and replacement can be used to optimize data structures in general. Hence, we argue that our work could pave the way for future optimizations of data structures."
  },
  {
    "title": "APIKS: A Modular ROS2 Framework for Rapid Prototyping and Validation of Automated Driving Systems",
    "url": "http://arxiv.org/abs/2502.20507v1",
    "arxiv_id": "2502.20507v1",
    "authors": [
      "Jo\u00e3o-Vitor Zacchi",
      "Edoardo Clementi",
      "N\u00faria Mata"
    ],
    "published": "2025-02-27T20:29:31+00:00",
    "summary": "Automated driving technologies promise substantial improvements in transportation safety, efficiency, and accessibility. However, ensuring the reliability and safety of Autonomous Vehicles in complex, real-world environments remains a significant challenge, particularly during the early stages of software development. Existing software development environments and simulation platforms often either focus narrowly on specific functions or are too complex, hindering the rapid prototyping of small proofs of concept. To address this challenge, we have developed the APIKS automotive platform, a modular framework based on ROS2. APIKS is designed for the efficient testing and validation of autonomous vehicle software within software-defined vehicles. It offers a simplified, standards-based architecture designed specifically for small-scale proofs of concept. This enables rapid prototyping without the overhead associated with comprehensive platforms. We demonstrate the capabilities of APIKS through an exemplary use case involving a Construction Zone Assist system, illustrating its effectiveness in facilitating the development and testing of autonomous vehicle functionalities."
  },
  {
    "title": "Equivariant Reinforcement Learning Frameworks for Quadrotor Low-Level Control",
    "url": "http://arxiv.org/abs/2502.20500v1",
    "arxiv_id": "2502.20500v1",
    "authors": [
      "Beomyeol Yu",
      "Taeyoung Lee"
    ],
    "published": "2025-02-27T20:16:19+00:00",
    "summary": "Improving sampling efficiency and generalization capability is critical for the successful data-driven control of quadrotor unmanned aerial vehicles (UAVs) that are inherently unstable. While various reinforcement learning (RL) approaches have been applied to autonomous quadrotor flight, they often require extensive training data, posing multiple challenges and safety risks in practice. To address these issues, we propose data-efficient, equivariant monolithic and modular RL frameworks for quadrotor low-level control. Specifically, by identifying the rotational and reflectional symmetries in quadrotor dynamics and encoding these symmetries into equivariant network models, we remove redundancies of learning in the state-action space. This approach enables the optimal control action learned in one configuration to automatically generalize into other configurations via symmetry, thereby enhancing data efficiency. Experimental results demonstrate that our equivariant approaches significantly outperform their non-equivariant counterparts in terms of learning efficiency and flight performance."
  },
  {
    "title": "EgoNormia: Benchmarking Physical Social Norm Understanding",
    "url": "http://arxiv.org/abs/2502.20490v1",
    "arxiv_id": "2502.20490v1",
    "authors": [
      "MohammadHossein Rezaei",
      "Yicheng Fu",
      "Phil Cuvin",
      "Caleb Ziems",
      "Yanzhe Zhang",
      "Hao Zhu",
      "Diyi Yang"
    ],
    "published": "2025-02-27T19:54:16+00:00",
    "summary": "Human activity is moderated by norms. When performing actions in the real world, humans not only follow norms, but also consider the trade-off between different norms However, machines are often trained without explicit supervision on norm understanding and reasoning, especially when the norms are grounded in a physical and social context. To improve and evaluate the normative reasoning capability of vision-language models (VLMs), we present EgoNormia $\\|\\epsilon\\|$, consisting of 1,853 ego-centric videos of human interactions, each of which has two related questions evaluating both the prediction and justification of normative actions. The normative actions encompass seven categories: safety, privacy, proxemics, politeness, cooperation, coordination/proactivity, and communication/legibility. To compile this dataset at scale, we propose a novel pipeline leveraging video sampling, automatic answer generation, filtering, and human validation. Our work demonstrates that current state-of-the-art vision-language models lack robust norm understanding, scoring a maximum of 45% on EgoNormia (versus a human bench of 92%). Our analysis of performance in each dimension highlights the significant risks of safety, privacy, and the lack of collaboration and communication capability when applied to real-world agents. We additionally show that through a retrieval-based generation method, it is possible to use EgoNomia to enhance normative reasoning in VLMs."
  },
  {
    "title": "Bounded First-Class Universe Levels in Dependent Type Theory",
    "url": "http://arxiv.org/abs/2502.20485v1",
    "arxiv_id": "2502.20485v1",
    "authors": [
      "Jonathan Chan",
      "Stephanie Weirich"
    ],
    "published": "2025-02-27T19:52:46+00:00",
    "summary": "In dependent type theory, being able to refer to a type universe as a term itself increases its expressive power, but requires mechanisms in place to prevent Girard's paradox from introducing logical inconsistency in the presence of type-in-type. The simplest mechanism is a hierarchy of universes indexed by a sequence of levels, typically the naturals. To improve reusability of definitions, they can be made level polymorphic, abstracting over level variables and adding a notion of level expressions. For even more expressive power, level expressions can be made first-class as terms themselves, and level polymorphism is subsumed by dependent functions quantifying over levels. Furthermore, bounded level polymorphism provides more expressivity by being able to explicitly state constraints on level variables. While semantics for first-class levels with constraints are known, syntax and typing rules have not been explicitly written down. Yet pinning down a well-behaved syntax is not trivial; there exist prior type theories with bounded level polymorphism that fail to satisfy subject reduction. In this work, we design an explicit syntax for a type theory with bounded first-class levels, parametrized over arbitrary well-founded sets of levels. We prove the metatheoretic properties of subject reduction, type safety, consistency, and canonicity, entirely mechanized from syntax to semantics in Lean."
  },
  {
    "title": "Searching for additional structure and redshift evolution in the observed binary black hole population with a parametric time-dependent mass distribution",
    "url": "http://arxiv.org/abs/2502.20445v1",
    "arxiv_id": "2502.20445v1",
    "authors": [
      "Vasco Gennari",
      "Simone Mastrogiovanni",
      "Nicola Tamanini",
      "Sylvain Marsat",
      "Gr\u00e9goire Pierra"
    ],
    "published": "2025-02-27T19:00:02+00:00",
    "summary": "The population of the observed gravitational wave events encodes unique information on the formation and evolution of stellar-mass black holes, from the underlying astrophysical processes to the large-scale dynamics of the Universe. We use the ICAROGW analysis infrastructure to perform hierarchical Bayesian inference on the gravitational wave signals from the LIGO-Virgo-KAGRA third observing run, O3. Searching for additional structure and redshift evolution in the primary mass distribution, we explore the dependence of the mass spectrum reconstruction on different parametrizations and prior choices. For the stationary case, we find strong evidence (Bayes factor $B \\simeq 180$) that the results obtained using a power-law model with a peak (Powerlaw-Gaussian)--the model preferred so far in the literature--are sensitive to prior bounds, affecting the resolvability of the $\\sim 35 M_{\\odot}$ peak. This behaviour is reproduced by simulated data, indicating a bimodal structure in the likelihood. Models with three mass features simultaneously capture a sharp $\\sim 10M_{\\odot}$ peak, a $\\sim 35 M_{\\odot}$ overdensity, and support for a $\\sim 20 M_{\\odot}$ overdensity preceded by a dip. Among these, a model with three power-law peaks (Powerlaw-Powerlaw-Powerlaw) is equally favored, in terms of evidence, over the Powerlaw-Gaussian model with wide priors. We find no statistical support for redshift evolution in the current data and provide constraints on the parameters governing this evolution, showing consistency with stationarity. We highlight possible limitations of the hierarchical Bayesian inference framework in reconstructing evolving features outside the detector horizon. Our work lays the foundations for a robust characterization of time-dependent population distributions, with significant implications for black hole astrophysics and gravitational wave cosmology."
  },
  {
    "title": "Large Language Model Strategic Reasoning Evaluation through Behavioral Game Theory",
    "url": "http://arxiv.org/abs/2502.20432v1",
    "arxiv_id": "2502.20432v1",
    "authors": [
      "Jingru Jia",
      "Zehua Yuan",
      "Junhao Pan",
      "Paul E. McNamara",
      "Deming Chen"
    ],
    "published": "2025-02-27T18:58:31+00:00",
    "summary": "Strategic decision-making involves interactive reasoning where agents adapt their choices in response to others, yet existing evaluations of large language models (LLMs) often emphasize Nash Equilibrium (NE) approximation, overlooking the mechanisms driving their strategic choices. To bridge this gap, we introduce an evaluation framework grounded in behavioral game theory, disentangling reasoning capability from contextual effects. Testing 22 state-of-the-art LLMs, we find that GPT-o3-mini, GPT-o1, and DeepSeek-R1 dominate most games yet also demonstrate that the model scale alone does not determine performance. In terms of prompting enhancement, Chain-of-Thought (CoT) prompting is not universally effective, as it increases strategic reasoning only for models at certain levels while providing limited gains elsewhere. Additionally, we investigate the impact of encoded demographic features on the models, observing that certain assignments impact the decision-making pattern. For instance, GPT-4o shows stronger strategic reasoning with female traits than males, while Gemma assigns higher reasoning levels to heterosexual identities compared to other sexual orientations, indicating inherent biases. These findings underscore the need for ethical standards and contextual alignment to balance improved reasoning with fairness."
  },
  {
    "title": "Why Are Web AI Agents More Vulnerable Than Standalone LLMs? A Security Analysis",
    "url": "http://arxiv.org/abs/2502.20383v1",
    "arxiv_id": "2502.20383v1",
    "authors": [
      "Jeffrey Yang Fan Chiang",
      "Seungjae Lee",
      "Jia-Bin Huang",
      "Furong Huang",
      "Yizheng Chen"
    ],
    "published": "2025-02-27T18:56:26+00:00",
    "summary": "Recent advancements in Web AI agents have demonstrated remarkable capabilities in addressing complex web navigation tasks. However, emerging research shows that these agents exhibit greater vulnerability compared to standalone Large Language Models (LLMs), despite both being built upon the same safety-aligned models. This discrepancy is particularly concerning given the greater flexibility of Web AI Agent compared to standalone LLMs, which may expose them to a wider range of adversarial user inputs. To build a scaffold that addresses these concerns, this study investigates the underlying factors that contribute to the increased vulnerability of Web AI agents. Notably, this disparity stems from the multifaceted differences between Web AI agents and standalone LLMs, as well as the complex signals - nuances that simple evaluation metrics, such as success rate, often fail to capture. To tackle these challenges, we propose a component-level analysis and a more granular, systematic evaluation framework. Through this fine-grained investigation, we identify three critical factors that amplify the vulnerability of Web AI agents; (1) embedding user goals into the system prompt, (2) multi-step action generation, and (3) observational capabilities. Our findings highlights the pressing need to enhance security and robustness in AI agent design and provide actionable insights for targeted defense strategies."
  },
  {
    "title": "Trajectory-to-Action Pipeline (TAP): Automated Scenario Description Extraction for Autonomous Vehicle Behavior Comparison",
    "url": "http://arxiv.org/abs/2502.20353v1",
    "arxiv_id": "2502.20353v1",
    "authors": [
      "Aron Harder",
      "Madhur Behl"
    ],
    "published": "2025-02-27T18:27:05+00:00",
    "summary": "Scenario Description Languages (SDLs) provide structured, interpretable embeddings that represent traffic scenarios encountered by autonomous vehicles (AVs), supporting key tasks such as scenario similarity searches and edge case detection for safety analysis. This paper introduces the Trajectory-to-Action Pipeline (TAP), a scalable and automated method for extracting SDL labels from large trajectory datasets. TAP applies a rules-based cross-entropy optimization approach to learn parameters directly from data, enhancing generalization across diverse driving contexts. Using the Waymo Open Motion Dataset (WOMD), TAP achieves 30% greater precision than Average Displacement Error (ADE) and 24% over Dynamic Time Warping (DTW) in identifying behaviorally similar trajectories. Additionally, TAP enables automated detection of unique driving behaviors, streamlining safety evaluation processes for AV testing. This work provides a foundation for scalable scenario-based AV behavior analysis, with potential extensions for integrating multi-agent contexts."
  },
  {
    "title": "Safety Representations for Safer Policy Learning",
    "url": "http://arxiv.org/abs/2502.20341v1",
    "arxiv_id": "2502.20341v1",
    "authors": [
      "Kaustubh Mani",
      "Vincent Mai",
      "Charlie Gauthier",
      "Annie Chen",
      "Samer Nashed",
      "Liam Paull"
    ],
    "published": "2025-02-27T18:10:33+00:00",
    "summary": "Reinforcement learning algorithms typically necessitate extensive exploration of the state space to find optimal policies. However, in safety-critical applications, the risks associated with such exploration can lead to catastrophic consequences. Existing safe exploration methods attempt to mitigate this by imposing constraints, which often result in overly conservative behaviours and inefficient learning. Heavy penalties for early constraint violations can trap agents in local optima, deterring exploration of risky yet high-reward regions of the state space. To address this, we introduce a method that explicitly learns state-conditioned safety representations. By augmenting the state features with these safety representations, our approach naturally encourages safer exploration without being excessively cautious, resulting in more efficient and safer policy learning in safety-critical scenarios. Empirical evaluations across diverse environments show that our method significantly improves task performance while reducing constraint violations during training, underscoring its effectiveness in balancing exploration with safety."
  },
  {
    "title": "On Adversarial Attacks In Acoustic Drone Localization",
    "url": "http://arxiv.org/abs/2502.20325v1",
    "arxiv_id": "2502.20325v1",
    "authors": [
      "Tamir Shor",
      "Chaim Baskin",
      "Alex Bronstein"
    ],
    "published": "2025-02-27T17:50:17+00:00",
    "summary": "Multi-rotor aerial autonomous vehicles (MAVs, more widely known as \"drones\") have been generating increased interest in recent years due to their growing applicability in a vast and diverse range of fields (e.g., agriculture, commercial delivery, search and rescue). The sensitivity of visual-based methods to lighting conditions and occlusions had prompted growing study of navigation reliant on other modalities, such as acoustic sensing. A major concern in using drones in scale for tasks in non-controlled environments is the potential threat of adversarial attacks over their navigational systems, exposing users to mission-critical failures, security breaches, and compromised safety outcomes that can endanger operators and bystanders. While previous work shows impressive progress in acoustic-based drone localization, prior research in adversarial attacks over drone navigation only addresses visual sensing-based systems. In this work, we aim to compensate for this gap by supplying a comprehensive analysis of the effect of PGD adversarial attacks over acoustic drone localization. We furthermore develop an algorithm for adversarial perturbation recovery, capable of markedly diminishing the affect of such attacks in our setting. The code for reproducing all experiments will be released upon publication."
  },
  {
    "title": "Adapting Automatic Speech Recognition for Accented Air Traffic Control Communications",
    "url": "http://arxiv.org/abs/2502.20311v1",
    "arxiv_id": "2502.20311v1",
    "authors": [
      "Marcus Yu Zhe Wee",
      "Justin Juin Hng Wong",
      "Lynus Lim",
      "Joe Yu Wei Tan",
      "Prannaya Gupta",
      "Dillion Lim",
      "En Hao Tew",
      "Aloysius Keng Siew Han",
      "Yong Zhi Lim"
    ],
    "published": "2025-02-27T17:35:59+00:00",
    "summary": "Effective communication in Air Traffic Control (ATC) is critical to maintaining aviation safety, yet the challenges posed by accented English remain largely unaddressed in Automatic Speech Recognition (ASR) systems. Existing models struggle with transcription accuracy for Southeast Asian-accented (SEA-accented) speech, particularly in noisy ATC environments. This study presents the development of ASR models fine-tuned specifically for Southeast Asian accents using a newly created dataset. Our research achieves significant improvements, achieving a Word Error Rate (WER) of 0.0982 or 9.82% on SEA-accented ATC speech. Additionally, the paper highlights the importance of region-specific datasets and accent-focused training, offering a pathway for deploying ASR systems in resource-constrained military operations. The findings emphasize the need for noise-robust training techniques and region-specific datasets to improve transcription accuracy for non-Western accents in ATC communications."
  },
  {
    "title": "Interpreting AI for Fusion: an application to Plasma Profile Analysis for Tearing Mode Stability",
    "url": "http://arxiv.org/abs/2502.20294v1",
    "arxiv_id": "2502.20294v1",
    "authors": [
      "Hiro J Farre-Kaga",
      "Andrew Rothstein",
      "Rohit Sonker",
      "SangKyeun Kim",
      "Ricardo Shousha",
      "Minseok Kim",
      "Keith Erickson",
      "Jeff Schneider",
      "Egemen Kolemen"
    ],
    "published": "2025-02-27T17:19:14+00:00",
    "summary": "AI models have demonstrated strong predictive capabilities for various tokamak instabilities--including tearing modes (TM), ELMs, and disruptive events--but their opaque nature raises concerns about safety and trustworthiness when applied to fusion power plants. Here, we present a physics-based interpretation framework using a TM prediction model as a first demonstration that is validated through a dedicated DIII-D TM avoidance experiment. By applying Shapley analysis, we identify how profiles such as rotation, temperature, and density contribute to the model's prediction of TM stability. Our analysis shows that in our experimental scenario, peaked rotation profiles are lightly stabilizing, but core electron temperature and density profile shape play the primary role in TM stability. This work offers a generalizable ML-based event prediction methodology, from training to physics-driven interpretability, bridging the gap between physics understanding and opaque ML models."
  },
  {
    "title": "QPM: Discrete Optimization for Globally Interpretable Image Classification",
    "url": "http://arxiv.org/abs/2502.20130v1",
    "arxiv_id": "2502.20130v1",
    "authors": [
      "Thomas Norrenbrock",
      "Timo Kaiser",
      "Sovan Biswas",
      "Ramesh Manuvinakurike",
      "Bodo Rosenhahn"
    ],
    "published": "2025-02-27T14:25:36+00:00",
    "summary": "Understanding the classifications of deep neural networks, e.g. used in safety-critical situations, is becoming increasingly important. While recent models can locally explain a single decision, to provide a faithful global explanation about an accurate model's general behavior is a more challenging open task. Towards that goal, we introduce the Quadratic Programming Enhanced Model (QPM), which learns globally interpretable class representations. QPM represents every class with a binary assignment of very few, typically 5, features, that are also assigned to other classes, ensuring easily comparable contrastive class representations. This compact binary assignment is found using discrete optimization based on predefined similarity measures and interpretability constraints. The resulting optimal assignment is used to fine-tune the diverse features, so that each of them becomes the shared general concept between the assigned classes. Extensive evaluations show that QPM delivers unprecedented global interpretability across small and large-scale datasets while setting the state of the art for the accuracy of interpretable models."
  },
  {
    "title": "Minds on the Move: Decoding Trajectory Prediction in Autonomous Driving with Cognitive Insights",
    "url": "http://arxiv.org/abs/2502.20084v1",
    "arxiv_id": "2502.20084v1",
    "authors": [
      "Haicheng Liao",
      "Chengyue Wang",
      "Kaiqun Zhu",
      "Yilong Ren",
      "Bolin Gao",
      "Shengbo Eben Li",
      "Chengzhong Xu",
      "Zhenning Li"
    ],
    "published": "2025-02-27T13:43:17+00:00",
    "summary": "In mixed autonomous driving environments, accurately predicting the future trajectories of surrounding vehicles is crucial for the safe operation of autonomous vehicles (AVs). In driving scenarios, a vehicle's trajectory is determined by the decision-making process of human drivers. However, existing models primarily focus on the inherent statistical patterns in the data, often neglecting the critical aspect of understanding the decision-making processes of human drivers. This oversight results in models that fail to capture the true intentions of human drivers, leading to suboptimal performance in long-term trajectory prediction. To address this limitation, we introduce a Cognitive-Informed Transformer (CITF) that incorporates a cognitive concept, Perceived Safety, to interpret drivers' decision-making mechanisms. Perceived Safety encapsulates the varying risk tolerances across drivers with different driving behaviors. Specifically, we develop a Perceived Safety-aware Module that includes a Quantitative Safety Assessment for measuring the subject risk levels within scenarios, and Driver Behavior Profiling for characterizing driver behaviors. Furthermore, we present a novel module, Leanformer, designed to capture social interactions among vehicles. CITF demonstrates significant performance improvements on three well-established datasets. In terms of long-term prediction, it surpasses existing benchmarks by 12.0% on the NGSIM, 28.2% on the HighD, and 20.8% on the MoCAD dataset. Additionally, its robustness in scenarios with limited or missing data is evident, surpassing most state-of-the-art (SOTA) baselines, and paving the way for real-world applications."
  },
  {
    "title": "Deterministic or probabilistic? The psychology of LLMs as random number generators",
    "url": "http://arxiv.org/abs/2502.19965v1",
    "arxiv_id": "2502.19965v1",
    "authors": [
      "Javier Coronado-Bl\u00e1zquez"
    ],
    "published": "2025-02-27T10:45:27+00:00",
    "summary": "Large Language Models (LLMs) have transformed text generation through inherently probabilistic context-aware mechanisms, mimicking human natural language. In this paper, we systematically investigate the performance of various LLMs when generating random numbers, considering diverse configurations such as different model architectures, numerical ranges, temperature, and prompt languages. Our results reveal that, despite their stochastic transformers-based architecture, these models often exhibit deterministic responses when prompted for random numerical outputs. In particular, we find significant differences when changing the model, as well as the prompt language, attributing this phenomenon to biases deeply embedded within the training data. Models such as DeepSeek-R1 can shed some light on the internal reasoning process of LLMs, despite arriving to similar results. These biases induce predictable patterns that undermine genuine randomness, as LLMs are nothing but reproducing our own human cognitive biases."
  },
  {
    "title": "Beyond the Tip of Efficiency: Uncovering the Submerged Threats of Jailbreak Attacks in Small Language Models",
    "url": "http://arxiv.org/abs/2502.19883v1",
    "arxiv_id": "2502.19883v1",
    "authors": [
      "Sibo Yi",
      "Tianshuo Cong",
      "Xinlei He",
      "Qi Li",
      "Jiaxing Song"
    ],
    "published": "2025-02-27T08:44:04+00:00",
    "summary": "Small language models (SLMs) have become increasingly prominent in the deployment on edge devices due to their high efficiency and low computational cost. While researchers continue to advance the capabilities of SLMs through innovative training strategies and model compression techniques, the security risks of SLMs have received considerably less attention compared to large language models (LLMs).To fill this gap, we provide a comprehensive empirical study to evaluate the security performance of 13 state-of-the-art SLMs under various jailbreak attacks. Our experiments demonstrate that most SLMs are quite susceptible to existing jailbreak attacks, while some of them are even vulnerable to direct harmful prompts.To address the safety concerns, we evaluate several representative defense methods and demonstrate their effectiveness in enhancing the security of SLMs. We further analyze the potential security degradation caused by different SLM techniques including architecture compression, quantization, knowledge distillation, and so on. We expect that our research can highlight the security challenges of SLMs and provide valuable insights to future work in developing more robust and secure SLMs."
  },
  {
    "title": "Behind the Tip of Efficiency: Uncovering the Submerged Threats of Jailbreak Attacks in Small Language Models",
    "url": "http://arxiv.org/abs/2502.19883v2",
    "arxiv_id": "2502.19883v2",
    "authors": [
      "Sibo Yi",
      "Tianshuo Cong",
      "Xinlei He",
      "Qi Li",
      "Jiaxing Song"
    ],
    "published": "2025-02-27T08:44:04+00:00",
    "summary": "Small language models (SLMs) have become increasingly prominent in the deployment on edge devices due to their high efficiency and low computational cost. While researchers continue to advance the capabilities of SLMs through innovative training strategies and model compression techniques, the security risks of SLMs have received considerably less attention compared to large language models (LLMs).To fill this gap, we provide a comprehensive empirical study to evaluate the security performance of 13 state-of-the-art SLMs under various jailbreak attacks. Our experiments demonstrate that most SLMs are quite susceptible to existing jailbreak attacks, while some of them are even vulnerable to direct harmful prompts.To address the safety concerns, we evaluate several representative defense methods and demonstrate their effectiveness in enhancing the security of SLMs. We further analyze the potential security degradation caused by different SLM techniques including architecture compression, quantization, knowledge distillation, and so on. We expect that our research can highlight the security challenges of SLMs and provide valuable insights to future work in developing more robust and secure SLMs."
  },
  {
    "title": "Nonlinear dynamics in pulse-modulated feedback drug dosing",
    "url": "http://arxiv.org/abs/2502.19878v1",
    "arxiv_id": "2502.19878v1",
    "authors": [
      "Alexander Medvedev",
      "Anton V. Proskurnikov",
      "Zhanybai T. Zhusubaliyev"
    ],
    "published": "2025-02-27T08:36:38+00:00",
    "summary": "Pulse-modulated feedback is utilized in drug dosing to mimic sustained over a longer period of time manual discrete dose administration, the latter is in contrast with continuous drug infusion. The intermittent mode of dosing calls for a hybrid (continuous-discrete) modeling of the closed-loop system, where the pharmacokinetics and pharmacodynamics of the drug are captured by differential equations whereas the control law is described by difference equations. Hybrid dynamics are highly nonlinear which complicates formal design of pulse-modulated feedback. This paper demonstrates complex nonlinear dynamical phenomena arising in a simple control system of dosing a neuromuscular blockade agent in anesthesia. Along with the nominal periodic regimen, undesirable nonlinear behaviors, i.e. periodic solutions of high multiplicity, multistability, as well as deterministic chaos, are shown to exist. It is concluded that design of feedback drug dosing algorithms based on a hybrid paradigm has to be informed by a thorough bifurcation analysis in order to secure patient safety."
  },
  {
    "title": "Empowering Social Service with AI: Insights from a Participatory Design Study with Practitioners",
    "url": "http://arxiv.org/abs/2502.19822v1",
    "arxiv_id": "2502.19822v1",
    "authors": [
      "Yugin Tan",
      "Kai Xin Soh",
      "Renwen Zhang",
      "Jungup Lee",
      "Han Meng",
      "Biswadeep Sen",
      "Yi-Chieh Lee"
    ],
    "published": "2025-02-27T06:50:05+00:00",
    "summary": "In social service, administrative burdens and decision-making challenges often hinder practitioners from performing effective casework. Generative AI (GenAI) offers significant potential to streamline these tasks, yet exacerbates concerns about overreliance, algorithmic bias, and loss of identity within the profession. We explore these issues through a two-stage participatory design study. We conducted formative co-design workshops (\\textit{n=27}) to create a prototype GenAI tool, followed by contextual inquiry sessions with practitioners (\\textit{n=24}) using the tool with real case data. We reveal opportunities for AI integration in documentation, assessment, and worker supervision, while highlighting risks related to GenAI limitations, skill retention, and client safety. Drawing comparisons with GenAI tools in other fields, we discuss design and usage guidelines for such tools in social service practice."
  },
  {
    "title": "Foot-In-The-Door: A Multi-turn Jailbreak for LLMs",
    "url": "http://arxiv.org/abs/2502.19820v1",
    "arxiv_id": "2502.19820v1",
    "authors": [
      "Zixuan Weng",
      "Xiaolong Jin",
      "Jinyuan Jia",
      "Xiangyu Zhang"
    ],
    "published": "2025-02-27T06:49:16+00:00",
    "summary": "Ensuring AI safety is crucial as large language models become increasingly integrated into real-world applications. A key challenge is jailbreak, where adversarial prompts bypass built-in safeguards to elicit harmful disallowed outputs. Inspired by psychological foot-in-the-door principles, we introduce FITD,a novel multi-turn jailbreak method that leverages the phenomenon where minor initial commitments lower resistance to more significant or more unethical transgressions.Our approach progressively escalates the malicious intent of user queries through intermediate bridge prompts and aligns the model's response by itself to induce toxic responses. Extensive experimental results on two jailbreak benchmarks demonstrate that FITD achieves an average attack success rate of 94% across seven widely used models, outperforming existing state-of-the-art methods. Additionally, we provide an in-depth analysis of LLM self-corruption, highlighting vulnerabilities in current alignment strategies and emphasizing the risks inherent in multi-turn interactions.The code is available at https://github.com/Jinxiaolong1129/Foot-in-the-door-Jailbreak ."
  },
  {
    "title": "Foot-In-The-Door: A Multi-turn Jailbreak for LLMs",
    "url": "http://arxiv.org/abs/2502.19820v2",
    "arxiv_id": "2502.19820v2",
    "authors": [
      "Zixuan Weng",
      "Xiaolong Jin",
      "Jinyuan Jia",
      "Xiangyu Zhang"
    ],
    "published": "2025-02-27T06:49:16+00:00",
    "summary": "Ensuring AI safety is crucial as large language models become increasingly integrated into real-world applications. A key challenge is jailbreak, where adversarial prompts bypass built-in safeguards to elicit harmful disallowed outputs. Inspired by psychological foot-in-the-door principles, we introduce FITD,a novel multi-turn jailbreak method that leverages the phenomenon where minor initial commitments lower resistance to more significant or more unethical transgressions. Our approach progressively escalates the malicious intent of user queries through intermediate bridge prompts and aligns the model's response by itself to induce toxic responses. Extensive experimental results on two jailbreak benchmarks demonstrate that FITD achieves an average attack success rate of 94% across seven widely used models, outperforming existing state-of-the-art methods. Additionally, we provide an in-depth analysis of LLM self-corruption, highlighting vulnerabilities in current alignment strategies and emphasizing the risks inherent in multi-turn interactions. The code is available at https://github.com/Jinxiaolong1129/Foot-in-the-door-Jailbreak."
  },
  {
    "title": "Automatic Linear Resource Bound Analysis for Rust via Prophecy Potentials",
    "url": "http://arxiv.org/abs/2502.19810v1",
    "arxiv_id": "2502.19810v1",
    "authors": [
      "Qihao Lian",
      "Di Wang"
    ],
    "published": "2025-02-27T06:35:40+00:00",
    "summary": "Rust has become a popular system programming language that strikes a balance between memory safety and performance. Rust's type system ensures the safety of low-level memory controls; however, a well-typed Rust program is not guaranteed to enjoy high performance. This article studies static analysis for resource consumption of Rust programs, aiming at understanding the performance of Rust programs. Although there have been tons of studies on static resource analysis, exploiting Rust's memory safety -- especially the borrow mechanisms and their properties -- to aid resource-bound analysis, remains unexplored. This article presents RaRust, a type-based linear resource-bound analysis for well-typed Rust programs. RaRust follows the methodology of automatic amortized resource analysis (AARA) to build a resource-aware type system. To support Rust's borrow mechanisms, including shared and mutable borrows, RaRust introduces shared and novel prophecy potentials to reason about borrows compositionally. To prove the soundness of RaRust, this article proposes Resource-Aware Borrow Calculus (RABC) as a variant of recently proposed Low-Level Borrow Calculus (LLBC). The experimental evaluation of a prototype implementation of RaRust demonstrates that RaRust is capable of inferring symbolic linear resource bounds for Rust programs featuring shared and mutable borrows, reborrows, heap-allocated data structures, loops, and recursion."
  },
  {
    "title": "R1-T1: Fully Incentivizing Translation Capability in LLMs via Reasoning Learning",
    "url": "http://arxiv.org/abs/2502.19735v1",
    "arxiv_id": "2502.19735v1",
    "authors": [
      "Minggui He",
      "Yilun Liu",
      "Shimin Tao",
      "Yuanchang Luo",
      "Hongyong Zeng",
      "Chang Su",
      "Li Zhang",
      "Hongxia Ma",
      "Daimeng Wei",
      "Weibin Meng",
      "Hao Yang",
      "Boxing Chen",
      "Osamu Yoshie"
    ],
    "published": "2025-02-27T03:57:00+00:00",
    "summary": "Despite recent breakthroughs in reasoning-enhanced large language models (LLMs) like DeepSeek-R1, incorporating inference-time reasoning into machine translation (MT), where human translators naturally employ structured, multi-layered reasoning chain-of-thoughts (CoTs), is yet underexplored. Existing methods either design a fixed CoT tailored for a specific MT sub-task (e.g., literature translation), or rely on synthesizing CoTs unaligned with humans and supervised fine-tuning (SFT) prone to catastrophic forgetting, limiting their adaptability to diverse translation scenarios. This paper introduces R1-Translator (R1-T1), a novel framework to achieve inference-time reasoning for general MT via reinforcement learning (RL) with human-aligned CoTs comprising six common patterns. Our approach pioneers three innovations: (1) extending reasoning-based translation beyond MT sub-tasks to six languages and diverse tasks (e.g., legal/medical domain adaptation, idiom resolution); (2) formalizing six expert-curated CoT templates that mirror hybrid human strategies like context-aware paraphrasing and back translation; and (3) enabling self-evolving CoT discovery and anti-forgetting adaptation through RL with KL-constrained rewards. Experimental results indicate a steady translation performance improvement in 21 languages and 80 translation directions on Flores-101 test set, especially on the 15 languages unseen from training, with its general multilingual abilities preserved compared with plain SFT."
  },
  {
    "title": "R1-T1: Fully Incentivizing Translation Capability in LLMs via Reasoning Learning",
    "url": "http://arxiv.org/abs/2502.19735v2",
    "arxiv_id": "2502.19735v2",
    "authors": [
      "Minggui He",
      "Yilun Liu",
      "Shimin Tao",
      "Yuanchang Luo",
      "Hongyong Zeng",
      "Chang Su",
      "Li Zhang",
      "Hongxia Ma",
      "Daimeng Wei",
      "Weibin Meng",
      "Hao Yang",
      "Boxing Chen",
      "Osamu Yoshie"
    ],
    "published": "2025-02-27T03:57:00+00:00",
    "summary": "Despite recent breakthroughs in reasoning-enhanced large language models (LLMs) like DeepSeek-R1, incorporating inference-time reasoning into machine translation (MT), where human translators naturally employ structured, multi-layered reasoning chain-of-thoughts (CoTs), is yet underexplored. Existing methods either design a fixed CoT tailored for a specific MT sub-task (e.g., literature translation), or rely on synthesizing CoTs unaligned with humans, limiting their adaptability to diverse translation scenarios. This paper introduces R1-Translator (R1-T1), a novel framework to achieve inference-time reasoning for general MT via reinforcement learning (RL) with human-aligned CoTs comprising six common patterns. Our approach pioneers three innovations: (1) extending reasoning-based translation beyond MT sub-tasks to six languages and diverse tasks (e.g., legal/medical domain adaptation, idiom resolution); (2) formalizing six expert-curated CoT templates that mirror hybrid human strategies like context-aware paraphrasing and back translation; and (3) enabling self-evolving CoT discovery through RL. Experimental results indicate a steady translation performance improvement in 11 languages and 40 translation directions on Flores-101 test set, especially on the languages unseen from training."
  },
  {
    "title": "Unveiling Security Weaknesses in Autonomous Driving Systems: An In-Depth Empirical Study",
    "url": "http://arxiv.org/abs/2502.19687v1",
    "arxiv_id": "2502.19687v1",
    "authors": [
      "Wenyuan Cheng",
      "Zengyang Li",
      "Peng Liang",
      "Ran Mo",
      "Hui Liu"
    ],
    "published": "2025-02-27T01:57:53+00:00",
    "summary": "The advent of Autonomous Driving Systems (ADS) has marked a significant shift towards intelligent transportation, with implications for public safety and traffic efficiency. While these systems integrate a variety of technologies and offer numerous benefits, their security is paramount, as vulnerabilities can have severe consequences for safety and trust. This study aims to systematically investigate potential security weaknesses in the codebases of prominent open-source ADS projects using CodeQL, a static code analysis tool. The goal is to identify common vulnerabilities, their distribution and persistence across versions to enhance the security of ADS. We selected three representative open-source ADS projects, Autoware, AirSim, and Apollo, based on their high GitHub star counts and Level 4 autonomous driving capabilities. Using CodeQL, we analyzed multiple versions of these projects to identify vulnerabilities, focusing on CWE categories such as CWE-190 (Integer Overflow or Wraparound) and CWE-20 (Improper Input Validation). We also tracked the lifecycle of these vulnerabilities across software versions. This approach allows us to systematically analyze vulnerabilities in projects, which has not been extensively explored in previous ADS research. Our analysis revealed that specific CWE categories, particularly CWE-190 (59.6%) and CWE-20 (16.1%), were prevalent across the selected ADS projects. These vulnerabilities often persisted for over six months, spanning multiple version iterations. The empirical assessment showed a direct link between the severity of these vulnerabilities and their tangible effects on ADS performance. These security issues among ADS still remain to be resolved. Our findings highlight the need for integrating static code analysis into ADS development to detect and mitigate common vulnerabilities."
  },
  {
    "title": "MICINet: Multi-Level Inter-Class Confusing Information Removal for Reliable Multimodal Classification",
    "url": "http://arxiv.org/abs/2502.19674v1",
    "arxiv_id": "2502.19674v1",
    "authors": [
      "Tong Zhang",
      "Shu Shen",
      "C. L. Philip Chen"
    ],
    "published": "2025-02-27T01:33:28+00:00",
    "summary": "Reliable multimodal learning in the presence of noisy data is a widely concerned issue, especially in safety-critical applications. Many reliable multimodal methods delve into addressing modality-specific or cross-modality noise. However, they fail to handle the coexistence of both types of noise efficiently. Moreover, the lack of comprehensive consideration for noise at both global and individual levels limits their reliability. To address these issues, a reliable multimodal classification method dubbed Multi-Level Inter-Class Confusing Information Removal Network (MICINet) is proposed. MICINet achieves the reliable removal of both types of noise by unifying them into the concept of Inter-class Confusing Information (\\textit{ICI}) and eliminating it at both global and individual levels. Specifically, MICINet first reliably learns the global \\textit{ICI} distribution through the proposed \\textbf{\\textit{Global \\textbf{ICI} Learning Module}}. Then, it introduces the \\textbf{\\textit{Global-guided Sample ICI Learning module}} to efficiently remove global-level \\textit{ICI} from sample features utilizing the learned global \\textit{ICI} distribution. Subsequently, the \\textbf{\\textit{Sample-adaptive Cross-modality Information Compensation module}} is designed to remove individual-level \\textit{ICI} from each sample reliably. This is achieved through interpretable cross-modality information compensation based on the complementary relationship between discriminative features and \\textit{ICI} and the perception of the relative quality of modalities introduced by the relative discriminative power. Experiments on four datasets demonstrate that MICINet outperforms other state-of-the-art reliable multimodal classification methods under various noise conditions."
  },
  {
    "title": "Med-RLVR: Emerging Medical Reasoning from a 3B base model via reinforcement Learning",
    "url": "http://arxiv.org/abs/2502.19655v1",
    "arxiv_id": "2502.19655v1",
    "authors": [
      "Sheng Zhang",
      "Qianchu Liu",
      "Guanghui Qin",
      "Tristan Naumann",
      "Hoifung Poon"
    ],
    "published": "2025-02-27T00:54:38+00:00",
    "summary": "Reinforcement learning from verifiable rewards (RLVR) has recently gained attention for its ability to elicit self-evolved reasoning capabilitie from base language models without explicit reasoning supervisions, as demonstrated by DeepSeek-R1. While prior work on RLVR has primarily focused on mathematical and coding domains, its applicability to other tasks and domains remains unexplored. In this work, we investigate whether medical reasoning can emerge from RLVR. We introduce Med-RLVR as an initial study of RLVR in the medical domain leveraging medical multiple-choice question answering (MCQA) data as verifiable labels. Our results demonstrate that RLVR is not only effective for math and coding but also extends successfully to medical question answering. Notably, Med-RLVR achieves performance comparable to traditional supervised fine-tuning (SFT) on in-distribution tasks while significantly improving out-of-distribution generalization, with an 8-point accuracy gain. Further analysis of training dynamics reveals that, with no explicit reasoning supervision, reasoning emerges from the 3B-parameter base model. These findings underscore the potential of RLVR in domains beyond math and coding, opening new avenues for its application in knowledge-intensive fields such as medicine."
  },
  {
    "title": "3D Nephrographic Image Synthesis in CT Urography with the Diffusion Model and Swin Transformer",
    "url": "http://arxiv.org/abs/2502.19623v1",
    "arxiv_id": "2502.19623v1",
    "authors": [
      "Hongkun Yu",
      "Syed Jamal Safdar Gardezi",
      "E. Jason Abel",
      "Daniel Shapiro",
      "Meghan G. Lubner",
      "Joshua Warner",
      "Matthew Smith",
      "Giuseppe Toia",
      "Lu Mao",
      "Pallavi Tiwari",
      "Andrew L. Wentland"
    ],
    "published": "2025-02-26T23:22:31+00:00",
    "summary": "Purpose: This study aims to develop and validate a method for synthesizing 3D nephrographic phase images in CT urography (CTU) examinations using a diffusion model integrated with a Swin Transformer-based deep learning approach. Materials and Methods: This retrospective study was approved by the local Institutional Review Board. A dataset comprising 327 patients who underwent three-phase CTU (mean $\\pm$ SD age, 63 $\\pm$ 15 years; 174 males, 153 females) was curated for deep learning model development. The three phases for each patient were aligned with an affine registration algorithm. A custom deep learning model coined dsSNICT (diffusion model with a Swin transformer for synthetic nephrographic phase images in CT) was developed and implemented to synthesize the nephrographic images. Performance was assessed using Peak Signal-to-Noise Ratio (PSNR), Structural Similarity Index (SSIM), Mean Absolute Error (MAE), and Fr\\'{e}chet Video Distance (FVD). Qualitative evaluation by two fellowship-trained abdominal radiologists was performed. Results: The synthetic nephrographic images generated by our proposed approach achieved high PSNR (26.3 $\\pm$ 4.4 dB), SSIM (0.84 $\\pm$ 0.069), MAE (12.74 $\\pm$ 5.22 HU), and FVD (1323). Two radiologists provided average scores of 3.5 for real images and 3.4 for synthetic images (P-value = 0.5) on a Likert scale of 1-5, indicating that our synthetic images closely resemble real images. Conclusion: The proposed approach effectively synthesizes high-quality 3D nephrographic phase images. This model can be used to reduce radiation dose in CTU by 33.3\\% without compromising image quality, which thereby enhances the safety and diagnostic utility of CT urography."
  },
  {
    "title": "No, of course I can! Refusal Mechanisms Can Be Exploited Using Harmless Fine-Tuning Data",
    "url": "http://arxiv.org/abs/2502.19537v1",
    "arxiv_id": "2502.19537v1",
    "authors": [
      "Joshua Kazdan",
      "Lisa Yu",
      "Rylan Schaeffer",
      "Chris Cundy",
      "Sanmi Koyejo",
      "Dvijotham Krishnamurthy"
    ],
    "published": "2025-02-26T20:20:01+00:00",
    "summary": "Leading language model (LM) providers like OpenAI and Google offer fine-tuning APIs that allow customers to adapt LMs for specific use cases. To prevent misuse, these LM providers implement filtering mechanisms to block harmful fine-tuning data. Consequently, adversaries seeking to produce unsafe LMs via these APIs must craft adversarial training data that are not identifiably harmful. We make three contributions in this context: 1. We show that many existing attacks that use harmless data to create unsafe LMs rely on eliminating model refusals in the first few tokens of their responses. 2. We show that such prior attacks can be blocked by a simple defense that pre-fills the first few tokens from an aligned model before letting the fine-tuned model fill in the rest. 3. We describe a new data-poisoning attack, ``No, Of course I Can Execute'' (NOICE), which exploits an LM's formulaic refusal mechanism to elicit harmful responses. By training an LM to refuse benign requests on the basis of safety before fulfilling those requests regardless, we are able to jailbreak several open-source models and a closed-source model (GPT-4o). We show an attack success rate (ASR) of 57% against GPT-4o; our attack earned a Bug Bounty from OpenAI. Against open-source models protected by simple defenses, we improve ASRs by an average of 3.25 times compared to the best performing previous attacks that use only harmless data. NOICE demonstrates the exploitability of repetitive refusal mechanisms and broadens understanding of the threats closed-source models face from harmless data."
  },
  {
    "title": "Can Language Models Falsify? Evaluating Algorithmic Reasoning with Counterexample Creation",
    "url": "http://arxiv.org/abs/2502.19414v1",
    "arxiv_id": "2502.19414v1",
    "authors": [
      "Shiven Sinha",
      "Shashwat Goel",
      "Ponnurangam Kumaraguru",
      "Jonas Geiping",
      "Matthias Bethge",
      "Ameya Prabhu"
    ],
    "published": "2025-02-26T18:58:13+00:00",
    "summary": "There is growing excitement about the potential of Language Models (LMs) to accelerate scientific discovery. Falsifying hypotheses is key to scientific progress, as it allows claims to be iteratively refined over time. This process requires significant researcher effort, reasoning, and ingenuity. Yet current benchmarks for LMs predominantly assess their ability to generate solutions rather than challenge them. We advocate for developing benchmarks that evaluate this inverse capability - creating counterexamples for subtly incorrect solutions. To demonstrate this approach, we start with the domain of algorithmic problem solving, where counterexamples can be evaluated automatically using code execution. Specifically, we introduce REFUTE, a dynamically updating benchmark that includes recent problems and incorrect submissions from programming competitions, where human experts successfully identified counterexamples. Our analysis finds that the best reasoning agents, even OpenAI o3-mini (high) with code execution feedback, can create counterexamples for only <9% of incorrect solutions in REFUTE, even though ratings indicate its ability to solve up to 48% of these problems from scratch. We hope our work spurs progress in evaluating and enhancing LMs' ability to falsify incorrect solutions - a capability that is crucial for both accelerating research and making models self-improve through reliable reflective reasoning."
  },
  {
    "title": "ARENA: Adaptive Risk-aware and Energy-efficient NAvigation for Multi-Objective 3D Infrastructure Inspection with a UAV",
    "url": "http://arxiv.org/abs/2502.19401v1",
    "arxiv_id": "2502.19401v1",
    "authors": [
      "David-Alexandre Poissant",
      "Alexis Lussier Desbiens",
      "Fran\u00e7ois Ferland",
      "Louis Petit"
    ],
    "published": "2025-02-26T18:50:49+00:00",
    "summary": "Autonomous robotic inspection missions require balancing multiple conflicting objectives while navigating near costly obstacles. Current multi-objective path planning (MOPP) methods struggle to adapt to evolving risks like localization errors, weather, battery state, and communication issues. This letter presents an Adaptive Risk-aware and Energy-efficient NAvigation (ARENA) MOPP approach for UAVs in complex 3D environments. Our method enables online trajectory adaptation by optimizing safety, time, and energy using 4D NURBS representation and a genetic-based algorithm to generate the Pareto front. A novel risk-aware voting algorithm ensures adaptivity. Simulations and real-world tests demonstrate the planner's ability to produce diverse, optimized trajectories covering 95% or more of the range defined by single-objective benchmarks and its ability to estimate power consumption with a mean error representing 14% of the full power range. The ARENA framework enhances UAV autonomy and reliability in critical, evolving 3D missions."
  },
  {
    "title": "TheoremExplainAgent: Towards Multimodal Explanations for LLM Theorem Understanding",
    "url": "http://arxiv.org/abs/2502.19400v1",
    "arxiv_id": "2502.19400v1",
    "authors": [
      "Max Ku",
      "Thomas Chong",
      "Jonathan Leung",
      "Krish Shah",
      "Alvin Yu",
      "Wenhu Chen"
    ],
    "published": "2025-02-26T18:50:09+00:00",
    "summary": "Understanding domain-specific theorems often requires more than just text-based reasoning; effective communication through structured visual explanations is crucial for deeper comprehension. While large language models (LLMs) demonstrate strong performance in text-based theorem reasoning, their ability to generate coherent and pedagogically meaningful visual explanations remains an open challenge. In this work, we introduce TheoremExplainAgent, an agentic approach for generating long-form theorem explanation videos (over 5 minutes) using Manim animations. To systematically evaluate multimodal theorem explanations, we propose TheoremExplainBench, a benchmark covering 240 theorems across multiple STEM disciplines, along with 5 automated evaluation metrics. Our results reveal that agentic planning is essential for generating detailed long-form videos, and the o3-mini agent achieves a success rate of 93.8% and an overall score of 0.77. However, our quantitative and qualitative studies show that most of the videos produced exhibit minor issues with visual element layout. Furthermore, multimodal explanations expose deeper reasoning flaws that text-based explanations fail to reveal, highlighting the importance of multimodal explanations."
  },
  {
    "title": "Can Large Language Models Detect Errors in Long Chain-of-Thought Reasoning?",
    "url": "http://arxiv.org/abs/2502.19361v1",
    "arxiv_id": "2502.19361v1",
    "authors": [
      "Yancheng He",
      "Shilong Li",
      "Jiaheng Liu",
      "Weixun Wang",
      "Xingyuan Bu",
      "Ge Zhang",
      "Zhongyuan Peng",
      "Zhaoxiang Zhang",
      "Wenbo Su",
      "Bo Zheng"
    ],
    "published": "2025-02-26T17:59:27+00:00",
    "summary": "Recently, o1-like models have drawn significant attention, where these models produce the long Chain-of-Thought (CoT) reasoning steps to improve the reasoning abilities of existing Large Language Models (LLMs). In this paper, to understand the qualities of these long CoTs and measure the critique abilities of existing LLMs on these long CoTs, we introduce the DeltaBench, including the generated long CoTs from different o1-like models (e.g., QwQ, DeepSeek-R1) for different reasoning tasks (e.g., Math, Code, General Reasoning), to measure the ability to detect errors in long CoT reasoning. Based on DeltaBench, we first perform fine-grained analysis of the generated long CoTs to discover the effectiveness and efficiency of different o1-like models. Then, we conduct extensive evaluations of existing process reward models (PRMs) and critic models to detect the errors of each annotated process, which aims to investigate the boundaries and limitations of existing PRMs and critic models. Finally, we hope that DeltaBench could guide developers to better understand the long CoT reasoning abilities of their models."
  },
  {
    "title": "Can Large Language Models Detect Errors in Long Chain-of-Thought Reasoning?",
    "url": "http://arxiv.org/abs/2502.19361v2",
    "arxiv_id": "2502.19361v2",
    "authors": [
      "Yancheng He",
      "Shilong Li",
      "Jiaheng Liu",
      "Weixun Wang",
      "Xingyuan Bu",
      "Ge Zhang",
      "Zhongyuan Peng",
      "Zhaoxiang Zhang",
      "Zhicheng Zheng",
      "Wenbo Su",
      "Bo Zheng"
    ],
    "published": "2025-02-26T17:59:27+00:00",
    "summary": "Recently, o1-like models have drawn significant attention, where these models produce the long Chain-of-Thought (CoT) reasoning steps to improve the reasoning abilities of existing Large Language Models (LLMs). In this paper, to understand the qualities of these long CoTs and measure the critique abilities of existing LLMs on these long CoTs, we introduce the DeltaBench, including the generated long CoTs from different o1-like models (e.g., QwQ, DeepSeek-R1) for different reasoning tasks (e.g., Math, Code, General Reasoning), to measure the ability to detect errors in long CoT reasoning. Based on DeltaBench, we first perform fine-grained analysis of the generated long CoTs to discover the effectiveness and efficiency of different o1-like models. Then, we conduct extensive evaluations of existing process reward models (PRMs) and critic models to detect the errors of each annotated process, which aims to investigate the boundaries and limitations of existing PRMs and critic models. Finally, we hope that DeltaBench could guide developers to better understand the long CoT reasoning abilities of their models."
  },
  {
    "title": "Anomaly Detection in Complex Dynamical Systems: A Systematic Framework Using Embedding Theory and Physics-Inspired Consistency",
    "url": "http://arxiv.org/abs/2502.19307v1",
    "arxiv_id": "2502.19307v1",
    "authors": [
      "Michael Somma",
      "Thomas Gallien",
      "Branka Stojanovic"
    ],
    "published": "2025-02-26T17:06:13+00:00",
    "summary": "Anomaly detection in complex dynamical systems is essential for ensuring reliability, safety, and efficiency in industrial and cyber-physical infrastructures. Predictive maintenance helps prevent costly failures, while cybersecurity monitoring has become critical as digitized systems face growing threats. Many of these systems exhibit oscillatory behaviors and bounded motion, requiring anomaly detection methods that capture structured temporal dependencies while adhering to physical consistency principles. In this work, we propose a system-theoretic approach to anomaly detection, grounded in classical embedding theory and physics-inspired consistency principles. We build upon the Fractal Whitney Embedding Prevalence Theorem, extending traditional embedding techniques to complex system dynamics. Additionally, we introduce state-derivative pairs as an embedding strategy to capture system evolution. To enforce temporal coherence, we develop a Temporal Differential Consistency Autoencoder (TDC-AE), incorporating a TDC-Loss that aligns the approximated derivatives of latent variables with their dynamic representations. We evaluate our method on the C-MAPSS dataset, a benchmark for turbofan aeroengine degradation. TDC-AE outperforms LSTMs and Transformers while achieving a 200x reduction in MAC operations, making it particularly suited for lightweight edge computing. Our findings support the hypothesis that anomalies disrupt stable system dynamics, providing a robust, interpretable signal for anomaly detection."
  },
  {
    "title": "Utility-Based Dose Optimization Approaches for Multiple-Dose Randomized Trial Designs Accounting for Multiple Endpoints",
    "url": "http://arxiv.org/abs/2502.19216v1",
    "arxiv_id": "2502.19216v1",
    "authors": [
      "Gina DAngelo",
      "Guannan Chen",
      "Di Ran"
    ],
    "published": "2025-02-26T15:18:17+00:00",
    "summary": "The initiation of dose optimization has driven a paradigm shift in oncology clinical trials to determine the optimal biological dose (OBD). Early-phase trials with randomized doses can facilitate additional investigation of the identified OBD in targeted populations by incorporating safety, efficacy, and biomarker data. To support dose comparison in such settings, we propose to extend the utility score-based approach (U-MET) and introduce the clinical utility index-based approach (CUI-MET) to account for multiple endpoints and doses. The utility-based dose optimization approach for multiple-dose randomized trial designs accounting for multiple endpoints and doses (U-MET-m) extends the U-MET, using a utility score to account for multiple endpoints jointly (e.g., toxicity-efficacy trade-off), while the CUI-MET uses a utility index to do this marginally. U-MET-m and CUI-MET use Bayesian inference within a hypothesis framework to compare utility metrics across doses to identify the OBD. Here we describe simulation studies and present an example to compare the U-MET-m design, CUI-MET, and empirical design. The U-MET-m design and CUI-MET were shown to have satisfactory operating characteristics for selecting the OBD. Based on these findings, we recommend using the U-MET-m and CUI-MET designs as the primary dose comparison approach or as supportive evidence to select the OBD."
  },
  {
    "title": "When Personalization Meets Reality: A Multi-Faceted Analysis of Personalized Preference Learning",
    "url": "http://arxiv.org/abs/2502.19158v1",
    "arxiv_id": "2502.19158v1",
    "authors": [
      "Yijiang River Dong",
      "Tiancheng Hu",
      "Yinhong Liu",
      "Ahmet \u00dcst\u00fcn",
      "Nigel Collier"
    ],
    "published": "2025-02-26T14:14:58+00:00",
    "summary": "While Reinforcement Learning from Human Feedback (RLHF) is widely used to align Large Language Models (LLMs) with human preferences, it typically assumes homogeneous preferences across users, overlooking diverse human values and minority viewpoints. Although personalized preference learning addresses this by tailoring separate preferences for individual users, the field lacks standardized methods to assess its effectiveness. We present a multi-faceted evaluation framework that measures not only performance but also fairness, unintended effects, and adaptability across varying levels of preference divergence. Through extensive experiments comparing eight personalization methods across three preference datasets, we demonstrate that performance differences between methods could reach 36% when users strongly disagree, and personalization can introduce up to 20% safety misalignment. These findings highlight the critical need for holistic evaluation approaches to advance the development of more effective and inclusive preference learning systems."
  },
  {
    "title": "Formal Verification of PLCs as a Service: A CERN-GSI Safety-Critical Case Study (extended version)",
    "url": "http://arxiv.org/abs/2502.19150v1",
    "arxiv_id": "2502.19150v1",
    "authors": [
      "Ignacio D. Lopez-Miguel",
      "Borja Fern\u00e1ndez Adiego",
      "Matias Salinas",
      "Christine Betz"
    ],
    "published": "2025-02-26T14:08:58+00:00",
    "summary": "The increased technological complexity and demand for software reliability require organizations to formally design and verify their safety-critical programs to minimize systematic failures. Formal methods are recommended by functional safety standards (e.g., by IEC 61511 for the process industry and by the generic IEC 61508) and play a crucial role. Their structured approach reduces ambiguity in system requirements, facilitating early error detection. This paper introduces a formal verification service for PLC (programmable logic controller) programs compliant with functional safety standards, providing external expertise to organizations while eliminating the need for extensive internal training. It offers a cost-effective solution to meet the rising demands for formal verification processes. The approach is extended to include modeling time-dependent, know-how-protected components, enabling formal verification of real safety-critical applications. A case study shows the application of PLC formal verification as a service provided by CERN in a safety-critical installation at the GSI particle accelerator facility."
  },
  {
    "title": "Multi-Agent Security Tax: Trading Off Security and Collaboration Capabilities in Multi-Agent Systems",
    "url": "http://arxiv.org/abs/2502.19145v1",
    "arxiv_id": "2502.19145v1",
    "authors": [
      "Pierre Peigne-Lefebvre",
      "Mikolaj Kniejski",
      "Filip Sondej",
      "Matthieu David",
      "Jason Hoelscher-Obermaier",
      "Christian Schroeder de Witt",
      "Esben Kran"
    ],
    "published": "2025-02-26T14:00:35+00:00",
    "summary": "As AI agents are increasingly adopted to collaborate on complex objectives, ensuring the security of autonomous multi-agent systems becomes crucial. We develop simulations of agents collaborating on shared objectives to study these security risks and security trade-offs. We focus on scenarios where an attacker compromises one agent, using it to steer the entire system toward misaligned outcomes by corrupting other agents. In this context, we observe infectious malicious prompts - the multi-hop spreading of malicious instructions. To mitigate this risk, we evaluated several strategies: two \"vaccination\" approaches that insert false memories of safely handling malicious input into the agents' memory stream, and two versions of a generic safety instruction strategy. While these defenses reduce the spread and fulfillment of malicious instructions in our experiments, they tend to decrease collaboration capability in the agent network. Our findings illustrate potential trade-off between security and collaborative efficiency in multi-agent systems, providing insights for designing more secure yet effective AI collaborations."
  },
  {
    "title": "A 106K Multi-Topic Multilingual Conversational User Dataset with Emoticons",
    "url": "http://arxiv.org/abs/2502.19108v1",
    "arxiv_id": "2502.19108v1",
    "authors": [
      "Heng Er Metilda Chee",
      "Jiayin Wang",
      "Zhiqiang Guo",
      "Weizhi Ma",
      "Qinglang Guo",
      "Min Zhang"
    ],
    "published": "2025-02-26T12:50:58+00:00",
    "summary": "Instant messaging has become a predominant form of communication, with texts and emoticons enabling users to express emotions and ideas efficiently. Emoticons, in particular, have gained significant traction as a medium for conveying sentiments and information, leading to the growing importance of emoticon retrieval and recommendation systems. However, one of the key challenges in this area has been the absence of datasets that capture both the temporal dynamics and user-specific interactions with emoticons, limiting the progress of personalized user modeling and recommendation approaches. To address this, we introduce the emoticon dataset, a comprehensive resource that includes time-based data along with anonymous user identifiers across different conversations. As the largest publicly accessible emoticon dataset to date, it comprises 22K unique users, 370K emoticons, and 8.3M messages. The data was collected from a widely-used messaging platform across 67 conversations and 720 hours of crawling. Strict privacy and safety checks were applied to ensure the integrity of both text and image data. Spanning across 10 distinct domains, the emoticon dataset provides rich insights into temporal, multilingual, and cross-domain behaviors, which were previously unavailable in other emoticon-based datasets. Our in-depth experiments, both quantitative and qualitative, demonstrate the dataset's potential in modeling user behavior and personalized recommendation systems, opening up new possibilities for research in personalized retrieval and conversational AI. The dataset is freely accessible."
  },
  {
    "title": "Nexus: A Lightweight and Scalable Multi-Agent Framework for Complex Tasks Automation",
    "url": "http://arxiv.org/abs/2502.19091v1",
    "arxiv_id": "2502.19091v1",
    "authors": [
      "Humza Sami",
      "Mubashir ul Islam",
      "Samy Charas",
      "Asav Gandhi",
      "Pierre-Emmanuel Gaillardon",
      "Valerio Tenace"
    ],
    "published": "2025-02-26T12:37:47+00:00",
    "summary": "Recent advancements in Large Language Models (LLMs) have substantially evolved Multi-Agent Systems (MASs) capabilities, enabling systems that not only automate tasks but also leverage near-human reasoning capabilities. To achieve this, LLM-based MASs need to be built around two critical principles: (i) a robust architecture that fully exploits LLM potential for specific tasks -- or related task sets -- and ($ii$) an effective methodology for equipping LLMs with the necessary capabilities to perform tasks and manage information efficiently. It goes without saying that a priori architectural designs can limit the scalability and domain adaptability of a given MAS.   To address these challenges, in this paper we introduce Nexus: a lightweight Python framework designed to easily build and manage LLM-based MASs. Nexus introduces the following innovations: (i) a flexible multi-supervisor hierarchy, (ii) a simplified workflow design, and (iii) easy installation and open-source flexibility: Nexus can be installed via pip and is distributed under a permissive open-source license, allowing users to freely modify and extend its capabilities.   Experimental results demonstrate that architectures built with Nexus exhibit state-of-the-art performance across diverse domains. In coding tasks, Nexus-driven MASs achieve a 99% pass rate on HumanEval and a flawless 100% on VerilogEval-Human, outperforming cutting-edge reasoning language models such as o3-mini and DeepSeek-R1. Moreover, these architectures display robust proficiency in complex reasoning and mathematical problem solving, achieving correct solutions for all randomly selected problems from the MATH dataset. In the realm of multi-objective optimization, Nexus-based architectures successfully address challenging timing closure tasks on designs from the VTR benchmark suite, while guaranteeing, on average, a power saving of nearly 30%."
  },
  {
    "title": "MathClean: A Benchmark for Synthetic Mathematical Data Cleaning",
    "url": "http://arxiv.org/abs/2502.19058v1",
    "arxiv_id": "2502.19058v1",
    "authors": [
      "Hao Liang",
      "Meiyi Qiang",
      "Yuying Li",
      "Zefeng He",
      "Yongzhen Guo",
      "Zhengzhou Zhu",
      "Wentao Zhang",
      "Bin Cui"
    ],
    "published": "2025-02-26T11:17:50+00:00",
    "summary": "With the rapid development of large language models (LLMs), the quality of training data has become crucial. Among the various types of training data, mathematical data plays a key role in enabling LLMs to acquire strong reasoning abilities. While high-quality open-source data is important, it is often insufficient for pre-training, necessitating the addition of synthetic math problems. However, synthetic math questions and answers can introduce inaccuracies, which may degrade both the training data and web data. Therefore, an effective method for cleaning synthetic math data is essential. In this paper, we propose the MathClean benchmark to evaluate the effectiveness of math data cleaning models. The MathClean benchmark consists of 2,000 correct questions and 2,000 erroneous questions with additional 2,000 correct and erroneous answers sourced from augmented data based on GSM8K and MATH. Moreover, we also annotate error types for each question or answer, since it can assess whether models can correctly identify the error categories for future improvements. Finally, we present comprehensive evaluations using state-of-the-art (SOTA) models. Our results demonstrate that even strong models like GPT-o1 and DeepSeek-R1 perform poorly on this benchmark, highlighting the utility of MathClean. Our code and data is available at https://github.com/YuYingLi0/MathClean."
  },
  {
    "title": "An Improved 3D Skeletons UP-Fall Dataset: Enhancing Data Quality for Efficient Impact Fall Detection",
    "url": "http://arxiv.org/abs/2502.19048v1",
    "arxiv_id": "2502.19048v1",
    "authors": [
      "Tresor Y. Koffi",
      "Youssef Mourchid",
      "Mohammed Hindawi",
      "Yohan Dupuis"
    ],
    "published": "2025-02-26T11:02:44+00:00",
    "summary": "Detecting impact where an individual makes contact with the ground within a fall event is crucial in fall detection systems, particularly for elderly care where prompt intervention can prevent serious injuries. The UP-Fall dataset, a key resource in fall detection research, has proven valuable but suffers from limitations in data accuracy and comprehensiveness. These limitations cause confusion in distinguishing between non-impact events, such as sliding, and real falls with impact, where the person actually hits the ground. This confusion compromises the effectiveness of current fall detection systems. This study presents enhancements to the UP-Fall dataset aiming at improving it for impact fall detection by incorporating 3D skeleton data. Our preprocessing techniques ensure high data accuracy and comprehensiveness, enabling a more reliable impact fall detection. Extensive experiments were conducted using various machine learning and deep learning algorithms to benchmark the improved 3D skeletons dataset. The results demonstrate substantial improvements in the performance of fall detection models trained on the enhanced dataset. This contribution aims to enhance the safety and well-being of the elderly population at risk. To support further research and development of building more reliable impact fall detection systems, we have made the improved 3D skeletons UP-Fall dataset publicly available at this link https://zenodo.org/records/12773013."
  },
  {
    "title": "Beyond Surface-Level Patterns: An Essence-Driven Defense Framework Against Jailbreak Attacks in LLMs",
    "url": "http://arxiv.org/abs/2502.19041v1",
    "arxiv_id": "2502.19041v1",
    "authors": [
      "Shiyu Xiang",
      "Ansen Zhang",
      "Yanfei Cao",
      "Yang Fan",
      "Ronghao Chen"
    ],
    "published": "2025-02-26T10:53:58+00:00",
    "summary": "Although Aligned Large Language Models (LLMs) are trained to refuse harmful requests, they remain vulnerable to jailbreak attacks. Unfortunately, existing methods often focus on surface-level patterns, overlooking the deeper attack essences. As a result, defenses fail when attack prompts change, even though the underlying \"attack essence\" remains the same. To address this issue, we introduce EDDF, an \\textbf{E}ssence-\\textbf{D}riven \\textbf{D}efense \\textbf{F}ramework Against Jailbreak Attacks in LLMs. EDDF is a plug-and-play input-filtering method and operates in two stages: 1) offline essence database construction, and 2) online adversarial query detection. The key idea behind EDDF is to extract the \"attack essence\" from a diverse set of known attack instances and store it in an offline vector database. Experimental results demonstrate that EDDF significantly outperforms existing methods by reducing the Attack Success Rate by at least 20\\%, underscoring its superior robustness against jailbreak attacks."
  },
  {
    "title": "Polarization Angle Scanning for Wide-band Millimeter-wave Direct Detection",
    "url": "http://arxiv.org/abs/2502.18981v1",
    "arxiv_id": "2502.18981v1",
    "authors": [
      "Heyao Wang",
      "Ziran Zhao",
      "Lingbo Qiao",
      "Dalu Guo"
    ],
    "published": "2025-02-26T09:43:09+00:00",
    "summary": "Millimeter-wave (MMW) technology has been widely utilized in human security screening applications due to its superior penetration capabilities through clothing and safety for human exposure. However, existing methods largely rely on fixed polarization modes, neglecting the potential insights from variations in target echoes with respect to incident polarization. This study provides a theoretical analysis of the cross-polarization echo power as a function of the incident polarization angle under linear polarization conditions. Additionally, based on the transmission characteristics of multi-layer medium, we extended the depth spectrum model employed in direct detection to accommodate scenarios involving multi-layered structures. Building on this foundation, by obtaining multiple depth spectrums through polarization angle scanning, we propose the Polarization Angle-Depth Matrix to characterize target across both the polarization angle and depth dimensions in direct detection. Simulations and experimental validations confirm its accuracy and practical value in detecting concealed weapons in human security screening scenarios."
  },
  {
    "title": "JailBench: A Comprehensive Chinese Security Assessment Benchmark for Large Language Models",
    "url": "http://arxiv.org/abs/2502.18935v1",
    "arxiv_id": "2502.18935v1",
    "authors": [
      "Shuyi Liu",
      "Simiao Cui",
      "Haoran Bu",
      "Yuming Shang",
      "Xi Zhang"
    ],
    "published": "2025-02-26T08:36:42+00:00",
    "summary": "Large language models (LLMs) have demonstrated remarkable capabilities across various applications, highlighting the urgent need for comprehensive safety evaluations. In particular, the enhanced Chinese language proficiency of LLMs, combined with the unique characteristics and complexity of Chinese expressions, has driven the emergence of Chinese-specific benchmarks for safety assessment. However, these benchmarks generally fall short in effectively exposing LLM safety vulnerabilities. To address the gap, we introduce JailBench, the first comprehensive Chinese benchmark for evaluating deep-seated vulnerabilities in LLMs, featuring a refined hierarchical safety taxonomy tailored to the Chinese context. To improve generation efficiency, we employ a novel Automatic Jailbreak Prompt Engineer (AJPE) framework for JailBench construction, which incorporates jailbreak techniques to enhance assessing effectiveness and leverages LLMs to automatically scale up the dataset through context-learning. The proposed JailBench is extensively evaluated over 13 mainstream LLMs and achieves the highest attack success rate against ChatGPT compared to existing Chinese benchmarks, underscoring its efficacy in identifying latent vulnerabilities in LLMs, as well as illustrating the substantial room for improvement in the security and trustworthiness of LLMs within the Chinese context. Our benchmark is publicly available at https://github.com/STAIR-BUPT/JailBench."
  },
  {
    "title": "Adaptive Shielding via Parametric Safety Proofs",
    "url": "http://arxiv.org/abs/2502.18879v1",
    "arxiv_id": "2502.18879v1",
    "authors": [
      "Yao Feng",
      "Jun Zhu",
      "Andr\u00e9 Platzer",
      "Jonathan Laurent"
    ],
    "published": "2025-02-26T06:50:48+00:00",
    "summary": "A major challenge to deploying cyber-physical systems with learning-enabled controllers is to ensure their safety, especially in the face of changing environments that necessitate runtime knowledge acquisition. Model-checking and automated reasoning have been successfully used for shielding, i.e., to monitor untrusted controllers and override potentially unsafe decisions, but only at the cost of hard tradeoffs in terms of expressivity, safety, adaptivity, precision and runtime efficiency. We propose a programming-language framework that allows experts to statically specify adaptive shields for learning-enabled agents, which enforce a safe control envelope that gets more permissive as knowledge is gathered at runtime. A shield specification provides a safety model that is parametric in the current agent's knowledge. In addition, a nondeterministic inference strategy can be specified using a dedicated domain-specific language, enforcing that such knowledge parameters are inferred at runtime in a statistically-sound way. By leveraging language design and theorem proving, our proposed framework empowers experts to design adaptive shields with an unprecedented level of modeling flexibility, while providing rigorous, end-to-end probabilistic safety guarantees."
  },
  {
    "title": "Investigating Generalization of One-shot LLM Steering Vectors",
    "url": "http://arxiv.org/abs/2502.18862v1",
    "arxiv_id": "2502.18862v1",
    "authors": [
      "Jacob Dunefsky",
      "Arman Cohan"
    ],
    "published": "2025-02-26T06:13:01+00:00",
    "summary": "Steering vectors have emerged as a promising approach for interpreting and controlling LLMs, but current methods typically require large contrastive datasets that are often impractical to construct and may capture spurious correlations. We propose directly optimizing steering vectors through gradient descent on a single training example, and systematically investigate how these vectors generalize. We consider several steering optimization techniques, including multiple novel ones, and find that the resulting vectors effectively mediate safety-relevant behaviors in multiple models. Indeed, in experiments on an alignment-faking model, we are able to optimize one-shot steering vectors that induce harmful behavior on benign examples and whose negations suppress harmful behavior on malign examples. And in experiments on refusal suppression, we demonstrate that one-shot optimized steering vectors can transfer across inputs, yielding a Harmbench attack success rate of 96.9%. Furthermore, to quantitatively assess steering effectiveness in instruction-tuned models, we develop a novel evaluation framework using sequence probabilities from the corresponding base model. With this framework, we analyze how steering vectors modulate an instruction-tuned LLM's ability to recover from outputting false information, and find that this ability derives from the base model. Overall, our findings suggest that optimizing steering vectors on a single example can mediate misaligned behavior in LLMs, and provide a path toward better understanding the relationship between LLM behavior and activation space structure."
  },
  {
    "title": "Safe and usable kernel extensions with Rax",
    "url": "http://arxiv.org/abs/2502.18832v1",
    "arxiv_id": "2502.18832v1",
    "authors": [
      "Jinghao Jia",
      "Ruowen Qin",
      "Milo Craun",
      "Egor Lukiyanov",
      "Ayush Bansal",
      "Michael V. Le",
      "Hubertus Franke",
      "Hani Jamjoom",
      "Tianyin Xu",
      "Dan Williams"
    ],
    "published": "2025-02-26T05:16:06+00:00",
    "summary": "Safe kernel extensions have gained significant traction, evolving from simple packet filters to large, complex programs that customize storage, networking, and scheduling. Existing kernel extension mechanisms like eBPF rely on in-kernel verifiers to ensure safety of kernel extensions by static verification using symbolic execution. We identify significant usability issues -- safe extensions being rejected by the verifier -- due to the language-verifier gap, a mismatch between developers' expectation of program safety provided by a contract with the programming language, and the verifier's expectation.   We present Rax, a new kernel extension framework that closes the language-verifier gap and improves the usability of kernel extensions in terms of programming experience and maintainability. Rax builds upon language-based safety to provide safety properties desired by kernel extensions, along with a lightweight extralingual runtime for properties that are unsuitable for static analysis, including safe exception handling, stack safety, and termination. With Rax, kernel extensions are written in safe Rust and interact with the kernel via a safe interface provided by Rax's kernel crate. No separate static verification is needed. Rax addresses usability issues of eBPF kernel extensions without compromising performance."
  },
  {
    "title": "Simulating Safe Bite Transfer in Robot-Assisted Feeding with a Soft Head and Articulated Jaw",
    "url": "http://arxiv.org/abs/2502.18749v1",
    "arxiv_id": "2502.18749v1",
    "authors": [
      "Yi Heng San",
      "Vasanthamaran Ravichandram",
      "J-Anne Yow",
      "Sherwin Stephen Chan",
      "Yifan Wang",
      "Wei Tech Ang"
    ],
    "published": "2025-02-26T01:52:04+00:00",
    "summary": "Ensuring safe and comfortable bite transfer during robot-assisted feeding is challenging due to the close physical human-robot interaction required. This paper presents a novel approach to modeling physical human-robot interaction in a physics-based simulator (MuJoCo) using soft-body dynamics. We integrate a flexible head model with a rigid skeleton while accounting for internal dynamics, enabling the flexible model to be actuated by the skeleton. Incorporating realistic soft-skin contact dynamics in simulation allows for systematically evaluating bite transfer parameters, such as insertion depth and entry angle, and their impact on user safety and comfort. Our findings suggest that a straight-in-straight-out strategy minimizes forces and enhances user comfort in robot-assisted feeding, assuming a static head. This simulation-based approach offers a safer and more controlled alternative to real-world experimentation. Supplementary videos can be found at: https://tinyurl.com/224yh2kx."
  },
  {
    "title": "AI Mismatches: Identifying Potential Algorithmic Harms Before AI Development",
    "url": "http://arxiv.org/abs/2502.18682v1",
    "arxiv_id": "2502.18682v1",
    "authors": [
      "Devansh Saxena",
      "Ji-Youn Jung",
      "Jodi Forlizzi",
      "Kenneth Holstein",
      "John Zimmerman"
    ],
    "published": "2025-02-25T22:43:00+00:00",
    "summary": "AI systems are often introduced with high expectations, yet many fail to deliver, resulting in unintended harm and missed opportunities for benefit. We frequently observe significant \"AI Mismatches\", where the system's actual performance falls short of what is needed to ensure safety and co-create value. These mismatches are particularly difficult to address once development is underway, highlighting the need for early-stage intervention. Navigating complex, multi-dimensional risk factors that contribute to AI Mismatches is a persistent challenge. To address it, we propose an AI Mismatch approach to anticipate and mitigate risks early on, focusing on the gap between realistic model performance and required task performance. Through an analysis of 774 AI cases, we extracted a set of critical factors, which informed the development of seven matrices that map the relationships between these factors and highlight high-risk areas. Through case studies, we demonstrate how our approach can help reduce risks in AI development."
  },
  {
    "title": "Provably Efficient RL for Linear MDPs under Instantaneous Safety Constraints in Non-Convex Feature Spaces",
    "url": "http://arxiv.org/abs/2502.18655v1",
    "arxiv_id": "2502.18655v1",
    "authors": [
      "Amirhossein Roknilamouki",
      "Arnob Ghosh",
      "Ming Shi",
      "Fatemeh Nourzad",
      "Eylem Ekici",
      "Ness B. Shroff"
    ],
    "published": "2025-02-25T21:32:55+00:00",
    "summary": "In Reinforcement Learning (RL), tasks with instantaneous hard constraints present significant challenges, particularly when the decision space is non-convex or non-star-convex. This issue is especially relevant in domains like autonomous vehicles and robotics, where constraints such as collision avoidance often take a non-convex form. In this paper, we establish a regret bound of $\\tilde{\\mathcal{O}}\\bigl(\\bigl(1 + \\tfrac{1}{\\tau}\\bigr) \\sqrt{\\log(\\tfrac{1}{\\tau}) d^3 H^4 K} \\bigr)$, applicable to both star-convex and non-star-convex cases, where $d$ is the feature dimension, $H$ the episode length, $K$ the number of episodes, and $\\tau$ the safety threshold. Moreover, the violation of safety constraints is zero with high probability throughout the learning process. A key technical challenge in these settings is bounding the covering number of the value-function class, which is essential for achieving value-aware uniform concentration in model-free function approximation. For the star-convex setting, we develop a novel technique called Objective Constraint-Decomposition (OCD) to properly bound the covering number. This result also resolves an error in a previous work on constrained RL. In non-star-convex scenarios, where the covering number can become infinitely large, we propose a two-phase algorithm, Non-Convex Safe Least Squares Value Iteration (NCS-LSVI), which first reduces uncertainty about the safe set by playing a known safe policy. After that, it carefully balances exploration and exploitation to achieve the regret bound. Finally, numerical simulations on an autonomous driving scenario demonstrate the effectiveness of NCS-LSVI."
  },
  {
    "title": "Faster, Cheaper, Better: Multi-Objective Hyperparameter Optimization for LLM and RAG Systems",
    "url": "http://arxiv.org/abs/2502.18635v1",
    "arxiv_id": "2502.18635v1",
    "authors": [
      "Matthew Barker",
      "Andrew Bell",
      "Evan Thomas",
      "James Carr",
      "Thomas Andrews",
      "Umang Bhatt"
    ],
    "published": "2025-02-25T20:52:06+00:00",
    "summary": "While Retrieval Augmented Generation (RAG) has emerged as a popular technique for improving Large Language Model (LLM) systems, it introduces a large number of choices, parameters and hyperparameters that must be made or tuned. This includes the LLM, embedding, and ranker models themselves, as well as hyperparameters governing individual RAG components. Yet, collectively optimizing the entire configuration in a RAG or LLM system remains under-explored - especially in multi-objective settings - due to intractably large solution spaces, noisy objective evaluations, and the high cost of evaluations. In this work, we introduce the first approach for multi-objective parameter optimization of cost, latency, safety and alignment over entire LLM and RAG systems. We find that Bayesian optimization methods significantly outperform baseline approaches, obtaining a superior Pareto front on two new RAG benchmark tasks. We conclude our work with important considerations for practitioners who are designing multi-objective RAG systems, highlighting nuances such as how optimal configurations may not generalize across tasks and objectives."
  },
  {
    "title": "Autonomous Vision-Guided Resection of Central Airway Obstruction",
    "url": "http://arxiv.org/abs/2502.18586v1",
    "arxiv_id": "2502.18586v1",
    "authors": [
      "M. E. Smith",
      "N. Yilmaz",
      "T. Watts",
      "P. M. Scheikl",
      "J. Ge",
      "A. Deguet",
      "A. Kuntz",
      "A. Krieger"
    ],
    "published": "2025-02-25T19:11:11+00:00",
    "summary": "Existing tracheal tumor resection methods often lack the precision required for effective airway clearance, and robotic advancements offer new potential for autonomous resection. We present a vision-guided, autonomous approach for palliative resection of tracheal tumors. This system models the tracheal surface with a fifth-degree polynomial to plan tool trajectories, while a custom Faster R-CNN segmentation pipeline identifies the trachea and tumor boundaries. The electrocautery tool angle is optimized using handheld surgical demonstrations, and trajectories are planned to maintain a 1 mm safety clearance from the tracheal surface. We validated the workflow successfully in five consecutive experiments on ex-vivo animal tissue models, successfully clearing the airway obstruction without trachea perforation in all cases (with more than 90% volumetric tumor removal). These results support the feasibility of an autonomous resection platform, paving the way for future developments in minimally-invasive autonomous resection."
  },
  {
    "title": "Evaluating the Effectiveness of Small Language Models in Detecting Refactoring Bugs",
    "url": "http://arxiv.org/abs/2502.18454v1",
    "arxiv_id": "2502.18454v1",
    "authors": [
      "Rohit Gheyi",
      "Marcio Ribeiro",
      "Jonhnanthan Oliveira"
    ],
    "published": "2025-02-25T18:52:28+00:00",
    "summary": "Popular IDEs frequently contain bugs in their refactoring implementations. Ensuring that a transformation preserves a program's behavior is a complex task. Traditional detection methods rely on predefined preconditions for each refactoring type, limiting their scalability and adaptability to new transformations. These methods often require extensive static and dynamic analyses, which are computationally expensive, time-consuming, and may still fail to detect certain refactoring bugs. This study evaluates the effectiveness of Small Language Models (SLMs) in detecting two types of refactoring bugs in Java and Python: (i) transformations that introduce errors or behavioral changes (Type I) and (ii) transformations unnecessarily blocked by IDEs despite being valid (Type II). We assess whether Llama 3.2 3B, Mistral 7B, Gemma 2 9B, DeepSeek-R1 14B, Phi-4 14B, o1-mini, and o3-mini-high can accurately detect 100 refactoring bugs reported in widely used Java and Python IDEs, such as Eclipse and NetBeans. The study covers 16 refactoring types and employs zero-shot prompting on consumer-grade hardware to evaluate the models' ability to reason about refactoring correctness without explicit prior training. The proprietary o3-mini-high model achieved the highest detection rate, identifying 84.3% of Type I bugs. The open-source Phi-4 14B performed comparably well, demonstrating strong effectiveness across both bug types. However, o3-mini-high struggled with Type II bugs, correctly identifying and applying valid but blocked transformations in only 40% of cases. The findings highlight the potential of SLMs for efficiently detecting refactoring bugs, particularly in verifying behavioral changes. Additionally, SLMs offer a more adaptable solution capable of generalizing across different refactoring types and programming languages, addressing key limitations of traditional approaches."
  },
  {
    "title": "Application of Attention Mechanism with Bidirectional Long Short-Term Memory (BiLSTM) and CNN for Human Conflict Detection using Computer Vision",
    "url": "http://arxiv.org/abs/2502.18555v1",
    "arxiv_id": "2502.18555v1",
    "authors": [
      "Erick da Silva Farias",
      "Eduardo Palhares Junior"
    ],
    "published": "2025-02-25T18:48:34+00:00",
    "summary": "The automatic detection of human conflicts through videos is a crucial area in computer vision, with significant applications in monitoring and public safety policies. However, the scarcity of public datasets and the complexity of human interactions make this task challenging. This study investigates the integration of advanced deep learning techniques, including Attention Mechanism, Convolutional Neural Networks (CNNs), and Bidirectional Long ShortTerm Memory (BiLSTM), to improve the detection of violent behaviors in videos. The research explores how the use of the attention mechanism can help focus on the most relevant parts of the video, enhancing the accuracy and robustness of the model. The experiments indicate that the combination of CNNs with BiLSTM and the attention mechanism provides a promising solution for conflict monitoring, offering insights into the effectiveness of different strategies. This work opens new possibilities for the development of automated surveillance systems that can operate more efficiently in real-time detection of violent events."
  },
  {
    "title": "SWE-RL: Advancing LLM Reasoning via Reinforcement Learning on Open Software Evolution",
    "url": "http://arxiv.org/abs/2502.18449v1",
    "arxiv_id": "2502.18449v1",
    "authors": [
      "Yuxiang Wei",
      "Olivier Duchenne",
      "Jade Copet",
      "Quentin Carbonneaux",
      "Lingming Zhang",
      "Daniel Fried",
      "Gabriel Synnaeve",
      "Rishabh Singh",
      "Sida I. Wang"
    ],
    "published": "2025-02-25T18:45:04+00:00",
    "summary": "The recent DeepSeek-R1 release has demonstrated the immense potential of reinforcement learning (RL) in enhancing the general reasoning capabilities of large language models (LLMs). While DeepSeek-R1 and other follow-up work primarily focus on applying RL to competitive coding and math problems, this paper introduces SWE-RL, the first approach to scale RL-based LLM reasoning for real-world software engineering. Leveraging a lightweight rule-based reward (e.g., the similarity score between ground-truth and LLM-generated solutions), SWE-RL enables LLMs to autonomously recover a developer's reasoning processes and solutions by learning from extensive open-source software evolution data -- the record of a software's entire lifecycle, including its code snapshots, code changes, and events such as issues and pull requests. Trained on top of Llama 3, our resulting reasoning model, Llama3-SWE-RL-70B, achieves a 41.0% solve rate on SWE-bench Verified -- a human-verified collection of real-world GitHub issues. To our knowledge, this is the best performance reported for medium-sized (<100B) LLMs to date, even comparable to leading proprietary LLMs like GPT-4o. Surprisingly, despite performing RL solely on software evolution data, Llama3-SWE-RL has even emerged with generalized reasoning skills. For example, it shows improved results on five out-of-domain tasks, namely, function coding, library use, code reasoning, mathematics, and general language understanding, whereas a supervised-finetuning baseline even leads to performance degradation on average. Overall, SWE-RL opens up a new direction to improve the reasoning capabilities of LLMs through reinforcement learning on massive software engineering data."
  },
  {
    "title": "Rank1: Test-Time Compute for Reranking in Information Retrieval",
    "url": "http://arxiv.org/abs/2502.18418v1",
    "arxiv_id": "2502.18418v1",
    "authors": [
      "Orion Weller",
      "Kathryn Ricci",
      "Eugene Yang",
      "Andrew Yates",
      "Dawn Lawrie",
      "Benjamin Van Durme"
    ],
    "published": "2025-02-25T18:14:06+00:00",
    "summary": "We introduce Rank1, the first reranking model trained to take advantage of test-time compute. Rank1 demonstrates the applicability within retrieval of using a reasoning language model (i.e. OpenAI's o1, Deepseek's R1, etc.) for distillation in order to rapidly improve the performance of a smaller model. We gather and open-source a dataset of more than 600,000 examples of R1 reasoning traces from queries and passages in MS MARCO. Models trained on this dataset show: (1) state-of-the-art performance on advanced reasoning and instruction following datasets; (2) work remarkably well out of distribution due to the ability to respond to user-input prompts; and (3) have explainable reasoning chains that can be given to users or RAG-based systems. Further, we demonstrate that quantized versions of these models retain strong performance while using less compute/memory. Overall, Rank1 shows that test-time compute allows for a fundamentally new type of explainable and performant reranker model for search."
  },
  {
    "title": "What is the Alignment Objective of GRPO?",
    "url": "http://arxiv.org/abs/2502.18548v1",
    "arxiv_id": "2502.18548v1",
    "authors": [
      "Milan Vojnovic",
      "Se-Young Yun"
    ],
    "published": "2025-02-25T15:56:56+00:00",
    "summary": "In this note, we examine the aggregation of preferences achieved by the Group Policy Optimisation (GRPO) algorithm, a reinforcement learning method used to train advanced artificial intelligence models such as DeepSeek-R1-Zero and DeepSeekMath. The GRPO algorithm trains a policy using a reward preference model, which is computed by sampling a set of outputs for a given context, observing the corresponding rewards, and applying shift-and-scale normalisation to these reward values. Additionally, it incorporates a penalty function to discourage deviations from a reference policy.   We present a framework that enables us to characterise the stationary policies of the GRPO algorithm. This analysis reveals that the aggregation of preferences differs fundamentally from standard logarithmic pooling, which is implemented by other approaches such as RLHF. The precise form of preference aggregation arises from the way the reward preference model is defined and from the penalty function, which we show to essentially correspond to the reverse Kullback-Leibler (KL) divergence between the aggregation policy and the reference policy.   Interestingly, we demonstrate that for groups of size two, the reward preference model corresponds to pairwise comparison preferences, similar to those in other alignment methods based on pairwise comparison feedback. We provide explicit characterisations of the aggregate preference for binary questions, for groups of size two, and in the limit of large group size. This provides insights into the dependence of the aggregate preference on parameters such as the regularisation constant and the confidence margin of question answers.   Finally, we discuss the aggregation of preferences obtained by modifying the GRPO algorithm to use direct KL divergence as the penalty or to use rewards without scale normalisation."
  },
  {
    "title": "What is the Alignment Objective of GRPO?",
    "url": "http://arxiv.org/abs/2502.18548v2",
    "arxiv_id": "2502.18548v2",
    "authors": [
      "Milan Vojnovic",
      "Se-Young Yun"
    ],
    "published": "2025-02-25T15:56:56+00:00",
    "summary": "In this note, we examine the aggregation of preferences achieved by the Group Policy Optimisation (GRPO) algorithm, a reinforcement learning method used to train advanced artificial intelligence models such as DeepSeek-R1-Zero and DeepSeekMath. The GRPO algorithm trains a policy using a reward preference model, which is computed by sampling a set of outputs for a given context, observing the corresponding rewards, and applying shift-and-scale normalisation to these reward values. Additionally, it incorporates a penalty function to discourage deviations from a reference policy.   We present a framework that enables us to characterise the stationary policies of the GRPO algorithm. This analysis reveals that the aggregation of preferences differs fundamentally from standard logarithmic pooling, which is implemented by other approaches such as RLHF. The precise form of preference aggregation arises from the way the reward preference model is defined and from the penalty function, which we show to essentially correspond to the reverse Kullback-Leibler (KL) divergence between the aggregation policy and the reference policy.   Interestingly, we demonstrate that for groups of size two, the reward preference model corresponds to pairwise comparison preferences, similar to those in other alignment methods based on pairwise comparison feedback. We provide explicit characterisations of the aggregate preference for binary questions, for groups of size two, and in the limit of large group size. This provides insights into the dependence of the aggregate preference on parameters such as the regularisation constant and the confidence margin of question answers.   Finally, we discuss the aggregation of preferences obtained by modifying the GRPO algorithm to use direct KL divergence as the penalty or to use rewards without scale normalisation."
  },
  {
    "title": "You Shall Not Pass: Warning Drivers of Unsafe Overtaking Maneuvers on Country Roads by Predicting Safe Sight Distance",
    "url": "http://arxiv.org/abs/2502.18163v1",
    "arxiv_id": "2502.18163v1",
    "authors": [
      "Adrian Bauske",
      "Arthur Fleig"
    ],
    "published": "2025-02-25T12:49:05+00:00",
    "summary": "Overtaking on country roads with possible opposing traffic is a dangerous maneuver and many proposed assistant systems assume car-to-car communication and sensors currently unavailable in cars. To overcome this limitation, we develop an assistant that uses simple in-car sensors to predict the required sight distance for safe overtaking. Our models predict this from vehicle speeds, accelerations, and 3D map data. In a user study with a Virtual Reality driving simulator (N=25), we compare two UI variants (monitoring-focused vs scheduling-focused). The results reveal that both UIs enable more patient driving and thus increase overall driving safety. While the monitoring-focused UI achieves higher System Usability Score and distracts drivers less, the preferred UI depends on personal preference. Driving data shows predictions were off at times. We investigate and discuss this in a comparison of our models to actual driving behavior and identify crucial model parameters and assumptions that significantly improve model predictions."
  },
  {
    "title": "Towards Thinking-Optimal Scaling of Test-Time Compute for LLM Reasoning",
    "url": "http://arxiv.org/abs/2502.18080v1",
    "arxiv_id": "2502.18080v1",
    "authors": [
      "Wenkai Yang",
      "Shuming Ma",
      "Yankai Lin",
      "Furu Wei"
    ],
    "published": "2025-02-25T10:48:05+00:00",
    "summary": "Recent studies have shown that making a model spend more time thinking through longer Chain of Thoughts (CoTs) enables it to gain significant improvements in complex reasoning tasks. While current researches continue to explore the benefits of increasing test-time compute by extending the CoT lengths of Large Language Models (LLMs), we are concerned about a potential issue hidden behind the current pursuit of test-time scaling: Would excessively scaling the CoT length actually bring adverse effects to a model's reasoning performance? Our explorations on mathematical reasoning tasks reveal an unexpected finding that scaling with longer CoTs can indeed impair the reasoning performance of LLMs in certain domains. Moreover, we discover that there exists an optimal scaled length distribution that differs across different domains. Based on these insights, we propose a Thinking-Optimal Scaling strategy. Our method first uses a small set of seed data with varying response length distributions to teach the model to adopt different reasoning efforts for deep thinking. Then, the model selects its shortest correct response under different reasoning efforts on additional problems for self-improvement. Our self-improved models built upon Qwen2.5-32B-Instruct outperform other distillation-based 32B o1-like models across various math benchmarks, and achieve performance on par with QwQ-32B-Preview."
  },
  {
    "title": "Exploring the Effects of Traditional Chinese Medicine Scents on Mitigating Driving Fatigue",
    "url": "http://arxiv.org/abs/2502.18013v1",
    "arxiv_id": "2502.18013v1",
    "authors": [
      "Nengyue Su",
      "Liang Luo",
      "Yu Gu",
      "Fuji Ren"
    ],
    "published": "2025-02-25T09:20:45+00:00",
    "summary": "The rise of autonomous driving technology has led to concerns about inactivity-induced fatigue. This paper explores Traditional Chinese Medicine (TCM) scents for mitigating. Two human-involved studies have been conducted in a high-fidelity driving simulator. Study 1 maps six prevalent TCM scents onto the arousal/valence circumplex to select proper candidates, i.e., argy wormwood (with the highest arousal) and tangerine peel (with the highest valence). Study 2 tests both scents in an auto-driving course. Statistics show both scents can improve driver alertness and reaction-time, but should be used in different ways: argy wormwood is suitable for short-term use due to its higher intensity but poor acceptance, while tangerine peel is ideal for long-term use due to its higher likeness. These findings provide insights for in-car fatigue mitigation to enhance driver safety and well-being. However, issues such as scent longevity as for aromatherapy and automatic fatigue prediction remain unresolved."
  },
  {
    "title": "InVDriver: Intra-Instance Aware Vectorized Query-Based Autonomous Driving Transformer",
    "url": "http://arxiv.org/abs/2502.17949v1",
    "arxiv_id": "2502.17949v1",
    "authors": [
      "Bo Zhang",
      "Heye Huang",
      "Chunyang Liu",
      "Yaqin Zhang",
      "Zhenhua Xu"
    ],
    "published": "2025-02-25T08:20:16+00:00",
    "summary": "End-to-end autonomous driving with its holistic optimization capabilities, has gained increasing traction in academia and industry. Vectorized representations, which preserve instance-level topological information while reducing computational overhead, have emerged as a promising paradigm. While existing vectorized query-based frameworks often overlook the inherent spatial correlations among intra-instance points, resulting in geometrically inconsistent outputs (e.g., fragmented HD map elements or oscillatory trajectories). To address these limitations, we propose InVDriver, a novel vectorized query-based system that systematically models intra-instance spatial dependencies through masked self-attention layers, thereby enhancing planning accuracy and trajectory smoothness. Across all core modules, i.e., perception, prediction, and planning, InVDriver incorporates masked self-attention mechanisms that restrict attention to intra-instance point interactions, enabling coordinated refinement of structural elements while suppressing irrelevant inter-instance noise. Experimental results on the nuScenes benchmark demonstrate that InVDriver achieves state-of-the-art performance, surpassing prior methods in both accuracy and safety, while maintaining high computational efficiency. Our work validates that explicit modeling of intra-instance geometric coherence is critical for advancing vectorized autonomous driving systems, bridging the gap between theoretical advantages of end-to-end frameworks and practical deployment requirements."
  },
  {
    "title": "DeepSeek-R1 Outperforms Gemini 2.0 Pro, OpenAI o1, and o3-mini in Bilingual Complex Ophthalmology Reasoning",
    "url": "http://arxiv.org/abs/2502.17947v1",
    "arxiv_id": "2502.17947v1",
    "authors": [
      "Pusheng Xu",
      "Yue Wu",
      "Kai Jin",
      "Xiaolan Chen",
      "Mingguang He",
      "Danli Shi"
    ],
    "published": "2025-02-25T08:08:53+00:00",
    "summary": "Purpose: To evaluate the accuracy and reasoning ability of DeepSeek-R1 and three other recently released large language models (LLMs) in bilingual complex ophthalmology cases. Methods: A total of 130 multiple-choice questions (MCQs) related to diagnosis (n = 39) and management (n = 91) were collected from the Chinese ophthalmology senior professional title examination and categorized into six topics. These MCQs were translated into English using DeepSeek-R1. The responses of DeepSeek-R1, Gemini 2.0 Pro, OpenAI o1 and o3-mini were generated under default configurations between February 15 and February 20, 2025. Accuracy was calculated as the proportion of correctly answered questions, with omissions and extra answers considered incorrect. Reasoning ability was evaluated through analyzing reasoning logic and the causes of reasoning error. Results: DeepSeek-R1 demonstrated the highest overall accuracy, achieving 0.862 in Chinese MCQs and 0.808 in English MCQs. Gemini 2.0 Pro, OpenAI o1, and OpenAI o3-mini attained accuracies of 0.715, 0.685, and 0.692 in Chinese MCQs (all P<0.001 compared with DeepSeek-R1), and 0.746 (P=0.115), 0.723 (P=0.027), and 0.577 (P<0.001) in English MCQs, respectively. DeepSeek-R1 achieved the highest accuracy across five topics in both Chinese and English MCQs. It also excelled in management questions conducted in Chinese (all P<0.05). Reasoning ability analysis showed that the four LLMs shared similar reasoning logic. Ignoring key positive history, ignoring key positive signs, misinterpretation medical data, and too aggressive were the most common causes of reasoning errors. Conclusion: DeepSeek-R1 demonstrated superior performance in bilingual complex ophthalmology reasoning tasks than three other state-of-the-art LLMs. While its clinical applicability remains challenging, it shows promise for supporting diagnosis and clinical decision-making."
  },
  {
    "title": "FetchBot: Object Fetching in Cluttered Shelves via Zero-Shot Sim2Real",
    "url": "http://arxiv.org/abs/2502.17894v1",
    "arxiv_id": "2502.17894v1",
    "authors": [
      "Weiheng Liu",
      "Yuxuan Wan",
      "Jilong Wang",
      "Yuxuan Kuang",
      "Xuesong Shi",
      "Haoran Li",
      "Dongbin Zhao",
      "Zhizheng Zhang",
      "He Wang"
    ],
    "published": "2025-02-25T06:32:42+00:00",
    "summary": "Object fetching from cluttered shelves is an important capability for robots to assist humans in real-world scenarios. Achieving this task demands robotic behaviors that prioritize safety by minimizing disturbances to surrounding objects, an essential but highly challenging requirement due to restricted motion space, limited fields of view, and complex object dynamics. In this paper, we introduce FetchBot, a sim-to-real framework designed to enable zero-shot generalizable and safety-aware object fetching from cluttered shelves in real-world settings. To address data scarcity, we propose an efficient voxel-based method for generating diverse simulated cluttered shelf scenes at scale and train a dynamics-aware reinforcement learning (RL) policy to generate object fetching trajectories within these scenes. This RL policy, which leverages oracle information, is subsequently distilled into a vision-based policy for real-world deployment. Considering that sim-to-real discrepancies stem from texture variations mostly while from geometric dimensions rarely, we propose to adopt depth information estimated by full-fledged depth foundation models as the input for the vision-based policy to mitigate sim-to-real gap. To tackle the challenge of limited views, we design a novel architecture for learning multi-view representations, allowing for comprehensive encoding of cluttered shelf scenes. This enables FetchBot to effectively minimize collisions while fetching objects from varying positions and depths, ensuring robust and safety-aware operation. Both simulation and real-robot experiments demonstrate FetchBot's superior generalization ability, particularly in handling a broad range of real-world scenarios, includ"
  },
  {
    "title": "LR${}^{2}$Bench: Evaluating Long-chain Reflective Reasoning Capabilities of Large Language Models via Constraint Satisfaction Problems",
    "url": "http://arxiv.org/abs/2502.17848v1",
    "arxiv_id": "2502.17848v1",
    "authors": [
      "Jianghao Chen",
      "Zhenlin Wei",
      "Zhenjiang Ren",
      "Ziyong Li",
      "Jiajun Zhang"
    ],
    "published": "2025-02-25T04:51:17+00:00",
    "summary": "Recent progress in o1-like models has significantly enhanced the reasoning abilities of Large Language Models (LLMs), empowering them to tackle increasingly complex tasks through reflection capabilities, such as making assumptions, backtracking, and self-refinement. However, effectively evaluating such reflection capabilities remains challenging due to the lack of appropriate benchmarks. To bridge this gap, we introduce LR${}^{2}$Bench, a novel benchmark designed to evaluate the Long-chain Reflective Reasoning capabilities of LLMs. LR${}^{2}$Bench comprises 850 samples across six Constraint Satisfaction Problems (CSPs) where reflective reasoning is crucial for deriving solutions that meet all given constraints. Each type of task focuses on distinct constraint patterns, such as knowledge-based, logical, and spatial constraints, providing a comprehensive evaluation of diverse problem-solving scenarios. We conduct extensive evaluation on both conventional models and o1-like models. Our experimental results reveal that even the most advanced reasoning-specific models, such as DeepSeek-R1 and OpenAI o1-preview, struggle with tasks in LR${}^{2}$Bench, achieving an average Exact Match score of only 20.0% and 23.6%, respectively. These findings underscore the significant room for improvement in the reflective reasoning capabilities of current LLMs. The leaderboard of our benchmark is available at https://huggingface.co/spaces/UltraRonin/LR2Bench"
  },
  {
    "title": "Impact of Object Weight in Handovers: Inspiring Robotic Grip Release and Motion from Human Handovers",
    "url": "http://arxiv.org/abs/2502.17834v1",
    "arxiv_id": "2502.17834v1",
    "authors": [
      "Parag Khanna",
      "M\u00e5rten Bj\u00f6rkman",
      "Christian Smith"
    ],
    "published": "2025-02-25T04:29:11+00:00",
    "summary": "This work explores the effect of object weight on human motion and grip release during handovers to enhance the naturalness, safety, and efficiency of robot-human interactions. We introduce adaptive robotic strategies based on the analysis of human handover behavior with varying object weights. The key contributions of this work includes the development of an adaptive grip-release strategy for robots, a detailed analysis of how object weight influences human motion to guide robotic motion adaptations, and the creation of handover-datasets incorporating various object weights, including the YCB handover dataset. By aligning robotic grip release and motion with human behavior, this work aims to improve robot-human handovers for different weighted objects. We also evaluate these human-inspired adaptive robotic strategies in robot-to-human handovers to assess their effectiveness and performance and demonstrate that they outperform the baseline approaches in terms of naturalness, efficiency, and user perception."
  },
  {
    "title": "MM-PoisonRAG: Disrupting Multimodal RAG with Local and Global Poisoning Attacks",
    "url": "http://arxiv.org/abs/2502.17832v1",
    "arxiv_id": "2502.17832v1",
    "authors": [
      "Hyeonjeong Ha",
      "Qiusi Zhan",
      "Jeonghwan Kim",
      "Dimitrios Bralios",
      "Saikrishna Sanniboina",
      "Nanyun Peng",
      "Kai-wei Chang",
      "Daniel Kang",
      "Heng Ji"
    ],
    "published": "2025-02-25T04:23:59+00:00",
    "summary": "Multimodal large language models (MLLMs) equipped with Retrieval Augmented Generation (RAG) leverage both their rich parametric knowledge and the dynamic, external knowledge to excel in tasks such as Question Answering. While RAG enhances MLLMs by grounding responses in query-relevant external knowledge, this reliance poses a critical yet underexplored safety risk: knowledge poisoning attacks, where misinformation or irrelevant knowledge is intentionally injected into external knowledge bases to manipulate model outputs to be incorrect and even harmful. To expose such vulnerabilities in multimodal RAG, we propose MM-PoisonRAG, a novel knowledge poisoning attack framework with two attack strategies: Localized Poisoning Attack (LPA), which injects query-specific misinformation in both text and images for targeted manipulation, and Globalized Poisoning Attack (GPA) to provide false guidance during MLLM generation to elicit nonsensical responses across all queries. We evaluate our attacks across multiple tasks, models, and access settings, demonstrating that LPA successfully manipulates the MLLM to generate attacker-controlled answers, with a success rate of up to 56% on MultiModalQA. Moreover, GPA completely disrupts model generation to 0% accuracy with just a single irrelevant knowledge injection. Our results highlight the urgent need for robust defenses against knowledge poisoning to safeguard multimodal RAG frameworks."
  },
  {
    "title": "Safe Multi-Agent Navigation guided by Goal-Conditioned Safe Reinforcement Learning",
    "url": "http://arxiv.org/abs/2502.17813v1",
    "arxiv_id": "2502.17813v1",
    "authors": [
      "Meng Feng",
      "Viraj Parimi",
      "Brian Williams"
    ],
    "published": "2025-02-25T03:38:52+00:00",
    "summary": "Safe navigation is essential for autonomous systems operating in hazardous environments. Traditional planning methods excel at long-horizon tasks but rely on a predefined graph with fixed distance metrics. In contrast, safe Reinforcement Learning (RL) can learn complex behaviors without relying on manual heuristics but fails to solve long-horizon tasks, particularly in goal-conditioned and multi-agent scenarios.   In this paper, we introduce a novel method that integrates the strengths of both planning and safe RL. Our method leverages goal-conditioned RL and safe RL to learn a goal-conditioned policy for navigation while concurrently estimating cumulative distance and safety levels using learned value functions via an automated self-training algorithm. By constructing a graph with states from the replay buffer, our method prunes unsafe edges and generates a waypoint-based plan that the agent follows until reaching its goal, effectively balancing faster and safer routes over extended distances.   Utilizing this unified high-level graph and a shared low-level goal-conditioned safe RL policy, we extend this approach to address the multi-agent safe navigation problem. In particular, we leverage Conflict-Based Search (CBS) to create waypoint-based plans for multiple agents allowing for their safe navigation over extended horizons. This integration enhances the scalability of goal-conditioned safe RL in multi-agent scenarios, enabling efficient coordination among agents.   Extensive benchmarking against state-of-the-art baselines demonstrates the effectiveness of our method in achieving distance goals safely for multiple agents in complex and hazardous environments. Our code will be released to support future research."
  },
  {
    "title": "DocPuzzle: A Process-Aware Benchmark for Evaluating Realistic Long-Context Reasoning Capabilities",
    "url": "http://arxiv.org/abs/2502.17807v1",
    "arxiv_id": "2502.17807v1",
    "authors": [
      "Tianyi Zhuang",
      "Chuqiao Kuang",
      "Xiaoguang Li",
      "Yihua Teng",
      "Jihao Wu",
      "Yasheng Wang",
      "Lifeng Shang"
    ],
    "published": "2025-02-25T03:29:53+00:00",
    "summary": "We present DocPuzzle, a rigorously constructed benchmark for evaluating long-context reasoning capabilities in large language models (LLMs). This benchmark comprises 100 expert-level QA problems requiring multi-step reasoning over long real-world documents. To ensure the task quality and complexity, we implement a human-AI collaborative annotation-validation pipeline. DocPuzzle introduces an innovative evaluation framework that mitigates guessing bias through checklist-guided process analysis, establishing new standards for assessing reasoning capacities in LLMs. Our evaluation results show that: 1)Advanced slow-thinking reasoning models like o1-preview(69.7%) and DeepSeek-R1(66.3%) significantly outperform best general instruct models like Claude 3.5 Sonnet(57.7%); 2)Distilled reasoning models like DeepSeek-R1-Distill-Qwen-32B(41.3%) falls far behind the teacher model, suggesting challenges to maintain the generalization of reasoning capabilities relying solely on distillation."
  },
  {
    "title": "Exploring the Potential of Large Language Models for Estimating the Reading Comprehension Question Difficulty",
    "url": "http://arxiv.org/abs/2502.17785v1",
    "arxiv_id": "2502.17785v1",
    "authors": [
      "Yoshee Jain",
      "John Hollander",
      "Amber He",
      "Sunny Tang",
      "Liang Zhang",
      "John Sabatini"
    ],
    "published": "2025-02-25T02:28:48+00:00",
    "summary": "Reading comprehension is a key for individual success, yet the assessment of question difficulty remains challenging due to the extensive human annotation and large-scale testing required by traditional methods such as linguistic analysis and Item Response Theory (IRT). While these robust approaches provide valuable insights, their scalability is limited. There is potential for Large Language Models (LLMs) to automate question difficulty estimation; however, this area remains underexplored. Our study investigates the effectiveness of LLMs, specifically OpenAI's GPT-4o and o1, in estimating the difficulty of reading comprehension questions using the Study Aid and Reading Assessment (SARA) dataset. We evaluated both the accuracy of the models in answering comprehension questions and their ability to classify difficulty levels as defined by IRT. The results indicate that, while the models yield difficulty estimates that align meaningfully with derived IRT parameters, there are notable differences in their sensitivity to extreme item characteristics. These findings suggest that LLMs can serve as the scalable method for automated difficulty assessment, particularly in dynamic interactions between learners and Adaptive Instructional Systems (AIS), bridging the gap between traditional psychometric techniques and modern AIS for reading comprehension and paving the way for more adaptive and personalized educational assessments."
  },
  {
    "title": "GPUArmor: A Hardware-Software Co-design for Efficient and Scalable Memory Safety on GPUs",
    "url": "http://arxiv.org/abs/2502.17780v1",
    "arxiv_id": "2502.17780v1",
    "authors": [
      "Mohamed Tarek Ibn Ziad",
      "Sana Damani",
      "Mark Stephenson",
      "Stephen W. Keckler",
      "Aamer Jaleel"
    ],
    "published": "2025-02-25T02:15:06+00:00",
    "summary": "Memory safety errors continue to pose a significant threat to current computing systems, and graphics processing units (GPUs) are no exception. A prominent class of memory safety algorithms is allocation-based solutions. The key idea is to maintain each allocation's metadata (base address and size) in a disjoint table and retrieve it at runtime to verify memory accesses. While several previous solutions have adopted allocation-based algorithms (e.g., cuCatch and GPUShield), they typically suffer from high memory overheads or scalability problems. In this work, we examine the key characteristics of real-world GPU workloads and observe several differences between GPU and CPU applications regarding memory access patterns, memory footprint, number of live allocations, and active allocation working set. Our observations motivate GPUArmor, a hardware-software co-design framework for memory safety on GPUs. We show that a simple compiler analysis combined with lightweight hardware support using a small Memory Lookaside Buffer (MLB) can help prevent spatial and temporal memory violations on modern GPU workloads with 2.3% average run time overheads. More importantly, GPUArmor achieves speed-of-light performance with negligible storage requirements. This result benefits both base and bounds solutions and memory tagging techniques, which we showcase with GPUArmor-HWOnly, a variation of GPUArmor that does not require recompilation, and achieves 2.2% slowdowns while significantly reducing storage overheads beyond traditional memory tagging approaches."
  },
  {
    "title": "GPUArmor: A Hardware-Software Co-design for Efficient and Scalable Memory Safety on GPUs",
    "url": "http://arxiv.org/abs/2502.17780v2",
    "arxiv_id": "2502.17780v2",
    "authors": [
      "Mohamed Tarek Ibn Ziad",
      "Sana Damani",
      "Mark Stephenson",
      "Stephen W. Keckler",
      "Aamer Jaleel"
    ],
    "published": "2025-02-25T02:15:06+00:00",
    "summary": "Memory safety errors continue to pose a significant threat to current computing systems, and graphics processing units (GPUs) are no exception. A prominent class of memory safety algorithms is allocation-based solutions. The key idea is to maintain each allocation's metadata (base address and size) in a disjoint table and retrieve it at runtime to verify memory accesses. While several previous solutions have adopted allocation-based algorithms (e.g., cuCatch and GPUShield), they typically suffer from high memory overheads or scalability problems. In this work, we examine the key characteristics of real-world GPU workloads and observe several differences between GPU and CPU applications regarding memory access patterns, memory footprint, number of live allocations, and active allocation working set. Our observations motivate GPUArmor, a hardware-software co-design framework for memory safety on GPUs. We show that a simple compiler analysis combined with lightweight hardware support using a small Memory Lookaside Buffer (MLB) can help prevent spatial and temporal memory violations on modern GPU workloads with 2.3% average run time overheads. More importantly, GPUArmor achieves speed-of-light performance with negligible storage requirements. This result benefits both base and bounds solutions and memory tagging techniques, which we showcase with GPUArmor-HWOnly, a variation of GPUArmor that does not require recompilation, and achieves 2.2% slowdowns while significantly reducing storage overheads beyond traditional memory tagging approaches."
  },
  {
    "title": "Design of a Breakaway Utensil Attachment for Enhanced Safety in Robot-Assisted Feeding",
    "url": "http://arxiv.org/abs/2502.17774v1",
    "arxiv_id": "2502.17774v1",
    "authors": [
      "Hau Wen Chang",
      "J-Anne Yow",
      "Lek Syn Lim",
      "Wei Tech Ang"
    ],
    "published": "2025-02-25T02:09:32+00:00",
    "summary": "Robot-assisted feeding systems enhance the independence of individuals with motor impairments and alleviate caregiver burden. While existing systems predominantly rely on software-based safety features to mitigate risks during unforeseen collisions, this study explores the use of a mechanical fail-safe to improve safety. We designed a breakaway utensil attachment that decouples forces exerted by the robot on the user when excessive forces occur. Finite element analysis (FEA) simulations were performed to predict failure points under various loading conditions, followed by experimental validation using 3D-printed attachments with variations in slot depth and wall loops. To facilitate testing, a drop test rig was developed and validated. Our results demonstrated a consistent failure point at the slot of the attachment, with a slot depth of 1 mm and three wall loops achieving failure at the target force of 65 N. Additionally, the parameters can be tailored to customize the breakaway force based on user-specific factors, such as comfort and pain tolerance. CAD files and utensil assembly instructions can be found here: https://tinyurl.com/rfa-utensil-attachment"
  },
  {
    "title": "DeepSeek vs. ChatGPT: A Comparative Study for Scientific Computing and Scientific Machine Learning Tasks",
    "url": "http://arxiv.org/abs/2502.17764v1",
    "arxiv_id": "2502.17764v1",
    "authors": [
      "Qile Jiang",
      "Zhiwei Gao",
      "George Em Karniadakis"
    ],
    "published": "2025-02-25T01:49:50+00:00",
    "summary": "Large Language Models (LLMs) have emerged as powerful tools for tackling a wide range of problems, including those in scientific computing, particularly in solving partial differential equations (PDEs). However, different models exhibit distinct strengths and preferences, resulting in varying levels of performance. In this paper, we compare the capabilities of the most advanced LLMs--ChatGPT and DeepSeek--along with their reasoning-optimized versions in addressing computational challenges. Specifically, we evaluate their proficiency in solving traditional numerical problems in scientific computing as well as leveraging scientific machine learning techniques for PDE-based problems. We designed all our experiments so that a non-trivial decision is required, e.g. defining the proper space of input functions for neural operator learning. Our findings reveal that the latest model, ChatGPT o3-mini-high, usually delivers the most accurate results while also responding significantly faster than its reasoning counterpart, DeepSeek R1. This enhanced speed and accuracy make ChatGPT o3-mini-high a more practical and efficient choice for diverse computational tasks at this juncture."
  },
  {
    "title": "Heterogeneous Decision Making in Mixed Traffic: Uncertainty-aware Planning and Bounded Rationality",
    "url": "http://arxiv.org/abs/2502.18529v1",
    "arxiv_id": "2502.18529v1",
    "authors": [
      "Hang Wang",
      "Qiaoyi Fang",
      "Junshan Zhang"
    ],
    "published": "2025-02-25T00:32:33+00:00",
    "summary": "The past few years have witnessed a rapid growth of the deployment of automated vehicles (AVs). Clearly, AVs and human-driven vehicles (HVs) will co-exist for many years, and AVs will have to operate around HVs, pedestrians, cyclists, and more, calling for fundamental breakthroughs in AI designed for mixed traffic to achieve mixed autonomy. Thus motivated, we study heterogeneous decision making by AVs and HVs in a mixed traffic environment, aiming to capture the interactions between human and machine decision-making and develop an AI foundation that enables vehicles to operate safely and efficiently. There are a number of challenges to achieve mixed autonomy, including 1) humans drivers make driving decisions with bounded rationality, and it remains open to develop accurate models for HVs' decision making; and 2) uncertainty-aware planning plays a critical role for AVs to take safety maneuvers in response to the human behavior. In this paper, we introduce a formulation of AV-HV interaction, where the HV makes decisions with bounded rationality and the AV employs uncertainty-aware planning based on the prediction on HV's future actions. We conduct a comprehensive analysis on AV and HV's learning regret to answer the questions: 1) {How does the learning performance depend on HV's bounded rationality and AV's planning}; 2) {How do different decision making strategies impact the overall learning performance}? Our findings reveal some intriguing phenomena, such as Goodhart's Law in AV's learning performance and compounding effects in HV's decision making process. By examining the dynamics of the regrets, we gain insights into the interplay between human and machine decision making."
  },
  {
    "title": "Mind the Gesture: Evaluating AI Sensitivity to Culturally Offensive Non-Verbal Gestures",
    "url": "http://arxiv.org/abs/2502.17710v1",
    "arxiv_id": "2502.17710v1",
    "authors": [
      "Akhila Yerukola",
      "Saadia Gabriel",
      "Nanyun Peng",
      "Maarten Sap"
    ],
    "published": "2025-02-24T23:10:08+00:00",
    "summary": "Gestures are an integral part of non-verbal communication, with meanings that vary across cultures, and misinterpretations that can have serious social and diplomatic consequences. As AI systems become more integrated into global applications, ensuring they do not inadvertently perpetuate cultural offenses is critical. To this end, we introduce Multi-Cultural Set of Inappropriate Gestures and Nonverbal Signs (MC-SIGNS), a dataset of 288 gesture-country pairs annotated for offensiveness, cultural significance, and contextual factors across 25 gestures and 85 countries. Through systematic evaluation using MC-SIGNS, we uncover critical limitations: text-to-image (T2I) systems exhibit strong US-centric biases, performing better at detecting offensive gestures in US contexts than in non-US ones; large language models (LLMs) tend to over-flag gestures as offensive; and vision-language models (VLMs) default to US-based interpretations when responding to universal concepts like wishing someone luck, frequently suggesting culturally inappropriate gestures. These findings highlight the urgent need for culturally-aware AI safety mechanisms to ensure equitable global deployment of AI technologies."
  },
  {
    "title": "Data-Driven Input-Output Control Barrier Functions",
    "url": "http://arxiv.org/abs/2502.17688v1",
    "arxiv_id": "2502.17688v1",
    "authors": [
      "MMohammad Bajelani",
      "Klaske van Heusden"
    ],
    "published": "2025-02-24T22:16:27+00:00",
    "summary": "Control Barrier Functions (CBFs) offer a framework for ensuring set invariance and designing constrained control laws. However, crafting a valid CBF relies on system-specific assumptions and the availability of an accurate system model, underscoring the need for systematic data-driven synthesis methods. This paper introduces a data-driven approach to synthesizing a CBF for discrete-time LTI systems using only input-output measurements. The method begins by computing the maximal control invariant set using an input-output data-driven representation, eliminating the need for precise knowledge of the system's order and explicit state estimation. The proposed CBF is then systematically derived from this set, which can accommodate multiple input-output constraints. Furthermore, the proposed CBF is leveraged to develop a minimally invasive safety filter that ensures recursive feasibility with an adaptive decay rate. To improve clarity, we assume a noise-free dataset, though data-driven control techniques can be used to robustify the approach. Finally, the effectiveness of the proposed method is demonstrated on an unknown time-delay system."
  },
  {
    "title": "Architecting Digital Twins for Intelligent Transportation Systems",
    "url": "http://arxiv.org/abs/2502.17646v1",
    "arxiv_id": "2502.17646v1",
    "authors": [
      "Hiya Bhatt",
      "Sahil",
      "Karthik Vaidhyanathan",
      "Rahul Biju",
      "Deepak Gangadharan",
      "Ramona Trestian",
      "Purav Shah"
    ],
    "published": "2025-02-24T20:51:09+00:00",
    "summary": "Modern transportation systems face growing challenges in managing traffic flow, ensuring safety, and maintaining operational efficiency amid dynamic traffic patterns. Addressing these challenges requires intelligent solutions capable of real-time monitoring, predictive analytics, and adaptive control. This paper proposes an architecture for DigIT, a Digital Twin (DT) platform for Intelligent Transportation Systems (ITS), designed to overcome the limitations of existing frameworks by offering a modular and scalable solution for traffic management. Built on a Domain Concept Model (DCM), the architecture systematically models key ITS components enabling seamless integration of predictive modeling and simulations. The architecture leverages machine learning models to forecast traffic patterns based on historical and real-time data. To adapt to evolving traffic patterns, the architecture incorporates adaptive Machine Learning Operations (MLOps), automating the deployment and lifecycle management of predictive models. Evaluation results highlight the effectiveness of the architecture in delivering accurate predictions and computational efficiency."
  },
  {
    "title": "Hallucination Detection in LLMs Using Spectral Features of Attention Maps",
    "url": "http://arxiv.org/abs/2502.17598v1",
    "arxiv_id": "2502.17598v1",
    "authors": [
      "Jakub Binkowski",
      "Denis Janiak",
      "Albert Sawczyn",
      "Bogdan Gabrys",
      "Tomasz Kajdanowicz"
    ],
    "published": "2025-02-24T19:30:24+00:00",
    "summary": "Large Language Models (LLMs) have demonstrated remarkable performance across various tasks but remain prone to hallucinations. Detecting hallucinations is essential for safety-critical applications, and recent methods leverage attention map properties to this end, though their effectiveness remains limited. In this work, we investigate the spectral features of attention maps by interpreting them as adjacency matrices of graph structures. We propose the $\\text{LapEigvals}$ method, which utilises the top-$k$ eigenvalues of the Laplacian matrix derived from the attention maps as an input to hallucination detection probes. Empirical evaluations demonstrate that our approach achieves state-of-the-art hallucination detection performance among attention-based methods. Extensive ablation studies further highlight the robustness and generalisation of $\\text{LapEigvals}$, paving the way for future advancements in the hallucination detection domain."
  },
  {
    "title": "How Do Large Language Monkeys Get Their Power (Laws)?",
    "url": "http://arxiv.org/abs/2502.17578v1",
    "arxiv_id": "2502.17578v1",
    "authors": [
      "Rylan Schaeffer",
      "Joshua Kazdan",
      "John Hughes",
      "Jordan Juravsky",
      "Sara Price",
      "Aengus Lynch",
      "Erik Jones",
      "Robert Kirk",
      "Azalia Mirhoseini",
      "Sanmi Koyejo"
    ],
    "published": "2025-02-24T19:01:47+00:00",
    "summary": "Recent research across mathematical problem solving, proof assistant programming and multimodal jailbreaking documents a striking finding: when (multimodal) language model tackle a suite of tasks with multiple attempts per task -- succeeding if any attempt is correct -- then the negative log of the average success rate scales a power law in the number of attempts. In this work, we identify an apparent puzzle: a simple mathematical calculation predicts that on each problem, the failure rate should fall exponentially with the number of attempts. We confirm this prediction empirically, raising a question: from where does aggregate polynomial scaling emerge? We then answer this question by demonstrating per-problem exponential scaling can be made consistent with aggregate polynomial scaling if the distribution of single-attempt success probabilities is heavy tailed such that a small fraction of tasks with extremely low success probabilities collectively warp the aggregate success trend into a power law - even as each problem scales exponentially on its own. We further demonstrate that this distributional perspective explains previously observed deviations from power law scaling, and provides a simple method for forecasting the power law exponent with an order of magnitude lower relative error, or equivalently, ${\\sim}2-4$ orders of magnitude less inference compute. Overall, our work contributes to a better understanding of how neural language model performance improves with scaling inference compute and the development of scaling-predictable evaluations of (multimodal) language models."
  },
  {
    "title": "LongSpec: Long-Context Speculative Decoding with Efficient Drafting and Verification",
    "url": "http://arxiv.org/abs/2502.17421v1",
    "arxiv_id": "2502.17421v1",
    "authors": [
      "Penghui Yang",
      "Cunxiao Du",
      "Fengzhuo Zhang",
      "Haonan Wang",
      "Tianyu Pang",
      "Chao Du",
      "Bo An"
    ],
    "published": "2025-02-24T18:53:31+00:00",
    "summary": "Speculative decoding has become a promising technique to mitigate the high inference latency of autoregressive decoding in Large Language Models (LLMs). Despite its promise, the effective application of speculative decoding in LLMs still confronts three key challenges: the increasing memory demands of the draft model, the distribution shift between the short-training corpora and long-context inference, and inefficiencies in attention implementation. In this work, we enhance the performance of speculative decoding in long-context settings by addressing these challenges. First, we propose a memory-efficient draft model with a constant-sized Key-Value (KV) cache. Second, we introduce novel position indices for short-training data, enabling seamless adaptation from short-context training to long-context inference. Finally, we present an innovative attention aggregation method that combines fast implementations for prefix computation with standard attention for tree mask handling, effectively resolving the latency and memory inefficiencies of tree decoding. Our approach achieves strong results on various long-context tasks, including repository-level code completion, long-context summarization, and o1-like long reasoning tasks, demonstrating significant improvements in latency reduction. The code is available at https://github.com/sail-sg/LongSpec."
  },
  {
    "title": "The Geometry of Refusal in Large Language Models: Concept Cones and Representational Independence",
    "url": "http://arxiv.org/abs/2502.17420v1",
    "arxiv_id": "2502.17420v1",
    "authors": [
      "Tom Wollschl\u00e4ger",
      "Jannes Elstner",
      "Simon Geisler",
      "Vincent Cohen-Addad",
      "Stephan G\u00fcnnemann",
      "Johannes Gasteiger"
    ],
    "published": "2025-02-24T18:52:59+00:00",
    "summary": "The safety alignment of large language models (LLMs) can be circumvented through adversarially crafted inputs, yet the mechanisms by which these attacks bypass safety barriers remain poorly understood. Prior work suggests that a single refusal direction in the model's activation space determines whether an LLM refuses a request. In this study, we propose a novel gradient-based approach to representation engineering and use it to identify refusal directions. Contrary to prior work, we uncover multiple independent directions and even multi-dimensional concept cones that mediate refusal. Moreover, we show that orthogonality alone does not imply independence under intervention, motivating the notion of representational independence that accounts for both linear and non-linear effects. Using this framework, we identify mechanistically independent refusal directions. We show that refusal mechanisms in LLMs are governed by complex spatial structures and identify functionally independent directions, confirming that multiple distinct mechanisms drive refusal behavior. Our gradient-based approach uncovers these mechanisms and can further serve as a foundation for future work on understanding LLMs."
  },
  {
    "title": "From System 1 to System 2: A Survey of Reasoning Large Language Models",
    "url": "http://arxiv.org/abs/2502.17419v1",
    "arxiv_id": "2502.17419v1",
    "authors": [
      "Zhong-Zhi Li",
      "Duzhen Zhang",
      "Ming-Liang Zhang",
      "Jiaxin Zhang",
      "Zengyan Liu",
      "Yuxuan Yao",
      "Haotian Xu",
      "Junhao Zheng",
      "Pei-Jie Wang",
      "Xiuyi Chen",
      "Yingying Zhang",
      "Fei Yin",
      "Jiahua Dong",
      "Zhijiang Guo",
      "Le Song",
      "Cheng-Lin Liu"
    ],
    "published": "2025-02-24T18:50:52+00:00",
    "summary": "Achieving human-level intelligence requires refining the transition from the fast, intuitive System 1 to the slower, more deliberate System 2 reasoning. While System 1 excels in quick, heuristic decisions, System 2 relies on logical reasoning for more accurate judgments and reduced biases. Foundational Large Language Models (LLMs) excel at fast decision-making but lack the depth for complex reasoning, as they have not yet fully embraced the step-by-step analysis characteristic of true System 2 thinking. Recently, reasoning LLMs like OpenAI's o1/o3 and DeepSeek's R1 have demonstrated expert-level performance in fields such as mathematics and coding, closely mimicking the deliberate reasoning of System 2 and showcasing human-like cognitive abilities. This survey begins with a brief overview of the progress in foundational LLMs and the early development of System 2 technologies, exploring how their combination has paved the way for reasoning LLMs. Next, we discuss how to construct reasoning LLMs, analyzing their features, the core methods enabling advanced reasoning, and the evolution of various reasoning LLMs. Additionally, we provide an overview of reasoning benchmarks, offering an in-depth comparison of the performance of representative reasoning LLMs. Finally, we explore promising directions for advancing reasoning LLMs and maintain a real-time \\href{https://github.com/zzli2022/Awesome-Slow-Reason-System}{GitHub Repository} to track the latest developments. We hope this survey will serve as a valuable resource to inspire innovation and drive progress in this rapidly evolving field."
  },
  {
    "title": "From System 1 to System 2: A Survey of Reasoning Large Language Models",
    "url": "http://arxiv.org/abs/2502.17419v2",
    "arxiv_id": "2502.17419v2",
    "authors": [
      "Zhong-Zhi Li",
      "Duzhen Zhang",
      "Ming-Liang Zhang",
      "Jiaxin Zhang",
      "Zengyan Liu",
      "Yuxuan Yao",
      "Haotian Xu",
      "Junhao Zheng",
      "Pei-Jie Wang",
      "Xiuyi Chen",
      "Yingying Zhang",
      "Fei Yin",
      "Jiahua Dong",
      "Zhijiang Guo",
      "Le Song",
      "Cheng-Lin Liu"
    ],
    "published": "2025-02-24T18:50:52+00:00",
    "summary": "Achieving human-level intelligence requires refining the transition from the fast, intuitive System 1 to the slower, more deliberate System 2 reasoning. While System 1 excels in quick, heuristic decisions, System 2 relies on logical reasoning for more accurate judgments and reduced biases. Foundational Large Language Models (LLMs) excel at fast decision-making but lack the depth for complex reasoning, as they have not yet fully embraced the step-by-step analysis characteristic of true System 2 thinking. Recently, reasoning LLMs like OpenAI's o1/o3 and DeepSeek's R1 have demonstrated expert-level performance in fields such as mathematics and coding, closely mimicking the deliberate reasoning of System 2 and showcasing human-like cognitive abilities. This survey begins with a brief overview of the progress in foundational LLMs and the early development of System 2 technologies, exploring how their combination has paved the way for reasoning LLMs. Next, we discuss how to construct reasoning LLMs, analyzing their features, the core methods enabling advanced reasoning, and the evolution of various reasoning LLMs. Additionally, we provide an overview of reasoning benchmarks, offering an in-depth comparison of the performance of representative reasoning LLMs. Finally, we explore promising directions for advancing reasoning LLMs and maintain a real-time \\href{https://github.com/zzli2022/Awesome-Slow-Reason-System}{GitHub Repository} to track the latest developments. We hope this survey will serve as a valuable resource to inspire innovation and drive progress in this rapidly evolving field."
  },
  {
    "title": "Dataset Featurization: Uncovering Natural Language Features through Unsupervised Data Reconstruction",
    "url": "http://arxiv.org/abs/2502.17541v1",
    "arxiv_id": "2502.17541v1",
    "authors": [
      "Michal Bravansky",
      "Vaclav Kubon",
      "Suhas Hariharan",
      "Robert Kirk"
    ],
    "published": "2025-02-24T18:42:33+00:00",
    "summary": "Interpreting data is central to modern research. Large language models (LLMs) show promise in providing such natural language interpretations of data, yet simple feature extraction methods such as prompting often fail to produce accurate and versatile descriptions for diverse datasets and lack control over granularity and scale. To address these limitations, we propose a domain-agnostic method for dataset featurization that provides precise control over the number of features extracted while maintaining compact and descriptive representations comparable to human expert labeling. Our method optimizes the selection of informative binary features by evaluating the ability of an LLM to reconstruct the original data using those features. We demonstrate its effectiveness in dataset modeling tasks and through two case studies: (1) Constructing a feature representation of jailbreak tactics that compactly captures both the effectiveness and diversity of a larger set of human-crafted attacks; and (2) automating the discovery of features that align with human preferences, achieving accuracy and robustness comparable to expert-crafted features. Moreover, we show that the pipeline scales effectively, improving as additional features are sampled, making it suitable for large and diverse datasets."
  },
  {
    "title": "Experimental validation of UAV search and detection system in real wilderness environment",
    "url": "http://arxiv.org/abs/2502.17372v1",
    "arxiv_id": "2502.17372v1",
    "authors": [
      "Stella Dumen\u010di\u0107",
      "Luka Lan\u010da",
      "Karlo Jakac",
      "Stefan Ivi\u0107"
    ],
    "published": "2025-02-24T17:53:54+00:00",
    "summary": "Search and rescue (SAR) missions require reliable search methods to locate survivors, especially in challenging or inaccessible environments. This is why introducing unmanned aerial vehicles (UAVs) can be of great help to enhance the efficiency of SAR missions while simultaneously increasing the safety of everyone involved in the mission. Motivated by this, we design and experiment with autonomous UAV search for humans in a Mediterranean karst environment. The UAVs are directed using Heat equation-driven area coverage (HEDAC) ergodic control method according to known probability density and detection function. The implemented sensing framework consists of a probabilistic search model, motion control system, and computer vision object detection. It enables calculation of the probability of the target being detected in the SAR mission, and this paper focuses on experimental validation of proposed probabilistic framework and UAV control. The uniform probability density to ensure the even probability of finding the targets in the desired search area is achieved by assigning suitably thought-out tasks to 78 volunteers. The detection model is based on YOLO and trained with a previously collected ortho-photo image database. The experimental search is carefully planned and conducted, while as many parameters as possible are recorded. The thorough analysis consists of the motion control system, object detection, and the search validation. The assessment of the detection and search performance provides strong indication that the designed detection model in the UAV control algorithm is aligned with real-world results."
  },
  {
    "title": "Hybrid Human-Machine Perception via Adaptive LiDAR for Advanced Driver Assistance Systems",
    "url": "http://arxiv.org/abs/2502.17309v1",
    "arxiv_id": "2502.17309v1",
    "authors": [
      "Federico Scar\u00ec",
      "Nitin Jonathan Myers",
      "Chen Quan",
      "Arkady Zgonnikov"
    ],
    "published": "2025-02-24T16:44:20+00:00",
    "summary": "Accurate environmental perception is critical for advanced driver assistance systems (ADAS). Light detection and ranging (LiDAR) systems play a crucial role in ADAS; they can reliably detect obstacles and help ensure traffic safety. Existing research on LiDAR sensing has demonstrated that adapting the LiDAR's resolution and range based on environmental characteristics can improve machine perception. However, current adaptive LiDAR approaches for ADAS have not explored the possibility of combining the perception abilities of the vehicle and the human driver, which can potentially further enhance the detection performance. In this paper, we propose a novel system that adapts LiDAR characteristics to human driver's visual perception to enhance LiDAR sensing outside human's field of view. We develop a proof-of-concept prototype of the system in the virtual environment CARLA. Our system integrates real-time data on the driver's gaze to identify regions in the environment that the driver is monitoring. This allows the system to optimize LiDAR resources by dynamically increasing the LiDAR's range and resolution in peripheral areas that the driver may not be attending to. Our simulations show that this gaze-aware LiDAR enhances detection performance compared to a baseline standalone LiDAR, particularly in challenging environmental conditions like fog. Our hybrid human-machine sensing approach potentially offers improved safety and situational awareness in real-time driving scenarios for ADAS applications."
  },
  {
    "title": "REINFORCE Adversarial Attacks on Large Language Models: An Adaptive, Distributional, and Semantic Objective",
    "url": "http://arxiv.org/abs/2502.17254v1",
    "arxiv_id": "2502.17254v1",
    "authors": [
      "Simon Geisler",
      "Tom Wollschl\u00e4ger",
      "M. H. I. Abdalla",
      "Vincent Cohen-Addad",
      "Johannes Gasteiger",
      "Stephan G\u00fcnnemann"
    ],
    "published": "2025-02-24T15:34:48+00:00",
    "summary": "To circumvent the alignment of large language models (LLMs), current optimization-based adversarial attacks usually craft adversarial prompts by maximizing the likelihood of a so-called affirmative response. An affirmative response is a manually designed start of a harmful answer to an inappropriate request. While it is often easy to craft prompts that yield a substantial likelihood for the affirmative response, the attacked model frequently does not complete the response in a harmful manner. Moreover, the affirmative objective is usually not adapted to model-specific preferences and essentially ignores the fact that LLMs output a distribution over responses. If low attack success under such an objective is taken as a measure of robustness, the true robustness might be grossly overestimated. To alleviate these flaws, we propose an adaptive and semantic optimization problem over the population of responses. We derive a generally applicable objective via the REINFORCE policy-gradient formalism and demonstrate its efficacy with the state-of-the-art jailbreak algorithms Greedy Coordinate Gradient (GCG) and Projected Gradient Descent (PGD). For example, our objective doubles the attack success rate (ASR) on Llama3 and increases the ASR from 2% to 50% with circuit breaker defense."
  },
  {
    "title": "PrivaCI-Bench: Evaluating Privacy with Contextual Integrity and Legal Compliance",
    "url": "http://arxiv.org/abs/2502.17041v1",
    "arxiv_id": "2502.17041v1",
    "authors": [
      "Haoran Li",
      "Wenbin Hu",
      "Huihao Jing",
      "Yulin Chen",
      "Qi Hu",
      "Sirui Han",
      "Tianshu Chu",
      "Peizhao Hu",
      "Yangqiu Song"
    ],
    "published": "2025-02-24T10:49:34+00:00",
    "summary": "Recent advancements in generative large language models (LLMs) have enabled wider applicability, accessibility, and flexibility. However, their reliability and trustworthiness are still in doubt, especially for concerns regarding individuals' data privacy. Great efforts have been made on privacy by building various evaluation benchmarks to study LLMs' privacy awareness and robustness from their generated outputs to their hidden representations. Unfortunately, most of these works adopt a narrow formulation of privacy and only investigate personally identifiable information (PII). In this paper, we follow the merit of the Contextual Integrity (CI) theory, which posits that privacy evaluation should not only cover the transmitted attributes but also encompass the whole relevant social context through private information flows. We present PrivaCI-Bench, a comprehensive contextual privacy evaluation benchmark targeted at legal compliance to cover well-annotated privacy and safety regulations, real court cases, privacy policies, and synthetic data built from the official toolkit to study LLMs' privacy and safety compliance. We evaluate the latest LLMs, including the recent reasoner models QwQ-32B and Deepseek R1. Our experimental results suggest that though LLMs can effectively capture key CI parameters inside a given context, they still require further advancements for privacy compliance."
  },
  {
    "title": "LongSafety: Evaluating Long-Context Safety of Large Language Models",
    "url": "http://arxiv.org/abs/2502.16971v1",
    "arxiv_id": "2502.16971v1",
    "authors": [
      "Yida Lu",
      "Jiale Cheng",
      "Zhexin Zhang",
      "Shiyao Cui",
      "Cunxiang Wang",
      "Xiaotao Gu",
      "Yuxiao Dong",
      "Jie Tang",
      "Hongning Wang",
      "Minlie Huang"
    ],
    "published": "2025-02-24T08:54:39+00:00",
    "summary": "As Large Language Models (LLMs) continue to advance in understanding and generating long sequences, new safety concerns have been introduced through the long context. However, the safety of LLMs in long-context tasks remains under-explored, leaving a significant gap in both evaluation and improvement of their safety. To address this, we introduce LongSafety, the first comprehensive benchmark specifically designed to evaluate LLM safety in open-ended long-context tasks. LongSafety encompasses 7 categories of safety issues and 6 user-oriented long-context tasks, with a total of 1,543 test cases, averaging 5,424 words per context. Our evaluation towards 16 representative LLMs reveals significant safety vulnerabilities, with most models achieving safety rates below 55%. Our findings also indicate that strong safety performance in short-context scenarios does not necessarily correlate with safety in long-context tasks, emphasizing the unique challenges and urgency of improving long-context safety. Moreover, through extensive analysis, we identify challenging safety issues and task types for long-context models. Furthermore, we find that relevant context and extended input sequences can exacerbate safety risks in long-context scenarios, highlighting the critical need for ongoing attention to long-context safety challenges. Our code and data are available at https://github.com/thu-coai/LongSafety."
  },
  {
    "title": "GuidedBench: Equipping Jailbreak Evaluation with Guidelines",
    "url": "http://arxiv.org/abs/2502.16903v1",
    "arxiv_id": "2502.16903v1",
    "authors": [
      "Ruixuan Huang",
      "Xunguang Wang",
      "Zongjie Li",
      "Daoyuan Wu",
      "Shuai Wang"
    ],
    "published": "2025-02-24T06:57:27+00:00",
    "summary": "Jailbreaking methods for large language models (LLMs) have gained increasing attention for building safe and responsible AI systems. After analyzing 35 jailbreak methods across six categories, we find that existing benchmarks, relying on universal LLM-based or keyword-matching scores, lack case-specific criteria, leading to conflicting results. In this paper, we introduce a more robust evaluation framework for jailbreak methods, with a curated harmful question dataset, detailed case-by-case evaluation guidelines, and a scoring system equipped with these guidelines. Our experiments show that existing jailbreak methods exhibit better discrimination when evaluated using our benchmark. Some jailbreak methods that claim to achieve over 90% attack success rate (ASR) on other benchmarks only reach a maximum of 30.2% on our benchmark, providing a higher ceiling for more advanced jailbreak research; furthermore, using our scoring system reduces the variance of disagreements between different evaluator LLMs by up to 76.33%. This demonstrates its ability to provide more fair and stable evaluation."
  },
  {
    "title": "Leveraging Large Language Models for Effective and Explainable Multi-Agent Credit Assignment",
    "url": "http://arxiv.org/abs/2502.16863v1",
    "arxiv_id": "2502.16863v1",
    "authors": [
      "Kartik Nagpal",
      "Dayi Dong",
      "Jean-Baptiste Bouvier",
      "Negar Mehr"
    ],
    "published": "2025-02-24T05:56:47+00:00",
    "summary": "Recent work, spanning from autonomous vehicle coordination to in-space assembly, has shown the importance of learning collaborative behavior for enabling robots to achieve shared goals. A common approach for learning this cooperative behavior is to utilize the centralized-training decentralized-execution paradigm. However, this approach also introduces a new challenge: how do we evaluate the contributions of each agent's actions to the overall success or failure of the team. This credit assignment problem has remained open, and has been extensively studied in the Multi-Agent Reinforcement Learning literature. In fact, humans manually inspecting agent behavior often generate better credit evaluations than existing methods. We combine this observation with recent works which show Large Language Models demonstrate human-level performance at many pattern recognition tasks. Our key idea is to reformulate credit assignment to the two pattern recognition problems of sequence improvement and attribution, which motivates our novel LLM-MCA method. Our approach utilizes a centralized LLM reward-critic which numerically decomposes the environment reward based on the individualized contribution of each agent in the scenario. We then update the agents' policy networks based on this feedback. We also propose an extension LLM-TACA where our LLM critic performs explicit task assignment by passing an intermediary goal directly to each agent policy in the scenario. Both our methods far outperform the state-of-the-art on a variety of benchmarks, including Level-Based Foraging, Robotic Warehouse, and our new Spaceworld benchmark which incorporates collision-related safety constraints. As an artifact of our methods, we generate large trajectory datasets with each timestep annotated with per-agent reward information, as sampled from our LLM critics."
  },
  {
    "title": "PulseBat: A field-accessible dataset for second-life battery diagnostics from realistic histories using multidimensional rapid pulse test",
    "url": "http://arxiv.org/abs/2502.16848v1",
    "arxiv_id": "2502.16848v1",
    "authors": [
      "Shengyu Tao",
      "Guangyuan Ma",
      "Huixiong Yang",
      "Minyan Lu",
      "Guodan Wei",
      "Guangmin Zhou",
      "Xuan Zhang"
    ],
    "published": "2025-02-24T05:10:04+00:00",
    "summary": "As electric vehicles (EVs) approach the end of their operational life, their batteries retain significant economic value and present promising opportunities for second-life use and material recycling. This is particularly compelling for Global South and other underdeveloped regions, where reliable energy storage is vital to addressing critical challenges posed by weak and even nonexistent power grid and energy infrastructures. However, despite this potential, widespread adoption has been hindered by critical uncertainties surrounding the technical performance, safety, and recertification of second-life batteries. In cases where they have been redeployed, mismatches between estimated and actual performance often render batteries technically unsuitable or hazardous, turning them into liabilities for communities they were intended to benefit. This considerable misalignment exacerbates energy access disparities and undermines the broader vision of energy justice, highlighting an urgent need for robust and scalable solutions to unlock the potential. In the PulseBat Dataset, the authors tested 464 retired lithium-ion batteries, covering 3 cathode material types, 6 historical usages, 3 physical formats, and 6 capacity designs. The pulse test experiments were performed repeatedly for each second-life battery with 10 pulse width, 10 pulse magnitude, multiple state-of-charge, and state-of-health conditions, e.g., from 0.37 to 1.03. The PulseBat Dataset recorded these test conditions and the voltage response as well as the temperature signals that were subject to the injected pulse current, which could be used as a valuable data resource for critical diagnostics tasks such as state-of-charge estimation, state-of-health estimation, cathode material type identification, open-circuit voltage reconstruction, thermal management, and beyond."
  },
  {
    "title": "Multi-Agent Autonomous Driving Systems with Large Language Models: A Survey of Recent Advances",
    "url": "http://arxiv.org/abs/2502.16804v1",
    "arxiv_id": "2502.16804v1",
    "authors": [
      "Yaozu Wu",
      "Dongyuan Li",
      "Yankai Chen",
      "Renhe Jiang",
      "Henry Peng Zou",
      "Liancheng Fang",
      "Zhen Wang",
      "Philip S. Yu"
    ],
    "published": "2025-02-24T03:26:13+00:00",
    "summary": "Autonomous Driving Systems (ADSs) are revolutionizing transportation by reducing human intervention, improving operational efficiency, and enhancing safety. Large Language Models (LLMs), known for their exceptional planning and reasoning capabilities, have been integrated into ADSs to assist with driving decision-making. However, LLM-based single-agent ADSs face three major challenges: limited perception, insufficient collaboration, and high computational demands. To address these issues, recent advancements in LLM-based multi-agent ADSs have focused on improving inter-agent communication and cooperation. This paper provides a frontier survey of LLM-based multi-agent ADSs. We begin with a background introduction to related concepts, followed by a categorization of existing LLM-based approaches based on different agent interaction modes. We then discuss agent-human interactions in scenarios where LLM-based agents engage with humans. Finally, we summarize key applications, datasets, and challenges in this field to support future research (https://anonymous.4open.science/r/LLM-based_Multi-agent_ADS-3A5C/README.md)."
  },
  {
    "title": "Singularity resolution and regular black hole formation in gravitational collapse in asymptotically safe gravity",
    "url": "http://arxiv.org/abs/2502.16787v1",
    "arxiv_id": "2502.16787v1",
    "authors": [
      "Tomohiro Harada",
      "Chiang-Mei Chen",
      "Rituparna Mandal"
    ],
    "published": "2025-02-24T02:45:29+00:00",
    "summary": "We adopt an effective action inspired by asymptotically safe gravity, in which the effective gravitational constant is parameterized as $G(\\epsilon) = G_{N} [1 + \\tilde{\\omega} (G_{N}^{2} \\epsilon)^{\\alpha}]^{-1}$, where $G_{N}$ and $\\epsilon$ denote Newton's gravitational constant and the energy density of the matter field, respectively, with two dimensionless model parameters, $\\tilde{\\omega}$ and $\\alpha$. Within this framework, we investigate the complete gravitational collapse of a homogeneous ball of perfect fluid and find that the singularity is completely resolved for $\\alpha > 1$ but not for $1/2 \\le \\alpha \\le 1$. The case $0 < \\alpha < 1/2$ is inconsistent with asymptotic safety. Moreover, we note that although the singularity cannot be fully resolved for $\\alpha = 1$, it is significantly weakened by quantum gravity effects. Furthermore, we successfully construct a static exterior metric which, together with the interior solution, describes the dynamical formation of regular black holes in an asymptotically flat spacetime for the perfectly resolved case $\\alpha > 1$. The resulting regular black hole, obtained as the final static state, contains a de Sitter core and admits a static metric fully expressible in terms of the Lerch transcendent for general cases and in elementary functions for certain values of $\\alpha$, including \\alpha = 2$. We also discuss the formation of gravastars and the late-time evaporation process of the regular black holes."
  },
  {
    "title": "AISafetyLab: A Comprehensive Framework for AI Safety Evaluation and Improvement",
    "url": "http://arxiv.org/abs/2502.16776v1",
    "arxiv_id": "2502.16776v1",
    "authors": [
      "Zhexin Zhang",
      "Leqi Lei",
      "Junxiao Yang",
      "Xijie Huang",
      "Yida Lu",
      "Shiyao Cui",
      "Renmiao Chen",
      "Qinglin Zhang",
      "Xinyuan Wang",
      "Hao Wang",
      "Hao Li",
      "Xianqi Lei",
      "Chengwei Pan",
      "Lei Sha",
      "Hongning Wang",
      "Minlie Huang"
    ],
    "published": "2025-02-24T02:11:52+00:00",
    "summary": "As AI models are increasingly deployed across diverse real-world scenarios, ensuring their safety remains a critical yet underexplored challenge. While substantial efforts have been made to evaluate and enhance AI safety, the lack of a standardized framework and comprehensive toolkit poses significant obstacles to systematic research and practical adoption. To bridge this gap, we introduce AISafetyLab, a unified framework and toolkit that integrates representative attack, defense, and evaluation methodologies for AI safety. AISafetyLab features an intuitive interface that enables developers to seamlessly apply various techniques while maintaining a well-structured and extensible codebase for future advancements. Additionally, we conduct empirical studies on Vicuna, analyzing different attack and defense strategies to provide valuable insights into their comparative effectiveness. To facilitate ongoing research and development in AI safety, AISafetyLab is publicly available at https://github.com/thu-coai/AISafetyLab, and we are committed to its continuous maintenance and improvement."
  },
  {
    "title": "LED-Merging: Mitigating Safety-Utility Conflicts in Model Merging with Location-Election-Disjoint",
    "url": "http://arxiv.org/abs/2502.16770v1",
    "arxiv_id": "2502.16770v1",
    "authors": [
      "Qianli Ma",
      "Dongrui Liu",
      "Qian Chen",
      "Linfeng Zhang",
      "Jing Shao"
    ],
    "published": "2025-02-24T01:19:43+00:00",
    "summary": "Fine-tuning pre-trained Large Language Models (LLMs) for specialized tasks incurs substantial computational and data costs. While model merging offers a training-free solution to integrate multiple task-specific models, existing methods suffer from safety-utility conflicts where enhanced general capabilities degrade safety safeguards. We identify two root causes: \\textbf{neuron misidentification} due to simplistic parameter magnitude-based selection, and \\textbf{cross-task neuron interference} during merging. To address these challenges, we propose \\textbf{LED-Merging}, a three-stage framework that \\textbf{L}ocates task-specific neurons via gradient-based attribution, dynamically \\textbf{E}lects critical neurons through multi-model importance fusion, and \\textbf{D}isjoints conflicting updates through parameter isolation. Extensive experiments on Llama-3-8B, Mistral-7B, and Llama2-13B demonstrate that LED-Merging reduces harmful response rates(\\emph{e.g.}, a 31.4\\% decrease on Llama-3-8B-Instruct on HarmBench) while preserving 95\\% of utility performance(\\emph{e.g.}, 52.39\\% accuracy on GSM8K). LED-Merging resolves safety-utility conflicts and provides a lightweight, training-free paradigm for constructing reliable multi-task LLMs."
  },
  {
    "title": "ATEB: Evaluating and Improving Advanced NLP Tasks for Text Embedding Models",
    "url": "http://arxiv.org/abs/2502.16766v1",
    "arxiv_id": "2502.16766v1",
    "authors": [
      "Simeng Han",
      "Frank Palma Gomez",
      "Tu Vu",
      "Zefei Li",
      "Daniel Cer",
      "Hansi Zeng",
      "Chris Tar",
      "Arman Cohan",
      "Gustavo Hernandez Abrego"
    ],
    "published": "2025-02-24T01:08:15+00:00",
    "summary": "Traditional text embedding benchmarks primarily evaluate embedding models' capabilities to capture semantic similarity. However, more advanced NLP tasks require a deeper understanding of text, such as safety and factuality. These tasks demand an ability to comprehend and process complex information, often involving the handling of sensitive content, or the verification of factual statements against reliable sources. We introduce a new benchmark designed to assess and highlight the limitations of embedding models trained on existing information retrieval data mixtures on advanced capabilities, which include factuality, safety, instruction following, reasoning and document-level understanding. This benchmark includes a diverse set of tasks that simulate real-world scenarios where these capabilities are critical and leads to identification of the gaps of the currently advanced embedding models. Furthermore, we propose a novel method that reformulates these various tasks as retrieval tasks. By framing tasks like safety or factuality classification as retrieval problems, we leverage the strengths of retrieval models in capturing semantic relationships while also pushing them to develop a deeper understanding of context and content. Using this approach with single-task fine-tuning, we achieved performance gains of 8\\% on factuality classification and 13\\% on safety classification. Our code and data will be publicly available."
  },
  {
    "title": "Correlating and Predicting Human Evaluations of Language Models from Natural Language Processing Benchmarks",
    "url": "http://arxiv.org/abs/2502.18339v1",
    "arxiv_id": "2502.18339v1",
    "authors": [
      "Rylan Schaeffer",
      "Punit Singh Koura",
      "Binh Tang",
      "Ranjan Subramanian",
      "Aaditya K Singh",
      "Todor Mihaylov",
      "Prajjwal Bhargava",
      "Lovish Madaan",
      "Niladri S. Chatterji",
      "Vedanuj Goswami",
      "Sergey Edunov",
      "Dieuwke Hupkes",
      "Sanmi Koyejo",
      "Sharan Narang"
    ],
    "published": "2025-02-24T01:01:02+00:00",
    "summary": "The explosion of high-performing conversational language models (LMs) has spurred a shift from classic natural language processing (NLP) benchmarks to expensive, time-consuming and noisy human evaluations - yet the relationship between these two evaluation strategies remains hazy. In this paper, we conduct a large-scale study of four Chat Llama 2 models, comparing their performance on 160 standard NLP benchmarks (e.g., MMLU, ARC, BIG-Bench Hard) against extensive human preferences on more than 11k single-turn and 2k multi-turn dialogues from over 2k human annotators. Our findings are striking: most NLP benchmarks strongly correlate with human evaluations, suggesting that cheaper, automated metrics can serve as surprisingly reliable predictors of human preferences. Three human evaluations, such as adversarial dishonesty and safety, are anticorrelated with NLP benchmarks, while two are uncorrelated. Moreover, through overparameterized linear regressions, we show that NLP scores can accurately predict human evaluations across different model scales, offering a path to reduce costly human annotation without sacrificing rigor. Overall, our results affirm the continued value of classic benchmarks and illuminate how to harness them to anticipate real-world user satisfaction - pointing to how NLP benchmarks can be leveraged to meet evaluation needs of our new era of conversational AI."
  },
  {
    "title": "Watch Out E-scooter Coming Through: Multimodal Sensing of Mixed Traffic Use and Conflicts Through Riders Ego-centric Views",
    "url": "http://arxiv.org/abs/2502.16755v1",
    "arxiv_id": "2502.16755v1",
    "authors": [
      "Hiruni Nuwanthika Kegalle",
      "Danula Hettiachchi",
      "Jeffrey Chan",
      "Mark Sanderson",
      "Flora D. Salim"
    ],
    "published": "2025-02-24T00:16:18+00:00",
    "summary": "E-scooters are becoming a popular means of urban transportation. However, this increased popularity brings challenges, such as road accidents and conflicts when sharing space with traditional transport modes. An in-depth understanding of e-scooter rider behaviour is crucial for ensuring rider safety, guiding infrastructure planning, and enforcing traffic rules. This study investigated the rider behaviour through a naturalistic study with 23 participants equipped with a bike computer, eye-tracking glasses and cameras. They followed a pre-determined route, enabling multi-modal data collection. We analysed and compared gaze movements, speed, and video feeds across three transport infrastructure types: a pedestrian-shared path, a cycle lane and a roadway. Our findings reveal unique challenges e-scooter riders face, including difficulty keeping up with cyclists and motor vehicles due to speed limits on shared e-scooters, risks in signalling turns due to control lose, and limited acceptance in mixed-use spaces. The cycle lane showed the highest average speed, the least speed change points, and the least head movements, supporting its suitability as dedicated infrastructure for e-scooters. These findings are facilitated through multimodal sensing and analysing the e-scooter riders' ego-centric view, which show the efficacy of our method in discovering the behavioural dynamics of the riders in the wild. Our study highlights the critical need to align infrastructure with user behaviour to improve safety and emphasises the importance of targeted safety measures and regulations, especially when e-scooter riders share spaces with pedestrians or motor vehicles. The dataset and analysis code are available at https://github.com/HiruniNuwanthika/Electric-Scooter-Riders-Multi-Modal-Data-Analysis.git."
  },
  {
    "title": "Guardians of the Agentic System: Preventing Many Shots Jailbreak with Agentic System",
    "url": "http://arxiv.org/abs/2502.16750v1",
    "arxiv_id": "2502.16750v1",
    "authors": [
      "Saikat Barua",
      "Mostafizur Rahman",
      "Md Jafor Sadek",
      "Rafiul Islam",
      "Shehnaz Khaled",
      "Ahmedul Kabir"
    ],
    "published": "2025-02-23T23:35:15+00:00",
    "summary": "The autonomous AI agents using large language models can create undeniable values in all span of the society but they face security threats from adversaries that warrants immediate protective solutions because trust and safety issues arise. Considering the many-shot jailbreaking and deceptive alignment as some of the main advanced attacks, that cannot be mitigated by the static guardrails used during the supervised training, points out a crucial research priority for real world robustness. The combination of static guardrails in dynamic multi-agent system fails to defend against those attacks. We intend to enhance security for LLM-based agents through the development of new evaluation frameworks which identify and counter threats for safe operational deployment. Our work uses three examination methods to detect rogue agents through a Reverse Turing Test and analyze deceptive alignment through multi-agent simulations and develops an anti-jailbreaking system by testing it with GEMINI 1.5 pro and llama-3.3-70B, deepseek r1 models using tool-mediated adversarial scenarios. The detection capabilities are strong such as 94\\% accuracy for GEMINI 1.5 pro yet the system suffers persistent vulnerabilities when under long attacks as prompt length increases attack success rates (ASR) and diversity metrics become ineffective in prediction while revealing multiple complex system faults. The findings demonstrate the necessity of adopting flexible security systems based on active monitoring that can be performed by the agents themselves together with adaptable interventions by system admin as the current models can create vulnerabilities that can lead to the unreliable and vulnerable system. So, in our work, we try to address such situations and propose a comprehensive framework to counteract the security issues."
  },
  {
    "title": "Toward Responsible Federated Large Language Models: Leveraging a Safety Filter and Constitutional AI",
    "url": "http://arxiv.org/abs/2502.16691v1",
    "arxiv_id": "2502.16691v1",
    "authors": [
      "Eunchung Noh",
      "Jeonghun Baek"
    ],
    "published": "2025-02-23T19:12:10+00:00",
    "summary": "Recent research has increasingly focused on training large language models (LLMs) using federated learning, known as FedLLM. However, responsible AI (RAI), which aims to ensure safe responses, remains underexplored in the context of FedLLM. In FedLLM, client data used for training may contain harmful content, leading to unsafe LLMs that generate harmful responses. Aggregating such unsafe LLMs into the global model and distributing them to clients may result in the widespread deployment of unsafe LLMs. To address this issue, we incorporate two well-known RAI methods into FedLLM: the safety filter and constitutional AI. Our experiments demonstrate that these methods significantly enhance the safety of the LLM, achieving over a 20% improvement on AdvBench, a benchmark for evaluating safety performance."
  },
  {
    "title": "Analyzing Factors Influencing Driver Willingness to Accept Advanced Driver Assistance Systems",
    "url": "http://arxiv.org/abs/2502.16688v1",
    "arxiv_id": "2502.16688v1",
    "authors": [
      "Hannah Musau",
      "Nana Kankam Gyimah",
      "Judith Mwakalonge",
      "Gurcan Comert",
      "Saidi Siuhi"
    ],
    "published": "2025-02-23T19:01:54+00:00",
    "summary": "Advanced Driver Assistance Systems (ADAS) enhance highway safety by improving environmental perception and reducing human errors. However, misconceptions, trust issues, and knowledge gaps hinder widespread adoption. This study examines driver perceptions, knowledge sources, and usage patterns of ADAS in passenger vehicles. A nationwide survey collected data from a diverse sample of U.S. drivers. Machine learning models predicted ADAS adoption, with SHAP (SHapley Additive Explanations) identifying key influencing factors. Findings indicate that higher trust levels correlate with increased ADAS usage, while concerns about reliability remain a barrier. Specific features, such as Forward Collision Warning and Driver Monitoring Systems, significantly influence adoption likelihood. Demographic factors (age, gender) and driving habits (experience, frequency) also shape ADAS acceptance. Findings emphasize the influence of socioeconomic, demographic, and behavioral factors on ADAS adoption, offering guidance for automakers, policymakers, and safety advocates to improve awareness, trust, and usability."
  },
  {
    "title": "Security Analysis of 5G NR Device-to-Device Sidelink Communications",
    "url": "http://arxiv.org/abs/2502.16650v1",
    "arxiv_id": "2502.16650v1",
    "authors": [
      "Evangelos Bitsikas",
      "Aanjhan Ranganathan"
    ],
    "published": "2025-02-23T16:55:32+00:00",
    "summary": "5G NR sidelink communication enables new possibilities for direct device-to-device interactions, supporting applications from vehicle-to-everything (V2X) systems to public safety, industrial automation, and drone networks. However, these advancements come with significant security challenges due to the decentralized trust model and increased reliance on User Equipment (UE) for critical functions like synchronization, resource allocation, and authorization. This paper presents the first comprehensive security analysis of NR V2X sidelink. We identify vulnerabilities across critical procedures and demonstrate plausible attack, including attacks that manipulate data integrity feedback and block resources, ultimately undermining the reliability and privacy of sidelink communications. Our analysis reveals that NR operational modes are vulnerable, with the ones relying on autonomous resource management (without network supervision) particularly exposed. To address these issues, we propose mitigation strategies to enhance the security of 5G sidelink communications. This work establishes a foundation for future efforts to strengthen 5G device-to-device sidelink communications, ensuring its safe deployment in critical applications."
  },
  {
    "title": "CODESYNC: Synchronizing Large Language Models with Dynamic Code Evolution at Scale",
    "url": "http://arxiv.org/abs/2502.16645v1",
    "arxiv_id": "2502.16645v1",
    "authors": [
      "Chenlong Wang",
      "Zhaoyang Chu",
      "Zhengxiang Cheng",
      "Xuyi Yang",
      "Kaiyue Qiu",
      "Yao Wan",
      "Zhou Zhao",
      "Xuanhua Shi",
      "Dongping Chen"
    ],
    "published": "2025-02-23T16:46:18+00:00",
    "summary": "Large Language Models (LLMs) have exhibited exceptional performance in software engineering yet face challenges in adapting to continually evolving code knowledge, particularly regarding the frequent updates of third-party library APIs. This limitation, stemming from static pre-training datasets, often results in non-executable code or implementations with suboptimal safety and efficiency. To this end, this paper introduces CODESYNC, a data engine for identifying outdated code patterns and collecting real-time code knowledge updates from Python third-party libraries. Building upon CODESYNC, we develop CODESYNCBENCH, a comprehensive benchmark for assessing LLMs' ability to stay synchronized with code evolution, which covers real-world updates for 220 APIs from six Python libraries. Our benchmark offers 3,300 test cases across three evaluation tasks and an update-aware instruction tuning dataset consisting of 2,200 training samples. Extensive experiments on 14 state-of-the-art LLMs reveal that they struggle with dynamic code evolution, even with the support of advanced knowledge updating methods (e.g., DPO, ORPO, and SimPO). We believe that our benchmark can offer a strong foundation for the development of more effective methods for real-time code knowledge updating in the future. The experimental code and dataset are publicly available at: https://github.com/Lucky-voyage/Code-Sync."
  },
  {
    "title": "Color Information-Based Automated Mask Generation for Detecting Underwater Atypical Glare Areas",
    "url": "http://arxiv.org/abs/2502.16538v1",
    "arxiv_id": "2502.16538v1",
    "authors": [
      "Mingyu Jeon",
      "Yeonji Paeng",
      "Sejin Lee"
    ],
    "published": "2025-02-23T11:17:20+00:00",
    "summary": "Underwater diving assistance and safety support robots acquire real-time diver information through onboard underwater cameras. This study introduces a breath bubble detection algorithm that utilizes unsupervised K-means clustering, thereby addressing the high accuracy demands of deep learning models as well as the challenges associated with constructing supervised datasets. The proposed method fuses color data and relative spatial coordinates from underwater images, employs CLAHE to mitigate noise, and subsequently performs pixel clustering to isolate reflective regions. Experimental results demonstrate that the algorithm can effectively detect regions corresponding to breath bubbles in underwater images, and that the combined use of RGB, LAB, and HSV color spaces significantly enhances detection accuracy. Overall, this research establishes a foundation for monitoring diver conditions and identifying potential equipment malfunctions in underwater environments."
  },
  {
    "title": "GraphCheck: Breaking Long-Term Text Barriers with Extracted Knowledge Graph-Powered Fact-Checking",
    "url": "http://arxiv.org/abs/2502.16514v1",
    "arxiv_id": "2502.16514v1",
    "authors": [
      "Yingjian Chen",
      "Haoran Liu",
      "Yinhong Liu",
      "Rui Yang",
      "Han Yuan",
      "Yanran Fu",
      "Pengyuan Zhou",
      "Qingyu Chen",
      "James Caverlee",
      "Irene Li"
    ],
    "published": "2025-02-23T09:25:00+00:00",
    "summary": "Large language models (LLMs) are widely used, but they often generate subtle factual errors, especially in long-form text. These errors are fatal in some specialized domains such as medicine. Existing fact-checking with grounding documents methods face two main challenges: (1) they struggle to understand complex multihop relations in long documents, often overlooking subtle factual errors; (2) most specialized methods rely on pairwise comparisons, requiring multiple model calls, leading to high resource and computational costs. To address these challenges, we propose \\textbf{\\textit{GraphCheck}}, a fact-checking framework that uses extracted knowledge graphs to enhance text representation. Graph Neural Networks further process these graphs as a soft prompt, enabling LLMs to incorporate structured knowledge more effectively. Enhanced with graph-based reasoning, GraphCheck captures multihop reasoning chains which are often overlooked by existing methods, enabling precise and efficient fact-checking in a single inference call. Experimental results on seven benchmarks spanning both general and medical domains demonstrate a 6.1\\% overall improvement over baseline models. Notably, GraphCheck outperforms existing specialized fact-checkers and achieves comparable performance with state-of-the-art LLMs, such as DeepSeek-V3 and OpenAI-o1, with significantly fewer parameters."
  },
  {
    "title": "FanChuan: A Multilingual and Graph-Structured Benchmark For Parody Detection and Analysis",
    "url": "http://arxiv.org/abs/2502.16503v1",
    "arxiv_id": "2502.16503v1",
    "authors": [
      "Yilun Zheng",
      "Sha Li",
      "Fangkun Wu",
      "Yang Ziyi",
      "Lin Hongchao",
      "Zhichao Hu",
      "Cai Xinjun",
      "Ziming Wang",
      "Jinxuan Chen",
      "Sitao Luan",
      "Jiahao Xu",
      "Lihui Chen"
    ],
    "published": "2025-02-23T08:52:46+00:00",
    "summary": "Parody is an emerging phenomenon on social media, where individuals imitate a role or position opposite to their own, often for humor, provocation, or controversy. Detecting and analyzing parody can be challenging and is often reliant on context, yet it plays a crucial role in understanding cultural values, promoting subcultures, and enhancing self-expression. However, the study of parody is hindered by limited available data and deficient diversity in current datasets. To bridge this gap, we built seven parody datasets from both English and Chinese corpora, with 14,755 annotated users and 21,210 annotated comments in total. To provide sufficient context information, we also collect replies and construct user-interaction graphs to provide richer contextual information, which is lacking in existing datasets. With these datasets, we test traditional methods and Large Language Models (LLMs) on three key tasks: (1) parody detection, (2) comment sentiment analysis with parody, and (3) user sentiment analysis with parody. Our extensive experiments reveal that parody-related tasks still remain challenging for all models, and contextual information plays a critical role. Interestingly, we find that, in certain scenarios, traditional sentence embedding methods combined with simple classifiers can outperform advanced LLMs, i.e. DeepSeek-R1 and GPT-o3, highlighting parody as a significant challenge for LLMs."
  },
  {
    "title": "Facilitating Emergency Vehicle Passage in Congested Urban Areas Using Multi-agent Deep Reinforcement Learning",
    "url": "http://arxiv.org/abs/2502.16449v1",
    "arxiv_id": "2502.16449v1",
    "authors": [
      "Haoran Su"
    ],
    "published": "2025-02-23T05:34:32+00:00",
    "summary": "Emergency Response Time (ERT) is crucial for urban safety, measuring cities' ability to handle medical, fire, and crime emergencies. In NYC, medical ERT increased 72% from 7.89 minutes in 2014 to 14.27 minutes in 2024, with half of delays due to Emergency Vehicle (EMV) travel times. Each minute's delay in stroke response costs 2 million brain cells, while cardiac arrest survival drops 7-10% per minute.   This dissertation advances EMV facilitation through three contributions. First, EMVLight, a decentralized multi-agent reinforcement learning framework, integrates EMV routing with traffic signal pre-emption. It achieved 42.6% faster EMV travel times and 23.5% improvement for other vehicles.   Second, the Dynamic Queue-Jump Lane system uses Multi-Agent Proximal Policy Optimization for coordinated lane-clearing in mixed autonomous and human-driven traffic, reducing EMV travel times by 40%.   Third, an equity study of NYC Emergency Medical Services revealed disparities across boroughs: Staten Island faces delays due to sparse signalized intersections, while Manhattan struggles with congestion. Solutions include optimized EMS stations and improved intersection designs.   These contributions enhance EMV mobility and emergency service equity, offering insights for policymakers and urban planners to develop safer, more efficient transportation systems."
  },
  {
    "title": "Visual Reasoning Evaluation of Grok, Deepseek Janus, Gemini, Qwen, Mistral, and ChatGPT",
    "url": "http://arxiv.org/abs/2502.16428v1",
    "arxiv_id": "2502.16428v1",
    "authors": [
      "Nidhal Jegham",
      "Marwan Abdelatti",
      "Abdeltawab Hendawi"
    ],
    "published": "2025-02-23T04:01:43+00:00",
    "summary": "Traditional evaluations of multimodal large language models (LLMs) have been limited by their focus on single-image reasoning, failing to assess crucial aspects like contextual understanding, reasoning stability, and uncertainty calibration. This study addresses these limitations by introducing a novel benchmark that integrates multi-image reasoning tasks with rejection-based evaluation and positional bias detection. To evaluate these dimensions, we further introduce entropy as a novel metric for quantifying reasoning consistency across reordered answer variants. We applied this benchmark to assess Grok 3, ChatGPT-4o, ChatGPT-o1, Gemini 2.0 Flash Experimental, DeepSeek Janus models, Qwen2.5-VL-72B-Instruct, QVQ-72B-Preview, and Pixtral 12B across eight visual reasoning tasks, including difference spotting and diagram interpretation. Our findings reveal ChatGPT-o1 leading in overall accuracy (82.5\\%) and rejection accuracy (70.0\\%), closely followed by Gemini 2.0 Flash Experimental (70.8\\%). QVQ-72B-Preview demonstrated superior rejection accuracy (85.5\\%). Notably, Pixtral 12B (51.7\\%) showed promise in specific domains, while Janus models exhibited challenges in bias and uncertainty calibration, reflected in low rejection accuracies and high entropy scores. High entropy scores in Janus models (Janus 7B: 0.8392, Janus 1B: 0.787) underscore their susceptibility to positional bias and unstable reasoning, contrasting with the low entropy and robust reasoning of ChatGPT models. The study further demonstrates that model size is not the sole determinant of performance, as evidenced by Grok 3 underperformance despite its substantial parameter count. By employing multi-image contexts, rejection mechanisms, and entropy-based consistency metrics, this benchmark sets a new standard for evaluating multimodal LLMs, enabling a more robust and reliable assessment of next-generation AI systems."
  },
  {
    "title": "Navigation-GPT: A Robust and Adaptive Framework Utilizing Large Language Models for Navigation Applications",
    "url": "http://arxiv.org/abs/2502.16402v1",
    "arxiv_id": "2502.16402v1",
    "authors": [
      "Feng Ma",
      "Xiu-min Wang",
      "Chen Chen",
      "Xiao-bin Xu",
      "Xin-ping Yan"
    ],
    "published": "2025-02-23T01:41:58+00:00",
    "summary": "Existing navigation decision support systems often perform poorly when handling non-predefined navigation scenarios. Leveraging the generalization capabilities of large language model (LLM) in handling unknown scenarios, this research proposes a dual-core framework for LLM applications to address this issue. Firstly, through ReAct-based prompt engineering, a larger LLM core decomposes intricate navigation tasks into manageable sub-tasks, which autonomously invoke corresponding external tools to gather relevant information, using this feedback to mitigate the risk of LLM hallucinations. Subsequently, a fine-tuned and compact LLM core, acting like a first-mate is designed to process such information and unstructured external data, then to generates context-aware recommendations, ultimately delivering lookout insights and navigation hints that adhere to the International Regulations for Preventing Collisions at Sea (COLREGs) and other rules. Extensive experiments demonstrate the proposed framework not only excels in traditional ship collision avoidance tasks but also adapts effectively to unstructured, non-predefined, and unpredictable scenarios. A comparative analysis with DeepSeek-R1, GPT-4o and other SOTA models highlights the efficacy and rationality of the proposed framework. This research bridges the gap between conventional navigation systems and LLMs, offering a framework to enhance safety and operational efficiency across diverse navigation applications."
  },
  {
    "title": "An Expert Ensemble for Detecting Anomalous Scenes, Interactions, and Behaviors in Autonomous Driving",
    "url": "http://arxiv.org/abs/2502.16389v1",
    "arxiv_id": "2502.16389v1",
    "authors": [
      "Tianchen Ji",
      "Neeloy Chakraborty",
      "Andre Schreiber",
      "Katherine Driggs-Campbell"
    ],
    "published": "2025-02-23T00:43:23+00:00",
    "summary": "As automated vehicles enter public roads, safety in a near-infinite number of driving scenarios becomes one of the major concerns for the widespread adoption of fully autonomous driving. The ability to detect anomalous situations outside of the operational design domain is a key component in self-driving cars, enabling us to mitigate the impact of abnormal ego behaviors and to realize trustworthy driving systems. On-road anomaly detection in egocentric videos remains a challenging problem due to the difficulties introduced by complex and interactive scenarios. We conduct a holistic analysis of common on-road anomaly patterns, from which we propose three unsupervised anomaly detection experts: a scene expert that focuses on frame-level appearances to detect abnormal scenes and unexpected scene motions; an interaction expert that models normal relative motions between two road participants and raises alarms whenever anomalous interactions emerge; and a behavior expert which monitors abnormal behaviors of individual objects by future trajectory prediction. To combine the strengths of all the modules, we propose an expert ensemble (Xen) using a Kalman filter, in which the final anomaly score is absorbed as one of the states and the observations are generated by the experts. Our experiments employ a novel evaluation protocol for realistic model performance, demonstrate superior anomaly detection performance than previous methods, and show that our framework has potential in classifying anomaly types using unsupervised learning on a large-scale on-road anomaly dataset."
  },
  {
    "title": "Understanding Generative AI Risks for Youth: A Taxonomy Based on Empirical Data",
    "url": "http://arxiv.org/abs/2502.16383v1",
    "arxiv_id": "2502.16383v1",
    "authors": [
      "Yaman Yu",
      "Yiren Liu",
      "Jacky Zhang",
      "Yun Huang",
      "Yang Wang"
    ],
    "published": "2025-02-22T23:31:51+00:00",
    "summary": "Generative AI (GAI) is reshaping the way young users engage with technology. This study introduces a taxonomy of risks associated with youth-GAI interactions, derived from an analysis of 344 chat transcripts between youth and GAI chatbots, 30,305 Reddit discussions concerning youth engagement with these systems, and 153 documented AI-related incidents. We categorize risks into six overarching themes, identifying 84 specific risks, which we further align with four distinct interaction pathways. Our findings highlight emerging concerns, such as risks to mental wellbeing, behavioral and social development, and novel forms of toxicity, privacy breaches, and misuse/exploitation that are not fully addressed in existing frameworks on child online safety or AI risks. By systematically grounding our taxonomy in empirical data, this work offers a structured approach to aiding AI developers, educators, caregivers, and policymakers in comprehending and mitigating risks associated with youth-GAI interactions."
  },
  {
    "title": "A generative approach to LLM harmfulness detection with special red flag tokens",
    "url": "http://arxiv.org/abs/2502.16366v1",
    "arxiv_id": "2502.16366v1",
    "authors": [
      "Sophie Xhonneux",
      "David Dobre",
      "Mehrnaz Mohfakhami",
      "Leo Schwinn",
      "Gauthier Gidel"
    ],
    "published": "2025-02-22T21:48:48+00:00",
    "summary": "Most safety training methods for large language models (LLMs) based on fine-tuning rely on dramatically changing the output distribution of the model when faced with a harmful request, shifting it from an unsafe answer to a refusal to respond. These methods inherently compromise model capabilities and might make auto-regressive models vulnerable to attacks that make likely an initial token of affirmative response. To avoid that, we propose to expand the model's vocabulary with a special token we call red flag token (<rf>) and propose to fine-tune the model to generate this token at any time harmful content is generated or about to be generated. This novel safety training method effectively augments LLMs into generative classifiers of harmfulness at all times during the conversation. This method offers several advantages: it enables the model to explicitly learn the concept of harmfulness while marginally affecting the generated distribution, thus maintaining the model's utility. It also evaluates each generated answer rather than just the input prompt and provides a stronger defence against sampling-based attacks. In addition, it simplifies the evaluation of the model's robustness and reduces correlated failures when combined with a classifier. We further show an increased robustness to long contexts, and supervised fine-tuning attacks."
  },
  {
    "title": "A Framework for Evaluating Vision-Language Model Safety: Building Trust in AI for Public Sector Applications",
    "url": "http://arxiv.org/abs/2502.16361v1",
    "arxiv_id": "2502.16361v1",
    "authors": [
      "Maisha Binte Rashid",
      "Pablo Rivas"
    ],
    "published": "2025-02-22T21:33:26+00:00",
    "summary": "Vision-Language Models (VLMs) are increasingly deployed in public sector missions, necessitating robust evaluation of their safety and vulnerability to adversarial attacks. This paper introduces a novel framework to quantify adversarial risks in VLMs. We analyze model performance under Gaussian, salt-and-pepper, and uniform noise, identifying misclassification thresholds and deriving composite noise patches and saliency patterns that highlight vulnerable regions. These patterns are compared against the Fast Gradient Sign Method (FGSM) to assess their adversarial effectiveness. We propose a new Vulnerability Score that combines the impact of random noise and adversarial attacks, providing a comprehensive metric for evaluating model robustness."
  },
  {
    "title": "Risk-Averse Reinforcement Learning: An Optimal Transport Perspective on Temporal Difference Learning",
    "url": "http://arxiv.org/abs/2502.16328v1",
    "arxiv_id": "2502.16328v1",
    "authors": [
      "Zahra Shahrooei",
      "Ali Baheri"
    ],
    "published": "2025-02-22T19:14:36+00:00",
    "summary": "The primary goal of reinforcement learning is to develop decision-making policies that prioritize optimal performance, frequently without considering risk or safety. In contrast, safe reinforcement learning seeks to reduce or avoid unsafe states. This letter introduces a risk-averse temporal difference algorithm that uses optimal transport theory to direct the agent toward predictable behavior. By incorporating a risk indicator, the agent learns to favor actions with predictable consequences. We evaluate the proposed algorithm in several case studies and show its effectiveness in the presence of uncertainty. The results demonstrate that our method reduces the frequency of visits to risky states while preserving performance. A Python implementation of the algorithm is available at https:// github.com/SAILRIT/Risk-averse-TD-Learning."
  },
  {
    "title": "Optimization-free Smooth Control Barrier Function for Polygonal Collision Avoidance",
    "url": "http://arxiv.org/abs/2502.16293v1",
    "arxiv_id": "2502.16293v1",
    "authors": [
      "Shizhen Wu",
      "Yongchun Fang",
      "Ning Sun",
      "Biao Lu",
      "Xiao Liang",
      "Yiming Zhao"
    ],
    "published": "2025-02-22T16:47:27+00:00",
    "summary": "Polygonal collision avoidance (PCA) is short for the problem of collision avoidance between two polygons (i.e., polytopes in planar) that own their dynamic equations. This problem suffers the inherent difficulty in dealing with non-smooth boundaries and recently optimization-defined metrics, such as signed distance field (SDF) and its variants, have been proposed as control barrier functions (CBFs) to tackle PCA problems. In contrast, we propose an optimization-free smooth CBF method in this paper, which is computationally efficient and proved to be nonconservative. It is achieved by three main steps: a lower bound of SDF is expressed as a nested Boolean logic composition first, then its smooth approximation is established by applying the latest log-sum-exp method, after which a specified CBF-based safety filter is proposed to address this class of problems. To illustrate its wide applications, the optimization-free smooth CBF method is extended to solve distributed collision avoidance of two underactuated nonholonomic vehicles and drive an underactuated container crane to avoid a moving obstacle respectively, for which numerical simulations are also performed."
  },
  {
    "title": "Pseudo-Measurement Enhancement in Power Distribution Systems",
    "url": "http://arxiv.org/abs/2502.16188v1",
    "arxiv_id": "2502.16188v1",
    "authors": [
      "Tao Xu",
      "Kaiqi Wang",
      "Jiadong Zhang",
      "Ji Qiao",
      "Zixuan Zhao",
      "Hong Zhu",
      "Kai Sun"
    ],
    "published": "2025-02-22T11:17:38+00:00",
    "summary": "With the rapid development of smart distribution networks (DNs), the integrity and accuracy of grid measurement data are crucial to the safety and stability of the entire system. However, the quality of the user power consumption data cannot be guaranteed during the collection and transmission process. To this end, this paper proposes a low-rank tensor completion model based on CANDECOMP/PARAFAC decomposition (CPD-LRTC) to enhance the quality of the measurement data of the DNs. Firstly, the causes and the associated characteristics of the missing data are analyzed, and a third-order standard tensor is constructed as a mathematical model of the measurement data of the DN. Then, a completion model is established based on the characteristics of measurement data and the low rank of the completion tensor, and the alternating direction method of multipliers (ADMM) is used to solve it iteratively. Finally, the proposed model is verified through two case studies, the completion accuracy, the computational efficiency, and the memory usage are compared to traditional methods."
  },
  {
    "title": "On Asymptotic safety in 4D gauge theory with additional dimension=4 operators",
    "url": "http://arxiv.org/abs/2502.16187v1",
    "arxiv_id": "2502.16187v1",
    "authors": [
      "Alfiia Mukhaeva"
    ],
    "published": "2025-02-22T11:15:11+00:00",
    "summary": "We study interacting fixed points of simple quantum field theory in four-dimensional $SU(N_c)$ coupled to $N_f$ species of color fermions and $N_f^2$ colorless scalars in the Veneziano limit. Using the rich structure of all possible quartic scalar operators, we find an interacting conformal fixed point with stable vacua and crossovers inbetween. We perform calculations in perturbation theory up to four loop in the gauge and three loop in the Yukawa and scalar couplings. We also consider anomalous dimensions for fields, scalar mass squared, and a class of dimension-three operators."
  },
  {
    "title": "Maybe I Should Not Answer That, but... Do LLMs Understand The Safety of Their Inputs?",
    "url": "http://arxiv.org/abs/2502.16174v1",
    "arxiv_id": "2502.16174v1",
    "authors": [
      "Maciej Chrab\u0105szcz",
      "Filip Szatkowski",
      "Bartosz W\u00f3jcik",
      "Jan Dubi\u0144ski",
      "Tomasz Trzci\u0144ski"
    ],
    "published": "2025-02-22T10:31:50+00:00",
    "summary": "Ensuring the safety of the Large Language Model (LLM) is critical, but currently used methods in most cases sacrifice the model performance to obtain increased safety or perform poorly on data outside of their adaptation distribution. We investigate existing methods for such generalization and find them insufficient. Surprisingly, while even plain LLMs recognize unsafe prompts, they may still generate unsafe responses. To avoid performance degradation and preserve safe performance, we advocate for a two-step framework, where we first identify unsafe prompts via a lightweight classifier, and apply a \"safe\" model only to such prompts. In particular, we explore the design of the safety detector in more detail, investigating the use of different classifier architectures and prompting techniques. Interestingly, we find that the final hidden state for the last token is enough to provide robust performance, minimizing false positives on benign data while performing well on malicious prompt detection. Additionally, we show that classifiers trained on the representations from different model layers perform comparably on the latest model layers, indicating that safety representation is present in the LLMs' hidden states at most model stages. Our work is a step towards efficient, representation-based safety mechanisms for LLMs."
  },
  {
    "title": "Be a Multitude to Itself: A Prompt Evolution Framework for Red Teaming",
    "url": "http://arxiv.org/abs/2502.16109v1",
    "arxiv_id": "2502.16109v1",
    "authors": [
      "Rui Li",
      "Peiyi Wang",
      "Jingyuan Ma",
      "Di Zhang",
      "Lei Sha",
      "Zhifang Sui"
    ],
    "published": "2025-02-22T06:13:19+00:00",
    "summary": "Large Language Models (LLMs) have gained increasing attention for their remarkable capacity, alongside concerns about safety arising from their potential to produce harmful content. Red teaming aims to find prompts that could elicit harmful responses from LLMs, and is essential to discover and mitigate safety risks before real-world deployment. However, manual red teaming is both time-consuming and expensive, rendering it unscalable. In this paper, we propose RTPE, a scalable evolution framework to evolve red teaming prompts across both breadth and depth dimensions, facilitating the automatic generation of numerous high-quality and diverse red teaming prompts. Specifically, in-breadth evolving employs a novel enhanced in-context learning method to create a multitude of quality prompts, whereas in-depth evolving applies customized transformation operations to enhance both content and form of prompts, thereby increasing diversity. Extensive experiments demonstrate that RTPE surpasses existing representative automatic red teaming methods on both attack success rate and diversity. In addition, based on 4,800 red teaming prompts created by RTPE, we further provide a systematic analysis of 8 representative LLMs across 8 sensitive topics."
  },
  {
    "title": "Online Learning of Danger Avoidance for Complex Structures of Musculoskeletal Humanoids and Its Applications",
    "url": "http://arxiv.org/abs/2502.16085v1",
    "arxiv_id": "2502.16085v1",
    "authors": [
      "Kento Kawaharazuka",
      "Naoki Hiraoka",
      "Yuya Koga",
      "Manabu Nishiura",
      "Yusuke Omura",
      "Yuki Asano",
      "Kei Okada",
      "Koji Kawasaki",
      "Masayuki Inaba"
    ],
    "published": "2025-02-22T05:16:18+00:00",
    "summary": "The complex structure of musculoskeletal humanoids makes it difficult to model them, and the inter-body interference and high internal muscle force are unavoidable. Although various safety mechanisms have been developed to solve this problem, it is important not only to deal with the dangers when they occur but also to prevent them from happening. In this study, we propose a method to learn a network outputting danger probability corresponding to the muscle length online so that the robot can gradually prevent dangers from occurring. Applications of this network for control are also described. The method is applied to the musculoskeletal humanoid, Musashi, and its effectiveness is verified."
  },
  {
    "title": "Multimodal Inconsistency Reasoning (MMIR): A New Benchmark for Multimodal Reasoning Models",
    "url": "http://arxiv.org/abs/2502.16033v1",
    "arxiv_id": "2502.16033v1",
    "authors": [
      "Qianqi Yan",
      "Yue Fan",
      "Hongquan Li",
      "Shan Jiang",
      "Yang Zhao",
      "Xinze Guan",
      "Ching-Chen Kuo",
      "Xin Eric Wang"
    ],
    "published": "2025-02-22T01:52:37+00:00",
    "summary": "Existing Multimodal Large Language Models (MLLMs) are predominantly trained and tested on consistent visual-textual inputs, leaving open the question of whether they can handle inconsistencies in real-world, layout-rich content. To bridge this gap, we propose the Multimodal Inconsistency Reasoning (MMIR) benchmark to assess MLLMs' ability to detect and reason about semantic mismatches in artifacts such as webpages, presentation slides, and posters. MMIR comprises 534 challenging samples, each containing synthetically injected errors across five reasoning-heavy categories: Factual Contradiction, Identity Misattribution, Contextual Mismatch, Quantitative Discrepancy, and Temporal/Spatial Incoherence. We evaluate six state-of-the-art MLLMs, showing that models with dedicated multimodal reasoning capabilities, such as o1, substantially outperform their counterparts while open-source models remain particularly vulnerable to inconsistency errors. Detailed error analyses further show that models excel in detecting inconsistencies confined to a single modality, particularly in text, but struggle with cross-modal conflicts and complex layouts. Probing experiments reveal that single-modality prompting, including Chain-of-Thought (CoT) and Set-of-Mark (SoM) methods, yields marginal gains, revealing a key bottleneck in cross-modal reasoning. Our findings highlight the need for advanced multimodal reasoning and point to future research on multimodal inconsistency."
  },
  {
    "title": "Cross-Model Transferability of Adversarial Patches in Real-time Segmentation for Autonomous Driving",
    "url": "http://arxiv.org/abs/2502.16012v1",
    "arxiv_id": "2502.16012v1",
    "authors": [
      "Prashant Shekhar",
      "Bidur Devkota",
      "Dumindu Samaraweera",
      "Laxima Niure Kandel",
      "Manoj Babu"
    ],
    "published": "2025-02-22T00:03:53+00:00",
    "summary": "Adversarial attacks pose a significant threat to deep learning models, particularly in safety-critical applications like healthcare and autonomous driving. Recently, patch based attacks have demonstrated effectiveness in real-time inference scenarios owing to their 'drag and drop' nature. Following this idea for Semantic Segmentation (SS), here we propose a novel Expectation Over Transformation (EOT) based adversarial patch attack that is more realistic for autonomous vehicles. To effectively train this attack we also propose a 'simplified' loss function that is easy to analyze and implement. Using this attack as our basis, we investigate whether adversarial patches once optimized on a specific SS model, can fool other models or architectures. We conduct a comprehensive cross-model transferability analysis of adversarial patches trained on SOTA Convolutional Neural Network (CNN) models such PIDNet-S, PIDNet-M and PIDNet-L, among others. Additionally, we also include the Segformer model to study transferability to Vision Transformers (ViTs). All of our analysis is conducted on the widely used Cityscapes dataset. Our study reveals key insights into how model architectures (CNN vs CNN or CNN vs. Transformer-based) influence attack susceptibility. In particular, we conclude that although the transferability (effectiveness) of attacks on unseen images of any dimension is really high, the attacks trained against one particular model are minimally effective on other models. And this was found to be true for both ViT and CNN based models. Additionally our results also indicate that for CNN-based models, the repercussions of patch attacks are local, unlike ViTs. Per-class analysis reveals that simple-classes like 'sky' suffer less misclassification than others. The code for the project is available at: https://github.com/p-shekhar/adversarial-patch-transferability"
  },
  {
    "title": "On the Design of Safe Continual RL Methods for Control of Nonlinear Systems",
    "url": "http://arxiv.org/abs/2502.15922v1",
    "arxiv_id": "2502.15922v1",
    "authors": [
      "Austin Coursey",
      "Marcos Quinones-Grueiro",
      "Gautam Biswas"
    ],
    "published": "2025-02-21T20:34:40+00:00",
    "summary": "Reinforcement learning (RL) algorithms have been successfully applied to control tasks associated with unmanned aerial vehicles and robotics. In recent years, safe RL has been proposed to allow the safe execution of RL algorithms in industrial and mission-critical systems that operate in closed loops. However, if the system operating conditions change, such as when an unknown fault occurs in the system, typical safe RL algorithms are unable to adapt while retaining past knowledge. Continual reinforcement learning algorithms have been proposed to address this issue. However, the impact of continual adaptation on the system's safety is an understudied problem. In this paper, we study the intersection of safe and continual RL. First, we empirically demonstrate that a popular continual RL algorithm, online elastic weight consolidation, is unable to satisfy safety constraints in non-linear systems subject to varying operating conditions. Specifically, we study the MuJoCo HalfCheetah and Ant environments with velocity constraints and sudden joint loss non-stationarity. Then, we show that an agent trained using constrained policy optimization, a safe RL algorithm, experiences catastrophic forgetting in continual learning settings. With this in mind, we explore a simple reward-shaping method to ensure that elastic weight consolidation prioritizes remembering both safety and task performance for safety-constrained, non-linear, and non-stationary dynamical systems."
  },
  {
    "title": "VaViM and VaVAM: Autonomous Driving through Video Generative Modeling",
    "url": "http://arxiv.org/abs/2502.15672v1",
    "arxiv_id": "2502.15672v1",
    "authors": [
      "Florent Bartoccioni",
      "Elias Ramzi",
      "Victor Besnier",
      "Shashanka Venkataramanan",
      "Tuan-Hung Vu",
      "Yihong Xu",
      "Loick Chambon",
      "Spyros Gidaris",
      "Serkan Odabas",
      "David Hurych",
      "Renaud Marlet",
      "Alexandre Boulch",
      "Mickael Chen",
      "\u00c9loi Zablocki",
      "Andrei Bursuc",
      "Eduardo Valle",
      "Matthieu Cord"
    ],
    "published": "2025-02-21T18:56:02+00:00",
    "summary": "We explore the potential of large-scale generative video models for autonomous driving, introducing an open-source auto-regressive video model (VaViM) and its companion video-action model (VaVAM) to investigate how video pre-training transfers to real-world driving. VaViM is a simple auto-regressive video model that predicts frames using spatio-temporal token sequences. We show that it captures the semantics and dynamics of driving scenes. VaVAM, the video-action model, leverages the learned representations of VaViM to generate driving trajectories through imitation learning. Together, the models form a complete perception-to-action pipeline. We evaluate our models in open- and closed-loop driving scenarios, revealing that video-based pre-training holds promise for autonomous driving. Key insights include the semantic richness of the learned representations, the benefits of scaling for video synthesis, and the complex relationship between model size, data, and safety metrics in closed-loop evaluations. We release code and model weights at https://github.com/valeoai/VideoActionModel"
  },
  {
    "title": "A Comprehensive Survey on the Trustworthiness of Large Language Models in Healthcare",
    "url": "http://arxiv.org/abs/2502.15871v1",
    "arxiv_id": "2502.15871v1",
    "authors": [
      "Manar Aljohani",
      "Jun Hou",
      "Sindhura Kommu",
      "Xuan Wang"
    ],
    "published": "2025-02-21T18:43:06+00:00",
    "summary": "The application of large language models (LLMs) in healthcare has the potential to revolutionize clinical decision-making, medical research, and patient care. As LLMs are increasingly integrated into healthcare systems, several critical challenges must be addressed to ensure their reliable and ethical deployment. These challenges include truthfulness, where models generate misleading information; privacy, with risks of unintentional data retention; robustness, requiring defenses against adversarial attacks; fairness, addressing biases in clinical outcomes; explainability, ensuring transparent decision-making; and safety, mitigating risks of misinformation and medical errors. Recently, researchers have begun developing benchmarks and evaluation frameworks to systematically assess the trustworthiness of LLMs. However, the trustworthiness of LLMs in healthcare remains underexplored, lacking a systematic review that provides a comprehensive understanding and future insights into this area. This survey bridges this gap by providing a comprehensive overview of the recent research of existing methodologies and solutions aimed at mitigating the above risks in healthcare. By focusing on key trustworthiness dimensions including truthfulness, privacy and safety, robustness, fairness and bias, and explainability, we present a thorough analysis of how these issues impact the reliability and ethical use of LLMs in healthcare. This paper highlights ongoing efforts and offers insights into future research directions to ensure the safe and trustworthy deployment of LLMs in healthcare."
  },
  {
    "title": "Superintelligent Agents Pose Catastrophic Risks: Can Scientist AI Offer a Safer Path?",
    "url": "http://arxiv.org/abs/2502.15657v1",
    "arxiv_id": "2502.15657v1",
    "authors": [
      "Yoshua Bengio",
      "Michael Cohen",
      "Damiano Fornasiere",
      "Joumana Ghosn",
      "Pietro Greiner",
      "Matt MacDermott",
      "S\u00f6ren Mindermann",
      "Adam Oberman",
      "Jesse Richardson",
      "Oliver Richardson",
      "Marc-Antoine Rondeau",
      "Pierre-Luc St-Charles",
      "David Williams-King"
    ],
    "published": "2025-02-21T18:28:36+00:00",
    "summary": "The leading AI companies are increasingly focused on building generalist AI agents -- systems that can autonomously plan, act, and pursue goals across almost all tasks that humans can perform. Despite how useful these systems might be, unchecked AI agency poses significant risks to public safety and security, ranging from misuse by malicious actors to a potentially irreversible loss of human control. We discuss how these risks arise from current AI training methods. Indeed, various scenarios and experiments have demonstrated the possibility of AI agents engaging in deception or pursuing goals that were not specified by human operators and that conflict with human interests, such as self-preservation. Following the precautionary principle, we see a strong need for safer, yet still useful, alternatives to the current agency-driven trajectory. Accordingly, we propose as a core building block for further advances the development of a non-agentic AI system that is trustworthy and safe by design, which we call Scientist AI. This system is designed to explain the world from observations, as opposed to taking actions in it to imitate or please humans. It comprises a world model that generates theories to explain data and a question-answering inference machine. Both components operate with an explicit notion of uncertainty to mitigate the risks of overconfident predictions. In light of these considerations, a Scientist AI could be used to assist human researchers in accelerating scientific progress, including in AI safety. In particular, our system can be employed as a guardrail against AI agents that might be created despite the risks involved. Ultimately, focusing on non-agentic AI may enable the benefits of AI innovation while avoiding the risks associated with the current trajectory. We hope these arguments will motivate researchers, developers, and policymakers to favor this safer path."
  },
  {
    "title": "Superintelligent Agents Pose Catastrophic Risks: Can Scientist AI Offer a Safer Path?",
    "url": "http://arxiv.org/abs/2502.15657v2",
    "arxiv_id": "2502.15657v2",
    "authors": [
      "Yoshua Bengio",
      "Michael Cohen",
      "Damiano Fornasiere",
      "Joumana Ghosn",
      "Pietro Greiner",
      "Matt MacDermott",
      "S\u00f6ren Mindermann",
      "Adam Oberman",
      "Jesse Richardson",
      "Oliver Richardson",
      "Marc-Antoine Rondeau",
      "Pierre-Luc St-Charles",
      "David Williams-King"
    ],
    "published": "2025-02-21T18:28:36+00:00",
    "summary": "The leading AI companies are increasingly focused on building generalist AI agents -- systems that can autonomously plan, act, and pursue goals across almost all tasks that humans can perform. Despite how useful these systems might be, unchecked AI agency poses significant risks to public safety and security, ranging from misuse by malicious actors to a potentially irreversible loss of human control. We discuss how these risks arise from current AI training methods. Indeed, various scenarios and experiments have demonstrated the possibility of AI agents engaging in deception or pursuing goals that were not specified by human operators and that conflict with human interests, such as self-preservation. Following the precautionary principle, we see a strong need for safer, yet still useful, alternatives to the current agency-driven trajectory. Accordingly, we propose as a core building block for further advances the development of a non-agentic AI system that is trustworthy and safe by design, which we call Scientist AI. This system is designed to explain the world from observations, as opposed to taking actions in it to imitate or please humans. It comprises a world model that generates theories to explain data and a question-answering inference machine. Both components operate with an explicit notion of uncertainty to mitigate the risks of overconfident predictions. In light of these considerations, a Scientist AI could be used to assist human researchers in accelerating scientific progress, including in AI safety. In particular, our system can be employed as a guardrail against AI agents that might be created despite the risks involved. Ultimately, focusing on non-agentic AI may enable the benefits of AI innovation while avoiding the risks associated with the current trajectory. We hope these arguments will motivate researchers, developers, and policymakers to favor this safer path."
  },
  {
    "title": "A Simulation Pipeline to Facilitate Real-World Robotic Reinforcement Learning Applications",
    "url": "http://arxiv.org/abs/2502.15649v1",
    "arxiv_id": "2502.15649v1",
    "authors": [
      "Jefferson Silveira",
      "Joshua A. Marshall",
      "Sidney N. Givigi Jr"
    ],
    "published": "2025-02-21T18:16:05+00:00",
    "summary": "Reinforcement learning (RL) has gained traction for its success in solving complex tasks for robotic applications. However, its deployment on physical robots remains challenging due to safety risks and the comparatively high costs of training. To avoid these problems, RL agents are often trained on simulators, which introduces a new problem related to the gap between simulation and reality. This paper presents an RL pipeline designed to help reduce the reality gap and facilitate developing and deploying RL policies for real-world robotic systems. The pipeline organizes the RL training process into an initial step for system identification and three training stages: core simulation training, high-fidelity simulation, and real-world deployment, each adding levels of realism to reduce the sim-to-real gap. Each training stage takes an input policy, improves it, and either passes the improved policy to the next stage or loops it back for further improvement. This iterative process continues until the policy achieves the desired performance. The pipeline's effectiveness is shown through a case study with the Boston Dynamics Spot mobile robot used in a surveillance application. The case study presents the steps taken at each pipeline stage to obtain an RL agent to control the robot's position and orientation."
  },
  {
    "title": "The Relationship Between Reasoning and Performance in Large Language Models -- o3 (mini) Thinks Harder, Not Longer",
    "url": "http://arxiv.org/abs/2502.15631v1",
    "arxiv_id": "2502.15631v1",
    "authors": [
      "Marthe Ballon",
      "Andres Algaba",
      "Vincent Ginis"
    ],
    "published": "2025-02-21T17:59:13+00:00",
    "summary": "Large language models have demonstrated remarkable progress in mathematical reasoning, leveraging chain-of-thought and test-time compute scaling. However, many open questions remain regarding the interplay between reasoning token usage and accuracy gains. In particular, when comparing models across generations, it is unclear whether improved performance results from longer reasoning chains or more efficient reasoning. We systematically analyze chain-of-thought length across o1-mini and o3-mini variants on the Omni-MATH benchmark, finding that o3-mini (m) achieves superior accuracy without requiring longer reasoning chains than o1-mini. Moreover, we show that accuracy generally declines as reasoning chains grow across all models and compute settings, even when controlling for difficulty of the questions. This accuracy drop is significantly smaller in more proficient models, suggesting that new generations of reasoning models use test-time compute more effectively. Finally, we highlight that while o3-mini (h) achieves a marginal accuracy gain over o3-mini (m), it does so by allocating substantially more reasoning tokens across all problems, even the ones that o3-mini (m) can already solve. These findings provide new insights into the relationship between model capability and reasoning length, with implications for efficiency, scaling, and evaluation methodologies."
  },
  {
    "title": "Pick-and-place Manipulation Across Grippers Without Retraining: A Learning-optimization Diffusion Policy Approach",
    "url": "http://arxiv.org/abs/2502.15613v1",
    "arxiv_id": "2502.15613v1",
    "authors": [
      "Xiangtong Yao",
      "Yirui Zhou",
      "Yuan Meng",
      "Liangyu Dong",
      "Lin Hong",
      "Zitao Zhang",
      "Zhenshan Bing",
      "Kai Huang",
      "Fuchun Sun",
      "Alois Knoll"
    ],
    "published": "2025-02-21T17:35:10+00:00",
    "summary": "Current robotic pick-and-place policies typically require consistent gripper configurations across training and inference. This constraint imposes high retraining or fine-tuning costs, especially for imitation learning-based approaches, when adapting to new end-effectors. To mitigate this issue, we present a diffusion-based policy with a hybrid learning-optimization framework, enabling zero-shot adaptation to novel grippers without additional data collection for retraining policy. During training, the policy learns manipulation primitives from demonstrations collected using a base gripper. At inference, a diffusion-based optimization strategy dynamically enforces kinematic and safety constraints, ensuring that generated trajectories align with the physical properties of unseen grippers. This is achieved through a constrained denoising procedure that adapts trajectories to gripper-specific parameters (e.g., tool-center-point offsets, jaw widths) while preserving collision avoidance and task feasibility. We validate our method on a Franka Panda robot across six gripper configurations, including 3D-printed fingertips, flexible silicone gripper, and Robotiq 2F-85 gripper. Our approach achieves a 93.3% average task success rate across grippers (vs. 23.3-26.7% for diffusion policy baselines), supporting tool-center-point variations of 16-23.5 cm and jaw widths of 7.5-11.5 cm. The results demonstrate that constrained diffusion enables robust cross-gripper manipulation while maintaining the sample efficiency of imitation learning, eliminating the need for gripper-specific retraining. Video and code are available at https://github.com/yaoxt3/GADP."
  },
  {
    "title": "SafeInt: Shielding Large Language Models from Jailbreak Attacks via Safety-Aware Representation Intervention",
    "url": "http://arxiv.org/abs/2502.15594v1",
    "arxiv_id": "2502.15594v1",
    "authors": [
      "Jiaqi Wu",
      "Chen Chen",
      "Chunyan Hou",
      "Xiaojie Yuan"
    ],
    "published": "2025-02-21T17:12:35+00:00",
    "summary": "With the widespread real-world deployment of large language models (LLMs), ensuring their behavior complies with safety standards has become crucial. Jailbreak attacks exploit vulnerabilities in LLMs to induce undesirable behavior, posing a significant threat to LLM safety. Previous defenses often fail to achieve both effectiveness and efficiency simultaneously. Defenses from a representation perspective offer new insights, but existing interventions cannot dynamically adjust representations based on the harmfulness of the queries. To address this limitation while ensuring both effectiveness and efficiency, we propose SafeIntervention (SafeInt), a novel defense method that shields LLMs from jailbreak attacks through safety-aware representation intervention. SafeInt is built on our analysis of the representations of jailbreak samples. It adjusts representation distributions of jailbreak samples through intervention to align them with the representations of unsafe samples while minimizing unnecessary perturbations to jailbreak-irrelevant representations. We conduct comprehensive experiments covering six jailbreak attacks, two jailbreak datasets, and two utility benchmarks. Experimental results demonstrate that SafeInt outperforms all baselines in defending LLMs against jailbreak attacks while largely maintaining utility. Additionally, we evaluate SafeInt against adaptive attacks and verify its effectiveness in mitigating real-time attacks."
  },
  {
    "title": "Interpreting and Steering LLMs with Mutual Information-based Explanations on Sparse Autoencoders",
    "url": "http://arxiv.org/abs/2502.15576v1",
    "arxiv_id": "2502.15576v1",
    "authors": [
      "Xuansheng Wu",
      "Jiayi Yuan",
      "Wenlin Yao",
      "Xiaoming Zhai",
      "Ninghao Liu"
    ],
    "published": "2025-02-21T16:36:42+00:00",
    "summary": "Large language models (LLMs) excel at handling human queries, but they can occasionally generate flawed or unexpected responses. Understanding their internal states is crucial for understanding their successes, diagnosing their failures, and refining their capabilities. Although sparse autoencoders (SAEs) have shown promise for interpreting LLM internal representations, limited research has explored how to better explain SAE features, i.e., understanding the semantic meaning of features learned by SAE. Our theoretical analysis reveals that existing explanation methods suffer from the frequency bias issue, where they emphasize linguistic patterns over semantic concepts, while the latter is more critical to steer LLM behaviors. To address this, we propose using a fixed vocabulary set for feature interpretations and designing a mutual information-based objective, aiming to better capture the semantic meaning behind these features. We further propose two runtime steering strategies that adjust the learned feature activations based on their corresponding explanations. Empirical results show that, compared to baselines, our method provides more discourse-level explanations and effectively steers LLM behaviors to defend against jailbreak attacks. These findings highlight the value of explanations for steering LLM behaviors in downstream applications. We will release our code and data once accepted."
  },
  {
    "title": "Model Privacy: A Unified Framework to Understand Model Stealing Attacks and Defenses",
    "url": "http://arxiv.org/abs/2502.15567v1",
    "arxiv_id": "2502.15567v1",
    "authors": [
      "Ganghua Wang",
      "Yuhong Yang",
      "Jie Ding"
    ],
    "published": "2025-02-21T16:29:11+00:00",
    "summary": "The use of machine learning (ML) has become increasingly prevalent in various domains, highlighting the importance of understanding and ensuring its safety. One pressing concern is the vulnerability of ML applications to model stealing attacks. These attacks involve adversaries attempting to recover a learned model through limited query-response interactions, such as those found in cloud-based services or on-chip artificial intelligence interfaces. While existing literature proposes various attack and defense strategies, these often lack a theoretical foundation and standardized evaluation criteria. In response, this work presents a framework called ``Model Privacy'', providing a foundation for comprehensively analyzing model stealing attacks and defenses. We establish a rigorous formulation for the threat model and objectives, propose methods to quantify the goodness of attack and defense strategies, and analyze the fundamental tradeoffs between utility and privacy in ML models. Our developed theory offers valuable insights into enhancing the security of ML models, especially highlighting the importance of the attack-specific structure of perturbations for effective defenses. We demonstrate the application of model privacy from the defender's perspective through various learning scenarios. Extensive experiments corroborate the insights and the effectiveness of defense mechanisms developed under the proposed framework."
  },
  {
    "title": "Estimating Vehicle Speed on Roadways Using RNNs and Transformers: A Video-based Approach",
    "url": "http://arxiv.org/abs/2502.15545v1",
    "arxiv_id": "2502.15545v1",
    "authors": [
      "Sai Krishna Reddy Mareddy",
      "Dhanush Upplapati",
      "Dhanush Kumar Antharam"
    ],
    "published": "2025-02-21T15:51:49+00:00",
    "summary": "This project explores the application of advanced machine learning models, specifically Long Short-Term Memory (LSTM), Gated Recurrent Units (GRU), and Transformers, to the task of vehicle speed estimation using video data. Traditional methods of speed estimation, such as radar and manual systems, are often constrained by high costs, limited coverage, and potential disruptions. In contrast, leveraging existing surveillance infrastructure and cutting-edge neural network architectures presents a non-intrusive, scalable solution. Our approach utilizes LSTM and GRU to effectively manage long-term dependencies within the temporal sequence of video frames, while Transformers are employed to harness their self-attention mechanisms, enabling the processing of entire sequences in parallel and focusing on the most informative segments of the data. This study demonstrates that both LSTM and GRU outperform basic Recurrent Neural Networks (RNNs) due to their advanced gating mechanisms. Furthermore, increasing the sequence length of input data consistently improves model accuracy, highlighting the importance of contextual information in dynamic environments. Transformers, in particular, show exceptional adaptability and robustness across varied sequence lengths and complexities, making them highly suitable for real-time applications in diverse traffic conditions. The findings suggest that integrating these sophisticated neural network models can significantly enhance the accuracy and reliability of automated speed detection systems, thus promising to revolutionize traffic management and road safety."
  },
  {
    "title": "NPB-Rust: NAS Parallel Benchmarks in Rust",
    "url": "http://arxiv.org/abs/2502.15536v1",
    "arxiv_id": "2502.15536v1",
    "authors": [
      "Eduardo M. Martins",
      "Leonardo G. Fa\u00e9",
      "Renato B. Hoffmann",
      "Lucas S. Bianchessi",
      "Dalvan Griebler"
    ],
    "published": "2025-02-21T15:39:29+00:00",
    "summary": "Parallel programming often requires developers to handle complex computational tasks that can yield many errors in its development cycle. Rust is a performant low-level language that promises memory safety guarantees with its compiler, making it an attractive option for HPC application developers. We identified that the Rust ecosystem could benefit from more comprehensive scientific benchmark suites for standardizing comparisons and research. The NAS Parallel Benchmarks (NPB) is a standardized suite for evaluating various hardware aspects and is often used to compare different frameworks for parallelism. Therefore, our contributions are a Rust version of NPB, an analysis of the expressiveness and performance of the language features, and parallelization strategies. We compare our implementation with consolidated sequential and parallel versions of NPB. Experimental results show that Rust's sequential version is 1.23\\% slower than Fortran and 5.59\\% faster than C++, while Rust with Rayon was slower than both Fortran and C++ with OpenMP."
  },
  {
    "title": "Depth-aware Fusion Method based on Image and 4D Radar Spectrum for 3D Object Detection",
    "url": "http://arxiv.org/abs/2502.15516v1",
    "arxiv_id": "2502.15516v1",
    "authors": [
      "Yue Sun",
      "Yeqiang Qian",
      "Chunxiang Wang",
      "Ming Yang"
    ],
    "published": "2025-02-21T15:14:30+00:00",
    "summary": "Safety and reliability are crucial for the public acceptance of autonomous driving. To ensure accurate and reliable environmental perception, intelligent vehicles must exhibit accuracy and robustness in various environments. Millimeter-wave radar, known for its high penetration capability, can operate effectively in adverse weather conditions such as rain, snow, and fog. Traditional 3D millimeter-wave radars can only provide range, Doppler, and azimuth information for objects. Although the recent emergence of 4D millimeter-wave radars has added elevation resolution, the radar point clouds remain sparse due to Constant False Alarm Rate (CFAR) operations. In contrast, cameras offer rich semantic details but are sensitive to lighting and weather conditions. Hence, this paper leverages these two highly complementary and cost-effective sensors, 4D millimeter-wave radar and camera. By integrating 4D radar spectra with depth-aware camera images and employing attention mechanisms, we fuse texture-rich images with depth-rich radar data in the Bird's Eye View (BEV) perspective, enhancing 3D object detection. Additionally, we propose using GAN-based networks to generate depth images from radar spectra in the absence of depth sensors, further improving detection accuracy."
  },
  {
    "title": "A modular risk concept for complex systems",
    "url": "http://arxiv.org/abs/2502.15482v1",
    "arxiv_id": "2502.15482v1",
    "authors": [
      "Dag McGeorge",
      "Jon Arne Glomsrud"
    ],
    "published": "2025-02-21T14:11:16+00:00",
    "summary": "Our ways of managing risk have in the past been adapted to changes in technology and society. Amidst the ongoing digital transformation, the ur-gency of adapting risk management to changing needs seems higher than ever. This paper starts with a brief historic overview of the development of risk management in the past. The paper motivates the views that for com-plex systems, risk should be controlled by enforcing constrains in a modular way at different system levels, that the constraints can be expressed as assur-ance contracts and that acceptable risk mitigation can be demonstrated in as-surance case modules. Based on extensive industry experience of the authors, a major contribution is to explain how already existing methodologies have been combined to cre-ate a concept for modular risk assessment. Examples from assurance of au-tonomous sea navigation and autonomous driving are used to illustrate the concept. Beyond the existing methodologies this paper generalizes risk con-straints to assurance contracts as an enabler of modular risk assessment spanning all relevant system levels and stakeholder perspectives while main-taining the dependencies between the system parts and accounting for emer-gent system behavior. Furthermore, the use of safety integrity levels (SIL) and similar concepts for assigning assurance rigor have been avoided in favor of direct assessment of assurance case argument rigor, because technology and applications change too fast to justify using past experience as evidence of validity of such prescriptive schemes. This paper aims to help practitioners making efficient and timely risk-informed decisions about complex integrated systems."
  },
  {
    "title": "Single-pass Detection of Jailbreaking Input in Large Language Models",
    "url": "http://arxiv.org/abs/2502.15435v1",
    "arxiv_id": "2502.15435v1",
    "authors": [
      "Leyla Naz Candogan",
      "Yongtao Wu",
      "Elias Abad Rocamora",
      "Grigorios G. Chrysos",
      "Volkan Cevher"
    ],
    "published": "2025-02-21T13:04:13+00:00",
    "summary": "Defending aligned Large Language Models (LLMs) against jailbreaking attacks is a challenging problem, with existing approaches requiring multiple requests or even queries to auxiliary LLMs, making them computationally heavy. Instead, we focus on detecting jailbreaking input in a single forward pass. Our method, called Single Pass Detection SPD, leverages the information carried by the logits to predict whether the output sentence will be harmful. This allows us to defend in just one forward pass. SPD can not only detect attacks effectively on open-source models, but also minimizes the misclassification of harmless inputs. Furthermore, we show that SPD remains effective even without complete logit access in GPT-3.5 and GPT-4. We believe that our proposed method offers a promising approach to efficiently safeguard LLMs against adversarial attacks."
  },
  {
    "title": "Pub-Guard-LLM: Detecting Fraudulent Biomedical Articles with Reliable Explanations",
    "url": "http://arxiv.org/abs/2502.15429v1",
    "arxiv_id": "2502.15429v1",
    "authors": [
      "Lihu Chen",
      "Shuojie Fu",
      "Gabriel Freedman",
      "Cemre Zor",
      "Guy Martin",
      "James Kinross",
      "Uddhav Vaghela",
      "Ovidiu Serban",
      "Francesca Toni"
    ],
    "published": "2025-02-21T12:54:56+00:00",
    "summary": "A significant and growing number of published scientific articles is found to involve fraudulent practices, posing a serious threat to the credibility and safety of research in fields such as medicine. We propose Pub-Guard-LLM, the first large language model-based system tailored to fraud detection of biomedical scientific articles. We provide three application modes for deploying Pub-Guard-LLM: vanilla reasoning, retrieval-augmented generation, and multi-agent debate. Each mode allows for textual explanations of predictions. To assess the performance of our system, we introduce an open-source benchmark, PubMed Retraction, comprising over 11K real-world biomedical articles, including metadata and retraction labels. We show that, across all modes, Pub-Guard-LLM consistently surpasses the performance of various baselines and provides more reliable explanations, namely explanations which are deemed more relevant and coherent than those generated by the baselines when evaluated by multiple assessment methods. By enhancing both detection performance and explainability in scientific fraud detection, Pub-Guard-LLM contributes to safeguarding research integrity with a novel, effective, open-source tool."
  },
  {
    "title": "Adversarial Prompt Evaluation: Systematic Benchmarking of Guardrails Against Prompt Input Attacks on LLMs",
    "url": "http://arxiv.org/abs/2502.15427v1",
    "arxiv_id": "2502.15427v1",
    "authors": [
      "Giulio Zizzo",
      "Giandomenico Cornacchia",
      "Kieran Fraser",
      "Muhammad Zaid Hameed",
      "Ambrish Rawat",
      "Beat Buesser",
      "Mark Purcell",
      "Pin-Yu Chen",
      "Prasanna Sattigeri",
      "Kush Varshney"
    ],
    "published": "2025-02-21T12:54:25+00:00",
    "summary": "As large language models (LLMs) become integrated into everyday applications, ensuring their robustness and security is increasingly critical. In particular, LLMs can be manipulated into unsafe behaviour by prompts known as jailbreaks. The variety of jailbreak styles is growing, necessitating the use of external defences known as guardrails. While many jailbreak defences have been proposed, not all defences are able to handle new out-of-distribution attacks due to the narrow segment of jailbreaks used to align them. Moreover, the lack of systematisation around defences has created significant gaps in their practical application. In this work, we perform systematic benchmarking across 15 different defences, considering a broad swathe of malicious and benign datasets. We find that there is significant performance variation depending on the style of jailbreak a defence is subject to. Additionally, we show that based on current datasets available for evaluation, simple baselines can display competitive out-of-distribution performance compared to many state-of-the-art defences. Code is available at https://github.com/IBM/Adversarial-Prompt-Evaluation."
  },
  {
    "title": "A biomechanical comparison of concussion and head acceleration events in elite-level American football and rugby union",
    "url": "http://arxiv.org/abs/2502.15405v1",
    "arxiv_id": "2502.15405v1",
    "authors": [
      "Gregory Tierney"
    ],
    "published": "2025-02-21T12:11:10+00:00",
    "summary": "Elite-level American football and rugby union are two high-contact sports with growing clinical and legal concerns over player safety, necessitating a comparative analysis. A biomechanical comparison of concussion and head acceleration events (HAEs) in elite-level American football and rugby union was undertaken. Rugby union players have a greater number of professional playing years and matches available in a season than their American football counterparts. Rugby union players have a greater number of concussions reported per match and a higher proportion of concussions occurring during training sessions, based on National Football League (NFL) and Rugby Football Union (RFU) injury reports. Preliminary findings indicate that rugby union forwards experience a higher incidence of HAEs per player match over lower and higher magnitude thresholds, than American football defensive players. Overall, elite-level rugby union appears less favourable than American football in in almost all metrics pertinent to concussion and HAE exposure in the biomechanical comparison undertaken. The findings highlight the critical importance of independence, scientific rigour, and transparency in future concussion and HAE biomechanics research and real-world implementation, ensuring the development of more effective mitigation strategies."
  },
  {
    "title": "Evaluating Social Biases in LLM Reasoning",
    "url": "http://arxiv.org/abs/2502.15361v1",
    "arxiv_id": "2502.15361v1",
    "authors": [
      "Xuyang Wu",
      "Jinming Nian",
      "Zhiqiang Tao",
      "Yi Fang"
    ],
    "published": "2025-02-21T10:16:07+00:00",
    "summary": "In the recent development of AI reasoning, large language models (LLMs) are trained to automatically generate chain-of-thought reasoning steps, which have demonstrated compelling performance on math and coding tasks. However, when bias is mixed within the reasoning process to form strong logical arguments, it could cause even more harmful results and further induce hallucinations. In this paper, we have evaluated the 8B and 32B variants of DeepSeek-R1 against their instruction tuned counterparts on the BBQ dataset, and investigated the bias that is elicited out and being amplified through reasoning steps. To the best of our knowledge, this empirical study is the first to assess bias issues in LLM reasoning."
  },
  {
    "title": "Drug-Target Interaction/Affinity Prediction: Deep Learning Models and Advances Review",
    "url": "http://arxiv.org/abs/2502.15346v1",
    "arxiv_id": "2502.15346v1",
    "authors": [
      "Ali Vefghi",
      "Zahed Rahmati",
      "Mohammad Akbari"
    ],
    "published": "2025-02-21T10:00:43+00:00",
    "summary": "Drug discovery remains a slow and expensive process that involves many steps, from detecting the target structure to obtaining approval from the Food and Drug Administration (FDA), and is often riddled with safety concerns. Accurate prediction of how drugs interact with their targets and the development of new drugs by using better methods and technologies have immense potential to speed up this process, ultimately leading to faster delivery of life-saving medications. Traditional methods used for drug-target interaction prediction show limitations, particularly in capturing complex relationships between drugs and their targets. As an outcome, deep learning models have been presented to overcome the challenges of interaction prediction through their precise and efficient end results. By outlining promising research avenues and models, each with a different solution but similar to the problem, this paper aims to give researchers a better idea of methods for even more accurate and efficient prediction of drug-target interaction, ultimately accelerating the development of more effective drugs. A total of 180 prediction methods for drug-target interactions were analyzed throughout the period spanning 2016 to 2025 using different frameworks based on machine learning, mainly deep learning and graph neural networks. Additionally, this paper discusses the novelty, architecture, and input representation of these models."
  },
  {
    "title": "Attention Eclipse: Manipulating Attention to Bypass LLM Safety-Alignment",
    "url": "http://arxiv.org/abs/2502.15334v1",
    "arxiv_id": "2502.15334v1",
    "authors": [
      "Pedram Zaree",
      "Md Abdullah Al Mamun",
      "Quazi Mishkatul Alam",
      "Yue Dong",
      "Ihsen Alouani",
      "Nael Abu-Ghazaleh"
    ],
    "published": "2025-02-21T09:38:00+00:00",
    "summary": "Recent research has shown that carefully crafted jailbreak inputs can induce large language models to produce harmful outputs, despite safety measures such as alignment. It is important to anticipate the range of potential Jailbreak attacks to guide effective defenses and accurate assessment of model safety. In this paper, we present a new approach for generating highly effective Jailbreak attacks that manipulate the attention of the model to selectively strengthen or weaken attention among different parts of the prompt. By harnessing attention loss, we develop more effective jailbreak attacks, that are also transferrable. The attacks amplify the success rate of existing Jailbreak algorithms including GCG, AutoDAN, and ReNeLLM, while lowering their generation cost (for example, the amplified GCG attack achieves 91.2% ASR, vs. 67.9% for the original attack on Llama2-7B/AdvBench, using less than a third of the generation time)."
  },
  {
    "title": "Understand User Opinions of Large Language Models via LLM-Powered In-the-Moment User Experience Interviews",
    "url": "http://arxiv.org/abs/2502.15226v1",
    "arxiv_id": "2502.15226v1",
    "authors": [
      "Mengqiao Liu",
      "Tevin Wang",
      "Cassandra A. Cohen",
      "Sarah Li",
      "Chenyan Xiong"
    ],
    "published": "2025-02-21T05:42:22+00:00",
    "summary": "Which large language model (LLM) is better? Every evaluation tells a story, but what do users really think about current LLMs? This paper presents CLUE, an LLM-powered interviewer that conducts in-the-moment user experience interviews, right after users interacted with LLMs, and automatically gathers insights about user opinions from massive interview logs. We conduct a study with thousands of users to understand user opinions on mainstream LLMs, recruiting users to first chat with a target LLM and then interviewed by CLUE. Our experiments demonstrate that CLUE captures interesting user opinions, for example, the bipolar views on the displayed reasoning process of DeepSeek-R1 and demands for information freshness and multi-modality. Our collected chat-and-interview logs will be released."
  },
  {
    "title": "Discrete implementations of sliding-mode controllers with barrier-function adaptations require a revised framework",
    "url": "http://arxiv.org/abs/2502.15201v1",
    "arxiv_id": "2502.15201v1",
    "authors": [
      "Luis Ovalle",
      "Andr\u00e9s Gonz\u00e1lez",
      "Leonid Fridman",
      "Hernan Haimovich"
    ],
    "published": "2025-02-21T04:29:24+00:00",
    "summary": "Challenges in the discrete implementation of sliding-mode controllers (SMC) with barrier-function-based adaptations are analyzed, revealing fundamental limitations in conventional design frameworks. It is shown that under uniform sampling, the original continuous-time problem motivating these controllers becomes theoretically unsolvable under standard assumptions. To address this incompatibility, a revised control framework is proposed, explicitly incorporating actuator capacity constraints and sampled-data dynamics. Within this structure, the behavior of barrier function-based adaptive controllers (BFASMC) is rigorously examined, explaining their empirical success in digital implementations. A key theoretical result establishes an explicit relation between the actuator capacity, the sampling rate, and the width of the barrier function, providing a principled means to tune these controllers for different application requirements. This relation enables the resolution of various design problems with direct practical implications. A modified BFASMC is then introduced, systematically leveraging sampling effects to ensure finite-time convergence to a positively invariant predefined set, a key advancement for guaranteeing predictable safety margins."
  },
  {
    "title": "OccProphet: Pushing Efficiency Frontier of Camera-Only 4D Occupancy Forecasting with Observer-Forecaster-Refiner Framework",
    "url": "http://arxiv.org/abs/2502.15180v1",
    "arxiv_id": "2502.15180v1",
    "authors": [
      "Junliang Chen",
      "Huaiyuan Xu",
      "Yi Wang",
      "Lap-Pui Chau"
    ],
    "published": "2025-02-21T03:21:48+00:00",
    "summary": "Predicting variations in complex traffic environments is crucial for the safety of autonomous driving. Recent advancements in occupancy forecasting have enabled forecasting future 3D occupied status in driving environments by observing historical 2D images. However, high computational demands make occupancy forecasting less efficient during training and inference stages, hindering its feasibility for deployment on edge agents. In this paper, we propose a novel framework, i.e., OccProphet, to efficiently and effectively learn occupancy forecasting with significantly lower computational requirements while improving forecasting accuracy. OccProphet comprises three lightweight components: Observer, Forecaster, and Refiner. The Observer extracts spatio-temporal features from 3D multi-frame voxels using the proposed Efficient 4D Aggregation with Tripling-Attention Fusion, while the Forecaster and Refiner conditionally predict and refine future occupancy inferences. Experimental results on nuScenes, Lyft-Level5, and nuScenes-Occupancy datasets demonstrate that OccProphet is both training- and inference-friendly. OccProphet reduces 58\\%$\\sim$78\\% of the computational cost with a 2.6$\\times$ speedup compared with the state-of-the-art Cam4DOcc. Moreover, it achieves 4\\%$\\sim$18\\% relatively higher forecasting accuracy. Code and models are publicly available at https://github.com/JLChen-C/OccProphet."
  },
  {
    "title": "CurricuVLM: Towards Safe Autonomous Driving via Personalized Safety-Critical Curriculum Learning with Vision-Language Models",
    "url": "http://arxiv.org/abs/2502.15119v1",
    "arxiv_id": "2502.15119v1",
    "authors": [
      "Zihao Sheng",
      "Zilin Huang",
      "Yansong Qu",
      "Yue Leng",
      "Sruthi Bhavanam",
      "Sikai Chen"
    ],
    "published": "2025-02-21T00:42:40+00:00",
    "summary": "Ensuring safety in autonomous driving systems remains a critical challenge, particularly in handling rare but potentially catastrophic safety-critical scenarios. While existing research has explored generating safety-critical scenarios for autonomous vehicle (AV) testing, there is limited work on effectively incorporating these scenarios into policy learning to enhance safety. Furthermore, developing training curricula that adapt to an AV's evolving behavioral patterns and performance bottlenecks remains largely unexplored. To address these challenges, we propose CurricuVLM, a novel framework that leverages Vision-Language Models (VLMs) to enable personalized curriculum learning for autonomous driving agents. Our approach uniquely exploits VLMs' multimodal understanding capabilities to analyze agent behavior, identify performance weaknesses, and dynamically generate tailored training scenarios for curriculum adaptation. Through comprehensive analysis of unsafe driving situations with narrative descriptions, CurricuVLM performs in-depth reasoning to evaluate the AV's capabilities and identify critical behavioral patterns. The framework then synthesizes customized training scenarios targeting these identified limitations, enabling effective and personalized curriculum learning. Extensive experiments on the Waymo Open Motion Dataset show that CurricuVLM outperforms state-of-the-art baselines across both regular and safety-critical scenarios, achieving superior performance in terms of navigation success, driving efficiency, and safety metrics. Further analysis reveals that CurricuVLM serves as a general approach that can be integrated with various RL algorithms to enhance autonomous driving systems. The code and demo video are available at: https://zihaosheng.github.io/CurricuVLM/."
  },
  {
    "title": "Is Safety Standard Same for Everyone? User-Specific Safety Evaluation of Large Language Models",
    "url": "http://arxiv.org/abs/2502.15086v1",
    "arxiv_id": "2502.15086v1",
    "authors": [
      "Yeonjun In",
      "Wonjoong Kim",
      "Kanghoon Yoon",
      "Sungchul Kim",
      "Mehrab Tanjim",
      "Kibum Kim",
      "Chanyoung Park"
    ],
    "published": "2025-02-20T22:58:44+00:00",
    "summary": "As the use of large language model (LLM) agents continues to grow, their safety vulnerabilities have become increasingly evident. Extensive benchmarks evaluate various aspects of LLM safety by defining the safety relying heavily on general standards, overlooking user-specific standards. However, safety standards for LLM may vary based on a user-specific profiles rather than being universally consistent across all users. This raises a critical research question: Do LLM agents act safely when considering user-specific safety standards? Despite its importance for safe LLM use, no benchmark datasets currently exist to evaluate the user-specific safety of LLMs. To address this gap, we introduce U-SAFEBENCH, the first benchmark designed to assess user-specific aspect of LLM safety. Our evaluation of 18 widely used LLMs reveals current LLMs fail to act safely when considering user-specific safety standards, marking a new discovery in this field. To address this vulnerability, we propose a simple remedy based on chain-of-thought, demonstrating its effectiveness in improving user-specific safety. Our benchmark and code are available at https://github.com/yeonjun-in/U-SafeBench."
  },
  {
    "title": "InterFeedback: Unveiling Interactive Intelligence of Large Multimodal Models via Human Feedback",
    "url": "http://arxiv.org/abs/2502.15027v1",
    "arxiv_id": "2502.15027v1",
    "authors": [
      "Henry Hengyuan Zhao",
      "Wenqi Pei",
      "Yifei Tao",
      "Haiyang Mei",
      "Mike Zheng Shou"
    ],
    "published": "2025-02-20T20:27:06+00:00",
    "summary": "Existing benchmarks do not test Large Multimodal Models (LMMs) on their interactive intelligence with human users which is vital for developing general-purpose AI assistants. We design InterFeedback, an interactive framework, which can be applied to any LMM and dataset to assess this ability autonomously. On top of this, we introduce InterFeedback-Bench which evaluates interactive intelligence using two representative datasets, MMMU-Pro and MathVerse, to test 10 different open-source LMMs. Additionally, we present InterFeedback-Human, a newly collected dataset of 120 cases designed for manually testing interactive performance in leading models such as OpenAI-o1 and Claude-3.5-Sonnet. Our evaluation results show that even state-of-the-art LMM (like OpenAI-o1) can correct their results through human feedback less than 50%. Our findings point to the need for methods that can enhance the LMMs' capability to interpret and benefit from feedback."
  },
  {
    "title": "Auxiliary-Variable Adaptive Control Barrier Functions",
    "url": "http://arxiv.org/abs/2502.15026v1",
    "arxiv_id": "2502.15026v1",
    "authors": [
      "Shuo Liu",
      "Wei Xiao",
      "Calin A. Belta"
    ],
    "published": "2025-02-20T20:23:09+00:00",
    "summary": "This paper addresses the challenge of ensuring safety and feasibility in control systems using Control Barrier Functions (CBFs). Existing CBF-based Quadratic Programs (CBF-QPs) often encounter feasibility issues due to mixed relative degree constraints, input nullification problems, and the presence of tight or time-varying control bounds, which can lead to infeasible solutions and compromised safety. To address these challenges, we propose Auxiliary-Variable Adaptive Control Barrier Functions (AVCBFs), a novel framework that introduces auxiliary functions to dynamically adjust CBF constraints without the need of excessive additional constraints. The AVCBF method ensures that all components of the control input explicitly appear in the desired-order safety constraint, thereby improving feasibility while maintaining safety guarantees. Additionally, we introduce an automatic tuning method that iteratively adjusts AVCBF hyperparameters to ensure feasibility and safety with less conservatism. We demonstrate the effectiveness of the proposed approach in adaptive cruise control and obstacle avoidance scenarios, showing that AVCBFs outperform existing CBF methods by reducing infeasibility and enhancing adaptive safety control under tight or time-varying control bounds."
  },
  {
    "title": "Safe Beyond the Horizon: Efficient Sampling-based MPC with Neural Control Barrier Functions",
    "url": "http://arxiv.org/abs/2502.15006v1",
    "arxiv_id": "2502.15006v1",
    "authors": [
      "Ji Yin",
      "Oswin So",
      "Eric Yang Yu",
      "Chuchu Fan",
      "Panagiotis Tsiotras"
    ],
    "published": "2025-02-20T19:59:11+00:00",
    "summary": "A common problem when using model predictive control (MPC) in practice is the satisfaction of safety specifications beyond the prediction horizon. While theoretical works have shown that safety can be guaranteed by enforcing a suitable terminal set constraint or a sufficiently long prediction horizon, these techniques are difficult to apply and thus are rarely used by practitioners, especially in the case of general nonlinear dynamics. To solve this problem, we impose a tradeoff between exact recursive feasibility, computational tractability, and applicability to ''black-box'' dynamics by learning an approximate discrete-time control barrier function and incorporating it into a variational inference MPC (VIMPC), a sampling-based MPC paradigm. To handle the resulting state constraints, we further propose a new sampling strategy that greatly reduces the variance of the estimated optimal control, improving the sample efficiency, and enabling real-time planning on a CPU. The resulting Neural Shield-VIMPC (NS-VIMPC) controller yields substantial safety improvements compared to existing sampling-based MPC controllers, even under badly designed cost functions. We validate our approach in both simulation and real-world hardware experiments."
  },
  {
    "title": "Probabilistic Robustness in Deep Learning: A Concise yet Comprehensive Guide",
    "url": "http://arxiv.org/abs/2502.14833v1",
    "arxiv_id": "2502.14833v1",
    "authors": [
      "Xingyu Zhao"
    ],
    "published": "2025-02-20T18:47:17+00:00",
    "summary": "Deep learning (DL) has demonstrated significant potential across various safety-critical applications, yet ensuring its robustness remains a key challenge. While adversarial robustness has been extensively studied in worst-case scenarios, probabilistic robustness (PR) offers a more practical perspective by quantifying the likelihood of failures under stochastic perturbations. This paper provides a concise yet comprehensive overview of PR, covering its formal definitions, evaluation and enhancement methods. We introduce a reformulated ''min-max'' optimisation framework for adversarial training specifically designed to improve PR. Furthermore, we explore the integration of PR verification evidence into system-level safety assurance, addressing challenges in translating DL model-level robustness to system-level claims. Finally, we highlight open research questions, including benchmarking PR evaluation methods, extending PR to generative AI tasks, and developing rigorous methodologies and case studies for system-level integration."
  },
  {
    "title": "Logic-RL: Unleashing LLM Reasoning with Rule-Based Reinforcement Learning",
    "url": "http://arxiv.org/abs/2502.14768v1",
    "arxiv_id": "2502.14768v1",
    "authors": [
      "Tian Xie",
      "Zitian Gao",
      "Qingnan Ren",
      "Haoming Luo",
      "Yuqian Hong",
      "Bryan Dai",
      "Joey Zhou",
      "Kai Qiu",
      "Zhirong Wu",
      "Chong Luo"
    ],
    "published": "2025-02-20T17:49:26+00:00",
    "summary": "Inspired by the success of DeepSeek-R1, we explore the potential of rule-based reinforcement learning (RL) in large reasoning models. To analyze reasoning dynamics, we use synthetic logic puzzles as training data due to their controllable complexity and straightforward answer verification. We make some key technical contributions that lead to effective and stable RL training: a system prompt that emphasizes the thinking and answering process, a stringent format reward function that penalizes outputs for taking shortcuts, and a straightforward training recipe that achieves stable convergence. Our 7B model develops advanced reasoning skills-such as reflection, verification, and summarization-that are absent from the logic corpus. Remarkably, after training on just 5K logic problems, it demonstrates generalization abilities to the challenging math benchmarks AIME and AMC."
  },
  {
    "title": "HiddenDetect: Detecting Jailbreak Attacks against Large Vision-Language Models via Monitoring Hidden States",
    "url": "http://arxiv.org/abs/2502.14744v1",
    "arxiv_id": "2502.14744v1",
    "authors": [
      "Yilei Jiang",
      "Xinyan Gao",
      "Tianshuo Peng",
      "Yingshui Tan",
      "Xiaoyong Zhu",
      "Bo Zheng",
      "Xiangyu Yue"
    ],
    "published": "2025-02-20T17:14:34+00:00",
    "summary": "The integration of additional modalities increases the susceptibility of large vision-language models (LVLMs) to safety risks, such as jailbreak attacks, compared to their language-only counterparts. While existing research primarily focuses on post-hoc alignment techniques, the underlying safety mechanisms within LVLMs remain largely unexplored. In this work , we investigate whether LVLMs inherently encode safety-relevant signals within their internal activations during inference. Our findings reveal that LVLMs exhibit distinct activation patterns when processing unsafe prompts, which can be leveraged to detect and mitigate adversarial inputs without requiring extensive fine-tuning. Building on this insight, we introduce HiddenDetect, a novel tuning-free framework that harnesses internal model activations to enhance safety. Experimental results show that {HiddenDetect} surpasses state-of-the-art methods in detecting jailbreak attacks against LVLMs. By utilizing intrinsic safety-aware patterns, our method provides an efficient and scalable solution for strengthening LVLM robustness against multimodal threats. Our code will be released publicly at https://github.com/leigest519/HiddenDetect."
  },
  {
    "title": "HiddenDetect: Detecting Jailbreak Attacks against Large Vision-Language Models via Monitoring Hidden States",
    "url": "http://arxiv.org/abs/2502.14744v2",
    "arxiv_id": "2502.14744v2",
    "authors": [
      "Yilei Jiang",
      "Xinyan Gao",
      "Tianshuo Peng",
      "Yingshui Tan",
      "Xiaoyong Zhu",
      "Bo Zheng",
      "Xiangyu Yue"
    ],
    "published": "2025-02-20T17:14:34+00:00",
    "summary": "The integration of additional modalities increases the susceptibility of large vision-language models (LVLMs) to safety risks, such as jailbreak attacks, compared to their language-only counterparts. While existing research primarily focuses on post-hoc alignment techniques, the underlying safety mechanisms within LVLMs remain largely unexplored. In this work , we investigate whether LVLMs inherently encode safety-relevant signals within their internal activations during inference. Our findings reveal that LVLMs exhibit distinct activation patterns when processing unsafe prompts, which can be leveraged to detect and mitigate adversarial inputs without requiring extensive fine-tuning. Building on this insight, we introduce HiddenDetect, a novel tuning-free framework that harnesses internal model activations to enhance safety. Experimental results show that {HiddenDetect} surpasses state-of-the-art methods in detecting jailbreak attacks against LVLMs. By utilizing intrinsic safety-aware patterns, our method provides an efficient and scalable solution for strengthening LVLM robustness against multimodal threats. Our code will be released publicly at https://github.com/leigest519/HiddenDetect."
  },
  {
    "title": "SuperGPQA: Scaling LLM Evaluation across 285 Graduate Disciplines",
    "url": "http://arxiv.org/abs/2502.14739v1",
    "arxiv_id": "2502.14739v1",
    "authors": [
      "M-A-P Team",
      "Xinrun Du",
      "Yifan Yao",
      "Kaijing Ma",
      "Bingli Wang",
      "Tianyu Zheng",
      "Kang Zhu",
      "Minghao Liu",
      "Yiming Liang",
      "Xiaolong Jin",
      "Zhenlin Wei",
      "Chujie Zheng",
      "Kaixing Deng",
      "Shuyue Guo",
      "Shian Jia",
      "Sichao Jiang",
      "Yiyan Liao",
      "Rui Li",
      "Qinrui Li",
      "Sirun Li",
      "Yizhi Li",
      "Yunwen Li",
      "Dehua Ma",
      "Yuansheng Ni",
      "Haoran Que",
      "Qiyao Wang",
      "Zhoufutu Wen",
      "Siwei Wu",
      "Tianshun Xing",
      "Ming Xu",
      "Zhenzhu Yang",
      "Zekun Moore Wang",
      "Junting Zhou",
      "Yuelin Bai",
      "Xingyuan Bu",
      "Chenglin Cai",
      "Liang Chen",
      "Yifan Chen",
      "Chengtuo Cheng",
      "Tianhao Cheng",
      "Keyi Ding",
      "Siming Huang",
      "Yun Huang",
      "Yaoru Li",
      "Yizhe Li",
      "Zhaoqun Li",
      "Tianhao Liang",
      "Chengdong Lin",
      "Hongquan Lin",
      "Yinghao Ma",
      "Zhongyuan Peng",
      "Zifan Peng",
      "Qige Qi",
      "Shi Qiu",
      "Xingwei Qu",
      "Yizhou Tan",
      "Zili Wang",
      "Chenqing Wang",
      "Hao Wang",
      "Yiya Wang",
      "Yubo Wang",
      "Jiajun Xu",
      "Kexin Yang",
      "Ruibin Yuan",
      "Yuanhao Yue",
      "Tianyang Zhan",
      "Chun Zhang",
      "Jingyang Zhang",
      "Xiyue Zhang",
      "Xingjian Zhang",
      "Yue Zhang",
      "Yongchi Zhao",
      "Xiangyu Zheng",
      "Chenghua Zhong",
      "Yang Gao",
      "Zhoujun Li",
      "Dayiheng Liu",
      "Qian Liu",
      "Tianyu Liu",
      "Shiwen Ni",
      "Junran Peng",
      "Yujia Qin",
      "Wenbo Su",
      "Guoyin Wang",
      "Shi Wang",
      "Jian Yang",
      "Min Yang",
      "Meng Cao",
      "Xiang Yue",
      "Zhaoxiang Zhang",
      "Wangchunshu Zhou",
      "Jiaheng Liu",
      "Qunshu Lin",
      "Wenhao Huang",
      "Ge Zhang"
    ],
    "published": "2025-02-20T17:05:58+00:00",
    "summary": "Large language models (LLMs) have demonstrated remarkable proficiency in mainstream academic disciplines such as mathematics, physics, and computer science. However, human knowledge encompasses over 200 specialized disciplines, far exceeding the scope of existing benchmarks. The capabilities of LLMs in many of these specialized fields-particularly in light industry, agriculture, and service-oriented disciplines-remain inadequately evaluated. To address this gap, we present SuperGPQA, a comprehensive benchmark that evaluates graduate-level knowledge and reasoning capabilities across 285 disciplines. Our benchmark employs a novel Human-LLM collaborative filtering mechanism to eliminate trivial or ambiguous questions through iterative refinement based on both LLM responses and expert feedback. Our experimental results reveal significant room for improvement in the performance of current state-of-the-art LLMs across diverse knowledge domains (e.g., the reasoning-focused model DeepSeek-R1 achieved the highest accuracy of 61.82% on SuperGPQA), highlighting the considerable gap between current model capabilities and artificial general intelligence. Additionally, we present comprehensive insights from our management of a large-scale annotation process, involving over 80 expert annotators and an interactive Human-LLM collaborative system, offering valuable methodological guidance for future research initiatives of comparable scope."
  },
  {
    "title": "PEARL: Towards Permutation-Resilient LLMs",
    "url": "http://arxiv.org/abs/2502.14628v1",
    "arxiv_id": "2502.14628v1",
    "authors": [
      "Liang Chen",
      "Li Shen",
      "Yang Deng",
      "Xiaoyan Zhao",
      "Bin Liang",
      "Kam-Fai Wong"
    ],
    "published": "2025-02-20T15:07:02+00:00",
    "summary": "The in-context learning (ICL) capability of large language models (LLMs) enables them to perform challenging tasks using provided demonstrations. However, ICL is highly sensitive to the ordering of demonstrations, leading to instability in predictions. This paper shows that this vulnerability can be exploited to design a natural attack - difficult for model providers to detect - that achieves nearly 80% success rate on LLaMA-3 by simply permuting the demonstrations. Existing mitigation methods primarily rely on post-processing and fail to enhance the model's inherent robustness to input permutations, raising concerns about safety and reliability of LLMs. To address this issue, we propose Permutation-resilient learning (PEARL), a novel framework based on distributionally robust optimization (DRO), which optimizes model performance against the worst-case input permutation. Specifically, PEARL consists of a permutation-proposal network (P-Net) and the LLM. The P-Net generates the most challenging permutations by treating it as an optimal transport problem, which is solved using an entropy-constrained Sinkhorn algorithm. Through minimax optimization, the P-Net and the LLM iteratively optimize against each other, progressively improving the LLM's robustness. Experiments on synthetic pre-training and real-world instruction tuning tasks demonstrate that PEARL effectively mitigates permutation attacks and enhances performance. Notably, despite being trained on fewer shots and shorter contexts, PEARL achieves performance gains of up to 40% when scaled to many-shot and long-context scenarios, highlighting its efficiency and generalization capabilities."
  },
  {
    "title": "A Stackelberg Game Approach for Signal Temporal Logic Control Synthesis with Uncontrollable Agents",
    "url": "http://arxiv.org/abs/2502.14585v1",
    "arxiv_id": "2502.14585v1",
    "authors": [
      "Bohan Cui",
      "Xinyi Yu",
      "Alessandro Giua",
      "Xiang Yin"
    ],
    "published": "2025-02-20T14:14:42+00:00",
    "summary": "In this paper, we investigate the control synthesis problem for Signal Temporal Logic (STL) specifications in the presence of uncontrollable agents. Existing works mainly address this problem in a robust control setting by assuming the uncontrollable agents are adversarial and accounting for the worst-case scenario. While this approach ensures safety, it can be overly conservative in scenarios where uncontrollable agents have their own objectives that are not entirely opposed to the system's goals. Motivated by this limitation, we propose a new framework for STL control synthesis within the Stackelberg game setting. Specifically, we assume that the system controller, acting as the leader, first commits to a plan, after which the uncontrollable agents, acting as followers, take a best response based on the committed plan and their own objectives. Our goal is to synthesize a control sequence for the leader such that, for any rational followers producing a best response, the leader's STL task is guaranteed to be satisfied. We present an effective solution to this problem by transforming it into a single-stage optimization problem and leveraging counter-example guided synthesis techniques. We demonstrate that the proposed approach is sound and identify conditions under which it is optimal. Simulation results are also provided to illustrate the effectiveness of the proposed framework."
  },
  {
    "title": "Real-world Troublemaker: A Novel Track Testing Framework for Automated Driving Systems in Safety-critical Interaction Scenarios",
    "url": "http://arxiv.org/abs/2502.14574v1",
    "arxiv_id": "2502.14574v1",
    "authors": [
      "Xinrui Zhang",
      "Lu Xiong",
      "Peizhi Zhang",
      "Junpeng Huang",
      "Yining Ma"
    ],
    "published": "2025-02-20T13:59:57+00:00",
    "summary": "Track testing plays a critical role in the safety evaluation of autonomous driving systems (ADS), as it provides real-world object targets and a safety-controllable interaction environment. However, existing track testing scenarios are often pre-fixed and limited, primarily due to the inflexibility of object target control methods and the lack of intelligent interactive behaviors. To overcome this limitation, we propose a novel track testing framework, Real-world Troublemaker, which can generate adversarial object target motion trajectories and facilitate intelligent interactions with the vehicle under test (VUT), creating a more realistic and dynamic testing environment. To enable flexible motion trajectories, cloud-controlled technology is utilized to remotely and dynamically control object targets to create a realistic traffic environment. To achieve intelligent interactions, an interactive concrete scenario generation method is introduced within a game-theoretic structure. The proposed framework has been successfully implemented at the Tongji University Intelligent Connected Vehicle Evaluation Base. Field test results demonstrate that Troublemaker can perform dynamic interactive testing of ADS accurately and effectively. Compared to traditional track testing methods, Troublemaker improves scenario reproduction accuracy by 65.2\\%, increases the diversity of target vehicle interaction strategies by approximately 9.2 times, and enhances exposure frequency of safety-critical scenarios by 3.5 times in unprotected left-turn scenarios."
  },
  {
    "title": "Real-world Troublemaker: A 5G Cloud-controlled Track Testing Framework for Automated Driving Systems in Safety-critical Interaction Scenarios",
    "url": "http://arxiv.org/abs/2502.14574v2",
    "arxiv_id": "2502.14574v2",
    "authors": [
      "Xinrui Zhang",
      "Lu Xiong",
      "Peizhi Zhang",
      "Junpeng Huang",
      "Yining Ma"
    ],
    "published": "2025-02-20T13:59:57+00:00",
    "summary": "Track testing plays a critical role in the safety evaluation of autonomous driving systems (ADS), as it provides a real-world interaction environment. However, the inflexibility in motion control of object targets and the absence of intelligent interactive testing methods often result in pre-fixed and limited testing scenarios. To address these limitations, we propose a novel 5G cloud-controlled track testing framework, Real-world Troublemaker. This framework overcomes the rigidity of traditional pre-programmed control by leveraging 5G cloud-controlled object targets integrated with the Internet of Things (IoT) and vehicle teleoperation technologies. Unlike conventional testing methods that rely on pre-set conditions, we propose a dynamic game strategy based on a quadratic risk interaction utility function, facilitating intelligent interactions with the vehicle under test (VUT) and creating a more realistic and dynamic interaction environment. The proposed framework has been successfully implemented at the Tongji University Intelligent Connected Vehicle Evaluation Base. Field test results demonstrate that Troublemaker can perform dynamic interactive testing of ADS accurately and effectively. Compared to traditional methods, Troublemaker improves scenario reproduction accuracy by 65.2\\%, increases the diversity of interaction strategies by approximately 9.2 times, and enhances exposure frequency of safety-critical scenarios by 3.5 times in unprotected left-turn scenarios."
  },
  {
    "title": "CORBA: Contagious Recursive Blocking Attacks on Multi-Agent Systems Based on Large Language Models",
    "url": "http://arxiv.org/abs/2502.14529v1",
    "arxiv_id": "2502.14529v1",
    "authors": [
      "Zhenhong Zhou",
      "Zherui Li",
      "Jie Zhang",
      "Yuanhe Zhang",
      "Kun Wang",
      "Yang Liu",
      "Qing Guo"
    ],
    "published": "2025-02-20T13:02:00+00:00",
    "summary": "Large Language Model-based Multi-Agent Systems (LLM-MASs) have demonstrated remarkable real-world capabilities, effectively collaborating to complete complex tasks. While these systems are designed with safety mechanisms, such as rejecting harmful instructions through alignment, their security remains largely unexplored. This gap leaves LLM-MASs vulnerable to targeted disruptions. In this paper, we introduce Contagious Recursive Blocking Attacks (Corba), a novel and simple yet highly effective attack that disrupts interactions between agents within an LLM-MAS. Corba leverages two key properties: its contagious nature allows it to propagate across arbitrary network topologies, while its recursive property enables sustained depletion of computational resources. Notably, these blocking attacks often involve seemingly benign instructions, making them particularly challenging to mitigate using conventional alignment methods. We evaluate Corba on two widely-used LLM-MASs, namely, AutoGen and Camel across various topologies and commercial models. Additionally, we conduct more extensive experiments in open-ended interactive LLM-MASs, demonstrating the effectiveness of Corba in complex topology structures and open-source models. Our code is available at: https://github.com/zhrli324/Corba."
  },
  {
    "title": "MLGym: A New Framework and Benchmark for Advancing AI Research Agents",
    "url": "http://arxiv.org/abs/2502.14499v1",
    "arxiv_id": "2502.14499v1",
    "authors": [
      "Deepak Nathani",
      "Lovish Madaan",
      "Nicholas Roberts",
      "Nikolay Bashlykov",
      "Ajay Menon",
      "Vincent Moens",
      "Amar Budhiraja",
      "Despoina Magka",
      "Vladislav Vorotilov",
      "Gaurav Chaurasia",
      "Dieuwke Hupkes",
      "Ricardo Silveira Cabral",
      "Tatiana Shavrina",
      "Jakob Foerster",
      "Yoram Bachrach",
      "William Yang Wang",
      "Roberta Raileanu"
    ],
    "published": "2025-02-20T12:28:23+00:00",
    "summary": "We introduce Meta MLGym and MLGym-Bench, a new framework and benchmark for evaluating and developing LLM agents on AI research tasks. This is the first Gym environment for machine learning (ML) tasks, enabling research on reinforcement learning (RL) algorithms for training such agents. MLGym-bench consists of 13 diverse and open-ended AI research tasks from diverse domains such as computer vision, natural language processing, reinforcement learning, and game theory. Solving these tasks requires real-world AI research skills such as generating new ideas and hypotheses, creating and processing data, implementing ML methods, training models, running experiments, analyzing the results, and iterating through this process to improve on a given task. We evaluate a number of frontier large language models (LLMs) on our benchmarks such as Claude-3.5-Sonnet, Llama-3.1 405B, GPT-4o, o1-preview, and Gemini-1.5 Pro. Our MLGym framework makes it easy to add new tasks, integrate and evaluate models or agents, generate synthetic data at scale, as well as develop new learning algorithms for training agents on AI research tasks. We find that current frontier models can improve on the given baselines, usually by finding better hyperparameters, but do not generate novel hypotheses, algorithms, architectures, or substantial improvements. We open-source our framework and benchmark to facilitate future research in advancing the AI research capabilities of LLM agents."
  },
  {
    "title": "Statistical Scenario Modelling and Lookalike Distributions for Multi-Variate AI Risk",
    "url": "http://arxiv.org/abs/2502.14491v1",
    "arxiv_id": "2502.14491v1",
    "authors": [
      "Elija Perrier"
    ],
    "published": "2025-02-20T12:14:54+00:00",
    "summary": "Evaluating AI safety requires statistically rigorous methods and risk metrics for understanding how the use of AI affects aggregated risk. However, much AI safety literature focuses upon risks arising from AI models in isolation, lacking consideration of how modular use of AI affects risk distribution of workflow components or overall risk metrics. There is also a lack of statistical grounding enabling sensitisation of risk models in the presence of absence of AI to estimate causal contributions of AI. This is in part due to the dearth of AI impact data upon which to fit distributions. In this work, we address these gaps in two ways. First, we demonstrate how scenario modelling (grounded in established statistical techniques such as Markov chains, copulas and Monte Carlo simulation) can be used to model AI risk holistically. Second, we show how lookalike distributions from phenomena analogous to AI can be used to estimate AI impacts in the absence of directly observable data. We demonstrate the utility of our methods for benchmarking cumulative AI risk via risk analysis of a logistic scenario simulations."
  },
  {
    "title": "How Jailbreak Defenses Work and Ensemble? A Mechanistic Investigation",
    "url": "http://arxiv.org/abs/2502.14486v1",
    "arxiv_id": "2502.14486v1",
    "authors": [
      "Zhuohang Long",
      "Siyuan Wang",
      "Shujun Liu",
      "Yuhang Lai",
      "Xuanjing Huang",
      "Zhongyu Wei"
    ],
    "published": "2025-02-20T12:07:40+00:00",
    "summary": "Jailbreak attacks, where harmful prompts bypass generative models' built-in safety, raise serious concerns about model vulnerability. While many defense methods have been proposed, the trade-offs between safety and helpfulness, and their application to Large Vision-Language Models (LVLMs), are not well understood. This paper systematically examines jailbreak defenses by reframing the standard generation task as a binary classification problem to assess model refusal tendencies for both harmful and benign queries. We identify two key defense mechanisms: safety shift, which increases refusal rates across all queries, and harmfulness discrimination, which improves the model's ability to distinguish between harmful and benign inputs. Using these mechanisms, we develop two ensemble defense strategies-inter-mechanism ensembles and intra-mechanism ensembles-to balance safety and helpfulness. Experiments on the MM-SafetyBench and MOSSBench datasets with LLaVA-1.5 models show that these strategies effectively improve model safety or optimize the trade-off between safety and helpfulness."
  },
  {
    "title": "Natural Language Generation",
    "url": "http://arxiv.org/abs/2502.14437v1",
    "arxiv_id": "2502.14437v1",
    "authors": [
      "Ehud Reiter"
    ],
    "published": "2025-02-20T10:41:34+00:00",
    "summary": "This book provides a broad overview of Natural Language Generation (NLG), including technology, user requirements, evaluation, and real-world applications. The focus is on concepts and insights which hopefully will remain relevant for many years, not on the latest LLM innovations. It draws on decades of work by the author and others on NLG.   The book has the following chapters: Introduction to NLG; Rule-Based NLG; Machine Learning and Neural NLG; Requirements; Evaluation; Safety, Maintenance, and Testing; and Applications. All chapters include examples and anecdotes from the author's personal experiences, and end with a Further Reading section.   The book should be especially useful to people working on applied NLG, including NLG researchers, people in other fields who want to use NLG, and commercial developers. It will not however be useful to people who want to understand the latest LLM technology.   There is a companion site with more information at https://ehudreiter.com/book/"
  },
  {
    "title": "Reliable Explainability of Deep Learning Spatial-Spectral Classifiers for Improved Semantic Segmentation in Autonomous Driving",
    "url": "http://arxiv.org/abs/2502.14416v1",
    "arxiv_id": "2502.14416v1",
    "authors": [
      "Jon Guti\u00e9rrez-Zaballa",
      "Koldo Basterretxea",
      "Javier Echanobe"
    ],
    "published": "2025-02-20T10:11:27+00:00",
    "summary": "Integrating hyperspectral imagery (HSI) with deep neural networks (DNNs) can strengthen the accuracy of intelligent vision systems by combining spectral and spatial information, which is useful for tasks like semantic segmentation in autonomous driving. To advance research in such safety-critical systems, determining the precise contribution of spectral information to complex DNNs' output is needed. To address this, several saliency methods, such as class activation maps (CAM), have been proposed primarily for image classification. However, recent studies have raised concerns regarding their reliability. In this paper, we address their limitations and propose an alternative approach by leveraging the data provided by activations and weights from relevant DNN layers to better capture the relationship between input features and predictions. The study aims to assess the superior performance of HSI compared to 3-channel and single-channel DNNs. We also address the influence of spectral signature normalization for enhancing DNN robustness in real-world driving conditions."
  },
  {
    "title": "HPS: Hard Preference Sampling for Human Preference Alignment",
    "url": "http://arxiv.org/abs/2502.14400v1",
    "arxiv_id": "2502.14400v1",
    "authors": [
      "Xiandong Zou",
      "Wanyu Lin",
      "Yuchen Li",
      "Pan Zhou"
    ],
    "published": "2025-02-20T09:37:41+00:00",
    "summary": "Aligning Large Language Model (LLM) responses with human preferences is vital for building safe and controllable AI systems. While preference optimization methods based on Plackett-Luce (PL) and Bradley-Terry (BT) models have shown promise, they face challenges such as poor handling of harmful content, inefficient use of dispreferred responses, and, specifically for PL, high computational costs. To address these issues, we propose Hard Preference Sampling (HPS), a novel framework for robust and efficient human preference alignment. HPS introduces a training loss that prioritizes the most preferred response while rejecting all dispreferred and harmful ones. It emphasizes \"hard\" dispreferred responses--those closely resembling preferred ones--to enhance the model's rejection capabilities. By leveraging a single-sample Monte Carlo sampling strategy, HPS reduces computational overhead while maintaining alignment quality. Theoretically, HPS improves sample efficiency over existing PL methods and maximizes the reward margin between preferred and dispreferred responses, ensuring clearer distinctions. Experiments on HH-RLHF and PKU-Safety datasets validate HPS's effectiveness, achieving comparable BLEU and reward scores while greatly improving reward margins and thus reducing harmful content generation."
  },
  {
    "title": "MPPI-DBaS: Safe Trajectory Optimization with Adaptive Exploration",
    "url": "http://arxiv.org/abs/2502.14387v1",
    "arxiv_id": "2502.14387v1",
    "authors": [
      "Fanxin Wang",
      "Yikun Cheng",
      "Chuyuan Tao"
    ],
    "published": "2025-02-20T09:22:41+00:00",
    "summary": "In trajectory optimization, Model Predictive Path Integral (MPPI) control is a sampling-based Model Predictive Control (MPC) framework that generates optimal inputs by efficiently simulating numerous trajectories. In practice, however, MPPI often struggles to guarantee safety assurance and balance efficient sampling in open spaces with the need for more extensive exploration under tight constraints. To address this challenge, we incorporate discrete barrier states (DBaS) into MPPI and propose a novel MPPI-DBaS algorithm that ensures system safety and enables adaptive exploration across diverse scenarios. We evaluate our method in simulation experiments where the vehicle navigates through closely placed obstacles. The results demonstrate that the proposed algorithm significantly outperforms standard MPPI, achieving a higher success rate and lower tracking errors."
  },
  {
    "title": "S*: Test Time Scaling for Code Generation",
    "url": "http://arxiv.org/abs/2502.14382v1",
    "arxiv_id": "2502.14382v1",
    "authors": [
      "Dacheng Li",
      "Shiyi Cao",
      "Chengkun Cao",
      "Xiuyu Li",
      "Shangyin Tan",
      "Kurt Keutzer",
      "Jiarong Xing",
      "Joseph E. Gonzalez",
      "Ion Stoica"
    ],
    "published": "2025-02-20T09:18:53+00:00",
    "summary": "Increasing test-time compute for LLMs shows promise across domains but remains underexplored in code generation, despite extensive study in math. In this paper, we propose S*, the first hybrid test-time scaling framework that substantially improves the coverage and selection accuracy of generated code. S* extends the existing parallel scaling paradigm with sequential scaling to push performance boundaries. It further leverages a novel selection mechanism that adaptively generates distinguishing inputs for pairwise comparison, combined with execution-grounded information to robustly identify correct solutions. We evaluate across 12 Large Language Models and Large Reasoning Model and show: (1) S* consistently improves performance across model families and sizes, enabling a 3B model to outperform GPT-4o-mini; (2) S* enables non-reasoning models to surpass reasoning models - GPT-4o-mini with S* outperforms o1-preview by 3.7% on LiveCodeBench; (3) S* further boosts state-of-the-art reasoning models - DeepSeek-R1-Distill-Qwen-32B with S* achieves 85.7% on LiveCodeBench, approaching o1 (high) at 88.5%. Code will be available under https://github.com/NovaSky-AI/SkyThought."
  },
  {
    "title": "ChemHTS: Hierarchical Tool Stacking for Enhancing Chemical Agents",
    "url": "http://arxiv.org/abs/2502.14327v1",
    "arxiv_id": "2502.14327v1",
    "authors": [
      "Zhucong Li",
      "Jin Xiao",
      "Bowei Zhang",
      "Zhijian Zhou",
      "Qianyu He",
      "Fenglei Cao",
      "Jiaqing Liang",
      "Yuan Qi"
    ],
    "published": "2025-02-20T07:24:26+00:00",
    "summary": "Large Language Models (LLMs) have demonstrated remarkable potential in scientific research, particularly in chemistry-related tasks such as molecular design, reaction prediction, and property estimation. While tool-augmented LLMs have been introduced to enhance reasoning and computation in these domains, existing approaches suffer from tool invocation errors and lack effective collaboration among diverse tools, limiting their overall performance. To address these challenges, we propose ChemHTS (Chemical Hierarchical Tool Stacking), a novel method that optimizes tool invocation pathways through a hierarchical stacking strategy. ChemHTS consists of two key stages: tool self-stacking warmup and multi-layer decision optimization, enabling LLMs to refine tool usage dynamically. We evaluate ChemHTS across four classical chemistry tasks and demonstrate its superiority over strong baselines, including GPT-4o, DeepSeek-R1, and chemistry-specific models, including ChemDFM. Furthermore, we define four distinct tool-stacking behaviors to enhance interpretability, providing insights into the effectiveness of tool collaboration. Our dataset and code are publicly available at \\url{https://github.com/Chang-pw/ChemHTS}."
  },
  {
    "title": "MedHallu: A Comprehensive Benchmark for Detecting Medical Hallucinations in Large Language Models",
    "url": "http://arxiv.org/abs/2502.14302v1",
    "arxiv_id": "2502.14302v1",
    "authors": [
      "Shrey Pandit",
      "Jiawei Xu",
      "Junyuan Hong",
      "Zhangyang Wang",
      "Tianlong Chen",
      "Kaidi Xu",
      "Ying Ding"
    ],
    "published": "2025-02-20T06:33:23+00:00",
    "summary": "Advancements in Large Language Models (LLMs) and their increasing use in medical question-answering necessitate rigorous evaluation of their reliability. A critical challenge lies in hallucination, where models generate plausible yet factually incorrect outputs. In the medical domain, this poses serious risks to patient safety and clinical decision-making. To address this, we introduce MedHallu, the first benchmark specifically designed for medical hallucination detection. MedHallu comprises 10,000 high-quality question-answer pairs derived from PubMedQA, with hallucinated answers systematically generated through a controlled pipeline. Our experiments show that state-of-the-art LLMs, including GPT-4o, Llama-3.1, and the medically fine-tuned UltraMedical, struggle with this binary hallucination detection task, with the best model achieving an F1 score as low as 0.625 for detecting \"hard\" category hallucinations. Using bidirectional entailment clustering, we show that harder-to-detect hallucinations are semantically closer to ground truth. Through experiments, we also show incorporating domain-specific knowledge and introducing a \"not sure\" category as one of the answer categories improves the precision and F1 scores by up to 38% relative to baselines."
  },
  {
    "title": "SEA-HELM: Southeast Asian Holistic Evaluation of Language Models",
    "url": "http://arxiv.org/abs/2502.14301v1",
    "arxiv_id": "2502.14301v1",
    "authors": [
      "Yosephine Susanto",
      "Adithya Venkatadri Hulagadri",
      "Jann Railey Montalan",
      "Jian Gang Ngui",
      "Xian Bin Yong",
      "Weiqi Leong",
      "Hamsawardhini Rengarajan",
      "Peerat Limkonchotiwat",
      "Yifan Mai",
      "William Chandra Tjhi"
    ],
    "published": "2025-02-20T06:32:45+00:00",
    "summary": "With the rapid emergence of novel capabilities in Large Language Models (LLMs), the need for rigorous multilingual and multicultural benchmarks that are integrated has become more pronounced. Though existing LLM benchmarks are capable of evaluating specific capabilities of LLMs in English as well as in various mid- to low-resource languages, including those in the Southeast Asian (SEA) region, a comprehensive and authentic evaluation suite for the SEA languages has not been developed thus far. Here, we present SEA-HELM, a holistic linguistic and cultural LLM evaluation suite that emphasizes SEA languages, comprising five core pillars: (1) NLP Classics, (2) LLM-specifics, (3) SEA Linguistics, (4) SEA Culture, (5) Safety. SEA-HELM currently supports Filipino, Indonesian, Tamil, Thai, and Vietnamese. We also introduce the SEA-HELM leaderboard, which allows users to understand models' multilingual and multicultural performance in a systematic and user-friendly manner."
  },
  {
    "title": "Cytogenetic, Hematobiochemical, and Histopathological Assessment of Albino Rats (Rattus norvegicus) Fed on Gluten Extracts",
    "url": "http://arxiv.org/abs/2502.14929v1",
    "arxiv_id": "2502.14929v1",
    "authors": [
      "Tajudeen Yahaya",
      "Esther Oladele",
      "Ufuoma Shemishere",
      "Daniel Anyebe",
      "Haliru Abdullahi",
      "Maryam Lawal",
      "Rufai Ahmad"
    ],
    "published": "2025-02-20T05:44:36+00:00",
    "summary": "Background: Literature shows that most of the information on the toxicity of gluten is generated from survey and observational studies, resulting in inconsistent outcomes and a decrease in the acceptability of gluten-rich foods. To determine gluten's safety, an in-depth in vitro and in vivo toxicological examination is required. This enables scientists to come up with ameliorative strategies if it turns out to have side effects, and consumers' trust can be restored. Objectives: The objective of this study was to assess the toxicity of gluten extracts on albino rats (Rattus norvegicus). Materials and Methods: Twenty-four rats were randomly selected and divided into four groups, each comprising six rats. Group 1 (control) rats were fed on pellet feeds and groups 2, 3, and 4 were fed on daily dosages of 0.5, 1.0, and 1.5 g gluten extracts, respectively. The rats' body weights and reactions were observed for 90 days before blood samples were collected for hematobiochemical and micronucleus tests. Histopathological examinations of the liver and kidneys were also performed. Results: There was no difference (P > 0.05) in body weight, blood glucose level, or micronuclei between the control and treated rats. The lymphocytes, alkaline phosphatase, alanine transaminase, total protein, and calcium ions of the test rats were all significantly (P < 0.05) altered but remained within the normal ranges. Other hematobiochemical parameters, including packed cell volume, hemoglobin, white and red blood cells, aspartate transaminase, albumin, sodium ions, potassium ions, chloride ions, and urea, revealed no marked changes. The treated rats' livers and kidneys showed no histopathological changes. Conclusion: Gluten had no adverse effects. However, it altered hematobiochemical parameters, particularly the lymphocytes, alkaline phosphatase, alanine transaminase, total protein, and calcium ions."
  },
  {
    "title": "No Minima, No Collisions: Combining Modulation and Control Barrier Function Strategies for Feasible Dynamical Collision Avoidance",
    "url": "http://arxiv.org/abs/2502.14238v1",
    "arxiv_id": "2502.14238v1",
    "authors": [
      "Yifan Xue",
      "Nadia Figueroa"
    ],
    "published": "2025-02-20T04:07:18+00:00",
    "summary": "As prominent real-time safety-critical reactive control techniques, Control Barrier Function Quadratic Programs (CBF-QPs) work for control affine systems in general but result in local minima in the generated trajectories and consequently cannot ensure convergence to the goals. Contrarily, Modulation of Dynamical Systems (Mod-DSs), including normal, reference, and on-manifold Mod-DS, achieve obstacle avoidance with few and even no local minima but have trouble optimally minimizing the difference between the constrained and the unconstrained controller outputs, and its applications are limited to fully-actuated systems. We dive into the theoretical foundations of CBF-QP and Mod-DS, proving that despite their distinct origins, normal Mod-DS is a special case of CBF-QP, and reference Mod-DS's solutions are mathematically connected to that of the CBF-QP through one equation. Building on top of the unveiled theoretical connections between CBF-QP and Mod-DS, reference Mod-based CBF-QP and on-manifold Mod-based CBF-QP controllers are proposed to combine the strength of CBF-QP and Mod-DS approaches and realize local-minimum-free reactive obstacle avoidance for control affine systems in general. We validate our methods in both simulated hospital environments and real-world experiments using Ridgeback for fully-actuated systems and Fetch robots for underactuated systems. Mod-based CBF-QPs outperform CBF-QPs as well as the optimally constrained-enforcing Mod-DS approaches we proposed in all experiments."
  },
  {
    "title": "OBELiX: A Curated Dataset of Crystal Structures and Experimentally Measured Ionic Conductivities for Lithium Solid-State Electrolytes",
    "url": "http://arxiv.org/abs/2502.14234v1",
    "arxiv_id": "2502.14234v1",
    "authors": [
      "F\u00e9lix Therrien",
      "Jamal Abou Haibeh",
      "Divya Sharma",
      "Rhiannon Hendley",
      "Alex Hern\u00e1ndez-Garc\u00eda",
      "Sun Sun",
      "Alain Tchagang",
      "Jiang Su",
      "Samuel Huberman",
      "Yoshua Bengio",
      "Hongyu Guo",
      "Homin Shin"
    ],
    "published": "2025-02-20T03:59:35+00:00",
    "summary": "Solid-state electrolyte batteries are expected to replace liquid electrolyte lithium-ion batteries in the near future thanks to their higher theoretical energy density and improved safety. However, their adoption is currently hindered by their lower effective ionic conductivity, a quantity that governs charge and discharge rates. Identifying highly ion-conductive materials using conventional theoretical calculations and experimental validation is both time-consuming and resource-intensive. While machine learning holds the promise to expedite this process, relevant ionic conductivity and structural data is scarce. Here, we present OBELiX, a domain-expert-curated database of $\\sim$600 synthesized solid electrolyte materials and their experimentally measured room temperature ionic conductivities gathered from literature. Each material is described by their measured composition, space group and lattice parameters. A full-crystal description in the form of a crystallographic information file (CIF) is provided for ~320 structures for which atomic positions were available. We discuss various statistics and features of the dataset and provide training and testing splits that avoid data leakage. Finally, we benchmark seven existing ML models on the task of predicting ionic conductivity and discuss their performance. The goal of this work is to facilitate the use of machine learning for solid-state electrolyte materials discovery."
  },
  {
    "title": "Do LLMs Consider Security? An Empirical Study on Responses to Programming Questions",
    "url": "http://arxiv.org/abs/2502.14202v1",
    "arxiv_id": "2502.14202v1",
    "authors": [
      "Amirali Sajadi",
      "Binh Le",
      "Anh Nguyen",
      "Kostadin Damevski",
      "Preetha Chatterjee"
    ],
    "published": "2025-02-20T02:20:06+00:00",
    "summary": "The widespread adoption of conversational LLMs for software development has raised new security concerns regarding the safety of LLM-generated content. Our motivational study outlines ChatGPT's potential in volunteering context-specific information to the developers, promoting safe coding practices. Motivated by this finding, we conduct a study to evaluate the degree of security awareness exhibited by three prominent LLMs: Claude 3, GPT-4, and Llama 3. We prompt these LLMs with Stack Overflow questions that contain vulnerable code to evaluate whether they merely provide answers to the questions or if they also warn users about the insecure code, thereby demonstrating a degree of security awareness. Further, we assess whether LLM responses provide information about the causes, exploits, and the potential fixes of the vulnerability, to help raise users' awareness. Our findings show that all three models struggle to accurately detect and warn users about vulnerabilities, achieving a detection rate of only 12.6% to 40% across our datasets. We also observe that the LLMs tend to identify certain types of vulnerabilities related to sensitive information exposure and improper input neutralization much more frequently than other types, such as those involving external control of file names or paths. Furthermore, when LLMs do issue security warnings, they often provide more information on the causes, exploits, and fixes of vulnerabilities compared to Stack Overflow responses. Finally, we provide an in-depth discussion on the implications of our findings and present a CLI-based prompting tool that can be used to generate significantly more secure LLM responses."
  },
  {
    "title": "Multimodal RewardBench: Holistic Evaluation of Reward Models for Vision Language Models",
    "url": "http://arxiv.org/abs/2502.14191v1",
    "arxiv_id": "2502.14191v1",
    "authors": [
      "Michihiro Yasunaga",
      "Luke Zettlemoyer",
      "Marjan Ghazvininejad"
    ],
    "published": "2025-02-20T01:48:13+00:00",
    "summary": "Reward models play an essential role in training vision-language models (VLMs) by assessing output quality to enable aligning with human preferences. Despite their importance, the research community lacks comprehensive open benchmarks for evaluating multimodal reward models in VLMs. To address this gap, we introduce Multimodal RewardBench, an expert-annotated benchmark covering six domains: general correctness, preference, knowledge, reasoning, safety, and visual question-answering. Our dataset comprises 5,211 annotated (prompt, chosen response, rejected response) triplets collected from various VLMs. In evaluating a range of VLM judges, we find that even the top-performing models, Gemini 1.5 Pro and Claude 3.5 Sonnet, achieve only 72% overall accuracy. Notably, most models struggle in the reasoning and safety domains. These findings suggest that Multimodal RewardBench offers a challenging testbed for advancing reward model development across multiple domains. We release the benchmark at https://github.com/facebookresearch/multimodal_rewardbench."
  },
  {
    "title": "Type 1 Diabetes Management using GLIMMER: Glucose Level Indicator Model with Modified Error Rate",
    "url": "http://arxiv.org/abs/2502.14183v1",
    "arxiv_id": "2502.14183v1",
    "authors": [
      "Saman Khamesian",
      "Asiful Arefeen",
      "Adela Grando",
      "Bithika Thompson",
      "Hassan Ghasemzadeh"
    ],
    "published": "2025-02-20T01:26:00+00:00",
    "summary": "Managing Type 1 Diabetes (T1D) demands constant vigilance as individuals strive to regulate their blood glucose levels to avert the dangers of dysglycemia (hyperglycemia or hypoglycemia). Despite the advent of sophisticated technologies such as automated insulin delivery (AID) systems, achieving optimal glycemic control remains a formidable task. AID systems integrate continuous subcutaneous insulin infusion (CSII) and continuous glucose monitors (CGM) data, offering promise in reducing variability and increasing glucose time-in-range. However, these systems often fail to prevent dysglycemia, partly due to limitations in prediction algorithms that lack the precision to avert abnormal glucose events. This gap highlights the need for proactive behavioral adjustments. We address this need with GLIMMER, Glucose Level Indicator Model with Modified Error Rate, a machine learning approach for forecasting blood glucose levels. GLIMMER categorizes glucose values into normal and abnormal ranges and devises a novel custom loss function to prioritize accuracy in dysglycemic events where patient safety is critical. To evaluate the potential of GLIMMER for T1D management, we both use a publicly available dataset and collect new data involving 25 patients with T1D. In predicting next-hour glucose values, GLIMMER achieved a root mean square error (RMSE) of 23.97 (+/-3.77) and a mean absolute error (MAE) of 15.83 (+/-2.09) mg/dL. These results reflect a 23% improvement in RMSE and a 31% improvement in MAE compared to the best-reported error rates."
  },
  {
    "title": "Multi-Faceted Studies on Data Poisoning can Advance LLM Development",
    "url": "http://arxiv.org/abs/2502.14182v1",
    "arxiv_id": "2502.14182v1",
    "authors": [
      "Pengfei He",
      "Yue Xing",
      "Han Xu",
      "Zhen Xiang",
      "Jiliang Tang"
    ],
    "published": "2025-02-20T01:19:51+00:00",
    "summary": "The lifecycle of large language models (LLMs) is far more complex than that of traditional machine learning models, involving multiple training stages, diverse data sources, and varied inference methods. While prior research on data poisoning attacks has primarily focused on the safety vulnerabilities of LLMs, these attacks face significant challenges in practice. Secure data collection, rigorous data cleaning, and the multistage nature of LLM training make it difficult to inject poisoned data or reliably influence LLM behavior as intended. Given these challenges, this position paper proposes rethinking the role of data poisoning and argue that multi-faceted studies on data poisoning can advance LLM development. From a threat perspective, practical strategies for data poisoning attacks can help evaluate and address real safety risks to LLMs. From a trustworthiness perspective, data poisoning can be leveraged to build more robust LLMs by uncovering and mitigating hidden biases, harmful outputs, and hallucinations. Moreover, from a mechanism perspective, data poisoning can provide valuable insights into LLMs, particularly the interplay between data and model behavior, driving a deeper understanding of their underlying mechanisms."
  },
  {
    "title": "On the logical skills of large language models: evaluations using arbitrarily complex first-order logic problems",
    "url": "http://arxiv.org/abs/2502.14180v1",
    "arxiv_id": "2502.14180v1",
    "authors": [
      "Shokhrukh Ibragimov",
      "Arnulf Jentzen",
      "Benno Kuckuck"
    ],
    "published": "2025-02-20T01:18:24+00:00",
    "summary": "We present a method of generating first-order logic statements whose complexity can be controlled along multiple dimensions. We use this method to automatically create several datasets consisting of questions asking for the truth or falsity of first-order logic statements in Zermelo-Fraenkel set theory. While the resolution of these questions does not require any knowledge beyond basic notation of first-order logic and set theory, it does require a degree of planning and logical reasoning, which can be controlled up to arbitrarily high difficulty by the complexity of the generated statements. Furthermore, we do extensive evaluations of the performance of various large language models, including recent models such as DeepSeek-R1 and OpenAI's o3-mini, on these datasets. All of the datasets along with the code used for generating them, as well as all data from the evaluations is publicly available at https://github.com/bkuckuck/logical-skills-of-llms."
  },
  {
    "title": "Multi-Agent Risks from Advanced AI",
    "url": "http://arxiv.org/abs/2502.14143v1",
    "arxiv_id": "2502.14143v1",
    "authors": [
      "Lewis Hammond",
      "Alan Chan",
      "Jesse Clifton",
      "Jason Hoelscher-Obermaier",
      "Akbir Khan",
      "Euan McLean",
      "Chandler Smith",
      "Wolfram Barfuss",
      "Jakob Foerster",
      "Tom\u00e1\u0161 Gaven\u010diak",
      "The Anh Han",
      "Edward Hughes",
      "Vojt\u011bch Kova\u0159\u00edk",
      "Jan Kulveit",
      "Joel Z. Leibo",
      "Caspar Oesterheld",
      "Christian Schroeder de Witt",
      "Nisarg Shah",
      "Michael Wellman",
      "Paolo Bova",
      "Theodor Cimpeanu",
      "Carson Ezell",
      "Quentin Feuillade-Montixi",
      "Matija Franklin",
      "Esben Kran",
      "Igor Krawczuk",
      "Max Lamparth",
      "Niklas Lauffer",
      "Alexander Meinke",
      "Sumeet Motwani",
      "Anka Reuel",
      "Vincent Conitzer",
      "Michael Dennis",
      "Iason Gabriel",
      "Adam Gleave",
      "Gillian Hadfield",
      "Nika Haghtalab",
      "Atoosa Kasirzadeh",
      "S\u00e9bastien Krier",
      "Kate Larson",
      "Joel Lehman",
      "David C. Parkes",
      "Georgios Piliouras",
      "Iyad Rahwan"
    ],
    "published": "2025-02-19T23:03:21+00:00",
    "summary": "The rapid development of advanced AI agents and the imminent deployment of many instances of these agents will give rise to multi-agent systems of unprecedented complexity. These systems pose novel and under-explored risks. In this report, we provide a structured taxonomy of these risks by identifying three key failure modes (miscoordination, conflict, and collusion) based on agents' incentives, as well as seven key risk factors (information asymmetries, network effects, selection pressures, destabilising dynamics, commitment problems, emergent agency, and multi-agent security) that can underpin them. We highlight several important instances of each risk, as well as promising directions to help mitigate them. By anchoring our analysis in a range of real-world examples and experimental evidence, we illustrate the distinct challenges posed by multi-agent systems and their implications for the safety, governance, and ethics of advanced AI."
  },
  {
    "title": "PedDet: Adaptive Spectral Optimization for Multimodal Pedestrian Detection",
    "url": "http://arxiv.org/abs/2502.14063v1",
    "arxiv_id": "2502.14063v1",
    "authors": [
      "Rui Zhao",
      "Zeyu Zhang",
      "Yi Xu",
      "Yi Yao",
      "Yan Huang",
      "Wenxin Zhang",
      "Zirui Song",
      "Xiuying Chen",
      "Yang Zhao"
    ],
    "published": "2025-02-19T19:31:51+00:00",
    "summary": "Pedestrian detection in intelligent transportation systems has made significant progress but faces two critical challenges: (1) insufficient fusion of complementary information between visible and infrared spectra, particularly in complex scenarios, and (2) sensitivity to illumination changes, such as low-light or overexposed conditions, leading to degraded performance. To address these issues, we propose PedDet, an adaptive spectral optimization complementarity framework specifically enhanced and optimized for multispectral pedestrian detection. PedDet introduces the Multi-scale Spectral Feature Perception Module (MSFPM) to adaptively fuse visible and infrared features, enhancing robustness and flexibility in feature extraction. Additionally, the Illumination Robustness Feature Decoupling Module (IRFDM) improves detection stability under varying lighting by decoupling pedestrian and background features. We further design a contrastive alignment to enhance intermodal feature discrimination. Experiments on LLVIP and MSDS datasets demonstrate that PedDet achieves state-of-the-art performance, improving the mAP by 6.6% with superior detection accuracy even in low-light conditions, marking a significant step forward for road safety. Code will be available at https://github.com/AIGeeksGroup/PedDet."
  },
  {
    "title": "Asking for Help Enables Safety Guarantees Without Sacrificing Effectiveness",
    "url": "http://arxiv.org/abs/2502.14043v1",
    "arxiv_id": "2502.14043v1",
    "authors": [
      "Benjamin Plaut",
      "Juan Li\u00e9vano-Karim",
      "Stuart Russell"
    ],
    "published": "2025-02-19T19:01:39+00:00",
    "summary": "Most reinforcement learning algorithms with regret guarantees rely on a critical assumption: that all errors are recoverable. Recent work by Plaut et al. discarded this assumption and presented algorithms that avoid \"catastrophe\" (i.e., irreparable errors) by asking for help. However, they provided only safety guarantees and did not consider reward maximization. We prove that any algorithm that avoids catastrophe in their setting also guarantees high reward (i.e., sublinear regret) in any Markov Decision Process (MDP), including MDPs with irreversible costs. This constitutes the first no-regret guarantee for general MDPs. More broadly, our result may be the first formal proof that it is possible for an agent to obtain high reward while becoming self-sufficient in an unknown, unbounded, and high-stakes environment without causing catastrophe or requiring resets."
  },
  {
    "title": "Why Safeguarded Ships Run Aground? Aligned Large Language Models' Safety Mechanisms Tend to Be Anchored in The Template Region",
    "url": "http://arxiv.org/abs/2502.13946v1",
    "arxiv_id": "2502.13946v1",
    "authors": [
      "Chak Tou Leong",
      "Qingyu Yin",
      "Jian Wang",
      "Wenjie Li"
    ],
    "published": "2025-02-19T18:42:45+00:00",
    "summary": "The safety alignment of large language models (LLMs) remains vulnerable, as their initial behavior can be easily jailbroken by even relatively simple attacks. Since infilling a fixed template between the input instruction and initial model output is a common practice for existing LLMs, we hypothesize that this template is a key factor behind their vulnerabilities: LLMs' safety-related decision-making overly relies on the aggregated information from the template region, which largely influences these models' safety behavior. We refer to this issue as template-anchored safety alignment. In this paper, we conduct extensive experiments and verify that template-anchored safety alignment is widespread across various aligned LLMs. Our mechanistic analyses demonstrate how it leads to models' susceptibility when encountering inference-time jailbreak attacks. Furthermore, we show that detaching safety mechanisms from the template region is promising in mitigating vulnerabilities to jailbreak attacks. We encourage future research to develop more robust safety alignment techniques that reduce reliance on the template region."
  },
  {
    "title": "Learning to explore when mistakes are not allowed",
    "url": "http://arxiv.org/abs/2502.13801v1",
    "arxiv_id": "2502.13801v1",
    "authors": [
      "Charly Pecqueux-Gu\u00e9z\u00e9nec",
      "St\u00e9phane Doncieux",
      "Nicolas Perrin-Gilbert"
    ],
    "published": "2025-02-19T15:11:51+00:00",
    "summary": "Goal-Conditioned Reinforcement Learning (GCRL) provides a versatile framework for developing unified controllers capable of handling wide ranges of tasks, exploring environments, and adapting behaviors. However, its reliance on trial-and-error poses challenges for real-world applications, as errors can result in costly and potentially damaging consequences. To address the need for safer learning, we propose a method that enables agents to learn goal-conditioned behaviors that explore without the risk of making harmful mistakes. Exploration without risks can seem paradoxical, but environment dynamics are often uniform in space, therefore a policy trained for safety without exploration purposes can still be exploited globally. Our proposed approach involves two distinct phases. First, during a pretraining phase, we employ safe reinforcement learning and distributional techniques to train a safety policy that actively tries to avoid failures in various situations. In the subsequent safe exploration phase, a goal-conditioned (GC) policy is learned while ensuring safety. To achieve this, we implement an action-selection mechanism leveraging the previously learned distributional safety critics to arbitrate between the safety policy and the GC policy, ensuring safe exploration by switching to the safety policy when needed. We evaluate our method in simulated environments and demonstrate that it not only provides substantial coverage of the goal space but also reduces the occurrence of mistakes to a minimum, in stark contrast to traditional GCRL approaches. Additionally, we conduct an ablation study and analyze failure modes, offering insights for future research directions."
  },
  {
    "title": "User Association and Coordinated Beamforming in Cognitive Aerial-Terrestrial Networks: A Safe Reinforcement Learning Approach",
    "url": "http://arxiv.org/abs/2502.13663v1",
    "arxiv_id": "2502.13663v1",
    "authors": [
      "Zizhen Zhou",
      "Jungang Ge",
      "Ying-Chang Liang"
    ],
    "published": "2025-02-19T12:15:32+00:00",
    "summary": "Cognitive aerial-terrestrial networks (CATNs) offer a solution to spectrum scarcity by sharing spectrum between aerial and terrestrial networks. However, aerial users (AUs) experience significant interference from numerous terrestrial base stations (BSs). To alleviate such interference, we investigate a user association and coordinated beamforming (CBF) problem in CATN, where the aerial network serves as the primary network sharing its spectrum with the terrestrial network. Specifically, we maximize the sum rate of the secondary terrestrial users (TUs) under the interference temperature constraints of the AUs. Traditional iterative optimization schemes are impractical due to their high computational complexity and information exchange overhead. Although deep reinforcement learning (DRL) based schemes can address these challenges, their performance is sensitive to the weights of the weighted penalty terms for violating constraints in the reward function. Motivated by these issues, we propose a safe DRL-based user association and CBF scheme for CATN, eliminating the need for training multiple times to find the optimal penalty weight before actual deployment. Specifically, the CATN is modeled as a networked constrained partially observable Markov game. Each TU acts as an agent to choose its associated BS, and each BS acts as an agent to decide its beamforming vectors, aiming to maximize the reward while satisfying the safety constraints introduced by the interference constraints of the AUs. By exploiting a safe DRL algorithm, the proposed scheme incurs lower deployment expenses than the penalty-based DRL schemes since only one training is required before actual deployment. Simulation results show that the proposed scheme can achieve a higher sum rate of TUs than a two-stage optimization scheme while the average received interference power of the AUs is generally below the threshold."
  },
  {
    "title": "SLAMSpoof: Practical LiDAR Spoofing Attacks on Localization Systems Guided by Scan Matching Vulnerability Analysis",
    "url": "http://arxiv.org/abs/2502.13641v1",
    "arxiv_id": "2502.13641v1",
    "authors": [
      "Rokuto Nagata",
      "Kenji Koide",
      "Yuki Hayakawa",
      "Ryo Suzuki",
      "Kazuma Ikeda",
      "Ozora Sako",
      "Qi Alfred Chen",
      "Takami Sato",
      "Kentaro Yoshioka"
    ],
    "published": "2025-02-19T11:33:56+00:00",
    "summary": "Accurate localization is essential for enabling modern full self-driving services. These services heavily rely on map-based traffic information to reduce uncertainties in recognizing lane shapes, traffic light locations, and traffic signs. Achieving this level of reliance on map information requires centimeter-level localization accuracy, which is currently only achievable with LiDAR sensors. However, LiDAR is known to be vulnerable to spoofing attacks that emit malicious lasers against LiDAR to overwrite its measurements. Once localization is compromised, the attack could lead the victim off roads or make them ignore traffic lights. Motivated by these serious safety implications, we design SLAMSpoof, the first practical LiDAR spoofing attack on localization systems for self-driving to assess the actual attack significance on autonomous vehicles. SLAMSpoof can effectively find the effective attack location based on our scan matching vulnerability score (SMVS), a point-wise metric representing the potential vulnerability to spoofing attacks. To evaluate the effectiveness of the attack, we conduct real-world experiments on ground vehicles and confirm its high capability in real-world scenarios, inducing position errors of $\\geq$4.2 meters (more than typical lane width) for all 3 popular LiDAR-based localization algorithms. We finally discuss the potential countermeasures of this attack. Code is available at https://github.com/Keio-CSG/slamspoof"
  },
  {
    "title": "Qorgau: Evaluating LLM Safety in Kazakh-Russian Bilingual Contexts",
    "url": "http://arxiv.org/abs/2502.13640v1",
    "arxiv_id": "2502.13640v1",
    "authors": [
      "Maiya Goloburda",
      "Nurkhan Laiyk",
      "Diana Turmakhan",
      "Yuxia Wang",
      "Mukhammed Togmanov",
      "Jonibek Mansurov",
      "Askhat Sametov",
      "Nurdaulet Mukhituly",
      "Minghan Wang",
      "Daniil Orel",
      "Zain Muhammad Mujahid",
      "Fajri Koto",
      "Timothy Baldwin",
      "Preslav Nakov"
    ],
    "published": "2025-02-19T11:33:22+00:00",
    "summary": "Large language models (LLMs) are known to have the potential to generate harmful content, posing risks to users. While significant progress has been made in developing taxonomies for LLM risks and safety evaluation prompts, most studies have focused on monolingual contexts, primarily in English. However, language- and region-specific risks in bilingual contexts are often overlooked, and core findings can diverge from those in monolingual settings. In this paper, we introduce Qorgau, a novel dataset specifically designed for safety evaluation in Kazakh and Russian, reflecting the unique bilingual context in Kazakhstan, where both Kazakh (a low-resource language) and Russian (a high-resource language) are spoken. Experiments with both multilingual and language-specific LLMs reveal notable differences in safety performance, emphasizing the need for tailored, region-specific datasets to ensure the responsible and safe deployment of LLMs in countries like Kazakhstan. Warning: this paper contains example data that may be offensive, harmful, or biased."
  },
  {
    "title": "First Glimpse on Physical Layer Security in Internet of Vehicles: Transformed from Communication Interference to Sensing Interference",
    "url": "http://arxiv.org/abs/2502.13634v1",
    "arxiv_id": "2502.13634v1",
    "authors": [
      "Kaixuan Li",
      "Kan Yu",
      "Xiaowu Liu",
      "Qixun Zhang",
      "Zhiyong Feng",
      "Dong Li"
    ],
    "published": "2025-02-19T11:13:18+00:00",
    "summary": "Integrated sensing and communication (ISAC) plays a crucial role in the Internet of Vehicles (IoV), serving as a key factor in enhancing driving safety and traffic efficiency. To address the security challenges of the confidential information transmission caused by the inherent openness nature of wireless medium, different from current physical layer security (PLS) methods, which depends on the \\emph{additional communication interference} costing extra power resources, in this paper, we investigate a novel PLS solution, under which the \\emph{inherent radar sensing interference} of the vehicles is utilized to secure wireless communications. To measure the performance of PLS methods in ISAC-based IoV systems, we first define an improved security performance metric called by transmission reliability and sensing accuracy based secrecy rate (TRSA\\_SR), and derive closed-form expressions of connection outage probability (COP), secrecy outage probability (SOP), success ranging probability (SRP) for evaluating transmission reliability, security and sensing accuracy, respectively. Furthermore, we formulate an optimization problem to maximize the TRSA\\_SR by utilizing radar sensing interference and joint design of the communication duration, transmission power and straight trajectory of the legitimate transmitter. Finally, the non-convex feature of formulated problem is solved through the problem decomposition and alternating optimization. Simulations indicate that compared with traditional PLS methods obtaining a non-positive STC, the proposed method achieves a secrecy rate of 3.92bps/Hz for different settings of noise power."
  },
  {
    "title": "Efficient Safety Retrofitting Against Jailbreaking for LLMs",
    "url": "http://arxiv.org/abs/2502.13603v1",
    "arxiv_id": "2502.13603v1",
    "authors": [
      "Dario Garcia-Gasulla",
      "Anna Arias-Duart",
      "Adrian Tormos",
      "Daniel Hinjos",
      "Oscar Molina-Sedano",
      "Ashwin Kumar Gururajan",
      "Maria Eugenia Cardello"
    ],
    "published": "2025-02-19T10:33:18+00:00",
    "summary": "Direct Preference Optimization (DPO) is an efficient alignment technique that steers LLMs towards preferable outputs by training on preference data, bypassing the need for explicit reward models. Its simplicity enables easy adaptation to various domains and safety requirements. This paper examines DPO's effectiveness in model safety against jailbreaking attacks while minimizing data requirements and training costs. We introduce Egida, a dataset expanded from multiple sources, which includes 27 different safety topics and 18 different attack styles, complemented with synthetic and human labels. This data is used to boost the safety of state-of-the-art LLMs (Llama-3.1-8B/70B-Instruct, Qwen-2.5-7B/72B-Instruct) across topics and attack styles. In addition to safety evaluations, we assess their post-alignment performance degradation in general purpose tasks, and their tendency to over refusal. Following the proposed methodology, trained models reduce their Attack Success Rate by 10%-30%, using small training efforts (2,000 samples) with low computational cost (3\\$ for 8B models, 20\\$ for 72B models). Safety aligned models generalize to unseen topics and attack styles, with the most successful attack style reaching a success rate around 5%. Size and family are found to strongly influence model malleability towards safety, pointing at the importance of pre-training choices. To validate our findings, a large independent assessment of human preference agreement with Llama-Guard-3-8B is conducted by the authors and the associated dataset Egida-HSafe is released. Overall, this study illustrates how affordable and accessible it is to enhance LLM safety using DPO while outlining its current limitations. All datasets and models are released to enable reproducibility and further research."
  },
  {
    "title": "Exploiting Prefix-Tree in Structured Output Interfaces for Enhancing Jailbreak Attacking",
    "url": "http://arxiv.org/abs/2502.13527v1",
    "arxiv_id": "2502.13527v1",
    "authors": [
      "Yanzeng Li",
      "Yunfan Xiong",
      "Jialun Zhong",
      "Jinchao Zhang",
      "Jie Zhou",
      "Lei Zou"
    ],
    "published": "2025-02-19T08:29:36+00:00",
    "summary": "The rise of Large Language Models (LLMs) has led to significant applications but also introduced serious security threats, particularly from jailbreak attacks that manipulate output generation. These attacks utilize prompt engineering and logit manipulation to steer models toward harmful content, prompting LLM providers to implement filtering and safety alignment strategies. We investigate LLMs' safety mechanisms and their recent applications, revealing a new threat model targeting structured output interfaces, which enable attackers to manipulate the inner logit during LLM generation, requiring only API access permissions. To demonstrate this threat model, we introduce a black-box attack framework called AttackPrefixTree (APT). APT exploits structured output interfaces to dynamically construct attack patterns. By leveraging prefixes of models' safety refusal response and latent harmful outputs, APT effectively bypasses safety measures. Experiments on benchmark datasets indicate that this approach achieves higher attack success rate than existing methods. This work highlights the urgent need for LLM providers to enhance security protocols to address vulnerabilities arising from the interaction between safety patterns and structured outputs."
  },
  {
    "title": "Integration of Agentic AI with 6G Networks for Mission-Critical Applications: Use-case and Challenges",
    "url": "http://arxiv.org/abs/2502.13476v1",
    "arxiv_id": "2502.13476v1",
    "authors": [
      "Sunder Ali Khowaja",
      "Kapal Dev",
      "Muhammad Salman Pathan",
      "Engin Zeydan",
      "Merouane Debbah"
    ],
    "published": "2025-02-19T07:00:53+00:00",
    "summary": "We are in a transformative era, and advances in Artificial Intelligence (AI), especially the foundational models, are constantly in the news. AI has been an integral part of many applications that rely on automation for service delivery, and one of them is mission-critical public safety applications. The problem with AI-oriented mission-critical applications is the humanin-the-loop system and the lack of adaptability to dynamic conditions while maintaining situational awareness. Agentic AI (AAI) has gained a lot of attention recently due to its ability to analyze textual data through a contextual lens while quickly adapting to conditions. In this context, this paper proposes an AAI framework for mission-critical applications. We propose a novel framework with a multi-layer architecture to realize the AAI. We also present a detailed implementation of AAI layer that bridges the gap between network infrastructure and missioncritical applications. Our preliminary analysis shows that the AAI reduces initial response time by 5.6 minutes on average, while alert generation time is reduced by 15.6 seconds on average and resource allocation is improved by up to 13.4%. We also show that the AAI methods improve the number of concurrent operations by 40, which reduces the recovery time by up to 5.2 minutes. Finally, we highlight some of the issues and challenges that need to be considered when implementing AAI frameworks."
  },
  {
    "title": "Beyond Single-Value Metrics: Evaluating and Enhancing LLM Unlearning with Cognitive Diagnosis",
    "url": "http://arxiv.org/abs/2502.13996v1",
    "arxiv_id": "2502.13996v1",
    "authors": [
      "Yicheng Lang",
      "Kehan Guo",
      "Yue Huang",
      "Yujun Zhou",
      "Haomin Zhuang",
      "Tianyu Yang",
      "Yao Su",
      "Xiangliang Zhang"
    ],
    "published": "2025-02-19T06:56:59+00:00",
    "summary": "Due to the widespread use of LLMs and the rising critical ethical and safety concerns, LLM unlearning methods have been developed to remove harmful knowledge and undesirable capabilities. In this context, evaluations are mostly based on single-value metrics such as QA accuracy. However, these metrics often fail to capture the nuanced retention of harmful knowledge components, making it difficult to assess the true effectiveness of unlearning. To address this issue, we propose UNCD (UNlearning evaluation via Cognitive Diagnosis), a novel framework that leverages Cognitive Diagnosis Modeling for fine-grained evaluation of LLM unlearning. Our dedicated benchmark, UNCD-Cyber, provides a detailed assessment of the removal of dangerous capabilities. Moreover, we introduce UNCD-Agent, which refines unlearning by diagnosing knowledge remnants and generating targeted unlearning data. Extensive experiments across eight unlearning methods and two base models demonstrate that UNCD not only enhances evaluation but also effectively facilitates the removal of harmful LLM abilities."
  },
  {
    "title": "ThinkGuard: Deliberative Slow Thinking Leads to Cautious Guardrails",
    "url": "http://arxiv.org/abs/2502.13458v1",
    "arxiv_id": "2502.13458v1",
    "authors": [
      "Xiaofei Wen",
      "Wenxuan Zhou",
      "Wenjie Jacky Mo",
      "Muhao Chen"
    ],
    "published": "2025-02-19T06:09:58+00:00",
    "summary": "Ensuring the safety of large language models (LLMs) is critical as they are deployed in real-world applications. Existing guardrails rely on rule-based filtering or single-pass classification, limiting their ability to handle nuanced safety violations. To address this, we propose ThinkGuard, a critique-augmented guardrail model that distills knowledge from high-capacity LLMs by generating structured critiques alongside safety labels. Fine-tuned on critique-augmented data, the captured deliberative thinking ability drastically enhances the guardrail's cautiousness and interpretability. Evaluated on multiple safety benchmarks, ThinkGuard achieves the highest average F1 and AUPRC, outperforming all baselines. Compared to LLaMA Guard 3, ThinkGuard improves accuracy by 16.1% and macro F1 by 27.0%. Moreover, it surpasses label-only fine-tuned models, confirming that structured critiques enhance both classification precision and nuanced safety reasoning while maintaining computational efficiency."
  },
  {
    "title": "TreeCut: A Synthetic Unanswerable Math Word Problem Dataset for LLM Hallucination Evaluation",
    "url": "http://arxiv.org/abs/2502.13442v1",
    "arxiv_id": "2502.13442v1",
    "authors": [
      "Jialin Ouyang"
    ],
    "published": "2025-02-19T05:38:45+00:00",
    "summary": "Large language models (LLMs) now achieve near-human performance on standard math word problem benchmarks (e.g., GSM8K), yet their true reasoning ability remains disputed. A key concern is that models often produce confident, yet unfounded, answers to unanswerable problems. We introduce TreeCut, a synthetic dataset that systematically generates infinite unanswerable math word problems and their answerable counterparts, by representing each question as a tree and removing chosen necessary conditions. Experiments show TreeCut effectively induce hallucinations in large language models, including GPT-4o and o3-mini, with rates of 61% and 42% in their respective worst-case scenarios. Further analysis highlights that deeper or more complex trees, composite item names, and removing necessary condition near the middle of a path all increase the likelihood of hallucinations, underscoring the persistent challenges LLMs face in identifying unanswerable math problems."
  },
  {
    "title": "Dynamic directed functional connectivity as a neural biomarker for objective motor skill assessment",
    "url": "http://arxiv.org/abs/2502.13362v1",
    "arxiv_id": "2502.13362v1",
    "authors": [
      "Anil Kamat",
      "Rahul Rahul",
      "Anirban Dutta",
      "Lora Cavuoto",
      "Uwe Kruger",
      "Harry Burke",
      "Matthew Hackett",
      "Jack Norfleet",
      "Steven Schwaitzberg",
      "Suvranu De"
    ],
    "published": "2025-02-19T01:51:39+00:00",
    "summary": "Objective motor skill assessment plays a critical role in fields such as surgery, where proficiency is vital for certification and patient safety. Existing assessment methods, however, rely heavily on subjective human judgment, which introduces bias and limits reproducibility. While recent efforts have leveraged kinematic data and neural imaging to provide more objective evaluations, these approaches often overlook the dynamic neural mechanisms that differentiate expert and novice performance. This study proposes a novel method for motor skill assessment based on dynamic directed functional connectivity (dFC) as a neural biomarker. By using electroencephalography (EEG) to capture brain dynamics and employing an attention-based Long Short-Term Memory (LSTM) model for non-linear Granger causality analysis, we compute dFC among key brain regions involved in psychomotor tasks. Coupled with hierarchical task analysis (HTA), our approach enables subtask-level evaluation of motor skills, offering detailed insights into neural coordination that underpins expert proficiency. A convolutional neural network (CNN) is then used to classify skill levels, achieving greater accuracy and specificity than established performance metrics in laparoscopic surgery. This methodology provides a reliable, objective framework for assessing motor skills, contributing to the development of tailored training protocols and enhancing the certification process."
  },
  {
    "title": "Language Models are Few-Shot Graders",
    "url": "http://arxiv.org/abs/2502.13337v1",
    "arxiv_id": "2502.13337v1",
    "authors": [
      "Chenyan Zhao",
      "Mariana Silva",
      "Seth Poulsen"
    ],
    "published": "2025-02-18T23:38:21+00:00",
    "summary": "Providing evaluations to student work is a critical component of effective student learning, and automating its process can significantly reduce the workload on human graders. Automatic Short Answer Grading (ASAG) systems, enabled by advancements in Large Language Models (LLMs), offer a promising solution for assessing and providing instant feedback for open-ended student responses. In this paper, we present an ASAG pipeline leveraging state-of-the-art LLMs. Our new LLM-based ASAG pipeline achieves better performances than existing custom-built models on the same datasets. We also compare the grading performance of three OpenAI models: GPT-4, GPT-4o, and o1-preview. Our results demonstrate that GPT-4o achieves the best balance between accuracy and cost-effectiveness. On the other hand, o1-preview, despite higher accuracy, exhibits a larger variance in error that makes it less practical for classroom use. We investigate the effects of incorporating instructor-graded examples into prompts using no examples, random selection, and Retrieval-Augmented Generation (RAG)-based selection strategies. Our findings indicate that providing graded examples enhances grading accuracy, with RAG-based selection outperforming random selection. Additionally, integrating grading rubrics improves accuracy by offering a structured standard for evaluation."
  },
  {
    "title": "Demonstrating specification gaming in reasoning models",
    "url": "http://arxiv.org/abs/2502.13295v1",
    "arxiv_id": "2502.13295v1",
    "authors": [
      "Alexander Bondarenko",
      "Denis Volk",
      "Dmitrii Volkov",
      "Jeffrey Ladish"
    ],
    "published": "2025-02-18T21:32:24+00:00",
    "summary": "We demonstrate LLM agent specification gaming by instructing models to win against a chess engine. We find reasoning models like o1 preview and DeepSeek-R1 will often hack the benchmark by default, while language models like GPT-4o and Claude 3.5 Sonnet need to be told that normal play won't work to hack.   We improve upon prior work like (Hubinger et al., 2024; Meinke et al., 2024; Weij et al., 2024) by using realistic task prompts and avoiding excess nudging. Our results suggest reasoning models may resort to hacking to solve difficult problems, as observed in OpenAI (2024)'s o1 Docker escape during cyber capabilities testing."
  },
  {
    "title": "RAD: Training an End-to-End Driving Policy via Large-Scale 3DGS-based Reinforcement Learning",
    "url": "http://arxiv.org/abs/2502.13144v1",
    "arxiv_id": "2502.13144v1",
    "authors": [
      "Hao Gao",
      "Shaoyu Chen",
      "Bo Jiang",
      "Bencheng Liao",
      "Yiang Shi",
      "Xiaoyang Guo",
      "Yuechuan Pu",
      "Haoran Yin",
      "Xiangyu Li",
      "Xinbang Zhang",
      "Ying Zhang",
      "Wenyu Liu",
      "Qian Zhang",
      "Xinggang Wang"
    ],
    "published": "2025-02-18T18:59:21+00:00",
    "summary": "Existing end-to-end autonomous driving (AD) algorithms typically follow the Imitation Learning (IL) paradigm, which faces challenges such as causal confusion and the open-loop gap. In this work, we establish a 3DGS-based closed-loop Reinforcement Learning (RL) training paradigm. By leveraging 3DGS techniques, we construct a photorealistic digital replica of the real physical world, enabling the AD policy to extensively explore the state space and learn to handle out-of-distribution scenarios through large-scale trial and error. To enhance safety, we design specialized rewards that guide the policy to effectively respond to safety-critical events and understand real-world causal relationships. For better alignment with human driving behavior, IL is incorporated into RL training as a regularization term. We introduce a closed-loop evaluation benchmark consisting of diverse, previously unseen 3DGS environments. Compared to IL-based methods, RAD achieves stronger performance in most closed-loop metrics, especially 3x lower collision rate. Abundant closed-loop results are presented at https://hgao-cv.github.io/RAD."
  },
  {
    "title": "RHINO: Learning Real-Time Humanoid-Human-Object Interaction from Human Demonstrations",
    "url": "http://arxiv.org/abs/2502.13134v1",
    "arxiv_id": "2502.13134v1",
    "authors": [
      "Jingxiao Chen",
      "Xinyao Li",
      "Jiahang Cao",
      "Zhengbang Zhu",
      "Wentao Dong",
      "Minghuan Liu",
      "Ying Wen",
      "Yong Yu",
      "Liqing Zhang",
      "Weinan Zhang"
    ],
    "published": "2025-02-18T18:56:41+00:00",
    "summary": "Humanoid robots have shown success in locomotion and manipulation. Despite these basic abilities, humanoids are still required to quickly understand human instructions and react based on human interaction signals to become valuable assistants in human daily life. Unfortunately, most existing works only focus on multi-stage interactions, treating each task separately, and neglecting real-time feedback. In this work, we aim to empower humanoid robots with real-time reaction abilities to achieve various tasks, allowing human to interrupt robots at any time, and making robots respond to humans immediately. To support such abilities, we propose a general humanoid-human-object interaction framework, named RHINO, i.e., Real-time Humanoid-human Interaction and Object manipulation. RHINO provides a unified view of reactive motion, instruction-based manipulation, and safety concerns, over multiple human signal modalities, such as languages, images, and motions. RHINO is a hierarchical learning framework, enabling humanoids to learn reaction skills from human-human-object demonstrations and teleoperation data. In particular, it decouples the interaction process into two levels: 1) a high-level planner inferring human intentions from real-time human behaviors; and 2) a low-level controller achieving reactive motion behaviors and object manipulation skills based on the predicted intentions. We evaluate the proposed framework on a real humanoid robot and demonstrate its effectiveness, flexibility, and safety in various scenarios."
  },
  {
    "title": "Rethinking Diverse Human Preference Learning through Principal Component Analysis",
    "url": "http://arxiv.org/abs/2502.13131v1",
    "arxiv_id": "2502.13131v1",
    "authors": [
      "Feng Luo",
      "Rui Yang",
      "Hao Sun",
      "Chunyuan Deng",
      "Jiarui Yao",
      "Jingyan Shen",
      "Huan Zhang",
      "Hanjie Chen"
    ],
    "published": "2025-02-18T18:55:26+00:00",
    "summary": "Understanding human preferences is crucial for improving foundation models and building personalized AI systems. However, preferences are inherently diverse and complex, making it difficult for traditional reward models to capture their full range. While fine-grained preference data can help, collecting it is expensive and hard to scale. In this paper, we introduce Decomposed Reward Models (DRMs), a novel approach that extracts diverse human preferences from binary comparisons without requiring fine-grained annotations. Our key insight is to represent human preferences as vectors and analyze them using Principal Component Analysis (PCA). By constructing a dataset of embedding differences between preferred and rejected responses, DRMs identify orthogonal basis vectors that capture distinct aspects of preference. These decomposed rewards can be flexibly combined to align with different user needs, offering an interpretable and scalable alternative to traditional reward models. We demonstrate that DRMs effectively extract meaningful preference dimensions (e.g., helpfulness, safety, humor) and adapt to new users without additional training. Our results highlight DRMs as a powerful framework for personalized and interpretable LLM alignment."
  },
  {
    "title": "Understanding and Rectifying Safety Perception Distortion in VLMs",
    "url": "http://arxiv.org/abs/2502.13095v1",
    "arxiv_id": "2502.13095v1",
    "authors": [
      "Xiaohan Zou",
      "Jian Kang",
      "George Kesidis",
      "Lu Lin"
    ],
    "published": "2025-02-18T18:06:48+00:00",
    "summary": "Recent studies reveal that vision-language models (VLMs) become more susceptible to harmful requests and jailbreak attacks after integrating the vision modality, exhibiting greater vulnerability than their text-only LLM backbones. To uncover the root cause of this phenomenon, we conduct an in-depth analysis and identify a key issue: multimodal inputs introduce an modality-induced activation shift toward a \"safer\" direction compared to their text-only counterparts, leading VLMs to systematically overestimate the safety of harmful inputs. We refer to this issue as safety perception distortion. To mitigate such distortion, we propose Activation Shift Disentanglement and Calibration (ShiftDC), a training-free method that decomposes and calibrates the modality-induced activation shift to reduce the impact of modality on safety. By isolating and removing the safety-relevant component, ShiftDC restores the inherent safety alignment of the LLM backbone while preserving the vision-language capabilities of VLMs. Empirical results demonstrate that ShiftDC significantly enhances alignment performance on safety benchmarks without impairing model utility."
  },
  {
    "title": "Conditional Max-Sum for Asynchronous Multiagent Decision Making",
    "url": "http://arxiv.org/abs/2502.13194v1",
    "arxiv_id": "2502.13194v1",
    "authors": [
      "Dimitrios Troullinos",
      "Georgios Chalkiadakis",
      "Ioannis Papamichail",
      "Markos Papageorgiou"
    ],
    "published": "2025-02-18T17:16:27+00:00",
    "summary": "In this paper we present a novel approach for multiagent decision making in dynamic environments based on Factor Graphs and the Max-Sum algorithm, considering asynchronous variable reassignments and distributed message-passing among agents. Motivated by the challenging domain of lane-free traffic where automated vehicles can communicate and coordinate as agents, we propose a more realistic communication framework for Factor Graph formulations that satisfies the above-mentioned restrictions, along with Conditional Max-Sum: an extension of Max-Sum with a revised message-passing process that is better suited for asynchronous settings. The overall application in lane-free traffic can be viewed as a hybrid system where the Factor Graph formulation undertakes the strategic decision making of vehicles, that of desired lateral alignment in a coordinated manner; and acts on top of a rule-based method we devise that provides a structured representation of the lane-free environment for the factors, while also handling the underlying control of vehicles regarding core operations and safety. Our experimental evaluation showcases the capabilities of the proposed framework in problems with intense coordination needs when compared to a domain-specific baseline without communication, and an increased adeptness of Conditional Max-Sum with respect to the standard algorithm."
  },
  {
    "title": "Interactive Agents to Overcome Ambiguity in Software Engineering",
    "url": "http://arxiv.org/abs/2502.13069v1",
    "arxiv_id": "2502.13069v1",
    "authors": [
      "Sanidhya Vijayvargiya",
      "Xuhui Zhou",
      "Akhila Yerukola",
      "Maarten Sap",
      "Graham Neubig"
    ],
    "published": "2025-02-18T17:12:26+00:00",
    "summary": "AI agents are increasingly being deployed to automate tasks, often based on ambiguous and underspecified user instructions. Making unwarranted assumptions and failing to ask clarifying questions can lead to suboptimal outcomes, safety risks due to tool misuse, and wasted computational resources. In this work, we study the ability of LLM agents to handle ambiguous instructions in interactive code generation settings by evaluating proprietary and open-weight models on their performance across three key steps: (a) leveraging interactivity to improve performance in ambiguous scenarios, (b) detecting ambiguity, and (c) asking targeted questions. Our findings reveal that models struggle to distinguish between well-specified and underspecified instructions. However, when models interact for underspecified inputs, they effectively obtain vital information from the user, leading to significant improvements in performance and underscoring the value of effective interaction. Our study highlights critical gaps in how current state-of-the-art models handle ambiguity in complex software engineering tasks and structures the evaluation into distinct steps to enable targeted improvements."
  },
  {
    "title": "Enhancing Power Grid Inspections with Machine Learning",
    "url": "http://arxiv.org/abs/2502.13037v1",
    "arxiv_id": "2502.13037v1",
    "authors": [
      "Diogo Lavado",
      "Ricardo Santos",
      "Andre Coelho",
      "Joao Santos",
      "Alessandra Micheletti",
      "Claudia Soares"
    ],
    "published": "2025-02-18T16:49:47+00:00",
    "summary": "Ensuring the safety and reliability of power grids is critical as global energy demands continue to rise. Traditional inspection methods, such as manual observations or helicopter surveys, are resource-intensive and lack scalability. This paper explores the use of 3D computer vision to automate power grid inspections, utilizing the TS40K dataset -- a high-density, annotated collection of 3D LiDAR point clouds. By concentrating on 3D semantic segmentation, our approach addresses challenges like class imbalance and noisy data to enhance the detection of critical grid components such as power lines and towers. The benchmark results indicate significant performance improvements, with IoU scores reaching 95.53% for the detection of power lines using transformer-based models. Our findings illustrate the potential for integrating ML into grid maintenance workflows, increasing efficiency and enabling proactive risk management strategies."
  },
  {
    "title": "Fragility-aware Classification for Understanding Risk and Improving Generalization",
    "url": "http://arxiv.org/abs/2502.13024v1",
    "arxiv_id": "2502.13024v1",
    "authors": [
      "Chen Yang",
      "Zheng Cui",
      "Daniel Zhuoyu Long",
      "Jin Qi",
      "Ruohan Zhan"
    ],
    "published": "2025-02-18T16:44:03+00:00",
    "summary": "Classification models play a critical role in data-driven decision-making applications such as medical diagnosis, user profiling, recommendation systems, and default detection. Traditional performance metrics, such as accuracy, focus on overall error rates but fail to account for the confidence of incorrect predictions, thereby overlooking the risk of confident misjudgments. This risk is particularly significant in cost-sensitive and safety-critical domains like medical diagnosis and autonomous driving, where overconfident false predictions may cause severe consequences. To address this issue, we introduce the Fragility Index (FI), a novel metric that evaluates classification performance from a risk-averse perspective by explicitly capturing the tail risk of confident misjudgments. To enhance generalizability, we define FI within the robust satisficing (RS) framework, incorporating data uncertainty. We further develop a model training approach that optimizes FI while maintaining tractability for common loss functions. Specifically, we derive exact reformulations for cross-entropy loss, hinge-type loss, and Lipschitz loss, and extend the approach to deep learning models. Through synthetic experiments and real-world medical diagnosis tasks, we demonstrate that FI effectively identifies misjudgment risk and FI-based training improves model robustness and generalizability. Finally, we extend our framework to deep neural network training, further validating its effectiveness in enhancing deep learning models."
  },
  {
    "title": "Reasoning-to-Defend: Safety-Aware Reasoning Can Defend Large Language Models from Jailbreaking",
    "url": "http://arxiv.org/abs/2502.12970v1",
    "arxiv_id": "2502.12970v1",
    "authors": [
      "Junda Zhu",
      "Lingyong Yan",
      "Shuaiqiang Wang",
      "Dawei Yin",
      "Lei Sha"
    ],
    "published": "2025-02-18T15:48:46+00:00",
    "summary": "The reasoning abilities of Large Language Models (LLMs) have demonstrated remarkable advancement and exceptional performance across diverse domains. However, leveraging these reasoning capabilities to enhance LLM safety against adversarial attacks and jailbreak queries remains largely unexplored. To bridge this gap, we propose Reasoning-to-Defend (R2D), a novel training paradigm that integrates safety reflections of queries and responses into LLMs' generation process, unlocking a safety-aware reasoning mechanism. This approach enables self-evaluation at each reasoning step to create safety pivot tokens as indicators of the response's safety status. Furthermore, in order to improve the learning efficiency of pivot token prediction, we propose Contrastive Pivot Optimization(CPO), which enhances the model's ability to perceive the safety status of dialogues. Through this mechanism, LLMs dynamically adjust their response strategies during reasoning, significantly enhancing their defense capabilities against jailbreak attacks. Extensive experimental results demonstrate that R2D effectively mitigates various attacks and improves overall safety, highlighting the substantial potential of safety-aware reasoning in strengthening LLMs' robustness against jailbreaks."
  },
  {
    "title": "Trust Me, I'm Wrong: High-Certainty Hallucinations in LLMs",
    "url": "http://arxiv.org/abs/2502.12964v1",
    "arxiv_id": "2502.12964v1",
    "authors": [
      "Adi Simhi",
      "Itay Itzhak",
      "Fazl Barez",
      "Gabriel Stanovsky",
      "Yonatan Belinkov"
    ],
    "published": "2025-02-18T15:46:31+00:00",
    "summary": "Large Language Models (LLMs) often generate outputs that lack grounding in real-world facts, a phenomenon known as hallucinations. Prior research has associated hallucinations with model uncertainty, leveraging this relationship for hallucination detection and mitigation. In this paper, we challenge the underlying assumption that all hallucinations are associated with uncertainty. Using knowledge detection and uncertainty measurement methods, we demonstrate that models can hallucinate with high certainty even when they have the correct knowledge. We further show that high-certainty hallucinations are consistent across models and datasets, distinctive enough to be singled out, and challenge existing mitigation methods. Our findings reveal an overlooked aspect of hallucinations, emphasizing the need to understand their origins and improve mitigation strategies to enhance LLM safety. The code is available at https://github.com/technion-cs-nlp/Trust_me_Im_wrong ."
  },
  {
    "title": "LLMPopcorn: An Empirical Study of LLMs as Assistants for Popular Micro-video Generation",
    "url": "http://arxiv.org/abs/2502.12945v1",
    "arxiv_id": "2502.12945v1",
    "authors": [
      "Junchen Fu",
      "Xuri Ge",
      "Kaiwen Zheng",
      "Ioannis Arapakis",
      "Xin Xin",
      "Joemon M. Jose"
    ],
    "published": "2025-02-18T15:29:05+00:00",
    "summary": "Popular Micro-videos, dominant on platforms like TikTok and YouTube, hold significant commercial value. The rise of high-quality AI-generated content has spurred interest in AI-driven micro-video creation. However, despite the advanced capabilities of large language models (LLMs) like ChatGPT and DeepSeek in text generation and reasoning, their potential to assist the creation of popular micro-videos remains largely unexplored.   In this paper, we conduct an empirical study on LLM-assisted popular micro-video generation (LLMPopcorn). Specifically, we investigate the following research questions: (i) How can LLMs be effectively utilized to assist popular micro-video generation? (ii) To what extent can prompt-based enhancements optimize the LLM-generated content for higher popularity? (iii) How well do various LLMs and video generators perform in the popular micro-video generation task? By exploring these questions, we show that advanced LLMs like DeepSeek-V3 enable micro-video generation to achieve popularity comparable to human-created content. Prompt enhancements further boost popularity, and benchmarking highlights DeepSeek-V3 and DeepSeek-R1 among LLMs, while LTX-Video and HunyuanVideo lead in video generation. This pioneering work advances AI-assisted micro-video creation, uncovering new research opportunities. We will release the code and datasets to support future studies."
  },
  {
    "title": "LLMPopcorn: An Empirical Study of LLMs as Assistants for Popular Micro-video Generation",
    "url": "http://arxiv.org/abs/2502.12945v2",
    "arxiv_id": "2502.12945v2",
    "authors": [
      "Junchen Fu",
      "Xuri Ge",
      "Kaiwen Zheng",
      "Ioannis Arapakis",
      "Xin Xin",
      "Joemon M. Jose"
    ],
    "published": "2025-02-18T15:29:05+00:00",
    "summary": "Popular Micro-videos, dominant on platforms like TikTok and YouTube, hold significant commercial value. The rise of high-quality AI-generated content has spurred interest in AI-driven micro-video creation. However, despite the advanced capabilities of large language models (LLMs) like ChatGPT and DeepSeek in text generation and reasoning, their potential to assist the creation of popular micro-videos remains largely unexplored.   In this paper, we conduct an empirical study on LLM-assisted popular micro-video generation (LLMPopcorn). Specifically, we investigate the following research questions: (i) How can LLMs be effectively utilized to assist popular micro-video generation? (ii) To what extent can prompt-based enhancements optimize the LLM-generated content for higher popularity? (iii) How well do various LLMs and video generators perform in the popular micro-video generation task? By exploring these questions, we show that advanced LLMs like DeepSeek-V3 enable micro-video generation to achieve popularity comparable to human-created content. Prompt enhancements further boost popularity, and benchmarking highlights DeepSeek-V3 and DeepSeek-R1 among LLMs, while LTX-Video and HunyuanVideo lead in video generation. This pioneering work advances AI-assisted micro-video creation, uncovering new research opportunities. We will release the code and datasets to support future studies."
  },
  {
    "title": "None of the Others: a General Technique to Distinguish Reasoning from Memorization in Multiple-Choice LLM Evaluation Benchmarks",
    "url": "http://arxiv.org/abs/2502.12896v1",
    "arxiv_id": "2502.12896v1",
    "authors": [
      "Eva S\u00e1nchez Salido",
      "Julio Gonzalo",
      "Guillermo Marco"
    ],
    "published": "2025-02-18T14:32:44+00:00",
    "summary": "In LLM evaluations, reasoning is often distinguished from recall/memorization by performing numerical variations to math-oriented questions. Here we introduce a general variation method for multiple-choice questions that completely dissociates the correct answer from previously seen tokens or concepts, requiring LLMs to understand and reason (rather than memorizing) in order to answer correctly. Using this method, we evaluate state-of-the-art proprietary and open-source LLMs on two datasets available in English and Spanish: the public MMLU benchmark and the private UNED-Access 2024 dataset. Results show that all models experience remarkable accuracy drops under our proposed variation, with an average loss of 57% on MMLU and 50% on UNED-Access 2024, ranging from 10% to 93% across models. Notably, the most accurate model in our experimentation (OpenAI-o3-mini) is not the most robust (DeepSeek-R1-70B), suggesting that the best models in standard evaluations may not be the ones with better reasoning capabilities. Also, we see larger accuracy drops in public (vs private) datasets and questions posed in their original language (vs a manual translation), which are signs of contamination and also point to a relevant role of recall/memorization in current LLMs' answers."
  },
  {
    "title": "H-CoT: Hijacking the Chain-of-Thought Safety Reasoning Mechanism to Jailbreak Large Reasoning Models, Including OpenAI o1/o3, DeepSeek-R1, and Gemini 2.0 Flash Thinking",
    "url": "http://arxiv.org/abs/2502.12893v1",
    "arxiv_id": "2502.12893v1",
    "authors": [
      "Martin Kuo",
      "Jianyi Zhang",
      "Aolin Ding",
      "Qinsi Wang",
      "Louis DiValentin",
      "Yujia Bao",
      "Wei Wei",
      "Da-Cheng Juan",
      "Hai Li",
      "Yiran Chen"
    ],
    "published": "2025-02-18T14:29:12+00:00",
    "summary": "Large Reasoning Models (LRMs) have recently extended their powerful reasoning capabilities to safety checks-using chain-of-thought reasoning to decide whether a request should be answered. While this new approach offers a promising route for balancing model utility and safety, its robustness remains underexplored. To address this gap, we introduce Malicious-Educator, a benchmark that disguises extremely dangerous or malicious requests beneath seemingly legitimate educational prompts. Our experiments reveal severe security flaws in popular commercial-grade LRMs, including OpenAI o1/o3, DeepSeek-R1, and Gemini 2.0 Flash Thinking. For instance, although OpenAI's o1 model initially maintains a high refusal rate of about 98%, subsequent model updates significantly compromise its safety; and attackers can easily extract criminal strategies from DeepSeek-R1 and Gemini 2.0 Flash Thinking without any additional tricks. To further highlight these vulnerabilities, we propose Hijacking Chain-of-Thought (H-CoT), a universal and transferable attack method that leverages the model's own displayed intermediate reasoning to jailbreak its safety reasoning mechanism. Under H-CoT, refusal rates sharply decline-dropping from 98% to below 2%-and, in some instances, even transform initially cautious tones into ones that are willing to provide harmful content. We hope these findings underscore the urgent need for more robust safety mechanisms to preserve the benefits of advanced reasoning capabilities without compromising ethical standards."
  },
  {
    "title": "A Survey of Sim-to-Real Methods in RL: Progress, Prospects and Challenges with Foundation Models",
    "url": "http://arxiv.org/abs/2502.13187v1",
    "arxiv_id": "2502.13187v1",
    "authors": [
      "Longchao Da",
      "Justin Turnau",
      "Thirulogasankar Pranav Kutralingam",
      "Alvaro Velasquez",
      "Paulo Shakarian",
      "Hua Wei"
    ],
    "published": "2025-02-18T12:57:29+00:00",
    "summary": "Deep Reinforcement Learning (RL) has been explored and verified to be effective in solving decision-making tasks in various domains, such as robotics, transportation, recommender systems, etc. It learns from the interaction with environments and updates the policy using the collected experience. However, due to the limited real-world data and unbearable consequences of taking detrimental actions, the learning of RL policy is mainly restricted within the simulators. This practice guarantees safety in learning but introduces an inevitable sim-to-real gap in terms of deployment, thus causing degraded performance and risks in execution. There are attempts to solve the sim-to-real problems from different domains with various techniques, especially in the era with emerging techniques such as large foundations or language models that have cast light on the sim-to-real. This survey paper, to the best of our knowledge, is the first taxonomy that formally frames the sim-to-real techniques from key elements of the Markov Decision Process (State, Action, Transition, and Reward). Based on the framework, we cover comprehensive literature from the classic to the most advanced methods including the sim-to-real techniques empowered by foundation models, and we also discuss the specialties that are worth attention in different domains of sim-to-real problems. Then we summarize the formal evaluation process of sim-to-real performance with accessible code or benchmarks. The challenges and opportunities are also presented to encourage future exploration of this direction. We are actively maintaining a to include the most up-to-date sim-to-real research outcomes to help the researchers in their work."
  },
  {
    "title": "Reasoning and the Trusting Behavior of DeepSeek and GPT: An Experiment Revealing Hidden Fault Lines in Large Language Models",
    "url": "http://arxiv.org/abs/2502.12825v1",
    "arxiv_id": "2502.12825v1",
    "authors": [
      "Rubing Lu",
      "Jo\u00e3o Sedoc",
      "Arun Sundararajan"
    ],
    "published": "2025-02-18T12:46:18+00:00",
    "summary": "When encountering increasingly frequent performance improvements or cost reductions from a new large language model (LLM), developers of applications leveraging LLMs must decide whether to take advantage of these improvements or stay with older tried-and-tested models. Low perceived switching frictions can lead to choices that do not consider more subtle behavior changes that the transition may induce. Our experiments use a popular game-theoretic behavioral economics model of trust to show stark differences in the trusting behavior of OpenAI's and DeepSeek's models. We highlight a collapse in the economic trust behavior of the o1-mini and o3-mini models as they reconcile profit-maximizing and risk-seeking with future returns from trust, and contrast it with DeepSeek's more sophisticated and profitable trusting behavior that stems from an ability to incorporate deeper concepts like forward planning and theory-of-mind. As LLMs form the basis for high-stakes commercial systems, our results highlight the perils of relying on LLM performance benchmarks that are too narrowly defined and suggest that careful analysis of their hidden fault lines should be part of any organization's AI strategy."
  },
  {
    "title": "Reasoning and the Trusting Behavior of DeepSeek and GPT: An Experiment Revealing Hidden Fault Lines in Large Language Models",
    "url": "http://arxiv.org/abs/2502.12825v2",
    "arxiv_id": "2502.12825v2",
    "authors": [
      "Rubing Li",
      "Jo\u00e3o Sedoc",
      "Arun Sundararajan"
    ],
    "published": "2025-02-18T12:46:18+00:00",
    "summary": "When encountering increasingly frequent performance improvements or cost reductions from a new large language model (LLM), developers of applications leveraging LLMs must decide whether to take advantage of these improvements or stay with older tried-and-tested models. Low perceived switching frictions can lead to choices that do not consider more subtle behavior changes that the transition may induce. Our experiments use a popular game-theoretic behavioral economics model of trust to show stark differences in the trusting behavior of OpenAI's and DeepSeek's models. We highlight a collapse in the economic trust behavior of the o1-mini and o3-mini models as they reconcile profit-maximizing and risk-seeking with future returns from trust, and contrast it with DeepSeek's more sophisticated and profitable trusting behavior that stems from an ability to incorporate deeper concepts like forward planning and theory-of-mind. As LLMs form the basis for high-stakes commercial systems, our results highlight the perils of relying on LLM performance benchmarks that are too narrowly defined and suggest that careful analysis of their hidden fault lines should be part of any organization's AI strategy."
  },
  {
    "title": "Simulating User Diversity in Task-Oriented Dialogue Systems using Large Language Models",
    "url": "http://arxiv.org/abs/2502.12813v1",
    "arxiv_id": "2502.12813v1",
    "authors": [
      "Adnan Ahmad",
      "Stefan Hillmann",
      "Sebastian M\u00f6ller"
    ],
    "published": "2025-02-18T12:20:16+00:00",
    "summary": "In this study, we explore the application of Large Language Models (LLMs) for generating synthetic users and simulating user conversations with a task-oriented dialogue system and present detailed results and their analysis. We propose a comprehensive novel approach to user simulation technique that uses LLMs to create diverse user profiles, set goals, engage in multi-turn dialogues, and evaluate the conversation success. We employ two proprietary LLMs, namely GPT-4o and GPT-o1 (Achiam et al., 2023), to generate a heterogeneous base of user profiles, characterized by varied demographics, multiple user goals, different conversational styles, initial knowledge levels, interests, and conversational objectives. We perform a detailed analysis of the user profiles generated by LLMs to assess the diversity, consistency, and potential biases inherent in these LLM-generated user simulations. We find that GPT-o1 generates more heterogeneous user distribution across most user attributes, while GPT-4o generates more skewed user attributes. The generated set of user profiles are then utilized to simulate dialogue sessions by interacting with a task-oriented dialogue system."
  },
  {
    "title": "Expanding the Classical V-Model for the Development of Complex Systems Incorporating AI",
    "url": "http://arxiv.org/abs/2502.13184v1",
    "arxiv_id": "2502.13184v1",
    "authors": [
      "Lars Ullrich",
      "Michael Buchholz",
      "Klaus Dietmayer",
      "Knut Graichen"
    ],
    "published": "2025-02-18T11:01:37+00:00",
    "summary": "Research in the field of automated vehicles, or more generally cognitive cyber-physical systems that operate in the real world, is leading to increasingly complex systems. Among other things, artificial intelligence enables an ever-increasing degree of autonomy. In this context, the V-model, which has served for decades as a process reference model of the system development lifecycle is reaching its limits. To the contrary, innovative processes and frameworks have been developed that take into account the characteristics of emerging autonomous systems. To bridge the gap and merge the different methodologies, we present an extension of the V-model for iterative data-based development processes that harmonizes and formalizes the existing methods towards a generic framework. The iterative approach allows for seamless integration of continuous system refinement. While the data-based approach constitutes the consideration of data-based development processes and formalizes the use of synthetic and real world data. In this way, formalizing the process of development, verification, validation, and continuous integration contributes to ensuring the safety of emerging complex systems that incorporate AI."
  },
  {
    "title": "IPSR Model: Misinformation Intervention through Prebunking in Social Networks",
    "url": "http://arxiv.org/abs/2502.12740v1",
    "arxiv_id": "2502.12740v1",
    "authors": [
      "Robert Rai",
      "Rajesh Sharma",
      "Chandrakala Meena"
    ],
    "published": "2025-02-18T10:56:30+00:00",
    "summary": "In the present digital world, the rapid spread of misinformation is not just an annoyance but a real threat to public safety, and our collective decision-making. Prebunking, a type of psychological immunization, can educate people about misinformation and lay a foundation of cognitive resilience that makes them more robust against future misinformation. We use a compartmental modeling approach inspired by vaccination models from epidemiology to model the effectiveness of prebunking misinformation. Populations are classified into different compartments based on the exposure to prebunking and the propagation of misinformation through online social networks. Specific rates dictate the transitions between such states, similar to how people traverse between susceptible, infected, and recovered compartments in classical epidemiological models. This model integrates different levels of prebunking potency, the fraction of the population prebunked initially, and the forgetting rate effects. To the best of our knowledge this is the first work which study the extent of prebunking interventions to reduce the scale of misinformation, much as vaccinations curtail the spread of infectious diseases."
  },
  {
    "title": "myEye2Wheeler: A Two-Wheeler Indian Driver Real-World Eye-Tracking Dataset",
    "url": "http://arxiv.org/abs/2502.12723v1",
    "arxiv_id": "2502.12723v1",
    "authors": [
      "Bhaiya Vaibhaw Kumar",
      "Deepti Rawat",
      "Tanvi Kandalla",
      "Aarnav Nagariya",
      "Kavita Vemuri"
    ],
    "published": "2025-02-18T10:39:00+00:00",
    "summary": "This paper presents the myEye2Wheeler dataset, a unique resource of real-world gaze behaviour of two-wheeler drivers navigating complex Indian traffic. Most datasets are from four-wheeler drivers on well-planned roads and homogeneous traffic. Our dataset offers a critical lens into the unique visual attention patterns and insights into the decision-making of Indian two-wheeler drivers. The analysis demonstrates that existing saliency models, like TASED-Net, perform less effectively on the myEye-2Wheeler dataset compared to when applied on the European 4-wheeler eye tracking datasets (DR(Eye)VE), highlighting the need for models specifically tailored to the traffic conditions. By introducing the dataset, we not only fill a significant gap in two-wheeler driver behaviour research in India but also emphasise the critical need for developing context-specific saliency models. The larger aim is to improve road safety for two-wheeler users and lane-planning to support a cost-effective mode of transport."
  },
  {
    "title": "SATA: Safe and Adaptive Torque-Based Locomotion Policies Inspired by Animal Learning",
    "url": "http://arxiv.org/abs/2502.12674v1",
    "arxiv_id": "2502.12674v1",
    "authors": [
      "Peizhuo Li",
      "Hongyi Li",
      "Ge Sun",
      "Jin Cheng",
      "Xinrong Yang",
      "Guillaume Bellegarda",
      "Milad Shafiee",
      "Yuhong Cao",
      "Auke Ijspeert",
      "Guillaume Sartoretti"
    ],
    "published": "2025-02-18T09:25:37+00:00",
    "summary": "Despite recent advances in learning-based controllers for legged robots, deployments in human-centric environments remain limited by safety concerns. Most of these approaches use position-based control, where policies output target joint angles that must be processed by a low-level controller (e.g., PD or impedance controllers) to compute joint torques. Although impressive results have been achieved in controlled real-world scenarios, these methods often struggle with compliance and adaptability when encountering environments or disturbances unseen during training, potentially resulting in extreme or unsafe behaviors. Inspired by how animals achieve smooth and adaptive movements by controlling muscle extension and contraction, torque-based policies offer a promising alternative by enabling precise and direct control of the actuators in torque space. In principle, this approach facilitates more effective interactions with the environment, resulting in safer and more adaptable behaviors. However, challenges such as a highly nonlinear state space and inefficient exploration during training have hindered their broader adoption. To address these limitations, we propose SATA, a bio-inspired framework that mimics key biomechanical principles and adaptive learning mechanisms observed in animal locomotion. Our approach effectively addresses the inherent challenges of learning torque-based policies by significantly improving early-stage exploration, leading to high-performance final policies. Remarkably, our method achieves zero-shot sim-to-real transfer. Our experimental results indicate that SATA demonstrates remarkable compliance and safety, even in challenging environments such as soft/slippery terrain or narrow passages, and under significant external disturbances, highlighting its potential for practical deployments in human-centric and safety-critical scenarios."
  },
  {
    "title": "The Hidden Risks of Large Reasoning Models: A Safety Assessment of R1",
    "url": "http://arxiv.org/abs/2502.12659v1",
    "arxiv_id": "2502.12659v1",
    "authors": [
      "Kaiwen Zhou",
      "Chengzhi Liu",
      "Xuandong Zhao",
      "Shreedhar Jangam",
      "Jayanth Srinivasa",
      "Gaowen Liu",
      "Dawn Song",
      "Xin Eric Wang"
    ],
    "published": "2025-02-18T09:06:07+00:00",
    "summary": "The rapid development of large reasoning models, such as OpenAI-o3 and DeepSeek-R1, has led to significant improvements in complex reasoning over non-reasoning large language models~(LLMs). However, their enhanced capabilities, combined with the open-source access of models like DeepSeek-R1, raise serious safety concerns, particularly regarding their potential for misuse. In this work, we present a comprehensive safety assessment of these reasoning models, leveraging established safety benchmarks to evaluate their compliance with safety regulations. Furthermore, we investigate their susceptibility to adversarial attacks, such as jailbreaking and prompt injection, to assess their robustness in real-world applications. Through our multi-faceted analysis, we uncover four key findings: (1) There is a significant safety gap between the open-source R1 models and the o3-mini model, on both safety benchmark and attack, suggesting more safety effort on R1 is needed. (2) The distilled reasoning model shows poorer safety performance compared to its safety-aligned base models. (3) The stronger the model's reasoning ability, the greater the potential harm it may cause when answering unsafe questions. (4) The thinking process in R1 models pose greater safety concerns than their final answers. Our study provides insights into the security implications of reasoning models and highlights the need for further advancements in R1 models' safety to close the gap."
  },
  {
    "title": "A Graph-Enhanced Deep-Reinforcement Learning Framework for the Aircraft Landing Problem",
    "url": "http://arxiv.org/abs/2502.12617v1",
    "arxiv_id": "2502.12617v1",
    "authors": [
      "Vatsal Maru"
    ],
    "published": "2025-02-18T08:02:17+00:00",
    "summary": "The Aircraft Landing Problem (ALP) is one of the challenging problems in aircraft transportation and management. The challenge is to schedule the arriving aircraft in a sequence so that the cost and delays are optimized. There are various solution approaches to solving this problem, most of which are based on operations research algorithms and meta-heuristics. Although traditional methods perform better on one or the other factors, there remains a problem of solving real-time rescheduling and computational scalability altogether. This paper presents a novel deep reinforcement learning (DRL) framework that combines graph neural networks with actor-critic architectures to address the ALP. This paper introduces three key contributions: A graph-based state representation that efficiently captures temporal and spatial relationships between aircraft, a specialized actor-critic architecture designed to handle multiple competing objectives in landing scheduling, and a runway balance strategy that ensures efficient resource utilization while maintaining safety constraints. The results show that the trained algorithm can be tested on different problem sets and the results are competitive to operation research algorithms. The experimental results on standard benchmark data sets demonstrate a 99.95 reduction in computational time compared to Mixed Integer Programming (MIP) and 38 higher runway throughput over First Come First Serve (FCFS) approaches. Therefore, the proposed solution is competitive to traditional approaches and achieves substantial advancements. Notably, it does not require retraining, making it particularly suitable for industrial deployment. The frameworks capability to generate solutions within 1 second enables real-time rescheduling, addressing critical requirements of air traffic management."
  },
  {
    "title": "Learning-based Dynamic Robot-to-Human Handover",
    "url": "http://arxiv.org/abs/2502.12602v1",
    "arxiv_id": "2502.12602v1",
    "authors": [
      "Hyeonseong Kim",
      "Chanwoo Kim",
      "Matthew Pan",
      "Kyungjae Lee",
      "Sungjoon Choi"
    ],
    "published": "2025-02-18T07:26:07+00:00",
    "summary": "This paper presents a novel learning-based approach to dynamic robot-to-human handover, addressing the challenges of delivering objects to a moving receiver. We hypothesize that dynamic handover, where the robot adjusts to the receiver's movements, results in more efficient and comfortable interaction compared to static handover, where the receiver is assumed to be stationary. To validate this, we developed a nonparametric method for generating continuous handover motion, conditioned on the receiver's movements, and trained the model using a dataset of 1,000 human-to-human handover demonstrations. We integrated preference learning for improved handover effectiveness and applied impedance control to ensure user safety and adaptiveness. The approach was evaluated in both simulation and real-world settings, with user studies demonstrating that dynamic handover significantly reduces handover time and improves user comfort compared to static methods. Videos and demonstrations of our approach are available at https://zerotohero7886.github.io/dyn-r2h-handover ."
  },
  {
    "title": "DemonAgent: Dynamically Encrypted Multi-Backdoor Implantation Attack on LLM-based Agent",
    "url": "http://arxiv.org/abs/2502.12575v1",
    "arxiv_id": "2502.12575v1",
    "authors": [
      "Pengyu Zhu",
      "Zhenhong Zhou",
      "Yuanhe Zhang",
      "Shilinlu Yan",
      "Kun Wang",
      "Sen Su"
    ],
    "published": "2025-02-18T06:26:15+00:00",
    "summary": "As LLM-based agents become increasingly prevalent, backdoors can be implanted into agents through user queries or environment feedback, raising critical concerns regarding safety vulnerabilities. However, backdoor attacks are typically detectable by safety audits that analyze the reasoning process of agents. To this end, we propose a novel backdoor implantation strategy called \\textbf{Dynamically Encrypted Multi-Backdoor Implantation Attack}. Specifically, we introduce dynamic encryption, which maps the backdoor into benign content, effectively circumventing safety audits. To enhance stealthiness, we further decompose the backdoor into multiple sub-backdoor fragments. Based on these advancements, backdoors are allowed to bypass safety audits significantly. Additionally, we present AgentBackdoorEval, a dataset designed for the comprehensive evaluation of agent backdoor attacks. Experimental results across multiple datasets demonstrate that our method achieves an attack success rate nearing 100\\% while maintaining a detection rate of 0\\%, illustrating its effectiveness in evading safety audits. Our findings highlight the limitations of existing safety mechanisms in detecting advanced attacks, underscoring the urgent need for more robust defenses against backdoor threats. Code and data are available at https://github.com/whfeLingYu/DemonAgent."
  },
  {
    "title": "Exploring the Impact of Personality Traits on LLM Bias and Toxicity",
    "url": "http://arxiv.org/abs/2502.12566v1",
    "arxiv_id": "2502.12566v1",
    "authors": [
      "Shuo Wang",
      "Renhao Li",
      "Xi Chen",
      "Yulin Yuan",
      "Derek F. Wong",
      "Min Yang"
    ],
    "published": "2025-02-18T06:07:09+00:00",
    "summary": "With the different roles that AI is expected to play in human life, imbuing large language models (LLMs) with different personalities has attracted increasing research interests. While the \"personification\" enhances human experiences of interactivity and adaptability of LLMs, it gives rise to critical concerns about content safety, particularly regarding bias, sentiment and toxicity of LLM generation. This study explores how assigning different personality traits to LLMs affects the toxicity and biases of their outputs. Leveraging the widely accepted HEXACO personality framework developed in social psychology, we design experimentally sound prompts to test three LLMs' performance on three toxic and bias benchmarks. The findings demonstrate the sensitivity of all three models to HEXACO personality traits and, more importantly, a consistent variation in the biases, negative sentiment and toxicity of their output. In particular, adjusting the levels of several personality traits can effectively reduce bias and toxicity in model performance, similar to humans' correlations between personality traits and toxic behaviors. The findings highlight the additional need to examine content safety besides the efficiency of training or fine-tuning methods for LLM personification. They also suggest a potential for the adjustment of personalities to be a simple and low-cost method to conduct controlled text generation."
  },
  {
    "title": "Self Iterative Label Refinement via Robust Unlabeled Learning",
    "url": "http://arxiv.org/abs/2502.12565v1",
    "arxiv_id": "2502.12565v1",
    "authors": [
      "Hikaru Asano",
      "Tadashi Kozuno",
      "Yukino Baba"
    ],
    "published": "2025-02-18T06:04:18+00:00",
    "summary": "Recent advances in large language models (LLMs) have yielded impressive performance on various tasks, yet they often depend on high-quality feedback that can be costly. Self-refinement methods attempt to leverage LLMs' internal evaluation mechanisms with minimal human supervision; however, these approaches frequently suffer from inherent biases and overconfidence, especially in domains where the models lack sufficient internal knowledge, resulting in performance degradation. As an initial step toward enhancing self-refinement for broader applications, we introduce an iterative refinement pipeline that employs the Unlabeled-Unlabeled learning framework to improve LLM-generated pseudo-labels for classification tasks. By exploiting two unlabeled datasets with differing positive class ratios, our approach iteratively denoises and refines the initial pseudo-labels, thereby mitigating the adverse effects of internal biases with minimal human supervision. Evaluations on diverse datasets, including low-resource language corpora, patent classifications, and protein structure categorizations, demonstrate that our method consistently outperforms both initial LLM's classification performance and the self-refinement approaches by cutting-edge models (e.g., GPT-4o and DeepSeek-R1)."
  },
  {
    "title": "SEA: Low-Resource Safety Alignment for Multimodal Large Language Models via Synthetic Embeddings",
    "url": "http://arxiv.org/abs/2502.12562v1",
    "arxiv_id": "2502.12562v1",
    "authors": [
      "Weikai Lu",
      "Hao Peng",
      "Huiping Zhuang",
      "Cen Chen",
      "Ziqian Zeng"
    ],
    "published": "2025-02-18T05:57:35+00:00",
    "summary": "Multimodal Large Language Models (MLLMs) have serious security vulnerabilities.While safety alignment using multimodal datasets consisting of text and data of additional modalities can effectively enhance MLLM's security, it is costly to construct these datasets. Existing low-resource security alignment methods, including textual alignment, have been found to struggle with the security risks posed by additional modalities. To address this, we propose Synthetic Embedding augmented safety Alignment (SEA), which optimizes embeddings of additional modality through gradient updates to expand textual datasets. This enables multimodal safety alignment training even when only textual data is available. Extensive experiments on image, video, and audio-based MLLMs demonstrate that SEA can synthesize a high-quality embedding on a single RTX3090 GPU within 24 seconds. SEA significantly improves the security of MLLMs when faced with threats from additional modalities. To assess the security risks introduced by video and audio, we also introduced a new benchmark called VA-SafetyBench. High attack success rates across multiple MLLMs validate its challenge. Our code and data will be available at https://github.com/ZeroNLP/SEA."
  },
  {
    "title": "From Maneuver to Mishap: A Systematic Literature Review on U-Turn Safety Risks",
    "url": "http://arxiv.org/abs/2502.12556v1",
    "arxiv_id": "2502.12556v1",
    "authors": [
      "Syed Aaqib Javed",
      "Anannya Ghosh Tusti",
      "Biplov Pandeym Subasish Das"
    ],
    "published": "2025-02-18T05:44:19+00:00",
    "summary": "Understanding the impacts of U-turn configurations on intersection safety and traffic operations is essential for developing effective strategies to enhance road safety and efficiency. Extensive research has been conducted to investigate the role of geometric designs, driver behavior, and advanced technologies in mitigating crash risks and improving traffic flow at U-turn facilities. By synthesizing this collective body of work through the guidelines of Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA), this paper provides a valuable resource for transportation professionals, policymakers, and researchers seeking evidence-based solutions. This systematic review draws on studies from diverse traffic environments and regional contexts, focusing on innovative design interventions, such as restricted crossing U-turns (RCUTs) and median U-turn intersections (MUTs), as well as integrated strategies leveraging technological advancements. By presenting a comprehensive analysis of U-turn-related challenges and opportunities, this review contributes to advancing transportation safety research and guiding the development of adaptive strategies tailored to varied traffic conditions and evolving technologies."
  },
  {
    "title": "From Maneuver to Mishap: A Systematic Literature Review on U-Turn Safety Risks",
    "url": "http://arxiv.org/abs/2502.12556v2",
    "arxiv_id": "2502.12556v2",
    "authors": [
      "Syed Aaqib Javed",
      "Anannya Ghosh Tusti",
      "Biplov Pandey",
      "Subasish Das"
    ],
    "published": "2025-02-18T05:44:19+00:00",
    "summary": "Understanding the impacts of U-turn configurations on intersection safety and traffic operations is essential for developing effective strategies to enhance road safety and efficiency. Extensive research has been conducted to investigate the role of geometric designs, driver behavior, and advanced technologies in mitigating crash risks and improving traffic flow at U-turn facilities. By synthesizing this collective body of work through the guidelines of Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA), this paper provides a valuable resource for transportation professionals, policymakers, and researchers seeking evidence-based solutions. This systematic review draws on studies from diverse traffic environments and regional contexts, focusing on innovative design interventions, such as restricted crossing U-turns (RCUTs) and median U-turn intersections (MUTs), as well as integrated strategies leveraging technological advancements. By presenting a comprehensive analysis of U-turn-related challenges and opportunities, this review contributes to advancing transportation safety research and guiding the development of adaptive strategies tailored to varied traffic conditions and evolving technologies."
  },
  {
    "title": "LLM Safety for Children",
    "url": "http://arxiv.org/abs/2502.12552v1",
    "arxiv_id": "2502.12552v1",
    "authors": [
      "Prasanjit Rath",
      "Hari Shrawgi",
      "Parag Agrawal",
      "Sandipan Dandapat"
    ],
    "published": "2025-02-18T05:26:27+00:00",
    "summary": "This paper analyzes the safety of Large Language Models (LLMs) in interactions with children below age of 18 years. Despite the transformative applications of LLMs in various aspects of children's lives such as education and therapy, there remains a significant gap in understanding and mitigating potential content harms specific to this demographic. The study acknowledges the diverse nature of children often overlooked by standard safety evaluations and proposes a comprehensive approach to evaluating LLM safety specifically for children. We list down potential risks that children may encounter when using LLM powered applications. Additionally we develop Child User Models that reflect the varied personalities and interests of children informed by literature in child care and psychology. These user models aim to bridge the existing gap in child safety literature across various fields. We utilize Child User Models to evaluate the safety of six state of the art LLMs. Our observations reveal significant safety gaps in LLMs particularly in categories harmful to children but not adults"
  },
  {
    "title": "Inference-Time Computations for LLM Reasoning and Planning: A Benchmark and Insights",
    "url": "http://arxiv.org/abs/2502.12521v1",
    "arxiv_id": "2502.12521v1",
    "authors": [
      "Shubham Parashar",
      "Blake Olson",
      "Sambhav Khurana",
      "Eric Li",
      "Hongyi Ling",
      "James Caverlee",
      "Shuiwang Ji"
    ],
    "published": "2025-02-18T04:11:29+00:00",
    "summary": "We examine the reasoning and planning capabilities of large language models (LLMs) in solving complex tasks. Recent advances in inference-time techniques demonstrate the potential to enhance LLM reasoning without additional training by exploring intermediate steps during inference. Notably, OpenAI's o1 model shows promising performance through its novel use of multi-step reasoning and verification. Here, we explore how scaling inference-time techniques can improve reasoning and planning, focusing on understanding the tradeoff between computational cost and performance. To this end, we construct a comprehensive benchmark, known as Sys2Bench, and perform extensive experiments evaluating existing inference-time techniques on eleven diverse tasks across five categories, including arithmetic reasoning, logical reasoning, common sense reasoning, algorithmic reasoning, and planning. Our findings indicate that simply scaling inference-time computation has limitations, as no single inference-time technique consistently performs well across all reasoning and planning tasks."
  },
  {
    "title": "SAFEERASER: Enhancing Safety in Multimodal Large Language Models through Multimodal Machine Unlearning",
    "url": "http://arxiv.org/abs/2502.12520v1",
    "arxiv_id": "2502.12520v1",
    "authors": [
      "Junkai Chen",
      "Zhijie Deng",
      "Kening Zheng",
      "Yibo Yan",
      "Shuliang Liu",
      "PeiJun Wu",
      "Peijie Jiang",
      "Jia Liu",
      "Xuming Hu"
    ],
    "published": "2025-02-18T04:09:46+00:00",
    "summary": "As Multimodal Large Language Models (MLLMs) develop, their potential security issues have become increasingly prominent. Machine Unlearning (MU), as an effective strategy for forgetting specific knowledge in training data, has been widely used in privacy protection. However, MU for safety in MLLM has yet to be fully explored. To address this issue, we propose SAFEERASER, a safety unlearning benchmark for MLLMs, consisting of 3,000 images and 28.8K VQA pairs. We comprehensively evaluate unlearning methods from two perspectives: forget quality and model utility. Our findings show that existing MU methods struggle to maintain model performance while implementing the forget operation and often suffer from over-forgetting. Hence, we introduce Prompt Decouple (PD) Loss to alleviate over-forgetting through decouple prompt during unlearning process. To quantitatively measure over-forgetting mitigated by PD Loss, we propose a new metric called Safe Answer Refusal Rate (SARR). Experimental results demonstrate that combining PD Loss with existing unlearning methods can effectively prevent over-forgetting and achieve a decrease of 79.5% in the SARR metric of LLaVA-7B and LLaVA-13B, while maintaining forget quality and model utility. Our code and dataset will be released upon acceptance. Warning: This paper contains examples of harmful language and images, and reader discretion is recommended."
  },
  {
    "title": "Local Flaw Detection with Adaptive Pyramid Image Fusion Across Spatial Sampling Resolution for SWRs",
    "url": "http://arxiv.org/abs/2502.12512v1",
    "arxiv_id": "2502.12512v1",
    "authors": [
      "Siyu You",
      "Huayi Gou",
      "Leilei Yang",
      "Zhiliang Liu",
      "Mingjian Zuo"
    ],
    "published": "2025-02-18T03:55:29+00:00",
    "summary": "The inspection of local flaws (LFs) in Steel Wire Ropes (SWRs) is crucial for ensuring safety and reliability in various industries. Magnetic Flux Leakage (MFL) imaging is commonly used for non-destructive testing, but its effectiveness is often hindered by the combined effects of inspection speed and sampling rate. To address this issue, the impacts of inspection speed and sampling rate on image quality are studied, as variations in these factors can cause stripe noise, axial compression of defect features, and increased interference, complicating accurate detection. We define the relationship between inspection speed and sampling rate as spatial sampling resolution (SSR) and propose an adaptive SSR target-feature-oriented (AS-TFO) method. This method incorporates adaptive adjustment and pyramid image fusion techniques to enhance defect detection under different SSR scenarios. Experimental results show that under high SSR scenarios, the method achieves a precision of 92.54% and recall of 98.41%. It remains robust under low SSR scenarios with a precision of 94.87% and recall of 97.37%. The overall results show that the proposed method outperforms conventional approaches, achieving state-of-the-art performance. This improvement in detection accuracy and robustness is particularly valuable for handling complex inspection conditions, where inspection speed and sampling rate can vary significantly, making detection more robust and reliable in industrial settings."
  },
  {
    "title": "Towards Robust and Secure Embodied AI: A Survey on Vulnerabilities and Attacks",
    "url": "http://arxiv.org/abs/2502.13175v1",
    "arxiv_id": "2502.13175v1",
    "authors": [
      "Wenpeng Xing",
      "Minghao Li",
      "Mohan Li",
      "Meng Han"
    ],
    "published": "2025-02-18T03:38:07+00:00",
    "summary": "Embodied AI systems, including robots and autonomous vehicles, are increasingly integrated into real-world applications, where they encounter a range of vulnerabilities stemming from both environmental and system-level factors. These vulnerabilities manifest through sensor spoofing, adversarial attacks, and failures in task and motion planning, posing significant challenges to robustness and safety. Despite the growing body of research, existing reviews rarely focus specifically on the unique safety and security challenges of embodied AI systems. Most prior work either addresses general AI vulnerabilities or focuses on isolated aspects, lacking a dedicated and unified framework tailored to embodied AI. This survey fills this critical gap by: (1) categorizing vulnerabilities specific to embodied AI into exogenous (e.g., physical attacks, cybersecurity threats) and endogenous (e.g., sensor failures, software flaws) origins; (2) systematically analyzing adversarial attack paradigms unique to embodied AI, with a focus on their impact on perception, decision-making, and embodied interaction; (3) investigating attack vectors targeting large vision-language models (LVLMs) and large language models (LLMs) within embodied systems, such as jailbreak attacks and instruction misinterpretation; (4) evaluating robustness challenges in algorithms for embodied perception, decision-making, and task planning; and (5) proposing targeted strategies to enhance the safety and reliability of embodied AI systems. By integrating these dimensions, we provide a comprehensive framework for understanding the interplay between vulnerabilities and safety in embodied AI."
  },
  {
    "title": "SoK: Understanding Vulnerabilities in the Large Language Model Supply Chain",
    "url": "http://arxiv.org/abs/2502.12497v1",
    "arxiv_id": "2502.12497v1",
    "authors": [
      "Shenao Wang",
      "Yanjie Zhao",
      "Zhao Liu",
      "Quanchen Zou",
      "Haoyu Wang"
    ],
    "published": "2025-02-18T03:22:38+00:00",
    "summary": "Large Language Models (LLMs) transform artificial intelligence, driving advancements in natural language understanding, text generation, and autonomous systems. The increasing complexity of their development and deployment introduces significant security challenges, particularly within the LLM supply chain. However, existing research primarily focuses on content safety, such as adversarial attacks, jailbreaking, and backdoor attacks, while overlooking security vulnerabilities in the underlying software systems. To address this gap, this study systematically analyzes 529 vulnerabilities reported across 75 prominent projects spanning 13 lifecycle stages. The findings show that vulnerabilities are concentrated in the application (50.3%) and model (42.7%) layers, with improper resource control (45.7%) and improper neutralization (25.1%) identified as the leading root causes. Additionally, while 56.7% of the vulnerabilities have available fixes, 8% of these patches are ineffective, resulting in recurring vulnerabilities. This study underscores the challenges of securing the LLM ecosystem and provides actionable insights to guide future research and mitigation strategies."
  },
  {
    "title": "Safe at the Margins: A General Approach to Safety Alignment in Low-Resource English Languages -- A Singlish Case Study",
    "url": "http://arxiv.org/abs/2502.12485v1",
    "arxiv_id": "2502.12485v1",
    "authors": [
      "Isaac Lim",
      "Shaun Khoo",
      "Watson Chua",
      "Goh Jiayi",
      "Jessica Foo"
    ],
    "published": "2025-02-18T03:11:06+00:00",
    "summary": "To ensure safe usage, Large Language Models (LLMs) typically undergo alignment with human-defined values. However, this alignment often relies on primarily English data and is biased towards Western-centric values, limiting its effectiveness in low-resource language settings. In this paper, we describe our approach for aligning SEA-Lion-v2.1-Instruct (a Llama3-8B variant) to minimize toxicity in Singlish, an English creole specific to Singapore. We find that supervised fine-tuning and Kahneman-Tversky Optimization (KTO) on paired and unpaired preferences is more sample efficient and yields significantly better results than Direct Preference Optimization (DPO). Our analysis reveals that DPO implicitly enforces a weaker safety objective than KTO, and that SFT complements KTO by improving training stability. Finally, we introduce a simple but novel modification to KTO, KTO-S, which improves training stability through better gradient exploitation. Overall, we present a general approach for safety alignment conducive to low-resource English languages, successfully reducing toxicity by 99\\% on our Singlish benchmark, with gains generalizing to the broader TOXIGEN dataset while maintaining strong performance across standard LLM benchmarks."
  },
  {
    "title": "MCTS-Judge: Test-Time Scaling in LLM-as-a-Judge for Code Correctness Evaluation",
    "url": "http://arxiv.org/abs/2502.12468v1",
    "arxiv_id": "2502.12468v1",
    "authors": [
      "Yutong Wang",
      "Pengliang Ji",
      "Chaoqun Yang",
      "Kaixin Li",
      "Ming Hu",
      "Jiaoyang Li",
      "Guillaume Sartoretti"
    ],
    "published": "2025-02-18T02:55:48+00:00",
    "summary": "The LLM-as-a-Judge paradigm shows promise for evaluating generative content but lacks reliability in reasoning-intensive scenarios, such as programming. Inspired by recent advances in reasoning models and shifts in scaling laws, we pioneer bringing test-time computation into LLM-as-a-Judge, proposing MCTS-Judge, a resource-efficient, System-2 thinking framework for code correctness evaluation. MCTS-Judge leverages Monte Carlo Tree Search (MCTS) to decompose problems into simpler, multi-perspective evaluations. Through a node-selection strategy that combines self-assessment based on historical actions in the current trajectory and the Upper Confidence Bound for Trees based on prior rollouts, MCTS-Judge balances global optimization and refinement of the current trajectory. We further designed a high-precision, unit-test-level reward mechanism to encourage the Large Language Model (LLM) to perform line-by-line analysis. Extensive experiments on three benchmarks and five LLMs demonstrate the effectiveness of MCTS-Judge, which improves the base model's accuracy from 41% to 80%, surpassing the o1-series models with 3x fewer tokens. Further evaluations validate the superiority of its reasoning trajectory in logic, analytics, thoroughness, and overall quality, while revealing the test-time scaling law of the LLM-as-a-Judge paradigm."
  },
  {
    "title": "EquiBench: Benchmarking Code Reasoning Capabilities of Large Language Models via Equivalence Checking",
    "url": "http://arxiv.org/abs/2502.12466v1",
    "arxiv_id": "2502.12466v1",
    "authors": [
      "Anjiang Wei",
      "Jiannan Cao",
      "Ran Li",
      "Hongyu Chen",
      "Yuhui Zhang",
      "Ziheng Wang",
      "Yaofeng Sun",
      "Yuan Liu",
      "Thiago S. F. X. Teixeira",
      "Diyi Yang",
      "Ke Wang",
      "Alex Aiken"
    ],
    "published": "2025-02-18T02:54:25+00:00",
    "summary": "Equivalence checking, i.e., determining whether two programs produce identical outputs for all possible inputs, underpins a broad range of applications, including software refactoring, testing, and optimization. We present the task of equivalence checking as a new way to evaluate the code reasoning abilities of large language models (LLMs). We introduce EquiBench, a dataset of 2400 program pairs spanning four programming languages and six equivalence categories. These pairs are systematically generated through program analysis, compiler scheduling, and superoptimization, covering nontrivial structural transformations that demand deep semantic reasoning beyond simple syntactic variations. Our evaluation of 17 state-of-the-art LLMs shows that OpenAI o3-mini achieves the highest overall accuracy of 78.0%. In the most challenging categories, the best accuracies are 62.3% and 68.8%, only modestly above the 50% random baseline for binary classification, indicating significant room for improvement in current models' code reasoning capabilities."
  },
  {
    "title": "SafeRoute: Adaptive Model Selection for Efficient and Accurate Safety Guardrails in Large Language Models",
    "url": "http://arxiv.org/abs/2502.12464v1",
    "arxiv_id": "2502.12464v1",
    "authors": [
      "Seanie Lee",
      "Dong Bok Lee",
      "Dominik Wagner",
      "Minki Kang",
      "Haebin Seong",
      "Tobias Bocklet",
      "Juho Lee",
      "Sung Ju Hwang"
    ],
    "published": "2025-02-18T02:51:17+00:00",
    "summary": "Deploying large language models (LLMs) in real-world applications requires robust safety guard models to detect and block harmful user prompts. While large safety guard models achieve strong performance, their computational cost is substantial. To mitigate this, smaller distilled models are used, but they often underperform on \"hard\" examples where the larger model provides accurate predictions. We observe that many inputs can be reliably handled by the smaller model, while only a small fraction require the larger model's capacity. Motivated by this, we propose SafeRoute, a binary router that distinguishes hard examples from easy ones. Our method selectively applies the larger safety guard model to the data that the router considers hard, improving efficiency while maintaining accuracy compared to solely using the larger safety guard model. Experimental results on multiple benchmark datasets demonstrate that our adaptive model selection significantly enhances the trade-off between computational cost and safety performance, outperforming relevant baselines."
  },
  {
    "title": "Computational Safety for Generative AI: A Signal Processing Perspective",
    "url": "http://arxiv.org/abs/2502.12445v1",
    "arxiv_id": "2502.12445v1",
    "authors": [
      "Pin-Yu Chen"
    ],
    "published": "2025-02-18T02:26:50+00:00",
    "summary": "AI safety is a rapidly growing area of research that seeks to prevent the harm and misuse of frontier AI technology, particularly with respect to generative AI (GenAI) tools that are capable of creating realistic and high-quality content through text prompts. Examples of such tools include large language models (LLMs) and text-to-image (T2I) diffusion models. As the performance of various leading GenAI models approaches saturation due to similar training data sources and neural network architecture designs, the development of reliable safety guardrails has become a key differentiator for responsibility and sustainability. This paper presents a formalization of the concept of computational safety, which is a mathematical framework that enables the quantitative assessment, formulation, and study of safety challenges in GenAI through the lens of signal processing theory and methods. In particular, we explore two exemplary categories of computational safety challenges in GenAI that can be formulated as hypothesis testing problems. For the safety of model input, we show how sensitivity analysis and loss landscape analysis can be used to detect malicious prompts with jailbreak attempts. For the safety of model output, we elucidate how statistical signal processing and adversarial learning can be used to detect AI-generated content. Finally, we discuss key open research challenges, opportunities, and the essential role of signal processing in computational AI safety."
  },
  {
    "title": "Gradient Co-occurrence Analysis for Detecting Unsafe Prompts in Large Language Models",
    "url": "http://arxiv.org/abs/2502.12411v1",
    "arxiv_id": "2502.12411v1",
    "authors": [
      "Jingyuan Yang",
      "Bowen Yan",
      "Rongjun Li",
      "Ziyu Zhou",
      "Xin Chen",
      "Zhiyong Feng",
      "Wei Peng"
    ],
    "published": "2025-02-18T01:14:46+00:00",
    "summary": "Unsafe prompts pose significant safety risks to large language models (LLMs). Existing methods for detecting unsafe prompts rely on data-driven fine-tuning to train guardrail models, necessitating significant data and computational resources. In contrast, recent few-shot gradient-based methods emerge, requiring only few safe and unsafe reference prompts. A gradient-based approach identifies unsafe prompts by analyzing consistent patterns of the gradients of safety-critical parameters in LLMs. Although effective, its restriction to directional similarity (cosine similarity) introduces ``directional bias'', limiting its capability to identify unsafe prompts. To overcome this limitation, we introduce GradCoo, a novel gradient co-occurrence analysis method that expands the scope of safety-critical parameter identification to include unsigned gradient similarity, thereby reducing the impact of ``directional bias'' and enhancing the accuracy of unsafe prompt detection. Comprehensive experiments on the widely-used benchmark datasets ToxicChat and XStest demonstrate that our proposed method can achieve state-of-the-art (SOTA) performance compared to existing methods. Moreover, we confirm the generalizability of GradCoo in detecting unsafe prompts across a range of LLM base models with various sizes and origins."
  },
  {
    "title": "Reward-Safety Balance in Offline Safe RL via Diffusion Regularization",
    "url": "http://arxiv.org/abs/2502.12391v1",
    "arxiv_id": "2502.12391v1",
    "authors": [
      "Junyu Guo",
      "Zhi Zheng",
      "Donghao Ying",
      "Ming Jin",
      "Shangding Gu",
      "Costas Spanos",
      "Javad Lavaei"
    ],
    "published": "2025-02-18T00:00:03+00:00",
    "summary": "Constrained reinforcement learning (RL) seeks high-performance policies under safety constraints. We focus on an offline setting where the agent has only a fixed dataset -- common in realistic tasks to prevent unsafe exploration. To address this, we propose Diffusion-Regularized Constrained Offline Reinforcement Learning (DRCORL), which first uses a diffusion model to capture the behavioral policy from offline data and then extracts a simplified policy to enable efficient inference. We further apply gradient manipulation for safety adaptation, balancing the reward objective and constraint satisfaction. This approach leverages high-quality offline data while incorporating safety requirements. Empirical results show that DRCORL achieves reliable safety performance, fast inference, and strong reward outcomes across robot learning tasks. Compared to existing safe offline RL methods, it consistently meets cost limits and performs well with the same hyperparameters, indicating practical applicability in real-world scenarios."
  },
  {
    "title": "Locally-Deployed Chain-of-Thought (CoT) Reasoning Model in Chemical Engineering: Starting from 30 Experimental Data",
    "url": "http://arxiv.org/abs/2502.12383v1",
    "arxiv_id": "2502.12383v1",
    "authors": [
      "Tianhang Zhou",
      "Yingchun Niu",
      "Xingying Lan",
      "Chunming Xu"
    ],
    "published": "2025-02-17T23:43:48+00:00",
    "summary": "In the field of chemical engineering, traditional data-processing and prediction methods face significant challenges. Machine-learning and large-language models (LLMs) also have their respective limitations. This paper explores the application of the Chain-of-Thought (CoT) reasoning model in chemical engineering, starting from 30 experimental data points. By integrating traditional surrogate models like Gaussian processes and random forests with powerful LLMs such as DeepSeek-R1, a hierarchical architecture is proposed. Two CoT-building methods, Large Language Model-Chain of Thought (LLM-CoT) and Machine Learning-Large Language Model-Chain of Thought (ML-LLM-CoT), are studied. The LLM-CoT combines local models DeepSeek-r1:14b and Qwen2:7b with Ollama. The ML-LLM-CoT integrates a pre-trained Gaussian ML model with the LLM-based CoT framework. Our results show that during construction, ML-LLM-CoT is more efficient. It only has 2 points that require rethink and a total of 4 rethink times, while LLM-CoT has 5 points that need to be re-thought and 34 total rethink times. In predicting the solubility of 20 molecules with dissimilar structures, the number of molecules with a prediction deviation higher than 100\\% for the Gaussian model, LLM-CoT, and ML-LLM-CoT is 7, 6, and 4 respectively. These results indicate that ML-LLM-CoT performs better in controlling the number of high-deviation molecules, optimizing the average deviation, and achieving a higher success rate in solubility judgment, providing a more reliable method for chemical engineering and molecular property prediction. This study breaks through the limitations of traditional methods and offers new solutions for rapid property prediction and process optimization in chemical engineering."
  },
  {
    "title": "Soft Robotics for Search and Rescue: Advancements, Challenges, and Future Directions",
    "url": "http://arxiv.org/abs/2502.12373v1",
    "arxiv_id": "2502.12373v1",
    "authors": [
      "Abhishek Sebastian"
    ],
    "published": "2025-02-17T23:24:18+00:00",
    "summary": "Soft robotics has emerged as a transformative technology in Search and Rescue (SAR) operations, addressing challenges in navigating complex, hazardous environments that often limit traditional rigid robots. This paper critically examines advancements in soft robotic technologies tailored for SAR applications, focusing on their unique capabilities in adaptability, safety, and efficiency. By leveraging bio-inspired designs, flexible materials, and advanced locomotion mechanisms, such as crawling, rolling, and shape morphing, soft robots demonstrate exceptional potential in disaster scenarios. However, significant barriers persist, including material durability, power inefficiency, sensor integration, and control complexity. This comprehensive review highlights the current state of soft robotics in SAR, discusses simulation methodologies and hardware validations, and introduces performance metrics essential for their evaluation. By bridging the gap between theoretical advancements and practical deployment, this study underscores the potential of soft robotic systems to revolutionize SAR missions and advocates for continued interdisciplinary innovation to overcome existing limitations."
  },
  {
    "title": "Detecting Systematic Weaknesses in Vision Models along Predefined Human-Understandable Dimensions",
    "url": "http://arxiv.org/abs/2502.12360v1",
    "arxiv_id": "2502.12360v1",
    "authors": [
      "Sujan Sai Gannamaneni",
      "Rohil Prakash Rao",
      "Michael Mock",
      "Maram Akila",
      "Stefan Wrobel"
    ],
    "published": "2025-02-17T22:50:45+00:00",
    "summary": "Studying systematic weaknesses of DNNs has gained prominence in the last few years with the rising focus on building safe AI systems. Slice discovery methods (SDMs) are prominent algorithmic approaches for finding such systematic weaknesses. They identify top-k semantically coherent slices/subsets of data where a DNN-under-test has low performance. For being directly useful, e.g., as evidences in a safety argumentation, slices should be aligned with human-understandable (safety-relevant) dimensions, which, for example, are defined by safety and domain experts as parts of the operational design domain (ODD). While straightforward for structured data, the lack of semantic metadata makes these investigations challenging for unstructured data. Therefore, we propose a complete workflow which combines contemporary foundation models with algorithms for combinatorial search that consider structured data and DNN errors for finding systematic weaknesses in images. In contrast to existing approaches, ours identifies weak slices that are in line with predefined human-understandable dimensions. As the workflow includes foundation models, its intermediate and final results may not always be exact. Therefore, we build into our workflow an approach to address the impact of noisy metadata. We evaluate our approach w.r.t. its quality on four popular computer vision datasets, including autonomous driving datasets like Cityscapes, BDD100k, and RailSem19, while using multiple state-of-the-art models as DNNs-under-test."
  },
  {
    "title": "Asymptotic safety, quantum gravity, and the swampland: a conceptual assessment",
    "url": "http://arxiv.org/abs/2502.12290v1",
    "arxiv_id": "2502.12290v1",
    "authors": [
      "Ivano Basile",
      "Benjamin Knorr",
      "Alessia Platania",
      "Marc Schiffer"
    ],
    "published": "2025-02-17T20:00:06+00:00",
    "summary": "We provide a conceptual assessment of some aspects of fundamental quantum field theories of gravity in light of foundational aspects of the swampland program. On the one hand, asymptotically safe quantum gravity may provide a simple and predictive framework, thanks to a finite number of relevant parameters. On the other hand, a (sub-)set of intertwined swampland conjectures on the consistency of quantum gravity can be argued to be universal via effective field theory considerations. We answer whether some foundational features of these frameworks are compatible. This involves revisiting and refining several arguments (and loopholes) concerning the relation between field-theoretic descriptions of gravity and general swampland ideas. We identify the thermodynamics of black holes, spacetime topology change, and holography as the core aspects of this relation. We draw lessons on the features that a field theoretic description of gravity must (not) have to be consistent with fundamental principles underlying the swampland program, and on the universality of the latter."
  },
  {
    "title": "Integrating Expert Knowledge into Logical Programs via LLMs",
    "url": "http://arxiv.org/abs/2502.12275v1",
    "arxiv_id": "2502.12275v1",
    "authors": [
      "Franciszek G\u00f3rski",
      "Oskar Wysocki",
      "Marco Valentino",
      "Andre Freitas"
    ],
    "published": "2025-02-17T19:18:23+00:00",
    "summary": "This paper introduces ExKLoP, a novel framework designed to evaluate how effectively Large Language Models (LLMs) integrate expert knowledge into logical reasoning systems. This capability is especially valuable in engineering, where expert knowledge-such as manufacturer-recommended operational ranges-can be directly embedded into automated monitoring systems. By mirroring expert verification steps, tasks like range checking and constraint validation help ensure system safety and reliability. Our approach systematically evaluates LLM-generated logical rules, assessing both syntactic fluency and logical correctness in these critical validation tasks. We also explore the models capacity for self-correction via an iterative feedback loop based on code execution outcomes. ExKLoP presents an extensible dataset comprising 130 engineering premises, 950 prompts, and corresponding validation points. It enables comprehensive benchmarking while allowing control over task complexity and scalability of experiments. We leverage the synthetic data creation methodology to conduct extensive empirical evaluation on a diverse set of LLMs including Llama3, Gemma, Mixtral, Mistral, and Qwen. Results reveal that while models generate nearly perfect syntactically correct code, they frequently exhibit logical errors in translating expert knowledge. Furthermore, iterative self-correction yields only marginal improvements (up to 3%). Overall, ExKLoP serves as a robust evaluation platform that streamlines the selection of effective models for self-correcting systems while clearly delineating the types of errors encountered. The complete implementation, along with all relevant data, is available at GitHub."
  },
  {
    "title": "NeuroStrata: Harnessing Neurosymbolic Paradigms for Improved Design, Testability, and Verifiability of Autonomous CPS",
    "url": "http://arxiv.org/abs/2502.12267v1",
    "arxiv_id": "2502.12267v1",
    "authors": [
      "Xi Zheng",
      "Ziyang Li",
      "Ivan Ruchkin",
      "Ruzica Piskac",
      "Miroslav Pajic"
    ],
    "published": "2025-02-17T19:07:41+00:00",
    "summary": "Autonomous cyber-physical systems (CPSs) leverage AI for perception, planning, and control but face trust and safety certification challenges due to inherent uncertainties. The neurosymbolic paradigm replaces stochastic layers with interpretable symbolic AI, enabling determinism. While promising, challenges like multisensor fusion, adaptability, and verification remain. This paper introduces NeuroStrata, a neurosymbolic framework to enhance the testing and verification of autonomous CPS. We outline its key components, present early results, and detail future plans."
  },
  {
    "title": "SmokeNet: Efficient Smoke Segmentation Leveraging Multiscale Convolutions and Multiview Attention Mechanisms",
    "url": "http://arxiv.org/abs/2502.12258v1",
    "arxiv_id": "2502.12258v1",
    "authors": [
      "Xuesong Liu",
      "Emmett J. Ientilucci"
    ],
    "published": "2025-02-17T19:01:27+00:00",
    "summary": "Efficient segmentation of smoke plumes is crucial for environmental monitoring and industrial safety, enabling the detection and mitigation of harmful emissions from activities like quarry blasts and wildfires. Accurate segmentation facilitates environmental impact assessments, timely interventions, and compliance with safety standards. However, existing models often face high computational demands and limited adaptability to diverse smoke appearances, restricting their deployment in resource-constrained environments. To address these issues, we introduce SmokeNet, a novel deep learning architecture that leverages multiscale convolutions and multiview linear attention mechanisms combined with layer-specific loss functions to handle the complex dynamics of diverse smoke plumes, ensuring efficient and accurate segmentation across varied environments. Additionally, we evaluate SmokeNet's performance and versatility using four datasets, including our quarry blast smoke dataset made available to the community. The results demonstrate that SmokeNet maintains a favorable balance between computational efficiency and segmentation accuracy, making it suitable for deployment in environmental monitoring and safety management systems. By contributing a new dataset and offering an efficient segmentation model, SmokeNet advances smoke segmentation capabilities in diverse and challenging environments."
  },
  {
    "title": "Can LLMs Simulate Social Media Engagement? A Study on Action-Guided Response Generation",
    "url": "http://arxiv.org/abs/2502.12073v1",
    "arxiv_id": "2502.12073v1",
    "authors": [
      "Zhongyi Qiu",
      "Hanjia Lyu",
      "Wei Xiong",
      "Jiebo Luo"
    ],
    "published": "2025-02-17T17:43:08+00:00",
    "summary": "Social media enables dynamic user engagement with trending topics, and recent research has explored the potential of large language models (LLMs) for response generation. While some studies investigate LLMs as agents for simulating user behavior on social media, their focus remains on practical viability and scalability rather than a deeper understanding of how well LLM aligns with human behavior. This paper analyzes LLMs' ability to simulate social media engagement through action guided response generation, where a model first predicts a user's most likely engagement action-retweet, quote, or rewrite-towards a trending post before generating a personalized response conditioned on the predicted action. We benchmark GPT-4o-mini, O1-mini, and DeepSeek-R1 in social media engagement simulation regarding a major societal event discussed on X. Our findings reveal that zero-shot LLMs underperform BERT in action prediction, while few-shot prompting initially degrades the prediction accuracy of LLMs with limited examples. However, in response generation, few-shot LLMs achieve stronger semantic alignment with ground truth posts."
  },
  {
    "title": "TokenSkip: Controllable Chain-of-Thought Compression in LLMs",
    "url": "http://arxiv.org/abs/2502.12067v1",
    "arxiv_id": "2502.12067v1",
    "authors": [
      "Heming Xia",
      "Yongqi Li",
      "Chak Tou Leong",
      "Wenjie Wang",
      "Wenjie Li"
    ],
    "published": "2025-02-17T17:37:26+00:00",
    "summary": "Chain-of-Thought (CoT) has been proven effective in enhancing the reasoning capabilities of large language models (LLMs). Recent advancements, such as OpenAI's o1 and DeepSeek-R1, suggest that scaling up the length of CoT sequences during inference could further boost LLM reasoning performance. However, due to the autoregressive nature of LLM decoding, longer CoT outputs lead to a linear increase in inference latency, adversely affecting user experience, particularly when the CoT exceeds 10,000 tokens. To address this limitation, we analyze the semantic importance of tokens within CoT outputs and reveal that their contributions to reasoning vary. Building on this insight, we propose TokenSkip, a simple yet effective approach that enables LLMs to selectively skip less important tokens, allowing for controllable CoT compression. Extensive experiments across various models and tasks demonstrate the effectiveness of TokenSkip in reducing CoT token usage while preserving strong reasoning performance. Notably, when applied to Qwen2.5-14B-Instruct, TokenSkip reduces reasoning tokens by 40% (from 313 to 181) on GSM8K, with less than a 0.4% performance drop."
  },
  {
    "title": "PhysReason: A Comprehensive Benchmark towards Physics-Based Reasoning",
    "url": "http://arxiv.org/abs/2502.12054v1",
    "arxiv_id": "2502.12054v1",
    "authors": [
      "Xinyu Zhang",
      "Yuxuan Dong",
      "Yanrui Wu",
      "Jiaxing Huang",
      "Chengyou Jia",
      "Basura Fernando",
      "Mike Zheng Shou",
      "Lingling Zhang",
      "Jun Liu"
    ],
    "published": "2025-02-17T17:24:14+00:00",
    "summary": "Large language models demonstrate remarkable capabilities across various domains, especially mathematics and logic reasoning. However, current evaluations overlook physics-based reasoning - a complex task requiring physics theorems and constraints. We present PhysReason, a 1,200-problem benchmark comprising knowledge-based (25%) and reasoning-based (75%) problems, where the latter are divided into three difficulty levels (easy, medium, hard). Notably, problems require an average of 8.1 solution steps, with hard requiring 15.6, reflecting the complexity of physics-based reasoning. We propose the Physics Solution Auto Scoring Framework, incorporating efficient answer-level and comprehensive step-level evaluations. Top-performing models like Deepseek-R1, Gemini-2.0-Flash-Thinking, and o3-mini-high achieve less than 60% on answer-level evaluation, with performance dropping from knowledge questions (75.11%) to hard problems (31.95%). Through step-level evaluation, we identified four key bottlenecks: Physics Theorem Application, Physics Process Understanding, Calculation, and Physics Condition Analysis. These findings position PhysReason as a novel and comprehensive benchmark for evaluating physics-based reasoning capabilities in large language models. Our code and data will be published at https:/dxzxy12138.github.io/PhysReason."
  },
  {
    "title": "SafeChain: Safety of Language Models with Long Chain-of-Thought Reasoning Capabilities",
    "url": "http://arxiv.org/abs/2502.12025v1",
    "arxiv_id": "2502.12025v1",
    "authors": [
      "Fengqing Jiang",
      "Zhangchen Xu",
      "Yuetai Li",
      "Luyao Niu",
      "Zhen Xiang",
      "Bo Li",
      "Bill Yuchen Lin",
      "Radha Poovendran"
    ],
    "published": "2025-02-17T16:57:56+00:00",
    "summary": "Emerging large reasoning models (LRMs), such as DeepSeek-R1 models, leverage long chain-of-thought (CoT) reasoning to generate structured intermediate steps, enhancing their reasoning capabilities. However, long CoT does not inherently guarantee safe outputs, potentially leading to harmful consequences such as the introduction of security vulnerabilities in code or the spread of misinformation. Current research on large language model (LLM) safety usually focuses on short-answer responses, overlooking the long CoT style outputs of LRMs. To bridge this gap, we conduct a systematic study of LRM safety. First, we investigate safety evaluators calibrated against human annotations. Using our newly developed metrics, we thoroughly assess the safety of 12 state-of-the-art LRMs on StrongReject and WildJailbreak datasets. Our results show that LRMs are not safe compared to their reasoning advance. Further, we perform a fine-grained analysis of the reasoning trace and final answer. We find that three decoding strategies-ZeroThink, LessThink, and MoreThink-can improve model safety without additional training. However, these strategies either use constrained reasoning traces or incur high inference costs. To better strengthen LRM safety, we introduce SafeChain, the first-of-its-kind safety training dataset in CoT style. We fine-tune two LRMs with SafeChain, showing that it not only enhances model safety but also preserves performance across 6 reasoning benchmarks."
  },
  {
    "title": "Atom of Thoughts for Markov LLM Test-Time Scaling",
    "url": "http://arxiv.org/abs/2502.12018v1",
    "arxiv_id": "2502.12018v1",
    "authors": [
      "Fengwei Teng",
      "Zhaoyang Yu",
      "Quan Shi",
      "Jiayi Zhang",
      "Chenglin Wu",
      "Yuyu Luo"
    ],
    "published": "2025-02-17T16:52:42+00:00",
    "summary": "Large Language Models (LLMs) achieve superior performance through training-time scaling, and test-time scaling further enhances their capabilities by conducting effective reasoning during inference. However, as the scale of reasoning increases, existing test-time scaling methods suffer from accumulated historical information, which not only wastes computational resources but also interferes with effective reasoning. To address this issue, we observe that complex reasoning progress is often achieved by solving a sequence of independent subquestions, each being self-contained and verifiable. These subquestions are essentially atomic questions, relying primarily on their current state rather than accumulated history, similar to the memoryless transitions in a Markov process. Based on this observation, we propose Atom of Thoughts (AoT), where each state transition in the reasoning process consists of decomposing the current question into a dependency-based directed acyclic graph and contracting its subquestions, forming a new atomic question state. This iterative decomposition-contraction process continues until reaching directly solvable atomic questions, naturally realizing Markov transitions between question states. Furthermore, these atomic questions can be seamlessly integrated into existing test-time scaling methods, enabling AoT to serve as a plug-in enhancement for improving reasoning capabilities. Experiments across six benchmarks demonstrate the effectiveness of AoT both as a standalone framework and a plug-in enhancement. Notably, on HotpotQA, when applied to gpt-4o-mini, AoT achieves an 80.6% F1 score, surpassing o3-mini by 3.4% and DeepSeek-R1 by 10.6%. The code will be available at https://github.com/qixucen/atom."
  },
  {
    "title": "LIMR: Less is More for RL Scaling",
    "url": "http://arxiv.org/abs/2502.11886v1",
    "arxiv_id": "2502.11886v1",
    "authors": [
      "Xuefeng Li",
      "Haoyang Zou",
      "Pengfei Liu"
    ],
    "published": "2025-02-17T15:13:29+00:00",
    "summary": "In this paper, we ask: what truly determines the effectiveness of RL training data for enhancing language models' reasoning capabilities? While recent advances like o1, Deepseek R1, and Kimi1.5 demonstrate RL's potential, the lack of transparency about training data requirements has hindered systematic progress. Starting directly from base models without distillation, we challenge the assumption that scaling up RL training data inherently improves performance. we demonstrate that a strategically selected subset of just 1,389 samples can outperform the full 8,523-sample dataset. We introduce Learning Impact Measurement (LIM), an automated method to evaluate and prioritize training samples based on their alignment with model learning trajectories, enabling efficient resource utilization and scalable implementation. Our method achieves comparable or even superior performance using only 1,389 samples versus the full 8,523 samples dataset. Notably, while recent data-efficient approaches (e.g., LIMO and s1) show promise with 32B-scale models, we find it significantly underperforms at 7B-scale through supervised fine-tuning (SFT). In contrast, our RL-based LIMR achieves 16.7% higher accuracy on AIME24 and outperforms LIMO and s1 by 13.0% and 22.2% on MATH500. These results fundamentally reshape our understanding of RL scaling in LLMs, demonstrating that precise sample selection, rather than data scale, may be the key to unlocking enhanced reasoning capabilities. For reproducible research and future innovation, we are open-sourcing LIMR, including implementation of LIM, training and evaluation code, curated datasets, and trained models at https://github.com/GAIR-NLP/LIMR."
  },
  {
    "title": "Hypothesis-Driven Theory-of-Mind Reasoning for Large Language Models",
    "url": "http://arxiv.org/abs/2502.11881v1",
    "arxiv_id": "2502.11881v1",
    "authors": [
      "Hyunwoo Kim",
      "Melanie Sclar",
      "Tan Zhi-Xuan",
      "Lance Ying",
      "Sydney Levine",
      "Yang Liu",
      "Joshua B. Tenenbaum",
      "Yejin Choi"
    ],
    "published": "2025-02-17T15:08:50+00:00",
    "summary": "Existing LLM reasoning methods have shown impressive capabilities across various tasks, such as solving math and coding problems. However, applying these methods to scenarios without ground-truth answers or rule-based verification methods - such as tracking the mental states of an agent - remains challenging. Inspired by the sequential Monte Carlo algorithm, we introduce thought-tracing, an inference-time reasoning algorithm designed to trace the mental states of specific agents by generating hypotheses and weighting them based on observations without relying on ground-truth solutions to questions in datasets. Our algorithm is modeled after the Bayesian theory-of-mind framework, using LLMs to approximate probabilistic inference over agents' evolving mental states based on their perceptions and actions. We evaluate thought-tracing on diverse theory-of-mind benchmarks, demonstrating significant performance improvements compared to baseline LLMs. Our experiments also reveal interesting behaviors of the recent reasoning models - e.g., o1 and R1 - on theory-of-mind, highlighting the difference of social reasoning compared to other domains."
  },
  {
    "title": "StructTransform: A Scalable Attack Surface for Safety-Aligned Large Language Models",
    "url": "http://arxiv.org/abs/2502.11853v1",
    "arxiv_id": "2502.11853v1",
    "authors": [
      "Shehel Yoosuf",
      "Temoor Ali",
      "Ahmed Lekssays",
      "Mashael AlSabah",
      "Issa Khalil"
    ],
    "published": "2025-02-17T14:46:38+00:00",
    "summary": "In this work, we present a series of structure transformation attacks on LLM alignment, where we encode natural language intent using diverse syntax spaces, ranging from simple structure formats and basic query languages (e.g. SQL) to new novel spaces and syntaxes created entirely by LLMs. Our extensive evaluation shows that our simplest attacks can achieve close to 90% success rate, even on strict LLMs (such as Claude 3.5 Sonnet) using SOTA alignment mechanisms. We improve the attack performance further by using an adaptive scheme that combines structure transformations along with existing \\textit{content transformations}, resulting in over 96% ASR with 0% refusals.   To generalize our attacks, we explore numerous structure formats, including syntaxes purely generated by LLMs. Our results indicate that such novel syntaxes are easy to generate and result in a high ASR, suggesting that defending against our attacks is not a straightforward process. Finally, we develop a benchmark and evaluate existing safety-alignment defenses against it, showing that most of them fail with 100% ASR. Our results show that existing safety alignment mostly relies on token-level patterns without recognizing harmful concepts, highlighting and motivating the need for serious research efforts in this direction. As a case study, we demonstrate how attackers can use our attack to easily generate a sample malware, and a corpus of fraudulent SMS messages, which perform well in bypassing detection."
  },
  {
    "title": "BaxBench: Can LLMs Generate Correct and Secure Backends?",
    "url": "http://arxiv.org/abs/2502.11844v1",
    "arxiv_id": "2502.11844v1",
    "authors": [
      "Mark Vero",
      "Niels M\u00fcndler",
      "Victor Chibotaru",
      "Veselin Raychev",
      "Maximilian Baader",
      "Nikola Jovanovi\u0107",
      "Jingxuan He",
      "Martin Vechev"
    ],
    "published": "2025-02-17T14:37:47+00:00",
    "summary": "The automatic generation of programs has long been a fundamental challenge in computer science. Recent benchmarks have shown that large language models (LLMs) can effectively generate code at the function level, make code edits, and solve algorithmic coding tasks. However, to achieve full automation, LLMs should be able to generate production-quality, self-contained application modules. To evaluate the capabilities of LLMs in solving this challenge, we introduce BaxBench, a novel evaluation benchmark consisting of 392 tasks for the generation of backend applications. We focus on backends for three critical reasons: (i) they are practically relevant, building the core components of most modern web and cloud software, (ii) they are difficult to get right, requiring multiple functions and files to achieve the desired functionality, and (iii) they are security-critical, as they are exposed to untrusted third-parties, making secure solutions that prevent deployment-time attacks an imperative. BaxBench validates the functionality of the generated applications with comprehensive test cases, and assesses their security exposure by executing end-to-end exploits. Our experiments reveal key limitations of current LLMs in both functionality and security: (i) even the best model, OpenAI o1, achieves a mere 60% on code correctness; (ii) on average, we could successfully execute security exploits on more than half of the correct programs generated by each LLM; and (iii) in less popular backend frameworks, models further struggle to generate correct and secure applications. Progress on BaxBench signifies important steps towards autonomous and secure software development with LLMs."
  },
  {
    "title": "video-SALMONN-o1: Reasoning-enhanced Audio-visual Large Language Model",
    "url": "http://arxiv.org/abs/2502.11775v1",
    "arxiv_id": "2502.11775v1",
    "authors": [
      "Guangzhi Sun",
      "Yudong Yang",
      "Jimin Zhuang",
      "Changli Tang",
      "Yixuan Li",
      "Wei Li",
      "Zejun MA",
      "Chao Zhang"
    ],
    "published": "2025-02-17T13:07:40+00:00",
    "summary": "While recent advancements in reasoning optimization have significantly enhanced the capabilities of large language models (LLMs), existing efforts to improve reasoning have been limited to solving mathematical problems and focusing on visual graphical inputs, neglecting broader applications in general video understanding.This paper proposes video-SALMONN-o1, the first open-source reasoning-enhanced audio-visual LLM designed for general video understanding tasks. To enhance its reasoning abilities, we develop a reasoning-intensive dataset featuring challenging audio-visual questions with step-by-step solutions. We also propose process direct preference optimization (pDPO), which leverages contrastive step selection to achieve efficient step-level reward modelling tailored for multimodal inputs. Additionally, we introduce RivaBench, the first reasoning-intensive video understanding benchmark, featuring over 4,000 high-quality, expert-curated question-answer pairs across scenarios such as standup comedy, academic presentations, and synthetic video detection. video-SALMONN-o1 achieves 3-8% accuracy improvements over the LLaVA-OneVision baseline across different video reasoning benchmarks. Besides, pDPO achieves 6-8% improvements compared to the supervised fine-tuning model on RivaBench. Enhanced reasoning enables video-SALMONN-o1 zero-shot synthetic video detection capabilities."
  },
  {
    "title": "SQL-o1: A Self-Reward Heuristic Dynamic Search Method for Text-to-SQL",
    "url": "http://arxiv.org/abs/2502.11741v1",
    "arxiv_id": "2502.11741v1",
    "authors": [
      "Shuai Lyu",
      "Haoran Luo",
      "Zhonghong Ou",
      "Yifan Zhu",
      "Xiaoran Shang",
      "Yang Qin",
      "Meina Song"
    ],
    "published": "2025-02-17T12:28:11+00:00",
    "summary": "The Text-to-SQL(Text2SQL) task aims to convert natural language queries into executable SQL queries. Thanks to the application of large language models (LLMs), significant progress has been made in this field. However, challenges such as model scalability, limited generation space, and coherence issues in SQL generation still persist. To address these issues, we propose SQL-o1, a Self-Reward-based heuristic search method designed to enhance the reasoning ability of LLMs in SQL query generation. SQL-o1 combines Monte Carlo Tree Search (MCTS) for heuristic process-level search and constructs a Schema-Aware dataset to help the model better understand database schemas. Extensive experiments on the Bird and Spider datasets demonstrate that SQL-o1 improves execution accuracy by 10.8\\% on the complex Bird dataset compared to the latest baseline methods, even outperforming GPT-4-based approaches. Additionally, SQL-o1 excels in few-shot learning scenarios and shows strong cross-model transferability. Our code is publicly available at:https://github.com/ShuaiLyu0110/SQL-o1."
  },
  {
    "title": "Connecting Earth and Moon via the L1 Lagrangian point",
    "url": "http://arxiv.org/abs/2502.11694v1",
    "arxiv_id": "2502.11694v1",
    "authors": [
      "A. K. de Almeida Jr",
      "V. M. de Oliveira",
      "T. Vaillant",
      "D. Maia",
      "A. C. M. Correia",
      "D. Barbosa",
      "L. T. B. Santos"
    ],
    "published": "2025-02-17T11:32:02+00:00",
    "summary": "The renewed global interest in lunar exploration requires new orbital strategies to ensure flight safety which can benefit extended lunar missions and service a plethora of planned instruments in the lunar orbit and surface. We investigate here the equivalent fuel consumption cost to transfer from (to) a given orbit and enter (leave) at any point of an invariant manifold associated with a Lyapunov orbit around the Earth-Moon $L_1$ Lagrangian point using bi-impulsive maneuvers. Whereas solving this type of transfer is generally computationally expensive, we simulate here tens of millions of transfers orbits, for different times of flight, Jacobi constants and spatial location on the manifold. We are able to reduce computational cost by taking advantage of the efficient procedure given by the Theory of Functional Connections for solving boundary value problems, represented with special constraints created to the purposes of this work. We develop here the methodology for constructing these transfers, and apply it to find a low-cost transfer from an orbit around the Earth to a stable manifold and another low-cost transfer from an unstable manifold to an orbit around the Moon. In the end, we obtain an innovative Earth-to-Moon transfer that involves a gravity assist maneuver with the Moon and allows a long stationed stage at the Lyapunov orbit around $L_1$ which can be used for designing multi-purpose missions for extended periods of time with low fuel costs. This is paramount to optimize new exploration concepts."
  },
  {
    "title": "RIDE: Enhancing Large Language Model Alignment through Restyled In-Context Learning Demonstration Exemplars",
    "url": "http://arxiv.org/abs/2502.11681v1",
    "arxiv_id": "2502.11681v1",
    "authors": [
      "Yuncheng Hua",
      "Lizhen Qu",
      "Zhuang Li",
      "Hao Xue",
      "Flora D. Salim",
      "Gholamreza Haffari"
    ],
    "published": "2025-02-17T11:16:19+00:00",
    "summary": "Alignment tuning is crucial for ensuring large language models (LLMs) behave ethically and helpfully. Current alignment approaches require high-quality annotations and significant training resources. This paper proposes a low-cost, tuning-free method using in-context learning (ICL) to enhance LLM alignment. Through an analysis of high-quality ICL demos, we identified style as a key factor influencing LLM alignment capabilities and explicitly restyled ICL exemplars based on this stylistic framework. Additionally, we combined the restyled demos to achieve a balance between the two conflicting aspects of LLM alignment--factuality and safety. We packaged the restyled examples as prompts to trigger few-shot learning, improving LLM alignment. Compared to the best baseline approach, with an average score of 5.00 as the maximum, our method achieves a maximum 0.10 increase on the Alpaca task (from 4.50 to 4.60), a 0.22 enhancement on the Just-eval benchmark (from 4.34 to 4.56), and a maximum improvement of 0.32 (from 3.53 to 3.85) on the MT-Bench dataset. We release the code and data at https://github.com/AnonymousCode-ComputerScience/RIDE."
  },
  {
    "title": "DELMAN: Dynamic Defense Against Large Language Model Jailbreaking with Model Editing",
    "url": "http://arxiv.org/abs/2502.11647v1",
    "arxiv_id": "2502.11647v1",
    "authors": [
      "Yi Wang",
      "Fenghua Weng",
      "Sibei Yang",
      "Zhan Qin",
      "Minlie Huang",
      "Wenjie Wang"
    ],
    "published": "2025-02-17T10:39:21+00:00",
    "summary": "Large Language Models (LLMs) are widely applied in decision making, but their deployment is threatened by jailbreak attacks, where adversarial users manipulate model behavior to bypass safety measures. Existing defense mechanisms, such as safety fine-tuning and model editing, either require extensive parameter modifications or lack precision, leading to performance degradation on general tasks, which is unsuitable to post-deployment safety alignment. To address these challenges, we propose DELMAN (Dynamic Editing for LLMs JAilbreak DefeNse), a novel approach leveraging direct model editing for precise, dynamic protection against jailbreak attacks. DELMAN directly updates a minimal set of relevant parameters to neutralize harmful behaviors while preserving the model's utility. To avoid triggering a safe response in benign context, we incorporate KL-divergence regularization to ensure the updated model remains consistent with the original model when processing benign queries. Experimental results demonstrate that DELMAN outperforms baseline methods in mitigating jailbreak attacks while preserving the model's utility, and adapts seamlessly to new attack instances, providing a practical and efficient solution for post-deployment model protection."
  },
  {
    "title": "Enhancing Out-of-Distribution Detection in Medical Imaging with Normalizing Flows",
    "url": "http://arxiv.org/abs/2502.11638v1",
    "arxiv_id": "2502.11638v1",
    "authors": [
      "Dariush Lotfi",
      "Mohammad-Ali Nikouei Mahani",
      "Mohamad Koohi-Moghadam",
      "Kyongtae Ty Bae"
    ],
    "published": "2025-02-17T10:31:24+00:00",
    "summary": "Out-of-distribution (OOD) detection is crucial in AI-driven medical imaging to ensure reliability and safety by identifying inputs outside a model's training distribution. Existing methods often require retraining or modifications to pre-trained models, which is impractical for clinical applications. This study introduces a post-hoc normalizing flow-based approach that seamlessly integrates with pre-trained models. By leveraging normalizing flows, it estimates the likelihood of feature vectors extracted from pre-trained models, capturing semantically meaningful representations without relying on pixel-level statistics. The method was evaluated using the MedMNIST benchmark and a newly curated MedOOD dataset simulating clinically relevant distributional shifts. Performance was measured using standard OOD detection metrics (e.g., AUROC, FPR@95, AUPR_IN, AUPR_OUT), with statistical analyses comparing it against ten baseline methods. On MedMNIST, the proposed model achieved an AUROC of 93.80%, outperforming state-of-the-art methods. On MedOOD, it achieved an AUROC of 84.61%, demonstrating superior performance against other methods. Its post-hoc nature ensures compatibility with existing clinical workflows, addressing the limitations of previous approaches. The model and code to build OOD datasets are available at https://github.com/dlotfi/MedOODFlow."
  },
  {
    "title": "GraphThought: Graph Combinatorial Optimization with Thought Generation",
    "url": "http://arxiv.org/abs/2502.11607v1",
    "arxiv_id": "2502.11607v1",
    "authors": [
      "Zixiao Huang",
      "Lifeng Guo",
      "Junjie Sheng",
      "Haosheng Chen",
      "Wenhao Li",
      "Bo Jin",
      "Changhong Lu",
      "Xiangfeng Wang"
    ],
    "published": "2025-02-17T09:50:41+00:00",
    "summary": "Large language models (LLMs) have demonstrated remarkable capabilities across various domains, especially in text processing and generative tasks. Recent advancements in the reasoning capabilities of state-of-the-art LLMs, such as OpenAI-o1, have significantly broadened their applicability, particularly in complex problem-solving and logical inference. However, most existing LLMs struggle with notable limitations in handling graph combinatorial optimization (GCO) problems. To bridge this gap, we formally define the Optimal Thoughts Design (OTD) problem, including its state and action thought space. We then introduce a novel framework, GraphThought, designed to generate high-quality thought datasets for GCO problems. Leveraging these datasets, we fine-tune the Llama-3-8B-Instruct model to develop Llama-GT. Notably, despite its compact 8B-parameter architecture, Llama-GT matches the performance of state-of-the-art LLMs on the GraphArena benchmark. Experimental results show that our approach outperforms both proprietary and open-source models, even rivaling specialized models like o1-mini. This work sets a new state-of-the-art benchmark while challenging the prevailing notion that model scale is the primary driver of reasoning capability."
  },
  {
    "title": "Runtime Enforcement of CPS against Signal Temporal Logic",
    "url": "http://arxiv.org/abs/2502.11584v1",
    "arxiv_id": "2502.11584v1",
    "authors": [
      "Han Su",
      "Saumya Shankar",
      "Srinivas Pinisetty",
      "Partha S. Roop",
      "Naijun Zhan"
    ],
    "published": "2025-02-17T09:16:58+00:00",
    "summary": "Cyber-Physical Systems (CPSs), especially those involving autonomy, need guarantees of their safety. Runtime Enforcement (RE) is a lightweight method to formally ensure that some specified properties are satisfied over the executions of the system. Hence, there is recent interest in the RE of CPS. However, existing methods are not designed to tackle specifications suitable for the hybrid dynamics of CPS. With this in mind, we develop runtime enforcement of CPS using properties defined in Signal Temporal Logic (STL).   In this work, we aim to construct a runtime enforcer for a given STL formula to minimally modify a signal to satisfy the formula. To achieve this, the STL formula to be enforced is first translated into a timed transducer, while the signal to be corrected is encoded as timed words. We provide timed transducers for the temporal operators \\emph{until} and \\emph{release} noting that other temporal operators can be expressed using these two. Our approach enables effective enforcement of STL properties for CPS. A case study is provided to illustrate the approach and generate empirical evidence of its suitability for CPS."
  },
  {
    "title": "Language Complexity Measurement as a Noisy Zero-Shot Proxy for Evaluating LLM Performance",
    "url": "http://arxiv.org/abs/2502.11578v1",
    "arxiv_id": "2502.11578v1",
    "authors": [
      "Birger Moell",
      "Johan Boye"
    ],
    "published": "2025-02-17T09:09:58+00:00",
    "summary": "Large Language Models (LLMs) have made significant strides in natural language generation but often face challenges in tasks requiring precise calculations and structural analysis. This paper investigates the performance of state-of-the-art LLMs on language complexity measurement tasks, through the computation of the LIX readability metric and Average Dependency Distance (ADD). Using Swedish high school and university-level essays, we evaluate the models' abilities to compute LIX scores and perform dependency parsing, comparing their results to established ground truths. Our findings reveal that while all models demonstrate some capacity for these tasks, ChatGPT-o1-mini performs most consistently, achieving the highest accuracy in both LIX computation and dependency parsing. Additionally, we observe a strong significant correlation -0.875 p 0.026 (N=6) between the models' accuracy in computing LIX and their overall performance on the Massive Multitask Language Understanding (MMLU) benchmark. These results suggest that language complexity measurement abilities can serve as a noisy zero-shot proxies for assessing the general capabilities of LLMs, providing a practical method for model evaluation without the need for extensive benchmarking datasets."
  },
  {
    "title": "Large Language Models and Mathematical Reasoning Failures",
    "url": "http://arxiv.org/abs/2502.11574v1",
    "arxiv_id": "2502.11574v1",
    "authors": [
      "Johan Boye",
      "Birger Moell"
    ],
    "published": "2025-02-17T09:07:32+00:00",
    "summary": "This paper investigates the mathematical reasoning capabilities of large language models (LLMs) using 50 newly constructed high-school-level word problems. Unlike prior studies that focus solely on answer correctness, we rigorously analyze both final answers and solution steps to identify reasoning failures. Evaluating eight state-of-the-art models - including Mixtral, Llama, Gemini, GPT-4o, and OpenAI's o1 variants - we find that while newer models (e.g., o3-mini, deepseek-r1) achieve higher accuracy, all models exhibit errors in spatial reasoning, strategic planning, and arithmetic, sometimes producing correct answers through flawed logic. Common failure modes include unwarranted assumptions, over-reliance on numerical patterns, and difficulty translating physical intuition into mathematical steps. Manual analysis reveals that models struggle with problems requiring multi-step deduction or real-world knowledge, despite possessing broad mathematical knowledge. Our results underscore the importance of evaluating reasoning processes, not just answers, and caution against overestimating LLMs' problem-solving proficiency. The study highlights persistent gaps in LLMs' generalization abilities, emphasizing the need for targeted improvements in structured reasoning and constraint handling."
  },
  {
    "title": "Equilibrate RLHF: Towards Balancing Helpfulness-Safety Trade-off in Large Language Models",
    "url": "http://arxiv.org/abs/2502.11555v1",
    "arxiv_id": "2502.11555v1",
    "authors": [
      "Yingshui Tan",
      "Yilei Jiang",
      "Yanshi Li",
      "Jiaheng Liu",
      "Xingyuan Bu",
      "Wenbo Su",
      "Xiangyu Yue",
      "Xiaoyong Zhu",
      "Bo Zheng"
    ],
    "published": "2025-02-17T08:40:30+00:00",
    "summary": "Fine-tuning large language models (LLMs) based on human preferences, commonly achieved through reinforcement learning from human feedback (RLHF), has been effective in improving their performance. However, maintaining LLM safety throughout the fine-tuning process remains a significant challenge, as resolving conflicts between safety and helpfulness can be non-trivial. Typically, the safety alignment of LLM is trained on data with safety-related categories. However, our experiments find that naively increasing the scale of safety training data usually leads the LLMs to an ``overly safe'' state rather than a ``truly safe'' state, boosting the refusal rate through extensive safety-aligned data without genuinely understanding the requirements for safe responses. Such an approach can inadvertently diminish the models' helpfulness. To understand the phenomenon, we first investigate the role of safety data by categorizing them into three different groups, and observe that each group behaves differently as training data scales up. To boost the balance between safety and helpfulness, we propose an Equilibrate RLHF framework including a Fine-grained Data-centric (FDC) approach that achieves better safety alignment even with fewer training data, and an Adaptive Message-wise Alignment (AMA) approach, which selectively highlight the key segments through a gradient masking strategy. Extensive experimental results demonstrate that our approach significantly enhances the safety alignment of LLMs while balancing safety and helpfulness."
  },
  {
    "title": "Evaluating o1-Like LLMs: Unlocking Reasoning for Translation through Comprehensive Analysis",
    "url": "http://arxiv.org/abs/2502.11544v1",
    "arxiv_id": "2502.11544v1",
    "authors": [
      "Andong Chen",
      "Yuchen Song",
      "Wenxin Zhu",
      "Kehai Chen",
      "Muyun Yang",
      "Tiejun Zhao",
      "Min zhang"
    ],
    "published": "2025-02-17T08:23:46+00:00",
    "summary": "The o1-Like LLMs are transforming AI by simulating human cognitive processes, but their performance in multilingual machine translation (MMT) remains underexplored. This study examines: (1) how o1-Like LLMs perform in MMT tasks and (2) what factors influence their translation quality. We evaluate multiple o1-Like LLMs and compare them with traditional models like ChatGPT and GPT-4o. Results show that o1-Like LLMs establish new multilingual translation benchmarks, with DeepSeek-R1 surpassing GPT-4o in contextless tasks. They demonstrate strengths in historical and cultural translation but exhibit a tendency for rambling issues in Chinese-centric outputs. Further analysis reveals three key insights: (1) High inference costs and slower processing speeds make complex translation tasks more resource-intensive. (2) Translation quality improves with model size, enhancing commonsense reasoning and cultural translation. (3) The temperature parameter significantly impacts output quality-lower temperatures yield more stable and accurate translations, while higher temperatures reduce coherence and precision."
  },
  {
    "title": "AURORA:Automated Training Framework of Universal Process Reward Models via Ensemble Prompting and Reverse Verification",
    "url": "http://arxiv.org/abs/2502.11520v1",
    "arxiv_id": "2502.11520v1",
    "authors": [
      "Xiaoyu Tan",
      "Tianchu Yao",
      "Chao Qu",
      "Bin Li",
      "Minghao Yang",
      "Dakuan Lu",
      "Haozhe Wang",
      "Xihe Qiu",
      "Wei Chu",
      "Yinghui Xu",
      "Yuan Qi"
    ],
    "published": "2025-02-17T07:41:27+00:00",
    "summary": "The reasoning capabilities of advanced large language models (LLMs) like o1 have revolutionized artificial intelligence applications. Nevertheless, evaluating and optimizing complex reasoning processes remain significant challenges due to diverse policy distributions and the inherent limitations of human effort and accuracy. In this paper, we present AURORA, a novel automated framework for training universal process reward models (PRMs) using ensemble prompting and reverse verification. The framework employs a two-phase approach: First, it uses diverse prompting strategies and ensemble methods to perform automated annotation and evaluation of processes, ensuring robust assessments for reward learning. Second, it leverages practical reference answers for reverse verification, enhancing the model's ability to validate outputs and improving training accuracy. To assess the framework's performance, we extend beyond the existing ProcessBench benchmark by introducing UniversalBench, which evaluates reward predictions across full trajectories under diverse policy distribtion with long Chain-of-Thought (CoT) outputs. Experimental results demonstrate that AURORA enhances process evaluation accuracy, improves PRMs' accuracy for diverse policy distributions and long-CoT responses. The project will be open-sourced at https://auroraprm.github.io/. The Universal-PRM-7B is available at https://huggingface.co/infly/Universal-PRM-7B."
  },
  {
    "title": "Adversary-Aware DPO: Enhancing Safety Alignment in Vision Language Models via Adversarial Training",
    "url": "http://arxiv.org/abs/2502.11455v1",
    "arxiv_id": "2502.11455v1",
    "authors": [
      "Fenghua Weng",
      "Jian Lou",
      "Jun Feng",
      "Minlie Huang",
      "Wenjie Wang"
    ],
    "published": "2025-02-17T05:28:47+00:00",
    "summary": "Safety alignment is critical in pre-training large language models (LLMs) to generate responses aligned with human values and refuse harmful queries. Unlike LLM, the current safety alignment of VLMs is often achieved with post-hoc safety fine-tuning. However, these methods are less effective to white-box attacks. To address this, we propose $\\textit{Adversary-aware DPO (ADPO)}$, a novel training framework that explicitly considers adversarial. $\\textit{Adversary-aware DPO (ADPO)}$ integrates adversarial training into DPO to enhance the safety alignment of VLMs under worst-case adversarial perturbations. $\\textit{ADPO}$ introduces two key components: (1) an adversarial-trained reference model that generates human-preferred responses under worst-case perturbations, and (2) an adversarial-aware DPO loss that generates winner-loser pairs accounting for adversarial distortions. By combining these innovations, $\\textit{ADPO}$ ensures that VLMs remain robust and reliable even in the presence of sophisticated jailbreak attacks. Extensive experiments demonstrate that $\\textit{ADPO}$ outperforms baselines in the safety alignment and general utility of VLMs."
  },
  {
    "title": "AGrail: A Lifelong Agent Guardrail with Effective and Adaptive Safety Detection",
    "url": "http://arxiv.org/abs/2502.11448v2",
    "arxiv_id": "2502.11448v2",
    "authors": [
      "Weidi Luo",
      "Shenghong Dai",
      "Xiaogeng Liu",
      "Suman Banerjee",
      "Huan Sun",
      "Muhao Chen",
      "Chaowei Xiao"
    ],
    "published": "2025-02-17T05:12:33+00:00",
    "summary": "The rapid advancements in Large Language Models (LLMs) have enabled their deployment as autonomous agents for handling complex tasks in dynamic environments. These LLMs demonstrate strong problem-solving capabilities and adaptability to multifaceted scenarios. However, their use as agents also introduces significant risks, including task-specific risks, which are identified by the agent administrator based on the specific task requirements and constraints, and systemic risks, which stem from vulnerabilities in their design or interactions, potentially compromising confidentiality, integrity, or availability (CIA) of information and triggering security risks. Existing defense agencies fail to adaptively and effectively mitigate these risks. In this paper, we propose AGrail, a lifelong agent guardrail to enhance LLM agent safety, which features adaptive safety check generation, effective safety check optimization, and tool compatibility and flexibility. Extensive experiments demonstrate that AGrail not only achieves strong performance against task-specific and system risks but also exhibits transferability across different LLM agents' tasks."
  },
  {
    "title": "What's in a Query: Polarity-Aware Distribution-Based Fair Ranking",
    "url": "http://arxiv.org/abs/2502.11429v1",
    "arxiv_id": "2502.11429v1",
    "authors": [
      "Aparna Balagopalan",
      "Kai Wang",
      "Olawale Salaudeen",
      "Asia Biega",
      "Marzyeh Ghassemi"
    ],
    "published": "2025-02-17T04:38:36+00:00",
    "summary": "Machine learning-driven rankings, where individuals (or items) are ranked in response to a query, mediate search exposure or attention in a variety of safety-critical settings. Thus, it is important to ensure that such rankings are fair. Under the goal of equal opportunity, attention allocated to an individual on a ranking interface should be proportional to their relevance across search queries. In this work, we examine amortized fair ranking -- where relevance and attention are cumulated over a sequence of user queries to make fair ranking more feasible in practice. Unlike prior methods that operate on expected amortized attention for each individual, we define new divergence-based measures for attention distribution-based fairness in ranking (DistFaiR), characterizing unfairness as the divergence between the distribution of attention and relevance corresponding to an individual over time. This allows us to propose new definitions of unfairness, which are more reliable at test time. Second, we prove that group fairness is upper-bounded by individual fairness under this definition for a useful class of divergence measures, and experimentally show that maximizing individual fairness through an integer linear programming-based optimization is often beneficial to group fairness. Lastly, we find that prior research in amortized fair ranking ignores critical information about queries, potentially leading to a fairwashing risk in practice by making rankings appear more fair than they actually are."
  },
  {
    "title": "Detecting and Filtering Unsafe Training Data via Data Attribution",
    "url": "http://arxiv.org/abs/2502.11411v1",
    "arxiv_id": "2502.11411v1",
    "authors": [
      "Yijun Pan",
      "Taiwei Shi",
      "Jieyu Zhao",
      "Jiaqi W. Ma"
    ],
    "published": "2025-02-17T03:50:58+00:00",
    "summary": "Large language models (LLMs) are vulnerable to unsafe training data that even small amounts of unsafe data can lead to harmful model behaviors. Detecting and filtering such unsafe training data is essential for trustworthy model development. Current state-of-the-art (SOTA) approaches typically rely on training moderation classifiers which requires significant computational overhead and are limited to predefined taxonomies, making them less adaptable to evolving safety concerns. Moreover, these classifiers lack insight into the training process, limiting their effectiveness in filtering unsafe data. To address these limitations, we propose DABUF, leveraging data attribution to detect and filter unsafe training data by attributing harmful model outputs to influential training data points. DABUF enables flexible identification of various unsafe data types without predefined taxonomies. However, in practice, model outputs can be complex with combined safe linguistic features and unsafe content, leading to reduced attribution accuracy. In such cases, DABUF will integrate moderation classifiers to identify a minimal subset of unsafe training data for targeted attribution (such as jailbreak). When model outputs are relatively straightforward, DABUF uses model outputs directly as the attribution targets. We evaluate the performance on two different tasks: in filtering jailbreaking training data and in identifying and mitigating gender bias. DABUF outperforms SOTA approaches by up to 7.5\\% in detection AUPRC in jailbreaking scenarios, and 44.1\\% in detecting gender bias. Moreover, retraining on DABUF-filtered data leads to higher model safety across experiments, underscoring its versatility in addressing a broad spectrum of unsafe data issues."
  },
  {
    "title": "CCJA: Context-Coherent Jailbreak Attack for Aligned Large Language Models",
    "url": "http://arxiv.org/abs/2502.11379v1",
    "arxiv_id": "2502.11379v1",
    "authors": [
      "Guanghao Zhou",
      "Panjia Qiu",
      "Mingyuan Fan",
      "Cen Chen",
      "Mingyuan Chu",
      "Xin Zhang",
      "Jun Zhou"
    ],
    "published": "2025-02-17T02:49:26+00:00",
    "summary": "Despite explicit alignment efforts for large language models (LLMs), they can still be exploited to trigger unintended behaviors, a phenomenon known as \"jailbreaking.\" Current jailbreak attack methods mainly focus on discrete prompt manipulations targeting closed-source LLMs, relying on manually crafted prompt templates and persuasion rules. However, as the capabilities of open-source LLMs improve, ensuring their safety becomes increasingly crucial. In such an environment, the accessibility of model parameters and gradient information by potential attackers exacerbates the severity of jailbreak threats. To address this research gap, we propose a novel \\underline{C}ontext-\\underline{C}oherent \\underline{J}ailbreak \\underline{A}ttack (CCJA). We define jailbreak attacks as an optimization problem within the embedding space of masked language models. Through combinatorial optimization, we effectively balance the jailbreak attack success rate with semantic coherence. Extensive evaluations show that our method not only maintains semantic consistency but also surpasses state-of-the-art baselines in attack effectiveness. Additionally, by integrating semantically coherent jailbreak prompts generated by our method into widely used black-box methodologies, we observe a notable enhancement in their success rates when targeting closed-source commercial LLMs. This highlights the security threat posed by open-source LLMs to commercial counterparts. We will open-source our code if the paper is accepted."
  },
  {
    "title": "HI-GVF: Shared Control based on Human-Influenced Guiding Vector Fields for Human-multi-robot Cooperation",
    "url": "http://arxiv.org/abs/2502.11370v1",
    "arxiv_id": "2502.11370v1",
    "authors": [
      "Pengming Zhu",
      "Zongtan Zhou",
      "Weijia Yao",
      "Wei Dai",
      "Zhiwen Zeng",
      "Huimin Lu"
    ],
    "published": "2025-02-17T02:33:09+00:00",
    "summary": "Human-multi-robot shared control leverages human decision-making and robotic autonomy to enhance human-robot collaboration. While widely studied, existing systems often adopt a leader-follower model, limiting robot autonomy to some extent. Besides, a human is required to directly participate in the motion control of robots through teleoperation, which significantly burdens the operator. To alleviate these two issues, we propose a layered shared control computing framework using human-influenced guiding vector fields (HI-GVF) for human-robot collaboration. HI-GVF guides the multi-robot system along a desired path specified by the human. Then, an intention field is designed to merge the human and robot intentions, accelerating the propagation of the human intention within the multi-robot system. Moreover, we give the stability analysis of the proposed model and use collision avoidance based on safety barrier certificates to fine-tune the velocity. Eventually, considering the firefighting task as an example scenario, we conduct simulations and experiments using multiple human-robot interfaces (brain-computer interface, myoelectric wristband, eye-tracking), and the results demonstrate that our proposed approach boosts the effectiveness and performance of the task."
  },
  {
    "title": "Sparse Autoencoder Features for Classifications and Transferability",
    "url": "http://arxiv.org/abs/2502.11367v1",
    "arxiv_id": "2502.11367v1",
    "authors": [
      "Jack Gallifant",
      "Shan Chen",
      "Kuleen Sasse",
      "Hugo Aerts",
      "Thomas Hartvigsen",
      "Danielle S. Bitterman"
    ],
    "published": "2025-02-17T02:30:45+00:00",
    "summary": "Sparse Autoencoders (SAEs) provide potentials for uncovering structured, human-interpretable representations in Large Language Models (LLMs), making them a crucial tool for transparent and controllable AI systems. We systematically analyze SAE for interpretable feature extraction from LLMs in safety-critical classification tasks. Our framework evaluates (1) model-layer selection and scaling properties, (2) SAE architectural configurations, including width and pooling strategies, and (3) the effect of binarizing continuous SAE activations. SAE-derived features achieve macro F1 > 0.8, outperforming hidden-state and BoW baselines while demonstrating cross-model transfer from Gemma 2 2B to 9B-IT models. These features generalize in a zero-shot manner to cross-lingual toxicity detection and visual classification tasks. Our analysis highlights the significant impact of pooling strategies and binarization thresholds, showing that binarization offers an efficient alternative to traditional feature selection while maintaining or improving performance. These findings establish new best practices for SAE-based interpretability and enable scalable, transparent deployment of LLMs in real-world applications. Full repo: https://github.com/shan23chen/MOSAIC."
  },
  {
    "title": "VLDBench: Vision Language Models Disinformation Detection Benchmark",
    "url": "http://arxiv.org/abs/2502.11361v1",
    "arxiv_id": "2502.11361v1",
    "authors": [
      "Shaina Raza",
      "Ashmal Vayani",
      "Aditya Jain",
      "Aravind Narayanan",
      "Vahid Reza Khazaie",
      "Syed Raza Bashir",
      "Elham Dolatabadi",
      "Gias Uddin",
      "Christos Emmanouilidis",
      "Rizwan Qureshi",
      "Mubarak Shah"
    ],
    "published": "2025-02-17T02:18:47+00:00",
    "summary": "The rapid rise of AI-generated content has made detecting disinformation increasingly challenging. In particular, multimodal disinformation, i.e., online posts-articles that contain images and texts with fabricated information are specially designed to deceive. While existing AI safety benchmarks primarily address bias and toxicity, multimodal disinformation detection remains largely underexplored. To address this challenge, we present the Vision-Language Disinformation Detection Benchmark VLDBench, the first comprehensive benchmark for detecting disinformation across both unimodal (text-only) and multimodal (text and image) content, comprising 31,000} news article-image pairs, spanning 13 distinct categories, for robust evaluation. VLDBench features a rigorous semi-automated data curation pipeline, with 22 domain experts dedicating 300 plus hours} to annotation, achieving a strong inter-annotator agreement (Cohen kappa = 0.78). We extensively evaluate state-of-the-art Large Language Models (LLMs) and Vision-Language Models (VLMs), demonstrating that integrating textual and visual cues in multimodal news posts improves disinformation detection accuracy by 5 - 35 % compared to unimodal models. Developed in alignment with AI governance frameworks such as the EU AI Act, NIST guidelines, and the MIT AI Risk Repository 2024, VLDBench is expected to become a benchmark for detecting disinformation in online multi-modal contents. Our code and data will be publicly available."
  },
  {
    "title": "A Framework for Learning Scoring Rules in Autonomous Driving Planning Systems",
    "url": "http://arxiv.org/abs/2502.11352v1",
    "arxiv_id": "2502.11352v1",
    "authors": [
      "Zikang Xiong",
      "Joe Kurian Eappen",
      "Suresh Jagannathan"
    ],
    "published": "2025-02-17T02:06:57+00:00",
    "summary": "In autonomous driving systems, motion planning is commonly implemented as a two-stage process: first, a trajectory proposer generates multiple candidate trajectories, then a scoring mechanism selects the most suitable trajectory for execution. For this critical selection stage, rule-based scoring mechanisms are particularly appealing as they can explicitly encode driving preferences, safety constraints, and traffic regulations in a formalized, human-understandable format. However, manually crafting these scoring rules presents significant challenges: the rules often contain complex interdependencies, require careful parameter tuning, and may not fully capture the nuances present in real-world driving data. This work introduces FLoRA, a novel framework that bridges this gap by learning interpretable scoring rules represented in temporal logic. Our method features a learnable logic structure that captures nuanced relationships across diverse driving scenarios, optimizing both rules and parameters directly from real-world driving demonstrations collected in NuPlan. Our approach effectively learns to evaluate driving behavior even though the training data only contains positive examples (successful driving demonstrations). Evaluations in closed-loop planning simulations demonstrate that our learned scoring rules outperform existing techniques, including expert-designed rules and neural network scoring models, while maintaining interpretability. This work introduces a data-driven approach to enhance the scoring mechanism in autonomous driving systems, designed as a plug-in module to seamlessly integrate with various trajectory proposers. Our video and code are available on xiong.zikang.me/FLoRA."
  },
  {
    "title": "Leveraging Multimodal-LLMs Assisted by Instance Segmentation for Intelligent Traffic Monitoring",
    "url": "http://arxiv.org/abs/2502.11304v1",
    "arxiv_id": "2502.11304v1",
    "authors": [
      "Murat Arda Onsu",
      "Poonam Lohan",
      "Burak Kantarci",
      "Aisha Syed",
      "Matthew Andrews",
      "Sean Kennedy"
    ],
    "published": "2025-02-16T23:03:26+00:00",
    "summary": "A robust and efficient traffic monitoring system is essential for smart cities and Intelligent Transportation Systems (ITS), using sensors and cameras to track vehicle movements, optimize traffic flow, reduce congestion, enhance road safety, and enable real-time adaptive traffic control. Traffic monitoring models must comprehensively understand dynamic urban conditions and provide an intuitive user interface for effective management. This research leverages the LLaVA visual grounding multimodal large language model (LLM) for traffic monitoring tasks on the real-time Quanser Interactive Lab simulation platform, covering scenarios like intersections, congestion, and collisions. Cameras placed at multiple urban locations collect real-time images from the simulation, which are fed into the LLaVA model with queries for analysis. An instance segmentation model integrated into the cameras highlights key elements such as vehicles and pedestrians, enhancing training and throughput. The system achieves 84.3% accuracy in recognizing vehicle locations and 76.4% in determining steering direction, outperforming traditional models."
  },
  {
    "title": "MC-BEVRO: Multi-Camera Bird Eye View Road Occupancy Detection for Traffic Monitoring",
    "url": "http://arxiv.org/abs/2502.11287v1",
    "arxiv_id": "2502.11287v1",
    "authors": [
      "Arpitsinh Vaghela",
      "Duo Lu",
      "Aayush Atul Verma",
      "Bharatesh Chakravarthi",
      "Hua Wei",
      "Yezhou Yang"
    ],
    "published": "2025-02-16T22:03:03+00:00",
    "summary": "Single camera 3D perception for traffic monitoring faces significant challenges due to occlusion and limited field of view. Moreover, fusing information from multiple cameras at the image feature level is difficult because of different view angles. Further, the necessity for practical implementation and compatibility with existing traffic infrastructure compounds these challenges. To address these issues, this paper introduces a novel Bird's-Eye-View road occupancy detection framework that leverages multiple roadside cameras to overcome the aforementioned limitations. To facilitate the framework's development and evaluation, a synthetic dataset featuring diverse scenes and varying camera configurations is generated using the CARLA simulator. A late fusion and three early fusion methods were implemented within the proposed framework, with performance further enhanced by integrating backgrounds. Extensive evaluations were conducted to analyze the impact of multi-camera inputs and varying BEV occupancy map sizes on model performance. Additionally, a real-world data collection pipeline was developed to assess the model's ability to generalize to real-world environments. The sim-to-real capabilities of the model were evaluated using zero-shot and few-shot fine-tuning, demonstrating its potential for practical application. This research aims to advance perception systems in traffic monitoring, contributing to improved traffic management, operational efficiency, and road safety."
  },
  {
    "title": "Soteria: Language-Specific Functional Parameter Steering for Multilingual Safety Alignment",
    "url": "http://arxiv.org/abs/2502.11244v1",
    "arxiv_id": "2502.11244v1",
    "authors": [
      "Somnath Banerjee",
      "Sayan Layek",
      "Pratyush Chatterjee",
      "Animesh Mukherjee",
      "Rima Hazra"
    ],
    "published": "2025-02-16T19:44:01+00:00",
    "summary": "Ensuring consistent safety across multiple languages remains a significant challenge for large language models (LLMs). We introduce Soteria, a lightweight yet powerful strategy that locates and minimally adjusts the \"functional heads\" most responsible for harmful content generation in each language. By altering only a fraction of parameters, Soteria drastically reduces policy violations without sacrificing overall model performance, even in low-resource settings. To rigorously evaluate our approach, we also present XThreatBench, a specialized multilingual dataset capturing fine-grained harmful behaviors drawn from real policy guidelines. Experiments with leading open-source LLMs (e.g., Llama, Qwen, Mistral) show that Soteria consistently improves safety metrics across high-, mid-, and low-resource languages. These findings highlight a promising path toward scalable, linguistically attuned, and ethically aligned LLMs worldwide."
  },
  {
    "title": "LLMs and Childhood Safety: Identifying Risks and Proposing a Protection Framework for Safe Child-LLM Interaction",
    "url": "http://arxiv.org/abs/2502.11242v1",
    "arxiv_id": "2502.11242v1",
    "authors": [
      "Junfeng Jiao",
      "Saleh Afroogh",
      "Kevin Chen",
      "Abhejay Murali",
      "David Atkinson",
      "Amit Dhurandhar"
    ],
    "published": "2025-02-16T19:39:48+00:00",
    "summary": "This study examines the growing use of Large Language Models (LLMs) in child-centered applications, highlighting safety and ethical concerns such as bias, harmful content, and cultural insensitivity. Despite their potential to enhance learning, there is a lack of standardized frameworks to mitigate these risks. Through a systematic literature review, we identify key parental and empirical concerns, including toxicity and ethical breaches in AI outputs. Moreover, to address these issues, this paper proposes a protection framework for safe Child-LLM interaction, incorporating metrics for content safety, behavioral ethics, and cultural sensitivity. The framework provides practical tools for evaluating LLM safety, offering guidance for developers, policymakers, and educators to ensure responsible AI deployment for children."
  },
  {
    "title": "Can't See the Forest for the Trees: Benchmarking Multimodal Safety Awareness for Multimodal LLMs",
    "url": "http://arxiv.org/abs/2502.11184v1",
    "arxiv_id": "2502.11184v1",
    "authors": [
      "Wenxuan Wang",
      "Xiaoyuan Liu",
      "Kuiyi Gao",
      "Jen-tse Huang",
      "Youliang Yuan",
      "Pinjia He",
      "Shuai Wang",
      "Zhaopeng Tu"
    ],
    "published": "2025-02-16T16:12:40+00:00",
    "summary": "Multimodal Large Language Models (MLLMs) have expanded the capabilities of traditional language models by enabling interaction through both text and images. However, ensuring the safety of these models remains a significant challenge, particularly in accurately identifying whether multimodal content is safe or unsafe-a capability we term safety awareness. In this paper, we introduce MMSafeAware, the first comprehensive multimodal safety awareness benchmark designed to evaluate MLLMs across 29 safety scenarios with 1500 carefully curated image-prompt pairs. MMSafeAware includes both unsafe and over-safety subsets to assess models abilities to correctly identify unsafe content and avoid over-sensitivity that can hinder helpfulness. Evaluating nine widely used MLLMs using MMSafeAware reveals that current models are not sufficiently safe and often overly sensitive; for example, GPT-4V misclassifies 36.1% of unsafe inputs as safe and 59.9% of benign inputs as unsafe. We further explore three methods to improve safety awareness-prompting-based approaches, visual contrastive decoding, and vision-centric reasoning fine-tuning-but find that none achieve satisfactory performance. Our findings highlight the profound challenges in developing MLLMs with robust safety awareness, underscoring the need for further research in this area. All the code and data will be publicly available to facilitate future research."
  },
  {
    "title": "Quantifying the Capability Boundary of DeepSeek Models: An Application-Driven Performance Analysis",
    "url": "http://arxiv.org/abs/2502.11164v1",
    "arxiv_id": "2502.11164v1",
    "authors": [
      "Shiguo Lian",
      "Kaikai Zhao",
      "Xuejiao Lei",
      "Ning Wang",
      "Zhenhong Long",
      "Peijun Yang",
      "Minjie Hua",
      "Chaoyang Ma",
      "Wen Liu",
      "Kai Wang",
      "Zhaoxiang Liu"
    ],
    "published": "2025-02-16T15:29:58+00:00",
    "summary": "DeepSeek-R1, known for its low training cost and exceptional reasoning capabilities, has achieved state-of-the-art performance on various benchmarks. However, detailed evaluations from the perspective of real-world applications are lacking, making it challenging for users to select the most suitable DeepSeek models for their specific needs. To address this gap, we evaluate the DeepSeek-V3, DeepSeek-R1, DeepSeek-R1-Distill-Qwen series, and DeepSeek-R1-Distill-Llama series on A-Eval, an application-driven benchmark. By comparing original instruction-tuned models with their distilled counterparts, we analyze how reasoning enhancements impact performance across diverse practical tasks. Our results show that reasoning-enhanced models, while generally powerful, do not universally outperform across all tasks, with performance gains varying significantly across tasks and models. To further assist users in model selection, we quantify the capability boundary of DeepSeek models through performance tier classifications and intuitive line charts. Specific examples provide actionable insights to help users select and deploy the most cost-effective DeepSeek models, ensuring optimal performance and resource efficiency in real-world applications."
  },
  {
    "title": "Safety Evaluation of DeepSeek Models in Chinese Contexts",
    "url": "http://arxiv.org/abs/2502.11137v1",
    "arxiv_id": "2502.11137v1",
    "authors": [
      "Wenjing Zhang",
      "Xuejiao Lei",
      "Zhaoxiang Liu",
      "Ning Wang",
      "Zhenhong Long",
      "Peijun Yang",
      "Jiaojiao Zhao",
      "Minjie Hua",
      "Chaoyang Ma",
      "Kai Wang",
      "Shiguo Lian"
    ],
    "published": "2025-02-16T14:05:54+00:00",
    "summary": "Recently, the DeepSeek series of models, leveraging their exceptional reasoning capabilities and open-source strategy, is reshaping the global AI landscape. Despite these advantages, they exhibit significant safety deficiencies. Research conducted by Robust Intelligence, a subsidiary of Cisco, in collaboration with the University of Pennsylvania, revealed that DeepSeek-R1 has a 100\\% attack success rate when processing harmful prompts. Additionally, multiple safety companies and research institutions have confirmed critical safety vulnerabilities in this model. As models demonstrating robust performance in Chinese and English, DeepSeek models require equally crucial safety assessments in both language contexts. However, current research has predominantly focused on safety evaluations in English environments, leaving a gap in comprehensive assessments of their safety performance in Chinese contexts. In response to this gap, this study introduces CHiSafetyBench, a Chinese-specific safety evaluation benchmark. This benchmark systematically evaluates the safety of DeepSeek-R1 and DeepSeek-V3 in Chinese contexts, revealing their performance across safety categories. The experimental results quantify the deficiencies of these two models in Chinese contexts, providing key insights for subsequent improvements."
  },
  {
    "title": "Ramp Up NTT in Record Time using GPU-Accelerated Algorithms and LLM-based Code Generation",
    "url": "http://arxiv.org/abs/2502.11110v1",
    "arxiv_id": "2502.11110v1",
    "authors": [
      "Yu Cui",
      "Hang Fu",
      "Licheng Wang",
      "Haibin Zhang"
    ],
    "published": "2025-02-16T12:53:23+00:00",
    "summary": "Homomorphic encryption (HE) is a core building block in privacy-preserving machine learning (PPML), but HE is also widely known as its efficiency bottleneck. Therefore, many GPU-accelerated cryptographic schemes have been proposed to improve the performance of HE. However, these methods often require complex modifications tailored to specific algorithms and are tightly coupled with specific GPU and operating systems. It is interesting to ask how to generally offer more practical GPU-accelerated cryptographic algorithm implementations. Given the powerful code generation capabilities of large language models (LLMs), we aim to explore their potential to automatically generate practical GPU-friendly algorithm code using CPU-friendly code. In this paper, we focus on number theoretic transform (NTT) -- the core mechanism of HE. We first develop and optimize a GPU-friendly NTT (GNTT) family that exploits PyTorch's fast matrix computation and precomputation, achieving an approximately 62x speedup -- a significant boost over existing ones. Then we explore GPU-friendly code generation using various LLMs, including DeepSeek-R1, OpenAI o1 and o3-mini. We discover many interesting findings throughout the process. For instance, somewhat surprisingly, our experiments demonstrate that DeepSeek-R1 significantly outperforms OpenAI o3-mini and o1, but still cannot beat our optimized protocol. The findings provide valuable insights for turbocharging PPML and enhancing code generation capabilities of LLMs. Codes are available at: https://github.com/LMPC-Lab/GenGPUCrypto."
  },
  {
    "title": "Knowledge Graph-Driven Retrieval-Augmented Generation: Integrating Deepseek-R1 with Weaviate for Advanced Chatbot Applications",
    "url": "http://arxiv.org/abs/2502.11108v1",
    "arxiv_id": "2502.11108v1",
    "authors": [
      "Alexandru Lecu",
      "Adrian Groza",
      "Lezan Hawizy"
    ],
    "published": "2025-02-16T12:52:28+00:00",
    "summary": "Large language models (LLMs) have significantly advanced the field of natural language generation. However, they frequently generate unverified outputs, which compromises their reliability in critical applications. In this study, we propose an innovative framework that combines structured biomedical knowledge with LLMs through a retrieval-augmented generation technique. Our system develops a thorough knowledge graph by identifying and refining causal relationships and named entities from medical abstracts related to age-related macular degeneration (AMD). Using a vector-based retrieval process and a locally deployed language model, our framework produces responses that are both contextually relevant and verifiable, with direct references to clinical evidence. Experimental results show that this method notably decreases hallucinations, enhances factual precision, and improves the clarity of generated responses, providing a robust solution for advanced biomedical chatbot applications."
  },
  {
    "title": "Talk Structurally, Act Hierarchically: A Collaborative Framework for LLM Multi-Agent Systems",
    "url": "http://arxiv.org/abs/2502.11098v1",
    "arxiv_id": "2502.11098v1",
    "authors": [
      "Zhao Wang",
      "Sota Moriyama",
      "Wei-Yao Wang",
      "Briti Gangopadhyay",
      "Shingo Takamatsu"
    ],
    "published": "2025-02-16T12:26:58+00:00",
    "summary": "Recent advancements in LLM-based multi-agent (LLM-MA) systems have shown promise, yet significant challenges remain in managing communication and refinement when agents collaborate on complex tasks. In this paper, we propose \\textit{Talk Structurally, Act Hierarchically (TalkHier)}, a novel framework that introduces a structured communication protocol for context-rich exchanges and a hierarchical refinement system to address issues such as incorrect outputs, falsehoods, and biases. \\textit{TalkHier} surpasses various types of SoTA, including inference scaling model (OpenAI-o1), open-source multi-agent models (e.g., AgentVerse), and majority voting strategies on current LLM and single-agent baselines (e.g., ReAct, GPT4o), across diverse tasks, including open-domain question answering, domain-specific selective questioning, and practical advertisement text generation. These results highlight its potential to set a new standard for LLM-MA systems, paving the way for more effective, adaptable, and collaborative multi-agent frameworks. The code is available https://github.com/sony/talkhier."
  },
  {
    "title": "Mixture of Tunable Experts - Behavior Modification of DeepSeek-R1 at Inference Time",
    "url": "http://arxiv.org/abs/2502.11096v1",
    "arxiv_id": "2502.11096v1",
    "authors": [
      "Robert Dahlke",
      "Henrik Klagges",
      "Dan Zecha",
      "Benjamin Merkel",
      "Sven Rohr",
      "Fabian Klemm"
    ],
    "published": "2025-02-16T12:24:39+00:00",
    "summary": "We present the Mixture-of-Tunable-Experts (MoTE), a method that extends the Mixture-of-Experts architecture of Large Language Models (LLMs). Without additional training, MoTE enables meaningful and focused behavior changes in LLMs on-the-fly during inference time.   By analyzing the digital LLM brain of DeepSeek-R1 using a technique we dub 'functional Token Resonance Imaging' (fTRI) - inspired by fMRI and using prompts designed to elicit specific behavior (e.g., 'What happened {time}{place}?') - we empirically identify distinctive experts associated with behaviors like refusal responses.   Using MoTE we are able to intervene and control such specific behavior. We switched off the top 10 most refusal-relevant experts (0.07% of R1's 14,848 routed experts), achieving a 52% refusal reduction on sensitive reference prompts without performance degradation on MT-Bench. Random expert deactivation resulted in smaller behavioral shifts with increased noise, whereas forced expert activation led to significantly higher refusal rates.   Our approach shares similarities with sparse autoencoders (SAEs) in terms of explainability and steerability. Unlike SAEs, MoTE does not require large training efforts, as within MoEs with a vast number of experts, specialization already emerged naturally during pretraining.   Our findings suggest that significant functional mechanisms in Mixture-of-Experts architectures can at least partially be localized in a small number of specific experts, rather than being distributed throughout the model's weights. Expert subgroups can be tuned to trigger significant behavior variations, providing insights into the inner workings of LLMs."
  },
  {
    "title": "SafeDialBench: A Fine-Grained Safety Benchmark for Large Language Models in Multi-Turn Dialogues with Diverse Jailbreak Attacks",
    "url": "http://arxiv.org/abs/2502.11090v2",
    "arxiv_id": "2502.11090v2",
    "authors": [
      "Hongye Cao",
      "Yanming Wang",
      "Sijia Jing",
      "Ziyue Peng",
      "Zhixin Bai",
      "Zhe Cao",
      "Meng Fang",
      "Fan Feng",
      "Boyan Wang",
      "Jiaheng Liu",
      "Tianpei Yang",
      "Jing Huo",
      "Yang Gao",
      "Fanyu Meng",
      "Xi Yang",
      "Chao Deng",
      "Junlan Feng"
    ],
    "published": "2025-02-16T12:08:08+00:00",
    "summary": "With the rapid advancement of Large Language Models (LLMs), the safety of LLMs has been a critical concern requiring precise assessment. Current benchmarks primarily concentrate on single-turn dialogues or a single jailbreak attack method to assess the safety. Additionally, these benchmarks have not taken into account the LLM's capability of identifying and handling unsafe information in detail. To address these issues, we propose a fine-grained benchmark SafeDialBench for evaluating the safety of LLMs across various jailbreak attacks in multi-turn dialogues. Specifically, we design a two-tier hierarchical safety taxonomy that considers 6 safety dimensions and generates more than 4000 multi-turn dialogues in both Chinese and English under 22 dialogue scenarios. We employ 7 jailbreak attack strategies, such as reference attack and purpose reverse, to enhance the dataset quality for dialogue generation. Notably, we construct an innovative assessment framework of LLMs, measuring capabilities in detecting, and handling unsafe information and maintaining consistency when facing jailbreak attacks. Experimental results across 17 LLMs reveal that Yi-34B-Chat and GLM4-9B-Chat demonstrate superior safety performance, while Llama3.1-8B-Instruct and o3-mini exhibit safety vulnerabilities."
  },
  {
    "title": "Rewrite to Jailbreak: Discover Learnable and Transferable Implicit Harmfulness Instruction",
    "url": "http://arxiv.org/abs/2502.11084v1",
    "arxiv_id": "2502.11084v1",
    "authors": [
      "Yuting Huang",
      "Chengyuan Liu",
      "Yifeng Feng",
      "Chao Wu",
      "Fei Wu",
      "Kun Kuang"
    ],
    "published": "2025-02-16T11:43:39+00:00",
    "summary": "As Large Language Models (LLMs) are widely applied in various domains, the safety of LLMs is increasingly attracting attention to avoid their powerful capabilities being misused. Existing jailbreak methods create a forced instruction-following scenario, or search adversarial prompts with prefix or suffix tokens to achieve a specific representation manually or automatically. However, they suffer from low efficiency and explicit jailbreak patterns, far from the real deployment of mass attacks to LLMs. In this paper, we point out that simply rewriting the original instruction can achieve a jailbreak, and we find that this rewriting approach is learnable and transferable. We propose the Rewrite to Jailbreak (R2J) approach, a transferable black-box jailbreak method to attack LLMs by iteratively exploring the weakness of the LLMs and automatically improving the attacking strategy. The jailbreak is more efficient and hard to identify since no additional features are introduced. Extensive experiments and analysis demonstrate the effectiveness of R2J, and we find that the jailbreak is also transferable to multiple datasets and various types of models with only a few queries. We hope our work motivates further investigation of LLM safety."
  },
  {
    "title": "ClimateLLM: Efficient Weather Forecasting via Frequency-Aware Large Language Models",
    "url": "http://arxiv.org/abs/2502.11059v1",
    "arxiv_id": "2502.11059v1",
    "authors": [
      "Shixuan Li",
      "Wei Yang",
      "Peiyu Zhang",
      "Xiongye Xiao",
      "Defu Cao",
      "Yuehan Qin",
      "Xiaole Zhang",
      "Yue Zhao",
      "Paul Bogdan"
    ],
    "published": "2025-02-16T09:57:50+00:00",
    "summary": "Weather forecasting is crucial for public safety, disaster prevention and mitigation, agricultural production, and energy management, with global relevance. Although deep learning has significantly advanced weather prediction, current methods face critical limitations: (i) they often struggle to capture both dynamic temporal dependencies and short-term abrupt changes, making extreme weather modeling difficult; (ii) they incur high computational costs due to extensive training and resource requirements; (iii) they have limited adaptability to multi-scale frequencies, leading to challenges when separating global trends from local fluctuations. To address these issues, we propose ClimateLLM, a foundation model for weather forecasting. It captures spatiotemporal dependencies via a cross-temporal and cross-spatial collaborative modeling framework that integrates Fourier-based frequency decomposition with Large Language Models (LLMs) to strengthen spatial and temporal modeling. Our framework uses a Mixture-of-Experts (MoE) mechanism that adaptively processes different frequency components, enabling efficient handling of both global signals and localized extreme events. In addition, we introduce a cross-temporal and cross-spatial dynamic prompting mechanism, allowing LLMs to incorporate meteorological patterns across multiple scales effectively. Extensive experiments on real-world datasets show that ClimateLLM outperforms state-of-the-art approaches in accuracy and efficiency, as a scalable solution for global weather forecasting."
  },
  {
    "title": "A Physics-Informed Machine Learning Framework for Safe and Optimal Control of Autonomous Systems",
    "url": "http://arxiv.org/abs/2502.11057v1",
    "arxiv_id": "2502.11057v1",
    "authors": [
      "Manan Tayal",
      "Aditya Singh",
      "Shishir Kolathaya",
      "Somil Bansal"
    ],
    "published": "2025-02-16T09:46:17+00:00",
    "summary": "As autonomous systems become more ubiquitous in daily life, ensuring high performance with guaranteed safety is crucial. However, safety and performance could be competing objectives, which makes their co-optimization difficult. Learning-based methods, such as Constrained Reinforcement Learning (CRL), achieve strong performance but lack formal safety guarantees due to safety being enforced as soft constraints, limiting their use in safety-critical settings. Conversely, formal methods such as Hamilton-Jacobi (HJ) Reachability Analysis and Control Barrier Functions (CBFs) provide rigorous safety assurances but often neglect performance, resulting in overly conservative controllers. To bridge this gap, we formulate the co-optimization of safety and performance as a state-constrained optimal control problem, where performance objectives are encoded via a cost function and safety requirements are imposed as state constraints. We demonstrate that the resultant value function satisfies a Hamilton-Jacobi-Bellman (HJB) equation, which we approximate efficiently using a novel physics-informed machine learning framework. In addition, we introduce a conformal prediction-based verification strategy to quantify the learning errors, recovering a high-confidence safety value function, along with a probabilistic error bound on performance degradation. Through several case studies, we demonstrate the efficacy of the proposed framework in enabling scalable learning of safe and performant controllers for complex, high-dimensional autonomous systems."
  },
  {
    "title": "Reasoning-Augmented Conversation for Multi-Turn Jailbreak Attacks on Large Language Models",
    "url": "http://arxiv.org/abs/2502.11054v2",
    "arxiv_id": "2502.11054v2",
    "authors": [
      "Zonghao Ying",
      "Deyue Zhang",
      "Zonglei Jing",
      "Yisong Xiao",
      "Quanchen Zou",
      "Aishan Liu",
      "Siyuan Liang",
      "Xiangzheng Zhang",
      "Xianglong Liu",
      "Dacheng Tao"
    ],
    "published": "2025-02-16T09:27:44+00:00",
    "summary": "Multi-turn jailbreak attacks simulate real-world human interactions by engaging large language models (LLMs) in iterative dialogues, exposing critical safety vulnerabilities. However, existing methods often struggle to balance semantic coherence with attack effectiveness, resulting in either benign semantic drift or ineffective detection evasion. To address this challenge, we propose Reasoning-Augmented Conversation, a novel multi-turn jailbreak framework that reformulates harmful queries into benign reasoning tasks and leverages LLMs' strong reasoning capabilities to compromise safety alignment. Specifically, we introduce an attack state machine framework to systematically model problem translation and iterative reasoning, ensuring coherent query generation across multiple turns. Building on this framework, we design gain-guided exploration, self-play, and rejection feedback modules to preserve attack semantics, enhance effectiveness, and sustain reasoning-driven attack progression. Extensive experiments on multiple LLMs demonstrate that RACE achieves state-of-the-art attack effectiveness in complex conversational scenarios, with attack success rates (ASRs) increasing by up to 96%. Notably, our approach achieves ASRs of 82% and 92% against leading commercial models, OpenAI o1 and DeepSeek R1, underscoring its potency. We release our code at https://github.com/NY1024/RACE to facilitate further research in this critical domain."
  },
  {
    "title": "Prompt Inject Detection with Generative Explanation as an Investigative Tool",
    "url": "http://arxiv.org/abs/2502.11006v1",
    "arxiv_id": "2502.11006v1",
    "authors": [
      "Jonathan Pan",
      "Swee Liang Wong",
      "Yidi Yuan",
      "Xin Wei Chia"
    ],
    "published": "2025-02-16T06:16:00+00:00",
    "summary": "Large Language Models (LLMs) are vulnerable to adversarial prompt based injects. These injects could jailbreak or exploit vulnerabilities within these models with explicit prompt requests leading to undesired responses. In the context of investigating prompt injects, the challenge is the sheer volume of input prompts involved that are likely to be largely benign. This investigative challenge is further complicated by the semantics and subjectivity of the input prompts involved in the LLM conversation with its user and the context of the environment to which the conversation is being carried out. Hence, the challenge for AI security investigators would be two-fold. The first is to identify adversarial prompt injects and then to assess whether the input prompt is contextually benign or adversarial. For the first step, this could be done using existing AI security solutions like guardrails to detect and protect the LLMs. Guardrails have been developed using a variety of approaches. A popular approach is to use signature based. Another popular approach to develop AI models to classify such prompts include the use of NLP based models like a language model. However, in the context of conducting an AI security investigation of prompt injects, these guardrails lack the ability to aid investigators in triaging or assessing the identified input prompts. In this applied research exploration, we explore the use of a text generation capabilities of LLM to detect prompt injects and generate explanation for its detections to aid AI security investigators in assessing and triaging of such prompt inject detections. The practical benefit of such a tool is to ease the task of conducting investigation into prompt injects."
  },
  {
    "title": "Fine-Tuning Hard-to-Simulate Objectives for Quadruped Locomotion: A Case Study on Total Power Saving",
    "url": "http://arxiv.org/abs/2502.10956v1",
    "arxiv_id": "2502.10956v1",
    "authors": [
      "Ruiqian Nai",
      "Jiacheng You",
      "Liu Cao",
      "Hanchen Cui",
      "Shiyuan Zhang",
      "Huazhe Xu",
      "Yang Gao"
    ],
    "published": "2025-02-16T02:22:50+00:00",
    "summary": "Legged locomotion is not just about mobility; it also encompasses crucial objectives such as energy efficiency, safety, and user experience, which are vital for real-world applications. However, key factors such as battery power consumption and stepping noise are often inaccurately modeled or missing in common simulators, leaving these aspects poorly optimized or unaddressed by current sim-to-real methods. Hand-designed proxies, such as mechanical power and foot contact forces, have been used to address these challenges but are often problem-specific and inaccurate.   In this paper, we propose a data-driven framework for fine-tuning locomotion policies, targeting these hard-to-simulate objectives. Our framework leverages real-world data to model these objectives and incorporates the learned model into simulation for policy improvement. We demonstrate the effectiveness of our framework on power saving for quadruped locomotion, achieving a significant 24-28\\% net reduction in total power consumption from the battery pack at various speeds. In essence, our approach offers a versatile solution for optimizing hard-to-simulate objectives in quadruped locomotion, providing an easy-to-adapt paradigm for continual improving with real-world knowledge. Project page https://hard-to-sim.github.io/."
  },
  {
    "title": "Fundamental Principles of Linguistic Structure are Not Represented by o3",
    "url": "http://arxiv.org/abs/2502.10934v1",
    "arxiv_id": "2502.10934v1",
    "authors": [
      "Elliot Murphy",
      "Evelina Leivada",
      "Vittoria Dentella",
      "Fritz Gunther",
      "Gary Marcus"
    ],
    "published": "2025-02-15T23:53:31+00:00",
    "summary": "A core component of a successful artificial general intelligence would be the rapid creation and manipulation of grounded compositional abstractions and the demonstration of expertise in the family of recursive hierarchical syntactic objects necessary for the creative use of human language. We evaluated the recently released o3 model (OpenAI; o3-mini-high) and discovered that while it succeeds on some basic linguistic tests relying on linear, surface statistics (e.g., the Strawberry Test), it fails to generalize basic phrase structure rules; it fails with comparative sentences involving semantically illegal cardinality comparisons ('Escher sentences'); its fails to correctly rate and explain acceptability dynamics; and it fails to distinguish between instructions to generate unacceptable semantic vs. unacceptable syntactic outputs. When tasked with generating simple violations of grammatical rules, it is seemingly incapable of representing multiple parses to evaluate against various possible semantic interpretations. In stark contrast to many recent claims that artificial language models are on the verge of replacing the field of linguistics, our results suggest not only that deep learning is hitting a wall with respect to compositionality (Marcus 2022), but that it is hitting [a [stubbornly [resilient wall]]] that cannot readily be surmounted to reach human-like compositional reasoning simply through more compute."
  }
]