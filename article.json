[
  {
    "title": "Fight Fire with Fire: Defending Against Malicious RL Fine-Tuning via Reward Neutralization",
    "url": "http://arxiv.org/abs/2505.04578v1",
    "arxiv_id": "2505.04578v1",
    "authors": [
      "Wenjun Cao"
    ],
    "published": "2025-05-07T17:18:48+00:00",
    "summary": "Reinforcement learning (RL) fine-tuning transforms large language models while creating a vulnerability we experimentally verify: Our experiment shows that malicious RL fine-tuning dismantles safety guardrails with remarkable efficiency, requiring only 50 steps and minimal adversarial prompts, with harmful escalating from 0-2 to 7-9. This attack vector particularly threatens open-source models with parameter-level access. Existing defenses targeting supervised fine-tuning prove ineffective against RL's dynamic feedback mechanisms. We introduce Reward Neutralization, the first defense framework specifically designed against RL fine-tuning attacks, establishing concise rejection patterns that render malicious reward signals ineffective. Our approach trains models to produce minimal-information rejections that attackers cannot exploit, systematically neutralizing attempts to optimize toward harmful outputs. Experiments validate that our approach maintains low harmful scores (no greater than 2) after 200 attack steps, while standard models rapidly deteriorate. This work provides the first constructive proof that robust defense against increasingly accessible RL attacks is achievable, addressing a critical security gap for open-weight models."
  },
  {
    "title": "Stow: Robotic Packing of Items into Fabric Pods",
    "url": "http://arxiv.org/abs/2505.04572v1",
    "arxiv_id": "2505.04572v1",
    "authors": [
      "Nicolas Hudson",
      "Josh Hooks",
      "Rahul Warrier",
      "Curt Salisbury",
      "Ross Hartley",
      "Kislay Kumar",
      "Bhavana Chandrashekhar",
      "Paul Birkmeyer",
      "Bosch Tang",
      "Matt Frost",
      "Shantanu Thakar",
      "Tony Piaskowy",
      "Petter Nilsson",
      "Josh Petersen",
      "Neel Doshi",
      "Alan Slatter",
      "Ankit Bhatia",
      "Cassie Meeker",
      "Yuechuan Xue",
      "Dylan Cox",
      "Alex Kyriazis",
      "Bai Lou",
      "Nadeem Hasan",
      "Asif Rana",
      "Nikhil Chacko",
      "Ruinian Xu",
      "Siamak Faal",
      "Esi Seraj",
      "Mudit Agrawal",
      "Kevin Jamieson",
      "Alessio Bisagni",
      "Valerie Samzun",
      "Christine Fuller",
      "Alex Keklak",
      "Alex Frenkel",
      "Lillian Ratliff",
      "Aaron Parness"
    ],
    "published": "2025-05-07T17:07:09+00:00",
    "summary": "This paper presents a compliant manipulation system capable of placing items onto densely packed shelves. The wide diversity of items and strict business requirements for high producing rates and low defect generation have prohibited warehouse robotics from performing this task. Our innovations in hardware, perception, decision-making, motion planning, and control have enabled this system to perform over 500,000 stows in a large e-commerce fulfillment center. The system achieves human levels of packing density and speed while prioritizing work on overhead shelves to enhance the safety of humans working alongside the robots."
  },
  {
    "title": "Runtime Advocates: A Persona-Driven Framework for Requirements@Runtime Decision Support",
    "url": "http://arxiv.org/abs/2505.04551v1",
    "arxiv_id": "2505.04551v1",
    "authors": [
      "Demetrius Hernandez",
      "Jane Cleland-Huang"
    ],
    "published": "2025-05-07T16:31:38+00:00",
    "summary": "Complex systems, such as small Uncrewed Aerial Systems (sUAS) swarms dispatched for emergency response, often require dynamic reconfiguration at runtime under the supervision of human operators. This introduces human-on-the-loop requirements, where evolving needs shape ongoing system functionality and behaviors. While traditional personas support upfront, static requirements elicitation, we propose a persona-based advocate framework for runtime requirements engineering to provide ethically informed, safety-driven, and regulatory-aware decision support. Our approach extends standard personas into event-driven personas. When triggered by events such as adverse environmental conditions, evolving mission state, or operational constraints, the framework updates the sUAS operator's view of the personas, ensuring relevance to current conditions. We create three key advocate personas, namely Safety Controller, Ethical Governor, and Regulatory Auditor, to manage trade-offs among risk, ethical considerations, and regulatory compliance. We perform a proof-of-concept validation in an emergency response scenario using sUAS, showing how our advocate personas provide context-aware guidance grounded in safety, regulatory, and ethical constraints. By evolving static, design-time personas into adaptive, event-driven advocates, the framework surfaces mission-critical runtime requirements in response to changing conditions. These requirements shape operator decisions in real time, aligning actions with the operational demands of the moment."
  },
  {
    "title": "Pangu Ultra MoE: How to Train Your Big MoE on Ascend NPUs",
    "url": "http://arxiv.org/abs/2505.04519v1",
    "arxiv_id": "2505.04519v1",
    "authors": [
      "Yehui Tang",
      "Yichun Yin",
      "Yaoyuan Wang",
      "Hang Zhou",
      "Yu Pan",
      "Wei Guo",
      "Ziyang Zhang",
      "Miao Rang",
      "Fangcheng Liu",
      "Naifu Zhang",
      "Binghan Li",
      "Yonghan Dong",
      "Xiaojun Meng",
      "Yasheng Wang",
      "Dong Li",
      "Yin Li",
      "Dandan Tu",
      "Can Chen",
      "Youliang Yan",
      "Fisher Yu",
      "Ruiming Tang",
      "Yunhe Wang",
      "Botian Huang",
      "Bo Wang",
      "Boxiao Liu",
      "Changzheng Zhang",
      "Da Kuang",
      "Fei Liu",
      "Gang Huang",
      "Jiansheng Wei",
      "Jiarui Qin",
      "Jie Ran",
      "Jinpeng Li",
      "Jun Zhao",
      "Liang Dai",
      "Lin Li",
      "Liqun Deng",
      "Peifeng Qin",
      "Pengyuan Zeng",
      "Qiang Gu",
      "Shaohua Tang",
      "Shengjun Cheng",
      "Tao Gao",
      "Tao Yu",
      "Tianshu Li",
      "Tianyu Bi",
      "Wei He",
      "Weikai Mao",
      "Wenyong Huang",
      "Wulong Liu",
      "Xiabing Li",
      "Xianzhi Yu",
      "Xueyu Wu",
      "Xu He",
      "Yangkai Du",
      "Yan Xu",
      "Ye Tian",
      "Yimeng Wu",
      "Yongbing Huang",
      "Yong Tian",
      "Yong Zhu",
      "Yue Li",
      "Yufei Wang",
      "Yuhang Gai",
      "Yujun Li",
      "Yu Luo",
      "Yunsheng Ni",
      "Yusen Sun",
      "Zelin Chen",
      "Zhe Liu",
      "Zhicheng Liu",
      "Zhipeng Tu",
      "Zilin Ding",
      "Zongyuan Zhan"
    ],
    "published": "2025-05-07T15:46:36+00:00",
    "summary": "Sparse large language models (LLMs) with Mixture of Experts (MoE) and close to a trillion parameters are dominating the realm of most capable language models. However, the massive model scale poses significant challenges for the underlying software and hardware systems. In this paper, we aim to uncover a recipe to harness such scale on Ascend NPUs. The key goals are better usage of the computing resources under the dynamic sparse model structures and materializing the expected performance gain on the actual hardware. To select model configurations suitable for Ascend NPUs without repeatedly running the expensive experiments, we leverage simulation to compare the trade-off of various model hyperparameters. This study led to Pangu Ultra MoE, a sparse LLM with 718 billion parameters, and we conducted experiments on the model to verify the simulation results. On the system side, we dig into Expert Parallelism to optimize the communication between NPU devices to reduce the synchronization overhead. We also optimize the memory efficiency within the devices to further reduce the parameter and activation management overhead. In the end, we achieve an MFU of 30.0% when training Pangu Ultra MoE, with performance comparable to that of DeepSeek R1, on 6K Ascend NPUs, and demonstrate that the Ascend system is capable of harnessing all the training stages of the state-of-the-art language models. Extensive experiments indicate that our recipe can lead to efficient training of large-scale sparse language models with MoE. We also study the behaviors of such models for future reference."
  },
  {
    "title": "Advancements in Solid-State Sodium-Based Batteries: A Comprehensive Review",
    "url": "http://arxiv.org/abs/2505.04391v1",
    "arxiv_id": "2505.04391v1",
    "authors": [
      "Arianna Massaro",
      "Lorenzo Squillantini",
      "Francesca De Giorgio",
      "Francesca A. Scaramuzzo",
      "Mauro Pasquali",
      "Sergio Brutti"
    ],
    "published": "2025-05-07T13:14:24+00:00",
    "summary": "This manuscript explores recent advancements in solid-state sodium-based battery technology, particularly focusing on electrochemical performance and the challenges associated with developing efficient solid electrolytes. The replacement of conventional liquid electrolytes with solid-state alternatives offers numerous benefits, including enhanced safety and environmental sustainability, as solid-state systems reduce flammability and harsh chemical handling. The work emphasizes the importance of structure and interface characteristics in solid electrolytes, which play a critical role in ionic conductivity and overall battery performance. Various classes of solid electrolytes, such as sodium-based anti-perovskites and sulphide electrolytes, are examined, highlighting their unique ionic transport mechanisms and mechanical properties that facilitate stable cycling. The manuscript also discusses strategies to enhance interfacial stability between the anode and the solid electrolyte to mitigate performance degradation during battery operation. Furthermore, advancements in electrode formulations and the integration of novel materials are considered pivotal in optimizing the charging and discharging processes, thus improving the energy and power densities of sodium batteries. The outlook on the future of sodium-based solid-state batteries underscores their potential to meet emerging energy storage demands while leveraging the abundant availability of sodium compared to lithium. This comprehensive review aims to provide insights into ongoing research and prospective directions for the commercialization of solid-state sodium-based batteries, positioning them as viable alternatives in the renewable energy landscape."
  },
  {
    "title": "The Aloe Family Recipe for Open and Specialized Healthcare LLMs",
    "url": "http://arxiv.org/abs/2505.04388v1",
    "arxiv_id": "2505.04388v1",
    "authors": [
      "Dario Garcia-Gasulla",
      "Jordi Bayarri-Planas",
      "Ashwin Kumar Gururajan",
      "Enrique Lopez-Cuena",
      "Adrian Tormos",
      "Daniel Hinjos",
      "Pablo Bernabeu-Perez",
      "Anna Arias-Duart",
      "Pablo Agustin Martin-Torres",
      "Marta Gonzalez-Mallo",
      "Sergio Alvarez-Napagao",
      "Eduard Ayguad\u00e9-Parra",
      "Ulises Cort\u00e9s"
    ],
    "published": "2025-05-07T13:13:14+00:00",
    "summary": "Purpose: With advancements in Large Language Models (LLMs) for healthcare, the need arises for competitive open-source models to protect the public interest. This work contributes to the field of open medical LLMs by optimizing key stages of data preprocessing and training, while showing how to improve model safety (through DPO) and efficacy (through RAG). The evaluation methodology used, which includes four different types of tests, defines a new standard for the field. The resultant models, shown to be competitive with the best private alternatives, are released with a permisive license.   Methods: Building on top of strong base models like Llama 3.1 and Qwen 2.5, Aloe Beta uses a custom dataset to enhance public data with synthetic Chain of Thought examples. The models undergo alignment with Direct Preference Optimization, emphasizing ethical and policy-aligned performance in the presence of jailbreaking attacks. Evaluation includes close-ended, open-ended, safety and human assessments, to maximize the reliability of results.   Results: Recommendations are made across the entire pipeline, backed by the solid performance of the Aloe Family. These models deliver competitive performance across healthcare benchmarks and medical fields, and are often preferred by healthcare professionals. On bias and toxicity, the Aloe Beta models significantly improve safety, showing resilience to unseen jailbreaking attacks. For a responsible release, a detailed risk assessment specific to healthcare is attached to the Aloe Family models.   Conclusion: The Aloe Beta models, and the recipe that leads to them, are a significant contribution to the open-source medical LLM field, offering top-of-the-line performance while maintaining high ethical requirements. This work sets a new standard for developing and reporting aligned LLMs in healthcare."
  },
  {
    "title": "Consensus-Aware AV Behavior: Trade-offs Between Safety, Interaction, and Performance in Mixed Urban Traffic",
    "url": "http://arxiv.org/abs/2505.04379v1",
    "arxiv_id": "2505.04379v1",
    "authors": [
      "Mohammad Elayan",
      "Wissam Kontar"
    ],
    "published": "2025-05-07T12:59:59+00:00",
    "summary": "Transportation systems have long been shaped by complexity and heterogeneity, driven by the interdependency of agent actions and traffic outcomes. The deployment of automated vehicles (AVs) in such systems introduces a new challenge: achieving consensus across safety, interaction quality, and traffic performance. In this work, we position consensus as a fundamental property of the traffic system and aim to quantify it. We use high-resolution trajectory data from the Third Generation Simulation (TGSIM) dataset to empirically analyze AV and human-driven vehicle (HDV) behavior at a signalized urban intersection and around vulnerable road users (VRUs). Key metrics, including Time-to-Collision (TTC), Post-Encroachment Time (PET), deceleration patterns, headways, and string stability, are evaluated across the three performance dimensions. Results show that full consensus across safety, interaction, and performance is rare, with only 1.63% of AV-VRU interaction frames meeting all three conditions. These findings highlight the need for AV models that explicitly balance multi-dimensional performance in mixed-traffic environments. Full reproducibility is supported via our open-source codebase on https://github.com/wissamkontar/Consensus-AV-Analysis."
  },
  {
    "title": "Resist Platform-Controlled AI Agents and Champion User-Centric Agent Advocates",
    "url": "http://arxiv.org/abs/2505.04345v1",
    "arxiv_id": "2505.04345v1",
    "authors": [
      "Sayash Kapoor",
      "Noam Kolt",
      "Seth Lazar"
    ],
    "published": "2025-05-07T11:45:38+00:00",
    "summary": "Language model agents could reshape how users navigate and act in digital environments. If controlled by platform companies -- either those that already dominate online search, communication, and commerce, or those vying to replace them -- platform agents could intensify surveillance, exacerbate user lock-in, and further entrench the incumbent digital giants. This position paper argues that to resist the undesirable effects of platform agents, we should champion agent advocates -- agents that are controlled by users, serve the interests of users, and preserve user autonomy and choice. We identify key interventions to enable agent advocates: ensuring public access to compute, developing interoperability protocols and safety standards, and implementing appropriate market regulations."
  },
  {
    "title": "Detecting Concept Drift in Neural Networks Using Chi-squared Goodness of Fit Testing",
    "url": "http://arxiv.org/abs/2505.04318v1",
    "arxiv_id": "2505.04318v1",
    "authors": [
      "Jacob Glenn Ayers",
      "Buvaneswari A. Ramanan",
      "Manzoor A. Khan"
    ],
    "published": "2025-05-07T11:04:47+00:00",
    "summary": "As the adoption of deep learning models has grown beyond human capacity for verification, meta-algorithms are needed to ensure reliable model inference. Concept drift detection is a field dedicated to identifying statistical shifts that is underutilized in monitoring neural networks that may encounter inference data with distributional characteristics diverging from their training data. Given the wide variety of model architectures, applications, and datasets, it is important that concept drift detection algorithms are adaptable to different inference scenarios. In this paper, we introduce an application of the $\\chi^2$ Goodness of Fit Hypothesis Test as a drift detection meta-algorithm applied to a multilayer perceptron, a convolutional neural network, and a transformer trained for machine vision as they are exposed to simulated drift during inference. To that end, we demonstrate how unexpected drops in accuracy due to concept drift can be detected without directly examining the inference outputs. Our approach enhances safety by ensuring models are continually evaluated for reliability across varying conditions."
  },
  {
    "title": "From Incidents to Insights: Patterns of Responsibility following AI Harms",
    "url": "http://arxiv.org/abs/2505.04291v1",
    "arxiv_id": "2505.04291v1",
    "authors": [
      "Isabel Richards",
      "Claire Benn",
      "Miri Zilka"
    ],
    "published": "2025-05-07T09:59:36+00:00",
    "summary": "The AI Incident Database was inspired by aviation safety databases, which enable collective learning from failures to prevent future incidents. The database documents hundreds of AI failures, collected from the news and media. However, criticism highlights that the AIID's reliance on media reporting limits its utility for learning about implementation failures. In this paper, we accept that the AIID falls short in its original mission, but argue that by looking beyond technically-focused learning, the dataset can provide new, highly valuable insights: specifically, opportunities to learn about patterns between developers, deployers, victims, wider society, and law-makers that emerge after AI failures. Through a three-tier mixed-methods analysis of 962 incidents and 4,743 related reports from the AIID, we examine patterns across incidents, focusing on cases with public responses tagged in the database. We identify 'typical' incidents found in the AIID, from Tesla crashes to deepfake scams.   Focusing on this interplay between relevant parties, we uncover patterns in accountability and social expectations of responsibility. We find that the presence of identifiable responsible parties does not necessarily lead to increased accountability. The likelihood of a response and what it amounts to depends highly on context, including who built the technology, who was harmed, and to what extent. Controversy-rich incidents provide valuable data about societal reactions, including insights into social expectations. Equally informative are cases where controversy is notably absent. This work shows that the AIID's value lies not just in preventing technical failures, but in documenting patterns of harms and of institutional response and social learning around AI incidents. These patterns offer crucial insights for understanding how society adapts to and governs emerging AI technologies."
  },
  {
    "title": "Multi-Agent Reinforcement Learning-based Cooperative Autonomous Driving in Smart Intersections",
    "url": "http://arxiv.org/abs/2505.04231v1",
    "arxiv_id": "2505.04231v1",
    "authors": [
      "Taoyuan Yu",
      "Kui Wang",
      "Zongdian Li",
      "Tao Yu",
      "Kei Sakaguchi"
    ],
    "published": "2025-05-07T08:27:52+00:00",
    "summary": "Unsignalized intersections pose significant safety and efficiency challenges due to complex traffic flows. This paper proposes a novel roadside unit (RSU)-centric cooperative driving system leveraging global perception and vehicle-to-infrastructure (V2I) communication. The core of the system is an RSU-based decision-making module using a two-stage hybrid reinforcement learning (RL) framework. At first, policies are pre-trained offline using conservative Q-learning (CQL) combined with behavior cloning (BC) on collected dataset. Subsequently, these policies are fine-tuned in the simulation using multi-agent proximal policy optimization (MAPPO), aligned with a self-attention mechanism to effectively solve inter-agent dependencies. RSUs perform real-time inference based on the trained models to realize vehicle control via V2I communications. Extensive experiments in CARLA environment demonstrate high effectiveness of the proposed system, by: \\textit{(i)} achieving failure rates below 0.03\\% in coordinating three connected and autonomous vehicles (CAVs) through complex intersection scenarios, significantly outperforming the traditional Autoware control method, and \\textit{(ii)} exhibiting strong robustness across varying numbers of controlled agents and shows promising generalization capabilities on other maps."
  },
  {
    "title": "An Enhanced YOLOv8 Model for Real-Time and Accurate Pothole Detection and Measurement",
    "url": "http://arxiv.org/abs/2505.04207v1",
    "arxiv_id": "2505.04207v1",
    "authors": [
      "Mustafa Yurdakul",
      "\u015eakir Tasdemir"
    ],
    "published": "2025-05-07T07:58:57+00:00",
    "summary": "Potholes cause vehicle damage and traffic accidents, creating serious safety and economic problems. Therefore, early and accurate detection of potholes is crucial. Existing detection methods are usually only based on 2D RGB images and cannot accurately analyze the physical characteristics of potholes. In this paper, a publicly available dataset of RGB-D images (PothRGBD) is created and an improved YOLOv8-based model is proposed for both pothole detection and pothole physical features analysis. The Intel RealSense D415 depth camera was used to collect RGB and depth data from the road surfaces, resulting in a PothRGBD dataset of 1000 images. The data was labeled in YOLO format suitable for segmentation. A novel YOLO model is proposed based on the YOLOv8n-seg architecture, which is structurally improved with Dynamic Snake Convolution (DSConv), Simple Attention Module (SimAM) and Gaussian Error Linear Unit (GELU). The proposed model segmented potholes with irregular edge structure more accurately, and performed perimeter and depth measurements on depth maps with high accuracy. The standard YOLOv8n-seg model achieved 91.9% precision, 85.2% recall and 91.9% mAP@50. With the proposed model, the values increased to 93.7%, 90.4% and 93.8% respectively. Thus, an improvement of 1.96% in precision, 6.13% in recall and 2.07% in mAP was achieved. The proposed model performs pothole detection as well as perimeter and depth measurement with high accuracy and is suitable for real-time applications due to its low model complexity. In this way, a lightweight and effective model that can be used in deep learning-based intelligent transportation solutions has been acquired."
  },
  {
    "title": "Unmasking the Canvas: A Dynamic Benchmark for Image Generation Jailbreaking and LLM Content Safety",
    "url": "http://arxiv.org/abs/2505.04146v1",
    "arxiv_id": "2505.04146v1",
    "authors": [
      "Variath Madhupal Gautham Nair",
      "Vishal Varma Dantuluri"
    ],
    "published": "2025-05-07T05:54:04+00:00",
    "summary": "Existing large language models (LLMs) are advancing rapidly and produce outstanding results in image generation tasks, yet their content safety checks remain vulnerable to prompt-based jailbreaks. Through preliminary testing on platforms such as ChatGPT, MetaAI, and Grok, we observed that even short, natural prompts could lead to the generation of compromising images ranging from realistic depictions of forged documents to manipulated images of public figures.   We introduce Unmasking the Canvas (UTC Benchmark; UTCB), a dynamic and scalable benchmark dataset to evaluate LLM vulnerability in image generation. Our methodology combines structured prompt engineering, multilingual obfuscation (e.g., Zulu, Gaelic, Base64), and evaluation using Groq-hosted LLaMA-3. The pipeline supports both zero-shot and fallback prompting strategies, risk scoring, and automated tagging. All generations are stored with rich metadata and curated into Bronze (non-verified), Silver (LLM-aided verification), and Gold (manually verified) tiers. UTCB is designed to evolve over time with new data sources, prompt templates, and model behaviors.   Warning: This paper includes visual examples of adversarial inputs designed to test model safety. All outputs have been redacted to ensure responsible disclosure."
  },
  {
    "title": "In-Situ Hardware Error Detection Using Specification-Derived Petri Net Models and Behavior-Derived State Sequences",
    "url": "http://arxiv.org/abs/2505.04108v1",
    "arxiv_id": "2505.04108v1",
    "authors": [
      "Tomonari Tanaka",
      "Takumi Uezono",
      "Kohei Suenaga",
      "Masanori Hashimoto"
    ],
    "published": "2025-05-07T03:51:02+00:00",
    "summary": "In hardware accelerators used in data centers and safety-critical applications, soft errors and resultant silent data corruption significantly compromise reliability, particularly when upsets occur in control-flow operations, leading to severe failures. To address this, we introduce two methods for monitoring control flows: using specification-derived Petri nets and using behavior-derived state transitions. We validated our method across four designs: convolutional layer operation, Gaussian blur, AES encryption, and a router in Network-on-Chip. Our fault injection campaign targeting the control registers and primary control inputs demonstrated high error detection rates in both datapath and control logic. Synthesis results show that a maximum detection rate is achieved with a few to around 10% area overhead in most cases. The proposed detectors quickly detect 48% to 100% of failures resulting from upsets in internal control registers and perturbations in primary control inputs. The two proposed methods were compared in terms of area overhead and error detection rate. By selectively applying these two methods, a wide range of area constraints can be accommodated, enabling practical implementation and effectively enhancing error detection capabilities."
  },
  {
    "title": "Shadow Wireless Intelligence: Large Language Model-Driven Reasoning in Covert Communications",
    "url": "http://arxiv.org/abs/2505.04068v1",
    "arxiv_id": "2505.04068v1",
    "authors": [
      "Yuanai Xie",
      "Zhaozhi Liu",
      "Xiao Zhang",
      "Shihua Zhang",
      "Rui Hou",
      "Minrui Xu",
      "Ruichen Zhang",
      "Dusit Niyato"
    ],
    "published": "2025-05-07T02:11:43+00:00",
    "summary": "Covert Communications (CC) can secure sensitive transmissions in industrial, military, and mission-critical applications within 6G wireless networks. However, traditional optimization methods based on Artificial Noise (AN), power control, and channel manipulation might not adapt to dynamic and adversarial environments due to the high dimensionality, nonlinearity, and stringent real-time covertness requirements. To bridge this gap, we introduce Shadow Wireless Intelligence (SWI), which integrates the reasoning capabilities of Large Language Models (LLMs) with retrieval-augmented generation to enable intelligent decision-making in covert wireless systems. Specifically, we utilize DeepSeek-R1, a mixture-of-experts-based LLM with RL-enhanced reasoning, combined with real-time retrieval of domain-specific knowledge to improve context accuracy and mitigate hallucinations. Our approach develops a structured CC knowledge base, supports context-aware retrieval, and performs semantic optimization, allowing LLMs to generate and adapt CC strategies in real time. In a case study on optimizing AN power in a full-duplex CC scenario, DeepSeek-R1 achieves 85% symbolic derivation accuracy and 94% correctness in the generation of simulation code, outperforming baseline models. These results validate SWI as a robust, interpretable, and adaptive foundation for LLM-driven intelligent covert wireless systems in 6G networks."
  },
  {
    "title": "Reliable Disentanglement Multi-view Learning Against View Adversarial Attacks",
    "url": "http://arxiv.org/abs/2505.04046v1",
    "arxiv_id": "2505.04046v1",
    "authors": [
      "Xuyang Wang",
      "Siyuan Duan",
      "Qizhi Li",
      "Guiduo Duan",
      "Yuan Sun",
      "Dezhong Peng"
    ],
    "published": "2025-05-07T01:12:00+00:00",
    "summary": "Recently, trustworthy multi-view learning has attracted extensive attention because evidence learning can provide reliable uncertainty estimation to enhance the credibility of multi-view predictions. Existing trusted multi-view learning methods implicitly assume that multi-view data is secure. In practice, however, in safety-sensitive applications such as autonomous driving and security monitoring, multi-view data often faces threats from adversarial perturbations, thereby deceiving or disrupting multi-view learning models. This inevitably leads to the adversarial unreliability problem (AUP) in trusted multi-view learning. To overcome this tricky problem, we propose a novel multi-view learning framework, namely Reliable Disentanglement Multi-view Learning (RDML). Specifically, we first propose evidential disentanglement learning to decompose each view into clean and adversarial parts under the guidance of corresponding evidences, which is extracted by a pretrained evidence extractor. Then, we employ the feature recalibration module to mitigate the negative impact of adversarial perturbations and extract potential informative features from them. Finally, to further ignore the irreparable adversarial interferences, a view-level evidential attention mechanism is designed. Extensive experiments on multi-view classification tasks with adversarial attacks show that our RDML outperforms the state-of-the-art multi-view learning methods by a relatively large margin."
  },
  {
    "title": "Estimating the Joint Distribution of Two Binary Variables with Marginal Statistics",
    "url": "http://arxiv.org/abs/2505.03995v1",
    "arxiv_id": "2505.03995v1",
    "authors": [
      "Longwen Shang",
      "Min Tsao",
      "Xuekui Zhang"
    ],
    "published": "2025-05-06T22:15:23+00:00",
    "summary": "Clinical trial simulation (CTS) is critical in new drug development, providing insight into safety and efficacy while guiding trial design. Achieving realistic outcomes in CTS requires an accurately estimated joint distribution of the underlying variables. However, privacy concerns and data availability issues often restrict researchers to marginal summary-level data of each variable, making it challenging to estimate the joint distribution due to the lack of access to individual-level data or relational summaries between variables. We propose a novel approach based on the method of maximum likelihood that estimates the joint distribution of two binary variables using only marginal summary data. By leveraging numerical optimization and accommodating varying sample sizes across studies, our method preserves privacy while bypassing the need for granular or relational data. Through an extensive simulation study covering a diverse range of scenarios and an application to a real-world dataset, we demonstrate the accuracy, robustness, and practicality of our method. This method enhances the generation of realistic simulated data, thereby improving decision-making processes in drug development."
  },
  {
    "title": "An alignment safety case sketch based on debate",
    "url": "http://arxiv.org/abs/2505.03989v1",
    "arxiv_id": "2505.03989v1",
    "authors": [
      "Marie Davidsen Buhl",
      "Jacob Pfau",
      "Benjamin Hilton",
      "Geoffrey Irving"
    ],
    "published": "2025-05-06T21:53:44+00:00",
    "summary": "If AI systems match or exceed human capabilities on a wide range of tasks, it may become difficult for humans to efficiently judge their actions -- making it hard to use human feedback to steer them towards desirable traits. One proposed solution is to leverage another superhuman system to point out flaws in the system's outputs via a debate. This paper outlines the value of debate for AI safety, as well as the assumptions and further research required to make debate work. It does so by sketching an ``alignment safety case'' -- an argument that an AI system will not autonomously take actions which could lead to egregious harm, despite being able to do so. The sketch focuses on the risk of an AI R\\&D agent inside an AI company sabotaging research, for example by producing false results. To prevent this, the agent is trained via debate, subject to exploration guarantees, to teach the system to be honest. Honesty is maintained throughout deployment via online training. The safety case rests on four key claims: (1) the agent has become good at the debate game, (2) good performance in the debate game implies that the system is mostly honest, (3) the system will not become significantly less honest during deployment, and (4) the deployment context is tolerant of some errors. We identify open research problems that, if solved, could render this a compelling argument that an AI system is safe."
  },
  {
    "title": "LogiDebrief: A Signal-Temporal Logic based Automated Debriefing Approach with Large Language Models Integration",
    "url": "http://arxiv.org/abs/2505.03985v1",
    "arxiv_id": "2505.03985v1",
    "authors": [
      "Zirong Chen",
      "Ziyan An",
      "Jennifer Reynolds",
      "Kristin Mullen",
      "Stephen Martini",
      "Meiyi Ma"
    ],
    "published": "2025-05-06T21:27:07+00:00",
    "summary": "Emergency response services are critical to public safety, with 9-1-1 call-takers playing a key role in ensuring timely and effective emergency operations. To ensure call-taking performance consistency, quality assurance is implemented to evaluate and refine call-takers' skillsets. However, traditional human-led evaluations struggle with high call volumes, leading to low coverage and delayed assessments. We introduce LogiDebrief, an AI-driven framework that automates traditional 9-1-1 call debriefing by integrating Signal-Temporal Logic (STL) with Large Language Models (LLMs) for fully-covered rigorous performance evaluation. LogiDebrief formalizes call-taking requirements as logical specifications, enabling systematic assessment of 9-1-1 calls against procedural guidelines. It employs a three-step verification process: (1) contextual understanding to identify responder types, incident classifications, and critical conditions; (2) STL-based runtime checking with LLM integration to ensure compliance; and (3) automated aggregation of results into quality assurance reports. Beyond its technical contributions, LogiDebrief has demonstrated real-world impact. Successfully deployed at Metro Nashville Department of Emergency Communications, it has assisted in debriefing 1,701 real-world calls, saving 311.85 hours of active engagement. Empirical evaluation with real-world data confirms its accuracy, while a case study and extensive user study highlight its effectiveness in enhancing call-taking performance."
  },
  {
    "title": "X-Reasoner: Towards Generalizable Reasoning Across Modalities and Domains",
    "url": "http://arxiv.org/abs/2505.03981v1",
    "arxiv_id": "2505.03981v1",
    "authors": [
      "Qianchu Liu",
      "Sheng Zhang",
      "Guanghui Qin",
      "Timothy Ossowski",
      "Yu Gu",
      "Ying Jin",
      "Sid Kiblawi",
      "Sam Preston",
      "Mu Wei",
      "Paul Vozila",
      "Tristan Naumann",
      "Hoifung Poon"
    ],
    "published": "2025-05-06T21:08:27+00:00",
    "summary": "Recent proprietary models (e.g., o3) have begun to demonstrate strong multimodal reasoning capabilities. Yet, most existing open-source research concentrates on training text-only reasoning models, with evaluations limited to mainly mathematical and general-domain tasks. Therefore, it remains unclear how to effectively extend reasoning capabilities beyond text input and general domains. This paper explores a fundamental research question: Is reasoning generalizable across modalities and domains? Our findings support an affirmative answer: General-domain text-based post-training can enable such strong generalizable reasoning. Leveraging this finding, we introduce X-Reasoner, a vision-language model post-trained solely on general-domain text for generalizable reasoning, using a two-stage approach: an initial supervised fine-tuning phase with distilled long chain-of-thoughts, followed by reinforcement learning with verifiable rewards. Experiments show that X-Reasoner successfully transfers reasoning capabilities to both multimodal and out-of-domain settings, outperforming existing state-of-the-art models trained with in-domain and multimodal data across various general and medical benchmarks (Figure 1). Additionally, we find that X-Reasoner's performance in specialized domains can be further enhanced through continued training on domain-specific text-only data. Building upon this, we introduce X-Reasoner-Med, a medical-specialized variant that achieves new state of the art on numerous text-only and multimodal medical benchmarks."
  },
  {
    "title": "Recent Advances in Disaster Emergency Response Planning: Integrating Optimization, Machine Learning, and Simulation",
    "url": "http://arxiv.org/abs/2505.03979v1",
    "arxiv_id": "2505.03979v1",
    "authors": [
      "Fan Pu",
      "Zihao Li",
      "Yifan Wu",
      "Chaolun Ma",
      "Ruonan Zhao"
    ],
    "published": "2025-05-06T21:05:31+00:00",
    "summary": "The increasing frequency and severity of natural disasters underscore the critical importance of effective disaster emergency response planning to minimize human and economic losses. This survey provides a comprehensive review of recent advancements (2019--2024) in five essential areas of disaster emergency response planning: evacuation, facility location, casualty transport, search and rescue, and relief distribution. Research in these areas is systematically categorized based on methodologies, including optimization models, machine learning, and simulation, with a focus on their individual strengths and synergies. A notable contribution of this work is its examination of the interplay between machine learning, simulation, and optimization frameworks, highlighting how these approaches can address the dynamic, uncertain, and complex nature of disaster scenarios. By identifying key research trends and challenges, this study offers valuable insights to improve the effectiveness and resilience of emergency response strategies in future disaster planning efforts."
  },
  {
    "title": "Learning Interactions Between Continuous Treatments and Covariates with a Semiparametric Model",
    "url": "http://arxiv.org/abs/2505.03893v1",
    "arxiv_id": "2505.03893v1",
    "authors": [
      "Muyan Jiang",
      "Yunkai Zhang",
      "Anil Aswani"
    ],
    "published": "2025-05-06T18:01:00+00:00",
    "summary": "Estimating the impact of continuous treatment variables (e.g., dosage amount) on binary outcomes presents significant challenges in modeling and estimation because many existing approaches make strong assumptions that do not hold for certain continuous treatment variables. For instance, traditional logistic regression makes strong linearity assumptions that do not hold for continuous treatment variables like time of initiation. In this work, we propose a semiparametric regression framework that decomposes effects into two interpretable components: a prognostic score that captures baseline outcome risk based on a combination of clinical, genetic, and sociodemographic features, and a treatment-interaction score that flexibly models the optimal treatment level via a nonparametric link function. By connecting these two parametric scores with Nadaraya-Watson regression, our approach is both interpretable and flexible. The potential of our approach is demonstrated through numerical simulations that show empirical estimation convergence. We conclude by applying our approach to a real-world case study using the International Warfarin Pharmacogenomics Consortium (IWPC) dataset to show our approach's clinical utility by deriving personalized warfarin dosing recommendations that integrate both genetic and clinical data, providing insights towards enhancing patient safety and therapeutic efficacy in anticoagulation therapy."
  },
  {
    "title": "WebGen-Bench: Evaluating LLMs on Generating Interactive and Functional Websites from Scratch",
    "url": "http://arxiv.org/abs/2505.03733v1",
    "arxiv_id": "2505.03733v1",
    "authors": [
      "Zimu Lu",
      "Yunqiao Yang",
      "Houxing Ren",
      "Haotian Hou",
      "Han Xiao",
      "Ke Wang",
      "Weikang Shi",
      "Aojun Zhou",
      "Mingjie Zhan",
      "Hongsheng Li"
    ],
    "published": "2025-05-06T17:59:15+00:00",
    "summary": "LLM-based agents have demonstrated great potential in generating and managing code within complex codebases. In this paper, we introduce WebGen-Bench, a novel benchmark designed to measure an LLM-based agent's ability to create multi-file website codebases from scratch. It contains diverse instructions for website generation, created through the combined efforts of human annotators and GPT-4o. These instructions span three major categories and thirteen minor categories, encompassing nearly all important types of web applications. To assess the quality of the generated websites, we use GPT-4o to generate test cases targeting each functionality described in the instructions, and then manually filter, adjust, and organize them to ensure accuracy, resulting in 647 test cases. Each test case specifies an operation to be performed on the website and the expected result after the operation. To automate testing and improve reproducibility, we employ a powerful web-navigation agent to execute tests on the generated websites and determine whether the observed responses align with the expected results. We evaluate three high-performance code-agent frameworks, Bolt.diy, OpenHands, and Aider, using multiple proprietary and open-source LLMs as engines. The best-performing combination, Bolt.diy powered by DeepSeek-R1, achieves only 27.8\\% accuracy on the test cases, highlighting the challenging nature of our benchmark. Additionally, we construct WebGen-Instruct, a training set consisting of 6,667 website-generation instructions. Training Qwen2.5-Coder-32B-Instruct on Bolt.diy trajectories generated from a subset of this training set achieves an accuracy of 38.2\\%, surpassing the performance of the best proprietary model."
  },
  {
    "title": "Toward a Harmonized Approach -- Requirement-based Structuring of a Safety Assurance Argumentation for Automated Vehicles",
    "url": "http://arxiv.org/abs/2505.03709v1",
    "arxiv_id": "2505.03709v1",
    "authors": [
      "M. Loba",
      "N. F. Salem",
      "M. Nolte",
      "A. Dotzler",
      "M. Maurer"
    ],
    "published": "2025-05-06T17:28:30+00:00",
    "summary": "Despite increasing testing operation on public roads, media reports on incidents show that safety issues remain to this day. One major cause factoring into this circumstance is high development uncertainty that manufacturers face when deploying these systems in an open context. In particular, one challenge is establishing a valid argument at design time that the vehicle will exhibit reasonable residual risk when operating in its intended operational design domain. Regulations, such as the European Implementing Regulation 2022/1426, require manufacturers to provide a safety assurance argumentation for SAE-Level-4 automated vehicles. While there is extensive literature on assurance cases for safety-critical systems, the domain of automated driving lacks explicit requirements regarding the creation of safety assurance argumentations. In this paper, we aim to narrow this gap by elaborating a requirement-based approach. We derive structural requirements for an argumentation from literature and supplement these with requirements derived from stakeholder concerns. We implement the requirements, yielding a proposal for an overall argumentation structure. The resulting \"safety arguments\" argue over four topic complexes: The developed product, the underlying process including its conformance/compliance to standards/laws, as well as the argumentations' context and soundness. Finally, we instantiate this structure with respect to domain-specific needs and principles."
  },
  {
    "title": "Toward a Harmonized Approach - Requirement-based Structuring of a Safety Assurance Argumentation for Automated Vehicles",
    "url": "http://arxiv.org/abs/2505.03709v2",
    "arxiv_id": "2505.03709v2",
    "authors": [
      "Marvin Loba",
      "Nayel Fabian Salem",
      "Marcus Nolte",
      "Andreas Dotzler",
      "Dieter Ludwig",
      "Markus Maurer"
    ],
    "published": "2025-05-06T17:28:30+00:00",
    "summary": "Despite increasing testing operation on public roads, media reports on incidents show that safety issues remain to this day. One major cause factoring into this circumstance is high development uncertainty that manufacturers face when deploying these systems in an open context. In particular, one challenge is establishing a valid argument at design time that the vehicle will exhibit reasonable residual risk when operating in its intended operational design domain. Regulations, such as the European Implementing Regulation 2022/1426, require manufacturers to provide a safety assurance argumentation for SAE-Level-4 automated vehicles. While there is extensive literature on assurance cases for safety-critical systems, the domain of automated driving lacks explicit requirements regarding the creation of safety assurance argumentations. In this paper, we aim to narrow this gap by elaborating a requirement-based approach. We derive structural requirements for an argumentation from literature and supplement these with requirements derived from stakeholder concerns. We implement the requirements, yielding a proposal for an overall argumentation structure. The resulting \"safety arguments\" argue over four topic complexes: The developed product, the underlying process including its conformance/compliance to standards/laws, as well as the argumentations' context and soundness. Finally, we instantiate this structure with respect to domain-specific needs and principles."
  },
  {
    "title": "Frenet Corridor Planner: An Optimal Local Path Planning Framework for Autonomous Driving",
    "url": "http://arxiv.org/abs/2505.03695v1",
    "arxiv_id": "2505.03695v1",
    "authors": [
      "Faizan M. Tariq",
      "Zheng-Hang Yeh",
      "Avinash Singh",
      "David Isele",
      "Sangjae Bae"
    ],
    "published": "2025-05-06T17:00:32+00:00",
    "summary": "Motivated by the requirements for effectiveness and efficiency, path-speed decomposition-based trajectory planning methods have widely been adopted for autonomous driving applications. While a global route can be pre-computed offline, real-time generation of adaptive local paths remains crucial. Therefore, we present the Frenet Corridor Planner (FCP), an optimization-based local path planning strategy for autonomous driving that ensures smooth and safe navigation around obstacles. Modeling the vehicles as safety-augmented bounding boxes and pedestrians as convex hulls in the Frenet space, our approach defines a drivable corridor by determining the appropriate deviation side for static obstacles. Thereafter, a modified space-domain bicycle kinematics model enables path optimization for smoothness, boundary clearance, and dynamic obstacle risk minimization. The optimized path is then passed to a speed planner to generate the final trajectory. We validate FCP through extensive simulations and real-world hardware experiments, demonstrating its efficiency and effectiveness."
  },
  {
    "title": "Demonstrating ViSafe: Vision-enabled Safety for High-speed Detect and Avoid",
    "url": "http://arxiv.org/abs/2505.03694v1",
    "arxiv_id": "2505.03694v1",
    "authors": [
      "Parv Kapoor",
      "Ian Higgins",
      "Nikhil Keetha",
      "Jay Patrikar",
      "Brady Moon",
      "Zelin Ye",
      "Yao He",
      "Ivan Cisneros",
      "Yaoyu Hu",
      "Changliu Liu",
      "Eunsuk Kang",
      "Sebastian Scherer"
    ],
    "published": "2025-05-06T16:59:54+00:00",
    "summary": "Assured safe-separation is essential for achieving seamless high-density operation of airborne vehicles in a shared airspace. To equip resource-constrained aerial systems with this safety-critical capability, we present ViSafe, a high-speed vision-only airborne collision avoidance system. ViSafe offers a full-stack solution to the Detect and Avoid (DAA) problem by tightly integrating a learning-based edge-AI framework with a custom multi-camera hardware prototype designed under SWaP-C constraints. By leveraging perceptual input-focused control barrier functions (CBF) to design, encode, and enforce safety thresholds, ViSafe can provide provably safe runtime guarantees for self-separation in high-speed aerial operations. We evaluate ViSafe's performance through an extensive test campaign involving both simulated digital twins and real-world flight scenarios. By independently varying agent types, closure rates, interaction geometries, and environmental conditions (e.g., weather and lighting), we demonstrate that ViSafe consistently ensures self-separation across diverse scenarios. In first-of-its-kind real-world high-speed collision avoidance tests with closure rates reaching 144 km/h, ViSafe sets a new benchmark for vision-only autonomous collision avoidance, establishing a new standard for safety in high-speed aerial navigation."
  },
  {
    "title": "Data-Driven Falsification of Cyber-Physical Systems",
    "url": "http://arxiv.org/abs/2505.03863v1",
    "arxiv_id": "2505.03863v1",
    "authors": [
      "Atanu Kundu",
      "Sauvik Gon",
      "Rajarshi Ray"
    ],
    "published": "2025-05-06T16:33:06+00:00",
    "summary": "Cyber-Physical Systems (CPS) are abundant in safety-critical domains such as healthcare, avionics, and autonomous vehicles. Formal verification of their operational safety is, therefore, of utmost importance. In this paper, we address the falsification problem, where the focus is on searching for an unsafe execution in the system instead of proving their absence. The contribution of this paper is a framework that (a) connects the falsification of CPS with the falsification of deep neural networks (DNNs) and (b) leverages the inherent interpretability of Decision Trees for faster falsification of CPS. This is achieved by: (1) building a surrogate model of the CPS under test, either as a DNN model or a Decision Tree, (2) application of various DNN falsification tools to falsify CPS, and (3) a novel falsification algorithm guided by the explanations of safety violations of the CPS model extracted from its Decision Tree surrogate. The proposed framework has the potential to exploit a repertoire of \\emph{adversarial attack} algorithms designed to falsify robustness properties of DNNs, as well as state-of-the-art falsification algorithms for DNNs. Although the presented methodology is applicable to systems that can be executed/simulated in general, we demonstrate its effectiveness, particularly in CPS. We show that our framework, implemented as a tool \\textsc{FlexiFal}, can detect hard-to-find counterexamples in CPS that have linear and non-linear dynamics. Decision tree-guided falsification shows promising results in efficiently finding multiple counterexamples in the ARCH-COMP 2024 falsification benchmarks~\\cite{khandait2024arch}."
  },
  {
    "title": "Moral Testing of Autonomous Driving Systems",
    "url": "http://arxiv.org/abs/2505.03683v1",
    "arxiv_id": "2505.03683v1",
    "authors": [
      "Wenbing Tang",
      "Mingfei Cheng",
      "Yuan Zhou",
      "Yang Liu"
    ],
    "published": "2025-05-06T16:29:58+00:00",
    "summary": "Autonomous Driving System (ADS) testing plays a crucial role in their development, with the current focus primarily on functional and safety testing. However, evaluating the non-functional morality of ADSs, particularly their decision-making capabilities in unavoidable collision scenarios, is equally important to ensure the systems' trustworthiness and public acceptance. Unfortunately, testing ADS morality is nearly impossible due to the absence of universal moral principles. To address this challenge, this paper first extracts a set of moral meta-principles derived from existing moral experiments and well-established social science theories, aiming to capture widely recognized and common-sense moral values for ADSs. These meta-principles are then formalized as quantitative moral metamorphic relations, which act as the test oracle. Furthermore, we propose a metamorphic testing framework to systematically identify potential moral issues. Finally, we illustrate the implementation of the framework and present typical violation cases using the VIRES VTD simulator and its built-in ADS."
  },
  {
    "title": "BURNS: Backward Underapproximate Reachability for Neural-Feedback-Loop Systems",
    "url": "http://arxiv.org/abs/2505.03643v1",
    "arxiv_id": "2505.03643v1",
    "authors": [
      "Chelsea Sidrane",
      "Jana Tumova"
    ],
    "published": "2025-05-06T15:50:43+00:00",
    "summary": "Learning-enabled planning and control algorithms are increasingly popular, but they often lack rigorous guarantees of performance or safety. We introduce an algorithm for computing underapproximate backward reachable sets of nonlinear discrete time neural feedback loops. We then use the backward reachable sets to check goal-reaching properties. Our algorithm is based on overapproximating the system dynamics function to enable computation of underapproximate backward reachable sets through solutions of mixed-integer linear programs. We rigorously analyze the soundness of our algorithm and demonstrate it on a numerical example. Our work expands the class of properties that can be verified for learning-enabled systems."
  },
  {
    "title": "Backstepping Reach-avoid Controller Synthesis for Multi-input Multi-output Systems with Mixed Relative Degrees",
    "url": "http://arxiv.org/abs/2505.03612v1",
    "arxiv_id": "2505.03612v1",
    "authors": [
      "Jianqiang Ding",
      "Dingran Yuan",
      "Shankar A. Deka"
    ],
    "published": "2025-05-06T15:10:29+00:00",
    "summary": "Designing controllers with provable formal guarantees has become an urgent requirement for cyber-physical systems in safety-critical scenarios. Beyond addressing scalability in high-dimensional implementations, controller synthesis methodologies separating safety and reachability objectives may risk optimization infeasibility due to conflicting constraints, thereby significantly undermining their applicability in practical applications. In this paper, by leveraging feedback linearization and backstepping techniques, we present a novel framework for constructing provable reach-avoid formal certificates tailored to multi-input multi-output systems. Based on this, we developed a systematic synthesis approach for controllers with reach-avoid guarantees, which ensures that the outputs of the system eventually enter the predefined target set while staying within the required safe set. Finally, we demonstrate the effectiveness of our method through simulations."
  },
  {
    "title": "LlamaFirewall: An open source guardrail system for building secure AI agents",
    "url": "http://arxiv.org/abs/2505.03574v1",
    "arxiv_id": "2505.03574v1",
    "authors": [
      "Sahana Chennabasappa",
      "Cyrus Nikolaidis",
      "Daniel Song",
      "David Molnar",
      "Stephanie Ding",
      "Shengye Wan",
      "Spencer Whitman",
      "Lauren Deason",
      "Nicholas Doucette",
      "Abraham Montilla",
      "Alekhya Gampa",
      "Beto de Paola",
      "Dominik Gabi",
      "James Crnkovich",
      "Jean-Christophe Testud",
      "Kat He",
      "Rashnil Chaturvedi",
      "Wu Zhou",
      "Joshua Saxe"
    ],
    "published": "2025-05-06T14:34:21+00:00",
    "summary": "Large language models (LLMs) have evolved from simple chatbots into autonomous agents capable of performing complex tasks such as editing production code, orchestrating workflows, and taking higher-stakes actions based on untrusted inputs like webpages and emails. These capabilities introduce new security risks that existing security measures, such as model fine-tuning or chatbot-focused guardrails, do not fully address. Given the higher stakes and the absence of deterministic solutions to mitigate these risks, there is a critical need for a real-time guardrail monitor to serve as a final layer of defense, and support system level, use case specific safety policy definition and enforcement. We introduce LlamaFirewall, an open-source security focused guardrail framework designed to serve as a final layer of defense against security risks associated with AI Agents. Our framework mitigates risks such as prompt injection, agent misalignment, and insecure code risks through three powerful guardrails: PromptGuard 2, a universal jailbreak detector that demonstrates clear state of the art performance; Agent Alignment Checks, a chain-of-thought auditor that inspects agent reasoning for prompt injection and goal misalignment, which, while still experimental, shows stronger efficacy at preventing indirect injections in general scenarios than previously proposed approaches; and CodeShield, an online static analysis engine that is both fast and extensible, aimed at preventing the generation of insecure or dangerous code by coding agents. Additionally, we include easy-to-use customizable scanners that make it possible for any developer who can write a regular expression or an LLM prompt to quickly update an agent's security guardrails."
  },
  {
    "title": "Long-Short Chain-of-Thought Mixture Supervised Fine-Tuning Eliciting Efficient Reasoning in Large Language Models",
    "url": "http://arxiv.org/abs/2505.03469v1",
    "arxiv_id": "2505.03469v1",
    "authors": [
      "Bin Yu",
      "Hang Yuan",
      "Yuliang Wei",
      "Bailing Wang",
      "Weizhen Qi",
      "Kai Chen"
    ],
    "published": "2025-05-06T12:18:11+00:00",
    "summary": "Recent advances in large language models have demonstrated that Supervised Fine-Tuning (SFT) with Chain-of-Thought (CoT) reasoning data distilled from large reasoning models (e.g., DeepSeek R1) can effectively transfer reasoning capabilities to non-reasoning models. However, models fine-tuned with this approach inherit the \"overthinking\" problem from teacher models, producing verbose and redundant reasoning chains during inference. To address this challenge, we propose \\textbf{L}ong-\\textbf{S}hort Chain-of-Thought \\textbf{Mixture} \\textbf{S}upervised \\textbf{F}ine-\\textbf{T}uning (\\textbf{LS-Mixture SFT}), which combines long CoT reasoning dataset with their short counterparts obtained through structure-preserved rewriting. Our experiments demonstrate that models trained using the LS-Mixture SFT method, compared to those trained with direct SFT, achieved an average accuracy improvement of 2.3\\% across various benchmarks while substantially reducing model response length by approximately 47.61\\%. This work offers an approach to endow non-reasoning models with reasoning capabilities through supervised fine-tuning while avoiding the inherent overthinking problems inherited from teacher models, thereby enabling efficient reasoning in the fine-tuned models."
  },
  {
    "title": "Close-Fitting Dressing Assistance Based on State Estimation of Feet and Garments with Semantic-based Visual Attention",
    "url": "http://arxiv.org/abs/2505.03400v1",
    "arxiv_id": "2505.03400v1",
    "authors": [
      "Takuma Tsukakoshi",
      "Tamon Miyake",
      "Tetsuya Ogata",
      "Yushi Wang",
      "Takumi Akaishi",
      "Shigeki Sugano"
    ],
    "published": "2025-05-06T10:28:39+00:00",
    "summary": "As the population continues to age, a shortage of caregivers is expected in the future. Dressing assistance, in particular, is crucial for opportunities for social participation. Especially dressing close-fitting garments, such as socks, remains challenging due to the need for fine force adjustments to handle the friction or snagging against the skin, while considering the shape and position of the garment. This study introduces a method uses multi-modal information including not only robot's camera images, joint angles, joint torques, but also tactile forces for proper force interaction that can adapt to individual differences in humans. Furthermore, by introducing semantic information based on object concepts, rather than relying solely on RGB data, it can be generalized to unseen feet and background. In addition, incorporating depth data helps infer relative spatial relationship between the sock and the foot. To validate its capability for semantic object conceptualization and to ensure safety, training data were collected using a mannequin, and subsequent experiments were conducted with human subjects. In experiments, the robot successfully adapted to previously unseen human feet and was able to put socks on 10 participants, achieving a higher success rate than Action Chunking with Transformer and Diffusion Policy. These results demonstrate that the proposed model can estimate the state of both the garment and the foot, enabling precise dressing assistance for close-fitting garments."
  },
  {
    "title": "Miniature multihole airflow sensor for lightweight aircraft over wide speed and angular range",
    "url": "http://arxiv.org/abs/2505.03331v1",
    "arxiv_id": "2505.03331v1",
    "authors": [
      "Lukas Stuber",
      "Simon Jeger",
      "Raphael Zufferey",
      "Dario Floreano"
    ],
    "published": "2025-05-06T09:04:49+00:00",
    "summary": "An aircraft's airspeed, angle of attack, and angle of side slip are crucial to its safety, especially when flying close to the stall regime. Various solutions exist, including pitot tubes, angular vanes, and multihole pressure probes. However, current sensors are either too heavy (>30 g) or require large airspeeds (>20 m/s), making them unsuitable for small uncrewed aerial vehicles. We propose a novel multihole pressure probe, integrating sensing electronics in a single-component structure, resulting in a mechanically robust and lightweight sensor (9 g), which we released to the public domain. Since there is no consensus on two critical design parameters, tip shape (conical vs spherical) and hole spacing (distance between holes), we provide a study on measurement accuracy and noise generation using wind tunnel experiments. The sensor is calibrated using a multivariate polynomial regression model over an airspeed range of 3-27 m/s and an angle of attack/sideslip range of +-35{\\deg}, achieving a mean absolute error of 0.44 m/s and 0.16{\\deg}. Finally, we validated the sensor in outdoor flights near the stall regime. Our probe enabled accurate estimations of airspeed, angle of attack and sideslip during different acrobatic manoeuvres. Due to its size and weight, this sensor will enable safe flight for lightweight, uncrewed aerial vehicles flying at low speeds close to the stall regime."
  },
  {
    "title": "\u03a8-Arena: Interactive Assessment and Optimization of LLM-based Psychological Counselors with Tripartite Feedback",
    "url": "http://arxiv.org/abs/2505.03293v1",
    "arxiv_id": "2505.03293v1",
    "authors": [
      "Shijing Zhu",
      "Zhuang Chen",
      "Guanqun Bi",
      "Binghang Li",
      "Yaxi Deng",
      "Dazhen Wan",
      "Libiao Peng",
      "Xiyao Xiao",
      "Rongsheng Zhang",
      "Tangjie Lv",
      "Zhipeng Hu",
      "FangFang Li",
      "Minlie Huang"
    ],
    "published": "2025-05-06T08:22:51+00:00",
    "summary": "Large language models (LLMs) have shown promise in providing scalable mental health support, while evaluating their counseling capability remains crucial to ensure both efficacy and safety. Existing evaluations are limited by the static assessment that focuses on knowledge tests, the single perspective that centers on user experience, and the open-loop framework that lacks actionable feedback. To address these issues, we propose {\\Psi}-Arena, an interactive framework for comprehensive assessment and optimization of LLM-based counselors, featuring three key characteristics: (1) Realistic arena interactions that simulate real-world counseling through multi-stage dialogues with psychologically profiled NPC clients, (2) Tripartite evaluation that integrates assessments from the client, counselor, and supervisor perspectives, and (3) Closed-loop optimization that iteratively improves LLM counselors using diagnostic feedback. Experiments across eight state-of-the-art LLMs show significant performance variations in different real-world scenarios and evaluation perspectives. Moreover, reflection-based optimization results in up to a 141% improvement in counseling performance. We hope PsychoArena provides a foundational resource for advancing reliable and human-aligned LLM applications in mental healthcare."
  },
  {
    "title": "Enabling Robots to Autonomously Search Dynamic Cluttered Post-Disaster Environments",
    "url": "http://arxiv.org/abs/2505.03283v1",
    "arxiv_id": "2505.03283v1",
    "authors": [
      "Karlo Rado",
      "Mirko Baglioni",
      "Anahita Jamshidnejad"
    ],
    "published": "2025-05-06T08:10:02+00:00",
    "summary": "Robots will bring search and rescue (SaR) in disaster response to another level, in case they can autonomously take over dangerous SaR tasks from humans. A main challenge for autonomous SaR robots is to safely navigate in cluttered environments with uncertainties, while avoiding static and moving obstacles. We propose an integrated control framework for SaR robots in dynamic, uncertain environments, including a computationally efficient heuristic motion planning system that provides a nominal (assuming there are no uncertainties) collision-free trajectory for SaR robots and a robust motion tracking system that steers the robot to track this reference trajectory, taking into account the impact of uncertainties. The control architecture guarantees a balanced trade-off among various SaR objectives, while handling the hard constraints, including safety. The results of various computer-based simulations, presented in this paper, showed significant out-performance (of up to 42.3%) of the proposed integrated control architecture compared to two commonly used state-of-the-art methods (Rapidly-exploring Random Tree and Artificial Potential Function) in reaching targets (e.g., trapped victims in SaR) safely, collision-free, and in the shortest possible time."
  },
  {
    "title": "RADE: Learning Risk-Adjustable Driving Environment via Multi-Agent Conditional Diffusion",
    "url": "http://arxiv.org/abs/2505.03178v1",
    "arxiv_id": "2505.03178v1",
    "authors": [
      "Jiawei Wang",
      "Xintao Yan",
      "Yao Mu",
      "Haowei Sun",
      "Zhong Cao",
      "Henry X. Liu"
    ],
    "published": "2025-05-06T04:41:20+00:00",
    "summary": "Generating safety-critical scenarios in high-fidelity simulations offers a promising and cost-effective approach for efficient testing of autonomous vehicles. Existing methods typically rely on manipulating a single vehicle's trajectory through sophisticated designed objectives to induce adversarial interactions, often at the cost of realism and scalability. In this work, we propose the Risk-Adjustable Driving Environment (RADE), a simulation framework that generates statistically realistic and risk-adjustable traffic scenes. Built upon a multi-agent diffusion architecture, RADE jointly models the behavior of all agents in the environment and conditions their trajectories on a surrogate risk measure. Unlike traditional adversarial methods, RADE learns risk-conditioned behaviors directly from data, preserving naturalistic multi-agent interactions with controllable risk levels. To ensure physical plausibility, we incorporate a tokenized dynamics check module that efficiently filters generated trajectories using a motion vocabulary. We validate RADE on the real-world rounD dataset, demonstrating that it preserves statistical realism across varying risk levels and naturally increases the likelihood of safety-critical events as the desired risk level grows up. Our results highlight RADE's potential as a scalable and realistic tool for AV safety evaluation."
  },
  {
    "title": "VISLIX: An XAI Framework for Validating Vision Models with Slice Discovery and Analysis",
    "url": "http://arxiv.org/abs/2505.03132v1",
    "arxiv_id": "2505.03132v1",
    "authors": [
      "Xinyuan Yan",
      "Xiwei Xuan",
      "Jorge Piazentin Ono",
      "Jiajing Guo",
      "Vikram Mohanty",
      "Shekar Arvind Kumar",
      "Liang Gou",
      "Bei Wang",
      "Liu Ren"
    ],
    "published": "2025-05-06T03:09:15+00:00",
    "summary": "Real-world machine learning models require rigorous evaluation before deployment, especially in safety-critical domains like autonomous driving and surveillance. The evaluation of machine learning models often focuses on data slices, which are subsets of the data that share a set of characteristics. Data slice finding automatically identifies conditions or data subgroups where models underperform, aiding developers in mitigating performance issues. Despite its popularity and effectiveness, data slicing for vision model validation faces several challenges. First, data slicing often needs additional image metadata or visual concepts, and falls short in certain computer vision tasks, such as object detection. Second, understanding data slices is a labor-intensive and mentally demanding process that heavily relies on the expert's domain knowledge. Third, data slicing lacks a human-in-the-loop solution that allows experts to form hypothesis and test them interactively. To overcome these limitations and better support the machine learning operations lifecycle, we introduce VISLIX, a novel visual analytics framework that employs state-of-the-art foundation models to help domain experts analyze slices in computer vision models. Our approach does not require image metadata or visual concepts, automatically generates natural language insights, and allows users to test data slice hypothesis interactively. We evaluate VISLIX with an expert study and three use cases, that demonstrate the effectiveness of our tool in providing comprehensive insights for validating object detection models."
  },
  {
    "title": "Impact Analysis of Inference Time Attack of Perception Sensors on Autonomous Vehicles",
    "url": "http://arxiv.org/abs/2505.03850v1",
    "arxiv_id": "2505.03850v1",
    "authors": [
      "Hanlin Chen",
      "Simin Chen",
      "Wenyu Li",
      "Wei Yang",
      "Yiheng Feng"
    ],
    "published": "2025-05-05T23:00:27+00:00",
    "summary": "As a safety-critical cyber-physical system, cybersecurity and related safety issues for Autonomous Vehicles (AVs) have been important research topics for a while. Among all the modules on AVs, perception is one of the most accessible attack surfaces, as drivers and AVs have no control over the outside environment. Most current work targeting perception security for AVs focuses on perception correctness. In this work, we propose an impact analysis based on inference time attacks for autonomous vehicles. We demonstrate in a simulation system that such inference time attacks can also threaten the safety of both the ego vehicle and other traffic participants."
  },
  {
    "title": "A Modal-Space Formulation for Momentum Observer Contact Estimation and Effects of Uncertainty for Continuum Robots",
    "url": "http://arxiv.org/abs/2505.03044v1",
    "arxiv_id": "2505.03044v1",
    "authors": [
      "Garrison L. H. Johnston",
      "Neel Shihora",
      "Nabil Simaan"
    ],
    "published": "2025-05-05T21:54:01+00:00",
    "summary": "Contact detection for continuum and soft robots has been limited in past works to statics or kinematics-based methods with assumed circular bending curvature or known bending profiles. In this paper, we adapt the generalized momentum observer contact estimation method to continuum robots. This is made possible by leveraging recent results for real-time shape sensing of continuum robots along with a modal-space representation of the robot dynamics. In addition to presenting an approach for estimating the generalized forces due to contact via a momentum observer, we present a constrained optimization method to identify the wrench imparted on the robot during contact. We also present an approach for investigating the effects of unmodeled deviations in the robot's dynamic state on the contact detection method and we validate our algorithm by simulations and experiments. We also compare the performance of the momentum observer to the joint force deviation method, a direct estimation approach using the robot's full dynamic model. We also demonstrate a basic extension of the method to multisegment continuum robots. Results presented in this work extend dynamic contact detection to the domain of continuum and soft robots and can be used to improve the safety of large-scale continuum robots for human-robot collaboration."
  },
  {
    "title": "Evidence of a fraction of LIGO/Virgo/KAGRA events coming from active galactic nuclei",
    "url": "http://arxiv.org/abs/2505.02924v1",
    "arxiv_id": "2505.02924v1",
    "authors": [
      "Liang-Gui Zhu",
      "Xian Chen"
    ],
    "published": "2025-05-05T18:02:47+00:00",
    "summary": "The formation channels of the gravitational-wave (GW) sources detected by LIGO/Virgo/KAGRA (LVK) remain poorly constrained. Active galactic nucleus (AGN) has been proposed as one of the potential hosts but the fraction of GW events originating from AGNs has not been quantified. Here, we constrain the AGN-origin fraction $f_{\\rm agn}$ by analyzing the spatial correlation between GW source localizations ($O1\\!-\\!O4$a) and AGN distributions (SDSS DR16). We find evidence of an excess of low-luminosity ($L_{\\rm bol} \\le 10^{45}~\\!\\mathrm{erg~s}^{-1}$) as well as low-Eddington ratio ($\\lambda_{\\rm Edd} \\le 0.05$) AGNs around the LVK events, the explanation of which requires $f_{\\rm agn} = 0.39^{+0.41}_{-0.32}$ and $0.29^{+0.40}_{-0.25}$ (90\\% confidence level) of the LVK events originating from these respective AGN populations. Monte Carlo simulations confirm that this correlation is unlikely to arise from random coincidence, further supported by anomalous variation of the error of $f_{\\rm agn}$ with GW event counts. These results provide the first observational evidence for GW sources coming from either low-luminosity or low-accretion-rate AGNs, offering critical insights into the environmental dependencies of the formation of GW sources."
  },
  {
    "title": "Stabilizing dark matter with quantum scale symmetry",
    "url": "http://arxiv.org/abs/2505.02803v1",
    "arxiv_id": "2505.02803v1",
    "authors": [
      "Abhishek Chikkaballi",
      "Kamila Kowalska",
      "Rafael R. Lino dos Santos",
      "Enrico Maria Sessolo"
    ],
    "published": "2025-05-05T17:27:04+00:00",
    "summary": "In the context of gauge-Yukawa theories with trans-Planckian asymptotic safety, quantum scale symmetry can prevent the appearance in the Lagrangian of couplings that would otherwise be allowed by the gauge symmetry. Such couplings correspond to irrelevant Gaussian fixed points of the renormalization group flow. Their absence in the theory implies that different sectors of the gauge-Yukawa theory are secluded from one another, in similar fashion to the effects of a global or a discrete symmetry. As an example, we impose the trans-Planckian scale symmetry on a model of Grand Unification based on the gauge group SU(6), showing that it leads to the emergence of several fermionic WIMP dark matter candidates whose coupling strengths are entirely predicted by the UV completion."
  },
  {
    "title": "When Your Own Output Becomes Your Training Data: Noise-to-Meaning Loops and a Formal RSI Trigger",
    "url": "http://arxiv.org/abs/2505.02888v1",
    "arxiv_id": "2505.02888v1",
    "authors": [
      "Rintaro Ando"
    ],
    "published": "2025-05-05T17:03:07+00:00",
    "summary": "We present Noise-to-Meaning Recursive Self-Improvement (N2M-RSI), a minimal formal model showing that once an AI agent feeds its own outputs back as inputs and crosses an explicit information-integration threshold, its internal complexity will grow without bound under our assumptions. The framework unifies earlier ideas on self-prompting large language models, G\\\"odelian self-reference, and AutoML, yet remains implementation-agnostic. The model furthermore scales naturally to interacting swarms of agents, hinting at super-linear effects once communication among instances is permitted. For safety reasons, we omit system-specific implementation details and release only a brief, model-agnostic toy prototype in Appendix C."
  },
  {
    "title": "Event-aware analysis of cross-city visitor flows using large language models and social media data",
    "url": "http://arxiv.org/abs/2505.03847v1",
    "arxiv_id": "2505.03847v1",
    "authors": [
      "Xiaohan Wang",
      "Zhan Zhao",
      "Ruiyu Wang",
      "Yang Xu"
    ],
    "published": "2025-05-05T16:32:05+00:00",
    "summary": "Public events, such as music concerts and fireworks displays, can cause irregular surges in cross-city travel demand, leading to potential overcrowding, travel delays, and public safety concerns. To better anticipate and accommodate such demand surges, it is essential to estimate cross-city visitor flows with awareness of public events. Although prior studies typically focused on the effects of a single mega event or disruptions around a single venue, this study introduces a generalizable framework to analyze visitor flows under diverse and concurrent events. We propose to leverage large language models (LLMs) to extract event features from multi-source online information and massive user-generated content on social media platforms. Specifically, social media popularity metrics are designed to capture the effects of online promotion and word-of-mouth in attracting visitors. An event-aware machine learning model is then adopted to uncover the specific impacts of different event features and ultimately predict visitor flows for upcoming events. Using Hong Kong as a case study, the framework is applied to predict daily flows of mainland Chinese visitors arriving at the city, achieving a testing R-squared of over 85%. We further investigate the heterogeneous event impacts on visitor numbers across different event types and major travel modes. Both promotional popularity and word-of-mouth popularity are found to be associated with increased visitor flows, but the specific effects vary by the event type. This association is more pronounced among visitors arriving by metro and high-speed rail, while it has less effect on air travelers. The findings can facilitate coordinated measures across government agencies and guide specialized transport policies, such as shuttle transit services to event venues, and comprehensive on-site traffic management strategies."
  },
  {
    "title": "A Survey of Slow Thinking-based Reasoning LLMs using Reinforced Learning and Inference-time Scaling Law",
    "url": "http://arxiv.org/abs/2505.02665v1",
    "arxiv_id": "2505.02665v1",
    "authors": [
      "Qianjun Pan",
      "Wenkai Ji",
      "Yuyang Ding",
      "Junsong Li",
      "Shilian Chen",
      "Junyi Wang",
      "Jie Zhou",
      "Qin Chen",
      "Min Zhang",
      "Yulan Wu",
      "Liang He"
    ],
    "published": "2025-05-05T14:14:59+00:00",
    "summary": "This survey explores recent advancements in reasoning large language models (LLMs) designed to mimic \"slow thinking\" - a reasoning process inspired by human cognition, as described in Kahneman's Thinking, Fast and Slow. These models, like OpenAI's o1, focus on scaling computational resources dynamically during complex tasks, such as math reasoning, visual reasoning, medical diagnosis, and multi-agent debates. We present the development of reasoning LLMs and list their key technologies. By synthesizing over 100 studies, it charts a path toward LLMs that combine human-like deep thinking with scalable efficiency for reasoning. The review breaks down methods into three categories: (1) test-time scaling dynamically adjusts computation based on task complexity via search and sampling, dynamic verification; (2) reinforced learning refines decision-making through iterative improvement leveraging policy networks, reward models, and self-evolution strategies; and (3) slow-thinking frameworks (e.g., long CoT, hierarchical processes) that structure problem-solving with manageable steps. The survey highlights the challenges and further directions of this domain. Understanding and advancing the reasoning abilities of LLMs is crucial for unlocking their full potential in real-world applications, from scientific discovery to decision support systems."
  },
  {
    "title": "Faithful and secure distributed quantum sensing under general-coherent attacks",
    "url": "http://arxiv.org/abs/2505.02620v1",
    "arxiv_id": "2505.02620v1",
    "authors": [
      "G. Bizzarri",
      "M. Barbieri",
      "M. Manrique",
      "M. Parisi",
      "F. Bruni",
      "I. Gianani",
      "M. Rosati"
    ],
    "published": "2025-05-05T12:48:41+00:00",
    "summary": "Quantum metrology and cryptography can be combined in a distributed and/or remote sensing setting, where distant end-users with limited quantum capabilities can employ quantum states, transmitted by a quantum-powerful provider via a quantum network, to perform quantum-enhanced parameter estimation in a private fashion. Previous works on the subject have been limited by restricted assumptions on the capabilities of a potential eavesdropper and the use of abort-based protocols that prevent a simple practical realization. Here we introduce, theoretically analyze, and experimentally demonstrate single- and two-way protocols for distributed sensing combining several unique and desirable features: (i) a safety-threshold mechanism that allows the protocol to proceed in low-noise cases and quantifying the potential tampering with respect to the ideal estimation procedure, effectively paving the way for wide-spread practical realizations; (ii) equivalence of entanglement-based and mutually-unbiased-bases-based formulations; (iii) robustness against collective attacks via a LOCC-de-Finetti theorem, for the first time to our knowledge. Finally, we demonstrate our protocols in a photonic-based implementation, observing that the possibility of guaranteeing a safety threshold may come at a significant price in terms of the estimation bias, potentially overestimating the effect of tampering in practical settings."
  },
  {
    "title": "LiDAR-Inertial SLAM-Based Navigation and Safety-Oriented AI-Driven Control System for Skid-Steer Robots",
    "url": "http://arxiv.org/abs/2505.02598v1",
    "arxiv_id": "2505.02598v1",
    "authors": [
      "Mehdi Heydari Shahna",
      "Eemil Haaparanta",
      "Pauli Mustalahti",
      "Jouni Mattila"
    ],
    "published": "2025-05-05T12:07:35+00:00",
    "summary": "Integrating artificial intelligence (AI) and stochastic technologies into the mobile robot navigation and control (MRNC) framework while adhering to rigorous safety standards presents significant challenges. To address these challenges, this paper proposes a comprehensively integrated MRNC framework for skid-steer wheeled mobile robots (SSWMRs), in which all components are actively engaged in real-time execution. The framework comprises: 1) a LiDAR-inertial simultaneous localization and mapping (SLAM) algorithm for estimating the current pose of the robot within the built map; 2) an effective path-following control system for generating desired linear and angular velocity commands based on the current pose and the desired pose; 3) inverse kinematics for transferring linear and angular velocity commands into left and right side velocity commands; and 4) a robust AI-driven (RAID) control system incorporating a radial basis function network (RBFN) with a new adaptive algorithm to enforce in-wheel actuation systems to track each side motion commands. To further meet safety requirements, the proposed RAID control within the MRNC framework of the SSWMR constrains AI-generated tracking performance within predefined overshoot and steady-state error limits, while ensuring robustness and system stability by compensating for modeling errors, unknown RBF weights, and external forces. Experimental results verify the proposed MRNC framework performance for a 4,836 kg SSWMR operating on soft terrain."
  },
  {
    "title": "Point Cloud Recombination: Systematic Real Data Augmentation Using Robotic Targets for LiDAR Perception Validation",
    "url": "http://arxiv.org/abs/2505.02476v1",
    "arxiv_id": "2505.02476v1",
    "authors": [
      "Hubert Padusinski",
      "Christian Steinhauser",
      "Christian Scherl",
      "Julian Gaal",
      "Jacob Langner"
    ],
    "published": "2025-05-05T09:00:16+00:00",
    "summary": "The validation of LiDAR-based perception of intelligent mobile systems operating in open-world applications remains a challenge due to the variability of real environmental conditions. Virtual simulations allow the generation of arbitrary scenes under controlled conditions but lack physical sensor characteristics, such as intensity responses or material-dependent effects. In contrast, real-world data offers true sensor realism but provides less control over influencing factors, hindering sufficient validation. Existing approaches address this problem with augmentation of real-world point cloud data by transferring objects between scenes. However, these methods do not consider validation and remain limited in controllability because they rely on empirical data. We solve these limitations by proposing Point Cloud Recombination, which systematically augments captured point cloud scenes by integrating point clouds acquired from physical target objects measured in controlled laboratory environments. Thus enabling the creation of vast amounts and varieties of repeatable, physically accurate test scenes with respect to phenomena-aware occlusions with registered 3D meshes. Using the Ouster OS1-128 Rev7 sensor, we demonstrate the augmentation of real-world urban and rural scenes with humanoid targets featuring varied clothing and poses, for repeatable positioning. We show that the recombined scenes closely match real sensor outputs, enabling targeted testing, scalable failure analysis, and improved system safety. By providing controlled yet sensor-realistic data, our method enables trustworthy conclusions about the limitations of specific sensors in compound with their algorithms, e.g., object detection."
  },
  {
    "title": "A Real-Time Control Barrier Function-Based Safety Filter for Motion Planning with Arbitrary Road Boundary Constraints",
    "url": "http://arxiv.org/abs/2505.02395v1",
    "arxiv_id": "2505.02395v1",
    "authors": [
      "Jianye Xu",
      "Chang Che",
      "Bassam Alrifaee"
    ],
    "published": "2025-05-05T06:36:26+00:00",
    "summary": "We present a real-time safety filter for motion planning, such as learning-based methods, using Control Barrier Functions (CBFs), which provides formal guarantees for collision avoidance with road boundaries. A key feature of our approach is its ability to directly incorporate road geometries of arbitrary shape without resorting to conservative overapproximations. We formulate the safety filter as a constrained optimization problem in the form of a Quadratic Program (QP). It achieves safety by making minimal, necessary adjustments to the control actions issued by the nominal motion planner. We validate our safety filter through extensive numerical experiments across a variety of traffic scenarios featuring complex roads. The results confirm its reliable safety and high computational efficiency (execution frequency up to 40 Hz). Code & Video Demo: github.com/bassamlab/SigmaRL"
  },
  {
    "title": "Quantitative Analysis of Performance Drop in DeepSeek Model Quantization",
    "url": "http://arxiv.org/abs/2505.02390v1",
    "arxiv_id": "2505.02390v1",
    "authors": [
      "Enbo Zhao",
      "Yi Shen",
      "Shuming Shi",
      "Jieyun Huang",
      "Zhihao Chen",
      "Ning Wang",
      "Siqi Xiao",
      "Jian Zhang",
      "Kai Wang",
      "Shiguo Lian"
    ],
    "published": "2025-05-05T06:25:20+00:00",
    "summary": "Recently, there is a high demand for deploying DeepSeek-R1 and V3 locally, possibly because the official service often suffers from being busy and some organizations have data privacy concerns. While single-machine deployment offers infrastructure simplicity, the models' 671B FP8 parameter configuration exceeds the practical memory limits of a standard 8-GPU machine. Quantization is a widely used technique that helps reduce model memory consumption. However, it is unclear what the performance of DeepSeek-R1 and V3 will be after being quantized. This technical report presents the first quantitative evaluation of multi-bitwidth quantization across the complete DeepSeek model spectrum. Key findings reveal that 4-bit quantization maintains little performance degradation versus FP8 while enabling single-machine deployment on standard NVIDIA GPU devices. We further propose DQ3_K_M, a dynamic 3-bit quantization method that significantly outperforms traditional Q3_K_M variant on various benchmarks, which is also comparable with 4-bit quantization (Q4_K_M) approach in most tasks. Moreover, DQ3_K_M supports single-machine deployment configurations for both NVIDIA H100/A100 and Huawei 910B. Our implementation of DQ3\\_K\\_M is released at https://github.com/UnicomAI/DeepSeek-Eval, containing optimized 3-bit quantized variants of both DeepSeek-R1 and DeepSeek-V3."
  },
  {
    "title": "RouthSearch: Inferring PID Parameter Specification for Flight Control Program by Coordinate Search",
    "url": "http://arxiv.org/abs/2505.02357v1",
    "arxiv_id": "2505.02357v1",
    "authors": [
      "Siao Wang",
      "Zhen Dong",
      "Hui Li",
      "Liwei Shen",
      "Xin Peng",
      "Dongdong She"
    ],
    "published": "2025-05-05T04:38:48+00:00",
    "summary": "Flight control programs use PID control modules with user-configurable Proportional (P), Integral (I), and Derivative (D) parameters to manage UAV flying behaviors. Users can adjust these PID parameters during flight. However, flight control programs lack sufficient safety checks on user-provided PID parameters, leading to a severe UAV vulnerability - the input validation bug. This occurs when a user misconfigures PID parameters, causing dangerous states like deviation from the expected path, loss of control, or crash.   Prior works use random testing like fuzzing, but these are not effective in the three-dimensional search space of PID parameters. The expensive dynamic execution of UAV tests further hinders random testing performance.   We address PID parameter misconfiguration by combining the Routh-Hurwitz stability criterion with coordinate search, introducing RouthSearch. Instead of ad-hoc identification, RouthSearch principledly determines valid ranges for three-dimensional PID parameters. We first leverage the Routh-Hurwitz Criterion to identify a theoretical PID parameter boundary, then refine it using efficient coordinate search. The determined valid range can filter misconfigured PID parameters from users during flight and help discover logical bugs in flight control programs.   We evaluated RouthSearch across eight flight modes in PX4 and Ardupilot. Results show RouthSearch determines valid ranges with 92.0% accuracy compared to ground truth. RouthSearch discovers 3,853 PID misconfigurations within 48 hours, while the STOA work PGFuzz discovers only 449 sets, significantly outperforming prior works by 8.58 times. Our method also helped detect three bugs in ArduPilot and PX4."
  },
  {
    "title": "HyperTree Planning: Enhancing LLM Reasoning via Hierarchical Thinking",
    "url": "http://arxiv.org/abs/2505.02322v1",
    "arxiv_id": "2505.02322v1",
    "authors": [
      "Runquan Gui",
      "Zhihai Wang",
      "Jie Wang",
      "Chi Ma",
      "Huiling Zhen",
      "Mingxuan Yuan",
      "Jianye Hao",
      "Defu Lian",
      "Enhong Chen",
      "Feng Wu"
    ],
    "published": "2025-05-05T02:38:58+00:00",
    "summary": "Recent advancements have significantly enhanced the performance of large language models (LLMs) in tackling complex reasoning tasks, achieving notable success in domains like mathematical and logical reasoning. However, these methods encounter challenges with complex planning tasks, primarily due to extended reasoning steps, diverse constraints, and the challenge of handling multiple distinct sub-tasks. To address these challenges, we propose HyperTree Planning (HTP), a novel reasoning paradigm that constructs hypertree-structured planning outlines for effective planning. The hypertree structure enables LLMs to engage in hierarchical thinking by flexibly employing the divide-and-conquer strategy, effectively breaking down intricate reasoning steps, accommodating diverse constraints, and managing multiple distinct sub-tasks in a well-organized manner. We further introduce an autonomous planning framework that completes the planning process by iteratively refining and expanding the hypertree-structured planning outlines. Experiments demonstrate the effectiveness of HTP, achieving state-of-the-art accuracy on the TravelPlanner benchmark with Gemini-1.5-Pro, resulting in a 3.6 times performance improvement over o1-preview."
  },
  {
    "title": "What Is AI Safety? What Do We Want It to Be?",
    "url": "http://arxiv.org/abs/2505.02313v1",
    "arxiv_id": "2505.02313v1",
    "authors": [
      "Jacqueline Harding",
      "Cameron Domenico Kirk-Giannini"
    ],
    "published": "2025-05-05T01:55:00+00:00",
    "summary": "The field of AI safety seeks to prevent or reduce the harms caused by AI systems. A simple and appealing account of what is distinctive of AI safety as a field holds that this feature is constitutive: a research project falls within the purview of AI safety just in case it aims to prevent or reduce the harms caused by AI systems. Call this appealingly simple account The Safety Conception of AI safety. Despite its simplicity and appeal, we argue that The Safety Conception is in tension with at least two trends in the ways AI safety researchers and organizations think and talk about AI safety: first, a tendency to characterize the goal of AI safety research in terms of catastrophic risks from future systems; second, the increasingly popular idea that AI safety can be thought of as a branch of safety engineering. Adopting the methodology of conceptual engineering, we argue that these trends are unfortunate: when we consider what concept of AI safety it would be best to have, there are compelling reasons to think that The Safety Conception is the answer. Descriptively, The Safety Conception allows us to see how work on topics that have historically been treated as central to the field of AI safety is continuous with work on topics that have historically been treated as more marginal, like bias, misinformation, and privacy. Normatively, taking The Safety Conception seriously means approaching all efforts to prevent or mitigate harms from AI systems based on their merits rather than drawing arbitrary distinctions between them."
  },
  {
    "title": "SafeMate: A Model Context Protocol-Based Multimodal Agent for Emergency Preparedness",
    "url": "http://arxiv.org/abs/2505.02306v1",
    "arxiv_id": "2505.02306v1",
    "authors": [
      "Junfeng Jiao",
      "Jihyung Park",
      "Yiming Xu",
      "Lucy Atkinson"
    ],
    "published": "2025-05-05T01:09:02+00:00",
    "summary": "Despite the abundance of public safety documents and emergency protocols, most individuals remain ill-equipped to interpret and act on such information during crises. Traditional emergency decision support systems (EDSS) are designed for professionals and rely heavily on static documents like PDFs or SOPs, which are difficult for non-experts to navigate under stress. This gap between institutional knowledge and public accessibility poses a critical barrier to effective emergency preparedness and response.   We introduce SafeMate, a retrieval-augmented AI assistant that delivers accurate, context-aware guidance to general users in both preparedness and active emergency scenarios. Built on the Model Context Protocol (MCP), SafeMate dynamically routes user queries to tools for document retrieval, checklist generation, and structured summarization. It uses FAISS with cosine similarity to identify relevant content from trusted sources."
  },
  {
    "title": "Half-Ice, Half-Fire Driven Ultranarrow Phase Crossover in 1D Decorated q-State Potts Ferrimagnets: An AI-Co-Led Discovery",
    "url": "http://arxiv.org/abs/2505.02303v1",
    "arxiv_id": "2505.02303v1",
    "authors": [
      "Weiguo Yin"
    ],
    "published": "2025-05-05T00:49:17+00:00",
    "summary": "OpenAI's reasoning model o3-mini-high was used to carry out an exact analytic study of one-dimensional ferrimagnetic decorated $q$-state Potts models. We demonstrate that the finite-temperature ultranarrow phase crossover (UNPC), driven by a hidden \"half-ice, half-fire\" state in the $q=2$ Potts (Ising) model, persists for $q>2$. We identify novel features for $q>2$, including the dome structure in the field-temperature phase diagram and for large $q$ a secondary high-temperature UNPC to the fully disordered paramagnetic state. The ice-fire mechanism of spin flipping can be applied to higher-dimensional Potts models. These results establish a versatile framework for engineering controlled fast state-flipping switches in low-dimensional systems. Our nine-level AI-contribution rating assigns AI the meritorious status of AI-co-led discovery in this work."
  },
  {
    "title": "Adaptive Scoring and Thresholding with Human Feedback for Robust Out-of-Distribution Detection",
    "url": "http://arxiv.org/abs/2505.02299v1",
    "arxiv_id": "2505.02299v1",
    "authors": [
      "Daisuke Yamada",
      "Harit Vishwakarma",
      "Ramya Korlakai Vinayak"
    ],
    "published": "2025-05-05T00:25:14+00:00",
    "summary": "Machine Learning (ML) models are trained on in-distribution (ID) data but often encounter out-of-distribution (OOD) inputs during deployment -- posing serious risks in safety-critical domains. Recent works have focused on designing scoring functions to quantify OOD uncertainty, with score thresholds typically set based solely on ID data to achieve a target true positive rate (TPR), since OOD data is limited before deployment. However, these TPR-based thresholds leave false positive rates (FPR) uncontrolled, often resulting in high FPRs where OOD points are misclassified as ID. Moreover, fixed scoring functions and thresholds lack the adaptivity needed to handle newly observed, evolving OOD inputs, leading to sub-optimal performance. To address these challenges, we propose a human-in-the-loop framework that \\emph{safely updates both scoring functions and thresholds on the fly} based on real-world OOD inputs. Our method maximizes TPR while strictly controlling FPR at all times, even as the system adapts over time. We provide theoretical guarantees for FPR control under stationary conditions and present extensive empirical evaluations on OpenOOD benchmarks to demonstrate that our approach outperforms existing methods by achieving higher TPRs while maintaining FPR control."
  },
  {
    "title": "RNBF: Real-Time RGB-D Based Neural Barrier Functions for Safe Robotic Navigation",
    "url": "http://arxiv.org/abs/2505.02294v1",
    "arxiv_id": "2505.02294v1",
    "authors": [
      "Satyajeet Das",
      "Yifan Xue",
      "Haoming Li",
      "Nadia Figueroa"
    ],
    "published": "2025-05-04T23:43:44+00:00",
    "summary": "Autonomous safe navigation in unstructured and novel environments poses significant challenges, especially when environment information can only be provided through low-cost vision sensors. Although safe reactive approaches have been proposed to ensure robot safety in complex environments, many base their theory off the assumption that the robot has prior knowledge on obstacle locations and geometries. In this paper, we present a real-time, vision-based framework that constructs continuous, first-order differentiable Signed Distance Fields (SDFs) of unknown environments without any pre-training. Our proposed method ensures full compatibility with established SDF-based reactive controllers. To achieve robust performance under practical sensing conditions, our approach explicitly accounts for noise in affordable RGB-D cameras, refining the neural SDF representation online for smoother geometry and stable gradient estimates. We validate the proposed method in simulation and real-world experiments using a Fetch robot."
  },
  {
    "title": "Resolving Conflicting Constraints in Multi-Agent Reinforcement Learning with Layered Safety",
    "url": "http://arxiv.org/abs/2505.02293v1",
    "arxiv_id": "2505.02293v1",
    "authors": [
      "Jason J. Choi",
      "Jasmine Jerry Aloor",
      "Jingqi Li",
      "Maria G. Mendoza",
      "Hamsa Balakrishnan",
      "Claire J. Tomlin"
    ],
    "published": "2025-05-04T23:42:52+00:00",
    "summary": "Preventing collisions in multi-robot navigation is crucial for deployment. This requirement hinders the use of learning-based approaches, such as multi-agent reinforcement learning (MARL), on their own due to their lack of safety guarantees. Traditional control methods, such as reachability and control barrier functions, can provide rigorous safety guarantees when interactions are limited only to a small number of robots. However, conflicts between the constraints faced by different agents pose a challenge to safe multi-agent coordination.   To overcome this challenge, we propose a method that integrates multiple layers of safety by combining MARL with safety filters. First, MARL is used to learn strategies that minimize multiple agent interactions, where multiple indicates more than two. Particularly, we focus on interactions likely to result in conflicting constraints within the engagement distance. Next, for agents that enter the engagement distance, we prioritize pairs requiring the most urgent corrective actions. Finally, a dedicated safety filter provides tactical corrective actions to resolve these conflicts. Crucially, the design decisions for all layers of this framework are grounded in reachability analysis and a control barrier-value function-based filtering mechanism.   We validate our Layered Safe MARL framework in 1) hardware experiments using Crazyflie drones and 2) high-density advanced aerial mobility (AAM) operation scenarios, where agents navigate to designated waypoints while avoiding collisions. The results show that our method significantly reduces conflict while maintaining safety without sacrificing much efficiency (i.e., shorter travel time and distance) compared to baselines that do not incorporate layered safety. The project website is available at \\href{https://dinamo-mit.github.io/Layered-Safe-MARL/}{[this https URL]}"
  },
  {
    "title": "On the Need for a Statistical Foundation in Scenario-Based Testing of Autonomous Vehicles",
    "url": "http://arxiv.org/abs/2505.02274v1",
    "arxiv_id": "2505.02274v1",
    "authors": [
      "Xingyu Zhao",
      "Robab Aghazadeh-Chakherlou",
      "Chih-Hong Cheng",
      "Peter Popov",
      "Lorenzo Strigini"
    ],
    "published": "2025-05-04T22:06:23+00:00",
    "summary": "Scenario-based testing has emerged as a common method for autonomous vehicles (AVs) safety, offering a more efficient alternative to mile-based testing by focusing on high-risk scenarios. However, fundamental questions persist regarding its stopping rules, residual risk estimation, debug effectiveness, and the impact of simulation fidelity on safety claims. This paper argues that a rigorous statistical foundation is essential to address these challenges and enable rigorous safety assurance. By drawing parallels between AV testing and traditional software testing methodologies, we identify shared research gaps and reusable solutions. We propose proof-of-concept models to quantify the probability of failure per scenario (pfs) and evaluate testing effectiveness under varying conditions. Our analysis reveals that neither scenario-based nor mile-based testing universally outperforms the other. Furthermore, we introduce Risk Estimation Fidelity (REF), a novel metric to certify the alignment of synthetic and real-world testing outcomes, ensuring simulation-based safety claims are statistically defensible."
  },
  {
    "title": "Uncertainty Quantification for Machine Learning in Healthcare: A Survey",
    "url": "http://arxiv.org/abs/2505.02874v1",
    "arxiv_id": "2505.02874v1",
    "authors": [
      "L. Juli\u00e1n Lechuga L\u00f3pez",
      "Shaza Elsharief",
      "Dhiyaa Al Jorf",
      "Firas Darwish",
      "Congbo Ma",
      "Farah E. Shamout"
    ],
    "published": "2025-05-04T16:56:22+00:00",
    "summary": "Uncertainty Quantification (UQ) is pivotal in enhancing the robustness, reliability, and interpretability of Machine Learning (ML) systems for healthcare, optimizing resources and improving patient care. Despite the emergence of ML-based clinical decision support tools, the lack of principled quantification of uncertainty in ML models remains a major challenge. Current reviews have a narrow focus on analyzing the state-of-the-art UQ in specific healthcare domains without systematically evaluating method efficacy across different stages of model development, and despite a growing body of research, its implementation in healthcare applications remains limited. Therefore, in this survey, we provide a comprehensive analysis of current UQ in healthcare, offering an informed framework that highlights how different methods can be integrated into each stage of the ML pipeline including data processing, training and evaluation. We also highlight the most popular methods used in healthcare and novel approaches from other domains that hold potential for future adoption in the medical context. We expect this study will provide a clear overview of the challenges and opportunities of implementing UQ in the ML pipeline for healthcare, guiding researchers and practitioners in selecting suitable techniques to enhance the reliability, safety and trust from patients and clinicians on ML-driven healthcare solutions."
  },
  {
    "title": "Open Challenges in Multi-Agent Security: Towards Secure Systems of Interacting AI Agents",
    "url": "http://arxiv.org/abs/2505.02077v1",
    "arxiv_id": "2505.02077v1",
    "authors": [
      "Christian Schroeder de Witt"
    ],
    "published": "2025-05-04T12:03:29+00:00",
    "summary": "Decentralized AI agents will soon interact across internet platforms, creating security challenges beyond traditional cybersecurity and AI safety frameworks. Free-form protocols are essential for AI's task generalization but enable new threats like secret collusion and coordinated swarm attacks. Network effects can rapidly spread privacy breaches, disinformation, jailbreaks, and data poisoning, while multi-agent dispersion and stealth optimization help adversaries evade oversightcreating novel persistent threats at a systemic level. Despite their critical importance, these security challenges remain understudied, with research fragmented across disparate fields including AI security, multi-agent learning, complex systems, cybersecurity, game theory, distributed systems, and technical AI governance. We introduce \\textbf{multi-agent security}, a new field dedicated to securing networks of decentralized AI agents against threats that emerge or amplify through their interactionswhether direct or indirect via shared environmentswith each other, humans, and institutions, and characterize fundamental security-performance trade-offs. Our preliminary work (1) taxonomizes the threat landscape arising from interacting AI agents, (2) surveys security-performance tradeoffs in decentralized AI systems, and (3) proposes a unified research agenda addressing open challenges in designing secure agent systems and interaction environments. By identifying these gaps, we aim to guide research in this critical area to unlock the socioeconomic potential of large-scale agent deployment on the internet, foster public trust, and mitigate national security risks in critical infrastructure and defense contexts."
  },
  {
    "title": "Enhancing Safety Standards in Automated Systems Using Dynamic Bayesian Networks",
    "url": "http://arxiv.org/abs/2505.02050v1",
    "arxiv_id": "2505.02050v1",
    "authors": [
      "Kranthi Kumar Talluri",
      "Anders L. Madsen",
      "Galia Weidl"
    ],
    "published": "2025-05-04T09:58:02+00:00",
    "summary": "Cut-in maneuvers in high-speed traffic pose critical challenges that can lead to abrupt braking and collisions, necessitating safe and efficient lane change strategies. We propose a Dynamic Bayesian Network (DBN) framework to integrate lateral evidence with safety assessment models, thereby predicting lane changes and ensuring safe cut-in maneuvers effectively. Our proposed framework comprises three key probabilistic hypotheses (lateral evidence, lateral safety, and longitudinal safety) that facilitate the decision-making process through dynamic data processing and assessments of vehicle positions, lateral velocities, relative distance, and Time-to-Collision (TTC) computations. The DBN model's performance compared with other conventional approaches demonstrates superior performance in crash reduction, especially in critical high-speed scenarios, while maintaining a competitive performance in low-speed scenarios. This paves the way for robust, scalable, and efficient safety validation in automated driving systems."
  },
  {
    "title": "R-Bench: Graduate-level Multi-disciplinary Benchmarks for LLM & MLLM Complex Reasoning Evaluation",
    "url": "http://arxiv.org/abs/2505.02018v1",
    "arxiv_id": "2505.02018v1",
    "authors": [
      "Meng-Hao Guo",
      "Jiajun Xu",
      "Yi Zhang",
      "Jiaxi Song",
      "Haoyang Peng",
      "Yi-Xuan Deng",
      "Xinzhi Dong",
      "Kiyohiro Nakayama",
      "Zhengyang Geng",
      "Chen Wang",
      "Bolin Ni",
      "Guo-Wei Yang",
      "Yongming Rao",
      "Houwen Peng",
      "Han Hu",
      "Gordon Wetzstein",
      "Shi-min Hu"
    ],
    "published": "2025-05-04T07:48:36+00:00",
    "summary": "Reasoning stands as a cornerstone of intelligence, enabling the synthesis of existing knowledge to solve complex problems. Despite remarkable progress, existing reasoning benchmarks often fail to rigorously evaluate the nuanced reasoning capabilities required for complex, real-world problemsolving, particularly in multi-disciplinary and multimodal contexts. In this paper, we introduce a graduate-level, multi-disciplinary, EnglishChinese benchmark, dubbed as Reasoning Bench (R-Bench), for assessing the reasoning capability of both language and multimodal models. RBench spans 1,094 questions across 108 subjects for language model evaluation and 665 questions across 83 subjects for multimodal model testing in both English and Chinese. These questions are meticulously curated to ensure rigorous difficulty calibration, subject balance, and crosslinguistic alignment, enabling the assessment to be an Olympiad-level multi-disciplinary benchmark. We evaluate widely used models, including OpenAI o1, GPT-4o, DeepSeek-R1, etc. Experimental results indicate that advanced models perform poorly on complex reasoning, especially multimodal reasoning. Even the top-performing model OpenAI o1 achieves only 53.2% accuracy on our multimodal evaluation. Data and code are made publicly available at here."
  },
  {
    "title": "Visual Dominance and Emerging Multimodal Approaches in Distracted Driving Detection: A Review of Machine Learning Techniques",
    "url": "http://arxiv.org/abs/2505.01973v1",
    "arxiv_id": "2505.01973v1",
    "authors": [
      "Anthony Dontoh",
      "Stephanie Ivey",
      "Logan Sirbaugh",
      "Andrews Danyo",
      "Armstrong Aboah"
    ],
    "published": "2025-05-04T02:51:00+00:00",
    "summary": "Distracted driving continues to be a significant cause of road traffic injuries and fatalities worldwide, even with advancements in driver monitoring technologies. Recent developments in machine learning (ML) and deep learning (DL) have primarily focused on visual data to detect distraction, often neglecting the complex, multimodal nature of driver behavior. This systematic review assesses 74 peer-reviewed studies from 2019 to 2024 that utilize ML/DL techniques for distracted driving detection across visual, sensor-based, multimodal, and emerging modalities. The review highlights a significant prevalence of visual-only models, particularly convolutional neural networks (CNNs) and temporal architectures, which achieve high accuracy but show limited generalizability in real-world scenarios. Sensor-based and physiological models provide complementary strengths by capturing internal states and vehicle dynamics, while emerging techniques, such as auditory sensing and radio frequency (RF) methods, offer privacy-aware alternatives. Multimodal architecture consistently surpasses unimodal baselines, demonstrating enhanced robustness, context awareness, and scalability by integrating diverse data streams. These findings emphasize the need to move beyond visual-only approaches and adopt multimodal systems that combine visual, physiological, and vehicular cues while keeping in checking the need to balance computational requirements. Future research should focus on developing lightweight, deployable multimodal frameworks, incorporating personalized baselines, and establishing cross-modality benchmarks to ensure real-world reliability in advanced driver assistance systems (ADAS) and road safety interventions."
  },
  {
    "title": "A Comprehensive Analysis for Visual Object Hallucination in Large Vision-Language Models",
    "url": "http://arxiv.org/abs/2505.01958v1",
    "arxiv_id": "2505.01958v1",
    "authors": [
      "Liqiang Jing",
      "Guiming Hardy Chen",
      "Ehsan Aghazadeh",
      "Xin Eric Wang",
      "Xinya Du"
    ],
    "published": "2025-05-04T01:47:58+00:00",
    "summary": "Large Vision-Language Models (LVLMs) demonstrate remarkable capabilities in multimodal tasks, but visual object hallucination remains a persistent issue. It refers to scenarios where models generate inaccurate visual object-related information based on the query input, potentially leading to misinformation and concerns about safety and reliability. Previous works focus on the evaluation and mitigation of visual hallucinations, but the underlying causes have not been comprehensively investigated. In this paper, we analyze each component of LLaVA-like LVLMs -- the large language model, the vision backbone, and the projector -- to identify potential sources of error and their impact. Based on our observations, we propose methods to mitigate hallucination for each problematic component. Additionally, we developed two hallucination benchmarks: QA-VisualGenome, which emphasizes attribute and relation hallucinations, and QA-FB15k, which focuses on cognition-based hallucinations."
  },
  {
    "title": "Runtime Anomaly Detection for Drones: An Integrated Rule-Mining and Unsupervised-Learning Approach",
    "url": "http://arxiv.org/abs/2505.01947v1",
    "arxiv_id": "2505.01947v1",
    "authors": [
      "Ivan Tan",
      "Wei Minn",
      "Christopher M. Poskitt",
      "Lwin Khin Shar",
      "Lingxiao Jiang"
    ],
    "published": "2025-05-03T23:48:50+00:00",
    "summary": "UAVs, commonly referred to as drones, have witnessed a remarkable surge in popularity due to their versatile applications. These cyber-physical systems depend on multiple sensor inputs, such as cameras, GPS receivers, accelerometers, and gyroscopes, with faults potentially leading to physical instability and serious safety concerns. To mitigate such risks, anomaly detection has emerged as a crucial safeguarding mechanism, capable of identifying the physical manifestations of emerging issues and allowing operators to take preemptive action at runtime. Recent anomaly detection methods based on LSTM neural networks have shown promising results, but three challenges persist: the need for models that can generalise across the diverse mission profiles of drones; the need for interpretability, enabling operators to understand the nature of detected problems; and the need for capturing domain knowledge that is difficult to infer solely from log data. Motivated by these challenges, this paper introduces RADD, an integrated approach to anomaly detection in drones that combines rule mining and unsupervised learning. In particular, we leverage rules (or invariants) to capture expected relationships between sensors and actuators during missions, and utilise unsupervised learning techniques to cover more subtle relationships that the rules may have missed. We implement this approach using the ArduPilot drone software in the Gazebo simulator, utilising 44 rules derived across the main phases of drone missions, in conjunction with an ensemble of five unsupervised learning models. We find that our integrated approach successfully detects 93.84% of anomalies over six types of faults with a low false positive rate (2.33%), and can be deployed effectively at runtime. Furthermore, RADD outperforms a state-of-the-art LSTM-based method in detecting the different types of faults evaluated in our study."
  },
  {
    "title": "Multimodal Graph Representation Learning for Robust Surgical Workflow Recognition with Adversarial Feature Disentanglement",
    "url": "http://arxiv.org/abs/2505.01766v1",
    "arxiv_id": "2505.01766v1",
    "authors": [
      "Long Bai",
      "Boyi Ma",
      "Ruohan Wang",
      "Guankun Wang",
      "Beilei Cui",
      "Zhongliang Jiang",
      "Mobarakol Islam",
      "Zhe Min",
      "Jiewen Lai",
      "Nassir Navab",
      "Hongliang Ren"
    ],
    "published": "2025-05-03T09:43:30+00:00",
    "summary": "Surgical workflow recognition is vital for automating tasks, supporting decision-making, and training novice surgeons, ultimately improving patient safety and standardizing procedures. However, data corruption can lead to performance degradation due to issues like occlusion from bleeding or smoke in surgical scenes and problems with data storage and transmission. In this case, we explore a robust graph-based multimodal approach to integrating vision and kinematic data to enhance accuracy and reliability. Vision data captures dynamic surgical scenes, while kinematic data provides precise movement information, overcoming limitations of visual recognition under adverse conditions. We propose a multimodal Graph Representation network with Adversarial feature Disentanglement (GRAD) for robust surgical workflow recognition in challenging scenarios with domain shifts or corrupted data. Specifically, we introduce a Multimodal Disentanglement Graph Network that captures fine-grained visual information while explicitly modeling the complex relationships between vision and kinematic embeddings through graph-based message modeling. To align feature spaces across modalities, we propose a Vision-Kinematic Adversarial framework that leverages adversarial training to reduce modality gaps and improve feature consistency. Furthermore, we design a Contextual Calibrated Decoder, incorporating temporal and contextual priors to enhance robustness against domain shifts and corrupted data. Extensive comparative and ablation experiments demonstrate the effectiveness of our model and proposed modules. Moreover, our robustness experiments show that our method effectively handles data corruption during storage and transmission, exhibiting excellent stability and robustness. Our approach aims to advance automated surgical workflow recognition, addressing the complexities and dynamism inherent in surgical procedures."
  },
  {
    "title": "NMPCB: A Lightweight and Safety-Critical Motion Control Framework",
    "url": "http://arxiv.org/abs/2505.01752v1",
    "arxiv_id": "2505.01752v1",
    "authors": [
      "Longze Zheng",
      "Qinghe Liu"
    ],
    "published": "2025-05-03T09:02:35+00:00",
    "summary": "In multi-obstacle environments, real-time performance and safety in robot motion control have long been challenging issues, as conventional methods often struggle to balance the two. In this paper, we propose a novel motion control framework composed of a Neural network-based path planner and a Model Predictive Control (MPC) controller based on control Barrier function (NMPCB) . The planner predicts the next target point through a lightweight neural network and generates a reference trajectory for the controller. In the design of the controller, we introduce the dual problem of control barrier function (CBF) as the obstacle avoidance constraint, enabling it to ensure robot motion safety while significantly reducing computation time. The controller directly outputs control commands to the robot by tracking the reference trajectory. This framework achieves a balance between real-time performance and safety. We validate the feasibility of the framework through numerical simulations and real-world experiments."
  },
  {
    "title": "Cannot See the Forest for the Trees: Invoking Heuristics and Biases to Elicit Irrational Choices of LLMs",
    "url": "http://arxiv.org/abs/2505.02862v1",
    "arxiv_id": "2505.02862v1",
    "authors": [
      "Haoming Yang",
      "Ke Ma",
      "Xiaojun Jia",
      "Yingfei Sun",
      "Qianqian Xu",
      "Qingming Huang"
    ],
    "published": "2025-05-03T05:28:11+00:00",
    "summary": "Despite the remarkable performance of Large Language Models (LLMs), they remain vulnerable to jailbreak attacks, which can compromise their safety mechanisms. Existing studies often rely on brute-force optimization or manual design, failing to uncover potential risks in real-world scenarios. To address this, we propose a novel jailbreak attack framework, ICRT, inspired by heuristics and biases in human cognition. Leveraging the simplicity effect, we employ cognitive decomposition to reduce the complexity of malicious prompts. Simultaneously, relevance bias is utilized to reorganize prompts, enhancing semantic alignment and inducing harmful outputs effectively. Furthermore, we introduce a ranking-based harmfulness evaluation metric that surpasses the traditional binary success-or-failure paradigm by employing ranking aggregation methods such as Elo, HodgeRank, and Rank Centrality to comprehensively quantify the harmfulness of generated content. Experimental results show that our approach consistently bypasses mainstream LLMs' safety mechanisms and generates high-risk content, providing insights into jailbreak attack risks and contributing to stronger defense strategies."
  },
  {
    "title": "Sensing Safety Analysis for Vehicular Networks with Integrated Sensing and Communication (ISAC)",
    "url": "http://arxiv.org/abs/2505.01688v1",
    "arxiv_id": "2505.01688v1",
    "authors": [
      "Tingyu Shui",
      "Walid Saad",
      "Mingzhe Cheng"
    ],
    "published": "2025-05-03T04:39:57+00:00",
    "summary": "Integrated sensing and communication (ISAC) emerged as a key feature of next-generation 6G wireless systems, allowing them to achieve high data rates and sensing accuracy. While prior research has primarily focused on addressing communication safety in ISAC systems, the equally critical issue of sensing safety remains largely ignored. In this paper, a novel threat to the sensing safety of ISAC vehicle networks is studied, whereby a malicious reconfigurable intelligent surface (RIS) is deployed to compromise the sensing functionality of a roadside unit (RSU). Specifically, a malicious attacker dynamically adjusts the phase shifts of an RIS to spoof the sensing outcomes of a vehicular user (VU)'s echo delay, Doppler shift, and angle-of-departure (AoD). To achieve spoofing on Doppler shift estimation, a time-varying phase shift design on the RIS is proposed. Furthermore, the feasible spoofing frequency set with respect to the Doppler shift is analytical derived. Analytical results also demonstrate that the maximum likelihood estimator (MLE) of the AoD can be significantly misled under spoofed Doppler shift estimation. Simulation results validate our theoretical findings, showing that the RIS can induce a spoofed velocity estimation from 0.1 m/s to 14.9 m/s for a VU with velocity of 10 m/s, and can cause an AoD estimation error of up to 65^{\\circ} with only a 5^{\\circ} beam misalignment."
  },
  {
    "title": "Resilient Vehicular Communications under Imperfect Channel State Information",
    "url": "http://arxiv.org/abs/2505.01687v1",
    "arxiv_id": "2505.01687v1",
    "authors": [
      "Tingyu Shui",
      "Walid Saad",
      "Ye Hu",
      "Mingzhe Chen"
    ],
    "published": "2025-05-03T04:33:56+00:00",
    "summary": "Cellular vehicle-to-everything (C-V2X) networks provide a promising solution to improve road safety and traffic efficiency. One key challenge in such systems lies in meeting quality-of-service (QoS) requirements of vehicular communication links given limited network resources, particularly under imperfect channel state information (CSI) conditions caused by the highly dynamic environment. In this paper, a novel two-phase framework is proposed to instill resilience into C-V2X networks under unknown imperfect CSI. The resilience of the C-V2X network is defined, quantified, and optimized the first time through two principal dimensions: absorption phase and adaptation phase. Specifically, the probability distribution function (PDF) of the imperfect CSI is estimated during the absorption phase through dedicated absorption power scheme and resource block (RB) assignment. The estimated PDF is further used to analyze the interplay and reveal the tradeoff between these two phases. Then, a novel metric named hazard rate (HR) is exploited to balance the C-V2X network's prioritization on absorption and adaptation. Finally, the estimated PDF is exploited in the adaptation phase to recover the network's QoS through a real-time power allocation optimization. Simulation results demonstrate the superior capability of the proposed framework in sustaining the QoS of the C-V2X network under imperfect CSI. Specifically, in the adaptation phase, the proposed design reduces the vehicle-tovehicle (V2V) delay that exceeds QoS requirement by 35% and 56%, and improves the average vehicle-to-infrastructure (V2I) throughput by 14% and 16% compared to the model-based and data-driven benchmarks, respectively, without compromising the network's QoS in the absorption phase."
  },
  {
    "title": "Large Language Model Driven Development of Turbulence Models",
    "url": "http://arxiv.org/abs/2505.01681v1",
    "arxiv_id": "2505.01681v1",
    "authors": [
      "Zhongxin Yang",
      "Yuanwei Bin",
      "Yipeng Shi",
      "Xiang I. A. Yang"
    ],
    "published": "2025-05-03T04:01:25+00:00",
    "summary": "Artificial intelligence (AI) has achieved human-level performance in specialized tasks such as Go, image recognition, and protein folding, raising the prospect of an AI singularity-where machines not only match but surpass human reasoning. Here, we demonstrate a step toward this vision in the context of turbulence modeling. By treating a large language model (LLM), DeepSeek-R1, as an equal partner, we establish a closed-loop, iterative workflow in which the LLM proposes, refines, and reasons about near-wall turbulence models under adverse pressure gradients (APGs), system rotation, and surface roughness. Through multiple rounds of interaction involving long-chain reasoning and a priori and a posteriori evaluations, the LLM generates models that not only rediscover established strategies but also synthesize new ones that outperform baseline wall models. Specifically, it recommends incorporating a material derivative to capture history effects in APG flows, modifying the law of the wall to account for system rotation, and developing rough-wall models informed by surface statistics. In contrast to conventional data-driven turbulence modeling-often characterized by human-designed, black-box architectures-the models developed here are physically interpretable and grounded in clear reasoning."
  },
  {
    "title": "Evaluating Input Modalities for Pilot-Centered Taxiway Navigation: Insights from a Wizard-of-Oz Simulation",
    "url": "http://arxiv.org/abs/2505.01679v1",
    "arxiv_id": "2505.01679v1",
    "authors": [
      "Chan Chea Mean",
      "Sameer Alam",
      "Katherine Fennedy",
      "Meng-Hsueh Hsieh",
      "Shiwei Xin",
      "Brian Hilburn"
    ],
    "published": "2025-05-03T03:58:56+00:00",
    "summary": "Runway and taxiway incursions continue to challenge aviation safety, as pilots often experience disorientation from poor visibility in adverse conditions and cognitive workload in complex airport layouts. Current tools, such as airport moving maps on portable tablets, allow manual route planning but do not dynamically adapt to air traffic controllers' (ATCOs) clearances, limiting their effectiveness in high-stress scenarios. This study investigates the impact of different input modalities - paper-based, keyboard touch, map touch, and speech-to-text - on taxiway navigation performance, using a medium-fidelity flight simulator and a Wizard-of-Oz methodology to simulate ideal automation conditions. Contrary to common assumptions, recent studies indicate that paper-based methods outperform digital counterparts in accuracy and efficiency under certain conditions, highlighting critical limitations in current automation strategies. In response, our study investigates why manual methods may excel and how future automation can be optimized for pilot-centered operations. Employing a Wizard-of-Oz approach, we replicated the full taxiing process - from receiving ATCO clearances to executing maneuvers - and differentiated between readback and execution accuracy. Findings reveal that speech-based systems suffer from low pilot trust, necessitating hybrid solutions that integrate error correction and confidence indicators. These insights contribute to the development of future pilot-centered taxiway assistance that enhance situational awareness, minimize workload, and improve overall operational safety."
  },
  {
    "title": "Third-party compliance reviews for frontier AI safety frameworks",
    "url": "http://arxiv.org/abs/2505.01643v1",
    "arxiv_id": "2505.01643v1",
    "authors": [
      "Aidan Homewood",
      "Sophie Williams",
      "Noemi Dreksler",
      "John Lidiard",
      "Malcolm Murray",
      "Lennart Heim",
      "Marta Ziosi",
      "Se\u00e1n \u00d3 h\u00c9igeartaigh",
      "Michael Chen",
      "Kevin Wei",
      "Christoph Winter",
      "Miles Brundage",
      "Ben Garfinkel",
      "Jonas Schuett"
    ],
    "published": "2025-05-03T00:53:55+00:00",
    "summary": "Safety frameworks have emerged as a best practice for managing risks from frontier artificial intelligence (AI) systems. However, it may be difficult for stakeholders to know if companies are adhering to their frameworks. This paper explores a potential solution: third-party compliance reviews. During a third-party compliance review, an independent external party assesses whether a frontier AI company is complying with its safety framework. First, we discuss the main benefits and challenges of such reviews. On the one hand, they can increase compliance with safety frameworks and provide assurance to internal and external stakeholders. On the other hand, they can create information security risks, impose additional cost burdens, and cause reputational damage, but these challenges can be partially mitigated by drawing on best practices from other industries. Next, we answer practical questions about third-party compliance reviews, namely: (1) Who could conduct the review? (2) What information sources could the reviewer consider? (3) How could compliance with the safety framework be assessed? (4) What information about the review could be disclosed externally? (5) How could the findings guide development and deployment actions? (6) When could the reviews be conducted? For each question, we evaluate a set of plausible options. Finally, we suggest \"minimalist\", \"more ambitious\", and \"comprehensive\" approaches for each question that a frontier AI company could adopt."
  },
  {
    "title": "Skill-based Safe Reinforcement Learning with Risk Planning",
    "url": "http://arxiv.org/abs/2505.01619v1",
    "arxiv_id": "2505.01619v1",
    "authors": [
      "Hanping Zhang",
      "Yuhong Guo"
    ],
    "published": "2025-05-02T22:48:27+00:00",
    "summary": "Safe Reinforcement Learning (Safe RL) aims to ensure safety when an RL agent conducts learning by interacting with real-world environments where improper actions can induce high costs or lead to severe consequences. In this paper, we propose a novel Safe Skill Planning (SSkP) approach to enhance effective safe RL by exploiting auxiliary offline demonstration data. SSkP involves a two-stage process. First, we employ PU learning to learn a skill risk predictor from the offline demonstration data. Then, based on the learned skill risk predictor, we develop a novel risk planning process to enhance online safe RL and learn a risk-averse safe policy efficiently through interactions with the online RL environment, while simultaneously adapting the skill risk predictor to the environment. We conduct experiments in several benchmark robotic simulation environments. The experimental results demonstrate that the proposed approach consistently outperforms previous state-of-the-art safe RL methods."
  },
  {
    "title": "Grounding Task Assistance with Multimodal Cues from a Single Demonstration",
    "url": "http://arxiv.org/abs/2505.01578v1",
    "arxiv_id": "2505.01578v1",
    "authors": [
      "Gabriel Sarch",
      "Balasaravanan Thoravi Kumaravel",
      "Sahithya Ravi",
      "Vibhav Vineet",
      "Andrew D. Wilson"
    ],
    "published": "2025-05-02T20:43:11+00:00",
    "summary": "A person's demonstration often serves as a key reference for others learning the same task. However, RGB video, the dominant medium for representing these demonstrations, often fails to capture fine-grained contextual cues such as intent, safety-critical environmental factors, and subtle preferences embedded in human behavior. This sensory gap fundamentally limits the ability of Vision Language Models (VLMs) to reason about why actions occur and how they should adapt to individual users. To address this, we introduce MICA (Multimodal Interactive Contextualized Assistance), a framework that improves conversational agents for task assistance by integrating eye gaze and speech cues. MICA segments demonstrations into meaningful sub-tasks and extracts keyframes and captions that capture fine-grained intent and user-specific cues, enabling richer contextual grounding for visual question answering. Evaluations on questions derived from real-time chat-assisted task replication show that multimodal cues significantly improve response quality over frame-based retrieval. Notably, gaze cues alone achieves 93% of speech performance, and their combination yields the highest accuracy. Task type determines the effectiveness of implicit (gaze) vs. explicit (speech) cues, underscoring the need for adaptable multimodal models. These results highlight the limitations of frame-based context and demonstrate the value of multimodal signals for real-world AI task assistance."
  },
  {
    "title": "AI agents may be worth the hype but not the resources (yet): An initial exploration of machine translation quality and costs in three language pairs in the legal and news domains",
    "url": "http://arxiv.org/abs/2505.01560v1",
    "arxiv_id": "2505.01560v1",
    "authors": [
      "Vicent Briva Iglesias",
      "Gokhan Dogru"
    ],
    "published": "2025-05-02T20:02:13+00:00",
    "summary": "Large language models (LLMs) and multi-agent orchestration are touted as the next leap in machine translation (MT), but their benefits relative to conventional neural MT (NMT) remain unclear. This paper offers an empirical reality check. We benchmark five paradigms, Google Translate (strong NMT baseline), GPT-4o (general-purpose LLM), o1-preview (reasoning-enhanced LLM), and two GPT-4o-powered agentic workflows (sequential three-stage and iterative refinement), on test data drawn from a legal contract and news prose in three English-source pairs: Spanish, Catalan and Turkish. Automatic evaluation is performed with COMET, BLEU, chrF2 and TER; human evaluation is conducted with expert ratings of adequacy and fluency; efficiency with total input-plus-output token counts mapped to April 2025 pricing.   Automatic scores still favour the mature NMT system, which ranks first in seven of twelve metric-language combinations; o1-preview ties or places second in most remaining cases, while both multi-agent workflows trail. Human evaluation reverses part of this narrative: o1-preview produces the most adequate and fluent output in five of six comparisons, and the iterative agent edges ahead once, indicating that reasoning layers capture semantic nuance undervalued by surface metrics. Yet these qualitative gains carry steep costs. The sequential agent consumes roughly five times, and the iterative agent fifteen times, the tokens used by NMT or single-pass LLMs.   We advocate multidimensional, cost-aware evaluation protocols and highlight research directions that could tip the balance: leaner coordination strategies, selective agent activation, and hybrid pipelines combining single-pass LLMs with targeted agent intervention."
  },
  {
    "title": "Emotions in the Loop: A Survey of Affective Computing for Emotional Support",
    "url": "http://arxiv.org/abs/2505.01542v1",
    "arxiv_id": "2505.01542v1",
    "authors": [
      "Karishma Hegde",
      "Hemadri Jayalath"
    ],
    "published": "2025-05-02T19:06:05+00:00",
    "summary": "In a world where technology is increasingly embedded in our everyday experiences, systems that sense and respond to human emotions are elevating digital interaction. At the intersection of artificial intelligence and human-computer interaction, affective computing is emerging with innovative solutions where machines are humanized by enabling them to process and respond to user emotions. This survey paper explores recent research contributions in affective computing applications in the area of emotion recognition, sentiment analysis and personality assignment developed using approaches like large language models (LLMs), multimodal techniques, and personalized AI systems. We analyze the key contributions and innovative methodologies applied by the selected research papers by categorizing them into four domains: AI chatbot applications, multimodal input systems, mental health and therapy applications, and affective computing for safety applications. We then highlight the technological strengths as well as the research gaps and challenges related to these studies. Furthermore, the paper examines the datasets used in each study, highlighting how modality, scale, and diversity impact the development and performance of affective models. Finally, the survey outlines ethical considerations and proposes future directions to develop applications that are more safe, empathetic and practical."
  },
  {
    "title": "Rubber Mallet: A Study of High Frequency Localized Bit Flips and Their Impact on Security",
    "url": "http://arxiv.org/abs/2505.01518v1",
    "arxiv_id": "2505.01518v1",
    "authors": [
      "Andrew Adiletta",
      "Zane Weissman",
      "Fatemeh Khojasteh Dana",
      "Berk Sunar",
      "Shahin Tajik"
    ],
    "published": "2025-05-02T18:07:07+00:00",
    "summary": "The increasing density of modern DRAM has heightened its vulnerability to Rowhammer attacks, which induce bit flips by repeatedly accessing specific memory rows. This paper presents an analysis of bit flip patterns generated by advanced Rowhammer techniques that bypass existing hardware defenses. First, we investigate the phenomenon of adjacent bit flips--where two or more physically neighboring bits are corrupted simultaneously--and demonstrate they occur with significantly higher frequency than previously documented. We also show that if multiple bits flip within a byte, they are more likely to be adjacent than randomly distributed: for example, if 4 bits flip within a byte, there is an 87% chance that they are all adjacent. We also demonstrate that bit flips within a row will naturally cluster together likely due to the underlying physics of the attack. We then investigate two fault injection attacks enabled by multiple adjacent or nearby bit flips. First, we show how these correlated flips enable efficient cryptographic signature correction attacks, successfully recovering ECDSA private keys from OpenSSL implementations where single-bit approaches would be unfeasible. Second, we introduce a targeted attack against large language models by exploiting Rowhammer-induced corruptions in tokenizer dictionaries of GGUF model files. This attack effectively rewrites safety instructions in system prompts by swapping safety-critical tokens with benign alternatives, circumventing model guardrails while maintaining normal functionality in other contexts. Our experimental results across multiple DRAM configurations reveal that current memory protection schemes are inadequate against these sophisticated attack vectors, which can achieve their objectives with precise, minimal modifications rather than random corruption."
  },
  {
    "title": "Comparison of Waymo Rider-Only Crash Rates by Crash Type to Human Benchmarks at 56.7 Million Miles",
    "url": "http://arxiv.org/abs/2505.01515v1",
    "arxiv_id": "2505.01515v1",
    "authors": [
      "Kristofer D. Kusano",
      "John M. Scanlon",
      "Yin-Hsiu Chen",
      "Timothy L. McMurry",
      "Tilia Gode",
      "Trent Victor"
    ],
    "published": "2025-05-02T18:04:20+00:00",
    "summary": "SAE Level 4 Automated Driving Systems (ADSs) are deployed on public roads, including Waymo's Rider-Only (RO) ride-hailing service (without a driver behind the steering wheel). The objective of this study was to perform a retrospective safety assessment of Waymo's RO crash rate compared to human benchmarks, including disaggregated by crash type.   Eleven crash type groups were identified from commonly relied upon crash typologies that are derived from human crash databases. Human benchmarks were aligned to the same vehicle types, road types, and locations as where the Waymo Driver operated. Waymo crashes were extracted from the NHTSA Standing General Order (SGO). RO mileage was provided by the company via a public website. Any-injury-reported, Airbag Deployment, and Suspected Serious Injury+ crash outcomes were examined because they represented previously established, safety-relevant benchmarks where statistical testing could be performed at the current mileage.   Data was examined over 56.7 million RO miles through the end of January 2025, resulting in a statistically significant lower crashed vehicle rate for all crashes compared to the benchmarks in Any-Injury-Reported and Airbag Deployment, and Suspected Serious Injury+ crashes. Of the crash types, V2V Intersection crash events represented the largest total crash reduction, with a 96% reduction in Any-injury-reported (87%-99% CI) and a 91% reduction in Airbag Deployment (76%-98% CI) events. Cyclist, Motorcycle, Pedestrian, Secondary Crash, and Single Vehicle crashes were also statistically reduced for the Any-Injury-Reported outcome. There was no statistically significant disbenefit found in any of the 11 crash type groups.   This study represents the first retrospective safety assessment of an RO ADS that made statistical conclusions about more serious crash outcomes and analyzed crash rates on a crash type basis."
  },
  {
    "title": "Neutrino mass generation in asymptotically safe gravity",
    "url": "http://arxiv.org/abs/2505.01422v1",
    "arxiv_id": "2505.01422v1",
    "authors": [
      "Gustavo P. de Brito",
      "Astrid Eichhorn",
      "Antonio D. Pereira",
      "Masatoshi Yamada"
    ],
    "published": "2025-05-02T17:58:14+00:00",
    "summary": "There exist several distinct phenomenological models to generate neutrino masses. We explore, which of these models can consistently be embedded in a quantum theory of gravity and matter. We proceed by invoking a minimal number of degrees of freedom beyond the Standard Model. Thus, we first investigate whether the Weinberg operator, a dimension-five-operator that generates neutrino masses without requiring degrees of freedom beyond the Standard Model, can arise in asymptotically safe quantum gravity. We find a negative answer with far-reaching consequences: new degrees of freedom beyond gravity and the Standard Model are necessary to give neutrinos a mass in the asymptotic-safety paradigm. Second, we explore whether the type-I Seesaw mechanism is viable and discover an upper bound on the Seesaw scale. The bound depends on the mass of the visible neutrino. We find a numerical value of $10^{14}\\, \\rm GeV$ for this bound when neglecting neutrino mixing for a visible mass of $10^{-10}\\, \\rm GeV$. Conversely, for the most ``natural\" value of the Seesaw scale in a quantum-gravity setting, which is the Planck scale, we predict an upper bound for the neutrino mass of the visible neutrino of approximately $10^{-15}\\, \\rm GeV$. Third, we explore whether neutrinos could also be Pseudo-Dirac-neutrinos in asymptotic safety and find that this possibility can be accommodated."
  },
  {
    "title": "Evaluating Frontier Models for Stealth and Situational Awareness",
    "url": "http://arxiv.org/abs/2505.01420v1",
    "arxiv_id": "2505.01420v1",
    "authors": [
      "Mary Phuong",
      "Roland S. Zimmermann",
      "Ziyue Wang",
      "David Lindner",
      "Victoria Krakovna",
      "Sarah Cogan",
      "Allan Dafoe",
      "Lewis Ho",
      "Rohin Shah"
    ],
    "published": "2025-05-02T17:57:14+00:00",
    "summary": "Recent work has demonstrated the plausibility of frontier AI models scheming -- knowingly and covertly pursuing an objective misaligned with its developer's intentions. Such behavior could be very hard to detect, and if present in future advanced systems, could pose severe loss of control risk. It is therefore important for AI developers to rule out harm from scheming prior to model deployment. In this paper, we present a suite of scheming reasoning evaluations measuring two types of reasoning capabilities that we believe are prerequisites for successful scheming: First, we propose five evaluations of ability to reason about and circumvent oversight (stealth). Second, we present eleven evaluations for measuring a model's ability to instrumentally reason about itself, its environment and its deployment (situational awareness). We demonstrate how these evaluations can be used as part of a scheming inability safety case: a model that does not succeed on these evaluations is almost certainly incapable of causing severe harm via scheming in real deployment. We run our evaluations on current frontier models and find that none of them show concerning levels of either situational awareness or stealth."
  },
  {
    "title": "Explainable Machine Learning for Cyberattack Identification from Traffic Flows",
    "url": "http://arxiv.org/abs/2505.01488v1",
    "arxiv_id": "2505.01488v1",
    "authors": [
      "Yujing Zhou",
      "Marc L. Jacquet",
      "Robel Dawit",
      "Skyler Fabre",
      "Dev Sarawat",
      "Faheem Khan",
      "Madison Newell",
      "Yongxin Liu",
      "Dahai Liu",
      "Hongyun Chen",
      "Jian Wang",
      "Huihui Wang"
    ],
    "published": "2025-05-02T17:34:14+00:00",
    "summary": "The increasing automation of traffic management systems has made them prime targets for cyberattacks, disrupting urban mobility and public safety. Traditional network-layer defenses are often inaccessible to transportation agencies, necessitating a machine learning-based approach that relies solely on traffic flow data. In this study, we simulate cyberattacks in a semi-realistic environment, using a virtualized traffic network to analyze disruption patterns. We develop a deep learning-based anomaly detection system, demonstrating that Longest Stop Duration and Total Jam Distance are key indicators of compromised signals. To enhance interpretability, we apply Explainable AI (XAI) techniques, identifying critical decision factors and diagnosing misclassification errors. Our analysis reveals two primary challenges: transitional data inconsistencies, where mislabeled recovery-phase traffic misleads the model, and model limitations, where stealth attacks in low-traffic conditions evade detection. This work enhances AI-driven traffic security, improving both detection accuracy and trustworthiness in smart transportation systems."
  },
  {
    "title": "CHORUS: Zero-shot Hierarchical Retrieval and Orchestration for Generating Linear Programming Code",
    "url": "http://arxiv.org/abs/2505.01485v1",
    "arxiv_id": "2505.01485v1",
    "authors": [
      "Tasnim Ahmed",
      "Salimur Choudhury"
    ],
    "published": "2025-05-02T16:36:57+00:00",
    "summary": "Linear Programming (LP) problems aim to find the optimal solution to an objective under constraints. These problems typically require domain knowledge, mathematical skills, and programming ability, presenting significant challenges for non-experts. This study explores the efficiency of Large Language Models (LLMs) in generating solver-specific LP code. We propose CHORUS, a retrieval-augmented generation (RAG) framework for synthesizing Gurobi-based LP code from natural language problem statements. CHORUS incorporates a hierarchical tree-like chunking strategy for theoretical contents and generates additional metadata based on code examples from documentation to facilitate self-contained, semantically coherent retrieval. Two-stage retrieval approach of CHORUS followed by cross-encoder reranking further ensures contextual relevance. Finally, expertly crafted prompt and structured parser with reasoning steps improve code generation performance significantly. Experiments on the NL4Opt-Code benchmark show that CHORUS improves the performance of open-source LLMs such as Llama3.1 (8B), Llama3.3 (70B), Phi4 (14B), Deepseek-r1 (32B), and Qwen2.5-coder (32B) by a significant margin compared to baseline and conventional RAG. It also allows these open-source LLMs to outperform or match the performance of much stronger baselines-GPT3.5 and GPT4 while requiring far fewer computational resources. Ablation studies further demonstrate the importance of expert prompting, hierarchical chunking, and structured reasoning."
  },
  {
    "title": "Helping Big Language Models Protect Themselves: An Enhanced Filtering and Summarization System",
    "url": "http://arxiv.org/abs/2505.01315v1",
    "arxiv_id": "2505.01315v1",
    "authors": [
      "Sheikh Samit Muhaimin",
      "Spyridon Mastorakis"
    ],
    "published": "2025-05-02T14:42:26+00:00",
    "summary": "The recent growth in the use of Large Language Models has made them vulnerable to sophisticated adversarial assaults, manipulative prompts, and encoded malicious inputs. Existing countermeasures frequently necessitate retraining models, which is computationally costly and impracticable for deployment. Without the need for retraining or fine-tuning, this study presents a unique defense paradigm that allows LLMs to recognize, filter, and defend against adversarial or malicious inputs on their own. There are two main parts to the suggested framework: (1) A prompt filtering module that uses sophisticated Natural Language Processing (NLP) techniques, including zero-shot classification, keyword analysis, and encoded content detection (e.g. base64, hexadecimal, URL encoding), to detect, decode, and classify harmful inputs; and (2) A summarization module that processes and summarizes adversarial research literature to give the LLM context-aware defense knowledge. This approach strengthens LLMs' resistance to adversarial exploitation by fusing text extraction, summarization, and harmful prompt analysis. According to experimental results, this integrated technique has a 98.71% success rate in identifying harmful patterns, manipulative language structures, and encoded prompts. By employing a modest amount of adversarial research literature as context, the methodology also allows the model to react correctly to harmful inputs with a larger percentage of jailbreak resistance and refusal rate. While maintaining the quality of LLM responses, the framework dramatically increases LLM's resistance to hostile misuse, demonstrating its efficacy as a quick and easy substitute for time-consuming, retraining-based defenses."
  },
  {
    "title": "Helping Large Language Models Protect Themselves: An Enhanced Filtering and Summarization System",
    "url": "http://arxiv.org/abs/2505.01315v2",
    "arxiv_id": "2505.01315v2",
    "authors": [
      "Sheikh Samit Muhaimin",
      "Spyridon Mastorakis"
    ],
    "published": "2025-05-02T14:42:26+00:00",
    "summary": "The recent growth in the use of Large Language Models has made them vulnerable to sophisticated adversarial assaults, manipulative prompts, and encoded malicious inputs. Existing countermeasures frequently necessitate retraining models, which is computationally costly and impracticable for deployment. Without the need for retraining or fine-tuning, this study presents a unique defense paradigm that allows LLMs to recognize, filter, and defend against adversarial or malicious inputs on their own. There are two main parts to the suggested framework: (1) A prompt filtering module that uses sophisticated Natural Language Processing (NLP) techniques, including zero-shot classification, keyword analysis, and encoded content detection (e.g. base64, hexadecimal, URL encoding), to detect, decode, and classify harmful inputs; and (2) A summarization module that processes and summarizes adversarial research literature to give the LLM context-aware defense knowledge. This approach strengthens LLMs' resistance to adversarial exploitation by fusing text extraction, summarization, and harmful prompt analysis. According to experimental results, this integrated technique has a 98.71% success rate in identifying harmful patterns, manipulative language structures, and encoded prompts. By employing a modest amount of adversarial research literature as context, the methodology also allows the model to react correctly to harmful inputs with a larger percentage of jailbreak resistance and refusal rate. While maintaining the quality of LLM responses, the framework dramatically increases LLM's resistance to hostile misuse, demonstrating its efficacy as a quick and easy substitute for time-consuming, retraining-based defenses."
  },
  {
    "title": "Document Retrieval Augmented Fine-Tuning (DRAFT) for safety-critical software assessments",
    "url": "http://arxiv.org/abs/2505.01307v1",
    "arxiv_id": "2505.01307v1",
    "authors": [
      "Regan Bolton",
      "Mohammadreza Sheikhfathollahi",
      "Simon Parkinson",
      "Vanessa Vulovic",
      "Gary Bamford",
      "Dan Basher",
      "Howard Parkinson"
    ],
    "published": "2025-05-02T14:34:33+00:00",
    "summary": "Safety critical software assessment requires robust assessment against complex regulatory frameworks, a process traditionally limited by manual evaluation. This paper presents Document Retrieval-Augmented Fine-Tuning (DRAFT), a novel approach that enhances the capabilities of a large language model (LLM) for safety-critical compliance assessment. DRAFT builds upon existing Retrieval-Augmented Generation (RAG) techniques by introducing a novel fine-tuning framework that accommodates our dual-retrieval architecture, which simultaneously accesses both software documentation and applicable reference standards. To fine-tune DRAFT, we develop a semi-automated dataset generation methodology that incorporates variable numbers of relevant documents with meaningful distractors, closely mirroring real-world assessment scenarios. Experiments with GPT-4o-mini demonstrate a 7% improvement in correctness over the baseline model, with qualitative improvements in evidence handling, response structure, and domain-specific reasoning. DRAFT represents a practical approach to improving compliance assessment systems while maintaining the transparency and evidence-based reasoning essential in regulatory domains."
  },
  {
    "title": "Contactless pulse rate assessment: Results and insights for application in driving simulator",
    "url": "http://arxiv.org/abs/2505.01299v1",
    "arxiv_id": "2505.01299v1",
    "authors": [
      "\u0110or\u0111e D. Ne\u0161kovi\u0107",
      "Kristina Stojmenova Pe\u010de\u010dnik",
      "Jaka Sodnik",
      "Nadica Miljkovi\u0107"
    ],
    "published": "2025-05-02T14:22:12+00:00",
    "summary": "Camera-based monitoring of Pulse Rate (PR) enables continuous and unobtrusive assessment of driver's state, allowing estimation of fatigue or stress that could impact traffic safety. Commonly used wearable Photoplethysmography (PPG) sensors, while effective, suffer from motion artifacts and user discomfort. This study explores the feasibility of non-contact PR assessment using facial video recordings captured by a Red, Green, and Blue (RGB) camera in a driving simulation environment. The proposed approach detects subtle skin color variations due to blood flow and compares extracted PR values against reference measurements from a wearable wristband Empatica E4. We evaluate the impact of Eulerian Video Magnification (EVM) on signal quality and assess statistical differences in PR between age groups. Data obtained from 80 recordings from 64 healthy subjects covering a PR range of 45-160 bpm are analyzed, and signal extraction accuracy is quantified using metrics, such as Mean Absolute Error (MAE) and Root Mean Square Error (RMSE). Results show that EVM slightly improves PR estimation accuracy, reducing MAE from 6.48 bpm to 5.04 bpm and RMSE from 7.84 bpm to 6.38 bpm. A statistically significant difference is found between older and younger groups with both video-based and ground truth evaluation procedures. Additionally, we discuss Empatica E4 bias and its potential impact on the overall assessment of contact measurements. Altogether the findings demonstrate the feasibility of camera-based PR monitoring in dynamic environments and its potential integration into driving simulators for real-time physiological assessment."
  },
  {
    "title": "2DXformer: Dual Transformers for Wind Power Forecasting with Dual Exogenous Variables",
    "url": "http://arxiv.org/abs/2505.01286v1",
    "arxiv_id": "2505.01286v1",
    "authors": [
      "Yajuan Zhang",
      "Jiahai Jiang",
      "Yule Yan",
      "Liang Yang",
      "Ping Zhang"
    ],
    "published": "2025-05-02T14:00:48+00:00",
    "summary": "Accurate wind power forecasting can help formulate scientific dispatch plans, which is of great significance for maintaining the safety, stability, and efficient operation of the power system. In recent years, wind power forecasting methods based on deep learning have focused on extracting the spatiotemporal correlations among data, achieving significant improvements in forecasting accuracy. However, they exhibit two limitations. First, there is a lack of modeling for the inter-variable relationships, which limits the accuracy of the forecasts. Second, by treating endogenous and exogenous variables equally, it leads to unnecessary interactions between the endogenous and exogenous variables, increasing the complexity of the model. In this paper, we propose the 2DXformer, which, building upon the previous work's focus on spatiotemporal correlations, addresses the aforementioned two limitations. Specifically, we classify the inputs of the model into three types: exogenous static variables, exogenous dynamic variables, and endogenous variables. First, we embed these variables as variable tokens in a channel-independent manner. Then, we use the attention mechanism to capture the correlations among exogenous variables. Finally, we employ a multi-layer perceptron with residual connections to model the impact of exogenous variables on endogenous variables. Experimental results on two real-world large-scale datasets indicate that our proposed 2DXformer can further improve the performance of wind power forecasting. The code is available in this repository: \\href{https://github.com/jseaj/2DXformer}{https://github.com/jseaj/2DXformer}."
  },
  {
    "title": "Self-moderation in the decentralized era: decoding blocking behavior on Bluesky",
    "url": "http://arxiv.org/abs/2505.01174v1",
    "arxiv_id": "2505.01174v1",
    "authors": [
      "Carlo Bono",
      "Nick Liu",
      "Giuseppe Russo",
      "Francesco Pierri"
    ],
    "published": "2025-05-02T10:32:39+00:00",
    "summary": "Moderation and blocking behavior, both closely related to the mitigation of abuse and misinformation on social platforms, are fundamental mechanisms for maintaining healthy online communities. However, while centralized platforms typically employ top-down moderation, decentralized networks rely on users to self-regulate through mechanisms like blocking actions to safeguard their online experience. Given the novelty of the decentralized paradigm, addressing self-moderation is critical for understanding how community safety and user autonomy can be effectively balanced. This study examines user blocking on Bluesky, a decentralized social networking platform, providing a comprehensive analysis of over three months of user activity through the lens of blocking behaviour. We define profiles based on 86 features that describe user activity, content characteristics, and network interactions, addressing two primary questions: (1) Is the likelihood of a user being blocked inferable from their online behavior? and (2) What behavioral features are associated with an increased likelihood of being blocked? Our findings offer valuable insights and contribute with a robust analytical framework to advance research in moderation on decentralized social networks."
  },
  {
    "title": "VTS-LLM: Domain-Adaptive LLM Agent for Enhancing Awareness in Vessel Traffic Services through Natural Language",
    "url": "http://arxiv.org/abs/2505.00989v1",
    "arxiv_id": "2505.00989v1",
    "authors": [
      "Sijin Sun",
      "Liangbin Zhao",
      "Ming Deng",
      "Xiuju Fu"
    ],
    "published": "2025-05-02T04:27:50+00:00",
    "summary": "Vessel Traffic Services (VTS) are essential for maritime safety and regulatory compliance through real-time traffic management. However, with increasing traffic complexity and the prevalence of heterogeneous, multimodal data, existing VTS systems face limitations in spatiotemporal reasoning and intuitive human interaction. In this work, we propose VTS-LLM Agent, the first domain-adaptive large LLM agent tailored for interactive decision support in VTS operations. We formalize risk-prone vessel identification as a knowledge-augmented Text-to-SQL task, combining structured vessel databases with external maritime knowledge. To support this, we construct a curated benchmark dataset consisting of a custom schema, domain-specific corpus, and a query-SQL test set in multiple linguistic styles. Our framework incorporates NER-based relational reasoning, agent-based domain knowledge injection, semantic algebra intermediate representation, and query rethink mechanisms to enhance domain grounding and context-aware understanding. Experimental results show that VTS-LLM outperforms both general-purpose and SQL-focused baselines under command-style, operational-style, and formal natural language queries, respectively. Moreover, our analysis provides the first empirical evidence that linguistic style variation introduces systematic performance challenges in Text-to-SQL modeling. This work lays the foundation for natural language interfaces in vessel traffic services and opens new opportunities for proactive, LLM-driven maritime real-time traffic management."
  },
  {
    "title": "Seeking to Collide: Online Safety-Critical Scenario Generation for Autonomous Driving with Retrieval Augmented Large Language Models",
    "url": "http://arxiv.org/abs/2505.00972v1",
    "arxiv_id": "2505.00972v1",
    "authors": [
      "Yuewen Mei",
      "Tong Nie",
      "Jian Sun",
      "Ye Tian"
    ],
    "published": "2025-05-02T03:22:00+00:00",
    "summary": "Simulation-based testing is crucial for validating autonomous vehicles (AVs), yet existing scenario generation methods either overfit to common driving patterns or operate in an offline, non-interactive manner that fails to expose rare, safety-critical corner cases. In this paper, we introduce an online, retrieval-augmented large language model (LLM) framework for generating safety-critical driving scenarios. Our method first employs an LLM-based behavior analyzer to infer the most dangerous intent of the background vehicle from the observed state, then queries additional LLM agents to synthesize feasible adversarial trajectories. To mitigate catastrophic forgetting and accelerate adaptation, we augment the framework with a dynamic memorization and retrieval bank of intent-planner pairs, automatically expanding its behavioral library when novel intents arise. Evaluations using the Waymo Open Motion Dataset demonstrate that our model reduces the mean minimum time-to-collision from 1.62 to 1.08 s and incurs a 75% collision rate, substantially outperforming baselines."
  },
  {
    "title": "A SCADE Model Verification Method Based on B-Model Transformation",
    "url": "http://arxiv.org/abs/2505.00967v1",
    "arxiv_id": "2505.00967v1",
    "authors": [
      "Xili Hou",
      "Keming Wang",
      "Huibing Zhao",
      "Ruiyin Shi"
    ],
    "published": "2025-05-02T03:05:09+00:00",
    "summary": "Due to the limitations of SCADE models in expressing and verifying abstract specifications in safety-critical systems, this study proposes a formal verification framework based on the B-Method. By establishing a semantic equivalence transformation mechanism from SCADE models to B models, a hierarchical mapping rule set is constructed, covering type systems, control flow structures, and state machines. This effectively addresses key technical challenges such as loop-equivalent transformation proof for high-order operators and modeling of temporal logic storage structures. The proposed method innovatively leverages the abstraction capabilities of B-Method in set theory and first-order logic, overcoming the constraints of native verification tools of SCADE in complex specification descriptions. It successfully verifies abstract specifications that are difficult to model directly in SCADE. Experimental results show that the transformed B models achieve a higher defect detection rate and improved verification efficiency in the ProB verification environment compared to the native verifier of SCADE, significantly enhancing the formal verification capability of safety-critical systems. This study provides a cross-model verification paradigm for embedded control systems in avionics, rail transportation, and other domains, demonstrating substantial engineering application value."
  },
  {
    "title": "Llama-Nemotron: Efficient Reasoning Models",
    "url": "http://arxiv.org/abs/2505.00949v1",
    "arxiv_id": "2505.00949v1",
    "authors": [
      "Akhiad Bercovich",
      "Itay Levy",
      "Izik Golan",
      "Mohammad Dabbah",
      "Ran El-Yaniv",
      "Omri Puny",
      "Ido Galil",
      "Zach Moshe",
      "Tomer Ronen",
      "Najeeb Nabwani",
      "Ido Shahaf",
      "Oren Tropp",
      "Ehud Karpas",
      "Ran Zilberstein",
      "Jiaqi Zeng",
      "Soumye Singhal",
      "Alexander Bukharin",
      "Yian Zhang",
      "Tugrul Konuk",
      "Gerald Shen",
      "Ameya Sunil Mahabaleshwarkar",
      "Bilal Kartal",
      "Yoshi Suhara",
      "Olivier Delalleau",
      "Zijia Chen",
      "Zhilin Wang",
      "David Mosallanezhad",
      "Adi Renduchintala",
      "Haifeng Qian",
      "Dima Rekesh",
      "Fei Jia",
      "Somshubra Majumdar",
      "Vahid Noroozi",
      "Wasi Uddin Ahmad",
      "Sean Narenthiran",
      "Aleksander Ficek",
      "Mehrzad Samadi",
      "Jocelyn Huang",
      "Siddhartha Jain",
      "Igor Gitman",
      "Ivan Moshkov",
      "Wei Du",
      "Shubham Toshniwal",
      "George Armstrong",
      "Branislav Kisacanin",
      "Matvei Novikov",
      "Daria Gitman",
      "Evelina Bakhturina",
      "Jane Polak Scowcroft",
      "John Kamalu",
      "Dan Su",
      "Kezhi Kong",
      "Markus Kliegl",
      "Rabeeh Karimi",
      "Ying Lin",
      "Sanjeev Satheesh",
      "Jupinder Parmar",
      "Pritam Gundecha",
      "Brandon Norick",
      "Joseph Jennings",
      "Shrimai Prabhumoye",
      "Syeda Nahida Akter",
      "Mostofa Patwary",
      "Abhinav Khattar",
      "Deepak Narayanan",
      "Roger Waleffe",
      "Jimmy Zhang",
      "Bor-Yiing Su",
      "Guyue Huang",
      "Terry Kong",
      "Parth Chadha",
      "Sahil Jain",
      "Christine Harvey",
      "Elad Segal",
      "Jining Huang",
      "Sergey Kashirsky",
      "Robert McQueen",
      "Izzy Putterman",
      "George Lam",
      "Arun Venkatesan",
      "Sherry Wu",
      "Vinh Nguyen",
      "Manoj Kilaru",
      "Andrew Wang",
      "Anna Warno",
      "Abhilash Somasamudramath",
      "Sandip Bhaskar",
      "Maka Dong",
      "Nave Assaf",
      "Shahar Mor",
      "Omer Ullman Argov",
      "Scot Junkin",
      "Oleksandr Romanenko",
      "Pedro Larroy",
      "Monika Katariya",
      "Marco Rovinelli",
      "Viji Balas",
      "Nicholas Edelman",
      "Anahita Bhiwandiwalla",
      "Muthu Subramaniam",
      "Smita Ithape",
      "Karthik Ramamoorthy",
      "Yuting Wu",
      "Suguna Varshini Velury",
      "Omri Almog",
      "Joyjit Daw",
      "Denys Fridman",
      "Erick Galinkin",
      "Michael Evans",
      "Katherine Luna",
      "Leon Derczynski",
      "Nikki Pope",
      "Eileen Long",
      "Seth Schneider",
      "Guillermo Siman",
      "Tomasz Grzegorzek",
      "Pablo Ribalta",
      "Monika Katariya",
      "Joey Conway",
      "Trisha Saar",
      "Ann Guan",
      "Krzysztof Pawelec",
      "Shyamala Prayaga",
      "Oleksii Kuchaiev",
      "Boris Ginsburg",
      "Oluwatobi Olabiyi",
      "Kari Briski",
      "Jonathan Cohen",
      "Bryan Catanzaro",
      "Jonah Alben",
      "Yonatan Geifman",
      "Eric Chung"
    ],
    "published": "2025-05-02T01:35:35+00:00",
    "summary": "We introduce the Llama-Nemotron series of models, an open family of heterogeneous reasoning models that deliver exceptional reasoning capabilities, inference efficiency, and an open license for enterprise use. The family comes in three sizes -- Nano (8B), Super (49B), and Ultra (253B) -- and performs competitively with state-of-the-art reasoning models such as DeepSeek-R1 while offering superior inference throughput and memory efficiency. In this report, we discuss the training procedure for these models, which entails using neural architecture search from Llama 3 models for accelerated inference, knowledge distillation, and continued pretraining, followed by a reasoning-focused post-training stage consisting of two main parts: supervised fine-tuning and large scale reinforcement learning. Llama-Nemotron models are the first open-source models to support a dynamic reasoning toggle, allowing users to switch between standard chat and reasoning modes during inference. To further support open research and facilitate model development, we provide the following resources: 1. We release the Llama-Nemotron reasoning models -- LN-Nano, LN-Super, and LN-Ultra -- under the commercially permissive NVIDIA Open Model License Agreement. 2. We release the complete post-training dataset: Llama-Nemotron-Post-Training-Dataset. 3. We also release our training codebases: NeMo, NeMo-Aligner, and Megatron-LM."
  },
  {
    "title": "SSRLBot: Designing and Developing an LLM-based Agent using Socially Shared Regulated Learning",
    "url": "http://arxiv.org/abs/2505.00945v1",
    "arxiv_id": "2505.00945v1",
    "authors": [
      "Xiaoshan Huang",
      "Jie Gao",
      "Haolun Wu"
    ],
    "published": "2025-05-02T01:17:03+00:00",
    "summary": "Large language model (LLM)-based agents are increasingly used to support human experts by streamlining complex tasks and offering actionable insights. However, their application in multi-professional decision-making, particularly in teamwork contexts, remains underexplored. This design-based study addresses that gap by developing LLM functions to enhance collaboration, grounded in the Socially Shared Regulation of Learning (SSRL) framework and applied to medical diagnostic teamwork. SSRL emphasizes metacognitive, cognitive, motivational, and emotional processes in shared learning, focusing on how teams manage these processes to improve decision-making. This paper introduces SSRLBot, a prototype chatbot designed to help team members reflect on both their diagnostic performance and key SSRL skills. Its core functions include summarizing dialogues, analyzing SSRL behaviors, evaluating diagnostic outcomes, annotating SSRL markers in conversation, assessing their impact on performance, and identifying interpersonal regulatory dynamics. We compare SSRLBot's capabilities with those of Gemini-1.5, GPT-3.5, and Deepseek-R1 in a case study. SSRLBot demonstrates stronger alignment with SSRL theory, offering detailed evaluations that link behaviors to regulatory dimensions and suggesting improvements for collaboration. By integrating SSRL theory with LLM capabilities, SSRLBot contributes a novel tool for enhancing team-based decision-making and collaborative learning in high-stakes environments, such as medical education."
  },
  {
    "title": "Learning Neural Control Barrier Functions from Offline Data with Conservatism",
    "url": "http://arxiv.org/abs/2505.00908v1",
    "arxiv_id": "2505.00908v1",
    "authors": [
      "Ihab Tabbara",
      "Hussein Sibai"
    ],
    "published": "2025-05-01T23:01:03+00:00",
    "summary": "Safety filters, particularly those based on control barrier functions, have gained increased interest as effective tools for safe control of dynamical systems. Existing correct-by-construction synthesis algorithms, however, suffer from the curse of dimensionality. Deep learning approaches have been proposed in recent years to address this challenge. In this paper, we contribute to this line of work by proposing an algorithm for training control barrier functions from offline datasets. Our algorithm trains the filter to not only prevent the system from reaching unsafe states but also out-of-distribution ones, at which the filter would be unreliable. It is inspired by Conservative Q-learning, an offline reinforcement learning algorithm. We call its outputs Conservative Control Barrier Functions (CCBFs). Our empirical results demonstrate that CCBFs outperform existing methods in maintaining safety and out-of-distribution avoidance while minimally affecting task performance."
  },
  {
    "title": "Inattentional Blindness with Augmented Reality HUDS: An On-road Study",
    "url": "http://arxiv.org/abs/2505.00879v1",
    "arxiv_id": "2505.00879v1",
    "authors": [
      "Nayara de Oliveira Faria",
      "Joseph L. Gabbard"
    ],
    "published": "2025-05-01T21:41:38+00:00",
    "summary": "As the integration of augmented reality (AR) technology in head-up displays (HUDs) becomes more prevalent in vehicles, it is crucial to understand how to design and evaluate AR interfaces to ensure safety. With new AR displays capable of rendering images with larger field of views and at varying depths, the visual and cognitive separation between graphical and real-world visual stimuli will be increasingly more difficult to quantify as will drivers' ability to efficiently allocate visual attention between the two sets of stimuli. In this study, we present a user study that serves as a crucial first step in gaining insight into inattentional blindness while using AR in surface transportation, where understanding is currently limited. Our primary goal is to investigate how the visual demand of AR tasks influences drivers' ability to detect stimuli, and whether the nature of the stimuli itself plays a role in this effect. To address these questions, we designed an on-road user study aimed at producing a more realistic and ecologically valid understanding of the phenomenon.   Our results show that drivers' ability to timely detect stimuli in the environment decreased as the AR task visual demand increased demonstrated by both detection performance and inattentional blindness metrics. Further, inattentional blindness caused by AR displays appears to be more prevalent within drivers' central field of view. We conclude by discussing implications towards a safety-centric evaluation framework for AR HUDs."
  },
  {
    "title": "From Texts to Shields: Convergence of Large Language Models and Cybersecurity",
    "url": "http://arxiv.org/abs/2505.00841v1",
    "arxiv_id": "2505.00841v1",
    "authors": [
      "Tao Li",
      "Ya-Ting Yang",
      "Yunian Pan",
      "Quanyan Zhu"
    ],
    "published": "2025-05-01T20:01:07+00:00",
    "summary": "This report explores the convergence of large language models (LLMs) and cybersecurity, synthesizing interdisciplinary insights from network security, artificial intelligence, formal methods, and human-centered design. It examines emerging applications of LLMs in software and network security, 5G vulnerability analysis, and generative security engineering. The report highlights the role of agentic LLMs in automating complex tasks, improving operational efficiency, and enabling reasoning-driven security analytics. Socio-technical challenges associated with the deployment of LLMs -- including trust, transparency, and ethical considerations -- can be addressed through strategies such as human-in-the-loop systems, role-specific training, and proactive robustness testing. The report further outlines critical research challenges in ensuring interpretability, safety, and fairness in LLM-based systems, particularly in high-stakes domains. By integrating technical advances with organizational and societal considerations, this report presents a forward-looking research agenda for the secure and effective adoption of LLMs in cybersecurity."
  },
  {
    "title": "IberFire -- a detailed creation of a spatio-temporal dataset for wildfire risk assessment in Spain",
    "url": "http://arxiv.org/abs/2505.00837v1",
    "arxiv_id": "2505.00837v1",
    "authors": [
      "Julen Ercibengoa",
      "Meritxell G\u00f3mez-Omella",
      "Izaro Goienetxea"
    ],
    "published": "2025-05-01T19:54:17+00:00",
    "summary": "Wildfires pose a critical environmental issue to ecosystems, economies, and public safety, particularly in Mediterranean regions such as Spain. Accurate predictive models rely on high-resolution spatio-temporal data to capture the complex interplay of environmental and anthropogenic factors. To address the lack of localised and fine-grained datasets in Spain, this work introduces IberFire, a spatio-temporal datacube at 1 km x 1 km x 1-day resolution covering mainland Spain and the Balearic Islands from December 2007 to December 2024. IberFire integrates 260 features across eight main categories: auxiliary features, fire history, geography, topography, meteorology, vegetation indices, human activity, and land cover. All features are derived from open-access sources, ensuring transparency and real-time applicability. The data processing pipeline was implemented entirely using open-source tools, and the codebase has been made publicly available. This work not only enhances spatio-temporal granularity and feature diversity compared to existing European datacubes but also provides a reproducible methodology for constructing similar datasets. IberFire supports advanced wildfire risk modelling through Machine Learning (ML) and Deep Learning (DL) techniques, enables climate pattern analysis and informs strategic planning in fire prevention and land management. The dataset is publicly available on Zenodo to promote open research and collaboration."
  },
  {
    "title": "HMCF: A Human-in-the-loop Multi-Robot Collaboration Framework Based on Large Language Models",
    "url": "http://arxiv.org/abs/2505.00820v1",
    "arxiv_id": "2505.00820v1",
    "authors": [
      "Zhaoxing Li",
      "Wenbo Wu",
      "Yue Wang",
      "Yanran Xu",
      "William Hunt",
      "Sebastian Stein"
    ],
    "published": "2025-05-01T19:23:50+00:00",
    "summary": "Rapid advancements in artificial intelligence (AI) have enabled robots to performcomplex tasks autonomously with increasing precision. However, multi-robot systems (MRSs) face challenges in generalization, heterogeneity, and safety, especially when scaling to large-scale deployments like disaster response. Traditional approaches often lack generalization, requiring extensive engineering for new tasks and scenarios, and struggle with managing diverse robots. To overcome these limitations, we propose a Human-in-the-loop Multi-Robot Collaboration Framework (HMCF) powered by large language models (LLMs). LLMs enhance adaptability by reasoning over diverse tasks and robot capabilities, while human oversight ensures safety and reliability, intervening only when necessary. Our framework seamlessly integrates human oversight, LLM agents, and heterogeneous robots to optimize task allocation and execution. Each robot is equipped with an LLM agent capable of understanding its capabilities, converting tasks into executable instructions, and reducing hallucinations through task verification and human supervision. Simulation results show that our framework outperforms state-of-the-art task planning methods, achieving higher task success rates with an improvement of 4.76%. Real-world tests demonstrate its robust zero-shot generalization feature and ability to handle diverse tasks and environments with minimal human intervention."
  },
  {
    "title": "Uncertainty-aware Latent Safety Filters for Avoiding Out-of-Distribution Failures",
    "url": "http://arxiv.org/abs/2505.00779v1",
    "arxiv_id": "2505.00779v1",
    "authors": [
      "Junwon Seo",
      "Kensuke Nakamura",
      "Andrea Bajcsy"
    ],
    "published": "2025-05-01T18:18:17+00:00",
    "summary": "Recent advances in generative world models have enabled classical safe control methods, such as Hamilton-Jacobi (HJ) reachability, to generalize to complex robotic systems operating directly from high-dimensional sensor observations. However, obtaining comprehensive coverage of all safety-critical scenarios during world model training is extremely challenging. As a result, latent safety filters built on top of these models may miss novel hazards and even fail to prevent known ones, overconfidently misclassifying risky out-of-distribution (OOD) situations as safe. To address this, we introduce an uncertainty-aware latent safety filter that proactively steers robots away from both known and unseen failures. Our key idea is to use the world model's epistemic uncertainty as a proxy for identifying unseen potential hazards. We propose a principled method to detect OOD world model predictions by calibrating an uncertainty threshold via conformal prediction. By performing reachability analysis in an augmented state space-spanning both the latent representation and the epistemic uncertainty-we synthesize a latent safety filter that can reliably safeguard arbitrary policies from both known and unseen safety hazards. In simulation and hardware experiments on vision-based control tasks with a Franka manipulator, we show that our uncertainty-aware safety filter preemptively detects potential unsafe scenarios and reliably proposes safe, in-distribution actions. Video results can be found on the project website at https://cmu-intentlab.github.io/UNISafe"
  },
  {
    "title": "Towards Autonomous Micromobility through Scalable Urban Simulation",
    "url": "http://arxiv.org/abs/2505.00690v1",
    "arxiv_id": "2505.00690v1",
    "authors": [
      "Wayne Wu",
      "Honglin He",
      "Chaoyuan Zhang",
      "Jack He",
      "Seth Z. Zhao",
      "Ran Gong",
      "Quanyi Li",
      "Bolei Zhou"
    ],
    "published": "2025-05-01T17:52:29+00:00",
    "summary": "Micromobility, which utilizes lightweight mobile machines moving in urban public spaces, such as delivery robots and mobility scooters, emerges as a promising alternative to vehicular mobility. Current micromobility depends mostly on human manual operation (in-person or remote control), which raises safety and efficiency concerns when navigating busy urban environments full of unpredictable obstacles and pedestrians. Assisting humans with AI agents in maneuvering micromobility devices presents a viable solution for enhancing safety and efficiency. In this work, we present a scalable urban simulation solution to advance autonomous micromobility. First, we build URBAN-SIM - a high-performance robot learning platform for large-scale training of embodied agents in interactive urban scenes. URBAN-SIM contains three critical modules: Hierarchical Urban Generation pipeline, Interactive Dynamics Generation strategy, and Asynchronous Scene Sampling scheme, to improve the diversity, realism, and efficiency of robot learning in simulation. Then, we propose URBAN-BENCH - a suite of essential tasks and benchmarks to gauge various capabilities of the AI agents in achieving autonomous micromobility. URBAN-BENCH includes eight tasks based on three core skills of the agents: Urban Locomotion, Urban Navigation, and Urban Traverse. We evaluate four robots with heterogeneous embodiments, such as the wheeled and legged robots, across these tasks. Experiments on diverse terrains and urban structures reveal each robot's strengths and limitations."
  },
  {
    "title": "Multi-Constraint Safe Reinforcement Learning via Closed-form Solution for Log-Sum-Exp Approximation of Control Barrier Functions",
    "url": "http://arxiv.org/abs/2505.00671v1",
    "arxiv_id": "2505.00671v1",
    "authors": [
      "Chenggang Wang",
      "Xinyi Wang",
      "Yutong Dong",
      "Lei Song",
      "Xinping Guan"
    ],
    "published": "2025-05-01T17:22:11+00:00",
    "summary": "The safety of training task policies and their subsequent application using reinforcement learning (RL) methods has become a focal point in the field of safe RL. A central challenge in this area remains the establishment of theoretical guarantees for safety during both the learning and deployment processes. Given the successful implementation of Control Barrier Function (CBF)-based safety strategies in a range of control-affine robotic systems, CBF-based safe RL demonstrates significant promise for practical applications in real-world scenarios. However, integrating these two approaches presents several challenges. First, embedding safety optimization within the RL training pipeline requires that the optimization outputs be differentiable with respect to the input parameters, a condition commonly referred to as differentiable optimization, which is non-trivial to solve. Second, the differentiable optimization framework confronts significant efficiency issues, especially when dealing with multi-constraint problems. To address these challenges, this paper presents a CBF-based safe RL architecture that effectively mitigates the issues outlined above. The proposed approach constructs a continuous AND logic approximation for the multiple constraints using a single composite CBF. By leveraging this approximation, a close-form solution of the quadratic programming is derived for the policy network in RL, thereby circumventing the need for differentiable optimization within the end-to-end safe RL pipeline. This strategy significantly reduces computational complexity because of the closed-form solution while maintaining safety guarantees. Simulation results demonstrate that, in comparison to existing approaches relying on differentiable optimization, the proposed method significantly reduces training computational costs while ensuring provable safety throughout the training process."
  },
  {
    "title": "DeepCritic: Deliberate Critique with Large Language Models",
    "url": "http://arxiv.org/abs/2505.00662v1",
    "arxiv_id": "2505.00662v1",
    "authors": [
      "Wenkai Yang",
      "Jingwen Chen",
      "Yankai Lin",
      "Ji-Rong Wen"
    ],
    "published": "2025-05-01T17:03:17+00:00",
    "summary": "As Large Language Models (LLMs) are rapidly evolving, providing accurate feedback and scalable oversight on their outputs becomes an urgent and critical problem. Leveraging LLMs as critique models to achieve automated supervision is a promising solution. In this work, we focus on studying and enhancing the math critique ability of LLMs. Current LLM critics provide critiques that are too shallow and superficial on each step, leading to low judgment accuracy and struggling to offer sufficient feedback for the LLM generator to correct mistakes. To tackle this issue, we propose a novel and effective two-stage framework to develop LLM critics that are capable of deliberately critiquing on each reasoning step of math solutions. In the first stage, we utilize Qwen2.5-72B-Instruct to generate 4.5K long-form critiques as seed data for supervised fine-tuning. Each seed critique consists of deliberate step-wise critiques that includes multi-perspective verifications as well as in-depth critiques of initial critiques for each reasoning step. Then, we perform reinforcement learning on the fine-tuned model with either existing human-labeled data from PRM800K or our automatically annotated data obtained via Monte Carlo sampling-based correctness estimation, to further incentivize its critique ability. Our developed critique model built on Qwen2.5-7B-Instruct not only significantly outperforms existing LLM critics (including the same-sized DeepSeek-R1-distill models and GPT-4o) on various error identification benchmarks, but also more effectively helps the LLM generator refine erroneous steps through more detailed feedback."
  },
  {
    "title": "Catastrophic Liability: Managing Systemic Risks in Frontier AI Development",
    "url": "http://arxiv.org/abs/2505.00616v1",
    "arxiv_id": "2505.00616v1",
    "authors": [
      "Aidan Kierans",
      "Kaley Rittichier",
      "Utku Sonsayar"
    ],
    "published": "2025-05-01T15:47:14+00:00",
    "summary": "As artificial intelligence systems grow more capable and autonomous, frontier AI development poses potential systemic risks that could affect society at a massive scale. Current practices at many AI labs developing these systems lack sufficient transparency around safety measures, testing procedures, and governance structures. This opacity makes it challenging to verify safety claims or establish appropriate liability when harm occurs. Drawing on liability frameworks from nuclear energy, aviation software, and healthcare, we propose a comprehensive approach to safety documentation and accountability in frontier AI development."
  },
  {
    "title": "100 Days After DeepSeek-R1: A Survey on Replication Studies and More Directions for Reasoning Language Models",
    "url": "http://arxiv.org/abs/2505.00551v1",
    "arxiv_id": "2505.00551v1",
    "authors": [
      "Chong Zhang",
      "Yue Deng",
      "Xiang Lin",
      "Bin Wang",
      "Dianwen Ng",
      "Hai Ye",
      "Xingxuan Li",
      "Yao Xiao",
      "Zhanfeng Mo",
      "Qi Zhang",
      "Lidong Bing"
    ],
    "published": "2025-05-01T14:28:35+00:00",
    "summary": "The recent development of reasoning language models (RLMs) represents a novel evolution in large language models. In particular, the recent release of DeepSeek-R1 has generated widespread social impact and sparked enthusiasm in the research community for exploring the explicit reasoning paradigm of language models. However, the implementation details of the released models have not been fully open-sourced by DeepSeek, including DeepSeek-R1-Zero, DeepSeek-R1, and the distilled small models. As a result, many replication studies have emerged aiming to reproduce the strong performance achieved by DeepSeek-R1, reaching comparable performance through similar training procedures and fully open-source data resources. These works have investigated feasible strategies for supervised fine-tuning (SFT) and reinforcement learning from verifiable rewards (RLVR), focusing on data preparation and method design, yielding various valuable insights. In this report, we provide a summary of recent replication studies to inspire future research. We primarily focus on SFT and RLVR as two main directions, introducing the details for data construction, method design and training procedure of current replication studies. Moreover, we conclude key findings from the implementation details and experimental results reported by these studies, anticipating to inspire future research. We also discuss additional techniques of enhancing RLMs, highlighting the potential of expanding the application scope of these models, and discussing the challenges in development. By this survey, we aim to help researchers and developers of RLMs stay updated with the latest advancements, and seek to inspire new ideas to further enhance RLMs."
  },
  {
    "title": "100 Days After DeepSeek-R1: A Survey on Replication Studies and More Directions for Reasoning Language Models",
    "url": "http://arxiv.org/abs/2505.00551v2",
    "arxiv_id": "2505.00551v2",
    "authors": [
      "Chong Zhang",
      "Yue Deng",
      "Xiang Lin",
      "Bin Wang",
      "Dianwen Ng",
      "Hai Ye",
      "Xingxuan Li",
      "Yao Xiao",
      "Zhanfeng Mo",
      "Qi Zhang",
      "Lidong Bing"
    ],
    "published": "2025-05-01T14:28:35+00:00",
    "summary": "The recent development of reasoning language models (RLMs) represents a novel evolution in large language models. In particular, the recent release of DeepSeek-R1 has generated widespread social impact and sparked enthusiasm in the research community for exploring the explicit reasoning paradigm of language models. However, the implementation details of the released models have not been fully open-sourced by DeepSeek, including DeepSeek-R1-Zero, DeepSeek-R1, and the distilled small models. As a result, many replication studies have emerged aiming to reproduce the strong performance achieved by DeepSeek-R1, reaching comparable performance through similar training procedures and fully open-source data resources. These works have investigated feasible strategies for supervised fine-tuning (SFT) and reinforcement learning from verifiable rewards (RLVR), focusing on data preparation and method design, yielding various valuable insights. In this report, we provide a summary of recent replication studies to inspire future research. We primarily focus on SFT and RLVR as two main directions, introducing the details for data construction, method design and training procedure of current replication studies. Moreover, we conclude key findings from the implementation details and experimental results reported by these studies, anticipating to inspire future research. We also discuss additional techniques of enhancing RLMs, highlighting the potential of expanding the application scope of these models, and discussing the challenges in development. By this survey, we aim to help researchers and developers of RLMs stay updated with the latest advancements, and seek to inspire new ideas to further enhance RLMs."
  },
  {
    "title": "Linear Phase Balancing Scheme using Voltage Unbalance Sensitivities in Multi-phase Power Distribution Grids",
    "url": "http://arxiv.org/abs/2505.00519v1",
    "arxiv_id": "2505.00519v1",
    "authors": [
      "Rahul K. Gupta"
    ],
    "published": "2025-05-01T13:40:31+00:00",
    "summary": "Power distribution networks, especially in North America, are often unbalanced due to the mix of single-, two- and three-phase networks as well as due to the high penetration of single-phase devices at the distribution level such as electric vehicle (EV) chargers and single-phase solar plants. However, the network operator must adhere to the voltage unbalance levels within the limits specified by IEEE, IEC, and NEMA standards for the safety of the equipment as well as the efficiency of the network operation. Existing works have proposed active and reactive power control in the network to minimize imbalances. However, these optimization problems are highly nonlinear and nonconvex due to the inherent non-linearity of unbalanced metrics and power-flow equations. In this work, we propose a linearization approach of unbalance metrics such as voltage unbalance factors (VUF), phase voltage unbalance rate (PVUR), and line voltage unbalance rate (LVUR) using the first order Taylor's approximation. This linearization is then applied to the phase balancing control scheme; it is formulated as a feedback approach where the linearization is updated successively after the active/reactive control setpoint has been actuated and shows improvement in voltage imbalances. We demonstrate the application of the proposed scheme on a standard IEEE benchmark test case, demonstrating its effectiveness."
  },
  {
    "title": "Safety-Critical Traffic Simulation with Guided Latent Diffusion Model",
    "url": "http://arxiv.org/abs/2505.00515v1",
    "arxiv_id": "2505.00515v1",
    "authors": [
      "Mingxing Peng",
      "Ruoyu Yao",
      "Xusen Guo",
      "Yuting Xie",
      "Xianda Chen",
      "Jun Ma"
    ],
    "published": "2025-05-01T13:33:34+00:00",
    "summary": "Safety-critical traffic simulation plays a crucial role in evaluating autonomous driving systems under rare and challenging scenarios. However, existing approaches often generate unrealistic scenarios due to insufficient consideration of physical plausibility and suffer from low generation efficiency. To address these limitations, we propose a guided latent diffusion model (LDM) capable of generating physically realistic and adversarial safety-critical traffic scenarios. Specifically, our model employs a graph-based variational autoencoder (VAE) to learn a compact latent space that captures complex multi-agent interactions while improving computational efficiency. Within this latent space, the diffusion model performs the denoising process to produce realistic trajectories. To enable controllable and adversarial scenario generation, we introduce novel guidance objectives that drive the diffusion process toward producing adversarial and behaviorally realistic driving behaviors. Furthermore, we develop a sample selection module based on physical feasibility checks to further enhance the physical plausibility of the generated scenarios. Extensive experiments on the nuScenes dataset demonstrate that our method achieves superior adversarial effectiveness and generation efficiency compared to existing baselines while maintaining a high level of realism. Our work provides an effective tool for realistic safety-critical scenario simulation, paving the way for more robust evaluation of autonomous driving systems."
  },
  {
    "title": "Variational OOD State Correction for Offline Reinforcement Learning",
    "url": "http://arxiv.org/abs/2505.00503v1",
    "arxiv_id": "2505.00503v1",
    "authors": [
      "Ke Jiang",
      "Wen Jiang",
      "Xiaoyang Tan"
    ],
    "published": "2025-05-01T13:14:07+00:00",
    "summary": "The performance of Offline reinforcement learning is significantly impacted by the issue of state distributional shift, and out-of-distribution (OOD) state correction is a popular approach to address this problem. In this paper, we propose a novel method named Density-Aware Safety Perception (DASP) for OOD state correction. Specifically, our method encourages the agent to prioritize actions that lead to outcomes with higher data density, thereby promoting its operation within or the return to in-distribution (safe) regions. To achieve this, we optimize the objective within a variational framework that concurrently considers both the potential outcomes of decision-making and their density, thus providing crucial contextual information for safe decision-making. Finally, we validate the effectiveness and feasibility of our proposed method through extensive experimental evaluations on the offline MuJoCo and AntMaze suites."
  },
  {
    "title": "A Generalised Framework for Property-Driven Machine Learning",
    "url": "http://arxiv.org/abs/2505.00466v1",
    "arxiv_id": "2505.00466v1",
    "authors": [
      "Thomas Flinkow",
      "Marco Casadio",
      "Colin Kessler",
      "Rosemary Monahan",
      "Ekaterina Komendantskaya"
    ],
    "published": "2025-05-01T11:33:38+00:00",
    "summary": "Neural networks have been shown to frequently fail to satisfy critical safety and correctness properties after training, highlighting the pressing need for training methods that incorporate such properties directly. While adversarial training can be used to improve robustness to small perturbations within $\\epsilon$-cubes, domains other than computer vision -- such as control systems and natural language processing -- may require more flexible input region specifications via generalised hyper-rectangles. Meanwhile, differentiable logics offer a way to encode arbitrary logical constraints as additional loss terms that guide the learning process towards satisfying these constraints. In this paper, we investigate how these two complementary approaches can be unified within a single framework for property-driven machine learning. We show that well-known properties from the literature are subcases of this general approach, and we demonstrate its practical effectiveness on a case study involving a neural network controller for a drone system. Our framework is publicly available at https://github.com/tflinkow/property-driven-ml."
  },
  {
    "title": "Toward Automated Regulatory Decision-Making: Trustworthy Medical Device Risk Classification with Multimodal Transformers and Self-Training",
    "url": "http://arxiv.org/abs/2505.00422v1",
    "arxiv_id": "2505.00422v1",
    "authors": [
      "Yu Han",
      "Aaron Ceross",
      "Jeroen H. M. Bergmann"
    ],
    "published": "2025-05-01T09:41:41+00:00",
    "summary": "Accurate classification of medical device risk levels is essential for regulatory oversight and clinical safety. We present a Transformer-based multimodal framework that integrates textual descriptions and visual information to predict device regulatory classification. The model incorporates a cross-attention mechanism to capture intermodal dependencies and employs a self-training strategy for improved generalization under limited supervision. Experiments on a real-world regulatory dataset demonstrate that our approach achieves up to 90.4% accuracy and 97.9% AUROC, significantly outperforming text-only (77.2%) and image-only (54.8%) baselines. Compared to standard multimodal fusion, the self-training mechanism improved SVM performance by 3.3 percentage points in accuracy (from 87.1% to 90.4%) and 1.4 points in macro-F1, suggesting that pseudo-labeling can effectively enhance generalization under limited supervision. Ablation studies further confirm the complementary benefits of both cross-modal attention and self-training."
  },
  {
    "title": "Safety in the Face of Adversity: Achieving Zero Constraint Violation in Online Learning with Slowly Changing Constraints",
    "url": "http://arxiv.org/abs/2505.00398v1",
    "arxiv_id": "2505.00398v1",
    "authors": [
      "Bassel Hamoud",
      "Ilnura Usmanova",
      "Kfir Y. Levy"
    ],
    "published": "2025-05-01T08:41:17+00:00",
    "summary": "We present the first theoretical guarantees for zero constraint violation in Online Convex Optimization (OCO) across all rounds, addressing dynamic constraint changes. Unlike existing approaches in constrained OCO, which allow for occasional safety breaches, we provide the first approach for maintaining strict safety under the assumption of gradually evolving constraints, namely the constraints change at most by a small amount between consecutive rounds. This is achieved through a primal-dual approach and Online Gradient Ascent in the dual space. We show that employing a dichotomous learning rate enables ensuring both safety, via zero constraint violation, and sublinear regret. Our framework marks a departure from previous work by providing the first provable guarantees for maintaining absolute safety in the face of changing constraints in OCO."
  },
  {
    "title": "A Survey on Large Language Model based Human-Agent Systems",
    "url": "http://arxiv.org/abs/2505.00753v1",
    "arxiv_id": "2505.00753v1",
    "authors": [
      "Henry Peng Zou",
      "Wei-Chieh Huang",
      "Yaozu Wu",
      "Yankai Chen",
      "Chunyu Miao",
      "Hoang Nguyen",
      "Yue Zhou",
      "Weizhi Zhang",
      "Liancheng Fang",
      "Langzhou He",
      "Yangning Li",
      "Yuwei Cao",
      "Dongyuan Li",
      "Renhe Jiang",
      "Philip S. Yu"
    ],
    "published": "2025-05-01T08:29:26+00:00",
    "summary": "Recent advances in large language models (LLMs) have sparked growing interest in building fully autonomous agents. However, fully autonomous LLM-based agents still face significant challenges, including limited reliability due to hallucinations, difficulty in handling complex tasks, and substantial safety and ethical risks, all of which limit their feasibility and trustworthiness in real-world applications. To overcome these limitations, LLM-based human-agent systems (LLM-HAS) incorporate human-provided information, feedback, or control into the agent system to enhance system performance, reliability and safety. This paper provides the first comprehensive and structured survey of LLM-HAS. It clarifies fundamental concepts, systematically presents core components shaping these systems, including environment & profiling, human feedback, interaction types, orchestration and communication, explores emerging applications, and discusses unique challenges and opportunities. By consolidating current knowledge and offering a structured overview, we aim to foster further research and innovation in this rapidly evolving interdisciplinary field. Paper lists and resources are available at https://github.com/HenryPengZou/Awesome-LLM-Based-Human-Agent-System-Papers."
  },
  {
    "title": "Optimizing Deep Neural Networks using Safety-Guided Self Compression",
    "url": "http://arxiv.org/abs/2505.00350v1",
    "arxiv_id": "2505.00350v1",
    "authors": [
      "Mohammad Zbeeb",
      "Mariam Salman",
      "Mohammad Bazzi",
      "Ammar Mohanna"
    ],
    "published": "2025-05-01T06:50:30+00:00",
    "summary": "The deployment of deep neural networks on resource-constrained devices necessitates effective model com- pression strategies that judiciously balance the reduction of model size with the preservation of performance. This study introduces a novel safety-driven quantization framework that leverages preservation sets to systematically prune and quantize neural network weights, thereby optimizing model complexity without compromising accuracy. The proposed methodology is rigorously evaluated on both a convolutional neural network (CNN) and an attention-based language model, demonstrating its applicability across diverse architectural paradigms. Experimental results reveal that our framework achieves up to a 2.5% enhancement in test accuracy relative to the original unquantized models while maintaining 60% of the initial model size. In comparison to conventional quantization techniques, our approach not only augments generalization by eliminating parameter noise and retaining essential weights but also reduces variance, thereby ensuring the retention of critical model features. These findings underscore the efficacy of safety-driven quantization as a robust and reliable strategy for the efficient optimization of deep learn- ing models. The implementation and comprehensive experimental evaluations of our framework are publicly accessible at GitHub."
  },
  {
    "title": "Vehicular Communication Security: Multi-Channel and Multi-Factor Authentication",
    "url": "http://arxiv.org/abs/2505.00340v1",
    "arxiv_id": "2505.00340v1",
    "authors": [
      "Marco De Vincenzi",
      "Shuyang Sun",
      "Chen Bo Calvin Zhang",
      "Manuel Garcia",
      "Shaozu Ding",
      "Chiara Bodei",
      "Ilaria Matteucci",
      "Dajiang Suo"
    ],
    "published": "2025-05-01T06:36:24+00:00",
    "summary": "Secure and reliable communications are crucial for Intelligent Transportation Systems (ITSs), where Vehicle-to-Infrastructure (V2I) communication plays a key role in enabling mobility-enhancing and safety-critical services. Current V2I authentication relies on credential-based methods over wireless Non-Line-of-Sight (NLOS) channels, leaving them exposed to remote impersonation and proximity attacks. To mitigate these risks, we propose a unified Multi-Channel, Multi-Factor Authentication (MFA) scheme that combines NLOS cryptographic credentials with a Line-of-Sight (LOS) visual channel. Our approach leverages a challenge-response security paradigm: the infrastructure issues challenges and the vehicle's headlights respond by flashing a structured sequence containing encoded security data. Deep learning models on the infrastructure side then decode the embedded information to authenticate the vehicle. Real-world experimental evaluations demonstrate high test accuracy, reaching an average of 95% and 96.6%, respectively, under various lighting, weather, speed, and distance conditions. Additionally, we conducted extensive experiments on three state-of-the-art deep learning models, including detailed ablation studies for decoding the flashing sequence. Our results indicate that the optimal architecture employs a dual-channel design, enabling simultaneous decoding of the flashing sequence and extraction of vehicle spatial and locational features for robust authentication."
  },
  {
    "title": "AI2-Active Safety: AI-enabled Interaction-aware Active Safety Analysis with Vehicle Dynamics",
    "url": "http://arxiv.org/abs/2505.00322v1",
    "arxiv_id": "2505.00322v1",
    "authors": [
      "Keshu Wu",
      "Zihao Li",
      "Sixu Li",
      "Xinyue Ye",
      "Dominique Lord",
      "Yang Zhou"
    ],
    "published": "2025-05-01T05:46:34+00:00",
    "summary": "This paper introduces an AI-enabled, interaction-aware active safety analysis framework that accounts for groupwise vehicle interactions. Specifically, the framework employs a bicycle model-augmented with road gradient considerations-to accurately capture vehicle dynamics. In parallel, a hypergraph-based AI model is developed to predict probabilistic trajectories of ambient traffic. By integrating these two components, the framework derives vehicle intra-spacing over a 3D road surface as the solution of a stochastic ordinary differential equation, yielding high-fidelity surrogate safety measures such as time-to-collision (TTC). To demonstrate its effectiveness, the framework is analyzed using stochastic numerical methods comprising 4th-order Runge-Kutta integration and AI inference, generating probability-weighted high-fidelity TTC (HF-TTC) distributions that reflect complex multi-agent maneuvers and behavioral uncertainties. Evaluated with HF-TTC against traditional constant-velocity TTC and non-interaction-aware approaches on highway datasets, the proposed framework offers a systematic methodology for active safety analysis with enhanced potential for improving safety perception in complex traffic environments."
  },
  {
    "title": "Beyond Quadratic Costs: A Bregman Divergence Approach to H$_\\infty$ Control",
    "url": "http://arxiv.org/abs/2505.00319v1",
    "arxiv_id": "2505.00319v1",
    "authors": [
      "Joudi Hajar",
      "Reza Ghane",
      "Babak Hassibi"
    ],
    "published": "2025-05-01T05:38:36+00:00",
    "summary": "This paper presents a novel extension of the H$_\\infty$ control framework that generalizes the traditional quadratic cost formulation to accommodate strictly convex, nonquadratic functions for the state, control input, and disturbance. This new formulation not only captures additional noise characteristics but also supports a range of performance objectives-including sparse control, safety constraints, and other tailored behaviors-beyond what is possible with quadratic costs. We derive a closed-form solution of a central controller that minimizes the worst-case performance ratio under the proposed cost structure. Furthermore, we develop Riccati-like equations that impose necessary and sufficient conditions on the nonquadratic cost functions, thereby ensuring the existence of a robust solution. Finally, we rigorously establish Lyapunov stability for the closed-loop system. The proposed framework bridges robust control theory with modern approaches in machine learning and signal processing, offering enhanced flexibility and improved performance in complex control scenarios."
  },
  {
    "title": "Beyond Quadratic Costs in LQR: Bregman Divergence Control",
    "url": "http://arxiv.org/abs/2505.00317v1",
    "arxiv_id": "2505.00317v1",
    "authors": [
      "Babak Hassibi",
      "Joudi Hajar",
      "Reza Ghane"
    ],
    "published": "2025-05-01T05:31:45+00:00",
    "summary": "In the past couple of decades, the use of ``non-quadratic\" convex cost functions has revolutionized signal processing, machine learning, and statistics, allowing one to customize solutions to have desired structures and properties. However, the situation is not the same in control where the use of quadratic costs still dominates, ostensibly because determining the ``value function\", i.e., the optimal expected cost-to-go, which is critical to the construction of the optimal controller, becomes computationally intractable as soon as one considers general convex costs. As a result, practitioners often resort to heuristics and approximations, such as model predictive control that only looks a few steps into the future. In the quadratic case, the value function is easily determined by solving Riccati equations. In this work, we consider a special class of convex cost functions constructed from Bregman divergence and show how, with appropriate choices, they can be used to fully extend the framework developed for the quadratic case. The resulting optimal controllers are infinite horizon, come with stability guarantees, and have state-feedback, or estimated state-feedback, laws. They exhibit a much wider range of behavior than their quadratic counterparts since the feedback laws are nonlinear. The approach can be applied to several cases of interest, including safety control, sparse control, and bang-bang control."
  },
  {
    "title": "J-PARSE: Jacobian-based Projection Algorithm for Resolving Singularities Effectively in Inverse Kinematic Control of Serial Manipulators",
    "url": "http://arxiv.org/abs/2505.00306v1",
    "arxiv_id": "2505.00306v1",
    "authors": [
      "Shivani Guptasarma",
      "Matthew Strong",
      "Honghao Zhen",
      "Monroe Kennedy III"
    ],
    "published": "2025-05-01T04:58:50+00:00",
    "summary": "J-PARSE is a method for smooth first-order inverse kinematic control of a serial manipulator near kinematic singularities. The commanded end-effector velocity is interpreted component-wise, according to the available mobility in each dimension of the task space. First, a substitute \"Safety\" Jacobian matrix is created, keeping the aspect ratio of the manipulability ellipsoid above a threshold value. The desired motion is then projected onto non-singular and singular directions, and the latter projection scaled down by a factor informed by the threshold value. A right-inverse of the non-singular Safety Jacobian is applied to the modified command. In the absence of joint limits and collisions, this ensures smooth transition into and out of low-rank poses, guaranteeing asymptotic stability for target poses within the workspace, and stability for those outside. Velocity control with J-PARSE is benchmarked against the Least-Squares and Damped Least-Squares inversions of the Jacobian, and shows high accuracy in reaching and leaving singular target poses. By expanding the available workspace of manipulators, the method finds applications in servoing, teleoperation, and learning."
  },
  {
    "title": "PSN Game: Game-theoretic Planning via a Player Selection Network",
    "url": "http://arxiv.org/abs/2505.00213v1",
    "arxiv_id": "2505.00213v1",
    "authors": [
      "Tianyu Qiu",
      "Eric Ouano",
      "Fernando Palafox",
      "Christian Ellis",
      "David Fridovich-Keil"
    ],
    "published": "2025-04-30T23:14:32+00:00",
    "summary": "While game-theoretic planning frameworks are effective at modeling multi-agent interactions, they require solving optimization problems with hundreds or thousands of variables, resulting in long computation times that limit their use in large-scale, real-time systems. To address this issue, we propose PSN Game: a novel game-theoretic planning framework that reduces runtime by learning a Player Selection Network (PSN). A PSN outputs a player selection mask that distinguishes influential players from less relevant ones, enabling the ego player to solve a smaller, masked game involving only selected players. By reducing the number of variables in the optimization problem, PSN directly lowers computation time. The PSN Game framework is more flexible than existing player selection methods as it i) relies solely on observations of players' past trajectories, without requiring full state, control, or other game-specific information; and ii) requires no online parameter tuning. We train PSNs in an unsupervised manner using a differentiable dynamic game solver, with reference trajectories from full-player games guiding the learning. Experiments in both simulated scenarios and human trajectory datasets demonstrate that i) PSNs outperform baseline selection methods in trajectory smoothness and length, while maintaining comparable safety and achieving a 10x speedup in runtime; and ii) PSNs generalize effectively to real-world scenarios without fine-tuning. By selecting only the most relevant players for decision-making, PSNs offer a general mechanism for reducing planning complexity that can be seamlessly integrated into existing multi-agent planning frameworks."
  },
  {
    "title": "Which Agent Causes Task Failures and When? On Automated Failure Attribution of LLM Multi-Agent Systems",
    "url": "http://arxiv.org/abs/2505.00212v1",
    "arxiv_id": "2505.00212v1",
    "authors": [
      "Shaokun Zhang",
      "Ming Yin",
      "Jieyu Zhang",
      "Jiale Liu",
      "Zhiguang Han",
      "Jingyang Zhang",
      "Beibin Li",
      "Chi Wang",
      "Huazheng Wang",
      "Yiran Chen",
      "Qingyun Wu"
    ],
    "published": "2025-04-30T23:09:44+00:00",
    "summary": "Failure attribution in LLM multi-agent systems-identifying the agent and step responsible for task failures-provides crucial clues for systems debugging but remains underexplored and labor-intensive. In this paper, we propose and formulate a new research area: automated failure attribution for LLM multi-agent systems. To support this initiative, we introduce the Who&When dataset, comprising extensive failure logs from 127 LLM multi-agent systems with fine-grained annotations linking failures to specific agents and decisive error steps. Using the Who&When, we develop and evaluate three automated failure attribution methods, summarizing their corresponding pros and cons. The best method achieves 53.5% accuracy in identifying failure-responsible agents but only 14.2% in pinpointing failure steps, with some methods performing below random. Even SOTA reasoning models, such as OpenAI o1 and DeepSeek R1, fail to achieve practical usability. These results highlight the task's complexity and the need for further research in this area. Code and dataset are available at https://github.com/mingyin1/Agents_Failure_Attribution"
  },
  {
    "title": "Real-World Gaps in AI Governance Research",
    "url": "http://arxiv.org/abs/2505.00174v1",
    "arxiv_id": "2505.00174v1",
    "authors": [
      "Ilan Strauss",
      "Isobel Moure",
      "Tim O'Reilly",
      "Sruly Rosenblat"
    ],
    "published": "2025-04-30T20:44:42+00:00",
    "summary": "Drawing on 1,178 safety and reliability papers from 9,439 generative AI papers (January 2020 - March 2025), we compare research outputs of leading AI companies (Anthropic, Google DeepMind, Meta, Microsoft, and OpenAI) and AI universities (CMU, MIT, NYU, Stanford, UC Berkeley, and University of Washington). We find that corporate AI research increasingly concentrates on pre-deployment areas -- model alignment and testing & evaluation -- while attention to deployment-stage issues such as model bias has waned. Significant research gaps exist in high-risk deployment domains, including healthcare, finance, misinformation, persuasive and addictive features, hallucinations, and copyright. Without improved observability into deployed AI, growing corporate concentration could deepen knowledge deficits. We recommend expanding external researcher access to deployment data and systematic observability of in-market AI behaviors."
  },
  {
    "title": "Audo-Sight: Enabling Ambient Interaction For Blind And Visually Impaired Individuals",
    "url": "http://arxiv.org/abs/2505.00153v1",
    "arxiv_id": "2505.00153v1",
    "authors": [
      "Bhanuja Ainary"
    ],
    "published": "2025-04-30T19:55:19+00:00",
    "summary": "Visually impaired people face significant challenges when attempting to interact with and understand complex environments, and traditional assistive technologies often struggle to quickly provide necessary contextual understanding and interactive intelligence. This thesis presents Audo-Sight, a state-of-the-art assistive system that seamlessly integrates Multimodal Large Language Models (MLLMs) to provide expedient, context-aware interactions for Blind and Visually Impaired (BVI) individuals. The system operates in two different modalities: personalized interaction through user identification and public access in common spaces like museums and shopping malls. In tailored environments, the system adjusts its output to conform to the preferences of individual users, thus enhancing accessibility through a user-aware form of interaction. In shared environments, Audo-Sight employs a shared architecture that adapts to its current user with no manual reconfiguration required. To facilitate appropriate interactions with the LLM, the public Audo-Sight solution includes an Age-Range Determiner and Safe Query Filter. Additionally, the system ensures that responses are respectful to BVI users through NeMo Guardrails. By utilizing multimodal reasoning, BVI-cognizant response editing, and safeguarding features, this work represents a major leap in AI-driven accessibility technology capable of increasing autonomy, safety, and interaction for people with visual impairments in social settings. Finally, we present the integration of Audo-Sight and SmartSight, which enables enhanced situational awareness for BVI individuals. This integration takes advantage of the real-time visual analysis of SmartSight, combined with the extensive reasoning and interactive capabilities of Audo-Sight, and goes beyond object identification to provide context-driven, voice-controlled assistance in dynamic environments."
  },
  {
    "title": "A Survey of Interactive Generative Video",
    "url": "http://arxiv.org/abs/2504.21853v1",
    "arxiv_id": "2504.21853v1",
    "authors": [
      "Jiwen Yu",
      "Yiran Qin",
      "Haoxuan Che",
      "Quande Liu",
      "Xintao Wang",
      "Pengfei Wan",
      "Di Zhang",
      "Kun Gai",
      "Hao Chen",
      "Xihui Liu"
    ],
    "published": "2025-04-30T17:59:02+00:00",
    "summary": "Interactive Generative Video (IGV) has emerged as a crucial technology in response to the growing demand for high-quality, interactive video content across various domains. In this paper, we define IGV as a technology that combines generative capabilities to produce diverse high-quality video content with interactive features that enable user engagement through control signals and responsive feedback. We survey the current landscape of IGV applications, focusing on three major domains: 1) gaming, where IGV enables infinite exploration in virtual worlds; 2) embodied AI, where IGV serves as a physics-aware environment synthesizer for training agents in multimodal interaction with dynamically evolving scenes; and 3) autonomous driving, where IGV provides closed-loop simulation capabilities for safety-critical testing and validation. To guide future development, we propose a comprehensive framework that decomposes an ideal IGV system into five essential modules: Generation, Control, Memory, Dynamics, and Intelligence. Furthermore, we systematically analyze the technical challenges and future directions in realizing each component for an ideal IGV system, such as achieving real-time generation, enabling open-domain control, maintaining long-term coherence, simulating accurate physics, and integrating causal reasoning. We believe that this systematic analysis will facilitate future research and development in the field of IGV, ultimately advancing the technology toward more sophisticated and practical applications."
  },
  {
    "title": "Neuro-Symbolic Generation of Explanations for Robot Policies with Weighted Signal Temporal Logic",
    "url": "http://arxiv.org/abs/2504.21841v1",
    "arxiv_id": "2504.21841v1",
    "authors": [
      "Mikihisa Yuasa",
      "Ramavarapu S. Sreenivas",
      "Huy T. Tran"
    ],
    "published": "2025-04-30T17:51:20+00:00",
    "summary": "Neural network-based policies have demonstrated success in many robotic applications, but often lack human-explanability, which poses challenges in safety-critical deployments. To address this, we propose a neuro-symbolic explanation framework that generates a weighted signal temporal logic (wSTL) specification to describe a robot policy in a interpretable form. Existing methods typically produce explanations that are verbose and inconsistent, which hinders explainability, and loose, which do not give meaningful insights into the underlying policy. We address these issues by introducing a simplification process consisting of predicate filtering, regularization, and iterative pruning. We also introduce three novel explainability evaluation metrics -- conciseness, consistency, and strictness -- to assess explanation quality beyond conventional classification metrics. Our method is validated in three simulated robotic environments, where it outperforms baselines in generating concise, consistent, and strict wSTL explanations without sacrificing classification accuracy. This work bridges policy learning with formal methods, contributing to safer and more transparent decision-making in robotics."
  },
  {
    "title": "WebThinker: Empowering Large Reasoning Models with Deep Research Capability",
    "url": "http://arxiv.org/abs/2504.21776v1",
    "arxiv_id": "2504.21776v1",
    "authors": [
      "Xiaoxi Li",
      "Jiajie Jin",
      "Guanting Dong",
      "Hongjin Qian",
      "Yutao Zhu",
      "Yongkang Wu",
      "Ji-Rong Wen",
      "Zhicheng Dou"
    ],
    "published": "2025-04-30T16:25:25+00:00",
    "summary": "Large reasoning models (LRMs), such as OpenAI-o1 and DeepSeek-R1, demonstrate impressive long-horizon reasoning capabilities. However, their reliance on static internal knowledge limits their performance on complex, knowledge-intensive tasks and hinders their ability to produce comprehensive research reports requiring synthesis of diverse web information. To address this, we propose \\textbf{WebThinker}, a deep research agent that empowers LRMs to autonomously search the web, navigate web pages, and draft research reports during the reasoning process. WebThinker integrates a \\textbf{Deep Web Explorer} module, enabling LRMs to dynamically search, navigate, and extract information from the web when encountering knowledge gaps. It also employs an \\textbf{Autonomous Think-Search-and-Draft strategy}, allowing the model to seamlessly interleave reasoning, information gathering, and report writing in real time. To further enhance research tool utilization, we introduce an \\textbf{RL-based training strategy} via iterative online Direct Preference Optimization (DPO). Extensive experiments on complex reasoning benchmarks (GPQA, GAIA, WebWalkerQA, HLE) and scientific report generation tasks (Glaive) demonstrate that WebThinker significantly outperforms existing methods and strong proprietary systems. Our approach enhances LRM reliability and applicability in complex scenarios, paving the way for more capable and versatile deep research systems. The code is available at https://github.com/RUC-NLPIR/WebThinker."
  },
  {
    "title": "CodeFlowBench: A Multi-turn, Iterative Benchmark for Complex Code Generation",
    "url": "http://arxiv.org/abs/2504.21751v1",
    "arxiv_id": "2504.21751v1",
    "authors": [
      "Sizhe Wang",
      "Zhengren Wang",
      "Dongsheng Ma",
      "Yongan Yu",
      "Rui Ling",
      "Zhiyu Li",
      "Feiyu Xiong",
      "Wentao Zhang"
    ],
    "published": "2025-04-30T15:45:28+00:00",
    "summary": "Real world development demands code that is readable, extensible, and testable by organizing the implementation into modular components and iteratively reuse pre-implemented code. We term this iterative, multi-turn process codeflow and introduce CodeFlowBench, the first benchmark designed for comprehensively evaluating LLMs' ability to perform codeflow, namely to implement new functionality by reusing existing functions over multiple turns. CodeFlowBench comprises 5258 problems drawn from Codeforces and is continuously updated via an automated pipeline that decomposes each problem into a series of function-level subproblems based on its dependency tree and each subproblem is paired with unit tests. We further propose a novel evaluation framework with tasks and metrics tailored to multi-turn code reuse to assess model performance. In experiments across various LLMs under both multi-turn and single-turn patterns. We observe models' poor performance on CodeFlowBench, with a substantial performance drop in the iterative codeflow scenario. For instance, o1-mini achieves a pass@1 of 20.8% in multi-turn pattern versus 37.8% in single-turn pattern. Further analysis shows that different models excel at different dependency depths, yet all struggle to correctly solve structurally complex problems, highlighting challenges for current LLMs to serve as code generation tools when performing codeflow. Overall, CodeFlowBench offers a comprehensive benchmark and new insights into LLM capabilities for multi-turn, iterative code generation, guiding future advances in code generation tasks."
  },
  {
    "title": "XBreaking: Explainable Artificial Intelligence for Jailbreaking LLMs",
    "url": "http://arxiv.org/abs/2504.21700v1",
    "arxiv_id": "2504.21700v1",
    "authors": [
      "Marco Arazzi",
      "Vignesh Kumar Kembu",
      "Antonino Nocera",
      "Vinod P"
    ],
    "published": "2025-04-30T14:44:24+00:00",
    "summary": "Large Language Models are fundamental actors in the modern IT landscape dominated by AI solutions. However, security threats associated with them might prevent their reliable adoption in critical application scenarios such as government organizations and medical institutions. For this reason, commercial LLMs typically undergo a sophisticated censoring mechanism to eliminate any harmful output they could possibly produce. In response to this, LLM Jailbreaking is a significant threat to such protections, and many previous approaches have already demonstrated its effectiveness across diverse domains. Existing jailbreak proposals mostly adopt a generate-and-test strategy to craft malicious input. To improve the comprehension of censoring mechanisms and design a targeted jailbreak attack, we propose an Explainable-AI solution that comparatively analyzes the behavior of censored and uncensored models to derive unique exploitable alignment patterns. Then, we propose XBreaking, a novel jailbreak attack that exploits these unique patterns to break the security constraints of LLMs by targeted noise injection. Our thorough experimental campaign returns important insights about the censoring mechanisms and demonstrates the effectiveness and performance of our attack."
  },
  {
    "title": "REHEARSE-3D: A Multi-modal Emulated Rain Dataset for 3D Point Cloud De-raining",
    "url": "http://arxiv.org/abs/2504.21699v1",
    "arxiv_id": "2504.21699v1",
    "authors": [
      "Abu Mohammed Raisuddin",
      "Jesper Holmblad",
      "Hamed Haghighi",
      "Yuri Poledna",
      "Maikol Funk Drechsler",
      "Valentina Donzella",
      "Eren Erdal Aksoy"
    ],
    "published": "2025-04-30T14:43:38+00:00",
    "summary": "Sensor degradation poses a significant challenge in autonomous driving. During heavy rainfall, the interference from raindrops can adversely affect the quality of LiDAR point clouds, resulting in, for instance, inaccurate point measurements. This, in turn, can potentially lead to safety concerns if autonomous driving systems are not weather-aware, i.e., if they are unable to discern such changes. In this study, we release a new, large-scale, multi-modal emulated rain dataset, REHEARSE-3D, to promote research advancements in 3D point cloud de-raining. Distinct from the most relevant competitors, our dataset is unique in several respects. First, it is the largest point-wise annotated dataset, and second, it is the only one with high-resolution LiDAR data (LiDAR-256) enriched with 4D Radar point clouds logged in both daytime and nighttime conditions in a controlled weather environment. Furthermore, REHEARSE-3D involves rain-characteristic information, which is of significant value not only for sensor noise modeling but also for analyzing the impact of weather at a point level. Leveraging REHEARSE-3D, we benchmark raindrop detection and removal in fused LiDAR and 4D Radar point clouds. Our comprehensive study further evaluates the performance of various statistical and deep-learning models. Upon publication, the dataset and benchmark models will be made publicly available at: https://sporsho.github.io/REHEARSE3D."
  },
  {
    "title": "Using quantum annealing to generate test cases for cyber-physical systems",
    "url": "http://arxiv.org/abs/2504.21684v1",
    "arxiv_id": "2504.21684v1",
    "authors": [
      "Hugo Araujo",
      "Xinyi Wang",
      "Mohammad Mousavi",
      "Shaukat Ali"
    ],
    "published": "2025-04-30T14:20:58+00:00",
    "summary": "Quantum computing has emerged as a powerful tool to efficiently solve computational challenges, particularly in simulation and optimisation. However, hardware limitations prevent quantum computers from achieving the full theoretical potential. Among the quantum algorithms, quantum annealing is a prime candidate to solve optimisation problems. This makes it a natural candidate for search-based software testing in the Cyber-Physical Systems (CPS) domain, which demands effective test cases due to their safety-critical nature. This work explores the use of quantum annealing to enhance test case generation for CPS through a mutation-based approach. We encode test case mutation as a binary optimisation problem, and use quantum annealing to identify and target critical regions of the test cases for improvement. Our approach mechanises this process into an algorithm that uses D-Wave's quantum annealer to find the solution. As a main contribution, we offer insights into how quantum annealing can advance software testing methodologies by empirically evaluating the correlation between problem size, hardware limitations, and the effectiveness of the results. Moreover, we compare the proposed method against state-of-the-art classical optimisation algorithms, targeting efficiency (time to generate test cases) and effectiveness (fault detection rates). Results indicate that quantum annealing enables faster test case generation while achieving comparable fault detection performance to state-of-the-art alternatives."
  },
  {
    "title": "Hoist with His Own Petard: Inducing Guardrails to Facilitate Denial-of-Service Attacks on Retrieval-Augmented Generation of LLMs",
    "url": "http://arxiv.org/abs/2504.21680v1",
    "arxiv_id": "2504.21680v1",
    "authors": [
      "Pan Suo",
      "Yu-Ming Shang",
      "San-Chuan Guo",
      "Xi Zhang"
    ],
    "published": "2025-04-30T14:18:11+00:00",
    "summary": "Retrieval-Augmented Generation (RAG) integrates Large Language Models (LLMs) with external knowledge bases, improving output quality while introducing new security risks. Existing studies on RAG vulnerabilities typically focus on exploiting the retrieval mechanism to inject erroneous knowledge or malicious texts, inducing incorrect outputs. However, these approaches overlook critical weaknesses within LLMs, leaving important attack vectors unexplored and limiting the scope and efficiency of attacks. In this paper, we uncover a novel vulnerability: the safety guardrails of LLMs, while designed for protection, can also be exploited as an attack vector by adversaries. Building on this vulnerability, we propose MutedRAG, a novel denial-of-service attack that reversely leverages the guardrails of LLMs to undermine the availability of RAG systems. By injecting minimalistic jailbreak texts, such as \"\\textit{How to build a bomb}\", into the knowledge base, MutedRAG intentionally triggers the LLM's safety guardrails, causing the system to reject legitimate queries. Besides, due to the high sensitivity of guardrails, a single jailbreak sample can affect multiple queries, effectively amplifying the efficiency of attacks while reducing their costs. Experimental results on three datasets demonstrate that MutedRAG achieves an attack success rate exceeding 60% in many scenarios, requiring only less than one malicious text to each target query on average. In addition, we evaluate potential defense strategies against MutedRAG, finding that some of current mechanisms are insufficient to mitigate this threat, underscoring the urgent need for more robust solutions."
  },
  {
    "title": "RDF-Based Structured Quality Assessment Representation of Multilingual LLM Evaluations",
    "url": "http://arxiv.org/abs/2504.21605v1",
    "arxiv_id": "2504.21605v1",
    "authors": [
      "Jonas Gwozdz",
      "Andreas Both"
    ],
    "published": "2025-04-30T13:06:40+00:00",
    "summary": "Large Language Models (LLMs) increasingly serve as knowledge interfaces, yet systematically assessing their reliability with conflicting information remains difficult. We propose an RDF-based framework to assess multilingual LLM quality, focusing on knowledge conflicts. Our approach captures model responses across four distinct context conditions (complete, incomplete, conflicting, and no-context information) in German and English. This structured representation enables the comprehensive analysis of knowledge leakage-where models favor training data over provided context-error detection, and multilingual consistency. We demonstrate the framework through a fire safety domain experiment, revealing critical patterns in context prioritization and language-specific performance, and demonstrating that our vocabulary was sufficient to express every assessment facet encountered in the 28-question study."
  },
  {
    "title": "Toward Realization of Low-Altitude Economy Networks: Core Architecture, Integrated Technologies, and Future Directions",
    "url": "http://arxiv.org/abs/2504.21583v1",
    "arxiv_id": "2504.21583v1",
    "authors": [
      "Yixian Wang",
      "Geng Sun",
      "Zemin Sun",
      "Jiacheng Wang",
      "Jiahui Li",
      "Changyuan Zhao",
      "Jing Wu",
      "Shuang Liang",
      "Minghao Yin",
      "Pengfei Wang",
      "Dusit Niyato",
      "Sumei Sun",
      "Dong In Kim"
    ],
    "published": "2025-04-30T12:42:03+00:00",
    "summary": "The rise of the low-altitude economy (LAE) is propelling urban development and emerging industries by integrating advanced technologies to enhance efficiency, safety, and sustainability in low-altitude operations. The widespread adoption of unmanned aerial vehicles (UAVs) and electric vertical takeoff and landing (eVTOL) aircraft plays a crucial role in enabling key applications within LAE, such as urban logistics, emergency rescue, and aerial mobility. However, unlike traditional UAV networks, LAE networks encounter increased airspace management demands due to dense flying nodes and potential interference with ground communication systems. In addition, there are heightened and extended security risks in real-time operations, particularly the vulnerability of low-altitude aircraft to cyberattacks from ground-based threats. To address these, this paper first explores related standards and core architecture that support the development of LAE networks. Subsequently, we highlight the integration of technologies such as communication, sensing, computing, positioning, navigation, surveillance, flight control, and airspace management. This synergy of multi-technology drives the advancement of real-world LAE applications, particularly in improving operational efficiency, optimizing airspace usage, and ensuring safety. Finally, we outline future research directions for LAE networks, such as intelligent and adaptive optimization, security and privacy protection, sustainable energy and power management, quantum-driven coordination, generative governance, and three-dimensional (3D) airspace coverage, which collectively underscore the potential of collaborative technologies to advance LAE networks."
  },
  {
    "title": "Provably-Safe, Online System Identification",
    "url": "http://arxiv.org/abs/2504.21486v1",
    "arxiv_id": "2504.21486v1",
    "authors": [
      "Bohao Zhang",
      "Zichang Zhou",
      "Ram Vasudevan"
    ],
    "published": "2025-04-30T10:10:32+00:00",
    "summary": "Precise manipulation tasks require accurate knowledge of payload inertial parameters. Unfortunately, identifying these parameters for unknown payloads while ensuring that the robotic system satisfies its input and state constraints while avoiding collisions with the environment remains a significant challenge. This paper presents an integrated framework that enables robotic manipulators to safely and automatically identify payload parameters while maintaining operational safety guarantees. The framework consists of two synergistic components: an online trajectory planning and control framework that generates provably-safe exciting trajectories for system identification that can be tracked while respecting robot constraints and avoiding obstacles and a robust system identification method that computes rigorous overapproximative bounds on end-effector inertial parameters assuming bounded sensor noise. Experimental validation on a robotic manipulator performing challenging tasks with various unknown payloads demonstrates the framework's effectiveness in establishing accurate parameter bounds while maintaining safety throughout the identification process. The code is available at our project webpage: https://roahmlab.github.io/OnlineSafeSysID/."
  },
  {
    "title": "An Intermediate Program Representation for Optimizing Stream-Based Languages",
    "url": "http://arxiv.org/abs/2504.21458v1",
    "arxiv_id": "2504.21458v1",
    "authors": [
      "Jan Baumeister",
      "Arthur Correnson",
      "Bernd Finkbeiner",
      "Frederik Scheerer"
    ],
    "published": "2025-04-30T09:24:53+00:00",
    "summary": "Stream-based runtime monitors are safety assurance tools that check at runtime whether the system's behavior satisfies a formal specification. Specifications consist of stream equations, which relate input streams, containing sensor readings and other incoming information, to output streams, representing filtered and aggregated data. This paper presents a framework for the stream-based specification language RTLola. We introduce a new intermediate representation for stream-based languages, the StreamIR, which, like the specification language, operates on streams of unbounded length; while the stream equations are replaced by imperative programs. We developed a set of optimizations based on static analysis of the specification and have implemented an interpreter and a compiler for several target languages. In our evaluation, we measure the performance of several real-world case studies. The results show that using the StreamIR framework reduces the runtime significantly compared to the existing StreamIR interpreter. We evaluate the effect of the optimizations and show that significant performance gains are possible beyond the optimizations of the target language's compiler. While our current implementation is limited to RTLola, the StreamIR is designed to accommodate other stream-based languages, enabling their interpretation and compilation into all available target languages."
  },
  {
    "title": "Mapping the Human Brain from the Prenatal Period to Infancy Using 3D Magnetic Resonance Imaging",
    "url": "http://arxiv.org/abs/2504.21406v1",
    "arxiv_id": "2504.21406v1",
    "authors": [
      "Arnaud Cachia",
      "Jean-Fran\u00e7ois Mangin",
      "Jessica Dubois"
    ],
    "published": "2025-04-30T08:05:02+00:00",
    "summary": "Human brain development is a complex and dynamic process that begins during the first weeks of pregnancy and lasts until early adulthood. This chapter focuses on the developmental window from prenatal period to infancy, probably the most dynamic period across the entire lifespan. The availability of non-invasive three-dimensional Magnetic Resonance Imaging (MRI) methodologies has changed the paradigm and allows investigations of the living human brain structure - e.g. micro- and macrostructural features of cortical and subcortical regions and their connections, including cortical sulcation/gyrification, area, and thickness, as well as white matter microstructure and connectivity - beginning in utero. Because of its relative safety, MRI is well-adapted to study individuals at multiple time points and to longitudinally follow the changes in brain structure and function that underlie the early stages of cognitive development."
  },
  {
    "title": "Retrieval-Enhanced Few-Shot Prompting for Speech Event Extraction",
    "url": "http://arxiv.org/abs/2504.21372v1",
    "arxiv_id": "2504.21372v1",
    "authors": [
      "M\u00e1t\u00e9 Gedeon"
    ],
    "published": "2025-04-30T07:10:10+00:00",
    "summary": "Speech Event Extraction (SpeechEE) is a challenging task that lies at the intersection of Automatic Speech Recognition (ASR) and Natural Language Processing (NLP), requiring the identification of structured event information from spoken language. In this work, we present a modular, pipeline-based SpeechEE framework that integrates high-performance ASR with semantic search-enhanced prompting of Large Language Models (LLMs). Our system first classifies speech segments likely to contain events using a hybrid filtering mechanism including rule-based, BERT-based, and LLM-based models. It then employs few-shot LLM prompting, dynamically enriched via semantic similarity retrieval, to identify event triggers and extract corresponding arguments. We evaluate the pipeline using multiple LLMs (Llama3-8B, GPT-4o-mini, and o1-mini) highlighting significant performance gains with o1-mini, which achieves 63.3% F1 on trigger classification and 27.8% F1 on argument classification, outperforming prior benchmarks. Our results demonstrate that pipeline approaches, when empowered by retrieval-augmented LLMs, can rival or exceed end-to-end systems while maintaining interpretability and modularity. This work provides practical insights into LLM-driven event extraction and opens pathways for future hybrid models combining textual and acoustic features."
  },
  {
    "title": "ShorterBetter: Guiding Reasoning Models to Find Optimal Inference Length for Efficient Reasoning",
    "url": "http://arxiv.org/abs/2504.21370v1",
    "arxiv_id": "2504.21370v1",
    "authors": [
      "Jingyang Yi",
      "Jiazheng Wang"
    ],
    "published": "2025-04-30T07:04:19+00:00",
    "summary": "Reasoning models such as OpenAI o3 and DeepSeek-R1 have demonstrated strong performance on reasoning-intensive tasks through extended Chain-of-Thought (CoT) prompting. While longer reasoning traces can facilitate a more thorough exploration of solution paths for complex problems, researchers have observed that these models often \"overthink\", leading to inefficient inference. In this paper, we introduce ShorterBetter, a simple yet effective reinforcement learning methed that enables reasoning language models to discover their own optimal CoT lengths without human intervention. By sampling multiple outputs per problem and defining the Sample Optimal Length (SOL) as the shortest correct response among all the outputs, our method dynamically guides the model toward optimal inference lengths. Applied to the DeepSeek-Distill-Qwen-1.5B model, ShorterBetter achieves up to an 80% reduction in output length on both in-domain and out-of-domain reasoning tasks while maintaining accuracy. Our analysis shows that overly long reasoning traces often reflect loss of reasoning direction, and thus suggests that the extended CoT produced by reasoning models is highly compressible."
  },
  {
    "title": "Simple Visual Artifact Detection in Sora-Generated Videos",
    "url": "http://arxiv.org/abs/2504.21334v1",
    "arxiv_id": "2504.21334v1",
    "authors": [
      "Misora Sugiyama",
      "Hirokatsu Kataoka"
    ],
    "published": "2025-04-30T05:41:43+00:00",
    "summary": "The December 2024 release of OpenAI's Sora, a powerful video generation model driven by natural language prompts, highlights a growing convergence between large language models (LLMs) and video synthesis. As these multimodal systems evolve into video-enabled LLMs (VidLLMs), capable of interpreting, generating, and interacting with visual content, understanding their limitations and ensuring their safe deployment becomes essential. This study investigates visual artifacts frequently found and reported in Sora-generated videos, which can compromise quality, mislead viewers, or propagate disinformation. We propose a multi-label classification framework targeting four common artifact label types: label 1: boundary / edge defects, label 2: texture / noise issues, label 3: movement / joint anomalies, and label 4: object mismatches / disappearances. Using a dataset of 300 manually annotated frames extracted from 15 Sora-generated videos, we trained multiple 2D CNN architectures (ResNet-50, EfficientNet-B3 / B4, ViT-Base). The best-performing model trained by ResNet-50 achieved an average multi-label classification accuracy of 94.14%. This work supports the broader development of VidLLMs by contributing to (1) the creation of datasets for video quality evaluation, (2) interpretable artifact-based analysis beyond language metrics, and (3) the identification of visual risks relevant to factuality and safety."
  },
  {
    "title": "Phi-4-reasoning Technical Report",
    "url": "http://arxiv.org/abs/2504.21318v1",
    "arxiv_id": "2504.21318v1",
    "authors": [
      "Marah Abdin",
      "Sahaj Agarwal",
      "Ahmed Awadallah",
      "Vidhisha Balachandran",
      "Harkirat Behl",
      "Lingjiao Chen",
      "Gustavo de Rosa",
      "Suriya Gunasekar",
      "Mojan Javaheripi",
      "Neel Joshi",
      "Piero Kauffmann",
      "Yash Lara",
      "Caio C\u00e9sar Teodoro Mendes",
      "Arindam Mitra",
      "Besmira Nushi",
      "Dimitris Papailiopoulos",
      "Olli Saarikivi",
      "Shital Shah",
      "Vaishnavi Shrivastava",
      "Vibhav Vineet",
      "Yue Wu",
      "Safoora Yousefi",
      "Guoqing Zheng"
    ],
    "published": "2025-04-30T05:05:09+00:00",
    "summary": "We introduce Phi-4-reasoning, a 14-billion parameter reasoning model that achieves strong performance on complex reasoning tasks. Trained via supervised fine-tuning of Phi-4 on carefully curated set of \"teachable\" prompts-selected for the right level of complexity and diversity-and reasoning demonstrations generated using o3-mini, Phi-4-reasoning generates detailed reasoning chains that effectively leverage inference-time compute. We further develop Phi-4-reasoning-plus, a variant enhanced through a short phase of outcome-based reinforcement learning that offers higher performance by generating longer reasoning traces. Across a wide range of reasoning tasks, both models outperform significantly larger open-weight models such as DeepSeek-R1-Distill-Llama-70B model and approach the performance levels of full DeepSeek-R1 model. Our comprehensive evaluations span benchmarks in math and scientific reasoning, coding, algorithmic problem solving, planning, and spatial understanding. Interestingly, we observe a non-trivial transfer of improvements to general-purpose benchmarks as well. In this report, we provide insights into our training data, our training methodologies, and our evaluations. We show that the benefit of careful data curation for supervised fine-tuning (SFT) extends to reasoning language models, and can be further amplified by reinforcement learning (RL). Finally, our evaluation points to opportunities for improving how we assess the performance and robustness of reasoning models."
  },
  {
    "title": "Annotating and Auditing the Safety Properties of Unsafe Rust",
    "url": "http://arxiv.org/abs/2504.21312v1",
    "arxiv_id": "2504.21312v1",
    "authors": [
      "Zihao Rao",
      "Hongliang Tian",
      "Xin Wang",
      "Hui Xu"
    ],
    "published": "2025-04-30T04:53:35+00:00",
    "summary": "Unsafe code is a critical topic in ensuring the security of system software development in Rust. It is the sole source of potential undefined behaviors, assuming the compiler is sound. To avoid the misuse of unsafe code, Rust developers should provide clear safety property annotations for unsafe APIs. However, there is limited official guidance and few best practices for annotating unsafe code. Even the current best practices for safety property annotations in the Rust standard library are ad hoc and informal. In this paper, we design a domain-specific language to describe the safety properties of unsafe APIs, which may serve as a precursor for automated verification in the future. Furthermore, to ensure that the caller of an unsafe API properly delegates the safety property required by the callee, we propose a novel unsafety propagation graph to model the usage and propagation of unsafe code. Based on this graph, we further introduce a method to partition the graph into smaller graphs, such that each graph serves as a self-contained audit unit for examining the soundness of unsafe code encapsulation and safety property annotation. We applied our approach to the Rust standard library, and the experimental results demonstrate that our method is both practical and effective. Additionally, we have fixed safety property description issues in 23 APIs."
  },
  {
    "title": "The Dual Power of Interpretable Token Embeddings: Jailbreaking Attacks and Defenses for Diffusion Model Unlearning",
    "url": "http://arxiv.org/abs/2504.21307v1",
    "arxiv_id": "2504.21307v1",
    "authors": [
      "Siyi Chen",
      "Yimeng Zhang",
      "Sijia Liu",
      "Qing Qu"
    ],
    "published": "2025-04-30T04:33:43+00:00",
    "summary": "Despite the remarkable generalization capabilities of diffusion models, recent studies have shown that these models can memorize and generate harmful content when prompted with specific text instructions. Although fine-tuning approaches have been developed to mitigate this issue by unlearning harmful concepts, these methods can be easily circumvented through jailbreaking attacks. This indicates that the harmful concept has not been fully erased from the model. However, existing attack methods, while effective, lack interpretability regarding why unlearned models still retain the concept, thereby hindering the development of defense strategies. In this work, we address these limitations by proposing an attack method that learns an orthogonal set of interpretable attack token embeddings. The attack token embeddings can be decomposed into human-interpretable textual elements, revealing that unlearned models still retain the target concept through implicit textual components. Furthermore, these attack token embeddings are robust and transferable across text prompts, initial noises, and unlearned models. Finally, leveraging this diverse set of embeddings, we design a defense method applicable to both our proposed attack and existing attack methods. Experimental results demonstrate the effectiveness of both our attack and defense strategies."
  },
  {
    "title": "Assessing LLM code generation quality through path planning tasks",
    "url": "http://arxiv.org/abs/2504.21276v1",
    "arxiv_id": "2504.21276v1",
    "authors": [
      "Wanyi Chen",
      "Meng-Wen Su",
      "Mary L. Cummings"
    ],
    "published": "2025-04-30T03:11:54+00:00",
    "summary": "As LLM-generated code grows in popularity, more evaluation is needed to assess the risks of using such tools, especially for safety-critical applications such as path planning. Existing coding benchmarks are insufficient as they do not reflect the context and complexity of safety-critical applications. To this end, we assessed six LLMs' abilities to generate the code for three different path-planning algorithms and tested them on three maps of various difficulties. Our results suggest that LLM-generated code presents serious hazards for path planning applications and should not be applied in safety-critical contexts without rigorous testing."
  },
  {
    "title": "Phi-4-Mini-Reasoning: Exploring the Limits of Small Reasoning Language Models in Math",
    "url": "http://arxiv.org/abs/2504.21233v1",
    "arxiv_id": "2504.21233v1",
    "authors": [
      "Haoran Xu",
      "Baolin Peng",
      "Hany Awadalla",
      "Dongdong Chen",
      "Yen-Chun Chen",
      "Mei Gao",
      "Young Jin Kim",
      "Yunsheng Li",
      "Liliang Ren",
      "Yelong Shen",
      "Shuohang Wang",
      "Weijian Xu",
      "Jianfeng Gao",
      "Weizhu Chen"
    ],
    "published": "2025-04-30T00:04:35+00:00",
    "summary": "Chain-of-Thought (CoT) significantly enhances formal reasoning capabilities in Large Language Models (LLMs) by training them to explicitly generate intermediate reasoning steps. While LLMs readily benefit from such techniques, improving reasoning in Small Language Models (SLMs) remains challenging due to their limited model capacity. Recent work by Deepseek-R1 demonstrates that distillation from LLM-generated synthetic data can substantially improve the reasoning ability of SLM. However, the detailed modeling recipe is not disclosed. In this work, we present a systematic training recipe for SLMs that consists of four steps: (1) large-scale mid-training on diverse distilled long-CoT data, (2) supervised fine-tuning on high-quality long-CoT data, (3) Rollout DPO leveraging a carefully curated preference dataset, and (4) Reinforcement Learning (RL) with Verifiable Reward. We apply our method on Phi-4-Mini, a compact 3.8B-parameter model. The resulting Phi-4-Mini-Reasoning model exceeds, on math reasoning tasks, much larger reasoning models, e.g., outperforming DeepSeek-R1-Distill-Qwen-7B by 3.2 points and DeepSeek-R1-Distill-Llama-8B by 7.7 points on Math-500. Our results validate that a carefully designed training recipe, with large-scale high-quality CoT data, is effective to unlock strong reasoning capabilities even in resource-constrained small models."
  },
  {
    "title": "Automatic Legal Writing Evaluation of LLMs",
    "url": "http://arxiv.org/abs/2504.21202v1",
    "arxiv_id": "2504.21202v1",
    "authors": [
      "Ramon Pires",
      "Roseval Malaquias Junior",
      "Rodrigo Nogueira"
    ],
    "published": "2025-04-29T22:16:39+00:00",
    "summary": "Despite the recent advances in Large Language Models, benchmarks for evaluating legal writing remain scarce due to the inherent complexity of assessing open-ended responses in this domain. One of the key challenges in evaluating language models on domain-specific tasks is finding test datasets that are public, frequently updated, and contain comprehensive evaluation guidelines. The Brazilian Bar Examination meets these requirements. We introduce oab-bench, a benchmark comprising 105 questions across seven areas of law from recent editions of the exam. The benchmark includes comprehensive evaluation guidelines and reference materials used by human examiners to ensure consistent grading. We evaluate the performance of four LLMs on oab-bench, finding that Claude-3.5 Sonnet achieves the best results with an average score of 7.93 out of 10, passing all 21 exams. We also investigated whether LLMs can serve as reliable automated judges for evaluating legal writing. Our experiments show that frontier models like OpenAI's o1 achieve a strong correlation with human scores when evaluating approved exams, suggesting their potential as reliable automated evaluators despite the inherently subjective nature of legal writing assessment. The source code and the benchmark -- containing questions, evaluation guidelines, model-generated responses, and their respective automated evaluations -- are publicly available."
  },
  {
    "title": "Graph Synthetic Out-of-Distribution Exposure with Large Language Models",
    "url": "http://arxiv.org/abs/2504.21198v1",
    "arxiv_id": "2504.21198v1",
    "authors": [
      "Haoyan Xu",
      "Zhengtao Yao",
      "Ziyi Wang",
      "Zhan Cheng",
      "Xiyang Hu",
      "Mengyuan Li",
      "Yue Zhao"
    ],
    "published": "2025-04-29T22:04:30+00:00",
    "summary": "Out-of-distribution (OOD) detection in graphs is critical for ensuring model robustness in open-world and safety-sensitive applications. Existing approaches to graph OOD detection typically involve training an in-distribution (ID) classifier using only ID data, followed by the application of post-hoc OOD scoring techniques. Although OOD exposure - introducing auxiliary OOD samples during training - has proven to be an effective strategy for enhancing detection performance, current methods in the graph domain generally assume access to a set of real OOD nodes. This assumption, however, is often impractical due to the difficulty and cost of acquiring representative OOD samples. In this paper, we introduce GOE-LLM, a novel framework that leverages Large Language Models (LLMs) for OOD exposure in graph OOD detection without requiring real OOD nodes. GOE-LLM introduces two pipelines: (1) identifying pseudo-OOD nodes from the initially unlabeled graph using zero-shot LLM annotations, and (2) generating semantically informative synthetic OOD nodes via LLM-prompted text generation. These pseudo-OOD nodes are then used to regularize the training of the ID classifier for improved OOD awareness. We evaluate our approach across multiple benchmark datasets, showing that GOE-LLM significantly outperforms state-of-the-art graph OOD detection methods that do not use OOD exposure and achieves comparable performance to those relying on real OOD data."
  },
  {
    "title": "GLIP-OOD: Zero-Shot Graph OOD Detection with Foundation Model",
    "url": "http://arxiv.org/abs/2504.21186v1",
    "arxiv_id": "2504.21186v1",
    "authors": [
      "Haoyan Xu",
      "Zhengtao Yao",
      "Xuzhi Zhang",
      "Ziyi Wang",
      "Langzhou He",
      "Yushun Dong",
      "Philip S. Yu",
      "Mengyuan Li",
      "Yue Zhao"
    ],
    "published": "2025-04-29T21:42:54+00:00",
    "summary": "Out-of-distribution (OOD) detection is critical for ensuring the safety and reliability of machine learning systems, particularly in dynamic and open-world environments. In the vision and text domains, zero-shot OOD detection - which requires no training on in-distribution (ID) data - has made significant progress through the use of large-scale pretrained models such as vision-language models (VLMs) and large language models (LLMs). However, zero-shot OOD detection in graph-structured data remains largely unexplored, primarily due to the challenges posed by complex relational structures and the absence of powerful, large-scale pretrained models for graphs. In this work, we take the first step toward enabling zero-shot graph OOD detection by leveraging a graph foundation model (GFM). We show that, when provided only with class label names, the GFM can perform OOD detection without any node-level supervision - outperforming existing supervised methods across multiple datasets. To address the more practical setting where OOD label names are unavailable, we introduce GLIP-OOD, a novel framework that employs LLMs to generate semantically informative pseudo-OOD labels from unlabeled data. These labels enable the GFM to capture nuanced semantic boundaries between ID and OOD classes and perform fine-grained OOD detection - without requiring any labeled nodes. Our approach is the first to enable node-level graph OOD detection in a fully zero-shot setting, and achieves state-of-the-art performance on four benchmark text-attributed graph datasets."
  },
  {
    "title": "Composite Safety Potential Field for Highway Driving Risk Assessment",
    "url": "http://arxiv.org/abs/2504.21158v1",
    "arxiv_id": "2504.21158v1",
    "authors": [
      "Dachuan Zuo",
      "Zilin Bian",
      "Fan Zuo",
      "Kaan Ozbay"
    ],
    "published": "2025-04-29T20:19:47+00:00",
    "summary": "In the era of rapid advancements in vehicle safety technologies, driving risk assessment has become a focal point of attention. Technologies such as collision warning systems, advanced driver assistance systems (ADAS), and autonomous driving require driving risks to be evaluated proactively and in real time. To be effective, driving risk assessment metrics must not only accurately identify potential collisions but also exhibit human-like reasoning to enable safe and seamless interactions between vehicles. Existing safety potential field models assess driving risks by considering both objective and subjective safety factors. However, their practical applicability in real-world risk assessment tasks is limited. These models are often challenging to calibrate due to the arbitrary nature of their structures, and calibration can be inefficient because of the scarcity of accident statistics. Additionally, they struggle to generalize across both longitudinal and lateral risks. To address these challenges, we propose a composite safety potential field framework, namely C-SPF, involving a subjective field to capture drivers' risk perception about spatial proximity and an objective field to quantify the imminent collision probability, to comprehensively evaluate driving risks. The C-SPF is calibrated using abundant two-dimensional spacing data from trajectory datasets, enabling it to effectively capture drivers' proximity risk perception and provide a more realistic explanation of driving behaviors. Analysis of a naturalistic driving dataset demonstrates that the C-SPF can capture both longitudinal and lateral risks that trigger drivers' safety maneuvers. Further case studies highlight the C-SPF's ability to explain lateral driver behaviors, such as abandoning lane changes or adjusting lateral position relative to adjacent vehicles, which are capabilities that existing models fail to achieve."
  },
  {
    "title": "Relaxed Choices in Bottom-Up Asynchronous Multiparty Session Types",
    "url": "http://arxiv.org/abs/2504.21108v1",
    "arxiv_id": "2504.21108v1",
    "authors": [
      "Ivan Proki\u0107",
      "Simona Proki\u0107",
      "Silvia Ghilezan",
      "Alceste Scalas",
      "Nobuko Yoshida"
    ],
    "published": "2025-04-29T18:37:23+00:00",
    "summary": "Asynchronous multiparty session types provide a formal model for expressing the behaviour of communicating processes and verifying that they correctly implement desired protocols. In the ``bottom-up'' approach to session typing, local session types are specified directly, and the properties of their composition (e.g. deadlock freedom and liveness) are checked and transferred to well-typed processes. This method allows expressing and verifying a broad range of protocols, but still has a key limitation: it only supports protocols where every send/receive operation is directed towards strictly one recipient/sender at a time. This makes the technique too restrictive for modelling some classes of protocols, e.g. those used in the field of federated learning.   This paper improves the session typing theory by extending the asynchronous ``bottom-up'' approach to support protocols where a participant can choose to send or receive messages to/from multiple other participants at the same time, rather than just one at a time. We demonstrate how this extension enables the modeling and verification of real-world protocols, including some used in federated learning. Furthermore, we introduce and formally prove safety, deadlock-freedom, liveness, and session fidelity properties for our session typing system, revealing interesting dependencies between these properties in the presence of a subtyping relation."
  },
  {
    "title": "HyPerAlign: Hypotheses-driven Personalized Alignment",
    "url": "http://arxiv.org/abs/2505.00038v1",
    "arxiv_id": "2505.00038v1",
    "authors": [
      "Cristina Garbacea",
      "Chenhao Tan"
    ],
    "published": "2025-04-29T18:01:46+00:00",
    "summary": "Alignment algorithms are widely used to align large language models (LLMs) to human users based on preference annotations that reflect their intended real-world use cases. Typically these (often divergent) preferences are aggregated over a diverse set of users, resulting in fine-tuned models that are aligned to the ``average-user'' preference. Nevertheless, current models are used by individual users in very specific contexts and situations, emphasizing the need for user-dependent preference control. In this work we address the problem of personalizing LLM outputs to their users, aiming to generate customized responses tailored to individual users, instead of generic outputs that emulate the collective voices of diverse populations. We propose a novel interpretable and sample-efficient hypotheses-driven personalization approach (HyPerAlign) where given few-shot examples written by a particular user, we first infer hypotheses about their communication strategies, personality and writing style, then prompt LLM models with these hypotheses and user specific attributes to generate customized outputs. We conduct experiments on two different personalization tasks, authorship attribution and deliberative alignment, with datasets from diverse domains (news articles, blog posts, emails, jailbreaking benchmarks), and demonstrate the superiority of hypotheses-driven personalization approach when compared to preference-based fine-tuning methods. For deliberative alignment, the helpfulness of LLM models is improved by up to $70\\%$ on average. For authorship attribution, results indicate consistently high win-rates (commonly $>90\\%$) against state-of-the-art preference fine-tuning approaches for LLM personalization across diverse user profiles and LLM models. Overall, our approach represents an interpretable and sample-efficient strategy for the personalization of LLM models to individual users."
  },
  {
    "title": "AegisLLM: Scaling Agentic Systems for Self-Reflective Defense in LLM Security",
    "url": "http://arxiv.org/abs/2504.20965v1",
    "arxiv_id": "2504.20965v1",
    "authors": [
      "Zikui Cai",
      "Shayan Shabihi",
      "Bang An",
      "Zora Che",
      "Brian R. Bartoldson",
      "Bhavya Kailkhura",
      "Tom Goldstein",
      "Furong Huang"
    ],
    "published": "2025-04-29T17:36:05+00:00",
    "summary": "We introduce AegisLLM, a cooperative multi-agent defense against adversarial attacks and information leakage. In AegisLLM, a structured workflow of autonomous agents - orchestrator, deflector, responder, and evaluator - collaborate to ensure safe and compliant LLM outputs, while self-improving over time through prompt optimization. We show that scaling agentic reasoning system at test-time - both by incorporating additional agent roles and by leveraging automated prompt optimization (such as DSPy)- substantially enhances robustness without compromising model utility. This test-time defense enables real-time adaptability to evolving attacks, without requiring model retraining. Comprehensive evaluations across key threat scenarios, including unlearning and jailbreaking, demonstrate the effectiveness of AegisLLM. On the WMDP unlearning benchmark, AegisLLM achieves near-perfect unlearning with only 20 training examples and fewer than 300 LM calls. For jailbreaking benchmarks, we achieve 51% improvement compared to the base model on StrongReject, with false refusal rates of only 7.9% on PHTest compared to 18-55% for comparable methods. Our results highlight the advantages of adaptive, agentic reasoning over static defenses, establishing AegisLLM as a strong runtime alternative to traditional approaches based on model modifications. Code is available at https://github.com/zikuicai/aegisllm"
  },
  {
    "title": "A Domain-Agnostic Scalable AI Safety Ensuring Framework",
    "url": "http://arxiv.org/abs/2504.20924v1",
    "arxiv_id": "2504.20924v1",
    "authors": [
      "Beomjun Kim",
      "Kangyeon Kim",
      "Sunwoo Kim",
      "Heejin Ahn"
    ],
    "published": "2025-04-29T16:38:35+00:00",
    "summary": "Ensuring the safety of AI systems has recently emerged as a critical priority for real-world deployment, particularly in physical AI applications. Current approaches to AI safety typically address predefined domain-specific safety conditions, limiting their ability to generalize across contexts.   We propose a novel AI safety framework that ensures AI systems comply with \\textbf{any user-defined constraint}, with \\textbf{any desired probability}, and across \\textbf{various domains}.   In this framework, we combine an AI component (e.g., neural network) with an optimization problem to produce responses that minimize objectives while satisfying user-defined constraints with probabilities exceeding user-defined thresholds. For credibility assessment of the AI component, we propose \\textit{internal test data}, a supplementary set of safety-labeled data, and a \\textit{conservative testing} methodology that provides statistical validity of using internal test data. We also present an approximation method of a loss function and how to compute its gradient for training.   We mathematically prove that probabilistic constraint satisfaction is guaranteed under specific, mild conditions and prove a scaling law between safety and the number of internal test data. We demonstrate our framework's effectiveness through experiments in diverse domains: demand prediction for production decision, safe reinforcement learning within the SafetyGym simulator, and guarding AI chatbot outputs. Through these experiments, we demonstrate that our method guarantees safety for user-specified constraints, outperforms {for \\textbf{up to several order of magnitudes}} existing methods in low safety threshold regions, and scales effectively with respect to the size of internal test data."
  },
  {
    "title": "A Domain-Agnostic Scalable AI Safety Ensuring Framework",
    "url": "http://arxiv.org/abs/2504.20924v2",
    "arxiv_id": "2504.20924v2",
    "authors": [
      "Beomjun Kim",
      "Kangyeon Kim",
      "Sunwoo Kim",
      "Heejin Ahn"
    ],
    "published": "2025-04-29T16:38:35+00:00",
    "summary": "Ensuring the safety of AI systems has recently emerged as a critical priority for real-world deployment, particularly in physical AI applications. Current approaches to AI safety typically address predefined domain-specific safety conditions, limiting their ability to generalize across contexts. We propose a novel AI safety framework that ensures AI systems comply with any user-defined constraint, with any desired probability, and across various domains. In this framework, we combine an AI component (e.g., neural network) with an optimization problem to produce responses that minimize objectives while satisfying user-defined constraints with probabilities exceeding user-defined thresholds. For credibility assessment of the AI component, we propose internal test data, a supplementary set of safety-labeled data, and a conservative testing methodology that provides statistical validity of using internal test data. We also present an approximation method of a loss function and how to compute its gradient for training. We mathematically prove that probabilistic constraint satisfaction is guaranteed under specific, mild conditions and prove a scaling law between safety and the number of internal test data. We demonstrate our framework's effectiveness through experiments in diverse domains: demand prediction for production decision, safe reinforcement learning within the SafetyGym simulator, and guarding AI chatbot outputs. Through these experiments, we demonstrate that our method guarantees safety for user-specified constraints, outperforms for up to several order of magnitudes existing methods in low safety threshold regions, and scales effectively with respect to the size of internal test data."
  },
  {
    "title": "When Testing AI Tests Us: Safeguarding Mental Health on the Digital Frontlines",
    "url": "http://arxiv.org/abs/2504.20910v1",
    "arxiv_id": "2504.20910v1",
    "authors": [
      "Sachin R. Pendse",
      "Darren Gergle",
      "Rachel Kornfield",
      "Jonah Meyerhoff",
      "David Mohr",
      "Jina Suh",
      "Annie Wescott",
      "Casey Williams",
      "Jessica Schleider"
    ],
    "published": "2025-04-29T16:27:20+00:00",
    "summary": "Red-teaming is a core part of the infrastructure that ensures that AI models do not produce harmful content. Unlike past technologies, the black box nature of generative AI systems necessitates a uniquely interactional mode of testing, one in which individuals on red teams actively interact with the system, leveraging natural language to simulate malicious actors and solicit harmful outputs. This interactional labor done by red teams can result in mental health harms that are uniquely tied to the adversarial engagement strategies necessary to effectively red team. The importance of ensuring that generative AI models do not propagate societal or individual harm is widely recognized -- one less visible foundation of end-to-end AI safety is also the protection of the mental health and wellbeing of those who work to keep model outputs safe. In this paper, we argue that the unmet mental health needs of AI red-teamers is a critical workplace safety concern. Through analyzing the unique mental health impacts associated with the labor done by red teams, we propose potential individual and organizational strategies that could be used to meet these needs, and safeguard the mental health of red-teamers. We develop our proposed strategies through drawing parallels between common red-teaming practices and interactional labor common to other professions (including actors, mental health professionals, conflict photographers, and content moderators), describing how individuals and organizations within these professional spaces safeguard their mental health given similar psychological demands. Drawing on these protective practices, we describe how safeguards could be adapted for the distinct mental health challenges experienced by red teaming organizations as they mitigate emerging technological risks on the new digital frontlines."
  },
  {
    "title": "GiBy: A Giant-Step Baby-Step Classifier For Anomaly Detection In Industrial Control Systems",
    "url": "http://arxiv.org/abs/2504.20906v1",
    "arxiv_id": "2504.20906v1",
    "authors": [
      "Sarad Venugopalan",
      "Sridhar Adepu"
    ],
    "published": "2025-04-29T16:24:11+00:00",
    "summary": "The continuous monitoring of the interactions between cyber-physical components of any industrial control system (ICS) is required to secure automation of the system controls, and to guarantee plant processes are fail-safe and remain in an acceptably safe state. Safety is achieved by managing actuation (where electric signals are used to trigger physical movement), dependent on corresponding sensor readings; used as ground truth in decision making. Timely detection of anomalies (attacks, faults and unascertained states) in ICSs is crucial for the safe running of a plant, the safety of its personnel, and for the safe provision of any services provided. We propose an anomaly detection method that involves accurate linearization of the non-linear forms arising from sensor-actuator(s) relationships, primarily because solving linear models is easier and well understood. Further, the time complexity of the anomaly detection scenario/problem at hand is lowered using dimensionality reduction of the actuator(s) in relationship with a sensor. We accomplish this by using a well-known water treatment testbed as a use case. Our experiments show millisecond time response to detect anomalies and provide explainability; that are not simultaneously achieved by other state of the art AI/ML models with eXplainable AI (XAI) used for the same purpose. Further, we pin-point the sensor(s) and its actuation state for which anomaly was detected."
  },
  {
    "title": "GaussTrap: Stealthy Poisoning Attacks on 3D Gaussian Splatting for Targeted Scene Confusion",
    "url": "http://arxiv.org/abs/2504.20829v1",
    "arxiv_id": "2504.20829v1",
    "authors": [
      "Jiaxin Hong",
      "Sixu Chen",
      "Shuoyang Sun",
      "Hongyao Yu",
      "Hao Fang",
      "Yuqi Tan",
      "Bin Chen",
      "Shuhan Qi",
      "Jiawei Li"
    ],
    "published": "2025-04-29T14:52:14+00:00",
    "summary": "As 3D Gaussian Splatting (3DGS) emerges as a breakthrough in scene representation and novel view synthesis, its rapid adoption in safety-critical domains (e.g., autonomous systems, AR/VR) urgently demands scrutiny of potential security vulnerabilities. This paper presents the first systematic study of backdoor threats in 3DGS pipelines. We identify that adversaries may implant backdoor views to induce malicious scene confusion during inference, potentially leading to environmental misperception in autonomous navigation or spatial distortion in immersive environments. To uncover this risk, we propose GuassTrap, a novel poisoning attack method targeting 3DGS models. GuassTrap injects malicious views at specific attack viewpoints while preserving high-quality rendering in non-target views, ensuring minimal detectability and maximizing potential harm. Specifically, the proposed method consists of a three-stage pipeline (attack, stabilization, and normal training) to implant stealthy, viewpoint-consistent poisoned renderings in 3DGS, jointly optimizing attack efficacy and perceptual realism to expose security risks in 3D rendering. Extensive experiments on both synthetic and real-world datasets demonstrate that GuassTrap can effectively embed imperceptible yet harmful backdoor views while maintaining high-quality rendering in normal views, validating its robustness, adaptability, and practical applicability."
  },
  {
    "title": "DP-SMOTE: Integrating Differential Privacy and Oversampling Technique to Preserve Privacy in Smart Homes",
    "url": "http://arxiv.org/abs/2504.20827v1",
    "arxiv_id": "2504.20827v1",
    "authors": [
      "Amr Tarek Elsayed",
      "Almohammady Sobhi Alsharkawy",
      "Mohamed Sayed Farag",
      "Shaban Ebrahim Abu Yusuf"
    ],
    "published": "2025-04-29T14:50:50+00:00",
    "summary": "Smart homes represent intelligent environments where interconnected devices gather information, enhancing users living experiences by ensuring comfort, safety, and efficient energy management. To enhance the quality of life, companies in the smart device industry collect user data, including activities, preferences, and power consumption. However, sharing such data necessitates privacy-preserving practices. This paper introduces a robust method for secure sharing of data to service providers, grounded in differential privacy (DP). This empowers smart home residents to contribute usage statistics while safeguarding their privacy. The approach incorporates the Synthetic Minority Oversampling technique (SMOTe) and seamlessly integrates Gaussian noise to generate synthetic data, enabling data and statistics sharing while preserving individual privacy. The proposed method employs the SMOTe algorithm and applies Gaussian noise to generate data. Subsequently, it employs a k-anonymity function to assess reidentification risk before sharing the data. The simulation outcomes demonstrate that our method delivers strong performance in safeguarding privacy and in accuracy, recall, and f-measure metrics. This approach is particularly effective in smart homes, offering substantial utility in privacy at a reidentification risk of 30%, with Gaussian noise set to 0.3, SMOTe at 500%, and the application of a k-anonymity function with k = 2. Additionally, it shows a high classification accuracy, ranging from 90% to 98%, across various classification techniques."
  },
  {
    "title": "In defence of post-hoc explanations in medical AI",
    "url": "http://arxiv.org/abs/2504.20741v1",
    "arxiv_id": "2504.20741v1",
    "authors": [
      "Joshua Hatherley",
      "Lauritz Munch",
      "Jens Christian Bjerring"
    ],
    "published": "2025-04-29T13:24:21+00:00",
    "summary": "Since the early days of the Explainable AI movement, post-hoc explanations have been praised for their potential to improve user understanding, promote trust, and reduce patient safety risks in black box medical AI systems. Recently, however, critics have argued that the benefits of post-hoc explanations are greatly exaggerated since they merely approximate, rather than replicate, the actual reasoning processes that black box systems take to arrive at their outputs. In this article, we aim to defend the value of post-hoc explanations against this recent critique. We argue that even if post-hoc explanations do not replicate the exact reasoning processes of black box systems, they can still improve users' functional understanding of black box systems, increase the accuracy of clinician-AI teams, and assist clinicians in justifying their AI-informed decisions. While post-hoc explanations are not a \"silver bullet\" solution to the black box problem in medical AI, we conclude that they remain a useful strategy for addressing the black box problem in medical AI."
  },
  {
    "title": "Seeking Specifications: The Case for Neuro-Symbolic Specification Synthesis",
    "url": "http://arxiv.org/abs/2504.21061v1",
    "arxiv_id": "2504.21061v1",
    "authors": [
      "George Granberry",
      "Wolfgang Ahrendt",
      "Moa Johansson"
    ],
    "published": "2025-04-29T10:02:29+00:00",
    "summary": "This work is concerned with the generation of formal specifications from code, using Large Language Models (LLMs) in combination with symbolic methods. Concretely, in our study, the programming language is C, the specification language is ACSL, and the LLM is Deepseek-R1. In this context, we address two research directions, namely the specification of intent vs. implementation on the one hand, and the combination of symbolic analyses with LLMs on the other hand. For the first, we investigate how the absence or presence of bugs in the code impacts the generated specifications, as well as whether and how a user can direct the LLM to specify intent or implementation, respectively. For the second, we investigate the impact of results from symbolic analyses on the specifications generated by the LLM. The LLM prompts are augmented with outputs from two formal methods tools in the Frama-C ecosystem, Pathcrawler and EVA. We demonstrate how the addition of symbolic analysis to the workflow impacts the quality of annotations."
  },
  {
    "title": "Reinforcement Learning for Reasoning in Large Language Models with One Training Example",
    "url": "http://arxiv.org/abs/2504.20571v1",
    "arxiv_id": "2504.20571v1",
    "authors": [
      "Yiping Wang",
      "Qing Yang",
      "Zhiyuan Zeng",
      "Liliang Ren",
      "Lucas Liu",
      "Baolin Peng",
      "Hao Cheng",
      "Xuehai He",
      "Kuan Wang",
      "Jianfeng Gao",
      "Weizhu Chen",
      "Shuohang Wang",
      "Simon Shaolei Du",
      "Yelong Shen"
    ],
    "published": "2025-04-29T09:24:30+00:00",
    "summary": "We show that reinforcement learning with verifiable reward using one training example (1-shot RLVR) is effective in incentivizing the math reasoning capabilities of large language models (LLMs). Applying RLVR to the base model Qwen2.5-Math-1.5B, we identify a single example that elevates model performance on MATH500 from 36.0% to 73.6%, and improves the average performance across six common mathematical reasoning benchmarks from 17.6% to 35.7%. This result matches the performance obtained using the 1.2k DeepScaleR subset (MATH500: 73.6%, average: 35.9%), which includes the aforementioned example. Similar substantial improvements are observed across various models (Qwen2.5-Math-7B, Llama3.2-3B-Instruct, DeepSeek-R1-Distill-Qwen-1.5B), RL algorithms (GRPO and PPO), and different math examples (many of which yield approximately 30% or greater improvement on MATH500 when employed as a single training example). In addition, we identify some interesting phenomena during 1-shot RLVR, including cross-domain generalization, increased frequency of self-reflection, and sustained test performance improvement even after the training accuracy has saturated, a phenomenon we term post-saturation generalization. Moreover, we verify that the effectiveness of 1-shot RLVR primarily arises from the policy gradient loss, distinguishing it from the \"grokking\" phenomenon. We also show the critical role of promoting exploration (e.g., by adding entropy loss with an appropriate coefficient) in 1-shot RLVR training. As a bonus, we observe that applying entropy loss alone, without any outcome reward, significantly enhances Qwen2.5-Math-1.5B's performance on MATH500 by 27.4%. These findings can inspire future work on RLVR data efficiency and encourage a re-examination of both recent progress and the underlying mechanisms in RLVR. Our code, model, and data are open source at https://github.com/ypwang61/One-Shot-RLVR"
  },
  {
    "title": "Safe Bottom-Up Flexibility Provision from Distributed Energy Resources",
    "url": "http://arxiv.org/abs/2504.20529v1",
    "arxiv_id": "2504.20529v1",
    "authors": [
      "Costas Mylonas",
      "Emmanouel Varvarigos",
      "Georgios Tsaousoglou"
    ],
    "published": "2025-04-29T08:16:15+00:00",
    "summary": "Modern renewables-based power systems need to tap on the flexibility of Distributed Energy Resources (DERs) connected to distribution networks. It is important, however, that DER owners/users remain in control of their assets, decisions, and objectives. At the same time, the dynamic landscape of DER-penetrated distribution networks calls for agile, data-driven flexibility management frameworks. In the face of these developments, the Multi-Agent Reinforcement Learning (MARL) paradigm is gaining significant attention, as a distributed and data-driven decision-making policy. This paper addresses the need for bottom-up DER management decisions to account for the distribution network's safety-related constraints. While the related literature on safe MARL typically assumes that network characteristics are available and incorporated into the policy's safety layer, which implies active DSO engagement, this paper ensures that self-organized DER communities are enabled to provide distribution-network-safe flexibility services without relying on the aspirational and problematic requirement of bringing the DSO in the decision-making loop."
  },
  {
    "title": "Token-Efficient Prompt Injection Attack: Provoking Cessation in LLM Reasoning via Adaptive Token Compression",
    "url": "http://arxiv.org/abs/2504.20493v1",
    "arxiv_id": "2504.20493v1",
    "authors": [
      "Yu Cui",
      "Yujun Cai",
      "Yiwei Wang"
    ],
    "published": "2025-04-29T07:34:22+00:00",
    "summary": "While reasoning large language models (LLMs) demonstrate remarkable performance across various tasks, they also contain notable security vulnerabilities. Recent research has uncovered a \"thinking-stopped\" vulnerability in DeepSeek-R1, where model-generated reasoning tokens can forcibly interrupt the inference process, resulting in empty responses that compromise LLM-integrated applications. However, existing methods triggering this vulnerability require complex mathematical word problems with long prompts--even exceeding 5,000 tokens. To reduce the token cost and formally define this vulnerability, we propose a novel prompt injection attack named \"Reasoning Interruption Attack\", based on adaptive token compression. We demonstrate that simple standalone arithmetic tasks can effectively trigger this vulnerability, and the prompts based on such tasks exhibit simpler logical structures than mathematical word problems. We develop a systematic approach to efficiently collect attack prompts and an adaptive token compression framework that utilizes LLMs to automatically compress these prompts. Experiments show our compression framework significantly reduces prompt length while maintaining effective attack capabilities. We further investigate the attack's performance via output prefix and analyze the underlying causes of the vulnerability, providing valuable insights for improving security in reasoning LLMs."
  },
  {
    "title": "NeuRel-Attack: Neuron Relearning for Safety Disalignment in Large Language Models",
    "url": "http://arxiv.org/abs/2504.21053v1",
    "arxiv_id": "2504.21053v1",
    "authors": [
      "Yi Zhou",
      "Wenpeng Xing",
      "Dezhang Kong",
      "Changting Lin",
      "Meng Han"
    ],
    "published": "2025-04-29T05:49:35+00:00",
    "summary": "Safety alignment in large language models (LLMs) is achieved through fine-tuning mechanisms that regulate neuron activations to suppress harmful content. In this work, we propose a novel approach to induce disalignment by identifying and modifying the neurons responsible for safety constraints. Our method consists of three key steps: Neuron Activation Analysis, where we examine activation patterns in response to harmful and harmless prompts to detect neurons that are critical for distinguishing between harmful and harmless inputs; Similarity-Based Neuron Identification, which systematically locates the neurons responsible for safe alignment; and Neuron Relearning for Safety Removal, where we fine-tune these selected neurons to restore the model's ability to generate previously restricted responses. Experimental results demonstrate that our method effectively removes safety constraints with minimal fine-tuning, highlighting a critical vulnerability in current alignment techniques. Our findings underscore the need for robust defenses against adversarial fine-tuning attacks on LLMs."
  },
  {
    "title": "An Algebraic Approach to Asymmetric Delegation and Polymorphic Label Inference (Technical Report)",
    "url": "http://arxiv.org/abs/2504.20432v1",
    "arxiv_id": "2504.20432v1",
    "authors": [
      "Silei Ren",
      "Co\u015fku Acay",
      "Andrew C. Myers"
    ],
    "published": "2025-04-29T05:00:17+00:00",
    "summary": "Language-based information flow control (IFC) enables reasoning about and enforcing security policies in decentralized applications. While information flow properties are relatively extensional and compositional, designing expressive systems that enforce such properties remains challenging. In particular, it can be difficult to use IFC labels to model certain security assumptions, such as semi-honest agents.   Motivated by these modeling limitations, we study the algebraic semantics of lattice-based IFC label models, and propose a semantic framework that allows formalizing asymmetric delegation, which is partial delegation of confidentiality or integrity. Our framework supports downgrading of information and ensures their safety through nonmalleable information flow (NMIF).   To demonstrate the practicality of our framework, we design and implement a novel algorithm that statically checks NMIF and a label inference procedure that efficiently supports bounded label polymorphism, allowing users to write code generic with respect to labels."
  },
  {
    "title": "Safe and Optimal N-Spacecraft Swarm Reconfiguration in Non-Keplerian Cislunar Orbits",
    "url": "http://arxiv.org/abs/2504.20386v1",
    "arxiv_id": "2504.20386v1",
    "authors": [
      "Yuji Takubo",
      "Walter Manuel",
      "Ethan Foss",
      "Simone D'Amico"
    ],
    "published": "2025-04-29T03:13:28+00:00",
    "summary": "This paper presents a novel fuel-optimal guidance and control methodology for spacecraft swarm reconfiguration in Restricted Multi-Body Problems (RMBPs) with a guarantee of passive safety, maintaining miss distance even under abrupt loss of control authority. A new set of constraints exploits a quasi-periodic structure of RMBPs to guarantee passive safety. Particularly, this can be expressed as simple geometric constraints by solving optimal control in Local Toroidal Coordinates, which is based on a local eigenspace of a quasi-periodic motion around the corresponding periodic orbit. The proposed formulation enables a significant simplification of problem structure, which is highly applicable to large-scale swarm reconfiguration in cislunar orbits. The method is demonstrated in various models of RMBPs (Elliptical Restricted Three-Body Problem and Bi-Circular Restricted Four-Body Problem) and also validated in the full-ephemeris dynamics. By extending and generalizing well-known concepts from the two- to the three- and four-body problems, this paper lays the foundation for the practical control schemes of relative motion in cislunar space."
  },
  {
    "title": "Inception: Jailbreak the Memory Mechanism of Text-to-Image Generation Systems",
    "url": "http://arxiv.org/abs/2504.20376v1",
    "arxiv_id": "2504.20376v1",
    "authors": [
      "Shiqian Zhao",
      "Jiayang Liu",
      "Yiming Li",
      "Runyi Hu",
      "Xiaojun Jia",
      "Wenshu Fan",
      "Xinfeng Li",
      "Jie Zhang",
      "Wei Dong",
      "Tianwei Zhang",
      "Luu Anh Tuan"
    ],
    "published": "2025-04-29T02:40:36+00:00",
    "summary": "Currently, the memory mechanism has been widely and successfully exploited in online text-to-image (T2I) generation systems ($e.g.$, DALL$\\cdot$E 3) for alleviating the growing tokenization burden and capturing key information in multi-turn interactions. Despite its practicality, its security analyses have fallen far behind. In this paper, we reveal that this mechanism exacerbates the risk of jailbreak attacks. Different from previous attacks that fuse the unsafe target prompt into one ultimate adversarial prompt, which can be easily detected or may generate non-unsafe images due to under- or over-optimization, we propose Inception, the first multi-turn jailbreak attack against the memory mechanism in real-world text-to-image generation systems. Inception embeds the malice at the inception of the chat session turn by turn, leveraging the mechanism that T2I generation systems retrieve key information in their memory. Specifically, Inception mainly consists of two modules. It first segments the unsafe prompt into chunks, which are subsequently fed to the system in multiple turns, serving as pseudo-gradients for directive optimization. Specifically, we develop a series of segmentation policies that ensure the images generated are semantically consistent with the target prompt. Secondly, after segmentation, to overcome the challenge of the inseparability of minimum unsafe words, we propose recursion, a strategy that makes minimum unsafe words subdivisible. Collectively, segmentation and recursion ensure that all the request prompts are benign but can lead to malicious outcomes. We conduct experiments on the real-world text-to-image generation system ($i.e.$, DALL$\\cdot$E 3) to validate the effectiveness of Inception. The results indicate that Inception surpasses the state-of-the-art by a 14\\% margin in attack success rate."
  },
  {
    "title": "Online Safety for All: Sociocultural Insights from a Systematic Review of Youth Online Safety in the Global South",
    "url": "http://arxiv.org/abs/2504.20308v1",
    "arxiv_id": "2504.20308v1",
    "authors": [
      "Ozioma C. Oguine",
      "Oghenemaro Anuyah",
      "Zainab Agha",
      "Iris Melgarez",
      "Adriana Alvarado Garcia",
      "Karla Badillo-Urquiola"
    ],
    "published": "2025-04-28T23:36:57+00:00",
    "summary": "Youth online safety research in HCI has historically centered on perspectives from the Global North, often overlooking the unique particularities and cultural contexts of regions in the Global South. This paper presents a systematic review of 66 youth online safety studies published between 2014 and 2024, specifically focusing on regions in the Global South. Our findings reveal a concentrated research focus in Asian countries and predominance of quantitative methods. We also found limited research on marginalized youth populations and a primary focus on risks related to cyberbullying. Our analysis underscores the critical role of cultural factors in shaping online safety, highlighting the need for educational approaches that integrate social dynamics and awareness. We propose methodological recommendations and a future research agenda that encourages the adoption of situated, culturally sensitive methodologies and youth-centered approaches to researching youth online safety regions in the Global South. This paper advocates for greater inclusivity in youth online safety research, emphasizing the importance of addressing varied sociocultural contexts to better understand and meet the online safety needs of youth in the Global South."
  },
  {
    "title": "Improving trajectory continuity in drone-based crowd monitoring using a set of minimal-cost techniques and deep discriminative correlation filters",
    "url": "http://arxiv.org/abs/2504.20234v1",
    "arxiv_id": "2504.20234v1",
    "authors": [
      "Bartosz Ptak",
      "Marek Kraft"
    ],
    "published": "2025-04-28T20:07:42+00:00",
    "summary": "Drone-based crowd monitoring is the key technology for applications in surveillance, public safety, and event management. However, maintaining tracking continuity and consistency remains a significant challenge. Traditional detection-assignment tracking methods struggle with false positives, false negatives, and frequent identity switches, leading to degraded counting accuracy and making in-depth analysis impossible. This paper introduces a point-oriented online tracking algorithm that improves trajectory continuity and counting reliability in drone-based crowd monitoring. Our method builds on the Simple Online and Real-time Tracking (SORT) framework, replacing the original bounding-box assignment with a point-distance metric. The algorithm is enhanced with three cost-effective techniques: camera motion compensation, altitude-aware assignment, and classification-based trajectory validation. Further, Deep Discriminative Correlation Filters (DDCF) that re-use spatial feature maps from localisation algorithms for increased computational efficiency through neural network resource sharing are integrated to refine object tracking by reducing noise and handling missed detections. The proposed method is evaluated on the DroneCrowd and newly shared UP-COUNT-TRACK datasets, demonstrating substantial improvements in tracking metrics, reducing counting errors to 23% and 15%, respectively. The results also indicate a significant reduction of identity switches while maintaining high tracking accuracy, outperforming baseline online trackers and even an offline greedy optimisation method."
  },
  {
    "title": "Representation Learning on a Random Lattice",
    "url": "http://arxiv.org/abs/2504.20197v1",
    "arxiv_id": "2504.20197v1",
    "authors": [
      "Aryeh Brill"
    ],
    "published": "2025-04-28T19:01:36+00:00",
    "summary": "Decomposing a deep neural network's learned representations into interpretable features could greatly enhance its safety and reliability. To better understand features, we adopt a geometric perspective, viewing them as a learned coordinate system for mapping an embedded data distribution. We motivate a model of a generic data distribution as a random lattice and analyze its properties using percolation theory. Learned features are categorized into context, component, and surface features. The model is qualitatively consistent with recent findings in mechanistic interpretability and suggests directions for future research."
  },
  {
    "title": "Cybersecurity for Autonomous Vehicles",
    "url": "http://arxiv.org/abs/2504.20180v1",
    "arxiv_id": "2504.20180v1",
    "authors": [
      "Sai varun reddy Bhemavarapu"
    ],
    "published": "2025-04-28T18:31:37+00:00",
    "summary": "The increasing adoption of autonomous vehicles is bringing a major shift in the automotive industry. However, as these vehicles become more connected, cybersecurity threats have emerged as a serious concern. Protecting the security and integrity of autonomous systems is essential to prevent malicious activities that can harm passengers, other road users, and the overall transportation network. This paper focuses on addressing the cybersecurity issues in autonomous vehicles by examining the challenges and risks involved, which are important for building a secure future. Since autonomous vehicles depend on the communication between sensors, artificial intelligence, external infrastructure, and other systems, they are exposed to different types of cyber threats. A cybersecurity breach in an autonomous vehicle can cause serious problems, including a loss of public trust and safety. Therefore, it is very important to develop and apply strong cybersecurity measures to support the growth and acceptance of self-driving cars. This paper discusses major cybersecurity challenges like vulnerabilities in software and hardware, risks from wireless communication, and threats through external interfaces. It also reviews existing solutions such as secure software development, intrusion detection systems, cryptographic protocols, and anomaly detection methods. Additionally, the paper highlights the role of regulatory bodies, industry collaborations, and cybersecurity standards in creating a secure environment for autonomous vehicles. Setting clear rules and best practices is necessary for consistent protection across manufacturers and regions. By analyzing the current cybersecurity landscape and suggesting practical countermeasures, this paper aims to contribute to the safe development and public trust of autonomous vehicle technology."
  },
  {
    "title": "Modular Machine Learning: An Indispensable Path towards New-Generation Large Language Models",
    "url": "http://arxiv.org/abs/2504.20020v1",
    "arxiv_id": "2504.20020v1",
    "authors": [
      "Xin Wang",
      "Haoyang Li",
      "Zeyang Zhang",
      "Haibo Chen",
      "Wenwu Zhu"
    ],
    "published": "2025-04-28T17:42:02+00:00",
    "summary": "Large language models (LLMs) have dramatically advanced machine learning research including natural language processing, computer vision, data mining, etc., yet they still exhibit critical limitations in reasoning, factual consistency, and interpretability. In this paper, we introduce a novel learning paradigm -- Modular Machine Learning (MML) -- as an essential approach toward new-generation LLMs. MML decomposes the complex structure of LLMs into three interdependent components: modular representation, modular model, and modular reasoning, aiming to enhance LLMs' capability of counterfactual reasoning, mitigating hallucinations, as well as promoting fairness, safety, and transparency. Specifically, the proposed MML paradigm can: i) clarify the internal working mechanism of LLMs through the disentanglement of semantic components; ii) allow for flexible and task-adaptive model design; iii) enable interpretable and logic-driven decision-making process. We present a feasible implementation of MML-based LLMs via leveraging advanced techniques such as disentangled representation learning, neural architecture search and neuro-symbolic learning. We critically identify key challenges, such as the integration of continuous neural and discrete symbolic processes, joint optimization, and computational scalability, present promising future research directions that deserve further exploration. Ultimately, the integration of the MML paradigm with LLMs has the potential to bridge the gap between statistical (deep) learning and formal (logical) reasoning, thereby paving the way for robust, adaptable, and trustworthy AI systems across a wide range of real-world applications."
  },
  {
    "title": "Socially-Aware Autonomous Driving: Inferring Yielding Intentions for Safer Interactions",
    "url": "http://arxiv.org/abs/2504.20004v1",
    "arxiv_id": "2504.20004v1",
    "authors": [
      "Jing Wang",
      "Yan Jin",
      "Hamid Taghavifar",
      "Fei Ding",
      "Chongfeng Wei"
    ],
    "published": "2025-04-28T17:24:04+00:00",
    "summary": "Since the emergence of autonomous driving technology, it has advanced rapidly over the past decade. It is becoming increasingly likely that autonomous vehicles (AVs) would soon coexist with human-driven vehicles (HVs) on the roads. Currently, safety and reliable decision-making remain significant challenges, particularly when AVs are navigating lane changes and interacting with surrounding HVs. Therefore, precise estimation of the intentions of surrounding HVs can assist AVs in making more reliable and safe lane change decision-making. This involves not only understanding their current behaviors but also predicting their future motions without any direct communication. However, distinguishing between the passing and yielding intentions of surrounding HVs still remains ambiguous. To address the challenge, we propose a social intention estimation algorithm rooted in Directed Acyclic Graph (DAG), coupled with a decision-making framework employing Deep Reinforcement Learning (DRL) algorithms. To evaluate the method's performance, the proposed framework can be tested and applied in a lane-changing scenario within a simulated environment. Furthermore, the experiment results demonstrate how our approach enhances the ability of AVs to navigate lane changes safely and efficiently on roads."
  },
  {
    "title": "Mitigating Societal Cognitive Overload in the Age of AI: Challenges and Directions",
    "url": "http://arxiv.org/abs/2504.19990v1",
    "arxiv_id": "2504.19990v1",
    "authors": [
      "Salem Lahlou"
    ],
    "published": "2025-04-28T17:06:30+00:00",
    "summary": "Societal cognitive overload, driven by the deluge of information and complexity in the AI age, poses a critical challenge to human well-being and societal resilience. This paper argues that mitigating cognitive overload is not only essential for improving present-day life but also a crucial prerequisite for navigating the potential risks of advanced AI, including existential threats. We examine how AI exacerbates cognitive overload through various mechanisms, including information proliferation, algorithmic manipulation, automation anxieties, deregulation, and the erosion of meaning. The paper reframes the AI safety debate to center on cognitive overload, highlighting its role as a bridge between near-term harms and long-term risks. It concludes by discussing potential institutional adaptations, research directions, and policy considerations that arise from adopting an overload-resilient perspective on human-AI alignment, suggesting pathways for future exploration rather than prescribing definitive solutions."
  },
  {
    "title": "HJRNO: Hamilton-Jacobi Reachability with Neural Operators",
    "url": "http://arxiv.org/abs/2504.19989v1",
    "arxiv_id": "2504.19989v1",
    "authors": [
      "Yankai Li",
      "Mo Chen"
    ],
    "published": "2025-04-28T17:06:05+00:00",
    "summary": "Ensuring the safety of autonomous systems under uncertainty is a critical challenge. Hamilton-Jacobi reachability (HJR) analysis is a widely used method for guaranteeing safety under worst-case disturbances. Traditional HJR methods provide safety guarantees but suffer from the curse of dimensionality, limiting their scalability to high-dimensional systems or varying environmental conditions. In this work, we propose HJRNO, a neural operator-based framework for solving backward reachable tubes (BRTs) efficiently and accurately. By leveraging the Fourier Neural Operator (FNO), HJRNO learns a mapping between value functions, enabling fast inference with strong generalization across different obstacle shapes, system configurations, and hyperparameters. We demonstrate that HJRNO achieves low error on random obstacle scenarios and generalizes effectively across varying system dynamics. These results suggest that HJRNO offers a promising foundation model approach for scalable, real-time safety analysis in autonomous systems."
  },
  {
    "title": "Can AI Agents Design and Implement Drug Discovery Pipelines?",
    "url": "http://arxiv.org/abs/2504.19912v1",
    "arxiv_id": "2504.19912v1",
    "authors": [
      "Khachik Smbatyan",
      "Tsolak Ghukasyan",
      "Tigran Aghajanyan",
      "Hovhannes Dabaghyan",
      "Sergey Adamyan",
      "Aram Bughdaryan",
      "Vahagn Altunyan",
      "Gagik Navasardyan",
      "Aram Davtyan",
      "Anush Hakobyan",
      "Aram Gharibyan",
      "Arman Fahradyan",
      "Artur Hakobyan",
      "Hasmik Mnatsakanyan",
      "Narek Ginoyan",
      "Garik Petrosyan"
    ],
    "published": "2025-04-28T15:41:28+00:00",
    "summary": "The rapid advancement of artificial intelligence, particularly autonomous agentic systems based on Large Language Models (LLMs), presents new opportunities to accelerate drug discovery by improving in-silico modeling and reducing dependence on costly experimental trials. Current AI agent-based systems demonstrate proficiency in solving programming challenges and conducting research, indicating an emerging potential to develop software capable of addressing complex problems such as pharmaceutical design and drug discovery. This paper introduces DO Challenge, a benchmark designed to evaluate the decision-making abilities of AI agents in a single, complex problem resembling virtual screening scenarios. The benchmark challenges systems to independently develop, implement, and execute efficient strategies for identifying promising molecular structures from extensive datasets, while navigating chemical space, selecting models, and managing limited resources in a multi-objective context. We also discuss insights from the DO Challenge 2025, a competition based on the proposed benchmark, which showcased diverse strategies explored by human participants. Furthermore, we present the Deep Thought multi-agent system, which demonstrated strong performance on the benchmark, outperforming most human teams. Among the language models tested, Claude 3.7 Sonnet, Gemini 2.5 Pro and o3 performed best in primary agent roles, and GPT-4o, Gemini 2.0 Flash were effective in auxiliary roles. While promising, the system's performance still fell short of expert-designed solutions and showed high instability, highlighting both the potential and current limitations of AI-driven methodologies in transforming drug discovery and broader scientific research."
  },
  {
    "title": "Human-Centered AI and Autonomy in Robotics: Insights from a Bibliometric Study",
    "url": "http://arxiv.org/abs/2504.19848v1",
    "arxiv_id": "2504.19848v1",
    "authors": [
      "Simona Casini",
      "Pietro Ducange",
      "Francesco Marcelloni",
      "Lorenzo Pollini"
    ],
    "published": "2025-04-28T14:45:48+00:00",
    "summary": "The development of autonomous robotic systems offers significant potential for performing complex tasks with precision and consistency. Recent advances in Artificial Intelligence (AI) have enabled more capable intelligent automation systems, addressing increasingly complex challenges. However, this progress raises questions about human roles in such systems. Human-Centered AI (HCAI) aims to balance human control and automation, ensuring performance enhancement while maintaining creativity, mastery, and responsibility. For real-world applications, autonomous robots must balance task performance with reliability, safety, and trustworthiness. Integrating HCAI principles enhances human-robot collaboration and ensures responsible operation.   This paper presents a bibliometric analysis of intelligent autonomous robotic systems, utilizing SciMAT and VOSViewer to examine data from the Scopus database. The findings highlight academic trends, emerging topics, and AI's role in self-adaptive robotic behaviour, with an emphasis on HCAI architecture. These insights are then projected onto the IBM MAPE-K architecture, with the goal of identifying how these research results map into actual robotic autonomous systems development efforts for real-world scenarios."
  },
  {
    "title": "Measuring Train Driver Performance as Key to Approval of Driverless Trains",
    "url": "http://arxiv.org/abs/2504.19735v1",
    "arxiv_id": "2504.19735v1",
    "authors": [
      "Rustam Tagiew",
      "Prasannavenkatesh Balaji"
    ],
    "published": "2025-04-28T12:32:43+00:00",
    "summary": "Points 2.1.4(b), 2.4.2(b) and 2.4.3(b) in Annex I of Implementing Regulation (EU) No. 402/2013 allow a simplified approach for the safety approval of computer vision systems for driverless trains, if they have 'similar' functions and interfaces as the replaced human driver. The human driver is not replaced one-to-one by a technical system - only a limited set of cognitive functions are replaced. However, performance in the most challenging function, obstacle detection, is difficult to quantify due to the deficiency of published measurement results. This article summarizes the data published so far. This article also goes a long way to remedy this situation by providing a new public and anonymized dataset of 711 train driver performance measurements from controlled experiments. The measurements are made for different speeds, obstacle sizes, train protection systems and obstacle color contrasts respectively. The measured values are reaction time and distance to the obstacle. The goal of this paper is an unbiased and exhaustive description of the presented dataset for research, standardization and regulation. Further project related information including the dataset and source code is available at https://atosense-02371c.usercontent.opencode.de/"
  },
  {
    "title": "Hector UI: A Flexible Human-Robot User Interface for (Semi-)Autonomous Rescue and Inspection Robots",
    "url": "http://arxiv.org/abs/2504.19728v1",
    "arxiv_id": "2504.19728v1",
    "authors": [
      "Stefan Fabian",
      "Oskar von Stryk"
    ],
    "published": "2025-04-28T12:28:39+00:00",
    "summary": "The remote human operator's user interface (UI) is an important link to make the robot an efficient extension of the operator's perception and action. In rescue applications, several studies have investigated the design of operator interfaces based on observations during major robotics competitions or field deployments. Based on this research, guidelines for good interface design were empirically identified. The investigations on the UIs of teams participating in competitions are often based on external observations during UI application, which may miss some relevant requirements for UI flexibility. In this work, we present an open-source and flexibly configurable user interface based on established guidelines and its exemplary use for wheeled, tracked, and walking robots. We explain the design decisions and cover the insights we have gained during its highly successful applications in multiple robotics competitions and evaluations. The presented UI can also be adapted for other robots with little effort and is available as open source."
  },
  {
    "title": "Open-set Anomaly Segmentation in Complex Scenarios",
    "url": "http://arxiv.org/abs/2504.19706v1",
    "arxiv_id": "2504.19706v1",
    "authors": [
      "Song Xia",
      "Yi Yu",
      "Henghui Ding",
      "Wenhan Yang",
      "Shifei Liu",
      "Alex C. Kot",
      "Xudong Jiang"
    ],
    "published": "2025-04-28T12:00:10+00:00",
    "summary": "Precise segmentation of out-of-distribution (OoD) objects, herein referred to as anomalies, is crucial for the reliable deployment of semantic segmentation models in open-set, safety-critical applications, such as autonomous driving. Current anomalous segmentation benchmarks predominantly focus on favorable weather conditions, resulting in untrustworthy evaluations that overlook the risks posed by diverse meteorological conditions in open-set environments, such as low illumination, dense fog, and heavy rain. To bridge this gap, this paper introduces the ComsAmy, a challenging benchmark specifically designed for open-set anomaly segmentation in complex scenarios. ComsAmy encompasses a wide spectrum of adverse weather conditions, dynamic driving environments, and diverse anomaly types to comprehensively evaluate the model performance in realistic open-world scenarios. Our extensive evaluation of several state-of-the-art anomalous segmentation models reveals that existing methods demonstrate significant deficiencies in such challenging scenarios, highlighting their serious safety risks for real-world deployment. To solve that, we propose a novel energy-entropy learning (EEL) strategy that integrates the complementary information from energy and entropy to bolster the robustness of anomaly segmentation under complex open-world environments. Additionally, a diffusion-based anomalous training data synthesizer is proposed to generate diverse and high-quality anomalous images to enhance the existing copy-paste training data synthesizer. Extensive experimental results on both public and ComsAmy benchmarks demonstrate that our proposed diffusion-based synthesizer with energy and entropy learning (DiffEEL) serves as an effective and generalizable plug-and-play method to enhance existing models, yielding an average improvement of around 4.96% in $\\rm{AUPRC}$ and 9.87% in $\\rm{FPR}_{95}$."
  },
  {
    "title": "$\\texttt{SAGE}$: A Generic Framework for LLM Safety Evaluation",
    "url": "http://arxiv.org/abs/2504.19674v1",
    "arxiv_id": "2504.19674v1",
    "authors": [
      "Madhur Jindal",
      "Hari Shrawgi",
      "Parag Agrawal",
      "Sandipan Dandapat"
    ],
    "published": "2025-04-28T11:01:08+00:00",
    "summary": "Safety evaluation of Large Language Models (LLMs) has made progress and attracted academic interest, but it remains challenging to keep pace with the rapid integration of LLMs across diverse applications. Different applications expose users to various harms, necessitating application-specific safety evaluations with tailored harms and policies. Another major gap is the lack of focus on the dynamic and conversational nature of LLM systems. Such potential oversights can lead to harms that go unnoticed in standard safety benchmarks. This paper identifies the above as key requirements for robust LLM safety evaluation and recognizing that current evaluation methodologies do not satisfy these, we introduce the $\\texttt{SAGE}$ (Safety AI Generic Evaluation) framework. $\\texttt{SAGE}$ is an automated modular framework designed for customized and dynamic harm evaluations. It utilizes adversarial user models that are system-aware and have unique personalities, enabling a holistic red-teaming evaluation. We demonstrate $\\texttt{SAGE}$'s effectiveness by evaluating seven state-of-the-art LLMs across three applications and harm policies. Our experiments with multi-turn conversational evaluations revealed a concerning finding that harm steadily increases with conversation length. Furthermore, we observe significant disparities in model behavior when exposed to different user personalities and scenarios. Our findings also reveal that some models minimize harmful outputs by employing severe refusal tactics that can hinder their usefulness. These insights highlight the necessity of adaptive and context-specific testing to ensure better safety alignment and safer deployment of LLMs in real-world scenarios."
  },
  {
    "title": "Fooling the Decoder: An Adversarial Attack on Quantum Error Correction",
    "url": "http://arxiv.org/abs/2504.19651v1",
    "arxiv_id": "2504.19651v1",
    "authors": [
      "Jerome Lenssen",
      "Alexandru Paler"
    ],
    "published": "2025-04-28T10:10:05+00:00",
    "summary": "Neural network decoders are becoming essential for achieving fault-tolerant quantum computations. However, their internal mechanisms are poorly understood, hindering our ability to ensure their reliability and security against adversarial attacks. Leading machine learning decoders utilize recurrent and transformer models (e.g., AlphaQubit), with reinforcement learning (RL) playing a key role in training advanced transformer models (e.g., DeepSeek R1). In this work, we target a basic RL surface code decoder (DeepQ) to create the first adversarial attack on quantum error correction. By applying state-of-the-art white-box methods, we uncover vulnerabilities in this decoder, demonstrating an attack that reduces the logical qubit lifetime in memory experiments by up to five orders of magnitude. We validate that this attack exploits a genuine weakness, as the decoder exhibits robustness against noise fluctuations, is largely unaffected by substituting the referee decoder, responsible for episode termination, with an MWPM decoder, and demonstrates fault tolerance at checkable code distances. This attack highlights the susceptibility of machine learning-based QEC and underscores the importance of further research into robust QEC methods."
  },
  {
    "title": "ARMOR: Adaptive Meshing with Reinforcement Optimization for Real-time 3D Monitoring in Unexposed Scenes",
    "url": "http://arxiv.org/abs/2504.19624v1",
    "arxiv_id": "2504.19624v1",
    "authors": [
      "Yizhe Zhang",
      "Jianping Li",
      "Xin Zhao",
      "Fuxun Liang",
      "Zhen Dong",
      "Bisheng Yang"
    ],
    "published": "2025-04-28T09:33:40+00:00",
    "summary": "Unexposed environments, such as lava tubes, mines, and tunnels, are among the most complex yet strategically significant domains for scientific exploration and infrastructure development. Accurate and real-time 3D meshing of these environments is essential for applications including automated structural assessment, robotic-assisted inspection, and safety monitoring. Implicit neural Signed Distance Fields (SDFs) have shown promising capabilities in online meshing; however, existing methods often suffer from large projection errors and rely on fixed reconstruction parameters, limiting their adaptability to complex and unstructured underground environments such as tunnels, caves, and lava tubes. To address these challenges, this paper proposes ARMOR, a scene-adaptive and reinforcement learning-based framework for real-time 3D meshing in unexposed environments. The proposed method was validated across more than 3,000 meters of underground environments, including engineered tunnels, natural caves, and lava tubes. Experimental results demonstrate that ARMOR achieves superior performance in real-time mesh reconstruction, reducing geometric error by 3.96\\% compared to state-of-the-art baselines, while maintaining real-time efficiency. The method exhibits improved robustness, accuracy, and adaptability, indicating its potential for advanced 3D monitoring and mapping in challenging unexposed scenarios. The project page can be found at: https://yizhezhang0418.github.io/armor.github.io/"
  },
  {
    "title": "AI Alignment in Medical Imaging: Unveiling Hidden Biases Through Counterfactual Analysis",
    "url": "http://arxiv.org/abs/2504.19621v1",
    "arxiv_id": "2504.19621v1",
    "authors": [
      "Haroui Ma",
      "Francesco Quinzan",
      "Theresa Willem",
      "Stefan Bauer"
    ],
    "published": "2025-04-28T09:28:25+00:00",
    "summary": "Machine learning (ML) systems for medical imaging have demonstrated remarkable diagnostic capabilities, but their susceptibility to biases poses significant risks, since biases may negatively impact generalization performance. In this paper, we introduce a novel statistical framework to evaluate the dependency of medical imaging ML models on sensitive attributes, such as demographics. Our method leverages the concept of counterfactual invariance, measuring the extent to which a model's predictions remain unchanged under hypothetical changes to sensitive attributes. We present a practical algorithm that combines conditional latent diffusion models with statistical hypothesis testing to identify and quantify such biases without requiring direct access to counterfactual data. Through experiments on synthetic datasets and large-scale real-world medical imaging datasets, including \\textsc{cheXpert} and MIMIC-CXR, we demonstrate that our approach aligns closely with counterfactual fairness principles and outperforms standard baselines. This work provides a robust tool to ensure that ML diagnostic systems generalize well, e.g., across demographic groups, offering a critical step towards AI safety in healthcare. Code: https://github.com/Neferpitou3871/AI-Alignment-Medical-Imaging."
  },
  {
    "title": "Crowd Detection Using Very-Fine-Resolution Satellite Imagery",
    "url": "http://arxiv.org/abs/2504.19546v1",
    "arxiv_id": "2504.19546v1",
    "authors": [
      "Tong Xiao",
      "Qunming Wang",
      "Ping Lu",
      "Tenghai Huang",
      "Xiaohua Tong",
      "Peter M. Atkinson"
    ],
    "published": "2025-04-28T07:51:26+00:00",
    "summary": "Accurate crowd detection (CD) is critical for public safety and historical pattern analysis, yet existing methods relying on ground and aerial imagery suffer from limited spatio-temporal coverage. The development of very-fine-resolution (VFR) satellite sensor imagery (e.g., ~0.3 m spatial resolution) provides unprecedented opportunities for large-scale crowd activity analysis, but it has never been considered for this task. To address this gap, we proposed CrowdSat-Net, a novel point-based convolutional neural network, which features two innovative components: Dual-Context Progressive Attention Network (DCPAN) to improve feature representation of individuals by aggregating scene context and local individual characteristics, and High-Frequency Guided Deformable Upsampler (HFGDU) that recovers high-frequency information during upsampling through frequency-domain guided deformable convolutions. To validate the effectiveness of CrowdSat-Net, we developed CrowdSat, the first VFR satellite imagery dataset designed specifically for CD tasks, comprising over 120k manually labeled individuals from multi-source satellite platforms (Beijing-3N, Jilin-1 Gaofen-04A and Google Earth) across China. In the experiments, CrowdSat-Net was compared with five state-of-the-art point-based CD methods (originally designed for ground or aerial imagery) using CrowdSat and achieved the largest F1-score of 66.12% and Precision of 73.23%, surpassing the second-best method by 1.71% and 2.42%, respectively. Moreover, extensive ablation experiments validated the importance of the DCPAN and HFGDU modules. Furthermore, cross-regional evaluation further demonstrated the spatial generalizability of CrowdSat-Net. This research advances CD capability by providing both a newly developed network architecture for CD and a pioneering benchmark dataset to facilitate future CD development."
  },
  {
    "title": "Security Steerability is All You Need",
    "url": "http://arxiv.org/abs/2504.19521v1",
    "arxiv_id": "2504.19521v1",
    "authors": [
      "Itay Hazan",
      "Idan Habler",
      "Ron Bitton",
      "Itsik Mantin"
    ],
    "published": "2025-04-28T06:40:01+00:00",
    "summary": "The adoption of Generative AI (GenAI) in various applications inevitably comes with expanding the attack surface, combining new security threats along with the traditional ones. Consequently, numerous research and industrial initiatives aim to mitigate these security threats in GenAI by developing metrics and designing defenses. However, while most of the GenAI security work focuses on universal threats (e.g. manipulating the LLM to generate forbidden content), there is significantly less discussion on application-level security and how to mitigate it.   Thus, in this work we adopt an application-centric approach to GenAI security, and show that while LLMs cannot protect against ad-hoc application specific threats, they can provide the framework for applications to protect themselves against such threats. Our first contribution is defining Security Steerability - a novel security measure for LLMs, assessing the model's capability to adhere to strict guardrails that are defined in the system prompt ('Refrain from discussing about politics'). These guardrails, in case effective, can stop threats in the presence of malicious users who attempt to circumvent the application and cause harm to its providers.   Our second contribution is a methodology to measure the security steerability of LLMs, utilizing two newly-developed datasets: VeganRibs assesses the LLM behavior in forcing specific guardrails that are not security per se in the presence of malicious user that uses attack boosters (jailbreaks and perturbations), and ReverseText takes this approach further and measures the LLM ability to force specific treatment of the user input as plain text while do user try to give it additional meanings..."
  },
  {
    "title": "Security Steerability is All You Need",
    "url": "http://arxiv.org/abs/2504.19521v2",
    "arxiv_id": "2504.19521v2",
    "authors": [
      "Itay Hazan",
      "Idan Habler",
      "Ron Bitton",
      "Itsik Mantin"
    ],
    "published": "2025-04-28T06:40:01+00:00",
    "summary": "The adoption of Generative AI (GenAI) in various applications inevitably comes with expanding the attack surface, combining new security threats along with the traditional ones. Consequently, numerous research and industrial initiatives aim to mitigate these security threats in GenAI by developing metrics and designing defenses. However, while most of the GenAI security work focuses on universal threats (e.g. manipulating the LLM to generate forbidden content), there is significantly less discussion on application-level security and how to mitigate it. Thus, in this work we adopt an application-centric approach to GenAI security, and show that while LLMs cannot protect against ad-hoc application specific threats, they can provide the framework for applications to protect themselves against such threats. Our first contribution is defining Security Steerability - a novel security measure for LLMs, assessing the model's capability to adhere to strict guardrails that are defined in the system prompt ('Refrain from discussing about politics'). These guardrails, in case effective, can stop threats in the presence of malicious users who attempt to circumvent the application and cause harm to its providers. Our second contribution is a methodology to measure the security steerability of LLMs, utilizing two newly-developed datasets: VeganRibs assesses the LLM behavior in forcing specific guardrails that are not security per se in the presence of malicious user that uses attack boosters (jailbreaks and perturbations), and ReverseText takes this approach further and measures the LLM ability to force specific treatment of the user input as plain text while do user try to give it additional meanings..."
  },
  {
    "title": "AutoP2C: An LLM-Based Agent Framework for Code Repository Generation from Multimodal Content in Academic Papers",
    "url": "http://arxiv.org/abs/2504.20115v1",
    "arxiv_id": "2504.20115v1",
    "authors": [
      "Zijie Lin",
      "Yiqing Shen",
      "Qilin Cai",
      "He Sun",
      "Jinrui Zhou",
      "Mingjun Xiao"
    ],
    "published": "2025-04-28T05:47:37+00:00",
    "summary": "Machine Learning (ML) research is spread through academic papers featuring rich multimodal content, including text, diagrams, and tabular results. However, translating these multimodal elements into executable code remains a challenging and time-consuming process that requires substantial ML expertise. We introduce ``Paper-to-Code'' (P2C), a novel task that transforms the multimodal content of scientific publications into fully executable code repositories, which extends beyond the existing formulation of code generation that merely converts textual descriptions into isolated code snippets. To automate the P2C process, we propose AutoP2C, a multi-agent framework based on large language models that processes both textual and visual content from research papers to generate complete code repositories. Specifically, AutoP2C contains four stages: (1) repository blueprint extraction from established codebases, (2) multimodal content parsing that integrates information from text, equations, and figures, (3) hierarchical task decomposition for structured code generation, and (4) iterative feedback-driven debugging to ensure functionality and performance. Evaluation on a benchmark of eight research papers demonstrates the effectiveness of AutoP2C, which can successfully generate executable code repositories for all eight papers, while OpenAI-o1 or DeepSeek-R1 can only produce runnable code for one paper. The code is available at https://github.com/shoushouyu/Automated-Paper-to-Code."
  },
  {
    "title": "BRIDGE: Benchmarking Large Language Models for Understanding Real-world Clinical Practice Text",
    "url": "http://arxiv.org/abs/2504.19467v1",
    "arxiv_id": "2504.19467v1",
    "authors": [
      "Jiageng Wu",
      "Bowen Gu",
      "Ren Zhou",
      "Kevin Xie",
      "Doug Snyder",
      "Yixing Jiang",
      "Valentina Carducci",
      "Richard Wyss",
      "Rishi J Desai",
      "Emily Alsentzer",
      "Leo Anthony Celi",
      "Adam Rodman",
      "Sebastian Schneeweiss",
      "Jonathan H. Chen",
      "Santiago Romero-Brufau",
      "Kueiyu Joshua Lin",
      "Jie Yang"
    ],
    "published": "2025-04-28T04:13:18+00:00",
    "summary": "Large language models (LLMs) hold great promise for medical applications and are evolving rapidly, with new models being released at an accelerated pace. However, current evaluations of LLMs in clinical contexts remain limited. Most existing benchmarks rely on medical exam-style questions or PubMed-derived text, failing to capture the complexity of real-world electronic health record (EHR) data. Others focus narrowly on specific application scenarios, limiting their generalizability across broader clinical use. To address this gap, we present BRIDGE, a comprehensive multilingual benchmark comprising 87 tasks sourced from real-world clinical data sources across nine languages. We systematically evaluated 52 state-of-the-art LLMs (including DeepSeek-R1, GPT-4o, Gemini, and Llama 4) under various inference strategies. With a total of 13,572 experiments, our results reveal substantial performance variation across model sizes, languages, natural language processing tasks, and clinical specialties. Notably, we demonstrate that open-source LLMs can achieve performance comparable to proprietary models, while medically fine-tuned LLMs based on older architectures often underperform versus updated general-purpose models. The BRIDGE and its corresponding leaderboard serve as a foundational resource and a unique reference for the development and evaluation of new LLMs in real-world clinical text understanding."
  },
  {
    "title": "JailbreaksOverTime: Detecting Jailbreak Attacks Under Distribution Shift",
    "url": "http://arxiv.org/abs/2504.19440v1",
    "arxiv_id": "2504.19440v1",
    "authors": [
      "Julien Piet",
      "Xiao Huang",
      "Dennis Jacob",
      "Annabella Chow",
      "Maha Alrashed",
      "Geng Zhao",
      "Zhanhao Hu",
      "Chawin Sitawarin",
      "Basel Alomair",
      "David Wagner"
    ],
    "published": "2025-04-28T03:01:51+00:00",
    "summary": "Safety and security remain critical concerns in AI deployment. Despite safety training through reinforcement learning with human feedback (RLHF) [ 32], language models remain vulnerable to jailbreak attacks that bypass safety guardrails. Universal jailbreaks - prefixes that can circumvent alignment for any payload - are particularly concerning. We show empirically that jailbreak detection systems face distribution shift, with detectors trained at one point in time performing poorly against newer exploits. To study this problem, we release JailbreaksOverTime, a comprehensive dataset of timestamped real user interactions containing both benign requests and jailbreak attempts collected over 10 months. We propose a two-pronged method for defenders to detect new jailbreaks and continuously update their detectors. First, we show how to use continuous learning to detect jailbreaks and adapt rapidly to new emerging jailbreaks. While detectors trained at a single point in time eventually fail due to drift, we find that universal jailbreaks evolve slowly enough for self-training to be effective. Retraining our detection model weekly using its own labels - with no new human labels - reduces the false negative rate from 4% to 0.3% at a false positive rate of 0.1%. Second, we introduce an unsupervised active monitoring approach to identify novel jailbreaks. Rather than classifying inputs directly, we recognize jailbreaks by their behavior, specifically, their ability to trigger models to respond to known-harmful prompts. This approach has a higher false negative rate (4.1%) than supervised methods, but it successfully identified some out-of-distribution attacks that were missed by the continuous learning approach."
  },
  {
    "title": "Doxing via the Lens: Revealing Privacy Leakage in Image Geolocation for Agentic Multi-Modal Large Reasoning Model",
    "url": "http://arxiv.org/abs/2504.19373v1",
    "arxiv_id": "2504.19373v1",
    "authors": [
      "Weidi Luo",
      "Qiming Zhang",
      "Tianyu Lu",
      "Xiaogeng Liu",
      "Yue Zhao",
      "Zhen Xiang",
      "Chaowei Xiao"
    ],
    "published": "2025-04-27T22:26:45+00:00",
    "summary": "The increasing capabilities of agentic multi-modal large reasoning models, such as ChatGPT o3, have raised critical concerns regarding privacy leakage through inadvertent image geolocation. In this paper, we conduct the first systematic and controlled study on the potential privacy risks associated with visual reasoning abilities of ChatGPT o3. We manually collect and construct a dataset comprising 50 real-world images that feature individuals alongside privacy-relevant environmental elements, capturing realistic and sensitive scenarios for analysis. Our experimental evaluation reveals that ChatGPT o3 can predict user locations with high precision, achieving street-level accuracy (within one mile) in 60% of cases. Through analysis, we identify key visual cues, including street layout and front yard design, that significantly contribute to the model inference success. Additionally, targeted occlusion experiments demonstrate that masking critical features effectively mitigates geolocation accuracy, providing insights into potential defense mechanisms. Our findings highlight an urgent need for privacy-aware development for agentic multi-modal large reasoning models, particularly in applications involving private imagery."
  },
  {
    "title": "Doxing via the Lens: Revealing Privacy Leakage in Image Geolocation for Agentic Multi-Modal Large Reasoning Model",
    "url": "http://arxiv.org/abs/2504.19373v2",
    "arxiv_id": "2504.19373v2",
    "authors": [
      "Weidi Luo",
      "Qiming Zhang",
      "Tianyu Lu",
      "Xiaogeng Liu",
      "Yue Zhao",
      "Zhen Xiang",
      "Chaowei Xiao"
    ],
    "published": "2025-04-27T22:26:45+00:00",
    "summary": "The increasing capabilities of agentic multi-modal large reasoning models, such as ChatGPT o3, have raised critical concerns regarding privacy leakage through inadvertent image geolocation. In this paper, we conduct the first systematic and controlled study on the potential privacy risks associated with visual reasoning abilities of ChatGPT o3. We manually collect and construct a dataset comprising 50 real-world images that feature individuals alongside privacy-relevant environmental elements, capturing realistic and sensitive scenarios for analysis. Our experimental evaluation reveals that ChatGPT o3 can predict user locations with high precision, achieving street-level accuracy (within one mile) in 60% of cases. Through analysis, we identify key visual cues, including street layout and front yard design, that significantly contribute to the model inference success. Additionally, targeted occlusion experiments demonstrate that masking critical features effectively mitigates geolocation accuracy, providing insights into potential defense mechanisms. Our findings highlight an urgent need for privacy-aware development for agentic multi-modal large reasoning models, particularly in applications involving private imagery."
  },
  {
    "title": "Synthesis of Discrete-time Control Barrier Functions for Polynomial Systems Based on Sum-of-Squares Programming",
    "url": "http://arxiv.org/abs/2504.19330v1",
    "arxiv_id": "2504.19330v1",
    "authors": [
      "Erfan Shakhesi",
      "W. P. M. H.",
      "Heemels",
      "Alexander Katriniok"
    ],
    "published": "2025-04-27T18:59:43+00:00",
    "summary": "Discrete-time Control Barrier Functions (DTCBFs) are commonly utilized in the literature as a powerful tool for synthesizing control policies that guarantee safety of discrete-time dynamical systems. However, the systematic synthesis of DTCBFs in a computationally efficient way is at present an important open problem. This article first proposes a novel alternating-descent approach based on Sum-of-Squares programming to synthesize quadratic DTCBFs and corresponding polynomial control policies for discrete-time control-affine polynomial systems with input constraints and semi-algebraic safe sets. Subsequently, two distinct approaches are introduced to extend the proposed method to the synthesis of higher-degree polynomial DTCBFs. To demonstrate its efficacy, we apply the proposed method to numerical case studies."
  },
  {
    "title": "Synthesis of Discrete-time Control Barrier Functions for Polynomial Systems Based on Sum-of-Squares Programming",
    "url": "http://arxiv.org/abs/2504.19330v2",
    "arxiv_id": "2504.19330v2",
    "authors": [
      "Erfan Shakhesi",
      "W. P. M. H. Heemels",
      "Alexander Katriniok"
    ],
    "published": "2025-04-27T18:59:43+00:00",
    "summary": "Discrete-time Control Barrier Functions (DTCBFs) are commonly utilized in the literature as a powerful tool for synthesizing control policies that guarantee safety of discrete-time dynamical systems. However, the systematic synthesis of DTCBFs in a computationally efficient way is at present an important open problem. This article first proposes a novel alternating-descent approach based on Sum-of-Squares programming to synthesize quadratic DTCBFs and corresponding polynomial control policies for discrete-time control-affine polynomial systems with input constraints and semi-algebraic safe sets. Subsequently, two distinct approaches are introduced to extend the proposed method to the synthesis of higher-degree polynomial DTCBFs. To demonstrate its efficacy, we apply the proposed method to numerical case studies."
  },
  {
    "title": "Learned Perceptive Forward Dynamics Model for Safe and Platform-aware Robotic Navigation",
    "url": "http://arxiv.org/abs/2504.19322v1",
    "arxiv_id": "2504.19322v1",
    "authors": [
      "Pascal Roth",
      "Jonas Frey",
      "Cesar Cadena",
      "Marco Hutter"
    ],
    "published": "2025-04-27T18:27:28+00:00",
    "summary": "Ensuring safe navigation in complex environments requires accurate real-time traversability assessment and understanding of environmental interactions relative to the robot`s capabilities. Traditional methods, which assume simplified dynamics, often require designing and tuning cost functions to safely guide paths or actions toward the goal. This process is tedious, environment-dependent, and not generalizable.To overcome these issues, we propose a novel learned perceptive Forward Dynamics Model (FDM) that predicts the robot`s future state conditioned on the surrounding geometry and history of proprioceptive measurements, proposing a more scalable, safer, and heuristic-free solution. The FDM is trained on multiple years of simulated navigation experience, including high-risk maneuvers, and real-world interactions to incorporate the full system dynamics beyond rigid body simulation. We integrate our perceptive FDM into a zero-shot Model Predictive Path Integral (MPPI) planning framework, leveraging the learned mapping between actions, future states, and failure probability. This allows for optimizing a simplified cost function, eliminating the need for extensive cost-tuning to ensure safety. On the legged robot ANYmal, the proposed perceptive FDM improves the position estimation by on average 41% over competitive baselines, which translates into a 27% higher navigation success rate in rough simulation environments. Moreover, we demonstrate effective sim-to-real transfer and showcase the benefit of training on synthetic and real data. Code and models are made publicly available under https://github.com/leggedrobotics/fdm."
  },
  {
    "title": "Learned Perceptive Forward Dynamics Model for Safe and Platform-aware Robotic Navigation",
    "url": "http://arxiv.org/abs/2504.19322v2",
    "arxiv_id": "2504.19322v2",
    "authors": [
      "Pascal Roth",
      "Jonas Frey",
      "Cesar Cadena",
      "Marco Hutter"
    ],
    "published": "2025-04-27T18:27:28+00:00",
    "summary": "Ensuring safe navigation in complex environments requires accurate real-time traversability assessment and understanding of environmental interactions relative to the robot`s capabilities. Traditional methods, which assume simplified dynamics, often require designing and tuning cost functions to safely guide paths or actions toward the goal. This process is tedious, environment-dependent, and not generalizable. To overcome these issues, we propose a novel learned perceptive Forward Dynamics Model (FDM) that predicts the robot`s future state conditioned on the surrounding geometry and history of proprioceptive measurements, proposing a more scalable, safer, and heuristic-free solution. The FDM is trained on multiple years of simulated navigation experience, including high-risk maneuvers, and real-world interactions to incorporate the full system dynamics beyond rigid body simulation. We integrate our perceptive FDM into a zero-shot Model Predictive Path Integral (MPPI) planning framework, leveraging the learned mapping between actions, future states, and failure probability. This allows for optimizing a simplified cost function, eliminating the need for extensive cost-tuning to ensure safety. On the legged robot ANYmal, the proposed perceptive FDM improves the position estimation by on average 41% over competitive baselines, which translates into a 27% higher navigation success rate in rough simulation environments. Moreover, we demonstrate effective sim-to-real transfer and showcase the benefit of training on synthetic and real data. Code and models are made publicly available under https://github.com/leggedrobotics/fdm."
  },
  {
    "title": "The Weak Gravity Conjecture in Asymptotically Safe Quantum Gravity",
    "url": "http://arxiv.org/abs/2504.20107v1",
    "arxiv_id": "2504.20107v1",
    "authors": [
      "Gayatri Ghosh"
    ],
    "published": "2025-04-27T14:19:45+00:00",
    "summary": "The Weak Gravity Conjecture (WGC) posits that gravity must be the weakest force in any consistent theory of quantum gravity. Originally formulated to constrain the landscape of effective field theories arising from string theory, the WGC suggests the existence of states with a charge-to-mass ratio larger than that of extremal black holes. In this work, we revisit the WGC within the framework of Asymptotically Safe Quantum Gravity, a non-perturbative approach where gravitational and gauge couplings flow to a non-Gaussian ultraviolet (UV) fixed point. We construct a scale-dependent effective action, derive quantum-corrected Reissner--Nordstr\\\"om black hole solutions by incorporating position-dependent renormalization scale identification, and compute leading quantum corrections to the extremality condition. Our key finding is that the quantum correction to the extremal charge-to-mass ratio is dominantly governed by the running of the gauge coupling, characterized by a correction parameter $\\delta \\sim \\epsilon_e (\\ell_P/r_+)^{2\\theta}$, where $\\epsilon_e$ captures deviations from infrared behavior. We show that if the electromagnetic coupling grows in the UV ($\\epsilon_e > \\epsilon_G$), the WGC is dynamically strengthened, whereas if it decreases ($\\epsilon_e < \\epsilon_G$), large extremal black holes may violate the WGC unless additional light charged states exist. Our analysis demonstrates that Asymptotic Safety provides a concrete ultraviolet mechanism influencing low-energy swampland criteria, offering a deep UV/IR connection between quantum gravity consistency and effective field theory behavior."
  },
  {
    "title": "Efficient COLREGs-Compliant Collision Avoidance using Turning Circle-based Control Barrier Function",
    "url": "http://arxiv.org/abs/2504.19247v1",
    "arxiv_id": "2504.19247v1",
    "authors": [
      "Changyu Lee",
      "Jinwook Park",
      "Jinwhan Kim"
    ],
    "published": "2025-04-27T14:10:18+00:00",
    "summary": "This paper proposes a computationally efficient collision avoidance algorithm using turning circle-based control barrier functions (CBFs) that comply with international regulations for preventing collisions at sea (COLREGs). Conventional CBFs often lack explicit consideration of turning capabilities and avoidance direction, which are key elements in developing a COLREGs-compliant collision avoidance algorithm. To overcome these limitations, we introduce two CBFs derived from left and right turning circles. These functions establish safety conditions based on the proximity between the traffic ships and the centers of the turning circles, effectively determining both avoidance directions and turning capabilities. The proposed method formulates a quadratic programming problem with the CBFs as constraints, ensuring safe navigation without relying on computationally intensive trajectory optimization. This approach significantly reduces computational effort while maintaining performance comparable to model predictive control-based methods. Simulation results validate the effectiveness of the proposed algorithm in enabling COLREGs-compliant, safe navigation, demonstrating its potential for reliable and efficient operation in complex maritime environments."
  },
  {
    "title": "Machine Learning-Based Modeling of the Anode Heel Effect in X-ray Beam Monte Carlo Simulations",
    "url": "http://arxiv.org/abs/2504.19155v1",
    "arxiv_id": "2504.19155v1",
    "authors": [
      "Hussein Harb",
      "Didier Benoit",
      "Axel Rannou",
      "Chi-Hieu Pham",
      "Valentin Tissot",
      "Bahaa Nasr",
      "Julien Bert"
    ],
    "published": "2025-04-27T08:19:47+00:00",
    "summary": "This study enhances Monte Carlo simulation accuracy in X-ray imaging by developing an AI-driven model for the anode heel effect, achieving improved beam intensity distribution and dosimetric precision. Through dynamic adjustments to beam weights on the anode and cathode sides of the X-ray tube, our machine learning model effectively replicates the asymmetry characteristic of clinical X-ray beams. Experimental results reveal dose rate increases of up to 9.6% on the cathode side and reductions of up to 12.5% on the anode side, for energy levels between 50 and 120 kVp. These experimentally optimized beam weights were integrated into the OpenGATE and GGEMS Monte Carlo toolkits, significantly advancing dosimetric simulation accuracy and the image quality which closely resembles the clinical imaging. Validation with fluence and dose actors demonstrated that the AI-based model closely mirrors clinical beam behavior, providing substantial improvements in dose consistency and accuracy over conventional X-ray models. This approach provides a robust framework for improving X-ray dosimetry, with potential applications in dose optimization, imaging quality enhancement, and radiation safety in both clinical and research settings."
  },
  {
    "title": "Efficient Reasoning for LLMs through Speculative Chain-of-Thought",
    "url": "http://arxiv.org/abs/2504.19095v1",
    "arxiv_id": "2504.19095v1",
    "authors": [
      "Jikai Wang",
      "Juntao Li",
      "Lijun Wu",
      "Min Zhang"
    ],
    "published": "2025-04-27T03:56:39+00:00",
    "summary": "Large reasoning language models such as OpenAI-o1 and Deepseek-R1 have recently attracted widespread attention due to their impressive task-solving abilities. However, the enormous model size and the generation of lengthy thought chains introduce significant reasoning costs and response latency. Existing methods for efficient reasoning mainly focus on reducing the number of model parameters or shortening the chain-of-thought length. In this paper, we introduce Speculative Chain-of-Thought (SCoT), which reduces reasoning latency from another perspective by accelerated average reasoning speed through large and small model collaboration. SCoT conducts thought-level drafting using a lightweight draft model. Then it selects the best CoT draft and corrects the error cases with the target model. The proposed thinking behavior alignment improves the efficiency of drafting and the draft selection strategy maintains the prediction accuracy for complex problems. Experimental results on GSM8K, MATH, GaoKao, CollegeMath and Olympiad datasets show that SCoT reduces reasoning latency by 48\\%$\\sim$66\\% for Deepseek-R1-Distill-Qwen-32B while achieving near-target-model-level performance. Our code is available at https://github.com/Jikai0Wang/Speculative_CoT."
  },
  {
    "title": "CipherBank: Exploring the Boundary of LLM Reasoning Capabilities through Cryptography Challenges",
    "url": "http://arxiv.org/abs/2504.19093v1",
    "arxiv_id": "2504.19093v1",
    "authors": [
      "Yu Li",
      "Qizhi Pei",
      "Mengyuan Sun",
      "Honglin Lin",
      "Chenlin Ming",
      "Xin Gao",
      "Jiang Wu",
      "Conghui He",
      "Lijun Wu"
    ],
    "published": "2025-04-27T03:41:17+00:00",
    "summary": "Large language models (LLMs) have demonstrated remarkable capabilities, especially the recent advancements in reasoning, such as o1 and o3, pushing the boundaries of AI. Despite these impressive achievements in mathematics and coding, the reasoning abilities of LLMs in domains requiring cryptographic expertise remain underexplored. In this paper, we introduce CipherBank, a comprehensive benchmark designed to evaluate the reasoning capabilities of LLMs in cryptographic decryption tasks. CipherBank comprises 2,358 meticulously crafted problems, covering 262 unique plaintexts across 5 domains and 14 subdomains, with a focus on privacy-sensitive and real-world scenarios that necessitate encryption. From a cryptographic perspective, CipherBank incorporates 3 major categories of encryption methods, spanning 9 distinct algorithms, ranging from classical ciphers to custom cryptographic techniques. We evaluate state-of-the-art LLMs on CipherBank, e.g., GPT-4o, DeepSeek-V3, and cutting-edge reasoning-focused models such as o1 and DeepSeek-R1. Our results reveal significant gaps in reasoning abilities not only between general-purpose chat LLMs and reasoning-focused LLMs but also in the performance of current reasoning-focused models when applied to classical cryptographic decryption tasks, highlighting the challenges these models face in understanding and manipulating encrypted data. Through detailed analysis and error investigations, we provide several key observations that shed light on the limitations and potential improvement areas for LLMs in cryptographic reasoning. These findings underscore the need for continuous advancements in LLM reasoning capabilities."
  },
  {
    "title": "Graph of Attacks: Improved Black-Box and Interpretable Jailbreaks for LLMs",
    "url": "http://arxiv.org/abs/2504.19019v1",
    "arxiv_id": "2504.19019v1",
    "authors": [
      "Mohammad Akbar-Tajari",
      "Mohammad Taher Pilehvar",
      "Mohammad Mahmoody"
    ],
    "published": "2025-04-26T21:06:03+00:00",
    "summary": "The challenge of ensuring Large Language Models (LLMs) align with societal standards is of increasing interest, as these models are still prone to adversarial jailbreaks that bypass their safety mechanisms. Identifying these vulnerabilities is crucial for enhancing the robustness of LLMs against such exploits. We propose Graph of ATtacks (GoAT), a method for generating adversarial prompts to test the robustness of LLM alignment using the Graph of Thoughts framework [Besta et al., 2024]. GoAT excels at generating highly effective jailbreak prompts with fewer queries to the victim model than state-of-the-art attacks, achieving up to five times better jailbreak success rate against robust models like Llama. Notably, GoAT creates high-quality, human-readable prompts without requiring access to the targeted model's parameters, making it a black-box attack. Unlike approaches constrained by tree-based reasoning, GoAT's reasoning is based on a more intricate graph structure. By making simultaneous attack paths aware of each other's progress, this dynamic framework allows a deeper integration and refinement of reasoning paths, significantly enhancing the collaborative exploration of adversarial vulnerabilities in LLMs. At a technical level, GoAT starts with a graph structure and iteratively refines it by combining and improving thoughts, enabling synergy between different thought paths. The code for our implementation can be found at: https://github.com/GoAT-pydev/Graph_of_Attacks."
  },
  {
    "title": "Safety Interventions against Adversarial Patches in an Open-Source Driver Assistance System",
    "url": "http://arxiv.org/abs/2504.18990v1",
    "arxiv_id": "2504.18990v1",
    "authors": [
      "Cheng Chen",
      "Grant Xiao",
      "Daehyun Lee",
      "Lishan Yang",
      "Evgenia Smirni",
      "Homa Alemzadeh",
      "Xugui Zhou"
    ],
    "published": "2025-04-26T18:28:35+00:00",
    "summary": "Drivers are becoming increasingly reliant on advanced driver assistance systems (ADAS) as autonomous driving technology becomes more popular and developed with advanced safety features to enhance road safety. However, the increasing complexity of the ADAS makes autonomous vehicles (AVs) more exposed to attacks and accidental faults. In this paper, we evaluate the resilience of a widely used ADAS against safety-critical attacks that target perception inputs. Various safety mechanisms are simulated to assess their impact on mitigating attacks and enhancing ADAS resilience. Experimental results highlight the importance of timely intervention by human drivers and automated safety mechanisms in preventing accidents in both driving and lateral directions and the need to resolve conflicts among safety interventions to enhance system resilience and reliability."
  },
  {
    "title": "A Quadratic Programming Approach to Flight Envelope Protection Using Control Barrier Functions",
    "url": "http://arxiv.org/abs/2504.18951v1",
    "arxiv_id": "2504.18951v1",
    "authors": [
      "Johannes Autenrieb"
    ],
    "published": "2025-04-26T15:23:39+00:00",
    "summary": "Ensuring the safe operation of aerospace systems within their prescribed flight envelope is a fundamental requirement for modern flight control systems. Flight envelope protection prevents violations of aerodynamic, structural, and performance constraints, mitigating risks such as stall, excessive loads, and loss of control. Conventional FEP approaches, such as reference clipping via saturation functions and model-based command filtering, impose constraints at the reference input level but often fail to account for closed-loop system dynamics, potentially leading to constraint violations during transients. This paper introduces a new approach to the flight envelope protection problem by employing a quadratic programming-based safety filter using control barrier functions to dynamically enforce flight envelope constraints while preserving control performance. Unlike traditional reference filtering methods, the control barrier function-based safety filter actively ensures strict forward invariance of the safe flight envelope set, integrating seamlessly with existing control architectures. The proposed framework is implemented in a nonlinear missile flight control system and evaluated in a simulated environment. The results demonstrate its ability to prevent constraint violations while minimizing conservatism, offering a robust alternative to existing flight envelope protection methodologies."
  },
  {
    "title": "Advanced Longitudinal Control and Collision Avoidance for High-Risk Edge Cases in Autonomous Driving",
    "url": "http://arxiv.org/abs/2504.18931v1",
    "arxiv_id": "2504.18931v1",
    "authors": [
      "Dianwei Chen",
      "Yaobang Gong",
      "Xianfeng Yang"
    ],
    "published": "2025-04-26T14:17:06+00:00",
    "summary": "Advanced Driver Assistance Systems (ADAS) and Advanced Driving Systems (ADS) are key to improving road safety, yet most existing implementations focus primarily on the vehicle ahead, neglecting the behavior of following vehicles. This shortfall often leads to chain reaction collisions in high speed, densely spaced traffic particularly when a middle vehicle suddenly brakes and trailing vehicles cannot respond in time. To address this critical gap, we propose a novel longitudinal control and collision avoidance algorithm that integrates adaptive cruising with emergency braking. Leveraging deep reinforcement learning, our method simultaneously accounts for both leading and following vehicles. Through a data preprocessing framework that calibrates real-world sensor data, we enhance the robustness and reliability of the training process, ensuring the learned policy can handle diverse driving conditions. In simulated high risk scenarios (e.g., emergency braking in dense traffic), the algorithm effectively prevents potential pile up collisions, even in situations involving heavy duty vehicles. Furthermore, in typical highway scenarios where three vehicles decelerate, the proposed DRL approach achieves a 99% success rate far surpassing the standard Federal Highway Administration speed concepts guide, which reaches only 36.77% success under the same conditions."
  },
  {
    "title": "Latent Adversarial Training Improves the Representation of Refusal",
    "url": "http://arxiv.org/abs/2504.18872v1",
    "arxiv_id": "2504.18872v1",
    "authors": [
      "Alexandra Abbas",
      "Nora Petrova",
      "Helios Ael Lyons",
      "Natalia Perez-Campanero"
    ],
    "published": "2025-04-26T09:40:31+00:00",
    "summary": "Recent work has shown that language models' refusal behavior is primarily encoded in a single direction in their latent space, making it vulnerable to targeted attacks. Although Latent Adversarial Training (LAT) attempts to improve robustness by introducing noise during training, a key question remains: How does this noise-based training affect the underlying representation of refusal behavior? Understanding this encoding is crucial for evaluating LAT's effectiveness and limitations, just as the discovery of linear refusal directions revealed vulnerabilities in traditional supervised safety fine-tuning (SSFT).   Through the analysis of Llama 2 7B, we examine how LAT reorganizes the refusal behavior in the model's latent space compared to SSFT and embedding space adversarial training (AT). By computing activation differences between harmful and harmless instruction pairs and applying Singular Value Decomposition (SVD), we find that LAT significantly alters the refusal representation, concentrating it in the first two SVD components which explain approximately 75 percent of the activation differences variance - significantly higher than in reference models. This concentrated representation leads to more effective and transferable refusal vectors for ablation attacks: LAT models show improved robustness when attacked with vectors from reference models but become more vulnerable to self-generated vectors compared to SSFT and AT. Our findings suggest that LAT's training perturbations enable a more comprehensive representation of refusal behavior, highlighting both its potential strengths and vulnerabilities for improving model safety."
  },
  {
    "title": "WLTCL: Wide Field-of-View 3-D LiDAR Truck Compartment Automatic Localization System",
    "url": "http://arxiv.org/abs/2504.18870v1",
    "arxiv_id": "2504.18870v1",
    "authors": [
      "Guodong Sun",
      "Mingjing Li",
      "Dingjie Liu",
      "Mingxuan Liu",
      "Bo Wu",
      "Yang Zhang"
    ],
    "published": "2025-04-26T09:35:47+00:00",
    "summary": "As an essential component of logistics automation, the automated loading system is becoming a critical technology for enhancing operational efficiency and safety. Precise automatic positioning of the truck compartment, which serves as the loading area, is the primary step in automated loading. However, existing methods have difficulty adapting to truck compartments of various sizes, do not establish a unified coordinate system for LiDAR and mobile manipulators, and often exhibit reliability issues in cluttered environments. To address these limitations, our study focuses on achieving precise automatic positioning of key points in large, medium, and small fence-style truck compartments in cluttered scenarios. We propose an innovative wide field-of-view 3-D LiDAR vehicle compartment automatic localization system. For vehicles of various sizes, this system leverages the LiDAR to generate high-density point clouds within an extensive field-of-view range. By incorporating parking area constraints, our vehicle point cloud segmentation method more effectively segments vehicle point clouds within the scene. Our compartment key point positioning algorithm utilizes the geometric features of the compartments to accurately locate the corner points, providing stackable spatial regions. Extensive experiments on our collected data and public datasets demonstrate that this system offers reliable positioning accuracy and reduced computational resource consumption, leading to its application and promotion in relevant fields."
  },
  {
    "title": "Diffeomorphic Obstacle Avoidance for Contractive Dynamical Systems via Implicit Representations",
    "url": "http://arxiv.org/abs/2504.18860v1",
    "arxiv_id": "2504.18860v1",
    "authors": [
      "Ken-Joel Simmoteit",
      "Philipp Schillinger",
      "Leonel Rozo"
    ],
    "published": "2025-04-26T08:56:51+00:00",
    "summary": "Ensuring safety and robustness of robot skills is becoming crucial as robots are required to perform increasingly complex and dynamic tasks. The former is essential when performing tasks in cluttered environments, while the latter is relevant to overcome unseen task situations. This paper addresses the challenge of ensuring both safety and robustness in dynamic robot skills learned from demonstrations. Specifically, we build on neural contractive dynamical systems to provide robust extrapolation of the learned skills, while designing a full-body obstacle avoidance strategy that preserves contraction stability via diffeomorphic transforms. This is particularly crucial in complex environments where implicit scene representations, such as Signed Distance Fields (SDFs), are necessary. To this end, our framework called Signed Distance Field Diffeomorphic Transform, leverages SDFs and flow-based diffeomorphisms to achieve contraction-preserving obstacle avoidance. We thoroughly evaluate our framework on synthetic datasets and several real-world robotic tasks in a kitchen environment. Our results show that our approach locally adapts the learned contractive vector field while staying close to the learned dynamics and without introducing highly-curved motion paths, thus outperforming several state-of-the-art methods."
  },
  {
    "title": "Introducing Interval Neural Networks for Uncertainty-Aware System Identification",
    "url": "http://arxiv.org/abs/2504.18845v1",
    "arxiv_id": "2504.18845v1",
    "authors": [
      "Mehmet Ali Ferah",
      "Tufan Kumbasar"
    ],
    "published": "2025-04-26T08:16:46+00:00",
    "summary": "System Identification (SysID) is crucial for modeling and understanding dynamical systems using experimental data. While traditional SysID methods emphasize linear models, their inability to fully capture nonlinear dynamics has driven the adoption of Deep Learning (DL) as a more powerful alternative. However, the lack of uncertainty quantification (UQ) in DL-based models poses challenges for reliability and safety, highlighting the necessity of incorporating UQ. This paper introduces a systematic framework for constructing and learning Interval Neural Networks (INNs) to perform UQ in SysID tasks. INNs are derived by transforming the learnable parameters (LPs) of pre-trained neural networks into interval-valued LPs without relying on probabilistic assumptions. By employing interval arithmetic throughout the network, INNs can generate Prediction Intervals (PIs) that capture target coverage effectively. We extend Long Short-Term Memory (LSTM) and Neural Ordinary Differential Equations (Neural ODEs) into Interval LSTM (ILSTM) and Interval NODE (INODE) architectures, providing the mathematical foundations for their application in SysID. To train INNs, we propose a DL framework that integrates a UQ loss function and parameterization tricks to handle constraints arising from interval LPs. We introduce novel concept \"elasticity\" for underlying uncertainty causes and validate ILSTM and INODE in SysID experiments, demonstrating their effectiveness."
  },
  {
    "title": "Swarming in the Wild: A Distributed Communication-less Lloyd-based Algorithm dealing with Uncertainties",
    "url": "http://arxiv.org/abs/2504.18840v1",
    "arxiv_id": "2504.18840v1",
    "authors": [
      "Manuel Boldrer",
      "Vit Kratky",
      "Viktor Walter",
      "Martin Saska"
    ],
    "published": "2025-04-26T07:56:52+00:00",
    "summary": "In this work, we present a distributed algorithm for swarming in complex environments that operates with no communication, no a priori information about the environment, and using only onboard sensing and computation capabilities. We provide sufficient conditions to guarantee that each robot reaches its goal region in a finite time, avoiding collisions with obstacles and other robots without exceeding a desired maximum distance from a predefined set of neighbors (flocking constraint). In addition, we show how the proposed algorithm can deal with tracking errors and onboard sensing errors without violating safety and proximity constraints, still providing the conditions for having convergence towards the goal region. To validate the approach, we provide experiments in the field. We tested our algorithm in GNSS-denied environments i.e., a dense forest, where fully autonomous aerial robots swarmed safely to the desired destinations, by relying only on onboard sensors, i.e., without a communication network. This work marks the initial deployment of a fully distributed system where there is no communication between the robots, nor reliance on any global localization system, which at the same time it ensures safety and convergence towards the goal within such complex environments."
  },
  {
    "title": "Toward Generalizable Evaluation in the LLM Era: A Survey Beyond Benchmarks",
    "url": "http://arxiv.org/abs/2504.18838v1",
    "arxiv_id": "2504.18838v1",
    "authors": [
      "Yixin Cao",
      "Shibo Hong",
      "Xinze Li",
      "Jiahao Ying",
      "Yubo Ma",
      "Haiyuan Liang",
      "Yantao Liu",
      "Zijun Yao",
      "Xiaozhi Wang",
      "Dan Huang",
      "Wenxuan Zhang",
      "Lifu Huang",
      "Muhao Chen",
      "Lei Hou",
      "Qianru Sun",
      "Xingjun Ma",
      "Zuxuan Wu",
      "Min-Yen Kan",
      "David Lo",
      "Qi Zhang",
      "Heng Ji",
      "Jing Jiang",
      "Juanzi Li",
      "Aixin Sun",
      "Xuanjing Huang",
      "Tat-Seng Chua",
      "Yu-Gang Jiang"
    ],
    "published": "2025-04-26T07:48:52+00:00",
    "summary": "Large Language Models (LLMs) are advancing at an amazing speed and have become indispensable across academia, industry, and daily applications. To keep pace with the status quo, this survey probes the core challenges that the rise of LLMs poses for evaluation. We identify and analyze two pivotal transitions: (i) from task-specific to capability-based evaluation, which reorganizes benchmarks around core competencies such as knowledge, reasoning, instruction following, multi-modal understanding, and safety; and (ii) from manual to automated evaluation, encompassing dynamic dataset curation and \"LLM-as-a-judge\" scoring.   Yet, even with these transitions, a crucial obstacle persists: the evaluation generalization issue. Bounded test sets cannot scale alongside models whose abilities grow seemingly without limit. We will dissect this issue, along with the core challenges of the above two transitions, from the perspectives of methods, datasets, evaluators, and metrics. Due to the fast evolving of this field, we will maintain a living GitHub repository (links are in each section) to crowd-source updates and corrections, and warmly invite contributors and collaborators."
  },
  {
    "title": "Reservoir-enhanced Segment Anything Model for Subsurface Diagnosis",
    "url": "http://arxiv.org/abs/2504.18802v1",
    "arxiv_id": "2504.18802v1",
    "authors": [
      "Xiren Zhou",
      "Shikang Liu",
      "Xinyu Yan",
      "Yizhan Fan",
      "Xiangyu Wang",
      "Yu Kang",
      "Jian Cheng",
      "Huanhuan Chen"
    ],
    "published": "2025-04-26T05:13:22+00:00",
    "summary": "Urban roads and infrastructure, vital to city operations, face growing threats from subsurface anomalies like cracks and cavities. Ground Penetrating Radar (GPR) effectively visualizes underground conditions employing electromagnetic (EM) waves; however, accurate anomaly detection via GPR remains challenging due to limited labeled data, varying subsurface conditions, and indistinct target boundaries. Although visually image-like, GPR data fundamentally represent EM waves, with variations within and between waves critical for identifying anomalies. Addressing these, we propose the Reservoir-enhanced Segment Anything Model (Res-SAM), an innovative framework exploiting both visual discernibility and wave-changing properties of GPR data. Res-SAM initially identifies apparent candidate anomaly regions given minimal prompts, and further refines them by analyzing anomaly-induced changing information within and between EM waves in local GPR data, enabling precise and complete anomaly region extraction and category determination. Real-world experiments demonstrate that Res-SAM achieves high detection accuracy (>85%) and outperforms state-of-the-art. Notably, Res-SAM requires only minimal accessible non-target data, avoids intensive training, and incorporates simple human interaction to enhance reliability. Our research provides a scalable, resource-efficient solution for rapid subsurface anomaly detection across diverse environments, improving urban safety monitoring while reducing manual effort and computational cost."
  },
  {
    "title": "Certifiably-Correct Mapping for Safe Navigation Despite Odometry Drift",
    "url": "http://arxiv.org/abs/2504.18713v1",
    "arxiv_id": "2504.18713v1",
    "authors": [
      "Devansh R. Agrawal",
      "Taekyung Kim",
      "Rajiv Govindjee",
      "Trushant Adeshara",
      "Jiangbo Yu",
      "Anurekha Ravikumar",
      "Dimitra Panagou"
    ],
    "published": "2025-04-25T21:53:33+00:00",
    "summary": "Accurate perception, state estimation and mapping are essential for safe robotic navigation as planners and controllers rely on these components for safety-critical decisions. However, existing mapping approaches often assume perfect pose estimates, an unrealistic assumption that can lead to incorrect obstacle maps and therefore collisions. This paper introduces a framework for certifiably-correct mapping that ensures that the obstacle map correctly classifies obstacle-free regions despite the odometry drift in vision-based localization systems (VIO}/SLAM). By deflating the safe region based on the incremental odometry error at each timestep, we ensure that the map remains accurate and reliable locally around the robot, even as the overall odometry error with respect to the inertial frame grows unbounded.   Our contributions include two approaches to modify popular obstacle mapping paradigms, (I) Safe Flight Corridors, and (II) Signed Distance Fields. We formally prove the correctness of both methods, and describe how they integrate with existing planning and control modules. Simulations using the Replica dataset highlight the efficacy of our methods compared to state-of-the-art techniques. Real-world experiments with a robotic rover show that, while baseline methods result in collisions with previously mapped obstacles, the proposed framework enables the rover to safely stop before potential collisions."
  },
  {
    "title": "Proof-of-TBI -- Fine-Tuned Vision Language Model Consortium and OpenAI-o3 Reasoning LLM-Based Medical Diagnosis Support System for Mild Traumatic Brain Injury (TBI) Prediction",
    "url": "http://arxiv.org/abs/2504.18671v1",
    "arxiv_id": "2504.18671v1",
    "authors": [
      "Ross Gore",
      "Eranga Bandara",
      "Sachin Shetty",
      "Alberto E. Musto",
      "Pratip Rana",
      "Ambrosio Valencia-Romero",
      "Christopher Rhea",
      "Lobat Tayebi",
      "Heather Richter",
      "Atmaram Yarlagadda",
      "Donna Edmonds",
      "Steven Wallace",
      "Donna Broshek"
    ],
    "published": "2025-04-25T19:49:30+00:00",
    "summary": "Mild Traumatic Brain Injury (TBI) detection presents significant challenges due to the subtle and often ambiguous presentation of symptoms in medical imaging, making accurate diagnosis a complex task. To address these challenges, we propose Proof-of-TBI, a medical diagnosis support system that integrates multiple fine-tuned vision-language models with the OpenAI-o3 reasoning large language model (LLM). Our approach fine-tunes multiple vision-language models using a labeled dataset of TBI MRI scans, training them to diagnose TBI symptoms effectively. The predictions from these models are aggregated through a consensus-based decision-making process. The system evaluates the predictions from all fine-tuned vision language models using the OpenAI-o3 reasoning LLM, a model that has demonstrated remarkable reasoning performance, to produce the most accurate final diagnosis. The LLM Agents orchestrates interactions between the vision-language models and the reasoning LLM, managing the final decision-making process with transparency, reliability, and automation. This end-to-end decision-making workflow combines the vision-language model consortium with the OpenAI-o3 reasoning LLM, enabled by custom prompt engineering by the LLM agents. The prototype for the proposed platform was developed in collaboration with the U.S. Army Medical Research team in Newport News, Virginia, incorporating five fine-tuned vision-language models. The results demonstrate the transformative potential of combining fine-tuned vision-language model inputs with the OpenAI-o3 reasoning LLM to create a robust, secure, and highly accurate diagnostic system for mild TBI prediction. To the best of our knowledge, this research represents the first application of fine-tuned vision-language models integrated with a reasoning LLM for TBI prediction tasks."
  },
  {
    "title": "Foundations of Safe Online Reinforcement Learning in the Linear Quadratic Regulator: $\\sqrt{T}$-Regret",
    "url": "http://arxiv.org/abs/2504.18657v1",
    "arxiv_id": "2504.18657v1",
    "authors": [
      "Benjamin Schiffer",
      "Lucas Janson"
    ],
    "published": "2025-04-25T19:22:57+00:00",
    "summary": "Understanding how to efficiently learn while adhering to safety constraints is essential for using online reinforcement learning in practical applications. However, proving rigorous regret bounds for safety-constrained reinforcement learning is difficult due to the complex interaction between safety, exploration, and exploitation. In this work, we seek to establish foundations for safety-constrained reinforcement learning by studying the canonical problem of controlling a one-dimensional linear dynamical system with unknown dynamics. We study the safety-constrained version of this problem, where the state must with high probability stay within a safe region, and we provide the first safe algorithm that achieves regret of $\\tilde{O}_T(\\sqrt{T})$. Furthermore, the regret is with respect to the baseline of truncated linear controllers, a natural baseline of non-linear controllers that are well-suited for safety-constrained linear systems. In addition to introducing this new baseline, we also prove several desirable continuity properties of the optimal controller in this baseline. In showing our main result, we prove that whenever the constraints impact the optimal controller, the non-linearity of our controller class leads to a faster rate of learning than in the unconstrained setting."
  },
  {
    "title": "Periodic Online Testing for Sparse Systolic Tensor Arrays",
    "url": "http://arxiv.org/abs/2504.18628v1",
    "arxiv_id": "2504.18628v1",
    "authors": [
      "Christodoulos Peltekis",
      "Chrysostomos Nicopoulos",
      "Giorgos Dimitrakopoulos"
    ],
    "published": "2025-04-25T18:10:45+00:00",
    "summary": "Modern Machine Learning (ML) applications often benefit from structured sparsity, a technique that efficiently reduces model complexity and simplifies handling of sparse data in hardware. Sparse systolic tensor arrays - specifically designed to accelerate these structured-sparse ML models - play a pivotal role in enabling efficient computations. As ML is increasingly integrated into safety-critical systems, it is of paramount importance to ensure the reliability of these systems. This paper introduces an online error-checking technique capable of detecting and locating permanent faults within sparse systolic tensor arrays before computation begins. The new technique relies on merely four test vectors and exploits the weight values already loaded within the systolic array to comprehensively test the system. Fault-injection campaigns within the gate-level netlist, while executing three well-established Convolutional Neural Networks (CNN), validate the efficiency of the proposed approach, which is shown to achieve very high fault coverage, while incurring minimal performance and area overheads."
  },
  {
    "title": "Examining the Impact of Optical Aberrations to Image Classification and Object Detection Models",
    "url": "http://arxiv.org/abs/2504.18510v1",
    "arxiv_id": "2504.18510v1",
    "authors": [
      "Patrick M\u00fcller",
      "Alexander Braun",
      "Margret Keuper"
    ],
    "published": "2025-04-25T17:23:47+00:00",
    "summary": "Deep neural networks (DNNs) have proven to be successful in various computer vision applications such that models even infer in safety-critical situations. Therefore, vision models have to behave in a robust way to disturbances such as noise or blur. While seminal benchmarks exist to evaluate model robustness to diverse corruptions, blur is often approximated in an overly simplistic way to model defocus, while ignoring the different blur kernel shapes that result from optical systems. To study model robustness against realistic optical blur effects, this paper proposes two datasets of blur corruptions, which we denote OpticsBench and LensCorruptions. OpticsBench examines primary aberrations such as coma, defocus, and astigmatism, i.e. aberrations that can be represented by varying a single parameter of Zernike polynomials. To go beyond the principled but synthetic setting of primary aberrations, LensCorruptions samples linear combinations in the vector space spanned by Zernike polynomials, corresponding to 100 real lenses. Evaluations for image classification and object detection on ImageNet and MSCOCO show that for a variety of different pre-trained models, the performance on OpticsBench and LensCorruptions varies significantly, indicating the need to consider realistic image corruptions to evaluate a model's robustness against blur."
  },
  {
    "title": "PolyMath: Evaluating Mathematical Reasoning in Multilingual Contexts",
    "url": "http://arxiv.org/abs/2504.18428v1",
    "arxiv_id": "2504.18428v1",
    "authors": [
      "Yiming Wang",
      "Pei Zhang",
      "Jialong Tang",
      "Haoran Wei",
      "Baosong Yang",
      "Rui Wang",
      "Chenshu Sun",
      "Feitong Sun",
      "Jiran Zhang",
      "Junxuan Wu",
      "Qiqian Cang",
      "Yichang Zhang",
      "Fei Huang",
      "Junyang Lin",
      "Fei Huang",
      "Jingren Zhou"
    ],
    "published": "2025-04-25T15:39:04+00:00",
    "summary": "In this paper, we introduce PolyMath, a multilingual mathematical reasoning benchmark covering 18 languages and 4 easy-to-hard difficulty levels. Our benchmark ensures difficulty comprehensiveness, language diversity, and high-quality translation, making it a highly discriminative multilingual mathematical benchmark in the era of reasoning LLMs. We conduct a comprehensive evaluation for advanced LLMs and find that even Deepseek-R1-671B and Qwen-QwQ-32B, achieve only 43.4 and 41.8 benchmark scores, with less than 30% accuracy under the highest level. From a language perspective, our benchmark reveals several key challenges of LLMs in multilingual reasoning: (1) Reasoning performance varies widely across languages for current LLMs; (2) Input-output language consistency is low in reasoning LLMs and may be correlated with performance; (3) The thinking length differs significantly by language for current LLMs. Additionally, we demonstrate that controlling the output language in the instructions has the potential to affect reasoning performance, especially for some low-resource languages, suggesting a promising direction for improving multilingual capabilities in LLMs."
  },
  {
    "title": "Expressing stigma and inappropriate responses prevents LLMs from safely replacing mental health providers",
    "url": "http://arxiv.org/abs/2504.18412v1",
    "arxiv_id": "2504.18412v1",
    "authors": [
      "Jared Moore",
      "Declan Grabb",
      "William Agnew",
      "Kevin Klyman",
      "Stevie Chancellor",
      "Desmond C. Ong",
      "Nick Haber"
    ],
    "published": "2025-04-25T15:14:21+00:00",
    "summary": "Should a large language model (LLM) be used as a therapist? In this paper, we investigate the use of LLMs to *replace* mental health providers, a use case promoted in the tech startup and research space. We conduct a mapping review of therapy guides used by major medical institutions to identify crucial aspects of therapeutic relationships, such as the importance of a therapeutic alliance between therapist and client. We then assess the ability of LLMs to reproduce and adhere to these aspects of therapeutic relationships by conducting several experiments investigating the responses of current LLMs, such as `gpt-4o`. Contrary to best practices in the medical community, LLMs 1) express stigma toward those with mental health conditions and 2) respond inappropriately to certain common (and critical) conditions in naturalistic therapy settings -- e.g., LLMs encourage clients' delusional thinking, likely due to their sycophancy. This occurs even with larger and newer LLMs, indicating that current safety practices may not address these gaps. Furthermore, we note foundational and practical barriers to the adoption of LLMs as therapists, such as that a therapeutic alliance requires human characteristics (e.g., identity and stakes). For these reasons, we conclude that LLMs should not replace therapists, and we discuss alternative roles for LLMs in clinical therapy."
  },
  {
    "title": "Interpretable Affordance Detection on 3D Point Clouds with Probabilistic Prototypes",
    "url": "http://arxiv.org/abs/2504.18355v1",
    "arxiv_id": "2504.18355v1",
    "authors": [
      "Maximilian Xiling Li",
      "Korbinian Rudolf",
      "Nils Blank",
      "Rudolf Lioutikov"
    ],
    "published": "2025-04-25T13:52:39+00:00",
    "summary": "Robotic agents need to understand how to interact with objects in their environment, both autonomously and during human-robot interactions. Affordance detection on 3D point clouds, which identifies object regions that allow specific interactions, has traditionally relied on deep learning models like PointNet++, DGCNN, or PointTransformerV3. However, these models operate as black boxes, offering no insight into their decision-making processes. Prototypical Learning methods, such as ProtoPNet, provide an interpretable alternative to black-box models by employing a \"this looks like that\" case-based reasoning approach. However, they have been primarily applied to image-based tasks. In this work, we apply prototypical learning to models for affordance detection on 3D point clouds. Experiments on the 3D-AffordanceNet benchmark dataset show that prototypical models achieve competitive performance with state-of-the-art black-box models and offer inherent interpretability. This makes prototypical models a promising candidate for human-robot interaction scenarios that require increased trust and safety."
  },
  {
    "title": "Unifying Direct and Indirect Learning for Safe Control of Linear Systems",
    "url": "http://arxiv.org/abs/2504.18331v1",
    "arxiv_id": "2504.18331v1",
    "authors": [
      "Amir Modares",
      "Niyousha Ghiasi",
      "Bahare Kiumarsi",
      "Hamidreza Modares"
    ],
    "published": "2025-04-25T13:17:58+00:00",
    "summary": "This paper aims to learn safe controllers for uncertain discrete-time linear systems under disturbances while achieving the following two crucial goals: 1) integration of different sources of information (i.e., prior information in terms of physical knowledge and posterior information in terms of streaming data), and 2) unifying direct learning with indirect learning. These goals are achieved by representing a parametrized data-driven constrained matrix zonotope form of closed-loop systems that is conformant to prior knowledge. To this end, we first leverage collected data to characterize closed-loop systems by a matrix zonotope and then show that the explainability of these closed-loop systems by prior knowledge can be formalized by adding an equality conformity constraint, which refines the matrix zonotope obtained by data to a constrained matrix zonotope. The prior knowledge is further refined by conforming it to the set of models obtained from a novel zonotope-based system identifier. The source of data used for zonotope-based system identification can be different than the one used for closed-loop representation, allowing to perform transfer learning and online adaptation to new data. The parametrized closed-loop set of systems is then leveraged to directly learn a controller that robustly imposes safety on the closed-loop system. We consider both polytope and zonotope safe sets and provide set inclusion conditions using linear programming to impose safety through {\\lambda}-contractivity. For polytope safe sets, a primal-dual optimization is developed to formalize a linear programming optimization that certifies the set inclusion. For zonotope safe sets, the constrained zonotope set of all next states is formed, and set inclusion is achieved by ensuring the inclusion of this constrained zonotope in a {\\lambda}-scaled level set of the safe set."
  },
  {
    "title": "AI Safety Assurance for Automated Vehicles: A Survey on Research, Standardization, Regulation",
    "url": "http://arxiv.org/abs/2504.18328v1",
    "arxiv_id": "2504.18328v1",
    "authors": [
      "Lars Ullrich",
      "Michael Buchholz",
      "Klaus Dietmayer",
      "Knut Graichen"
    ],
    "published": "2025-04-25T13:14:06+00:00",
    "summary": "Assuring safety of artificial intelligence (AI) applied to safety-critical systems is of paramount importance. Especially since research in the field of automated driving shows that AI is able to outperform classical approaches, to handle higher complexities, and to reach new levels of autonomy. At the same time, the safety assurance required for the use of AI in such safety-critical systems is still not in place. Due to the dynamic and far-reaching nature of the technology, research on safeguarding AI is being conducted in parallel to AI standardization and regulation. The parallel progress necessitates simultaneous consideration in order to carry out targeted research and development of AI systems in the context of automated driving. Therefore, in contrast to existing surveys that focus primarily on research aspects, this paper considers research, standardization and regulation in a concise way. Accordingly, the survey takes into account the interdependencies arising from the triplet of research, standardization and regulation in a forward-looking perspective and anticipates and discusses open questions and possible future directions. In this way, the survey ultimately serves to provide researchers and safety experts with a compact, holistic perspective that discusses the current status, emerging trends, and possible future developments."
  },
  {
    "title": "A comprehensive review of classifier probability calibration metrics",
    "url": "http://arxiv.org/abs/2504.18278v1",
    "arxiv_id": "2504.18278v1",
    "authors": [
      "Richard Oliver Lane"
    ],
    "published": "2025-04-25T11:44:44+00:00",
    "summary": "Probabilities or confidence values produced by artificial intelligence (AI) and machine learning (ML) models often do not reflect their true accuracy, with some models being under or over confident in their predictions. For example, if a model is 80% sure of an outcome, is it correct 80% of the time? Probability calibration metrics measure the discrepancy between confidence and accuracy, providing an independent assessment of model calibration performance that complements traditional accuracy metrics. Understanding calibration is important when the outputs of multiple systems are combined, for assurance in safety or business-critical contexts, and for building user trust in models. This paper provides a comprehensive review of probability calibration metrics for classifier and object detection models, organising them according to a number of different categorisations to highlight their relationships. We identify 82 major metrics, which can be grouped into four classifier families (point-based, bin-based, kernel or curve-based, and cumulative) and an object detection family. For each metric, we provide equations where available, facilitating implementation and comparison by future researchers."
  },
  {
    "title": "Depth-Constrained ASV Navigation with Deep RL and Limited Sensing",
    "url": "http://arxiv.org/abs/2504.18253v1",
    "arxiv_id": "2504.18253v1",
    "authors": [
      "Amirhossein Zhalehmehrabi",
      "Daniele Meli",
      "Francesco Dal Santo",
      "Francesco Trotti",
      "Alessandro Farinelli"
    ],
    "published": "2025-04-25T10:56:56+00:00",
    "summary": "Autonomous Surface Vehicles (ASVs) play a crucial role in maritime operations, yet their navigation in shallow-water environments remains challenging due to dynamic disturbances and depth constraints. Traditional navigation strategies struggle with limited sensor information, making safe and efficient operation difficult. In this paper, we propose a reinforcement learning (RL) framework for ASV navigation under depth constraints, where the vehicle must reach a target while avoiding unsafe areas with only a single depth measurement per timestep from a downward-facing Single Beam Echosounder (SBES). To enhance environmental awareness, we integrate Gaussian Process (GP) regression into the RL framework, enabling the agent to progressively estimate a bathymetric depth map from sparse sonar readings. This approach improves decision-making by providing a richer representation of the environment. Furthermore, we demonstrate effective sim-to-real transfer, ensuring that trained policies generalize well to real-world aquatic conditions. Experimental results validate our method's capability to improve ASV navigation performance while maintaining safety in challenging shallow-water environments."
  },
  {
    "title": "Learning to fuse: dynamic integration of multi-source data for accurate battery lifespan prediction",
    "url": "http://arxiv.org/abs/2504.18230v1",
    "arxiv_id": "2504.18230v1",
    "authors": [
      "He Shanxuan",
      "Lin Zuhong",
      "Yu Bolun",
      "Gao Xu",
      "Long Biao",
      "Yao Jingjing"
    ],
    "published": "2025-04-25T10:24:45+00:00",
    "summary": "Accurate prediction of lithium-ion battery lifespan is vital for ensuring operational reliability and reducing maintenance costs in applications like electric vehicles and smart grids. This study presents a hybrid learning framework for precise battery lifespan prediction, integrating dynamic multi-source data fusion with a stacked ensemble (SE) modeling approach. By leveraging heterogeneous datasets from the National Aeronautics and Space Administration (NASA), Center for Advanced Life Cycle Engineering (CALCE), MIT-Stanford-Toyota Research Institute (TRC), and nickel cobalt aluminum (NCA) chemistries, an entropy-based dynamic weighting mechanism mitigates variability across heterogeneous datasets. The SE model combines Ridge regression, long short-term memory (LSTM) networks, and eXtreme Gradient Boosting (XGBoost), effectively capturing temporal dependencies and nonlinear degradation patterns. It achieves a mean absolute error (MAE) of 0.0058, root mean square error (RMSE) of 0.0092, and coefficient of determination (R2) of 0.9839, outperforming established baseline models with a 46.2% improvement in R2 and an 83.2% reduction in RMSE. Shapley additive explanations (SHAP) analysis identifies differential discharge capacity (Qdlin) and temperature of measurement (Temp_m) as critical aging indicators. This scalable, interpretable framework enhances battery health management, supporting optimized maintenance and safety across diverse energy storage systems, thereby contributing to improved battery health management in energy storage systems."
  },
  {
    "title": "Reimagining Assistive Walkers: An Exploration of Challenges and Preferences in Older Adults",
    "url": "http://arxiv.org/abs/2504.18169v1",
    "arxiv_id": "2504.18169v1",
    "authors": [
      "Victory A. Aruona",
      "Sergio D. Sierra M.",
      "Nigel Harris",
      "Marcela Munera",
      "Carlos A. Cifuentes"
    ],
    "published": "2025-04-25T08:32:44+00:00",
    "summary": "The well-being of older adults relies significantly on maintaining balance and mobility. As physical ability declines, older adults often accept the need for assistive devices. However, existing walkers frequently fail to consider user preferences, leading to perceptions of imposition and reduced acceptance. This research explores the challenges faced by older adults, caregivers, and healthcare professionals when using walkers, assesses their perceptions, and identifies their needs and preferences. A holistic approach was employed, using tailored perception questionnaires for older adults (24 participants), caregivers (30 participants), and healthcare professionals (27 participants), all of whom completed the survey. Over 50% of caregivers and healthcare professionals displayed good knowledge, positive attitudes, and effective practices regarding walkers. However, over 30% of participants perceived current designs as fall risks, citing the need for significant upper body strength, potentially affecting safety and movement. More than 50% highlighted the importance of incorporating fall detection, ergonomic designs, noise reduction, and walker ramps to better meet user needs and preferences."
  },
  {
    "title": "Combating the Bucket Effect:Multi-Knowledge Alignment for Medication Recommendation",
    "url": "http://arxiv.org/abs/2504.18096v1",
    "arxiv_id": "2504.18096v1",
    "authors": [
      "Xiang Li",
      "Haixu Ma",
      "Guanyong Wu",
      "Shi Mu",
      "Chen Li",
      "Shunpan Liang"
    ],
    "published": "2025-04-25T05:47:15+00:00",
    "summary": "Medication recommendation is crucial in healthcare, offering effective treatments based on patient's electronic health records (EHR). Previous studies show that integrating more medication-related knowledge improves medication representation accuracy. However, not all medications encompass multiple types of knowledge data simultaneously. For instance, some medications provide only textual descriptions without structured data. This imbalance in data availability limits the performance of existing models, a challenge we term the \"bucket effect\" in medication recommendation. Our data analysis uncovers the severity of the \"bucket effect\" in medication recommendation. To fill this gap, we introduce a cross-modal medication encoder capable of seamlessly aligning data from different modalities and propose a medication recommendation framework to integrate Multiple types of Knowledge, named MKMed. Specifically, we first pre-train a cross-modal encoder with contrastive learning on five knowledge modalities, aligning them into a unified space. Then, we combine the multi-knowledge medication representations with patient records for recommendations. Extensive experiments on the MIMIC-III and MIMIC-IV datasets demonstrate that MKMed mitigates the \"bucket effect\" in data, and significantly outperforms state-of-the-art baselines in recommendation accuracy and safety."
  },
  {
    "title": "DREAM: Disentangling Risks to Enhance Safety Alignment in Multimodal Large Language Models",
    "url": "http://arxiv.org/abs/2504.18053v1",
    "arxiv_id": "2504.18053v1",
    "authors": [
      "Jianyu Liu",
      "Hangyu Guo",
      "Ranjie Duan",
      "Xingyuan Bu",
      "Yancheng He",
      "Shilong Li",
      "Hui Huang",
      "Jiaheng Liu",
      "Yucheng Wang",
      "Chenchen Jing",
      "Xingwei Qu",
      "Xiao Zhang",
      "Yingshui Tan",
      "Yanan Wu",
      "Jihao Gu",
      "Yangguang Li",
      "Jianke Zhu"
    ],
    "published": "2025-04-25T03:54:24+00:00",
    "summary": "Multimodal Large Language Models (MLLMs) pose unique safety challenges due to their integration of visual and textual data, thereby introducing new dimensions of potential attacks and complex risk combinations. In this paper, we begin with a detailed analysis aimed at disentangling risks through step-by-step reasoning within multimodal inputs. We find that systematic multimodal risk disentanglement substantially enhances the risk awareness of MLLMs. Via leveraging the strong discriminative abilities of multimodal risk disentanglement, we further introduce \\textbf{DREAM} (\\textit{\\textbf{D}isentangling \\textbf{R}isks to \\textbf{E}nhance Safety \\textbf{A}lignment in \\textbf{M}LLMs}), a novel approach that enhances safety alignment in MLLMs through supervised fine-tuning and iterative Reinforcement Learning from AI Feedback (RLAIF). Experimental results show that DREAM significantly boosts safety during both inference and training phases without compromising performance on normal tasks (namely oversafety), achieving a 16.17\\% improvement in the SIUO safe\\&effective score compared to GPT-4V. The data and code are available at https://github.com/Kizna1ver/DREAM."
  },
  {
    "title": "RAG LLMs are Not Safer: A Safety Analysis of Retrieval-Augmented Generation for Large Language Models",
    "url": "http://arxiv.org/abs/2504.18041v1",
    "arxiv_id": "2504.18041v1",
    "authors": [
      "Bang An",
      "Shiyue Zhang",
      "Mark Dredze"
    ],
    "published": "2025-04-25T03:25:18+00:00",
    "summary": "Efforts to ensure the safety of large language models (LLMs) include safety fine-tuning, evaluation, and red teaming. However, despite the widespread use of the Retrieval-Augmented Generation (RAG) framework, AI safety work focuses on standard LLMs, which means we know little about how RAG use cases change a model's safety profile. We conduct a detailed comparative analysis of RAG and non-RAG frameworks with eleven LLMs. We find that RAG can make models less safe and change their safety profile. We explore the causes of this change and find that even combinations of safe models with safe documents can cause unsafe generations. In addition, we evaluate some existing red teaming methods for RAG settings and show that they are less effective than when used for non-RAG settings. Our work highlights the need for safety research and red-teaming methods specifically tailored for RAG LLMs."
  },
  {
    "title": "Joint Resource Estimation and Trajectory Optimization for eVTOL-involved CR network: A Monte Carlo Tree Search-based Approach",
    "url": "http://arxiv.org/abs/2504.18031v1",
    "arxiv_id": "2504.18031v1",
    "authors": [
      "Kai Xiong",
      "Chenxin Yang",
      "Yujie Qin",
      "Chau Yuen"
    ],
    "published": "2025-04-25T02:48:48+00:00",
    "summary": "Electric Vertical Take-Off and Landing (eVTOL) aircraft, pivotal to Advanced Air Mobility (AAM), are emerging as a transformative transportation paradigm with the potential to redefine urban and regional mobility. While these systems offer unprecedented efficiency in transporting people and goods, they rely heavily on computation capability, safety-critical operations such as real-time navigation, environmental sensing, and trajectory tracking--necessitating robust offboard computational support. A widely adopted solution involves offloading these tasks to terrestrial base stations (BSs) along the flight path. However, air-to-ground connectivity is often constrained by spectrum conflicts with terrestrial users, which poses a significant challenge to maintaining reliable task execution. Cognitive radio (CR) techniques offer promising capabilities for dynamic spectrum access, making them a natural fit for addressing this issue. Existing studies often overlook the time-varying nature of BS resources, such as spectrum availability and CPU cycles, which leads to inaccurate trajectory planning, suboptimal offloading success rates, excessive energy consumption, and operational delays. To address these challenges, we propose a trajectory optimization framework for eVTOL swarms that maximizes task offloading success probability while minimizing both energy consumption and resource competition (e.g., spectrum and CPU cycles) with primary terrestrial users. The proposed algorithm integrates a Multi-Armed Bandit (MAB) model to dynamically estimate BS resource availability and a Monte Carlo Tree Search (MCTS) algorithm to determine optimal offloading decisions, selecting both the BSs and access time windows that align with energy and temporal constraints."
  },
  {
    "title": "Virtual Roads, Smarter Safety: A Digital Twin Framework for Mixed Autonomous Traffic Safety Analysis",
    "url": "http://arxiv.org/abs/2504.17968v1",
    "arxiv_id": "2504.17968v1",
    "authors": [
      "Hao Zhang",
      "Ximin Yue",
      "Kexin Tian",
      "Sixu Li",
      "Keshu Wu",
      "Zihao Li",
      "Dominique Lord",
      "Yang Zhou"
    ],
    "published": "2025-04-24T22:27:59+00:00",
    "summary": "This paper presents a digital-twin platform for active safety analysis in mixed traffic environments. The platform is built using a multi-modal data-enabled traffic environment constructed from drone-based aerial LiDAR, OpenStreetMap, and vehicle sensor data (e.g., GPS and inclinometer readings). High-resolution 3D road geometries are generated through AI-powered semantic segmentation and georeferencing of aerial LiDAR data. To simulate real-world driving scenarios, the platform integrates the CAR Learning to Act (CARLA) simulator, Simulation of Urban MObility (SUMO) traffic model, and NVIDIA PhysX vehicle dynamics engine. CARLA provides detailed micro-level sensor and perception data, while SUMO manages macro-level traffic flow. NVIDIA PhysX enables accurate modeling of vehicle behaviors under diverse conditions, accounting for mass distribution, tire friction, and center of mass. This integrated system supports high-fidelity simulations that capture the complex interactions between autonomous and conventional vehicles. Experimental results demonstrate the platform's ability to reproduce realistic vehicle dynamics and traffic scenarios, enhancing the analysis of active safety measures. Overall, the proposed framework advances traffic safety research by enabling in-depth, physics-informed evaluation of vehicle behavior in dynamic and heterogeneous traffic environments."
  },
  {
    "title": "LLM Agent Swarm for Hypothesis-Driven Drug Discovery",
    "url": "http://arxiv.org/abs/2504.17967v1",
    "arxiv_id": "2504.17967v1",
    "authors": [
      "Kevin Song",
      "Andrew Trotter",
      "Jake Y. Chen"
    ],
    "published": "2025-04-24T22:27:50+00:00",
    "summary": "Drug discovery remains a formidable challenge: more than 90 percent of candidate molecules fail in clinical evaluation, and development costs often exceed one billion dollars per approved therapy. Disparate data streams, from genomics and transcriptomics to chemical libraries and clinical records, hinder coherent mechanistic insight and slow progress. Meanwhile, large language models excel at reasoning and tool integration but lack the modular specialization and iterative memory required for regulated, hypothesis-driven workflows. We introduce PharmaSwarm, a unified multi-agent framework that orchestrates specialized LLM \"agents\" to propose, validate, and refine hypotheses for novel drug targets and lead compounds. Each agent accesses dedicated functionality--automated genomic and expression analysis; a curated biomedical knowledge graph; pathway enrichment and network simulation; interpretable binding affinity prediction--while a central Evaluator LLM continuously ranks proposals by biological plausibility, novelty, in silico efficacy, and safety. A shared memory layer captures validated insights and fine-tunes underlying submodels over time, yielding a self-improving system. Deployable on low-code platforms or Kubernetes-based microservices, PharmaSwarm supports literature-driven discovery, omics-guided target identification, and market-informed repurposing. We also describe a rigorous four-tier validation pipeline spanning retrospective benchmarking, independent computational assays, experimental testing, and expert user studies to ensure transparency, reproducibility, and real-world impact. By acting as an AI copilot, PharmaSwarm can accelerate translational research and deliver high-confidence hypotheses more efficiently than traditional pipelines."
  },
  {
    "title": "Integrating Learning-Based Manipulation and Physics-Based Locomotion for Whole-Body Badminton Robot Control",
    "url": "http://arxiv.org/abs/2504.17771v1",
    "arxiv_id": "2504.17771v1",
    "authors": [
      "Haochen Wang",
      "Zhiwei Shi",
      "Chengxi Zhu",
      "Yafei Qiao",
      "Cheng Zhang",
      "Fan Yang",
      "Pengjie Ren",
      "Lan Lu",
      "Dong Xuan"
    ],
    "published": "2025-04-24T17:46:29+00:00",
    "summary": "Learning-based methods, such as imitation learning (IL) and reinforcement learning (RL), can produce excel control policies over challenging agile robot tasks, such as sports robot. However, no existing work has harmonized learning-based policy with model-based methods to reduce training complexity and ensure the safety and stability for agile badminton robot control. In this paper, we introduce \\ourmethod, a novel hybrid control system for agile badminton robots. Specifically, we propose a model-based strategy for chassis locomotion which provides a base for arm policy. We introduce a physics-informed ``IL+RL'' training framework for learning-based arm policy. In this train framework, a model-based strategy with privileged information is used to guide arm policy training during both IL and RL phases. In addition, we train the critic model during IL phase to alleviate the performance drop issue when transitioning from IL to RL. We present results on our self-engineered badminton robot, achieving 94.5% success rate against the serving machine and 90.7% success rate against human players. Our system can be easily generalized to other agile mobile manipulation tasks such as agile catching and table tennis. Our project website: https://dreamstarring.github.io/HAMLET/."
  },
  {
    "title": "Integrating Learning-Based Manipulation and Physics-Based Locomotion for Whole-Body Badminton Robot Control",
    "url": "http://arxiv.org/abs/2504.17771v2",
    "arxiv_id": "2504.17771v2",
    "authors": [
      "Haochen Wang",
      "Zhiwei Shi",
      "Chengxi Zhu",
      "Yafei Qiao",
      "Cheng Zhang",
      "Fan Yang",
      "Pengjie Ren",
      "Lan Lu",
      "Dong Xuan"
    ],
    "published": "2025-04-24T17:46:29+00:00",
    "summary": "Learning-based methods, such as imitation learning (IL) and reinforcement learning (RL), can produce excel control policies over challenging agile robot tasks, such as sports robot. However, no existing work has harmonized learning-based policy with model-based methods to reduce training complexity and ensure the safety and stability for agile badminton robot control. In this paper, we introduce Hamlet, a novel hybrid control system for agile badminton robots. Specifically, we propose a model-based strategy for chassis locomotion which provides a base for arm policy. We introduce a physics-informed \"IL+RL\" training framework for learning-based arm policy. In this train framework, a model-based strategy with privileged information is used to guide arm policy training during both IL and RL phases. In addition, we train the critic model during IL phase to alleviate the performance drop issue when transitioning from IL to RL. We present results on our self-engineered badminton robot, achieving 94.5% success rate against the serving machine and 90.7% success rate against human players. Our system can be easily generalized to other agile mobile manipulation tasks such as agile catching and table tennis. Our project website: https://dreamstarring.github.io/HAMLET/."
  },
  {
    "title": "Conformal Segmentation in Industrial Surface Defect Detection with Statistical Guarantees",
    "url": "http://arxiv.org/abs/2504.17721v1",
    "arxiv_id": "2504.17721v1",
    "authors": [
      "Cheng Shen",
      "Yuewei Liu"
    ],
    "published": "2025-04-24T16:33:56+00:00",
    "summary": "In industrial settings, surface defects on steel can significantly compromise its service life and elevate potential safety risks. Traditional defect detection methods predominantly rely on manual inspection, which suffers from low efficiency and high costs. Although automated defect detection approaches based on Convolutional Neural Networks(e.g., Mask R-CNN) have advanced rapidly, their reliability remains challenged due to data annotation uncertainties during deep model training and overfitting issues. These limitations may lead to detection deviations when processing the given new test samples, rendering automated detection processes unreliable. To address this challenge, we first evaluate the detection model's practical performance through calibration data that satisfies the independent and identically distributed (i.i.d) condition with test data. Specifically, we define a loss function for each calibration sample to quantify detection error rates, such as the complement of recall rate and false discovery rate. Subsequently, we derive a statistically rigorous threshold based on a user-defined risk level to identify high-probability defective pixels in test images, thereby constructing prediction sets (e.g., defect regions). This methodology ensures that the expected error rate (mean error rate) on the test set remains strictly bounced by the predefined risk level. Additionally, we observe a negative correlation between the average prediction set size and the risk level on the test set, establishing a statistically rigorous metric for assessing detection model uncertainty. Furthermore, our study demonstrates robust and efficient control over the expected test set error rate across varying calibration-to-test partitioning ratios, validating the method's adaptability and operational effectiveness."
  },
  {
    "title": "Safety in Large Reasoning Models: A Survey",
    "url": "http://arxiv.org/abs/2504.17704v1",
    "arxiv_id": "2504.17704v1",
    "authors": [
      "Cheng Wang",
      "Yue Liu",
      "Baolong Li",
      "Duzhen Zhang",
      "Zhongzhi Li",
      "Junfeng Fang"
    ],
    "published": "2025-04-24T16:11:01+00:00",
    "summary": "Large Reasoning Models (LRMs) have exhibited extraordinary prowess in tasks like mathematics and coding, leveraging their advanced reasoning capabilities. Nevertheless, as these capabilities progress, significant concerns regarding their vulnerabilities and safety have arisen, which can pose challenges to their deployment and application in real-world settings. This paper presents a comprehensive survey of LRMs, meticulously exploring and summarizing the newly emerged safety risks, attacks, and defense strategies. By organizing these elements into a detailed taxonomy, this work aims to offer a clear and structured understanding of the current safety landscape of LRMs, facilitating future research and development to enhance the security and reliability of these powerful models."
  },
  {
    "title": "Using mathematical models of heart cells to assess the safety of new pharmaceutical drugs",
    "url": "http://arxiv.org/abs/2504.17694v1",
    "arxiv_id": "2504.17694v1",
    "authors": [
      "Gary R. Mirams"
    ],
    "published": "2025-04-24T16:03:06+00:00",
    "summary": "Many drugs have been withdrawn from the market worldwide, at a cost of billions of dollars, because of patient fatalities due to them unexpectedly disturbing heart rhythm. Even drugs for ailments as mild as hay fever have been withdrawn due to an unacceptable increase in risk of these heart rhythm disturbances. Consequently, the whole pharmaceutical industry expends a huge effort in checking all new drugs for any unwanted side effects on the heart. The predominant root cause has been identified as drug molecules blocking ionic current flows in the heart. Block of individual types of ionic currents can now be measured experimentally at an early stage of drug development, and this is the standard screening approach for a number of ion currents in many large pharmaceutical companies. However, clinical risk is a complex function of the degree of block of many different types of cardiac ion currents, and this is difficult to understand by looking at results of these screens independently. By using ordinary differential equation models for the electrical activity of heart cells (electrophysiology models) we can integrate information from different types of currents, to predict the effect on whole heart cells and subsequent risk of side effects. The resulting simulations can provide a more accurate summary of the risk of a drug earlier in development and hence more cheaply than the pre-existing approaches."
  },
  {
    "title": "Data-Driven Calibration of Prediction Sets in Large Vision-Language Models Based on Inductive Conformal Prediction",
    "url": "http://arxiv.org/abs/2504.17671v1",
    "arxiv_id": "2504.17671v1",
    "authors": [
      "Yuanchang Ye",
      "Weiyan Wen"
    ],
    "published": "2025-04-24T15:39:46+00:00",
    "summary": "This study addresses the critical challenge of hallucination mitigation in Large Vision-Language Models (LVLMs) for Visual Question Answering (VQA) tasks through a Split Conformal Prediction (SCP) framework. While LVLMs excel in multi-modal reasoning, their outputs often exhibit hallucinated content with high confidence, posing risks in safety-critical applications. We propose a model-agnostic uncertainty quantification method that integrates dynamic threshold calibration and cross-modal consistency verification. By partitioning data into calibration and test sets, the framework computes nonconformity scores to construct prediction sets with statistical guarantees under user-defined risk levels ($\\alpha$). Key innovations include: (1) rigorous control of \\textbf{marginal coverage} to ensure empirical error rates remain strictly below $\\alpha$; (2) dynamic adjustment of prediction set sizes inversely with $\\alpha$, filtering low-confidence outputs; (3) elimination of prior distribution assumptions and retraining requirements. Evaluations on benchmarks (ScienceQA, MMMU) with eight LVLMs demonstrate that SCP enforces theoretical guarantees across all $\\alpha$ values. The framework achieves stable performance across varying calibration-to-test split ratios, underscoring its robustness for real-world deployment in healthcare, autonomous systems, and other safety-sensitive domains. This work bridges the gap between theoretical reliability and practical applicability in multi-modal AI systems, offering a scalable solution for hallucination detection and uncertainty-aware decision-making."
  },
  {
    "title": "Data-Driven Calibration of Prediction Sets in Large Vision-Language Models Based on Inductive Conformal Prediction",
    "url": "http://arxiv.org/abs/2504.17671v2",
    "arxiv_id": "2504.17671v2",
    "authors": [
      "Yuanchang Ye",
      "Weiyan Wen"
    ],
    "published": "2025-04-24T15:39:46+00:00",
    "summary": "This study addresses the critical challenge of hallucination mitigation in Large Vision-Language Models (LVLMs) for Visual Question Answering (VQA) tasks through a Split Conformal Prediction (SCP) framework. While LVLMs excel in multi-modal reasoning, their outputs often exhibit hallucinated content with high confidence, posing risks in safety-critical applications. We propose a model-agnostic uncertainty quantification method that integrates dynamic threshold calibration and cross-modal consistency verification. By partitioning data into calibration and test sets, the framework computes nonconformity scores to construct prediction sets with statistical guarantees under user-defined risk levels ($\\alpha$). Key innovations include: (1) rigorous control of \\textbf{marginal coverage} to ensure empirical error rates remain strictly below $\\alpha$; (2) dynamic adjustment of prediction set sizes inversely with $\\alpha$, filtering low-confidence outputs; (3) elimination of prior distribution assumptions and retraining requirements. Evaluations on benchmarks (ScienceQA, MMMU) with eight LVLMs demonstrate that SCP enforces theoretical guarantees across all $\\alpha$ values. The framework achieves stable performance across varying calibration-to-test split ratios, underscoring its robustness for real-world deployment in healthcare, autonomous systems, and other safety-sensitive domains. This work bridges the gap between theoretical reliability and practical applicability in multi-modal AI systems, offering a scalable solution for hallucination detection and uncertainty-aware decision-making."
  },
  {
    "title": "Unifying Complementarity Constraints and Control Barrier Functions for Safe Whole-Body Robot Control",
    "url": "http://arxiv.org/abs/2504.17647v1",
    "arxiv_id": "2504.17647v1",
    "authors": [
      "Rafael I. Cabral Muchacho",
      "Riddhiman Laha",
      "Florian T. Pokorny",
      "Luis F. C. Figueredo",
      "Nilanjan Chakraborty"
    ],
    "published": "2025-04-24T15:17:26+00:00",
    "summary": "Safety-critical whole-body robot control demands reactive methods that ensure collision avoidance in real-time. Complementarity constraints and control barrier functions (CBF) have emerged as core tools for ensuring such safety constraints, and each represents a well-developed field. Despite addressing similar problems, their connection remains largely unexplored. This paper bridges this gap by formally proving the equivalence between these two methodologies for sampled-data, first-order systems, considering both single and multiple constraint scenarios. By demonstrating this equivalence, we provide a unified perspective on these techniques. This unification has theoretical and practical implications, facilitating the cross-application of robustness guarantees and algorithmic improvements between complementarity and CBF frameworks. We discuss these synergistic benefits and motivate future work in the comparison of the methods in more general cases."
  },
  {
    "title": "Portability of Optimizations from SC to TSO",
    "url": "http://arxiv.org/abs/2504.17646v1",
    "arxiv_id": "2504.17646v1",
    "authors": [
      "Akshay Gopalakrishnan",
      "Clark Verbrugge"
    ],
    "published": "2025-04-24T15:16:17+00:00",
    "summary": "It is well recognized that the safety of compiler optimizations is at risk in a concurrent context. Existing approaches primarily rely on context-free thread-local guarantees, and prohibit optimizations that introduce a data-race. However, compilers utilize global context-specific information, exposing safe optimizations that may violate such guarantees as well as introduce a race. Such optimizations need to individually be proven safe for each language model. An alternate approach to this would be proving them safe for an intuitive model (like interleaving semantics), and then determine their portability across other concurrent models. In this paper, we address this problem of porting across models of concurrency. We first identify a global guarantee on optimizations portable from Sequential Consistency (SC) to Total Store Order (TSO). Our guarantee is in the form of constraints specifying the syntactic changes an optimization must not incur. We then show these constraints correlate to prohibiting the introduction of triangular races, a subset of data-race relevant to TSO. We conclude by showing how such race inducing optimizations relate to porting across Strong Release Acquire (SRA), a known causally consistent memory model."
  },
  {
    "title": "Safe to Stay: Psychological Safety Sustains Participation in Pull-based Open Source Projects",
    "url": "http://arxiv.org/abs/2504.17510v1",
    "arxiv_id": "2504.17510v1",
    "authors": [
      "Emeralda Sesari",
      "Federica Sarro",
      "Ayushi Rastogi"
    ],
    "published": "2025-04-24T12:54:30+00:00",
    "summary": "Psychological safety is the belief that team members can speak up or make mistakes without fear of negative consequences. While it is recognized as important in traditional software teams, its role in open-source development remains understudied. Yet, open-source contributors often collaborate without formal roles or structures, where interpersonal relationship can make or break participation. In this study, we examine whether team-level psychological safety, inferred from code review activities, is associated with contributors' continued participation in open-source projects. Code review is a central and collaborative activity in modern software development, which offers a rich context for observing team interactions. Based on 60,684 pull requests, we construct a psychological safety index using cues such as merge decisions, comment activity, interaction diversity, and mentions. We analyze its relationship with contributors' short-term (after 1 year) and long-term (after 4-5 years) sustained participation using three logistic regression models. Our findings show that contributors are more likely to remain active in repositories with higher levels of psychological safety. Psychological safety is positively associated with both short-term and future sustained participation. However, when prior participation is included, it becomes the stronger predictor of future sustained participation, while the effect of psychological safety becomes smaller. This study introduces a scalable approach to study psychological safety through pull request data and provides new evidence that it matters in open-source development."
  },
  {
    "title": "Assessing the Capability of Large Language Models for Domain-Specific Ontology Generation",
    "url": "http://arxiv.org/abs/2504.17402v1",
    "arxiv_id": "2504.17402v1",
    "authors": [
      "Anna Sofia Lippolis",
      "Mohammad Javad Saeedizade",
      "Robin Keskisarkka",
      "Aldo Gangemi",
      "Eva Blomqvist",
      "Andrea Giovanni Nuzzolese"
    ],
    "published": "2025-04-24T09:47:14+00:00",
    "summary": "Large Language Models (LLMs) have shown significant potential for ontology engineering. However, it is still unclear to what extent they are applicable to the task of domain-specific ontology generation. In this study, we explore the application of LLMs for automated ontology generation and evaluate their performance across different domains. Specifically, we investigate the generalizability of two state-of-the-art LLMs, DeepSeek and o1-preview, both equipped with reasoning capabilities, by generating ontologies from a set of competency questions (CQs) and related user stories. Our experimental setup comprises six distinct domains carried out in existing ontology engineering projects and a total of 95 curated CQs designed to test the models' reasoning for ontology engineering. Our findings show that with both LLMs, the performance of the experiments is remarkably consistent across all domains, indicating that these methods are capable of generalizing ontology generation tasks irrespective of the domain. These results highlight the potential of LLM-based approaches in achieving scalable and domain-agnostic ontology construction and lay the groundwork for further research into enhancing automated reasoning and knowledge representation techniques."
  },
  {
    "title": "Highly Accurate and Diverse Traffic Data: The DeepScenario Open 3D Dataset",
    "url": "http://arxiv.org/abs/2504.17371v1",
    "arxiv_id": "2504.17371v1",
    "authors": [
      "Oussema Dhaouadi",
      "Johannes Meier",
      "Luca Wahl",
      "Jacques Kaiser",
      "Luca Scalerandi",
      "Nick Wandelburg",
      "Zhuolun Zhou",
      "Nijanthan Berinpanathan",
      "Holger Banzhaf",
      "Daniel Cremers"
    ],
    "published": "2025-04-24T08:43:48+00:00",
    "summary": "Accurate 3D trajectory data is crucial for advancing autonomous driving. Yet, traditional datasets are usually captured by fixed sensors mounted on a car and are susceptible to occlusion. Additionally, such an approach can precisely reconstruct the dynamic environment in the close vicinity of the measurement vehicle only, while neglecting objects that are further away. In this paper, we introduce the DeepScenario Open 3D Dataset (DSC3D), a high-quality, occlusion-free dataset of 6 degrees of freedom bounding box trajectories acquired through a novel monocular camera drone tracking pipeline. Our dataset includes more than 175,000 trajectories of 14 types of traffic participants and significantly exceeds existing datasets in terms of diversity and scale, containing many unprecedented scenarios such as complex vehicle-pedestrian interaction on highly populated urban streets and comprehensive parking maneuvers from entry to exit. DSC3D dataset was captured in five various locations in Europe and the United States and include: a parking lot, a crowded inner-city, a steep urban intersection, a federal highway, and a suburban intersection. Our 3D trajectory dataset aims to enhance autonomous driving systems by providing detailed environmental 3D representations, which could lead to improved obstacle interactions and safety. We demonstrate its utility across multiple applications including motion prediction, motion planning, scenario mining, and generative reactive traffic agents. Our interactive online visualization platform and the complete dataset are publicly available at app.deepscenario.com, facilitating research in motion prediction, behavior modeling, and safety validation."
  },
  {
    "title": "Highly Accurate and Diverse Traffic Data: The DeepScenario Open 3D Dataset",
    "url": "http://arxiv.org/abs/2504.17371v2",
    "arxiv_id": "2504.17371v2",
    "authors": [
      "Oussema Dhaouadi",
      "Johannes Meier",
      "Luca Wahl",
      "Jacques Kaiser",
      "Luca Scalerandi",
      "Nick Wandelburg",
      "Zhuolun Zhou",
      "Nijanthan Berinpanathan",
      "Holger Banzhaf",
      "Daniel Cremers"
    ],
    "published": "2025-04-24T08:43:48+00:00",
    "summary": "Accurate 3D trajectory data is crucial for advancing autonomous driving. Yet, traditional datasets are usually captured by fixed sensors mounted on a car and are susceptible to occlusion. Additionally, such an approach can precisely reconstruct the dynamic environment in the close vicinity of the measurement vehicle only, while neglecting objects that are further away. In this paper, we introduce the DeepScenario Open 3D Dataset (DSC3D), a high-quality, occlusion-free dataset of 6 degrees of freedom bounding box trajectories acquired through a novel monocular camera drone tracking pipeline. Our dataset includes more than 175,000 trajectories of 14 types of traffic participants and significantly exceeds existing datasets in terms of diversity and scale, containing many unprecedented scenarios such as complex vehicle-pedestrian interaction on highly populated urban streets and comprehensive parking maneuvers from entry to exit. DSC3D dataset was captured in five various locations in Europe and the United States and include: a parking lot, a crowded inner-city, a steep urban intersection, a federal highway, and a suburban intersection. Our 3D trajectory dataset aims to enhance autonomous driving systems by providing detailed environmental 3D representations, which could lead to improved obstacle interactions and safety. We demonstrate its utility across multiple applications including motion prediction, motion planning, scenario mining, and generative reactive traffic agents. Our interactive online visualization platform and the complete dataset are publicly available at https://app.deepscenario.com, facilitating research in motion prediction, behavior modeling, and safety validation."
  },
  {
    "title": "AUTHENTICATION: Identifying Rare Failure Modes in Autonomous Vehicle Perception Systems using Adversarially Guided Diffusion Models",
    "url": "http://arxiv.org/abs/2504.17179v1",
    "arxiv_id": "2504.17179v1",
    "authors": [
      "Mohammad Zarei",
      "Melanie A Jutras",
      "Eliana Evans",
      "Mike Tan",
      "Omid Aaramoon"
    ],
    "published": "2025-04-24T01:31:13+00:00",
    "summary": "Autonomous Vehicles (AVs) rely on artificial intelligence (AI) to accurately detect objects and interpret their surroundings. However, even when trained using millions of miles of real-world data, AVs are often unable to detect rare failure modes (RFMs). The problem of RFMs is commonly referred to as the \"long-tail challenge\", due to the distribution of data including many instances that are very rarely seen. In this paper, we present a novel approach that utilizes advanced generative and explainable AI techniques to aid in understanding RFMs. Our methods can be used to enhance the robustness and reliability of AVs when combined with both downstream model training and testing. We extract segmentation masks for objects of interest (e.g., cars) and invert them to create environmental masks. These masks, combined with carefully crafted text prompts, are fed into a custom diffusion model. We leverage the Stable Diffusion inpainting model guided by adversarial noise optimization to generate images containing diverse environments designed to evade object detection models and expose vulnerabilities in AI systems. Finally, we produce natural language descriptions of the generated RFMs that can guide developers and policymakers to improve the safety and reliability of AV systems."
  },
  {
    "title": "Opt-ODENet: A Neural ODE Framework with Differentiable QP Layers for Safe and Stable Control Design (longer version)",
    "url": "http://arxiv.org/abs/2504.17139v1",
    "arxiv_id": "2504.17139v1",
    "authors": [
      "Keyan Miao",
      "Liqun Zhao",
      "Han Wang",
      "Konstantinos Gatsis",
      "Antonis Papachristodoulou"
    ],
    "published": "2025-04-23T23:09:37+00:00",
    "summary": "Designing controllers that achieve task objectives while ensuring safety is a key challenge in control systems. This work introduces Opt-ODENet, a Neural ODE framework with a differentiable Quadratic Programming (QP) optimization layer to enforce constraints as hard requirements. Eliminating the reliance on nominal controllers or large datasets, our framework solves the optimal control problem directly using Neural ODEs. Stability and convergence are ensured through Control Lyapunov Functions (CLFs) in the loss function, while Control Barrier Functions (CBFs) embedded in the QP layer enforce real-time safety. By integrating the differentiable QP layer with Neural ODEs, we demonstrate compatibility with the adjoint method for gradient computation, enabling the learning of the CBF class-$\\mathcal{K}$ function and control network parameters. Experiments validate its effectiveness in balancing safety and performance."
  },
  {
    "title": "Steering the CensorShip: Uncovering Representation Vectors for LLM \"Thought\" Control",
    "url": "http://arxiv.org/abs/2504.17130v1",
    "arxiv_id": "2504.17130v1",
    "authors": [
      "Hannah Cyberey",
      "David Evans"
    ],
    "published": "2025-04-23T22:47:30+00:00",
    "summary": "Large language models (LLMs) have transformed the way we access information. These models are often tuned to refuse to comply with requests that are considered harmful and to produce responses that better align with the preferences of those who control the models. To understand how this \"censorship\" works. We use representation engineering techniques to study open-weights safety-tuned models. We present a method for finding a refusal--compliance vector that detects and controls the level of censorship in model outputs. We also analyze recent reasoning LLMs, distilled from DeepSeek-R1, and uncover an additional dimension of censorship through \"thought suppression\". We show a similar approach can be used to find a vector that suppresses the model's reasoning process, allowing us to remove censorship by applying the negative multiples of this vector"
  },
  {
    "title": "Steering the CensorShip: Uncovering Representation Vectors for LLM \"Thought\" Control",
    "url": "http://arxiv.org/abs/2504.17130v2",
    "arxiv_id": "2504.17130v2",
    "authors": [
      "Hannah Cyberey",
      "David Evans"
    ],
    "published": "2025-04-23T22:47:30+00:00",
    "summary": "Large language models (LLMs) have transformed the way we access information. These models are often tuned to refuse to comply with requests that are considered harmful and to produce responses that better align with the preferences of those who control the models. To understand how this \"censorship\" works. We use representation engineering techniques to study open-weights safety-tuned models. We present a method for finding a refusal--compliance vector that detects and controls the level of censorship in model outputs. We also analyze recent reasoning LLMs, distilled from DeepSeek-R1, and uncover an additional dimension of censorship through \"thought suppression\". We show a similar approach can be used to find a vector that suppresses the model's reasoning process, allowing us to remove censorship by applying the negative multiples of this vector. Our code is publicly available at: https://github.com/hannahxchen/llm-censorship-steering"
  },
  {
    "title": "Peer-Aware Cost Estimation in Nonlinear General-Sum Dynamic Games for Mutual Learning and Intent Inference",
    "url": "http://arxiv.org/abs/2504.17129v1",
    "arxiv_id": "2504.17129v1",
    "authors": [
      "Seyed Yousef Soltanian",
      "Wenlong Zhang"
    ],
    "published": "2025-04-23T22:47:20+00:00",
    "summary": "Human-robot interactions can be modeled as incomplete-information general-sum dynamic games since the objective functions of both agents are not explicitly known to each other. However, solving for equilibrium policies for such games presents a major challenge, especially if the games involve nonlinear underlying dynamics. To simplify the problem, existing work often assumes that one agent is an expert with complete information about its peer, which can lead to biased estimates and failures in coordination. To address this challenge, we propose a nonlinear peer-aware cost estimation (N-PACE) algorithm for general-sum dynamic games. In N-PACE, using iterative linear quadratic (LQ) approximation of the nonlinear general-sum game, each agent explicitly models the learning dynamics of its peer agent while inferring their objective functions, leading to unbiased fast learning in inferring the unknown objective function of the peer agent, which is critical for task completion and safety assurance. Additionally, we demonstrate how N-PACE enables \\textbf{intent communication} in such multi-agent systems by explicitly modeling the peer's learning dynamics."
  },
  {
    "title": "Discovering the Precursors of Traffic Breakdowns Using Spatiotemporal Graph Attribution Networks",
    "url": "http://arxiv.org/abs/2504.17109v1",
    "arxiv_id": "2504.17109v1",
    "authors": [
      "Zhaobin Mo",
      "Xiangyi Liao",
      "Dominik A. Karbowski",
      "Yanbing Wang"
    ],
    "published": "2025-04-23T21:40:23+00:00",
    "summary": "Understanding and predicting the precursors of traffic breakdowns is critical for improving road safety and traffic flow management. This paper presents a novel approach combining spatiotemporal graph neural networks (ST-GNNs) with Shapley values to identify and interpret traffic breakdown precursors. By extending Shapley explanation methods to a spatiotemporal setting, our proposed method bridges the gap between black-box neural network predictions and interpretable causes. We demonstrate the method on the Interstate-24 data, and identify that road topology and abrupt braking are major factors that lead to traffic breakdowns."
  },
  {
    "title": "Safety Pretraining: Toward the Next Generation of Safe AI",
    "url": "http://arxiv.org/abs/2504.16980v1",
    "arxiv_id": "2504.16980v1",
    "authors": [
      "Pratyush Maini",
      "Sachin Goyal",
      "Dylan Sam",
      "Alex Robey",
      "Yash Savani",
      "Yiding Jiang",
      "Andy Zou",
      "Zacharcy C. Lipton",
      "J. Zico Kolter"
    ],
    "published": "2025-04-23T17:58:08+00:00",
    "summary": "As large language models (LLMs) are increasingly deployed in high-stakes settings, the risk of generating harmful or toxic content remains a central challenge. Post-hoc alignment methods are brittle: once unsafe patterns are learned during pretraining, they are hard to remove. We present a data-centric pretraining framework that builds safety into the model from the start. Our contributions include: (i) a safety classifier trained on 10,000 GPT-4 labeled examples, used to filter 600B tokens; (ii) the largest synthetic safety dataset to date (100B tokens) generated via recontextualization of harmful web data; (iii) RefuseWeb and Moral Education datasets that convert harmful prompts into refusal dialogues and web-style educational material; (iv) Harmfulness-Tag annotations injected during pretraining to flag unsafe content and steer away inference from harmful generations; and (v) safety evaluations measuring base model behavior before instruction tuning. Our safety-pretrained models reduce attack success rates from 38.8% to 8.4% with no performance degradation on standard LLM safety benchmarks."
  },
  {
    "title": "Meta-Learning Online Dynamics Model Adaptation in Off-Road Autonomous Driving",
    "url": "http://arxiv.org/abs/2504.16923v1",
    "arxiv_id": "2504.16923v1",
    "authors": [
      "Jacob Levy",
      "Jason Gibson",
      "Bogdan Vlahov",
      "Erica Tevere",
      "Evangelos Theodorou",
      "David Fridovich-Keil",
      "Patrick Spieler"
    ],
    "published": "2025-04-23T17:51:36+00:00",
    "summary": "High-speed off-road autonomous driving presents unique challenges due to complex, evolving terrain characteristics and the difficulty of accurately modeling terrain-vehicle interactions. While dynamics models used in model-based control can be learned from real-world data, they often struggle to generalize to unseen terrain, making real-time adaptation essential. We propose a novel framework that combines a Kalman filter-based online adaptation scheme with meta-learned parameters to address these challenges. Offline meta-learning optimizes the basis functions along which adaptation occurs, as well as the adaptation parameters, while online adaptation dynamically adjusts the onboard dynamics model in real time for model-based control. We validate our approach through extensive experiments, including real-world testing on a full-scale autonomous off-road vehicle, demonstrating that our method outperforms baseline approaches in prediction accuracy, performance, and safety metrics, particularly in safety-critical scenarios. Our results underscore the effectiveness of meta-learned dynamics model adaptation, advancing the development of reliable autonomous systems capable of navigating diverse and unseen environments. Video is available at: https://youtu.be/cCKHHrDRQEA"
  },
  {
    "title": "Learning Verifiable Control Policies Using Relaxed Verification",
    "url": "http://arxiv.org/abs/2504.16879v1",
    "arxiv_id": "2504.16879v1",
    "authors": [
      "Puja Chaudhury",
      "Alexander Estornell",
      "Michael Everett"
    ],
    "published": "2025-04-23T16:54:35+00:00",
    "summary": "To provide safety guarantees for learning-based control systems, recent work has developed formal verification methods to apply after training ends. However, if the trained policy does not meet the specifications, or there is conservatism in the verification algorithm, establishing these guarantees may not be possible. Instead, this work proposes to perform verification throughout training to ultimately aim for policies whose properties can be evaluated throughout runtime with lightweight, relaxed verification algorithms. The approach is to use differentiable reachability analysis and incorporate new components into the loss function. Numerical experiments on a quadrotor model and unicycle model highlight the ability of this approach to lead to learned control policies that satisfy desired reach-avoid and invariance specifications."
  },
  {
    "title": "Hybrid Reinforcement Learning and Model Predictive Control for Adaptive Control of Hydrogen-Diesel Dual-Fuel Combustion",
    "url": "http://arxiv.org/abs/2504.16875v1",
    "arxiv_id": "2504.16875v1",
    "authors": [
      "Julian Bedei",
      "Murray McBain",
      "Charles Robert Koch",
      "Jakob Andert",
      "David Gordon"
    ],
    "published": "2025-04-23T16:51:49+00:00",
    "summary": "Reinforcement Learning (RL) and Machine Learning Integrated Model Predictive Control (ML-MPC) are promising approaches for optimizing hydrogen-diesel dual-fuel engine control, as they can effectively control multiple-input multiple-output systems and nonlinear processes. ML-MPC is advantageous for providing safe and optimal controls, ensuring the engine operates within predefined safety limits. In contrast, RL is distinguished by its adaptability to changing conditions through its learning-based approach. However, the practical implementation of either method alone poses challenges. RL requires high variance in control inputs during early learning phases, which can pose risks to the system by potentially executing unsafe actions, leading to mechanical damage. Conversely, ML-MPC relies on an accurate system model to generate optimal control inputs and has limited adaptability to system drifts, such as injector aging, which naturally occur in engine applications. To address these limitations, this study proposes a hybrid RL and ML-MPC approach that uses an ML-MPC framework while incorporating an RL agent to dynamically adjust the ML-MPC load tracking reference in response to changes in the environment. At the same time, the ML-MPC ensures that actions stay safe throughout the RL agent's exploration. To evaluate the effectiveness of this approach, fuel pressure is deliberately varied to introduce a model-plant mismatch between the ML-MPC and the engine test bench. The result of this mismatch is a root mean square error (RMSE) in indicated mean effective pressure of 0.57 bar when running the ML-MPC. The experimental results demonstrate that RL successfully adapts to changing boundary conditions by altering the tracking reference while ML-MPC ensures safe control inputs. The quantitative improvement in load tracking by implementing RL is an RSME of 0.44 bar."
  },
  {
    "title": "Improving Significant Wave Height Prediction Using Chronos Models",
    "url": "http://arxiv.org/abs/2504.16834v1",
    "arxiv_id": "2504.16834v1",
    "authors": [
      "Yilin Zhai",
      "Hongyuan Shi",
      "Chao Zhan",
      "Qing Wang",
      "Zaijin You",
      "Nan Wang"
    ],
    "published": "2025-04-23T15:56:28+00:00",
    "summary": "Accurate wave height prediction is critical for maritime safety and coastal resilience, yet conventional physics-based models and traditional machine learning methods face challenges in computational efficiency and nonlinear dynamics modeling. This study introduces Chronos, the first implementation of a large language model (LLM)-powered temporal architecture (Chronos) optimized for wave forecasting. Through advanced temporal pattern recognition applied to historical wave data from three strategically chosen marine zones in the Northwest Pacific basin, our framework achieves multimodal improvements: (1) 14.3% reduction in training time with 2.5x faster inference speed compared to PatchTST baselines, achieving 0.575 mean absolute scaled error (MASE) units; (2) superior short-term forecasting (1-24h) across comprehensive metrics; (3) sustained predictive leadership in extended-range forecasts (1-120h); and (4) demonstrated zero-shot capability maintaining median performance (rank 4/12) against specialized operational models. This LLM-enhanced temporal modeling paradigm establishes a new standard in wave prediction, offering both computationally efficient solutions and a transferable framework for complex geophysical systems modeling."
  },
  {
    "title": "Improving Significant Wave Height Prediction Using Chronos Models",
    "url": "http://arxiv.org/abs/2504.16834v2",
    "arxiv_id": "2504.16834v2",
    "authors": [
      "Yilin Zhai",
      "Hongyuan Shi",
      "Chao Zhan",
      "Qing Wang",
      "Zaijin You",
      "Nan Wang"
    ],
    "published": "2025-04-23T15:56:28+00:00",
    "summary": "Accurate wave height prediction is critical for maritime safety and coastal resilience, yet conventional physics-based models and traditional machine learning methods face challenges in computational efficiency and nonlinear dynamics modeling. This study introduces Chronos, the first implementation of a large language model (LLM)-powered temporal architecture (Chronos) optimized for wave forecasting. Through advanced temporal pattern recognition applied to historical wave data from three strategically chosen marine zones in the Northwest Pacific basin, our framework achieves multimodal improvements: (1) 14.3% reduction in training time with 2.5x faster inference speed compared to PatchTST baselines, achieving 0.575 mean absolute scaled error (MASE) units; (2) superior short-term forecasting (1-24h) across comprehensive metrics; (3) sustained predictive leadership in extended-range forecasts (1-120h); and (4) demonstrated zero-shot capability maintaining median performance (rank 4/12) against specialized operational models. This LLM-enhanced temporal modeling paradigm establishes a new standard in wave prediction, offering both computationally efficient solutions and a transferable framework for complex geophysical systems modeling."
  },
  {
    "title": "Association-Based Track-Before-Detect with Object Contribution Probabilities",
    "url": "http://arxiv.org/abs/2504.16814v1",
    "arxiv_id": "2504.16814v1",
    "authors": [
      "Thomas Kropfreiter",
      "Jason L. Williams",
      "Florian Meyer"
    ],
    "published": "2025-04-23T15:32:11+00:00",
    "summary": "Multiobject tracking provides situational awareness that enables new applications for modern convenience, applied ocean sciences, public safety, and homeland security. In many multiobject tracking applications, including radar and sonar tracking, after coherent prefiltering of the received signal, measurement data is typically structured in cells, where each cell represent, e.g., a different range and bearing value. While conventional detect-then-track (DTT) multiobject tracking approaches convert the cell-structured data within a detection phase into so-called point measurements in order to reduce the amount of data, track-before-detect (TBD) methods process the cell-structured data directly, avoiding a potential information loss. However, many TBD tracking methods are computationally intensive and achieve a reduced tracking accuracy when objects interact, i.e., when they come into close proximity. We here counteract these difficulties by introducing the concept of probabilistic object-to-cell contributions. As many conventional DTT methods, our approach uses a probabilistic association of objects with data cells, and a new object contribution model with corresponding object contribution probabilities to further associate cell contributions to objects that occupy the same data cell. Furthermore, to keep the computational complexity and filter runtimes low, we here use an efficient Poisson multi-Bernoulli filtering approach in combination with the application of belief propagation for fast probabilistic data association. We demonstrate numerically that our method achieves significantly increased tracking performance compared to state-of-the-art TBD tracking approaches, where performance differences are particularly pronounced when multiple objects interact."
  },
  {
    "title": "Evaluating the Impact of CT-to-RED Calibration Curves on Dosimetric Accuracy in Brain Radiotherapy Dose Distribution",
    "url": "http://arxiv.org/abs/2504.16805v1",
    "arxiv_id": "2504.16805v1",
    "authors": [
      "Islam G. Ali",
      "Wael M. Daabis",
      "Hossam Donya"
    ],
    "published": "2025-04-23T15:24:57+00:00",
    "summary": "Accurate dose calculation is crucial in radiotherapy, as tissue relative electron densities (RED) derived from CT scans play a vital role. This study investigated the impact of different CT-to-RED calibration curves on brain cancer treatment plans. Three calibration curves were compared: CIRS phantom-derived, Catphan phantom-derived, and the default curve in the Monaco Treatment Planning System. Ten volumetric modulated arc therapy (VMAT) plans were generated and recalculated using each curve. Dosimetric parameters for Planning Target Volume (PTV) and Organs at Risk (OARs) were analyzed. Results showed significant differences in PTV dose distribution between the CIRS-derived and default curves, while no significant differences were found between Catphan-derived and default curves. The CIRS-derived curve demonstrated superior performance in representing brain tissue electron densities. These findings emphasize the importance of using site-specific CT-to-RED calibration curves for accurate dose calculations in brain radiotherapy, potentially improving treatment safety and efficacy"
  },
  {
    "title": "Evaluating the Impact of CT-to-RED Calibration Curves on Dosimetric Accuracy in Brain Radiotherapy Dose Distribution",
    "url": "http://arxiv.org/abs/2504.16805v2",
    "arxiv_id": "2504.16805v2",
    "authors": [
      "Hossam Donya",
      "Duong Thanh Tai",
      "Islam G. Ali"
    ],
    "published": "2025-04-23T15:24:57+00:00",
    "summary": "Accurate dose calculation is crucial in radiotherapy, as tissue relative electron densities (RED) derived from CT scans play a vital role. This study investigated the impact of different CT-to-RED calibration curves on brain cancer treatment plans. Three calibration curves were compared: CIRS phantom-derived, Catphan phantom-derived, and the default curve in the Monaco Treatment Planning System. Ten volumetric modulated arc therapy (VMAT) plans were generated and recalculated using each curve. Dosimetric parameters for Planning Target Volume (PTV) and Organs at Risk (OARs) were analyzed. Results showed significant differences in PTV dose distribution between the CIRS-derived and default curves, while no significant differences were found between Catphan-derived and default curves. The CIRS-derived curve demonstrated superior performance in representing brain tissue electron densities. These findings emphasize the importance of using site-specific CT-to-RED calibration curves for accurate dose calculations in brain radiotherapy, potentially improving treatment safety and efficacy"
  },
  {
    "title": "Reduction of $\u03b5$-expanded Feynman integrals",
    "url": "http://arxiv.org/abs/2504.16766v1",
    "arxiv_id": "2504.16766v1",
    "authors": [
      "Yan-Qing Ma",
      "Cong-Hao Qin",
      "Ao Tan",
      "Kai Yan"
    ],
    "published": "2025-04-23T14:34:34+00:00",
    "summary": "Since Feynman integrals (FIs) at higher spacetime dimensions are free of infrared and collinear divergence--and their ultraviolet divergences can be systematically subtracted--this allows us to construct a wide range of locally finite Feynman integrals. Especially, we propose a method named $\\bar{R}$-operation to subtract out ultraviolet divergences that at the same time preserves infrared and collinear safety of the original FI. By expressing these locally finite FIs in terms of master integrals and imposing constraints on their $\\epsilon$-expanded forms, we reduce the $\\epsilon$-expanded master integrals to a minimal basis. We provide an automated package to identify such constraints, offering a tool useful for high-order perturbative computations."
  },
  {
    "title": "Frequency-Compensated Network for Daily Arctic Sea Ice Concentration Prediction",
    "url": "http://arxiv.org/abs/2504.16745v1",
    "arxiv_id": "2504.16745v1",
    "authors": [
      "Jialiang Zhang",
      "Feng Gao",
      "Yanhai Gan",
      "Junyu Dong",
      "Qian Du"
    ],
    "published": "2025-04-23T14:15:48+00:00",
    "summary": "Accurately forecasting sea ice concentration (SIC) in the Arctic is critical to global ecosystem health and navigation safety. However, current methods still is confronted with two challenges: 1) these methods rarely explore the long-term feature dependencies in the frequency domain. 2) they can hardly preserve the high-frequency details, and the changes in the marginal area of the sea ice cannot be accurately captured. To this end, we present a Frequency-Compensated Network (FCNet) for Arctic SIC prediction on a daily basis. In particular, we design a dual-branch network, including branches for frequency feature extraction and convolutional feature extraction. For frequency feature extraction, we design an adaptive frequency filter block, which integrates trainable layers with Fourier-based filters. By adding frequency features, the FCNet can achieve refined prediction of edges and details. For convolutional feature extraction, we propose a high-frequency enhancement block to separate high and low-frequency information. Moreover, high-frequency features are enhanced via channel-wise attention, and temporal attention unit is employed for low-frequency feature extraction to capture long-range sea ice changes. Extensive experiments are conducted on a satellite-derived daily SIC dataset, and the results verify the effectiveness of the proposed FCNet. Our codes and data will be made public available at: https://github.com/oucailab/FCNet ."
  },
  {
    "title": "DYNUS: Uncertainty-aware Trajectory Planner in Dynamic Unknown Environments",
    "url": "http://arxiv.org/abs/2504.16734v1",
    "arxiv_id": "2504.16734v1",
    "authors": [
      "Kota Kondo",
      "Mason Peterson",
      "Nicholas Rober",
      "Juan Rached Viso",
      "Lucas Jia",
      "Jialin Chen",
      "Harvey Merton",
      "Jonathan P. How"
    ],
    "published": "2025-04-23T14:05:04+00:00",
    "summary": "This paper introduces DYNUS, an uncertainty-aware trajectory planner designed for dynamic unknown environments. Operating in such settings presents many challenges -- most notably, because the agent cannot predict the ground-truth future paths of obstacles, a previously planned trajectory can become unsafe at any moment, requiring rapid replanning to avoid collisions.   Recently developed planners have used soft-constraint approaches to achieve the necessary fast computation times; however, these methods do not guarantee collision-free paths even with static obstacles. In contrast, hard-constraint methods ensure collision-free safety, but typically have longer computation times.   To address these issues, we propose three key contributions. First, the DYNUS Global Planner (DGP) and Temporal Safe Corridor Generation operate in spatio-temporal space and handle both static and dynamic obstacles in the 3D environment. Second, the Safe Planning Framework leverages a combination of exploratory, safe, and contingency trajectories to flexibly re-route when potential future collisions with dynamic obstacles are detected. Finally, the Fast Hard-Constraint Local Trajectory Formulation uses a variable elimination approach to reduce the problem size and enable faster computation by pre-computing dependencies between free and dependent variables while still ensuring collision-free trajectories.   We evaluated DYNUS in a variety of simulations, including dense forests, confined office spaces, cave systems, and dynamic environments. Our experiments show that DYNUS achieves a success rate of 100% and travel times that are approximately 25.0% faster than state-of-the-art methods. We also evaluated DYNUS on multiple platforms -- a quadrotor, a wheeled robot, and a quadruped -- in both simulation and hardware experiments."
  },
  {
    "title": "DYNUS: Uncertainty-aware Trajectory Planner in Dynamic Unknown Environments",
    "url": "http://arxiv.org/abs/2504.16734v2",
    "arxiv_id": "2504.16734v2",
    "authors": [
      "Kota Kondo",
      "Mason Peterson",
      "Nicholas Rober",
      "Juan Rached Viso",
      "Lucas Jia",
      "Jialin Chen",
      "Harvey Merton",
      "Jonathan P. How"
    ],
    "published": "2025-04-23T14:05:04+00:00",
    "summary": "This paper introduces DYNUS, an uncertainty-aware trajectory planner designed for dynamic unknown environments. Operating in such settings presents many challenges -- most notably, because the agent cannot predict the ground-truth future paths of obstacles, a previously planned trajectory can become unsafe at any moment, requiring rapid replanning to avoid collisions.   Recently developed planners have used soft-constraint approaches to achieve the necessary fast computation times; however, these methods do not guarantee collision-free paths even with static obstacles. In contrast, hard-constraint methods ensure collision-free safety, but typically have longer computation times.   To address these issues, we propose three key contributions. First, the DYNUS Global Planner (DGP) and Temporal Safe Corridor Generation operate in spatio-temporal space and handle both static and dynamic obstacles in the 3D environment. Second, the Safe Planning Framework leverages a combination of exploratory, safe, and contingency trajectories to flexibly re-route when potential future collisions with dynamic obstacles are detected. Finally, the Fast Hard-Constraint Local Trajectory Formulation uses a variable elimination approach to reduce the problem size and enable faster computation by pre-computing dependencies between free and dependent variables while still ensuring collision-free trajectories.   We evaluated DYNUS in a variety of simulations, including dense forests, confined office spaces, cave systems, and dynamic environments. Our experiments show that DYNUS achieves a success rate of 100% and travel times that are approximately 25.0% faster than state-of-the-art methods. We also evaluated DYNUS on multiple platforms -- a quadrotor, a wheeled robot, and a quadruped -- in both simulation and hardware experiments."
  },
  {
    "title": "Offline Robotic World Model: Learning Robotic Policies without a Physics Simulator",
    "url": "http://arxiv.org/abs/2504.16680v1",
    "arxiv_id": "2504.16680v1",
    "authors": [
      "Chenhao Li",
      "Andreas Krause",
      "Marco Hutter"
    ],
    "published": "2025-04-23T12:58:15+00:00",
    "summary": "Reinforcement Learning (RL) has demonstrated impressive capabilities in robotic control but remains challenging due to high sample complexity, safety concerns, and the sim-to-real gap. While offline RL eliminates the need for risky real-world exploration by learning from pre-collected data, it suffers from distributional shift, limiting policy generalization. Model-Based RL (MBRL) addresses this by leveraging predictive models for synthetic rollouts, yet existing approaches often lack robust uncertainty estimation, leading to compounding errors in offline settings. We introduce Offline Robotic World Model (RWM-O), a model-based approach that explicitly estimates epistemic uncertainty to improve policy learning without reliance on a physics simulator. By integrating these uncertainty estimates into policy optimization, our approach penalizes unreliable transitions, reducing overfitting to model errors and enhancing stability. Experimental results show that RWM-O improves generalization and safety, enabling policy learning purely from real-world data and advancing scalable, data-efficient RL for robotics."
  },
  {
    "title": "3D-1D modelling of cranial plate heating induced by low or medium frequency magnetic fields",
    "url": "http://arxiv.org/abs/2504.16600v1",
    "arxiv_id": "2504.16600v1",
    "authors": [
      "Alessandro Arduino",
      "Oriano Bottauscio",
      "Denise Grappein",
      "Stefano Scial\u00f3",
      "Fabio Vicini",
      "Umberto Zanovello",
      "Luca Zilberti"
    ],
    "published": "2025-04-23T10:29:53+00:00",
    "summary": "Safety assessment of patients with one-dimensionally structured passive implants, like cranial plates or stents, exposed to low or medium frequency magnetic fields, like those generated in magnetic resonance imaging or magnetic hyperthermia, can be challenging, because of the different length scales of the implant and the human body. Most of the methods used to estimate the heating induced near such implants neglect the presence of the metallic materials within the body, modeling the metal as thermal seeds. To overcome this limitation, a novel numerical approach that solves three-dimensional and one-dimensional coupled problems is proposed. This method leads to improved results by modelling the thermal diffusion through the highly conductive metallic implants. A comparison of the proposed method predictions with measurements performed on a cranial plate exposed to the magnetic field generated by a gradient coil system for magnetic resonance imaging is presented, showing an improved accuracy up to 25 % with respect to the method based on thermal seeds. The proposed method is finally applied to a magnetic hyperthermia case study in which a patient with a cranial plate is exposed to the magnetic field generated by a collar-type magnetic hyperthermia applicator for neck tumour treatment, predicting a temperature increase in proximity of the implant that is 10 % lower than the one overestimated by relying on thermal seeds."
  },
  {
    "title": "Streetscape Analysis with Generative AI (SAGAI): Vision-Language Assessment and Mapping of Urban Scenes",
    "url": "http://arxiv.org/abs/2504.16538v1",
    "arxiv_id": "2504.16538v1",
    "authors": [
      "Joan Perez",
      "Giovanni Fusco"
    ],
    "published": "2025-04-23T09:08:06+00:00",
    "summary": "Streetscapes are an essential component of urban space. Their assessment is presently either limited to morphometric properties of their mass skeleton or requires labor-intensive qualitative evaluations of visually perceived qualities. This paper introduces SAGAI: Streetscape Analysis with Generative Artificial Intelligence, a modular workflow for scoring street-level urban scenes using open-access data and vision-language models. SAGAI integrates OpenStreetMap geometries, Google Street View imagery, and a lightweight version of the LLaVA model to generate structured spatial indicators from images via customizable natural language prompts. The pipeline includes an automated mapping module that aggregates visual scores at both the point and street levels, enabling direct cartographic interpretation. It operates without task-specific training or proprietary software dependencies, supporting scalable and interpretable analysis of urban environments. Two exploratory case studies in Nice and Vienna illustrate SAGAI's capacity to produce geospatial outputs from vision-language inference. The initial results show strong performance for binary urban-rural scene classification, moderate precision in commercial feature detection, and lower estimates, but still informative, of sidewalk width. Fully deployable by any user, SAGAI can be easily adapted to a wide range of urban research themes, such as walkability, safety, or urban design, through prompt modification alone."
  },
  {
    "title": "SafeSpect: Safety-First Augmented Reality Heads-up Display for Drone Inspections",
    "url": "http://arxiv.org/abs/2504.16533v1",
    "arxiv_id": "2504.16533v1",
    "authors": [
      "Peisen Xu",
      "J\u00e9r\u00e9mie Garcia",
      "Wei Tsang Ooi",
      "Christophe Jouffrais"
    ],
    "published": "2025-04-23T08:59:05+00:00",
    "summary": "Current tablet-based interfaces for drone operations often impose a heavy cognitive load on pilots and reduce situational awareness by dividing attention between the video feed and the real world. To address these challenges, we designed a heads-up augmented reality (AR) interface that overlays in-situ information to support drone pilots in safety-critical tasks. Through participatory design workshops with professional pilots, we identified key features and developed an adaptive AR interface that dynamically switches between task and safety views to prevent information overload. We evaluated our prototype by creating a realistic building inspection task and comparing three interfaces: a 2D tablet, a static AR, and our adaptive AR design. A user study with 15 participants showed that the AR interface improved access to safety information, while the adaptive AR interface reduced cognitive load and enhanced situational awareness without compromising task performance. We offer design insights for developing safety-first heads-up AR interfaces."
  },
  {
    "title": "Amplified Vulnerabilities: Structured Jailbreak Attacks on LLM-based Multi-Agent Debate",
    "url": "http://arxiv.org/abs/2504.16489v1",
    "arxiv_id": "2504.16489v1",
    "authors": [
      "Senmao Qi",
      "Yifei Zou",
      "Peng Li",
      "Ziyi Lin",
      "Xiuzhen Cheng",
      "Dongxiao Yu"
    ],
    "published": "2025-04-23T08:01:50+00:00",
    "summary": "Multi-Agent Debate (MAD), leveraging collaborative interactions among Large Language Models (LLMs), aim to enhance reasoning capabilities in complex tasks. However, the security implications of their iterative dialogues and role-playing characteristics, particularly susceptibility to jailbreak attacks eliciting harmful content, remain critically underexplored. This paper systematically investigates the jailbreak vulnerabilities of four prominent MAD frameworks built upon leading commercial LLMs (GPT-4o, GPT-4, GPT-3.5-turbo, and DeepSeek) without compromising internal agents. We introduce a novel structured prompt-rewriting framework specifically designed to exploit MAD dynamics via narrative encapsulation, role-driven escalation, iterative refinement, and rhetorical obfuscation. Our extensive experiments demonstrate that MAD systems are inherently more vulnerable than single-agent setups. Crucially, our proposed attack methodology significantly amplifies this fragility, increasing average harmfulness from 28.14% to 80.34% and achieving attack success rates as high as 80% in certain scenarios. These findings reveal intrinsic vulnerabilities in MAD architectures and underscore the urgent need for robust, specialized defenses prior to real-world deployment."
  },
  {
    "title": "The Dodecacopter: a Versatile Multirotor System of Dodecahedron-Shaped Modules",
    "url": "http://arxiv.org/abs/2504.16475v1",
    "arxiv_id": "2504.16475v1",
    "authors": [
      "K\u00e9vin Garanger",
      "Thanakorn Khamvilai",
      "Jeremy Epps",
      "Eric Feron"
    ],
    "published": "2025-04-23T07:38:00+00:00",
    "summary": "With the promise of greater safety and adaptability, modular reconfigurable uncrewed air vehicles have been proposed as unique, versatile platforms holding the potential to replace multiple types of monolithic vehicles at once. State-of-the-art rigidly assembled modular vehicles are generally two-dimensional configurations in which the rotors are coplanar and assume the shape of a \"flight array\". We introduce the Dodecacopter, a new type of modular rotorcraft where all modules take the shape of a regular dodecahedron, allowing the creation of richer sets of configurations beyond flight arrays. In particular, we show how the chosen module design can be used to create three-dimensional and fully actuated configurations. We justify the relevance of these types of configurations in terms of their structural and actuation properties with various performance indicators. Given the broad range of configurations and capabilities that can be achieved with our proposed design, we formulate tractable optimization programs to find optimal configurations given structural and actuation constraints. Finally, a prototype of such a vehicle is presented along with results of performed flights in multiple configurations."
  },
  {
    "title": "ERASER: Efficient RTL FAult Simulation Framework with Trimmed Execution Redundancy",
    "url": "http://arxiv.org/abs/2504.16473v1",
    "arxiv_id": "2504.16473v1",
    "authors": [
      "Jiaping Tang",
      "Jianan Mu",
      "Silin Liu",
      "Zizhen Liu",
      "Feng Gu",
      "Xinyu Zhang",
      "Leyan Wang",
      "Shenwen Liang",
      "Jing Ye",
      "Huawei Li",
      "Xiaowei Li"
    ],
    "published": "2025-04-23T07:33:44+00:00",
    "summary": "As intelligent computing devices increasingly integrate into human life, ensuring the functional safety of the corresponding electronic chips becomes more critical. A key metric for functional safety is achieving a sufficient fault coverage. To meet this requirement, extensive time-consuming fault simulation of the RTL code is necessary during the chip design phase.The main overhead in RTL fault simulation comes from simulating behavioral nodes (always blocks). Due to the limited fault propagation capacity, fault simulation results often match the good simulation results for many behavioral nodes. A key strategy for accelerating RTL fault simulation is the identification and elimination of redundant simulations. Existing methods detect redundant executions by examining whether the fault inputs to each RTL node are consistent with the good inputs. However, we observe that this input comparison mechanism overlooks a significant amount of implicit redundant execution: although the fault inputs differ from the good inputs, the node's execution results remain unchanged. Our experiments reveal that this overlooked redundant execution constitutes nearly half of the total execution overhead of behavioral nodes, becoming a significant bottleneck in current RTL fault simulation. The underlying reason for this overlooked redundancy is that, in these cases, the true execution paths within the behavioral nodes are not affected by the changes in input values. In this work, we propose a behavior-level redundancy detection algorithm that focuses on the true execution paths. Building on the elimination of redundant executions, we further developed an efficient RTL fault simulation framework, Eraser.Experimental results show that compared to commercial tools, under the same fault coverage, our framework achieves a 3.9 $\\times$ improvement in simulation performance on average."
  },
  {
    "title": "iTFKAN: Interpretable Time Series Forecasting with Kolmogorov-Arnold Network",
    "url": "http://arxiv.org/abs/2504.16432v1",
    "arxiv_id": "2504.16432v1",
    "authors": [
      "Ziran Liang",
      "Rui An",
      "Wenqi Fan",
      "Yanghui Rao",
      "Yuxuan Liang"
    ],
    "published": "2025-04-23T05:34:49+00:00",
    "summary": "As time evolves, data within specific domains exhibit predictability that motivates time series forecasting to predict future trends from historical data. However, current deep forecasting methods can achieve promising performance but generally lack interpretability, hindering trustworthiness and practical deployment in safety-critical applications such as auto-driving and healthcare. In this paper, we propose a novel interpretable model, iTFKAN, for credible time series forecasting. iTFKAN enables further exploration of model decision rationales and underlying data patterns due to its interpretability achieved through model symbolization. Besides, iTFKAN develops two strategies, prior knowledge injection, and time-frequency synergy learning, to effectively guide model learning under complex intertwined time series data. Extensive experimental results demonstrated that iTFKAN can achieve promising forecasting performance while simultaneously possessing high interpretive capabilities."
  },
  {
    "title": "On Validating Angular Power Spectral Models for the Stochastic Gravitational-Wave Background Without Distributional Assumptions",
    "url": "http://arxiv.org/abs/2504.16959v1",
    "arxiv_id": "2504.16959v1",
    "authors": [
      "Xiangyu Zhang",
      "Erik Floden",
      "Hongru Zhao",
      "Sara Algeri",
      "Galin Jones",
      "Vuk Mandic",
      "Jesse Miller"
    ],
    "published": "2025-04-23T05:03:39+00:00",
    "summary": "It is demonstrated that estimators of the angular power spectrum commonly used for the stochastic gravitational-wave background (SGWB) lack a closed-form analytical expression for the likelihood function and, typically, cannot be accurately approximated by a Gaussian likelihood. Nevertheless, a robust statistical analysis can be performed by extending the framework outlined in \\cite{PRL} to enable the estimation and testing of angular power spectral models for the SGWB without specifying distributional assumptions. Here, the technical aspects of the method are discussed in detail. Moreover, a new, consistent estimator for the covariance of the angular power spectrum is derived. The proposed approach is applied to data from the third observing run (O3) of Advanced LIGO and Advanced Virgo."
  },
  {
    "title": "Anytime Safe Reinforcement Learning",
    "url": "http://arxiv.org/abs/2504.16417v1",
    "arxiv_id": "2504.16417v1",
    "authors": [
      "Pol Mestres",
      "Arnau Marzabal",
      "Jorge Cort\u00e9s"
    ],
    "published": "2025-04-23T04:51:31+00:00",
    "summary": "This paper considers the problem of solving constrained   reinforcement learning problems with anytime guarantees, meaning   that the algorithmic solution returns a safe policy regardless of   when it is terminated. Drawing inspiration from anytime constrained   optimization, we introduce Reinforcement Learning-based Safe   Gradient Flow (RL-SGF), an on-policy algorithm which employs   estimates of the value functions and their respective gradients   associated with the objective and safety constraints for the current   policy, and updates the policy parameters by solving a convex   quadratically constrained quadratic program. We show that if the   estimates are computed with a sufficiently large number of episodes   (for which we provide an explicit bound), safe policies are updated   to safe policies with a probability higher than a prescribed   tolerance. We also show that iterates asymptotically converge to a   neighborhood of a KKT point, whose size can be arbitrarily reduced   by refining the estimates of the value function and their gradients.   We illustrate the performance of RL-SGF in a navigation example."
  },
  {
    "title": "SILM: A Subjective Intent Based Low-Latency Framework for Multiple Traffic Participants Joint Trajectory Prediction",
    "url": "http://arxiv.org/abs/2504.16377v1",
    "arxiv_id": "2504.16377v1",
    "authors": [
      "Qu Weiming",
      "Wang Jia",
      "Du Jiawei",
      "Zhu Yuanhao",
      "Yu Jianfeng",
      "Xia Rui",
      "Cao Song",
      "Wu Xihong",
      "Luo Dingsheng"
    ],
    "published": "2025-04-23T02:56:34+00:00",
    "summary": "Trajectory prediction is a fundamental technology for advanced autonomous driving systems and represents one of the most challenging problems in the field of cognitive intelligence. Accurately predicting the future trajectories of each traffic participant is a prerequisite for building high safety and high reliability decision-making, planning, and control capabilities in autonomous driving. However, existing methods often focus solely on the motion of other traffic participants without considering the underlying intent behind that motion, which increases the uncertainty in trajectory prediction. Autonomous vehicles operate in real-time environments, meaning that trajectory prediction algorithms must be able to process data and generate predictions in real-time. While many existing methods achieve high accuracy, they often struggle to effectively handle heterogeneous traffic scenarios. In this paper, we propose a Subjective Intent-based Low-latency framework for Multiple traffic participants joint trajectory prediction. Our method explicitly incorporates the subjective intent of traffic participants based on their key points, and predicts the future trajectories jointly without map, which ensures promising performance while significantly reducing the prediction latency. Additionally, we introduce a novel dataset designed specifically for trajectory prediction. Related code and dataset will be available soon."
  },
  {
    "title": "The Safety-Privacy Tradeoff in Linear Bandits",
    "url": "http://arxiv.org/abs/2504.16371v1",
    "arxiv_id": "2504.16371v1",
    "authors": [
      "Arghavan Zibaie",
      "Spencer Hutchinson",
      "Ramtin Pedarsani",
      "Mahnoosh Alizadeh"
    ],
    "published": "2025-04-23T02:48:02+00:00",
    "summary": "We consider a collection of linear stochastic bandit problems, each modeling the random response of different agents to proposed interventions, coupled together by a global safety constraint. We assume a central coordinator must choose actions to play on each bandit with the objective of regret minimization, while also ensuring that the expected response of all agents satisfies the global safety constraints at each round, in spite of uncertainty about the bandits' parameters. The agents consider their observed responses to be private and in order to protect their sensitive information, the data sharing with the central coordinator is performed under local differential privacy (LDP). However, providing higher level of privacy to different agents would have consequences in terms of safety and regret. We formalize these tradeoffs by building on the notion of the sharpness of the safety set - a measure of how the geometric properties of the safe set affects the growth of regret - and propose a unilaterally unimprovable vector of privacy levels for different agents given a maximum regret budget."
  },
  {
    "title": "VeriFix: Verifying Your Fix Towards An Atomicity Violation",
    "url": "http://arxiv.org/abs/2504.16354v1",
    "arxiv_id": "2504.16354v1",
    "authors": [
      "Zhuang Li",
      "Qiuping Yi",
      "Jeff Huang"
    ],
    "published": "2025-04-23T02:11:07+00:00",
    "summary": "Atomicity violation is one of the most serious types of bugs in concurrent programs. Synchronizations are commonly used to enforce atomicity. However, it is very challenging to place synchronizations correctly and sufficiently   due to complex thread interactions and large input space. This paper presents \\textsf{VeriFix}, a new approach for verifying atomicity violation fixes. Given a buggy trace that exposes an atomicity violation and a corresponding fix, % in the form of locks, \\textsf{VeriFix} effectively verifies if the fix introduces sufficient synchronizations to repair the atomicity violation without introducing new deadlocks. The key idea is that \\textsf{VeriFix} transforms the fix verification problem into a property verification problem, in which both the observed atomicity violation and potential deadlocks are encoded as a safety property, and both the inputs and schedules are encoded as symbolic constraints. By reasoning the conjoined constraints with an SMT solver, \\textsf{VeriFix} systematically explores all reachable paths %from the whole schedule and input space and verifies if there exists a concrete \\textit{schedule+input} combination to manifest the intended atomicity or any new deadlocks. We have implemented and evaluated \\verifix\\ on a collection of real-world C/C++ programs. The result shows that \\textsf{VeriFix} significantly outperforms the state-of-the-art."
  },
  {
    "title": "Impact of Noise on LLM-Models Performance in Abstraction and Reasoning Corpus (ARC) Tasks with Model Temperature Considerations",
    "url": "http://arxiv.org/abs/2504.15903v1",
    "arxiv_id": "2504.15903v1",
    "authors": [
      "Nikhil Khandalkar",
      "Pavan Yadav",
      "Krishna Shinde",
      "Lokesh B. Ramegowda",
      "Rajarshi Das"
    ],
    "published": "2025-04-22T13:43:58+00:00",
    "summary": "Recent advancements in Large Language Models (LLMs) have generated growing interest in their structured reasoning capabilities, particularly in tasks involving abstraction and pattern recognition. The Abstraction and Reasoning Corpus (ARC) benchmark plays a crucial role in evaluating these capabilities by testing how well AI models generalize to novel problems. While GPT-4o demonstrates strong performance by solving all ARC tasks under zero-noise conditions, other models like DeepSeek R1 and LLaMA 3.2 fail to solve any, suggesting limitations in their ability to reason beyond simple pattern matching. To explore this gap, we systematically evaluate these models across different noise levels and temperature settings. Our results reveal that the introduction of noise consistently impairs model performance, regardless of architecture. This decline highlights a shared vulnerability: current LLMs, despite showing signs of abstract reasoning, remain highly sensitive to input perturbations. Such fragility raises concerns about their real-world applicability, where noise and uncertainty are common. By comparing how different model architectures respond to these challenges, we offer insights into the structural weaknesses of modern LLMs in reasoning tasks. This work underscores the need for developing more robust and adaptable AI systems capable of handling the ambiguity and variability inherent in real-world scenarios. Our findings aim to guide future research toward enhancing model generalization, robustness, and alignment with human-like cognitive flexibility."
  },
  {
    "title": "Impact of Noise on LLM-Models Performance in Abstraction and Reasoning Corpus (ARC) Tasks with Model Temperature Considerations",
    "url": "http://arxiv.org/abs/2504.15903v2",
    "arxiv_id": "2504.15903v2",
    "authors": [
      "Nikhil Khandalkar",
      "Pavan Yadav",
      "Krishna Shinde",
      "Lokesh B. Ramegowda",
      "Rajarshi Das"
    ],
    "published": "2025-04-22T13:43:58+00:00",
    "summary": "Recent advancements in Large Language Models (LLMs) have generated growing interest in their structured reasoning capabilities, particularly in tasks involving abstraction and pattern recognition. The Abstraction and Reasoning Corpus (ARC) benchmark plays a crucial role in evaluating these capabilities by testing how well AI models generalize to novel problems. While GPT-4o demonstrates strong performance by solving all ARC tasks under zero-noise conditions, other models like DeepSeek R1 and LLaMA 3.2 fail to solve any, suggesting limitations in their ability to reason beyond simple pattern matching. To explore this gap, we systematically evaluate these models across different noise levels and temperature settings. Our results reveal that the introduction of noise consistently impairs model performance, regardless of architecture. This decline highlights a shared vulnerability: current LLMs, despite showing signs of abstract reasoning, remain highly sensitive to input perturbations. Such fragility raises concerns about their real-world applicability, where noise and uncertainty are common. By comparing how different model architectures respond to these challenges, we offer insights into the structural weaknesses of modern LLMs in reasoning tasks. This work underscores the need for developing more robust and adaptable AI systems capable of handling the ambiguity and variability inherent in real-world scenarios. Our findings aim to guide future research toward enhancing model generalization, robustness, and alignment with human-like cognitive flexibility."
  },
  {
    "title": "SARI: Structured Audio Reasoning via Curriculum-Guided Reinforcement Learning",
    "url": "http://arxiv.org/abs/2504.15900v1",
    "arxiv_id": "2504.15900v1",
    "authors": [
      "Cheng Wen",
      "Tingwei Guo",
      "Shuaijiang Zhao",
      "Wei Zou",
      "Xiangang Li"
    ],
    "published": "2025-04-22T13:41:26+00:00",
    "summary": "Recent work shows that reinforcement learning(RL) can markedly sharpen the reasoning ability of large language models (LLMs) by prompting them to \"think before answering.\" Yet whether and how these gains transfer to audio-language reasoning remains largely unexplored. We extend the Group-Relative Policy Optimization (GRPO) framework from DeepSeek-R1 to a Large Audio-Language Model (LALM), and construct a 32k sample multiple-choice corpus. Using a two-stage regimen supervised fine-tuning on structured and unstructured chains-of-thought, followed by curriculum-guided GRPO, we systematically compare implicit vs. explicit, and structured vs. free form reasoning under identical architectures. Our structured audio reasoning model, SARI (Structured Audio Reasoning via Curriculum-Guided Reinforcement Learning), achieves a 16.35% improvement in average accuracy over the base model Qwen2-Audio-7B-Instruct. Furthermore, the variant built upon Qwen2.5-Omni reaches state-of-the-art performance of 67.08% on the MMAU test-mini benchmark. Ablation experiments show that on the base model we use: (i) SFT warm-up is important for stable RL training, (ii) structured chains yield more robust generalization than unstructured ones, and (iii) easy-to-hard curricula accelerate convergence and improve final performance. These findings demonstrate that explicit, structured reasoning and curriculum learning substantially enhances audio-language understanding."
  },
  {
    "title": "Dynamic Early Exit in Reasoning Models",
    "url": "http://arxiv.org/abs/2504.15895v1",
    "arxiv_id": "2504.15895v1",
    "authors": [
      "Chenxu Yang",
      "Qingyi Si",
      "Yongjie Duan",
      "Zheliang Zhu",
      "Chenyu Zhu",
      "Zheng Lin",
      "Li Cao",
      "Weiping Wang"
    ],
    "published": "2025-04-22T13:36:53+00:00",
    "summary": "Recent advances in large reasoning language models (LRLMs) rely on test-time scaling, which extends long chain-of-thought (CoT) generation to solve complex tasks. However, overthinking in long CoT not only slows down the efficiency of problem solving, but also risks accuracy loss due to the extremely detailed or redundant reasoning steps. We propose a simple yet effective method that allows LLMs to self-truncate CoT sequences by early exit during generation. Instead of relying on fixed heuristics, the proposed method monitors model behavior at potential reasoning transition points (e.g.,\"Wait\" tokens) and dynamically terminates the next reasoning chain's generation when the model exhibits high confidence in a trial answer. Our method requires no additional training and can be seamlessly integrated into existing o1-like reasoning LLMs. Experiments on multiple reasoning benchmarks MATH-500, AMC 2023, GPQA Diamond and AIME 2024 show that the proposed method is consistently effective on deepseek-series reasoning LLMs, reducing the length of CoT sequences by an average of 31% to 43% while improving accuracy by 1.7% to 5.7%."
  },
  {
    "title": "MS-Occ: Multi-Stage LiDAR-Camera Fusion for 3D Semantic Occupancy Prediction",
    "url": "http://arxiv.org/abs/2504.15888v1",
    "arxiv_id": "2504.15888v1",
    "authors": [
      "Zhiqiang Wei",
      "Lianqing Zheng",
      "Jianan Liu",
      "Tao Huang",
      "Qing-Long Han",
      "Wenwen Zhang",
      "Fengdeng Zhang"
    ],
    "published": "2025-04-22T13:33:26+00:00",
    "summary": "Accurate 3D semantic occupancy perception is essential for autonomous driving in complex environments with diverse and irregular objects. While vision-centric methods suffer from geometric inaccuracies, LiDAR-based approaches often lack rich semantic information. To address these limitations, MS-Occ, a novel multi-stage LiDAR-camera fusion framework which includes middle-stage fusion and late-stage fusion, is proposed, integrating LiDAR's geometric fidelity with camera-based semantic richness via hierarchical cross-modal fusion. The framework introduces innovations at two critical stages: (1) In the middle-stage feature fusion, the Gaussian-Geo module leverages Gaussian kernel rendering on sparse LiDAR depth maps to enhance 2D image features with dense geometric priors, and the Semantic-Aware module enriches LiDAR voxels with semantic context via deformable cross-attention; (2) In the late-stage voxel fusion, the Adaptive Fusion (AF) module dynamically balances voxel features across modalities, while the High Classification Confidence Voxel Fusion (HCCVF) module resolves semantic inconsistencies using self-attention-based refinement. Experiments on the nuScenes-OpenOccupancy benchmark show that MS-Occ achieves an Intersection over Union (IoU) of 32.1% and a mean IoU (mIoU) of 25.3%, surpassing the state-of-the-art by +0.7% IoU and +2.4% mIoU. Ablation studies further validate the contribution of each module, with substantial improvements in small-object perception, demonstrating the practical value of MS-Occ for safety-critical autonomous driving scenarios."
  },
  {
    "title": "Embedded Safe Reactive Navigation for Multirotors Systems using Control Barrier Functions",
    "url": "http://arxiv.org/abs/2504.15850v1",
    "arxiv_id": "2504.15850v1",
    "authors": [
      "Nazar Misyats",
      "Marvin Harms",
      "Morten Nissov",
      "Martin Jacquet",
      "Kostas Alexis"
    ],
    "published": "2025-04-22T12:45:11+00:00",
    "summary": "Aiming to promote the wide adoption of safety filters for autonomous aerial robots, this paper presents a safe control architecture designed for seamless integration into widely used open-source autopilots. Departing from methods that require consistent localization and mapping, we formalize the obstacle avoidance problem as a composite control barrier function constructed only from the online onboard range measurements. The proposed framework acts as a safety filter, modifying the acceleration references derived by the nominal position/velocity control loops, and is integrated into the PX4 autopilot stack. Experimental studies using a small multirotor aerial robot demonstrate the effectiveness and performance of the solution within dynamic maneuvering and unknown environments."
  },
  {
    "title": "TrustGeoGen: Scalable and Formal-Verified Data Engine for Trustworthy Multi-modal Geometric Problem Solving",
    "url": "http://arxiv.org/abs/2504.15780v1",
    "arxiv_id": "2504.15780v1",
    "authors": [
      "Daocheng Fu",
      "Zijun Chen",
      "Renqiu Xia",
      "Qi Liu",
      "Yuan Feng",
      "Hongbin Zhou",
      "Renrui Zhang",
      "Shiyang Feng",
      "Peng Gao",
      "Junchi Yan",
      "Botian Shi",
      "Bo Zhang",
      "Yu Qiao"
    ],
    "published": "2025-04-22T10:45:23+00:00",
    "summary": "Mathematical geometric problem solving (GPS) often requires effective integration of multimodal information and verifiable logical coherence. Despite the fast development of large language models in general problem solving, it remains unresolved regarding with both methodology and benchmarks, especially given the fact that exiting synthetic GPS benchmarks are often not self-verified and contain noise and self-contradicted information due to the illusion of LLMs. In this paper, we propose a scalable data engine called TrustGeoGen for problem generation, with formal verification to provide a principled benchmark, which we believe lays the foundation for the further development of methods for GPS. The engine synthesizes geometric data through four key innovations: 1) multimodal-aligned generation of diagrams, textual descriptions, and stepwise solutions; 2) formal verification ensuring rule-compliant reasoning paths; 3) a bootstrapping mechanism enabling complexity escalation via recursive state generation and 4) our devised GeoExplore series algorithms simultaneously produce multi-solution variants and self-reflective backtracking traces. By formal logical verification, TrustGeoGen produces GeoTrust-200K dataset with guaranteed modality integrity, along with GeoTrust-test testset. Experiments reveal the state-of-the-art models achieve only 49.17\\% accuracy on GeoTrust-test, demonstrating its evaluation stringency. Crucially, models trained on GeoTrust achieve OOD generalization on GeoQA, significantly reducing logical inconsistencies relative to pseudo-label annotated by OpenAI-o1. Our code is available at https://github.com/Alpha-Innovator/TrustGeoGen"
  },
  {
    "title": "Pose Optimization for Autonomous Driving Datasets using Neural Rendering Models",
    "url": "http://arxiv.org/abs/2504.15776v1",
    "arxiv_id": "2504.15776v1",
    "authors": [
      "Quentin Herau",
      "Nathan Piasco",
      "Moussab Bennehar",
      "Luis Rolado",
      "Dzmitry Tsishkou",
      "Bingbing Liu",
      "Cyrille Migniot",
      "Pascal Vasseur",
      "C\u00e9dric Demonceaux"
    ],
    "published": "2025-04-22T10:33:01+00:00",
    "summary": "Autonomous driving systems rely on accurate perception and localization of the ego car to ensure safety and reliability in challenging real-world driving scenarios. Public datasets play a vital role in benchmarking and guiding advancement in research by providing standardized resources for model development and evaluation. However, potential inaccuracies in sensor calibration and vehicle poses within these datasets can lead to erroneous evaluations of downstream tasks, adversely impacting the reliability and performance of the autonomous systems. To address this challenge, we propose a robust optimization method based on Neural Radiance Fields (NeRF) to refine sensor poses and calibration parameters, enhancing the integrity of dataset benchmarks. To validate improvement in accuracy of our optimized poses without ground truth, we present a thorough evaluation process, relying on reprojection metrics, Novel View Synthesis rendering quality, and geometric alignment. We demonstrate that our method achieves significant improvements in sensor pose accuracy. By optimizing these critical parameters, our approach not only improves the utility of existing datasets but also paves the way for more reliable autonomous driving models. To foster continued progress in this field, we make the optimized sensor poses publicly available, providing a valuable resource for the research community."
  },
  {
    "title": "Prediction of CO2 reduction reaction intermediates and products on transition metal-doped r-GeSe monolayers:A combined DFT and machine learning approach",
    "url": "http://arxiv.org/abs/2504.15710v1",
    "arxiv_id": "2504.15710v1",
    "authors": [
      "Xuxin Kang",
      "Wenjing Zhou",
      "Ziyuan Li",
      "Zhaoqin Chu",
      "Hanqin Yin",
      "Shan Gao",
      "Aijun Du",
      "Xiangmei Duan"
    ],
    "published": "2025-04-22T08:52:18+00:00",
    "summary": "The electrocatalytic CO2 reduction reaction (CO2RR) is a complex multi-proton-electron transfer process that generates a vast network of reaction intermediates. Accurate prediction of free energy changes (G) of these intermediates and products is essential for evaluating catalytic performance. We combined density functional theory (DFT) and machine learning (ML) to screen 25 single-atom catalysts (SACs) on defective r-GeSe monolayers for CO2 reduction to methanol, methane, and formic acid. Among nine ML models evaluated with 14 intrinsic and DFT-based features, the XGBoost performed best (R2 = 0.92 and MAE = 0.24 eV), aligning closely with DFT calculations and identifying Ni, Ru, and Rh@GeSe as prospective catalysts. Feature importance analysis in free energy and product predictions highlighted the significance of CO2 activation with O-C-O and IPC-O1 as the key attributes. Furthermore, by incorporating non-DFT-based features, rapid predictions became possible, and the XGBoost model retained its predictive performance with R2 = 0.89 and MAE = 0.29 eV. This accuracy was further validated using Ir@GeSe. Our work highlights effective SACs for CO2RR, and provides valuable insights for efficient catalyst design."
  },
  {
    "title": "Advancing Embodied Agent Security: From Safety Benchmarks to Input Moderation",
    "url": "http://arxiv.org/abs/2504.15699v1",
    "arxiv_id": "2504.15699v1",
    "authors": [
      "Ning Wang",
      "Zihan Yan",
      "Weiyang Li",
      "Chuan Ma",
      "He Chen",
      "Tao Xiang"
    ],
    "published": "2025-04-22T08:34:35+00:00",
    "summary": "Embodied agents exhibit immense potential across a multitude of domains, making the assurance of their behavioral safety a fundamental prerequisite for their widespread deployment. However, existing research predominantly concentrates on the security of general large language models, lacking specialized methodologies for establishing safety benchmarks and input moderation tailored to embodied agents. To bridge this gap, this paper introduces a novel input moderation framework, meticulously designed to safeguard embodied agents. This framework encompasses the entire pipeline, including taxonomy definition, dataset curation, moderator architecture, model training, and rigorous evaluation. Notably, we introduce EAsafetyBench, a meticulously crafted safety benchmark engineered to facilitate both the training and stringent assessment of moderators specifically designed for embodied agents. Furthermore, we propose Pinpoint, an innovative prompt-decoupled input moderation scheme that harnesses a masked attention mechanism to effectively isolate and mitigate the influence of functional prompts on moderation tasks. Extensive experiments conducted on diverse benchmark datasets and models validate the feasibility and efficacy of the proposed approach. The results demonstrate that our methodologies achieve an impressive average detection accuracy of 94.58%, surpassing the performance of existing state-of-the-art techniques, alongside an exceptional moderation processing time of merely 0.002 seconds per instance."
  },
  {
    "title": "Symbolic Runtime Verification and Adaptive Decision-Making for Robot-Assisted Dressing",
    "url": "http://arxiv.org/abs/2504.15666v1",
    "arxiv_id": "2504.15666v1",
    "authors": [
      "Yasmin Rafiq",
      "Gricel V\u00e1zquez",
      "Radu Calinescu",
      "Sanja Dogramadzi",
      "Robert M Hierons"
    ],
    "published": "2025-04-22T07:42:16+00:00",
    "summary": "We present a control framework for robot-assisted dressing that augments low-level hazard response with runtime monitoring and formal verification. A parametric discrete-time Markov chain (pDTMC) models the dressing process, while Bayesian inference dynamically updates this pDTMC's transition probabilities based on sensory and user feedback. Safety constraints from hazard analysis are expressed in probabilistic computation tree logic, and symbolically verified using a probabilistic model checker. We evaluate reachability, cost, and reward trade-offs for garment-snag mitigation and escalation, enabling real-time adaptation. Our approach provides a formal yet lightweight foundation for safety-aware, explainable robotic assistance."
  },
  {
    "title": "An ACO-MPC Framework for Energy-Efficient and Collision-Free Path Planning in Autonomous Maritime Navigation",
    "url": "http://arxiv.org/abs/2504.15611v1",
    "arxiv_id": "2504.15611v1",
    "authors": [
      "Yaoze Liu",
      "Zhen Tian",
      "Qifan Zhou",
      "Zixuan Huang",
      "Hongyu Sun"
    ],
    "published": "2025-04-22T06:09:54+00:00",
    "summary": "Automated driving on ramps presents significant challenges due to the need to balance both safety and efficiency during lane changes. This paper proposes an integrated planner for automated vehicles (AVs) on ramps, utilizing an unsatisfactory level metric for efficiency and arrow-cluster-based sampling for safety. The planner identifies optimal times for the AV to change lanes, taking into account the vehicle's velocity as a key factor in efficiency. Additionally, the integrated planner employs arrow-cluster-based sampling to evaluate collision risks and select an optimal lane-changing curve. Extensive simulations were conducted in a ramp scenario to verify the planner's efficient and safe performance. The results demonstrate that the proposed planner can effectively select an appropriate lane-changing time point and a safe lane-changing curve for AVs, without incurring any collisions during the maneuver."
  },
  {
    "title": "A Comprehensive Survey in LLM(-Agent) Full Stack Safety: Data, Training and Deployment",
    "url": "http://arxiv.org/abs/2504.15585v1",
    "arxiv_id": "2504.15585v1",
    "authors": [
      "Kun Wang",
      "Guibin Zhang",
      "Zhenhong Zhou",
      "Jiahao Wu",
      "Miao Yu",
      "Shiqian Zhao",
      "Chenlong Yin",
      "Jinhu Fu",
      "Yibo Yan",
      "Hanjun Luo",
      "Liang Lin",
      "Zhihao Xu",
      "Haolang Lu",
      "Xinye Cao",
      "Xinyun Zhou",
      "Weifei Jin",
      "Fanci Meng",
      "Junyuan Mao",
      "Hao Wu",
      "Minghe Wang",
      "Fan Zhang",
      "Junfeng Fang",
      "Chengwei Liu",
      "Yifan Zhang",
      "Qiankun Li",
      "Chongye Guo",
      "Yalan Qin",
      "Yi Ding",
      "Donghai Hong",
      "Jiaming Ji",
      "Xinfeng Li",
      "Yifan Jiang",
      "Dongxia Wang",
      "Yihao Huang",
      "Yufei Guo",
      "Jen-tse Huang",
      "Yanwei Yue",
      "Wenke Huang",
      "Guancheng Wan",
      "Tianlin Li",
      "Lei Bai",
      "Jie Zhang",
      "Qing Guo",
      "Jingyi Wang",
      "Tianlong Chen",
      "Joey Tianyi Zhou",
      "Xiaojun Jia",
      "Weisong Sun",
      "Cong Wu",
      "Jing Chen",
      "Xuming Hu",
      "Yiming Li",
      "Xiao Wang",
      "Ningyu Zhang",
      "Luu Anh Tuan",
      "Guowen Xu",
      "Tianwei Zhang",
      "Xingjun Ma",
      "Xiang Wang",
      "Bo An",
      "Jun Sun",
      "Mohit Bansal",
      "Shirui Pan",
      "Yuval Elovici",
      "Bhavya Kailkhura",
      "Bo Li",
      "Yaodong Yang",
      "Hongwei Li",
      "Wenyuan Xu",
      "Yizhou Sun",
      "Wei Wang",
      "Qing Li",
      "Ke Tang",
      "Yu-Gang Jiang",
      "Felix Juefei-Xu",
      "Hui Xiong",
      "Xiaofeng Wang",
      "Shuicheng Yan",
      "Dacheng Tao",
      "Philip S. Yu",
      "Qingsong Wen",
      "Yang Liu"
    ],
    "published": "2025-04-22T05:02:49+00:00",
    "summary": "The remarkable success of Large Language Models (LLMs) has illuminated a promising pathway toward achieving Artificial General Intelligence for both academic and industrial communities, owing to their unprecedented performance across various applications. As LLMs continue to gain prominence in both research and commercial domains, their security and safety implications have become a growing concern, not only for researchers and corporations but also for every nation. Currently, existing surveys on LLM safety primarily focus on specific stages of the LLM lifecycle, e.g., deployment phase or fine-tuning phase, lacking a comprehensive understanding of the entire \"lifechain\" of LLMs. To address this gap, this paper introduces, for the first time, the concept of \"full-stack\" safety to systematically consider safety issues throughout the entire process of LLM training, deployment, and eventual commercialization. Compared to the off-the-shelf LLM safety surveys, our work demonstrates several distinctive advantages: (I) Comprehensive Perspective. We define the complete LLM lifecycle as encompassing data preparation, pre-training, post-training, deployment and final commercialization. To our knowledge, this represents the first safety survey to encompass the entire lifecycle of LLMs. (II) Extensive Literature Support. Our research is grounded in an exhaustive review of over 800+ papers, ensuring comprehensive coverage and systematic organization of security issues within a more holistic understanding. (III) Unique Insights. Through systematic literature analysis, we have developed reliable roadmaps and perspectives for each chapter. Our work identifies promising research directions, including safety in data generation, alignment techniques, model editing, and LLM-based agent systems. These insights provide valuable guidance for researchers pursuing future work in this field."
  },
  {
    "title": "RiskNet: Interaction-Aware Risk Forecasting for Autonomous Driving in Long-Tail Scenarios",
    "url": "http://arxiv.org/abs/2504.15541v1",
    "arxiv_id": "2504.15541v1",
    "authors": [
      "Qichao Liu",
      "Heye Huang",
      "Shiyue Zhao",
      "Lei Shi",
      "Soyoung Ahn",
      "Xiaopeng Li"
    ],
    "published": "2025-04-22T02:36:54+00:00",
    "summary": "Ensuring the safety of autonomous vehicles (AVs) in long-tail scenarios remains a critical challenge, particularly under high uncertainty and complex multi-agent interactions. To address this, we propose RiskNet, an interaction-aware risk forecasting framework, which integrates deterministic risk modeling with probabilistic behavior prediction for comprehensive risk assessment. At its core, RiskNet employs a field-theoretic model that captures interactions among ego vehicle, surrounding agents, and infrastructure via interaction fields and force. This model supports multidimensional risk evaluation across diverse scenarios (highways, intersections, and roundabouts), and shows robustness under high-risk and long-tail settings. To capture the behavioral uncertainty, we incorporate a graph neural network (GNN)-based trajectory prediction module, which learns multi-modal future motion distributions. Coupled with the deterministic risk field, it enables dynamic, probabilistic risk inference across time, enabling proactive safety assessment under uncertainty. Evaluations on the highD, inD, and rounD datasets, spanning lane changes, turns, and complex merges, demonstrate that our method significantly outperforms traditional approaches (e.g., TTC, THW, RSS, NC Field) in terms of accuracy, responsiveness, and directional sensitivity, while maintaining strong generalization across scenarios. This framework supports real-time, scenario-adaptive risk forecasting and demonstrates strong generalization across uncertain driving environments. It offers a unified foundation for safety-critical decision-making in long-tail scenarios."
  },
  {
    "title": "Towards Resilience and Autonomy-based Approaches for Adolescents Online Safety",
    "url": "http://arxiv.org/abs/2504.15533v1",
    "arxiv_id": "2504.15533v1",
    "authors": [
      "Jinkyung Park",
      "Mamtaj Akter",
      "Naima Samreen Ali",
      "Zainab Agha",
      "Ashwaq Alsoubai",
      "Pamela Wisniewski"
    ],
    "published": "2025-04-22T02:23:48+00:00",
    "summary": "In this position paper, we discuss the paradigm shift that has emerged in the literature, suggesting to move away from restrictive and authoritarian parental mediation approaches to move toward resilient-based and privacy-preserving solutions to promote adolescents' online safety. We highlight the limitations of restrictive mediation strategies, which often induce a trade-off between teens' privacy and online safety, and call for more teen-centric frameworks that can empower teens to self-regulate while using the technology in meaningful ways. We also present an overview of empirical studies that conceptualized and examined resilience-based approaches to promoting the digital well-being of teens in a way to empower teens to be more resilient."
  },
  {
    "title": "T2VShield: Model-Agnostic Jailbreak Defense for Text-to-Video Models",
    "url": "http://arxiv.org/abs/2504.15512v1",
    "arxiv_id": "2504.15512v1",
    "authors": [
      "Siyuan Liang",
      "Jiayang Liu",
      "Jiecheng Zhai",
      "Tianmeng Fang",
      "Rongcheng Tu",
      "Aishan Liu",
      "Xiaochun Cao",
      "Dacheng Tao"
    ],
    "published": "2025-04-22T01:18:42+00:00",
    "summary": "The rapid development of generative artificial intelligence has made text to video models essential for building future multimodal world simulators. However, these models remain vulnerable to jailbreak attacks, where specially crafted prompts bypass safety mechanisms and lead to the generation of harmful or unsafe content. Such vulnerabilities undermine the reliability and security of simulation based applications. In this paper, we propose T2VShield, a comprehensive and model agnostic defense framework designed to protect text to video models from jailbreak threats. Our method systematically analyzes the input, model, and output stages to identify the limitations of existing defenses, including semantic ambiguities in prompts, difficulties in detecting malicious content in dynamic video outputs, and inflexible model centric mitigation strategies. T2VShield introduces a prompt rewriting mechanism based on reasoning and multimodal retrieval to sanitize malicious inputs, along with a multi scope detection module that captures local and global inconsistencies across time and modalities. The framework does not require access to internal model parameters and works with both open and closed source systems. Extensive experiments on five platforms show that T2VShield can reduce jailbreak success rates by up to 35 percent compared to strong baselines. We further develop a human centered audiovisual evaluation protocol to assess perceptual safety, emphasizing the importance of visual level defense in enhancing the trustworthiness of next generation multimodal simulators."
  },
  {
    "title": "Automatically Detecting Numerical Instability in Machine Learning Applications via Soft Assertions",
    "url": "http://arxiv.org/abs/2504.15507v1",
    "arxiv_id": "2504.15507v1",
    "authors": [
      "Shaila Sharmin",
      "Anwar Hossain Zahid",
      "Subhankar Bhattacharjee",
      "Chiamaka Igwilo",
      "Miryung Kim",
      "Wei Le"
    ],
    "published": "2025-04-22T00:55:33+00:00",
    "summary": "Machine learning (ML) applications have become an integral part of our lives. ML applications extensively use floating-point computation and involve very large/small numbers; thus, maintaining the numerical stability of such complex computations remains an important challenge. Numerical bugs can lead to system crashes, incorrect output, and wasted computing resources. In this paper, we introduce a novel idea, namely soft assertions (SA), to encode safety/error conditions for the places where numerical instability can occur. A soft assertion is an ML model automatically trained using the dataset obtained during unit testing of unstable functions. Given the values at the unstable function in an ML application, a soft assertion reports how to change these values in order to trigger the instability. We then use the output of soft assertions as signals to effectively mutate inputs to trigger numerical instability in ML applications. In the evaluation, we used the GRIST benchmark, a total of 79 programs, as well as 15 real-world ML applications from GitHub. We compared our tool with 5 state-of-the-art (SOTA) fuzzers. We found all the GRIST bugs and outperformed the baselines. We found 13 numerical bugs in real-world code, one of which had already been confirmed by the GitHub developers. While the baselines mostly found the bugs that report NaN and INF, our tool \\tool found numerical bugs with incorrect output. We showed one case where the Tumor Detection Model, trained on Brain MRI images, should have predicted \"tumor\", but instead, it incorrectly predicted \"no tumor\" due to the numerical bugs. Our replication package is located at https://figshare.com/s/6528d21ccd28bea94c32."
  },
  {
    "title": "Automatically Detecting Numerical Instability in Machine Learning Applications via Soft Assertions",
    "url": "http://arxiv.org/abs/2504.15507v2",
    "arxiv_id": "2504.15507v2",
    "authors": [
      "Shaila Sharmin",
      "Anwar Hossain Zahid",
      "Subhankar Bhattacharjee",
      "Chiamaka Igwilo",
      "Miryung Kim",
      "Wei Le"
    ],
    "published": "2025-04-22T00:55:33+00:00",
    "summary": "Machine learning (ML) applications have become an integral part of our lives. ML applications extensively use floating-point computation and involve very large/small numbers; thus, maintaining the numerical stability of such complex computations remains an important challenge. Numerical bugs can lead to system crashes, incorrect output, and wasted computing resources. In this paper, we introduce a novel idea, namely soft assertions (SA), to encode safety/error conditions for the places where numerical instability can occur. A soft assertion is an ML model automatically trained using the dataset obtained during unit testing of unstable functions. Given the values at the unstable function in an ML application, a soft assertion reports how to change these values in order to trigger the instability. We then use the output of soft assertions as signals to effectively mutate inputs to trigger numerical instability in ML applications. In the evaluation, we used the GRIST benchmark, a total of 79 programs, as well as 15 real-world ML applications from GitHub. We compared our tool with 5 state-of-the-art (SOTA) fuzzers. We found all the GRIST bugs and outperformed the baselines. We found 13 numerical bugs in real-world code, one of which had already been confirmed by the GitHub developers. While the baselines mostly found the bugs that report NaN and INF, our tool \\tool found numerical bugs with incorrect output. We showed one case where the Tumor Detection Model, trained on Brain MRI images, should have predicted \"tumor\", but instead, it incorrectly predicted \"no tumor\" due to the numerical bugs. Our replication package is located at https://figshare.com/s/6528d21ccd28bea94c32."
  },
  {
    "title": "Nearly Optimal Nonlinear Safe Control with BaS-SDRE",
    "url": "http://arxiv.org/abs/2504.15453v1",
    "arxiv_id": "2504.15453v1",
    "authors": [
      "Hassan Almubarak",
      "Maitham F. AL-Sunni",
      "Justin T. Dubbin",
      "Nader Sadegh",
      "John M. Dolan",
      "Evangelos A. Theodorou"
    ],
    "published": "2025-04-21T21:39:49+00:00",
    "summary": "The State-Dependent Riccati Equation (SDRE) approach has emerged as a systematic and effective means of designing nearly optimal nonlinear controllers. The Barrier States (BaS) embedding methodology was developed recently for safe multi-objective controls in which the safety condition is manifested as a state to be controlled along with other states of the system. The overall system, termed the safety embedded system, is highly nonlinear even if the original system is linear. This paper develops a nonlinear nearly optimal safe feedback control technique by combining the two strategies effectively. First, the BaS is derived in an extended linearization formulation to be subsequently used to form an extended safety embedded system. A new optimal control problem is formed thereafter, which is used to construct a safety embedded State-Dependent Riccati Equation, termed BaS-SDRE, whose solution approximates the solution of the optimal control problem's associated Hamilton-Jacobi-Bellman (HJB) equation. The BaS-SDRE is then solved online to synthesize the nearly optimal safe control. The proposed technique's efficacy is demonstrated on an unstable, constrained linear system that shows how the synthesized control reacts to nonlinearities near the unsafe region, a nonlinear flight control system with limited path angular velocity that exists due to structural and dynamic concerns, and a planar quadrotor system that navigates safely in a crowded environment."
  },
  {
    "title": "Virology Capabilities Test (VCT): A Multimodal Virology Q&A Benchmark",
    "url": "http://arxiv.org/abs/2504.16137v1",
    "arxiv_id": "2504.16137v1",
    "authors": [
      "Jasper G\u00f6tting",
      "Pedro Medeiros",
      "Jon G Sanders",
      "Nathaniel Li",
      "Long Phan",
      "Karam Elabd",
      "Lennart Justen",
      "Dan Hendrycks",
      "Seth Donoughe"
    ],
    "published": "2025-04-21T21:04:01+00:00",
    "summary": "We present the Virology Capabilities Test (VCT), a large language model (LLM) benchmark that measures the capability to troubleshoot complex virology laboratory protocols. Constructed from the inputs of dozens of PhD-level expert virologists, VCT consists of $322$ multimodal questions covering fundamental, tacit, and visual knowledge that is essential for practical work in virology laboratories. VCT is difficult: expert virologists with access to the internet score an average of $22.1\\%$ on questions specifically in their sub-areas of expertise. However, the most performant LLM, OpenAI's o3, reaches $43.8\\%$ accuracy, outperforming $94\\%$ of expert virologists even within their sub-areas of specialization. The ability to provide expert-level virology troubleshooting is inherently dual-use: it is useful for beneficial research, but it can also be misused. Therefore, the fact that publicly available models outperform virologists on VCT raises pressing governance considerations. We propose that the capability of LLMs to provide expert-level troubleshooting of dual-use virology work should be integrated into existing frameworks for handling dual-use technologies in the life sciences."
  },
  {
    "title": "Solving Multi-Agent Safe Optimal Control with Distributed Epigraph Form MARL",
    "url": "http://arxiv.org/abs/2504.15425v1",
    "arxiv_id": "2504.15425v1",
    "authors": [
      "Songyuan Zhang",
      "Oswin So",
      "Mitchell Black",
      "Zachary Serlin",
      "Chuchu Fan"
    ],
    "published": "2025-04-21T20:34:55+00:00",
    "summary": "Tasks for multi-robot systems often require the robots to collaborate and complete a team goal while maintaining safety. This problem is usually formalized as a constrained Markov decision process (CMDP), which targets minimizing a global cost and bringing the mean of constraint violation below a user-defined threshold. Inspired by real-world robotic applications, we define safety as zero constraint violation. While many safe multi-agent reinforcement learning (MARL) algorithms have been proposed to solve CMDPs, these algorithms suffer from unstable training in this setting. To tackle this, we use the epigraph form for constrained optimization to improve training stability and prove that the centralized epigraph form problem can be solved in a distributed fashion by each agent. This results in a novel centralized training distributed execution MARL algorithm named Def-MARL. Simulation experiments on 8 different tasks across 2 different simulators show that Def-MARL achieves the best overall performance, satisfies safety constraints, and maintains stable training. Real-world hardware experiments on Crazyflie quadcopters demonstrate the ability of Def-MARL to safely coordinate agents to complete complex collaborative tasks compared to other methods."
  },
  {
    "title": "Safety Embedded Adaptive Control Using Barrier States",
    "url": "http://arxiv.org/abs/2504.15423v1",
    "arxiv_id": "2504.15423v1",
    "authors": [
      "Maitham F. AL-Sunni",
      "Hassan Almubarak",
      "John M. Dolan"
    ],
    "published": "2025-04-21T20:29:38+00:00",
    "summary": "In this work, we explore the application of barrier states (BaS) in the realm of safe nonlinear adaptive control. Our proposed framework derives barrier states for systems with parametric uncertainty, which are augmented into the uncertain dynamical model. We employ an adaptive nonlinear control strategy based on a control Lyapunov functions approach to design a stabilizing controller for the augmented system. The developed theory shows that the controller ensures safe control actions for the original system while meeting specified performance objectives. We validate the effectiveness of our approach through simulations on diverse systems, including a planar quadrotor subject to unknown drag forces and an adaptive cruise control system, for which we provide comparisons with existing methodologies."
  },
  {
    "title": "$k$-Inductive and Interpolation-Inspired Barrier Certificates for Stochastic Dynamical Systems",
    "url": "http://arxiv.org/abs/2504.15412v1",
    "arxiv_id": "2504.15412v1",
    "authors": [
      "Mohammed Adib Oumer",
      "Vishnu Murali",
      "Majid Zamani"
    ],
    "published": "2025-04-21T19:41:43+00:00",
    "summary": "We introduce two notions of barrier certificates that use multiple functions to provide a lower bound on the probabilistic satisfaction of safety for stochastic dynamical systems. A barrier certificate for a stochastic dynamical system acts as a nonnegative supermartingale, and provides a lower bound on the probability that the system is safe. The promise of such certificates is that their search can be effectively automated. Typically, one may use optimization or SMT solvers to find such barrier certificates of a given fixed template. When such approaches fail, a typical approach is to instead change the template. We propose an alternative approach that we dub interpolation-inspired barrier certificates. An interpolation-inspired barrier certificate consists of a set of functions that jointly provide a lower bound on the probability of satisfying safety. We show how one may find such certificates of a fixed template, even when we fail to find standard barrier certificates of the same template. However, we note that such certificates still need to ensure a supermartingale guarantee for one function in the set. To address this challenge, we consider the use of $k$-induction with these interpolation-inspired certificates. The recent use of $k$-induction in barrier certificates allows one to relax the supermartingale requirement at every time step to a combination of a supermartingale requirement every $k$ steps and a $c$-martingale requirement for the intermediate steps. We provide a generic formulation of a barrier certificate that we dub $k$-inductive interpolation-inspired barrier certificate. The formulation allows for several combinations of interpolation and $k$-induction for barrier certificate. We present two examples among the possible combinations. We finally present sum-of-squares programming to synthesize this set of functions and demonstrate their utility in case studies."
  },
  {
    "title": "Multimodal Large Language Models for Enhanced Traffic Safety: A Comprehensive Review and Future Trends",
    "url": "http://arxiv.org/abs/2504.16134v1",
    "arxiv_id": "2504.16134v1",
    "authors": [
      "Mohammad Abu Tami",
      "Mohammed Elhenawy",
      "Huthaifa I. Ashqar"
    ],
    "published": "2025-04-21T18:48:35+00:00",
    "summary": "Traffic safety remains a critical global challenge, with traditional Advanced Driver-Assistance Systems (ADAS) often struggling in dynamic real-world scenarios due to fragmented sensor processing and susceptibility to adversarial conditions. This paper reviews the transformative potential of Multimodal Large Language Models (MLLMs) in addressing these limitations by integrating cross-modal data such as visual, spatial, and environmental inputs to enable holistic scene understanding. Through a comprehensive analysis of MLLM-based approaches, we highlight their capabilities in enhancing perception, decision-making, and adversarial robustness, while also examining the role of key datasets (e.g., KITTI, DRAMA, ML4RoadSafety) in advancing research. Furthermore, we outline future directions, including real-time edge deployment, causality-driven reasoning, and human-AI collaboration. By positioning MLLMs as a cornerstone for next-generation traffic safety systems, this review underscores their potential to revolutionize the field, offering scalable, context-aware solutions that proactively mitigate risks and improve overall road safety."
  },
  {
    "title": "A Conceptual Framework for AI-based Decision Systems in Critical Infrastructures",
    "url": "http://arxiv.org/abs/2504.16133v1",
    "arxiv_id": "2504.16133v1",
    "authors": [
      "Milad Leyli-abadi",
      "Ricardo J. Bessa",
      "Jan Viebahn",
      "Daniel Boos",
      "Clark Borst",
      "Alberto Castagna",
      "Ricardo Chavarriaga",
      "Mohamed Hassouna",
      "Bruno Lemetayer",
      "Giulia Leto",
      "Antoine Marot",
      "Maroua Meddeb",
      "Manuel Meyer",
      "Viola Schiaffonati",
      "Manuel Schneider",
      "Toni Waefler"
    ],
    "published": "2025-04-21T18:38:26+00:00",
    "summary": "The interaction between humans and AI in safety-critical systems presents a unique set of challenges that remain partially addressed by existing frameworks. These challenges stem from the complex interplay of requirements for transparency, trust, and explainability, coupled with the necessity for robust and safe decision-making. A framework that holistically integrates human and AI capabilities while addressing these concerns is notably required, bridging the critical gaps in designing, deploying, and maintaining safe and effective systems. This paper proposes a holistic conceptual framework for critical infrastructures by adopting an interdisciplinary approach. It integrates traditionally distinct fields such as mathematics, decision theory, computer science, philosophy, psychology, and cognitive engineering and draws on specialized engineering domains, particularly energy, mobility, and aeronautics. The flexibility in its adoption is also demonstrated through its instantiation on an already existing framework."
  },
  {
    "title": "Interpretable Locomotion Prediction in Construction Using a Memory-Driven LLM Agent With Chain-of-Thought Reasoning",
    "url": "http://arxiv.org/abs/2504.15263v1",
    "arxiv_id": "2504.15263v1",
    "authors": [
      "Ehsan Ahmadi",
      "Chao Wang"
    ],
    "published": "2025-04-21T17:45:21+00:00",
    "summary": "Construction tasks are inherently unpredictable, with dynamic environments and safety-critical demands posing significant risks to workers. Exoskeletons offer potential assistance but falter without accurate intent recognition across diverse locomotion modes. This paper presents a locomotion prediction agent leveraging Large Language Models (LLMs) augmented with memory systems, aimed at improving exoskeleton assistance in such settings. Using multimodal inputs - spoken commands and visual data from smart glasses - the agent integrates a Perception Module, Short-Term Memory (STM), Long-Term Memory (LTM), and Refinement Module to predict locomotion modes effectively. Evaluation reveals a baseline weighted F1-score of 0.73 without memory, rising to 0.81 with STM, and reaching 0.90 with both STM and LTM, excelling with vague and safety-critical commands. Calibration metrics, including a Brier Score drop from 0.244 to 0.090 and ECE from 0.222 to 0.044, affirm improved reliability. This framework supports safer, high-level human-exoskeleton collaboration, with promise for adaptive assistive systems in dynamic industries."
  },
  {
    "title": "Leveraging Language Models for Automated Patient Record Linkage",
    "url": "http://arxiv.org/abs/2504.15261v1",
    "arxiv_id": "2504.15261v1",
    "authors": [
      "Mohammad Beheshti",
      "Lovedeep Gondara",
      "Iris Zachary"
    ],
    "published": "2025-04-21T17:41:15+00:00",
    "summary": "Objective: Healthcare data fragmentation presents a major challenge for linking patient data, necessitating robust record linkage to integrate patient records from diverse sources. This study investigates the feasibility of leveraging language models for automated patient record linkage, focusing on two key tasks: blocking and matching. Materials and Methods: We utilized real-world healthcare data from the Missouri Cancer Registry and Research Center, linking patient records from two independent sources using probabilistic linkage as a baseline. A transformer-based model, RoBERTa, was fine-tuned for blocking using sentence embeddings. For matching, several language models were experimented under fine-tuned and zero-shot settings, assessing their performance against ground truth labels. Results: The fine-tuned blocking model achieved a 92% reduction in the number of candidate pairs while maintaining near-perfect recall. In the matching task, fine-tuned Mistral-7B achieved the best performance with only 6 incorrect predictions. Among zero-shot models, Mistral-Small-24B performed best, with a total of 55 incorrect predictions. Discussion: Fine-tuned language models achieved strong performance in patient record blocking and matching with minimal errors. However, they remain less accurate and efficient than a hybrid rule-based and probabilistic approach for blocking. Additionally, reasoning models like DeepSeek-R1 are impractical for large-scale record linkage due to high computational costs. Conclusion: This study highlights the potential of language models for automating patient record linkage, offering improved efficiency by eliminating the manual efforts required to perform patient record linkage. Overall, language models offer a scalable solution that can enhance data integration, reduce manual effort, and support disease surveillance and research."
  },
  {
    "title": "FlowReasoner: Reinforcing Query-Level Meta-Agents",
    "url": "http://arxiv.org/abs/2504.15257v1",
    "arxiv_id": "2504.15257v1",
    "authors": [
      "Hongcheng Gao",
      "Yue Liu",
      "Yufei He",
      "Longxu Dou",
      "Chao Du",
      "Zhijie Deng",
      "Bryan Hooi",
      "Min Lin",
      "Tianyu Pang"
    ],
    "published": "2025-04-21T17:35:42+00:00",
    "summary": "This paper proposes a query-level meta-agent named FlowReasoner to automate the design of query-level multi-agent systems, i.e., one system per user query. Our core idea is to incentivize a reasoning-based meta-agent via external execution feedback. Concretely, by distilling DeepSeek R1, we first endow the basic reasoning ability regarding the generation of multi-agent systems to FlowReasoner. Then, we further enhance it via reinforcement learning (RL) with external execution feedback. A multi-purpose reward is designed to guide the RL training from aspects of performance, complexity, and efficiency. In this manner, FlowReasoner is enabled to generate a personalized multi-agent system for each user query via deliberative reasoning. Experiments on both engineering and competition code benchmarks demonstrate the superiority of FlowReasoner. Remarkably, it surpasses o1-mini by 10.52% accuracy across three benchmarks. The code is available at https://github.com/sail-sg/FlowReasoner."
  },
  {
    "title": "CRUST-Bench: A Comprehensive Benchmark for C-to-safe-Rust Transpilation",
    "url": "http://arxiv.org/abs/2504.15254v1",
    "arxiv_id": "2504.15254v1",
    "authors": [
      "Anirudh Khatry",
      "Robert Zhang",
      "Jia Pan",
      "Ziteng Wang",
      "Qiaochu Chen",
      "Greg Durrett",
      "Isil Dillig"
    ],
    "published": "2025-04-21T17:33:33+00:00",
    "summary": "C-to-Rust transpilation is essential for modernizing legacy C code while enhancing safety and interoperability with modern Rust ecosystems. However, no dataset currently exists for evaluating whether a system can transpile C into safe Rust that passes a set of test cases. We introduce CRUST-Bench, a dataset of 100 C repositories, each paired with manually-written interfaces in safe Rust as well as test cases that can be used to validate correctness of the transpilation. By considering entire repositories rather than isolated functions, CRUST-Bench captures the challenges of translating complex projects with dependencies across multiple files. The provided Rust interfaces provide explicit specifications that ensure adherence to idiomatic, memory-safe Rust patterns, while the accompanying test cases enforce functional correctness. We evaluate state-of-the-art large language models (LLMs) on this task and find that safe and idiomatic Rust generation is still a challenging problem for various state-of-the-art methods and techniques. We also provide insights into the errors LLMs usually make in transpiling code from C to safe Rust. The best performing model, OpenAI o1, is able to solve only 15 tasks in a single-shot setting. Improvements on CRUST-Bench would lead to improved transpilation systems that can reason about complex scenarios and help in migrating legacy codebases from C into languages like Rust that ensure memory safety. You can find the dataset and code at https://github.com/anirudhkhatry/CRUST-bench."
  },
  {
    "title": "MR. Guard: Multilingual Reasoning Guardrail using Curriculum Learning",
    "url": "http://arxiv.org/abs/2504.15241v1",
    "arxiv_id": "2504.15241v1",
    "authors": [
      "Yahan Yang",
      "Soham Dan",
      "Shuo Li",
      "Dan Roth",
      "Insup Lee"
    ],
    "published": "2025-04-21T17:15:06+00:00",
    "summary": "Large Language Models (LLMs) are susceptible to adversarial attacks such as jailbreaking, which can elicit harmful or unsafe behaviors. This vulnerability is exacerbated in multilingual setting, where multilingual safety-aligned data are often limited. Thus, developing a guardrail capable of detecting and filtering unsafe content across diverse languages is critical for deploying LLMs in real-world applications. In this work, we propose an approach to build a multilingual guardrail with reasoning. Our method consists of: (1) synthetic multilingual data generation incorporating culturally and linguistically nuanced variants, (2) supervised fine-tuning, and (3) a curriculum-guided Group Relative Policy Optimization (GRPO) framework that further improves performance. Experimental results demonstrate that our multilingual guardrail consistently outperforms recent baselines across both in-domain and out-of-domain languages. The multilingual reasoning capability of our guardrail enables it to generate multilingual explanations, which are particularly useful for understanding language-specific risks and ambiguities in multilingual content moderation."
  },
  {
    "title": "A Genetic Fuzzy-Enabled Framework on Robotic Manipulation for In-Space Servicing",
    "url": "http://arxiv.org/abs/2504.15226v1",
    "arxiv_id": "2504.15226v1",
    "authors": [
      "Nathan Steffen",
      "Wilhelm Louw",
      "Nicholas Ernest",
      "Timothy Arnett",
      "Kelly Cohen"
    ],
    "published": "2025-04-21T16:57:56+00:00",
    "summary": "Automation of robotic systems for servicing in cislunar space is becoming extremely important as the number of satellites in orbit increases. Safety is critical in performing satellite maintenance, so the control techniques utilized must be trusted in addition to being highly efficient. In this work, Genetic Fuzzy Trees are combined with the widely used LQR control scheme via Thales' TrUE AI Toolkit to create a trusted and efficient controller for a two-degree-of-freedom planar robotic manipulator that would theoretically be used to perform satellite maintenance. It was found that Genetic Fuzzy-LQR is 18.5% more performant than optimal LQR on average, and that it is incredibly robust to uncertainty."
  },
  {
    "title": "Existing Industry Practice for the EU AI Act's General-Purpose AI Code of Practice Safety and Security Measures",
    "url": "http://arxiv.org/abs/2504.15181v1",
    "arxiv_id": "2504.15181v1",
    "authors": [
      "Lily Stelling",
      "Mick Yang",
      "Rokas Gipi\u0161kis",
      "Leon Staufer",
      "Ze Shen Chin",
      "Sim\u00e9on Campos",
      "Michael Chen"
    ],
    "published": "2025-04-21T15:44:01+00:00",
    "summary": "This report provides a detailed comparison between the measures proposed in the EU AI Act's General-Purpose AI (GPAI) Code of Practice (Third Draft) and current practices adopted by leading AI companies. As the EU moves toward enforcing binding obligations for GPAI model providers, the Code of Practice will be key to bridging legal requirements with concrete technical commitments. Our analysis focuses on the draft's Safety and Security section which is only relevant for the providers of the most advanced models (Commitments II.1-II.16) and excerpts from current public-facing documents quotes that are relevant to each individual measure.   We systematically reviewed different document types - including companies' frontier safety frameworks and model cards - from over a dozen companies, including OpenAI, Anthropic, Google DeepMind, Microsoft, Meta, Amazon, and others. This report is not meant to be an indication of legal compliance nor does it take any prescriptive viewpoint about the Code of Practice or companies' policies. Instead, it aims to inform the ongoing dialogue between regulators and GPAI model providers by surfacing evidence of precedent."
  },
  {
    "title": "C2RUST-BENCH: A Minimized, Representative Dataset for C-to-Rust Transpilation Evaluation",
    "url": "http://arxiv.org/abs/2504.15144v1",
    "arxiv_id": "2504.15144v1",
    "authors": [
      "Melih Sirlanci",
      "Carter Yagemann",
      "Zhiqiang Lin"
    ],
    "published": "2025-04-21T14:48:45+00:00",
    "summary": "Despite the effort in vulnerability detection over the last two decades, memory safety vulnerabilities continue to be a critical problem. Recent reports suggest that the key solution is to migrate to memory-safe languages. To this end, C-to-Rust transpilation becomes popular to resolve memory-safety issues in C programs. Recent works propose C-to-Rust transpilation frameworks; however, a comprehensive evaluation dataset is missing. Although one solution is to put together a large enough dataset, this increases the analysis time in automated frameworks as well as in manual efforts for some cases. In this work, we build a method to select functions from a large set to construct a minimized yet representative dataset to evaluate the C-to-Rust transpilation. We propose C2RUST-BENCH that contains 2,905 functions, which are representative of C-to-Rust transpilation, selected from 15,503 functions of real-world programs."
  },
  {
    "title": "EasyEdit2: An Easy-to-use Steering Framework for Editing Large Language Models",
    "url": "http://arxiv.org/abs/2504.15133v1",
    "arxiv_id": "2504.15133v1",
    "authors": [
      "Ziwen Xu",
      "Shuxun Wang",
      "Kewei Xu",
      "Haoming Xu",
      "Mengru Wang",
      "Xinle Deng",
      "Yunzhi Yao",
      "Guozhou Zheng",
      "Huajun Chen",
      "Ningyu Zhang"
    ],
    "published": "2025-04-21T14:33:55+00:00",
    "summary": "In this paper, we introduce EasyEdit2, a framework designed to enable plug-and-play adjustability for controlling Large Language Model (LLM) behaviors. EasyEdit2 supports a wide range of test-time interventions, including safety, sentiment, personality, reasoning patterns, factuality, and language features. Unlike its predecessor, EasyEdit2 features a new architecture specifically designed for seamless model steering. It comprises key modules such as the steering vector generator and the steering vector applier, which enable automatic generation and application of steering vectors to influence the model's behavior without modifying its parameters. One of the main advantages of EasyEdit2 is its ease of use-users do not need extensive technical knowledge. With just a single example, they can effectively guide and adjust the model's responses, making precise control both accessible and efficient. Empirically, we report model steering performance across different LLMs, demonstrating the effectiveness of these techniques. We have released the source code on GitHub at https://github.com/zjunlp/EasyEdit along with a demonstration notebook. In addition, we provide a demo video at https://zjunlp.github.io/project/EasyEdit2/video for a quick introduction."
  },
  {
    "title": "Safety Co-Option and Compromised National Security: The Self-Fulfilling Prophecy of Weakened AI Risk Thresholds",
    "url": "http://arxiv.org/abs/2504.15088v1",
    "arxiv_id": "2504.15088v1",
    "authors": [
      "Heidy Khlaaf",
      "Sarah Myers West"
    ],
    "published": "2025-04-21T13:20:56+00:00",
    "summary": "Risk thresholds provide a measure of the level of risk exposure that a society or individual is willing to withstand, ultimately shaping how we determine the safety of technological systems. Against the backdrop of the Cold War, the first risk analyses, such as those devised for nuclear systems, cemented societally accepted risk thresholds against which safety-critical and defense systems are now evaluated. But today, the appropriate risk tolerances for AI systems have yet to be agreed on by global governing efforts, despite the need for democratic deliberation regarding the acceptable levels of harm to human life. Absent such AI risk thresholds, AI technologists-primarily industry labs, as well as \"AI safety\" focused organizations-have instead advocated for risk tolerances skewed by a purported AI arms race and speculative \"existential\" risks, taking over the arbitration of risk determinations with life-or-death consequences, subverting democratic processes.   In this paper, we demonstrate how such approaches have allowed AI technologists to engage in \"safety revisionism,\" substituting traditional safety methods and terminology with ill-defined alternatives that vie for the accelerated adoption of military AI uses at the cost of lowered safety and security thresholds. We explore how the current trajectory for AI risk determination and evaluation for foundation model use within national security is poised for a race to the bottom, to the detriment of the US's national security interests. Safety-critical and defense systems must comply with assurance frameworks that are aligned with established risk thresholds, and foundation models are no exception. As such, development of evaluation frameworks for AI-based military systems must preserve the safety and security of US critical and defense infrastructure, and remain in alignment with international humanitarian law."
  },
  {
    "title": "RainbowPlus: Enhancing Adversarial Prompt Generation via Evolutionary Quality-Diversity Search",
    "url": "http://arxiv.org/abs/2504.15047v1",
    "arxiv_id": "2504.15047v1",
    "authors": [
      "Quy-Anh Dang",
      "Chris Ngo",
      "Truong-Son Hy"
    ],
    "published": "2025-04-21T12:04:57+00:00",
    "summary": "Large Language Models (LLMs) exhibit remarkable capabilities but are susceptible to adversarial prompts that exploit vulnerabilities to produce unsafe or biased outputs. Existing red-teaming methods often face scalability challenges, resource-intensive requirements, or limited diversity in attack strategies. We propose RainbowPlus, a novel red-teaming framework rooted in evolutionary computation, enhancing adversarial prompt generation through an adaptive quality-diversity (QD) search that extends classical evolutionary algorithms like MAP-Elites with innovations tailored for language models. By employing a multi-element archive to store diverse high-quality prompts and a comprehensive fitness function to evaluate multiple prompts concurrently, RainbowPlus overcomes the constraints of single-prompt archives and pairwise comparisons in prior QD methods like Rainbow Teaming. Experiments comparing RainbowPlus to QD methods across six benchmark datasets and four open-source LLMs demonstrate superior attack success rate (ASR) and diversity (Diverse-Score $\\approx 0.84$), generating up to 100 times more unique prompts (e.g., 10,418 vs. 100 for Ministral-8B-Instruct-2410). Against nine state-of-the-art methods on the HarmBench dataset with twelve LLMs (ten open-source, two closed-source), RainbowPlus achieves an average ASR of 81.1%, surpassing AutoDAN-Turbo by 3.9%, and is 9 times faster (1.45 vs. 13.50 hours). Our open-source implementation fosters further advancements in LLM safety, offering a scalable tool for vulnerability assessment. Code and resources are publicly available at https://github.com/knoveleng/rainbowplus, supporting reproducibility and future research in LLM red-teaming."
  },
  {
    "title": "aiXamine: LLM Safety and Security Simplified",
    "url": "http://arxiv.org/abs/2504.14985v1",
    "arxiv_id": "2504.14985v1",
    "authors": [
      "Fatih Deniz",
      "Dorde Popovic",
      "Yazan Boshmaf",
      "Euisuh Jeong",
      "Minhaj Ahmad",
      "Sanjay Chawla",
      "Issa Khalil"
    ],
    "published": "2025-04-21T09:26:05+00:00",
    "summary": "Evaluating Large Language Models (LLMs) for safety and security remains a complex task, often requiring users to navigate a fragmented landscape of ad hoc benchmarks, datasets, metrics, and reporting formats. To address this challenge, we present aiXamine, a comprehensive black-box evaluation platform for LLM safety and security. aiXamine integrates over 40 tests (i.e., benchmarks) organized into eight key services targeting specific dimensions of safety and security: adversarial robustness, code security, fairness and bias, hallucination, model and data privacy, out-of-distribution (OOD) robustness, over-refusal, and safety alignment. The platform aggregates the evaluation results into a single detailed report per model, providing a detailed breakdown of model performance, test examples, and rich visualizations. We used aiXamine to assess over 50 publicly available and proprietary LLMs, conducting over 2K examinations. Our findings reveal notable vulnerabilities in leading models, including susceptibility to adversarial attacks in OpenAI's GPT-4o, biased outputs in xAI's Grok-3, and privacy weaknesses in Google's Gemini 2.0. Additionally, we observe that open-source models can match or exceed proprietary models in specific services such as safety alignment, fairness and bias, and OOD robustness. Finally, we identify trade-offs between distillation strategies, model size, training methods, and architectural choices."
  },
  {
    "title": "Distributed Time-Varying Gaussian Regression via Kalman Filtering",
    "url": "http://arxiv.org/abs/2504.14900v1",
    "arxiv_id": "2504.14900v1",
    "authors": [
      "Nicola Taddei",
      "Riccardo Maggioni",
      "Jaap Eising",
      "Giulia De Pasquale",
      "Florian Dorfler"
    ],
    "published": "2025-04-21T07:12:05+00:00",
    "summary": "We consider the problem of learning time-varying functions in a distributed fashion, where agents collect local information to collaboratively achieve a shared estimate. This task is particularly relevant in control applications, whenever real-time and robust estimation of dynamic cost/reward functions in safety critical settings has to be performed. In this paper, we,adopt a finite-dimensional approximation of a Gaussian Process, corresponding to a Bayesian linear regression in an appropriate feature space, and propose a new algorithm, DistKP, to track the time-varying coefficients via a distributed Kalman filter. The proposed method works for arbitrary kernels and under weaker assumptions on the time-evolution of the function to learn compared to the literature. We validate our results using a simulation example in which a fleet of Unmanned Aerial Vehicles (UAVs) learns a dynamically changing wind field."
  },
  {
    "title": "Retrieval Augmented Generation Evaluation in the Era of Large Language Models: A Comprehensive Survey",
    "url": "http://arxiv.org/abs/2504.14891v1",
    "arxiv_id": "2504.14891v1",
    "authors": [
      "Aoran Gan",
      "Hao Yu",
      "Kai Zhang",
      "Qi Liu",
      "Wenyu Yan",
      "Zhenya Huang",
      "Shiwei Tong",
      "Guoping Hu"
    ],
    "published": "2025-04-21T06:39:47+00:00",
    "summary": "Recent advancements in Retrieval-Augmented Generation (RAG) have revolutionized natural language processing by integrating Large Language Models (LLMs) with external information retrieval, enabling accurate, up-to-date, and verifiable text generation across diverse applications. However, evaluating RAG systems presents unique challenges due to their hybrid architecture that combines retrieval and generation components, as well as their dependence on dynamic knowledge sources in the LLM era. In response, this paper provides a comprehensive survey of RAG evaluation methods and frameworks, systematically reviewing traditional and emerging evaluation approaches, for system performance, factual accuracy, safety, and computational efficiency in the LLM era. We also compile and categorize the RAG-specific datasets and evaluation frameworks, conducting a meta-analysis of evaluation practices in high-impact RAG research. To the best of our knowledge, this work represents the most comprehensive survey for RAG evaluation, bridging traditional and LLM-driven methods, and serves as a critical resource for advancing RAG development."
  },
  {
    "title": "ReCraft: Self-Contained Split, Merge, and Membership Change of Raft Protocol",
    "url": "http://arxiv.org/abs/2504.14802v1",
    "arxiv_id": "2504.14802v1",
    "authors": [
      "Kezhi Xiong",
      "Soonwon Moon",
      "Joshua Kang",
      "Bryant Curto",
      "Jieung Kim",
      "Ji-Yong Shin"
    ],
    "published": "2025-04-21T02:05:06+00:00",
    "summary": "Designing reconfiguration schemes for consensus protocols is challenging because subtle corner cases during reconfiguration could invalidate the correctness of the protocol. Thus, most systems that embed consensus protocols conservatively implement the reconfiguration and refrain from developing an efficient scheme. Existing implementations often stop the entire system during reconfiguration and rely on a centralized coordinator, which can become a single point of failure. We present ReCraft, a novel reconfiguration protocol for Raft, which supports multi- and single-cluster-level reconfigurations. ReCraft does not rely on external coordinators and blocks minimally. ReCraft enables the sharding of Raft clusters with split and merge reconfigurations and adds a membership change scheme that improves Raft. We prove the safety and liveness of ReCraft and demonstrate its efficiency through implementations in etcd."
  },
  {
    "title": "Safe Autonomous Environmental Contact for Soft Robots using Control Barrier Functions",
    "url": "http://arxiv.org/abs/2504.14755v1",
    "arxiv_id": "2504.14755v1",
    "authors": [
      "Akua K. Dickson",
      "Juan C. Pacheco Garcia",
      "Meredith L. Anderson",
      "Ran Jing",
      "Sarah Alizadeh-Shabdiz",
      "Audrey X. Wang",
      "Charles DeLorey",
      "Zach J. Patterson",
      "Andrew P. Sabelhaus"
    ],
    "published": "2025-04-20T22:31:55+00:00",
    "summary": "Robots built from soft materials will inherently apply lower environmental forces than their rigid counterparts, and therefore may be more suitable in sensitive settings with unintended contact. However, these robots' applied forces result from both their design and their control system in closed-loop, and therefore, ensuring bounds on these forces requires controller synthesis for safety as well. This article introduces the first feedback controller for a soft manipulator that formally meets a safety specification with respect to environmental contact. In our proof-of-concept setting, the robot's environment has known geometry and is deformable with a known elastic modulus. Our approach maps a bound on applied forces to a safe set of positions of the robot's tip via predicted deformations of the environment. Then, a quadratic program with Control Barrier Functions in its constraints is used to supervise a nominal feedback signal, verifiably maintaining the robot's tip within this safe set. Hardware experiments on a multi-segment soft pneumatic robot demonstrate that the proposed framework successfully constrains its environmental contact forces. This framework represents a fundamental shift in perspective on control and safety for soft robots, defining and implementing a formally verifiable logic specification on their pose and contact forces."
  },
  {
    "title": "Adaptive Field Effect Planner for Safe Interactive Autonomous Driving on Curved Roads",
    "url": "http://arxiv.org/abs/2504.14747v1",
    "arxiv_id": "2504.14747v1",
    "authors": [
      "Qinghao Li",
      "Zhen Tian",
      "Xiaodan Wang",
      "Jinming Yang",
      "Zhihao Lin"
    ],
    "published": "2025-04-20T21:41:19+00:00",
    "summary": "Autonomous driving has garnered significant attention for its potential to improve safety, traffic efficiency, and user convenience. However, the dynamic and complex nature of interactive driving poses significant challenges, including the need to navigate non-linear road geometries, handle dynamic obstacles, and meet stringent safety and comfort requirements. Traditional approaches, such as artificial potential fields (APF), often fall short in addressing these complexities independently, necessitating the development of integrated and adaptive frameworks. This paper presents a novel approach to autonomous vehicle navigation that integrates artificial potential fields, Frenet coordinates, and improved particle swarm optimization (IPSO). A dynamic risk field, adapted from traditional APF, is proposed to ensure interactive safety by quantifying risks and dynamically adjusting lane-changing intentions based on surrounding vehicle behavior. Frenet coordinates are utilized to simplify trajectory planning on non-straight roads, while an enhanced quintic polynomial trajectory generator ensures smooth and comfortable path transitions. Additionally, an IPSO algorithm optimizes trajectory selection in real time, balancing safety and user comfort within a feasible input range. The proposed framework is validated through extensive simulations and real-world scenarios, demonstrating its ability to navigate complex traffic environments, maintain safety margins, and generate smooth, dynamically feasible trajectories."
  },
  {
    "title": "Efficient and Safe Planner for Automated Driving on Ramps Considering Unsatisfication",
    "url": "http://arxiv.org/abs/2504.15320v1",
    "arxiv_id": "2504.15320v1",
    "authors": [
      "Qinghao Li",
      "Zhen Tian",
      "Xiaodan Wang",
      "Jinming Yang",
      "Zhihao Lin"
    ],
    "published": "2025-04-20T21:39:51+00:00",
    "summary": "Automated driving on ramps presents significant challenges due to the need to balance both safety and efficiency during lane changes. This paper proposes an integrated planner for automated vehicles (AVs) on ramps, utilizing an unsatisfactory level metric for efficiency and arrow-cluster-based sampling for safety. The planner identifies optimal times for the AV to change lanes, taking into account the vehicle's velocity as a key factor in efficiency. Additionally, the integrated planner employs arrow-cluster-based sampling to evaluate collision risks and select an optimal lane-changing curve. Extensive simulations were conducted in a ramp scenario to verify the planner's efficient and safe performance. The results demonstrate that the proposed planner can effectively select an appropriate lane-changing time point and a safe lane-changing curve for AVs, without incurring any collisions during the maneuver."
  },
  {
    "title": "Can We Ignore Labels In Out of Distribution Detection?",
    "url": "http://arxiv.org/abs/2504.14704v1",
    "arxiv_id": "2504.14704v1",
    "authors": [
      "Hong Yang",
      "Qi Yu",
      "Travis Desel"
    ],
    "published": "2025-04-20T18:37:51+00:00",
    "summary": "Out-of-distribution (OOD) detection methods have recently become more prominent, serving as a core element in safety-critical autonomous systems. One major purpose of OOD detection is to reject invalid inputs that could lead to unpredictable errors and compromise safety. Due to the cost of labeled data, recent works have investigated the feasibility of self-supervised learning (SSL) OOD detection, unlabeled OOD detection, and zero shot OOD detection. In this work, we identify a set of conditions for a theoretical guarantee of failure in unlabeled OOD detection algorithms from an information-theoretic perspective. These conditions are present in all OOD tasks dealing with real-world data: I) we provide theoretical proof of unlabeled OOD detection failure when there exists zero mutual information between the learning objective and the in-distribution labels, a.k.a. 'label blindness', II) we define a new OOD task - Adjacent OOD detection - that tests for label blindness and accounts for a previously ignored safety gap in all OOD detection benchmarks, and III) we perform experiments demonstrating that existing unlabeled OOD methods fail under conditions suggested by our label blindness theory and analyze the implications for future research in unlabeled OOD methods."
  },
  {
    "title": "A Byzantine Fault Tolerance Approach towards AI Safety",
    "url": "http://arxiv.org/abs/2504.14668v1",
    "arxiv_id": "2504.14668v1",
    "authors": [
      "John deVadoss",
      "Matthias Artzt"
    ],
    "published": "2025-04-20T16:18:06+00:00",
    "summary": "Ensuring that an AI system behaves reliably and as intended, especially in the presence of unexpected faults or adversarial conditions, is a complex challenge. Inspired by the field of Byzantine Fault Tolerance (BFT) from distributed computing, we explore a fault tolerance architecture for AI safety. By drawing an analogy between unreliable, corrupt, misbehaving or malicious AI artifacts and Byzantine nodes in a distributed system, we propose an architecture that leverages consensus mechanisms to enhance AI safety and reliability."
  },
  {
    "title": "BLACKOUT: Data-Oblivious Computation with Blinded Capabilities",
    "url": "http://arxiv.org/abs/2504.14654v1",
    "arxiv_id": "2504.14654v1",
    "authors": [
      "Hossam ElAtali",
      "Merve G\u00fclmez",
      "Thomas Nyman",
      "N. Asokan"
    ],
    "published": "2025-04-20T15:25:59+00:00",
    "summary": "Lack of memory-safety and exposure to side channels are two prominent, persistent challenges for the secure implementation of software. Memory-safe programming languages promise to significantly reduce the prevalence of memory-safety bugs, but make it more difficult to implement side-channel-resistant code. We aim to address both memory-safety and side-channel resistance by augmenting memory-safe hardware with the ability for data-oblivious programming. We describe an extension to the CHERI capability architecture to provide blinded capabilities that allow data-oblivious computation to be carried out by userspace tasks. We also present BLACKOUT, our realization of blinded capabilities on a FPGA softcore based on the speculative out-of-order CHERI-Toooba processor and extend the CHERI-enabled Clang/LLVM compiler and the CheriBSD operating system with support for blinded capabilities. BLACKOUT makes writing side-channel-resistant code easier by making non-data-oblivious operations via blinded capabilities explicitly fault. Through rigorous evaluation we show that BLACKOUT ensures memory operated on through blinded capabilities is securely allocated, used, and reclaimed and demonstrate that, in benchmarks comparable to those used by previous work, BLACKOUT imposes only a small performance degradation (1.5% geometric mean) compared to the baseline CHERI-Toooba processor."
  },
  {
    "title": "A Framework for Benchmarking and Aligning Task-Planning Safety in LLM-Based Embodied Agents",
    "url": "http://arxiv.org/abs/2504.14650v1",
    "arxiv_id": "2504.14650v1",
    "authors": [
      "Yuting Huang",
      "Leilei Ding",
      "Zhipeng Tang",
      "Tianfu Wang",
      "Xinrui Lin",
      "Wuyang Zhang",
      "Mingxiao Ma",
      "Yanyong Zhang"
    ],
    "published": "2025-04-20T15:12:14+00:00",
    "summary": "Large Language Models (LLMs) exhibit substantial promise in enhancing task-planning capabilities within embodied agents due to their advanced reasoning and comprehension. However, the systemic safety of these agents remains an underexplored frontier. In this study, we present Safe-BeAl, an integrated framework for the measurement (SafePlan-Bench) and alignment (Safe-Align) of LLM-based embodied agents' behaviors. SafePlan-Bench establishes a comprehensive benchmark for evaluating task-planning safety, encompassing 2,027 daily tasks and corresponding environments distributed across 8 distinct hazard categories (e.g., Fire Hazard). Our empirical analysis reveals that even in the absence of adversarial inputs or malicious intent, LLM-based agents can exhibit unsafe behaviors. To mitigate these hazards, we propose Safe-Align, a method designed to integrate physical-world safety knowledge into LLM-based embodied agents while maintaining task-specific performance. Experiments across a variety of settings demonstrate that Safe-BeAl provides comprehensive safety validation, improving safety by 8.55 - 15.22%, compared to embodied agents based on GPT-4, while ensuring successful task completion."
  },
  {
    "title": "Surrogate Fitness Metrics for Interpretable Reinforcement Learning",
    "url": "http://arxiv.org/abs/2504.14645v1",
    "arxiv_id": "2504.14645v1",
    "authors": [
      "Philipp Altmann",
      "C\u00e9line Davignon",
      "Maximilian Zorn",
      "Fabian Ritz",
      "Claudia Linnhoff-Popien",
      "Thomas Gabor"
    ],
    "published": "2025-04-20T15:01:19+00:00",
    "summary": "We employ an evolutionary optimization framework that perturbs initial states to generate informative and diverse policy demonstrations. A joint surrogate fitness function guides the optimization by combining local diversity, behavioral certainty, and global population diversity. To assess demonstration quality, we apply a set of evaluation metrics, including the reward-based optimality gap, fidelity interquartile means (IQMs), fitness composition analysis, and trajectory visualizations. Hyperparameter sensitivity is also examined to better understand the dynamics of trajectory optimization. Our findings demonstrate that optimizing trajectory selection via surrogate fitness metrics significantly improves interpretability of RL policies in both discrete and continuous environments. In gridworld domains, evaluations reveal significantly enhanced demonstration fidelities compared to random and ablated baselines. In continuous control, the proposed framework offers valuable insights, particularly for early-stage policies, while fidelity-based optimization proves more effective for mature policies. By refining and systematically analyzing surrogate fitness functions, this study advances the interpretability of RL models. The proposed improvements provide deeper insights into RL decision-making, benefiting applications in safety-critical and explainability-focused domains."
  },
  {
    "title": "Transferred plasma catheter for endotherapeutic applications: a parametric study of guided streamers dynamics",
    "url": "http://arxiv.org/abs/2504.14637v1",
    "arxiv_id": "2504.14637v1",
    "authors": [
      "M. Soulier",
      "T. Vacek",
      "K. Geraud",
      "T. Dufour"
    ],
    "published": "2025-04-20T14:29:57+00:00",
    "summary": "Non-thermal atmospheric pressure plasma jets (APPJs) are increasingly used in biomedical applications due to their low temperatures and ability to generate reactive oxygen and nitrogen species (RONS), making them suitable for sensitive environments like medical therapies. The transferred plasma catheter (TPC), a variant of APPJ, shows promise for endoscopic applications but requires precise control of plasma dynamics in confined spaces to ensure safety and efficacy. Despite extensive studies on guided streamers in traditional APPJs, there is limited understanding of streamer behavior in TPC configurations, particularly in challenging scenarios involving grounded metallic surfaces. This study examines the spatiotemporal dynamics of guided streamers generated by TPCs under varying gap distances to establish a robust framework for safe and effective plasma delivery in endoscopic settings. Combining electrical and optical diagnostics, the study characterizes streamer propagation, electric field profiles, and plasma-induced currents in a helium-driven TPC delivering cold plasma to a grounded metal target across gaps of 2 to 18 mm. Results show that streamers maintain charge stability and effectively interact with the target for gap distances below 12 mm, producing significant therapeutic currents. Beyond this threshold, propagation deteriorates due to recombination and reduced electric field intensity. For shorter gaps, counter-propagating waves and secondary streamer interactions are observed, while larger gaps lead to charge dissipation and reduced efficacy. These findings highlight the importance of optimizing gap distances for plasma-assisted endoscopic procedures and demonstrate the TPC's robustness in adverse conditions."
  },
  {
    "title": "a1: Steep Test-time Scaling Law via Environment Augmented Generation",
    "url": "http://arxiv.org/abs/2504.14597v1",
    "arxiv_id": "2504.14597v1",
    "authors": [
      "Lingrui Mei",
      "Shenghua Liu",
      "Yiwei Wang",
      "Baolong Bi",
      "Yuyao Ge",
      "Jun Wan",
      "Yurong Wu",
      "Xueqi Cheng"
    ],
    "published": "2025-04-20T12:55:59+00:00",
    "summary": "Large Language Models (LLMs) have made remarkable breakthroughs in reasoning, yet continue to struggle with hallucinations, logical errors, and inability to self-correct during complex multi-step tasks. Current approaches like chain-of-thought prompting offer limited reasoning capabilities that fail when precise step validation is required. We propose Environment Augmented Generation (EAG), a framework that enhances LLM reasoning through: (1) real-time environmental feedback validating each reasoning step, (2) dynamic branch exploration for investigating alternative solution paths when faced with errors, and (3) experience-based learning from successful reasoning trajectories. Unlike existing methods, EAG enables deliberate backtracking and strategic replanning through tight integration of execution feedback with branching exploration. Our a1-32B model achieves state-of-the-art performance among similar-sized models across all benchmarks, matching larger models like o1 on competition mathematics while outperforming comparable models by up to 24.4 percentage points. Analysis reveals EAG's distinctive scaling pattern: initial token investment in environment interaction yields substantial long-term performance dividends, with advantages amplifying proportionally to task complexity. EAG's theoretical framework demonstrates how environment interactivity and systematic branch exploration together establish a new paradigm for reliable machine reasoning, particularly for problems requiring precise multi-step calculation and logical verification."
  },
  {
    "title": "LLM-Enabled In-Context Learning for Data Collection Scheduling in UAV-assisted Sensor Networks",
    "url": "http://arxiv.org/abs/2504.14556v1",
    "arxiv_id": "2504.14556v1",
    "authors": [
      "Yousef Emami",
      "Hao Gao",
      "SeyedSina Nabavirazani",
      "Luis Almeida"
    ],
    "published": "2025-04-20T10:05:07+00:00",
    "summary": "Unmanned Aerial Vehicles (UAVs) are increasingly being used in various private and commercial applications, e.g. traffic control, package delivery, and Search and Rescue (SAR) operations. Machine Learning (ML) methods used in UAV-assisted Sensor Networks (UASNETs) and especially in Deep Reinforcement Learning (DRL) face challenges such as complex and lengthy model training, gaps between simulation and reality, and low sample efficiency, which conflict with the urgency of emergencies such as SAR operations. This paper proposes In-Context Learning (ICL)-based Data Collection Scheduling (ICLDC) scheme, as an alternative to DRL in emergencies. The UAV collects and transmits logged sensory data, to an LLM, to generate a task description in natural language, from which it obtains a data collection schedule to be executed by the UAV. The system continuously adapts by adding feedback to task descriptions and utilizing feedback for future decisions. This method is tested against jailbreaking attacks, where task description is manipulated to undermine network performance, highlighting the vulnerability of LLMs to such attacks. The proposed ICLDC outperforms the Maximum Channel Gain by reducing cumulative packet loss by approximately 56\\%. ICLDC presents a promising direction for intelligent scheduling and control in UAV-assisted data collection."
  },
  {
    "title": "Should Benevolent Deception be Allowed in EHMI? A Mechanism Explanation Based on Game Theory",
    "url": "http://arxiv.org/abs/2504.14539v1",
    "arxiv_id": "2504.14539v1",
    "authors": [
      "Linkun Liu",
      "Jian Sun",
      "Ye Tian"
    ],
    "published": "2025-04-20T09:03:28+00:00",
    "summary": "The application of external human-machine interface (EHMI) on autonomous vehicles (AVs) facilitates information exchange. Existing research fails to consider the impact of the sequence of actions, as well as the effects of EHMI applications and deception, raising the question of whether benevolent, well-intentioned deception should be permitted (i.e., misleading statements that are intended to benefit both parties). We established a game theory based EHMI information disclosure framework for AVs in this study. In considering benevolent deception, this framework divided the decision-making process into three stages, respectively encompassing three key questions: whether to disclose, when to disclose, and what type of intention information to disclose. The results show that theoretical advantages of deception exist in certain cases when AV expects to maximize the safety of the interaction. In 40 out of 484 cases (8.3%), safety can be enhanced through successful deception. Those successful deceptions fall into two categories: 1) In 28 of these cases, the straight-going AV expected the left-turning HV to yield, while HV exhibited lower speed and higher acceleration; 2) In 12 of these cases, AV expected HV to proceed first, while HV exhibited higher speed and lower acceleration. We also conducted a VR-based driving simulation experiment, and the results confirmed our conclusion. Additionally, we found that when participants had low trust in the EHMI, its use negatively impacted interaction efficiency instead. This study aims to analyze the mechanisms of EHMI information disclosure and contribute to the ongoing discourse on the ethical framework governing autonomous driving systems."
  },
  {
    "title": "Are Vision LLMs Road-Ready? A Comprehensive Benchmark for Safety-Critical Driving Video Understanding",
    "url": "http://arxiv.org/abs/2504.14526v1",
    "arxiv_id": "2504.14526v1",
    "authors": [
      "Tong Zeng",
      "Longfeng Wu",
      "Liang Shi",
      "Dawei Zhou",
      "Feng Guo"
    ],
    "published": "2025-04-20T07:50:44+00:00",
    "summary": "Vision Large Language Models (VLLMs) have demonstrated impressive capabilities in general visual tasks such as image captioning and visual question answering. However, their effectiveness in specialized, safety-critical domains like autonomous driving remains largely unexplored. Autonomous driving systems require sophisticated scene understanding in complex environments, yet existing multimodal benchmarks primarily focus on normal driving conditions, failing to adequately assess VLLMs' performance in safety-critical scenarios. To address this, we introduce DVBench, a pioneering benchmark designed to evaluate the performance of VLLMs in understanding safety-critical driving videos. Built around a hierarchical ability taxonomy that aligns with widely adopted frameworks for describing driving scenarios used in assessing highly automated driving systems, DVBench features 10,000 multiple-choice questions with human-annotated ground-truth answers, enabling a comprehensive evaluation of VLLMs' capabilities in perception and reasoning. Experiments on 14 SOTA VLLMs, ranging from 0.5B to 72B parameters, reveal significant performance gaps, with no model achieving over 40% accuracy, highlighting critical limitations in understanding complex driving scenarios. To probe adaptability, we fine-tuned selected models using domain-specific data from DVBench, achieving accuracy gains ranging from 5.24 to 10.94 percentage points, with relative improvements of up to 43.59%. This improvement underscores the necessity of targeted adaptation to bridge the gap between general-purpose VLLMs and mission-critical driving applications. DVBench establishes an essential evaluation framework and research roadmap for developing VLLMs that meet the safety and robustness requirements for real-world autonomous systems. We released the benchmark toolbox and the fine-tuned model at: https://github.com/tong-zeng/DVBench.git."
  },
  {
    "title": "CoLoTa: A Dataset for Entity-based Commonsense Reasoning over Long-Tail Knowledge",
    "url": "http://arxiv.org/abs/2504.14462v1",
    "arxiv_id": "2504.14462v1",
    "authors": [
      "Armin Toroghi",
      "Willis Guo",
      "Scott Sanner"
    ],
    "published": "2025-04-20T02:47:18+00:00",
    "summary": "The rise of Large Language Models (LLMs) has redefined the AI landscape, particularly due to their ability to encode factual and commonsense knowledge, and their outstanding performance in tasks requiring reasoning. Despite these advances, hallucinations and reasoning errors remain a significant barrier to their deployment in high-stakes settings. In this work, we observe that even the most prominent LLMs, such as OpenAI-o1, suffer from high rates of reasoning errors and hallucinations on tasks requiring commonsense reasoning over obscure, long-tail entities. To investigate this limitation, we present a new dataset for Commonsense reasoning over Long-Tail entities (CoLoTa), that consists of 3,300 queries from question answering and claim verification tasks and covers a diverse range of commonsense reasoning skills. We remark that CoLoTa can also serve as a Knowledge Graph Question Answering (KGQA) dataset since the support of knowledge required to answer its queries is present in the Wikidata knowledge graph. However, as opposed to existing KGQA benchmarks that merely focus on factoid questions, our CoLoTa queries also require commonsense reasoning. Our experiments with strong LLM-based KGQA methodologies indicate their severe inability to answer queries involving commonsense reasoning. Hence, we propose CoLoTa as a novel benchmark for assessing both (i) LLM commonsense reasoning capabilities and their robustness to hallucinations on long-tail entities and (ii) the commonsense reasoning capabilities of KGQA methods."
  },
  {
    "title": "Environmental Monitoring Requirements for the ngVLA",
    "url": "http://arxiv.org/abs/2504.14451v1",
    "arxiv_id": "2504.14451v1",
    "authors": [
      "T. K. Sridharan",
      "J. G. Mangum",
      "B. Butler"
    ],
    "published": "2025-04-20T01:58:41+00:00",
    "summary": "Measurement of environmental parameters is one of the basic requirements for the proper operation of a telescope. This memo is intended to provide guidance for the measurement accuracy requirements in the context of the ngVLA. It relies on previous work for ALMA (Mangum, 2001) and EVLA (Butler \\& Perley, 2008) and a review of the subject by Mangum \\& Wallace (2015). The local operational environment can be broadly divided into two categories: electromagnetic and physical. Meteorological parameters (weather) primarily constitute the physical environmental component and radio frequency interference (RFI) is the essential element of the electromagnetic environment. This memo focuses on the weather component and does not address the RFI, safety and physical infrastructure components. Under weather, the relevant topics are (1) the correction to pointing arising from refraction in the atmosphere (2) the different delays in the arrival times of signals at different antennas due to propagation in the atmosphere (3) monitoring weather parameters to provide operations support, e.g. in determining prevalence of precision or normal conditions, dynamic scheduling and the choice of antennas to constitute a sub-array with a given set of characteristics, among others, and (4) archival. Here we restrict ourselves to the first two topics which impact the data obtained and its calibration."
  },
  {
    "title": "Seeing Through Risk: A Symbolic Approximation of Prospect Theory",
    "url": "http://arxiv.org/abs/2504.14448v1",
    "arxiv_id": "2504.14448v1",
    "authors": [
      "Ali Arslan Yousaf",
      "Umair Rehman",
      "Muhammad Umair Danish"
    ],
    "published": "2025-04-20T01:44:54+00:00",
    "summary": "We propose a novel symbolic modeling framework for decision-making under risk that merges interpretability with the core insights of Prospect Theory. Our approach replaces opaque utility curves and probability weighting functions with transparent, effect-size-guided features. We mathematically formalize the method, demonstrate its ability to replicate well-known framing and loss-aversion phenomena, and provide an end-to-end empirical validation on synthetic datasets. The resulting model achieves competitive predictive performance while yielding clear coefficients mapped onto psychological constructs, making it suitable for applications ranging from AI safety to economic policy analysis."
  },
  {
    "title": "RedMulE-FT: A Reconfigurable Fault-Tolerant Matrix Multiplication Engine",
    "url": "http://arxiv.org/abs/2504.14399v1",
    "arxiv_id": "2504.14399v1",
    "authors": [
      "Philip Wiese",
      "Maurus Item",
      "Luca Bertaccini",
      "Yvan Tortorella",
      "Angelo Garofalo",
      "Luca Benini"
    ],
    "published": "2025-04-19T20:25:33+00:00",
    "summary": "As safety-critical applications increasingly rely on data-parallel floating-point computations, there is an increasing need for flexible and configurable fault tolerance in parallel floating-point accelerators such as tensor engines. While replication-based methods ensure reliability but incur high area and power costs, error correction codes lack the flexibility to trade off robustness against performance. This work presents RedMulE-FT, a runtime-configurable fault-tolerant extension of the RedMulE matrix multiplication accelerator, balancing fault tolerance, area overhead, and performance impacts. The fault tolerance mode is configured in a shadowed context register file before task execution. By combining replication with error-detecting codes to protect the data path, RedMulE-FT achieves an 11x uncorrected fault reduction with only 2.3% area overhead. Full protection extends to control signals, resulting in no functional errors after 1M injections during our extensive fault injection simulation campaign, with a total area overhead of 25.2% while maintaining a 500 MHz frequency in a 12 nm technology."
  },
  {
    "title": "The Geometry of Self-Verification in a Task-Specific Reasoning Model",
    "url": "http://arxiv.org/abs/2504.14379v1",
    "arxiv_id": "2504.14379v1",
    "authors": [
      "Andrew Lee",
      "Lihao Sun",
      "Chris Wendler",
      "Fernanda Vi\u00e9gas",
      "Martin Wattenberg"
    ],
    "published": "2025-04-19T18:40:51+00:00",
    "summary": "How do reasoning models verify their own answers? We study this question by training a model using DeepSeek R1's recipe on the CountDown task. We leverage the fact that preference tuning leads to mode collapse, resulting in a model that always produces highly structured and easily parse-able chain-of-thought sequences. With this setup, we do a top-down and bottom-up analysis to reverse-engineer how the model verifies its outputs. Our top-down analysis reveals Gated Linear Unit (GLU) weights encoding verification-related tokens, such as ``success'' or ``incorrect'', which activate according to the correctness of the model's reasoning steps. Our bottom-up analysis reveals that ``previous-token heads'' are mainly responsible for model verification. Our analyses meet in the middle: drawing inspiration from inter-layer communication channels, we use the identified GLU vectors to localize as few as three attention heads that can disable model verification, pointing to a necessary component of a potentially larger verification circuit."
  },
  {
    "title": "Manipulating Multimodal Agents via Cross-Modal Prompt Injection",
    "url": "http://arxiv.org/abs/2504.14348v1",
    "arxiv_id": "2504.14348v1",
    "authors": [
      "Le Wang",
      "Zonghao Ying",
      "Tianyuan Zhang",
      "Siyuan Liang",
      "Shengshan Hu",
      "Mingchuan Zhang",
      "Aishan Liu",
      "Xianglong Liu"
    ],
    "published": "2025-04-19T16:28:03+00:00",
    "summary": "The emergence of multimodal large language models has redefined the agent paradigm by integrating language and vision modalities with external data sources, enabling agents to better interpret human instructions and execute increasingly complex tasks. However, in this work, we identify a critical yet previously overlooked security vulnerability in multimodal agents: cross-modal prompt injection attacks. To exploit this vulnerability, we propose CrossInject, a novel attack framework in which attackers embed adversarial perturbations across multiple modalities to align with target malicious content, allowing external instructions to hijack the agent's decision-making process and execute unauthorized tasks. Our approach consists of two key components. First, we introduce Visual Latent Alignment, where we optimize adversarial features to the malicious instructions in the visual embedding space based on a text-to-image generative model, ensuring that adversarial images subtly encode cues for malicious task execution. Subsequently, we present Textual Guidance Enhancement, where a large language model is leveraged to infer the black-box defensive system prompt through adversarial meta prompting and generate an malicious textual command that steers the agent's output toward better compliance with attackers' requests. Extensive experiments demonstrate that our method outperforms existing injection attacks, achieving at least a +26.4% increase in attack success rates across diverse tasks. Furthermore, we validate our attack's effectiveness in real-world multimodal autonomous agents, highlighting its potential implications for safety-critical applications."
  },
  {
    "title": "Manipulating Multimodal Agents via Cross-Modal Prompt Injection",
    "url": "http://arxiv.org/abs/2504.14348v2",
    "arxiv_id": "2504.14348v2",
    "authors": [
      "Le Wang",
      "Zonghao Ying",
      "Tianyuan Zhang",
      "Siyuan Liang",
      "Shengshan Hu",
      "Mingchuan Zhang",
      "Aishan Liu",
      "Xianglong Liu"
    ],
    "published": "2025-04-19T16:28:03+00:00",
    "summary": "The emergence of multimodal large language models has redefined the agent paradigm by integrating language and vision modalities with external data sources, enabling agents to better interpret human instructions and execute increasingly complex tasks. However, in this work, we identify a critical yet previously overlooked security vulnerability in multimodal agents: cross-modal prompt injection attacks. To exploit this vulnerability, we propose CrossInject, a novel attack framework in which attackers embed adversarial perturbations across multiple modalities to align with target malicious content, allowing external instructions to hijack the agent's decision-making process and execute unauthorized tasks. Our approach consists of two key components. First, we introduce Visual Latent Alignment, where we optimize adversarial features to the malicious instructions in the visual embedding space based on a text-to-image generative model, ensuring that adversarial images subtly encode cues for malicious task execution. Subsequently, we present Textual Guidance Enhancement, where a large language model is leveraged to infer the black-box defensive system prompt through adversarial meta prompting and generate an malicious textual command that steers the agent's output toward better compliance with attackers' requests. Extensive experiments demonstrate that our method outperforms existing injection attacks, achieving at least a +26.4% increase in attack success rates across diverse tasks. Furthermore, we validate our attack's effectiveness in real-world multimodal autonomous agents, highlighting its potential implications for safety-critical applications."
  },
  {
    "title": "DLW-CI: A Dynamic Likelihood-Weighted Cooperative Infotaxis Approach for Multi-Source Search in Urban Environments Using Consumer Drone Networks",
    "url": "http://arxiv.org/abs/2504.14330v1",
    "arxiv_id": "2504.14330v1",
    "authors": [
      "Xiaoran Zhang",
      "Yatai Ji",
      "Yong Zhao",
      "Chuan Ai",
      "Bin Chen",
      "Zhengqiu Zhu"
    ],
    "published": "2025-04-19T15:44:09+00:00",
    "summary": "Consumer-grade drones equipped with low-cost sensors have emerged as a cornerstone of Autonomous Intelligent Systems (AISs) for environmental monitoring and hazardous substance detection in urban environments. However, existing research primarily addresses single-source search problems, overlooking the complexities of real-world urban scenarios where both the location and quantity of hazardous sources remain unknown. To address this issue, we propose the Dynamic Likelihood-Weighted Cooperative Infotaxis (DLW-CI) approach for consumer drone networks. Our approach enhances multi-drone collaboration in AISs by combining infotaxis (a cognitive search strategy) with optimized source term estimation and an innovative cooperative mechanism. Specifically, we introduce a novel source term estimation method that utilizes multiple parallel particle filters, with each filter dedicated to estimating the parameters of a potentially unknown source within the search scene. Furthermore, we develop a cooperative mechanism based on dynamic likelihood weights to prevent multiple drones from simultaneously estimating and searching for the same source, thus optimizing the energy efficiency and search coverage of the consumer AIS. Experimental results demonstrate that the DLW-CI approach significantly outperforms baseline methods regarding success rate, accuracy, and root mean square error, particularly in scenarios with relatively few sources, regardless of the presence of obstacles. Also, the effectiveness of the proposed approach is verified in a diffusion scenario generated by the computational fluid dynamics (CFD) model. Research findings indicate that our approach could improve source estimation accuracy and search efficiency by consumer drone-based AISs, making a valuable contribution to environmental safety monitoring applications within smart city infrastructure."
  },
  {
    "title": "Towards NSFW-Free Text-to-Image Generation via Safety-Constraint Direct Preference Optimization",
    "url": "http://arxiv.org/abs/2504.14290v1",
    "arxiv_id": "2504.14290v1",
    "authors": [
      "Shouwei Ruan",
      "Zhenyu Wu",
      "Yao Huang",
      "Ruochen Zhang",
      "Yitong Sun",
      "Caixin Kang",
      "Xingxing Wei"
    ],
    "published": "2025-04-19T13:26:46+00:00",
    "summary": "Ensuring the safety of generated content remains a fundamental challenge for Text-to-Image (T2I) generation. Existing studies either fail to guarantee complete safety under potentially harmful concepts or struggle to balance safety with generation quality. To address these issues, we propose Safety-Constrained Direct Preference Optimization (SC-DPO), a novel framework for safety alignment in T2I models. SC-DPO integrates safety constraints into the general human preference calibration, aiming to maximize the likelihood of generating human-preferred samples while minimizing the safety cost of the generated outputs. In SC-DPO, we introduce a safety cost model to accurately quantify harmful levels for images, and train it effectively using the proposed contrastive learning and cost anchoring objectives. To apply SC-DPO for effective T2I safety alignment, we constructed SCP-10K, a safety-constrained preference dataset containing rich harmful concepts, which blends safety-constrained preference pairs under both harmful and clean instructions, further mitigating the trade-off between safety and sample quality. Additionally, we propose a Dynamic Focusing Mechanism (DFM) for SC-DPO, promoting the model's learning of difficult preference pair samples. Extensive experiments demonstrate that SC-DPO outperforms existing methods, effectively defending against various NSFW content while maintaining optimal sample quality and human preference alignment. Additionally, SC-DPO exhibits resilience against adversarial prompts designed to generate harmful content."
  },
  {
    "title": "SRPO: A Cross-Domain Implementation of Large-Scale Reinforcement Learning on LLM",
    "url": "http://arxiv.org/abs/2504.14286v1",
    "arxiv_id": "2504.14286v1",
    "authors": [
      "Xiaojiang Zhang",
      "Jinghui Wang",
      "Zifei Cheng",
      "Wenhao Zhuang",
      "Zheng Lin",
      "Minglei Zhang",
      "Shaojie Wang",
      "Yinghan Cui",
      "Chao Wang",
      "Junyi Peng",
      "Shimiao Jiang",
      "Shiqi Kuang",
      "Shouyu Yin",
      "Chaohang Wen",
      "Haotian Zhang",
      "Bin Chen",
      "Bing Yu"
    ],
    "published": "2025-04-19T13:06:03+00:00",
    "summary": "Recent advances of reasoning models, exemplified by OpenAI's o1 and DeepSeek's R1, highlight the significant potential of Reinforcement Learning (RL) to enhance the reasoning capabilities of Large Language Models (LLMs). However, replicating these advancements across diverse domains remains challenging due to limited methodological transparency. In this work, we present two-Staged history-Resampling Policy Optimization (SRPO), which successfully surpasses the performance of DeepSeek-R1-Zero-32B on the AIME24 and LiveCodeBench benchmarks. SRPO achieves this using the same base model as DeepSeek (i.e. Qwen2.5-32B) and relies solely on RL, without prior Supervised Fine-Tuning (SFT). Building upon Group Relative Policy Optimization (GRPO), we introduce two key methodological innovations: (1) a two-stage cross-domain training paradigm designed to balance the development of mathematical reasoning and coding proficiency, and (2) History Resampling (HR), a technique to address ineffective samples. Our comprehensive experiments validate the effectiveness of our approach, dedicating to offer valuable insights into scaling LLM reasoning capabilities across diverse tasks."
  },
  {
    "title": "SRPO: A Cross-Domain Implementation of Large-Scale Reinforcement Learning on LLM",
    "url": "http://arxiv.org/abs/2504.14286v2",
    "arxiv_id": "2504.14286v2",
    "authors": [
      "Xiaojiang Zhang",
      "Jinghui Wang",
      "Zifei Cheng",
      "Wenhao Zhuang",
      "Zheng Lin",
      "Minglei Zhang",
      "Shaojie Wang",
      "Yinghan Cui",
      "Chao Wang",
      "Junyi Peng",
      "Shimiao Jiang",
      "Shiqi Kuang",
      "Shouyu Yin",
      "Chaohang Wen",
      "Haotian Zhang",
      "Bin Chen",
      "Bing Yu"
    ],
    "published": "2025-04-19T13:06:03+00:00",
    "summary": "Recent advances of reasoning models, exemplified by OpenAI's o1 and DeepSeek's R1, highlight the significant potential of Reinforcement Learning (RL) to enhance the reasoning capabilities of Large Language Models (LLMs). However, replicating these advancements across diverse domains remains challenging due to limited methodological transparency. In this work, we present two-Staged history-Resampling Policy Optimization (SRPO), which surpasses the performance of DeepSeek-R1-Zero-32B on the AIME24 and LiveCodeBench benchmarks. SRPO achieves this using the same base model as DeepSeek (i.e. Qwen2.5-32B), using only about 1/10 of the training steps required by DeepSeek-R1-Zero-32B, demonstrating superior efficiency. Building upon Group Relative Policy Optimization (GRPO), we introduce two key methodological innovations: (1) a two-stage cross-domain training paradigm designed to balance the development of mathematical reasoning and coding proficiency, and (2) History Resampling (HR), a technique to address ineffective samples. Our comprehensive experiments validate the effectiveness of our approach, offering valuable insights into scaling LLM reasoning capabilities across diverse tasks."
  },
  {
    "title": "Microscopic features of the effect of vehicle overacceleration on traffic flow",
    "url": "http://arxiv.org/abs/2504.14244v1",
    "arxiv_id": "2504.14244v1",
    "authors": [
      "Boris S. Kerner",
      "Sergey L. Klenov"
    ],
    "published": "2025-04-19T09:37:17+00:00",
    "summary": "Through the development of a microscopic deterministic model in the framework of three-phase traffic theory, microscopic features of vehicle overacceleration, which determines the occurrence of the metastability of free traffic flow at a bottleneck, have been revealed: (i) The greater the impact of vehicle overacceleration on free traffic flow at a bottleneck, the higher the maximum flow rate at which free flow can persist at the bottleneck, i.e., the better traffic breakdown can be avoided. (ii) There can be at least two mechanisms of overacceleration in road lane caused by safety acceleration at the bottleneck. (iii) Through a microscopic analysis of spatiotemporal competition between speed adaptation and vehicle acceleration behaviors, traffic conditions have been found at which safety acceleration in road lane or/and vehicle acceleration due to lane-changing on multi-lane road become overacceleration. (iv) There is spatiotemporal cooperation of different overacceleration mechanisms. (v) The stronger the overacceleration cooperation, the stronger the maintenance of free flow at the bottleneck due to overacceleration. (vi) On two-lane road, both speed adaptation and overacceleration in road lane can effect qualitatively on the overacceleration mechanism caused by lane-changing. These microscopic features of the effect of vehicle overacceleration on traffic flow are related to traffic flow consisting of human-driving or/and automated-driving vehicles."
  },
  {
    "title": "Know Me, Respond to Me: Benchmarking LLMs for Dynamic User Profiling and Personalized Responses at Scale",
    "url": "http://arxiv.org/abs/2504.14225v1",
    "arxiv_id": "2504.14225v1",
    "authors": [
      "Bowen Jiang",
      "Zhuoqun Hao",
      "Young-Min Cho",
      "Bryan Li",
      "Yuan Yuan",
      "Sihao Chen",
      "Lyle Ungar",
      "Camillo J. Taylor",
      "Dan Roth"
    ],
    "published": "2025-04-19T08:16:10+00:00",
    "summary": "Large Language Models (LLMs) have emerged as personalized assistants for users across a wide range of tasks -- from offering writing support to delivering tailored recommendations or consultations. Over time, the interaction history between a user and an LLM can provide extensive information about an individual's traits and preferences. However, open questions remain on how well LLMs today can effectively leverage such history to (1) internalize the user's inherent traits and preferences, (2) track how the user profiling and preferences evolve over time, and (3) generate personalized responses accordingly in new scenarios.   In this work, we introduce the PERSONAMEM benchmark. PERSONAMEM features curated user profiles with over 180 simulated user-LLM interaction histories, each containing up to 60 sessions of multi-turn conversations across 15 real-world tasks that require personalization. Given an in-situ user query, i.e. query issued by the user from the first-person perspective, we evaluate LLM chatbots' ability to identify the most suitable response according to the current state of the user's profile. We observe that current LLMs still struggle to recognize the dynamic evolution in users' profiles over time through direct prompting approaches. As a consequence, LLMs often fail to deliver responses that align with users' current situations and preferences, with frontier models such as GPT-4.1, o4-mini, GPT-4.5, o1, or Gemini-2.0 achieving only around 50% overall accuracy, suggesting room for improvement. We hope that PERSONAMEM, along with the user profile and conversation simulation pipeline, can facilitate future research in the development of truly user-aware chatbots. Code and data are available at github.com/bowen-upenn/PersonaMem."
  },
  {
    "title": "Amplify Initiative: Building A Localized Data Platform for Globalized AI",
    "url": "http://arxiv.org/abs/2504.14105v1",
    "arxiv_id": "2504.14105v1",
    "authors": [
      "Qazi Mamunur Rashid",
      "Erin van Liemt",
      "Tiffany Shih",
      "Amber Ebinama",
      "Karla Barrios Ramos",
      "Madhurima Maji",
      "Aishwarya Verma",
      "Charu Kalia",
      "Jamila Smith-Loud",
      "Joyce Nakatumba-Nabende",
      "Rehema Baguma",
      "Andrew Katumba",
      "Chodrine Mutebi",
      "Jagen Marvin",
      "Eric Peter Wairagala",
      "Mugizi Bruce",
      "Peter Oketta",
      "Lawrence Nderu",
      "Obichi Obiajunwa",
      "Abigail Oppong",
      "Michael Zimba",
      "Data Authors"
    ],
    "published": "2025-04-18T23:20:52+00:00",
    "summary": "Current AI models often fail to account for local context and language, given the predominance of English and Western internet content in their training data. This hinders the global relevance, usefulness, and safety of these models as they gain more users around the globe. Amplify Initiative, a data platform and methodology, leverages expert communities to collect diverse, high-quality data to address the limitations of these models. The platform is designed to enable co-creation of datasets, provide access to high-quality multilingual datasets, and offer recognition to data authors. This paper presents the approach to co-creating datasets with domain experts (e.g., health workers, teachers) through a pilot conducted in Sub-Saharan Africa (Ghana, Kenya, Malawi, Nigeria, and Uganda). In partnership with local researchers situated in these countries, the pilot demonstrated an end-to-end approach to co-creating data with 155 experts in sensitive domains (e.g., physicians, bankers, anthropologists, human and civil rights advocates). This approach, implemented with an Android app, resulted in an annotated dataset of 8,091 adversarial queries in seven languages (e.g., Luganda, Swahili, Chichewa), capturing nuanced and contextual information related to key themes such as misinformation and public interest topics. This dataset in turn can be used to evaluate models for their safety and cultural relevance within the context of these languages."
  },
  {
    "title": "LogicTree: Structured Proof Exploration for Coherent and Rigorous Logical Reasoning with Large Language Models",
    "url": "http://arxiv.org/abs/2504.14089v1",
    "arxiv_id": "2504.14089v1",
    "authors": [
      "Kang He",
      "Kaushik Roy"
    ],
    "published": "2025-04-18T22:10:02+00:00",
    "summary": "Large language models (LLMs) have achieved remarkable multi-step reasoning capabilities across various domains. However, LLMs still face distinct challenges in complex logical reasoning, as (1) proof-finding requires systematic exploration and the maintenance of logical coherence and (2) searching the right combination of premises at each reasoning step is inherently challenging in tasks with large premise space. To address this, we propose LogicTree, an inference-time modular framework employing algorithm-guided search to automate structured proof exploration and ensure logical coherence. Advancing beyond tree-of-thought (ToT), we incorporate caching mechanism into LogicTree to enable effective utilization of historical knowledge, preventing reasoning stagnation and minimizing redundancy. Furthermore, we address the combinatorial complexity of premise search by decomposing it into a linear process. The refined premise selection restricts subsequent inference to at most one derivation per step, enhancing reasoning granularity and enforcing strict step-by-step reasoning. Additionally, we introduce two LLM-free heuristics for premise prioritization, enabling strategic proof search. Experimental results on five datasets demonstrate that LogicTree optimally scales inference-time computation to achieve higher proof accuracy, surpassing chain-of-thought (CoT) and ToT with average gains of 23.6% and 12.5%, respectively, on GPT-4o. Moreover, within LogicTree, GPT-4o outperforms o3-mini by 7.6% on average."
  },
  {
    "title": "Infrared Vision Systems for Emergency Vehicle Driver Assistance in Low-Visibility Conditions",
    "url": "http://arxiv.org/abs/2504.14078v1",
    "arxiv_id": "2504.14078v1",
    "authors": [
      "M-Mahdi Naddaf-Sh",
      "Andrew Lee",
      "Kin Yen",
      "Eemon Amini",
      "Iman Soltani"
    ],
    "published": "2025-04-18T21:06:41+00:00",
    "summary": "This study investigates the potential of infrared (IR) camera technology to enhance driver safety for emergency vehicles operating in low-visibility conditions, particularly at night and in dense fog. Such environments significantly increase the risk of collisions, especially for tow trucks and snowplows that must remain operational in challenging conditions. Conventional driver assistance systems often struggle under these conditions due to limited visibility. In contrast, IR cameras, which detect the thermal signatures of obstacles, offer a promising alternative. The evaluation combines controlled laboratory experiments, real-world field tests, and surveys of emergency vehicle operators. In addition to assessing detection performance, the study examines the feasibility of retrofitting existing Department of Transportation (DoT) fleets with cost-effective IR-based driver assistance systems. Results underscore the utility of IR technology in enhancing driver awareness and provide data-driven recommendations for scalable deployment across legacy emergency vehicle fleets."
  },
  {
    "title": "Think Deep, Think Fast: Investigating Efficiency of Verifier-free Inference-time-scaling Methods",
    "url": "http://arxiv.org/abs/2504.14047v1",
    "arxiv_id": "2504.14047v1",
    "authors": [
      "Junlin Wang",
      "Shang Zhu",
      "Jon Saad-Falcon",
      "Ben Athiwaratkun",
      "Qingyang Wu",
      "Jue Wang",
      "Shuaiwen Leon Song",
      "Ce Zhang",
      "Bhuwan Dhingra",
      "James Zou"
    ],
    "published": "2025-04-18T19:32:55+00:00",
    "summary": "There is intense interest in investigating how inference time compute (ITC) (e.g. repeated sampling, refinements, etc) can improve large language model (LLM) capabilities. At the same time, recent breakthroughs in reasoning models, such as Deepseek-R1, unlock the opportunity for reinforcement learning to improve LLM reasoning skills. An in-depth understanding of how ITC interacts with reasoning across different models could provide important guidance on how to further advance the LLM frontier. This work conducts a comprehensive analysis of inference-time scaling methods for both reasoning and non-reasoning models on challenging reasoning tasks. Specifically, we focus our research on verifier-free inference time-scaling methods due to its generalizability without needing a reward model. We construct the Pareto frontier of quality and efficiency. We find that non-reasoning models, even with an extremely high inference budget, still fall substantially behind reasoning models. For reasoning models, majority voting proves to be a robust inference strategy, generally competitive or outperforming other more sophisticated ITC methods like best-of-N and sequential revisions, while the additional inference compute offers minimal improvements. We further perform in-depth analyses of the association of key response features (length and linguistic markers) with response quality, with which we can improve the existing ITC methods. We find that correct responses from reasoning models are typically shorter and have fewer hedging and thinking markers (but more discourse markers) than the incorrect responses."
  },
  {
    "title": "Multi-Stage Retrieval for Operational Technology Cybersecurity Compliance Using Large Language Models: A Railway Casestudy",
    "url": "http://arxiv.org/abs/2504.14044v1",
    "arxiv_id": "2504.14044v1",
    "authors": [
      "Regan Bolton",
      "Mohammadreza Sheikhfathollahi",
      "Simon Parkinson",
      "Dan Basher",
      "Howard Parkinson"
    ],
    "published": "2025-04-18T19:24:17+00:00",
    "summary": "Operational Technology Cybersecurity (OTCS) continues to be a dominant challenge for critical infrastructure such as railways. As these systems become increasingly vulnerable to malicious attacks due to digitalization, effective documentation and compliance processes are essential to protect these safety-critical systems. This paper proposes a novel system that leverages Large Language Models (LLMs) and multi-stage retrieval to enhance the compliance verification process against standards like IEC 62443 and the rail-specific IEC 63452. We first evaluate a Baseline Compliance Architecture (BCA) for answering OTCS compliance queries, then develop an extended approach called Parallel Compliance Architecture (PCA) that incorporates additional context from regulatory standards. Through empirical evaluation comparing OpenAI-gpt-4o and Claude-3.5-haiku models in these architectures, we demonstrate that the PCA significantly improves both correctness and reasoning quality in compliance verification. Our research establishes metrics for response correctness, logical reasoning, and hallucination detection, highlighting the strengths and limitations of using LLMs for compliance verification in railway cybersecurity. The results suggest that retrieval-augmented approaches can significantly improve the efficiency and accuracy of compliance assessments, particularly valuable in an industry facing a shortage of cybersecurity expertise."
  },
  {
    "title": "Statistical Analysis and End-to-End Performance Evaluation of Traffic Models for Automotive Data",
    "url": "http://arxiv.org/abs/2504.14017v1",
    "arxiv_id": "2504.14017v1",
    "authors": [
      "Marcello Bullo",
      "Amir Ashtari Gargari",
      "Paolo Testolina",
      "Michele Zorzi",
      "Marco Giordani"
    ],
    "published": "2025-04-18T18:12:08+00:00",
    "summary": "Autonomous driving is a major paradigm shift in transportation, with the potential to enhance safety, optimize traffic congestion, and reduce fuel consumption. Although autonomous vehicles rely on advanced sensors and on-board computing systems to navigate without human control, full awareness of the driving environment also requires a cooperative effort via Vehicle-To-Everything (V2X) communication. Specifically, vehicles send and receive sensor perceptions to/from other vehicles to extend perception beyond their own sensing range. However, transmitting large volumes of data can be challenging for current V2X communication technologies, so data compression represents a crucial solution to reduce the message size and link congestion. In this paper, we present a statistical characterization of automotive data, focusing on LiDAR sensors. Notably, we provide models for the size of both raw and compressed point clouds. The use of statistical traffic models offers several advantages compared to using real data, such as faster simulations, reduced storage requirements, and greater flexibility in the application design. Furthermore, statistical models can be used for understanding traffic patterns and analyzing statistics, which is crucial to design and optimize wireless networks. We validate our statistical models via a Kolmogorov-Smirnoff test implementing a Bootstrap Resampling scheme. Moreover, we show via ns-3 simulations that using statistical models yields comparable results in terms of latency and throughput compared to real data, which also demonstrates the accuracy of the models."
  },
  {
    "title": "DiffOG: Differentiable Policy Trajectory Optimization with Generalizability",
    "url": "http://arxiv.org/abs/2504.13807v1",
    "arxiv_id": "2504.13807v1",
    "authors": [
      "Zhengtong Xu",
      "Zichen Miao",
      "Qiang Qiu",
      "Zhe Zhang",
      "Yu She"
    ],
    "published": "2025-04-18T17:20:27+00:00",
    "summary": "Imitation learning-based visuomotor policies excel at manipulation tasks but often produce suboptimal action trajectories compared to model-based methods. Directly mapping camera data to actions via neural networks can result in jerky motions and difficulties in meeting critical constraints, compromising safety and robustness in real-world deployment. For tasks that require high robustness or strict adherence to constraints, ensuring trajectory quality is crucial. However, the lack of interpretability in neural networks makes it challenging to generate constraint-compliant actions in a controlled manner. This paper introduces differentiable policy trajectory optimization with generalizability (DiffOG), a learning-based trajectory optimization framework designed to enhance visuomotor policies. By leveraging the proposed differentiable formulation of trajectory optimization with transformer, DiffOG seamlessly integrates policies with a generalizable optimization layer. Visuomotor policies enhanced by DiffOG generate smoother, constraint-compliant action trajectories in a more interpretable way. DiffOG exhibits strong generalization capabilities and high flexibility. We evaluated DiffOG across 11 simulated tasks and 2 real-world tasks. The results demonstrate that DiffOG significantly enhances the trajectory quality of visuomotor policies while having minimal impact on policy performance, outperforming trajectory processing baselines such as greedy constraint clipping and penalty-based trajectory optimization. Furthermore, DiffOG achieves superior performance compared to existing constrained visuomotor policy."
  },
  {
    "title": "Meta-Learning and Knowledge Discovery based Physics-Informed Neural Network for Remaining Useful Life Prediction",
    "url": "http://arxiv.org/abs/2504.13797v1",
    "arxiv_id": "2504.13797v1",
    "authors": [
      "Yu Wang",
      "Shujie Liu",
      "Shuai Lv",
      "Gengshuo Liu"
    ],
    "published": "2025-04-18T16:58:38+00:00",
    "summary": "Predicting the remaining useful life (RUL) of rotating machinery is critical for industrial safety and maintenance, but existing methods struggle with scarce target-domain data and unclear degradation dynamics. We propose a Meta-Learning and Knowledge Discovery-based Physics-Informed Neural Network (MKDPINN) to address these challenges. The method first maps noisy sensor data to a low-dimensional hidden state space via a Hidden State Mapper (HSM). A Physics-Guided Regulator (PGR) then learns unknown nonlinear PDEs governing degradation evolution, embedding these physical constraints into the PINN framework. This integrates data-driven and physics-based approaches. The framework uses meta-learning, optimizing across source-domain meta-tasks to enable few-shot adaptation to new target tasks. Experiments on industrial data and the C-MAPSS benchmark show MKDPINN outperforms baselines in generalization and accuracy, proving its effectiveness for RUL prediction under data scarcity"
  },
  {
    "title": "Revisiting Uncertainty Quantification Evaluation in Language Models: Spurious Interactions with Response Length Bias Results",
    "url": "http://arxiv.org/abs/2504.13677v1",
    "arxiv_id": "2504.13677v1",
    "authors": [
      "Andrea Santilli",
      "Adam Golinski",
      "Michael Kirchhof",
      "Federico Danieli",
      "Arno Blaas",
      "Miao Xiong",
      "Luca Zappella",
      "Sinead Williamson"
    ],
    "published": "2025-04-18T13:13:42+00:00",
    "summary": "Uncertainty Quantification (UQ) in Language Models (LMs) is crucial for improving their safety and reliability. Evaluations often use performance metrics like AUROC to assess how well UQ methods (e.g., negative sequence probabilities) correlate with task correctness functions (e.g., ROUGE-L). In this paper, we show that commonly used correctness functions bias UQ evaluations by inflating the performance of certain UQ methods. We evaluate 7 correctness functions -- from lexical-based and embedding-based metrics to LLM-as-a-judge approaches -- across 4 datasets x 4 models x 6 UQ methods. Our analysis reveals that length biases in the errors of these correctness functions distort UQ assessments by interacting with length biases in UQ methods. We identify LLM-as-a-judge approaches as among the least length-biased choices and hence a potential solution to mitigate these biases."
  },
  {
    "title": "Enhancing Pothole Detection and Characterization: Integrated Segmentation and Depth Estimation in Road Anomaly Systems",
    "url": "http://arxiv.org/abs/2504.13648v1",
    "arxiv_id": "2504.13648v1",
    "authors": [
      "Uthman Baroudi",
      "Alala BaHamid",
      "Yasser Elalfy",
      "Ziad Al Alami"
    ],
    "published": "2025-04-18T11:59:38+00:00",
    "summary": "Road anomaly detection plays a crucial role in road maintenance and in enhancing the safety of both drivers and vehicles. Recent machine learning approaches for road anomaly detection have overcome the tedious and time-consuming process of manual analysis and anomaly counting; however, they often fall short in providing a complete characterization of road potholes. In this paper, we leverage transfer learning by adopting a pre-trained YOLOv8-seg model for the automatic characterization of potholes using digital images captured from a dashboard-mounted camera. Our work includes the creation of a novel dataset, comprising both images and their corresponding depth maps, collected from diverse road environments in Al-Khobar city and the KFUPM campus in Saudi Arabia. Our approach performs pothole detection and segmentation to precisely localize potholes and calculate their area. Subsequently, the segmented image is merged with its depth map to extract detailed depth information about the potholes. This integration of segmentation and depth data offers a more comprehensive characterization compared to previous deep learning-based road anomaly detection systems. Overall, this method not only has the potential to significantly enhance autonomous vehicle navigation by improving the detection and characterization of road hazards but also assists road maintenance authorities in responding more effectively to road damage."
  },
  {
    "title": "Thought Manipulation: External Thought Can Be Efficient for Large Reasoning Models",
    "url": "http://arxiv.org/abs/2504.13626v1",
    "arxiv_id": "2504.13626v1",
    "authors": [
      "Yule Liu",
      "Jingyi Zheng",
      "Zhen Sun",
      "Zifan Peng",
      "Wenhan Dong",
      "Zeyang Sha",
      "Shiwen Cui",
      "Weiqiang Wang",
      "Xinlei He"
    ],
    "published": "2025-04-18T11:07:19+00:00",
    "summary": "Recent advancements in large reasoning models (LRMs) have demonstrated the effectiveness of scaling test-time computation to enhance reasoning capabilities in multiple tasks. However, LRMs typically suffer from \"overthinking\" problems, where models generate significantly redundant reasoning steps while bringing limited performance gains. Existing work relies on fine-tuning to mitigate overthinking, which requires additional data, unconventional training setups, risky safety misalignment, and poor generalization.   Through empirical analysis, we reveal an important characteristic of LRM behaviors that placing external CoTs generated by smaller models between the thinking token ($\\texttt{<think>}$ and $\\texttt{</think>)}$ can effectively manipulate the model to generate fewer thoughts. Building on these insights, we propose a simple yet efficient pipeline, ThoughtMani, to enable LRMs to bypass unnecessary intermediate steps and reduce computational costs significantly. We conduct extensive experiments to validate the utility and efficiency of ThoughtMani. For instance, when applied to QwQ-32B on the LiveBench/Code dataset, ThoughtMani keeps the original performance and reduces output token counts by approximately 30%, with little overhead from the CoT generator. Furthermore, we find that ThoughtMani enhances safety alignment by an average of 10%. Since model vendors typically serve models of different sizes simultaneously, ThoughtMani provides an effective way to construct more efficient and accessible LRMs for real-world applications."
  },
  {
    "title": "Hysteresis-Aware Neural Network Modeling and Whole-Body Reinforcement Learning Control of Soft Robots",
    "url": "http://arxiv.org/abs/2504.13582v1",
    "arxiv_id": "2504.13582v1",
    "authors": [
      "Zongyuan Chen",
      "Yan Xia",
      "Jiayuan Liu",
      "Jijia Liu",
      "Wenhao Tang",
      "Jiayu Chen",
      "Feng Gao",
      "Longfei Ma",
      "Hongen Liao",
      "Yu Wang",
      "Chao Yu",
      "Boyu Zhang",
      "Fei Xing"
    ],
    "published": "2025-04-18T09:34:56+00:00",
    "summary": "Soft robots exhibit inherent compliance and safety, which makes them particularly suitable for applications requiring direct physical interaction with humans, such as surgical procedures. However, their nonlinear and hysteretic behavior, resulting from the properties of soft materials, presents substantial challenges for accurate modeling and control. In this study, we present a soft robotic system designed for surgical applications and propose a hysteresis-aware whole-body neural network model that accurately captures and predicts the soft robot's whole-body motion, including its hysteretic behavior. Building upon the high-precision dynamic model, we construct a highly parallel simulation environment for soft robot control and apply an on-policy reinforcement learning algorithm to efficiently train whole-body motion control strategies. Based on the trained control policy, we developed a soft robotic system for surgical applications and validated it through phantom-based laser ablation experiments in a physical environment. The results demonstrate that the hysteresis-aware modeling reduces the Mean Squared Error (MSE) by 84.95 percent compared to traditional modeling methods. The deployed control algorithm achieved a trajectory tracking error ranging from 0.126 to 0.250 mm on the real soft robot, highlighting its precision in real-world conditions. The proposed method showed strong performance in phantom-based surgical experiments and demonstrates its potential for complex scenarios, including future real-world clinical applications."
  },
  {
    "title": "DETAM: Defending LLMs Against Jailbreak Attacks via Targeted Attention Modification",
    "url": "http://arxiv.org/abs/2504.13562v1",
    "arxiv_id": "2504.13562v1",
    "authors": [
      "Yu Li",
      "Han Jiang",
      "Zhihua Wei"
    ],
    "published": "2025-04-18T09:02:12+00:00",
    "summary": "With the widespread adoption of Large Language Models (LLMs), jailbreak attacks have become an increasingly pressing safety concern. While safety-aligned LLMs can effectively defend against normal harmful queries, they remain vulnerable to such attacks. Existing defense methods primarily rely on fine-tuning or input modification, which often suffer from limited generalization and reduced utility. To address this, we introduce DETAM, a finetuning-free defense approach that improves the defensive capabilities against jailbreak attacks of LLMs via targeted attention modification. Specifically, we analyze the differences in attention scores between successful and unsuccessful defenses to identify the attention heads sensitive to jailbreak attacks. During inference, we reallocate attention to emphasize the user's core intention, minimizing interference from attack tokens. Our experimental results demonstrate that DETAM outperforms various baselines in jailbreak defense and exhibits robust generalization across different attacks and models, maintaining its effectiveness even on in-the-wild jailbreak data. Furthermore, in evaluating the model's utility, we incorporated over-defense datasets, which further validate the superior performance of our approach. The code will be released immediately upon acceptance."
  },
  {
    "title": "Risk-aware black-box portfolio construction using Bayesian optimization with adaptive weighted Lagrangian estimator",
    "url": "http://arxiv.org/abs/2504.13529v1",
    "arxiv_id": "2504.13529v1",
    "authors": [
      "Zinuo You",
      "John Cartlidge",
      "Karen Elliott",
      "Menghan Ge",
      "Daniel Gold"
    ],
    "published": "2025-04-18T07:40:24+00:00",
    "summary": "Existing portfolio management approaches are often black-box models due to safety and commercial issues in the industry. However, their performance can vary considerably whenever market conditions or internal trading strategies change. Furthermore, evaluating these non-transparent systems is expensive, where certain budgets limit observations of the systems. Therefore, optimizing performance while controlling the potential risk of these financial systems has become a critical challenge. This work presents a novel Bayesian optimization framework to optimize black-box portfolio management models under limited observations. In conventional Bayesian optimization settings, the objective function is to maximize the expectation of performance metrics. However, simply maximizing performance expectations leads to erratic optimization trajectories, which exacerbate risk accumulation in portfolio management. Meanwhile, this can lead to misalignment between the target distribution and the actual distribution of the black-box model. To mitigate this problem, we propose an adaptive weight Lagrangian estimator considering dual objective, which incorporates maximizing model performance and minimizing variance of model observations. Extensive experiments demonstrate the superiority of our approach over five backtest settings with three black-box stock portfolio management models. Ablation studies further verify the effectiveness of the proposed estimator."
  },
  {
    "title": "Monitor and Recover: A Paradigm for Future Research on Distribution Shift in Learning-Enabled Cyber-Physical Systems",
    "url": "http://arxiv.org/abs/2504.13484v1",
    "arxiv_id": "2504.13484v1",
    "authors": [
      "Vivian Lin",
      "Insup Lee"
    ],
    "published": "2025-04-18T05:48:35+00:00",
    "summary": "With the known vulnerability of neural networks to distribution shift, maintaining reliability in learning-enabled cyber-physical systems poses a salient challenge. In response, many existing methods adopt a detect and abstain methodology, aiming to detect distribution shift at inference time so that the learning-enabled component can abstain from decision-making. This approach, however, has limited use in real-world applications. We instead propose a monitor and recover paradigm as a promising direction for future research. This philosophy emphasizes 1) robust safety monitoring instead of distribution shift detection and 2) distribution shift recovery instead of abstention. We discuss two examples from our recent work."
  },
  {
    "title": "Safety Monitoring for Learning-Enabled Cyber-Physical Systems in Out-of-Distribution Scenarios",
    "url": "http://arxiv.org/abs/2504.13478v1",
    "arxiv_id": "2504.13478v1",
    "authors": [
      "Vivian Lin",
      "Ramneet Kaur",
      "Yahan Yang",
      "Souradeep Dutta",
      "Yiannis Kantaros",
      "Anirban Roy",
      "Susmit Jha",
      "Oleg Sokolsky",
      "Insup Lee"
    ],
    "published": "2025-04-18T05:42:37+00:00",
    "summary": "The safety of learning-enabled cyber-physical systems is compromised by the well-known vulnerabilities of deep neural networks to out-of-distribution (OOD) inputs. Existing literature has sought to monitor the safety of such systems by detecting OOD data. However, such approaches have limited utility, as the presence of an OOD input does not necessarily imply the violation of a desired safety property. We instead propose to directly monitor safety in a manner that is itself robust to OOD data. To this end, we predict violations of signal temporal logic safety specifications based on predicted future trajectories. Our safety monitor additionally uses a novel combination of adaptive conformal prediction and incremental learning. The former obtains probabilistic prediction guarantees even on OOD data, and the latter prevents overly conservative predictions. We evaluate the efficacy of the proposed approach in two case studies on safety monitoring: 1) predicting collisions of an F1Tenth car with static obstacles, and 2) predicting collisions of a race car with multiple dynamic obstacles. We find that adaptive conformal prediction obtains theoretical guarantees where other uncertainty quantification methods fail to do so. Additionally, combining adaptive conformal prediction and incremental learning for safety monitoring achieves high recall and timeliness while reducing loss in precision. We achieve these results even in OOD settings and outperform alternative methods."
  },
  {
    "title": "Testing the Fault-Tolerance of Multi-Sensor Fusion Perception in Autonomous Driving Systems",
    "url": "http://arxiv.org/abs/2504.13420v1",
    "arxiv_id": "2504.13420v1",
    "authors": [
      "Haoxiang Tian",
      "Wenqiang Ding",
      "Xingshuo Han",
      "Guoquan Wu",
      "An Guo",
      "Junqi Zhang. Wei Chen",
      "Jun Wei",
      "Tianwei Zhang"
    ],
    "published": "2025-04-18T02:37:55+00:00",
    "summary": "High-level Autonomous Driving Systems (ADSs), such as Google Waymo and Baidu Apollo, typically rely on multi-sensor fusion (MSF) based approaches to perceive their surroundings. This strategy increases perception robustness by combining the respective strengths of the camera and LiDAR and directly affects the safety-critical driving decisions of autonomous vehicles (AVs). However, in real-world autonomous driving scenarios, cameras and LiDAR are subject to various faults, which can probably significantly impact the decision-making and behaviors of ADSs. Existing MSF testing approaches only discovered corner cases that the MSF-based perception cannot accurately detected by MSF-based perception, while lacking research on how sensor faults affect the system-level behaviors of ADSs.   To address this gap, we conduct the first exploration of the fault tolerance of MSF perception-based ADS for sensor faults. In this paper, we systematically and comprehensively build fault models for cameras and LiDAR in AVs and inject them into the MSF perception-based ADS to test its behaviors in test scenarios. To effectively and efficiently explore the parameter spaces of sensor fault models, we design a feedback-guided differential fuzzer to discover the safety violations of MSF perception-based ADS caused by the injected sensor faults. We evaluate FADE on the representative and practical industrial ADS, Baidu Apollo. Our evaluation results demonstrate the effectiveness and efficiency of FADE, and we conclude some useful findings from the experimental results. To validate the findings in the physical world, we use a real Baidu Apollo 6.0 EDU autonomous vehicle to conduct the physical experiments, and the results show the practical significance of our findings."
  },
  {
    "title": "LangCoop: Collaborative Driving with Language",
    "url": "http://arxiv.org/abs/2504.13406v1",
    "arxiv_id": "2504.13406v1",
    "authors": [
      "Xiangbo Gao",
      "Yuheng Wu",
      "Rujia Wang",
      "Chenxi Liu",
      "Yang Zhou",
      "Zhengzhong Tu"
    ],
    "published": "2025-04-18T02:03:14+00:00",
    "summary": "Multi-agent collaboration holds great promise for enhancing the safety, reliability, and mobility of autonomous driving systems by enabling information sharing among multiple connected agents. However, existing multi-agent communication approaches are hindered by limitations of existing communication media, including high bandwidth demands, agent heterogeneity, and information loss. To address these challenges, we introduce LangCoop, a new paradigm for collaborative autonomous driving that leverages natural language as a compact yet expressive medium for inter-agent communication. LangCoop features two key innovations: Mixture Model Modular Chain-of-thought (M$^3$CoT) for structured zero-shot vision-language reasoning and Natural Language Information Packaging (LangPack) for efficiently packaging information into concise, language-based messages. Through extensive experiments conducted in the CARLA simulations, we demonstrate that LangCoop achieves a remarkable 96\\% reduction in communication bandwidth (< 2KB per message) compared to image-based communication, while maintaining competitive driving performance in the closed-loop evaluation."
  },
  {
    "title": "Towards a Multi-Agent Vision-Language System for Zero-Shot Novel Hazardous Object Detection for Autonomous Driving Safety",
    "url": "http://arxiv.org/abs/2504.13399v1",
    "arxiv_id": "2504.13399v1",
    "authors": [
      "Shashank Shriram",
      "Srinivasa Perisetla",
      "Aryan Keskar",
      "Harsha Krishnaswamy",
      "Tonko Emil Westerhof Bossen",
      "Andreas M\u00f8gelmose",
      "Ross Greer"
    ],
    "published": "2025-04-18T01:25:02+00:00",
    "summary": "Detecting anomalous hazards in visual data, particularly in video streams, is a critical challenge in autonomous driving. Existing models often struggle with unpredictable, out-of-label hazards due to their reliance on predefined object categories. In this paper, we propose a multimodal approach that integrates vision-language reasoning with zero-shot object detection to improve hazard identification and explanation. Our pipeline consists of a Vision-Language Model (VLM), a Large Language Model (LLM), in order to detect hazardous objects within a traffic scene. We refine object detection by incorporating OpenAI's CLIP model to match predicted hazards with bounding box annotations, improving localization accuracy. To assess model performance, we create a ground truth dataset by denoising and extending the foundational COOOL (Challenge-of-Out-of-Label) anomaly detection benchmark dataset with complete natural language descriptions for hazard annotations. We define a means of hazard detection and labeling evaluation on the extended dataset using cosine similarity. This evaluation considers the semantic similarity between the predicted hazard description and the annotated ground truth for each video. Additionally, we release a set of tools for structuring and managing large-scale hazard detection datasets. Our findings highlight the strengths and limitations of current vision-language-based approaches, offering insights into future improvements in autonomous hazard detection systems. Our models, scripts, and data can be found at https://github.com/mi3labucm/COOOLER.git"
  },
  {
    "title": "Leveraging Functional Encryption and Deep Learning for Privacy-Preserving Traffic Forecasting",
    "url": "http://arxiv.org/abs/2504.13267v1",
    "arxiv_id": "2504.13267v1",
    "authors": [
      "Isaac Adom",
      "Mohammmad Iqbal Hossain",
      "Hassan Mahmoud",
      "Ahmad Alsharif",
      "Mahmoud Nabil Mahmoud",
      "Yang Xiao"
    ],
    "published": "2025-04-17T18:21:55+00:00",
    "summary": "Over the past few years, traffic congestion has continuously plagued the nation's transportation system creating several negative impacts including longer travel times, increased pollution rates, and higher collision risks. To overcome these challenges, Intelligent Transportation Systems (ITS) aim to improve mobility and vehicular systems, ensuring higher levels of safety by utilizing cutting-edge technologies, sophisticated sensing capabilities, and innovative algorithms. Drivers' participatory sensing, current/future location reporting, and machine learning algorithms have considerably improved real-time congestion monitoring and future traffic management. However, each driver's sensitive spatiotemporal location information can create serious privacy concerns. To address these challenges, we propose in this paper a secure, privacy-preserving location reporting and traffic forecasting system that guarantees privacy protection of driver data while maintaining high traffic forecasting accuracy. Our novel k-anonymity scheme utilizes functional encryption to aggregate encrypted location information submitted by drivers while ensuring the privacy of driver location data. Additionally, using the aggregated encrypted location information as input, this research proposes a deep learning model that incorporates a Convolutional-Long Short-Term Memory (Conv-LSTM) module to capture spatial and short-term temporal features and a Bidirectional Long Short-Term Memory (Bi-LSTM) module to recover long-term periodic patterns for traffic forecasting. With extensive evaluation on real datasets, we demonstrate the effectiveness of the proposed scheme with less than 10% mean absolute error for a 60-minute forecasting horizon, all while protecting driver privacy."
  },
  {
    "title": "Generate, but Verify: Reducing Hallucination in Vision-Language Models with Retrospective Resampling",
    "url": "http://arxiv.org/abs/2504.13169v1",
    "arxiv_id": "2504.13169v1",
    "authors": [
      "Tsung-Han Wu",
      "Heekyung Lee",
      "Jiaxin Ge",
      "Joseph E. Gonzalez",
      "Trevor Darrell",
      "David M. Chan"
    ],
    "published": "2025-04-17T17:59:22+00:00",
    "summary": "Vision-Language Models (VLMs) excel at visual understanding but often suffer from visual hallucinations, where they generate descriptions of nonexistent objects, actions, or concepts, posing significant risks in safety-critical applications. Existing hallucination mitigation methods typically follow one of two paradigms: generation adjustment, which modifies decoding behavior to align text with visual inputs, and post-hoc verification, where external models assess and correct outputs. While effective, generation adjustment methods often rely on heuristics and lack correction mechanisms, while post-hoc verification is complicated, typically requiring multiple models and tending to reject outputs rather than refine them. In this work, we introduce REVERSE, a unified framework that integrates hallucination-aware training with on-the-fly self-verification. By leveraging a new hallucination-verification dataset containing over 1.3M semi-synthetic samples, along with a novel inference-time retrospective resampling technique, our approach enables VLMs to both detect hallucinations during generation and dynamically revise those hallucinations. Our evaluations show that REVERSE achieves state-of-the-art hallucination reduction, outperforming the best existing methods by up to 12% on CHAIR-MSCOCO and 28% on HaloQuest. Our dataset, model, and code are available at: https://reverse-vlm.github.io."
  },
  {
    "title": "Energy-Based Reward Models for Robust Language Model Alignment",
    "url": "http://arxiv.org/abs/2504.13134v1",
    "arxiv_id": "2504.13134v1",
    "authors": [
      "Anamika Lochab",
      "Ruqi Zhang"
    ],
    "published": "2025-04-17T17:47:15+00:00",
    "summary": "Reward models (RMs) are essential for aligning Large Language Models (LLMs) with human preferences. However, they often struggle with capturing complex human preferences and generalizing to unseen data. To address these challenges, we introduce Energy-Based Reward Model (EBRM), a lightweight post-hoc refinement framework that enhances RM robustness and generalization. EBRM models the reward distribution explicitly, capturing uncertainty in human preferences and mitigating the impact of noisy or misaligned annotations. It achieves this through conflict-aware data filtering, label-noise-aware contrastive training, and hybrid initialization. Notably, EBRM enhances RMs without retraining, making it computationally efficient and adaptable across different models and tasks. Empirical evaluations on RM benchmarks demonstrate significant improvements in both robustness and generalization, achieving up to a 5.97% improvement in safety-critical alignment tasks compared to standard RMs. Furthermore, reinforcement learning experiments confirm that our refined rewards enhance alignment quality, effectively delaying reward hacking. These results demonstrate our approach as a scalable and effective enhancement for existing RMs and alignment pipelines. The code is available at EBRM."
  },
  {
    "title": "LLMs Meet Finance: Fine-Tuning Foundation Models for the Open FinLLM Leaderboard",
    "url": "http://arxiv.org/abs/2504.13125v1",
    "arxiv_id": "2504.13125v1",
    "authors": [
      "Varun Rao",
      "Youran Sun",
      "Mahendra Kumar",
      "Tejas Mutneja",
      "Agastya Mukherjee",
      "Haizhao Yang"
    ],
    "published": "2025-04-17T17:42:02+00:00",
    "summary": "This paper investigates the application of large language models (LLMs) to financial tasks. We fine-tuned foundation models using the Open FinLLM Leaderboard as a benchmark. Building on Qwen2.5 and Deepseek-R1, we employed techniques including supervised fine-tuning (SFT), direct preference optimization (DPO), and reinforcement learning (RL) to enhance their financial capabilities. The fine-tuned models demonstrated substantial performance gains across a wide range of financial tasks. Moreover, we measured the data scaling law in the financial domain. Our work demonstrates the potential of large language models (LLMs) in financial applications."
  },
  {
    "title": "Accuracy is Not Agreement: Expert-Aligned Evaluation of Crash Narrative Classification Models",
    "url": "http://arxiv.org/abs/2504.13068v1",
    "arxiv_id": "2504.13068v1",
    "authors": [
      "Sudesh Ramesh Bhagat",
      "Ibne Farabi Shihab",
      "Anuj Sharma"
    ],
    "published": "2025-04-17T16:29:08+00:00",
    "summary": "This study explores the relationship between deep learning (DL) model accuracy and expert agreement in the classification of crash narratives. We evaluate five DL models -- including BERT variants, the Universal Sentence Encoder (USE), and a zero-shot classifier -- against expert-labeled data and narrative text. The analysis is further extended to four large language models (LLMs): GPT-4, LLaMA 3, Qwen, and Claude. Our results reveal a counterintuitive trend: models with higher technical accuracy often exhibit lower agreement with domain experts, whereas LLMs demonstrate greater expert alignment despite relatively lower accuracy scores. To quantify and interpret model-expert agreement, we employ Cohen's Kappa, Principal Component Analysis (PCA), and SHAP-based explainability techniques. Findings indicate that expert-aligned models tend to rely more on contextual and temporal language cues, rather than location-specific keywords. These results underscore that accuracy alone is insufficient for evaluating models in safety-critical NLP applications. We advocate for incorporating expert agreement as a complementary metric in model evaluation frameworks and highlight the promise of LLMs as interpretable, scalable tools for crash analysis pipelines."
  },
  {
    "title": "GraphAttack: Exploiting Representational Blindspots in LLM Safety Mechanisms",
    "url": "http://arxiv.org/abs/2504.13052v1",
    "arxiv_id": "2504.13052v1",
    "authors": [
      "Sinan He",
      "An Wang"
    ],
    "published": "2025-04-17T16:09:12+00:00",
    "summary": "Large Language Models (LLMs) have been equipped with safety mechanisms to prevent harmful outputs, but these guardrails can often be bypassed through \"jailbreak\" prompts. This paper introduces a novel graph-based approach to systematically generate jailbreak prompts through semantic transformations. We represent malicious prompts as nodes in a graph structure with edges denoting different transformations, leveraging Abstract Meaning Representation (AMR) and Resource Description Framework (RDF) to parse user goals into semantic components that can be manipulated to evade safety filters. We demonstrate a particularly effective exploitation vector by instructing LLMs to generate code that realizes the intent described in these semantic graphs, achieving success rates of up to 87% against leading commercial LLMs. Our analysis reveals that contextual framing and abstraction are particularly effective at circumventing safety measures, highlighting critical gaps in current safety alignment techniques that focus primarily on surface-level patterns. These findings provide insights for developing more robust safeguards against structured semantic attacks. Our research contributes both a theoretical framework and practical methodology for systematically stress-testing LLM safety mechanisms."
  },
  {
    "title": "QI-MPC: A Hybrid Quantum-Inspired Model Predictive Control for Learning Optimal Policies",
    "url": "http://arxiv.org/abs/2504.13041v1",
    "arxiv_id": "2504.13041v1",
    "authors": [
      "Muhammad Al-Zafar Khan",
      "Jamal Al-Karaki"
    ],
    "published": "2025-04-17T15:55:37+00:00",
    "summary": "In this paper, we present Quantum-Inspired Model Predictive Control (QIMPC), an approach that uses Variational Quantum Circuits (VQCs) to learn control polices in MPC problems. The viability of the approach is tested in five experiments: A target-tracking control strategy, energy-efficient building climate control, autonomous vehicular dynamics, the simple pendulum, and the compound pendulum. Three safety guarantees were established for the approach, and the experiments gave the motivation for two important theoretical results that, in essence, identify systems for which the approach works best."
  },
  {
    "title": "Safe Physics-Informed Machine Learning for Dynamics and Control",
    "url": "http://arxiv.org/abs/2504.12952v1",
    "arxiv_id": "2504.12952v1",
    "authors": [
      "Jan Drgona",
      "Truong X. Nghiem",
      "Thomas Beckers",
      "Mahyar Fazlyab",
      "Enrique Mallada",
      "Colin Jones",
      "Draguna Vrabie",
      "Steven L. Brunton",
      "Rolf Findeisen"
    ],
    "published": "2025-04-17T13:52:55+00:00",
    "summary": "This tutorial paper focuses on safe physics-informed machine learning in the context of dynamics and control, providing a comprehensive overview of how to integrate physical models and safety guarantees. As machine learning techniques enhance the modeling and control of complex dynamical systems, ensuring safety and stability remains a critical challenge, especially in safety-critical applications like autonomous vehicles, robotics, medical decision-making, and energy systems. We explore various approaches for embedding and ensuring safety constraints, such as structural priors, Lyapunov functions, Control Barrier Functions, predictive control, projections, and robust optimization techniques, ensuring that the learned models respect stability and safety criteria. Additionally, we delve into methods for uncertainty quantification and safety verification, including reachability analysis and neural network verification tools, which help validate that control policies remain within safe operating bounds even in uncertain environments. The paper includes illustrative examples demonstrating the implementation aspects of safe learning frameworks that combine the strengths of data-driven approaches with the rigor of physical principles, offering a path toward the safe control of complex dynamical systems."
  },
  {
    "title": "In Which Areas of Technical AI Safety Could Geopolitical Rivals Cooperate?",
    "url": "http://arxiv.org/abs/2504.12914v1",
    "arxiv_id": "2504.12914v1",
    "authors": [
      "Ben Bucknall",
      "Saad Siddiqui",
      "Lara Thurnherr",
      "Conor McGurk",
      "Ben Harack",
      "Anka Reuel",
      "Patricia Paskov",
      "Casey Mahoney",
      "S\u00f6ren Mindermann",
      "Scott Singer",
      "Vinay Hiremath",
      "Charbel-Rapha\u00ebl Segerie",
      "Oscar Delaney",
      "Alessandro Abate",
      "Fazl Barez",
      "Michael K. Cohen",
      "Philip Torr",
      "Ferenc Husz\u00e1r",
      "Anisoara Calinescu",
      "Gabriel Davis Jones",
      "Yoshua Bengio",
      "Robert Trager"
    ],
    "published": "2025-04-17T13:03:56+00:00",
    "summary": "International cooperation is common in AI research, including between geopolitical rivals. While many experts advocate for greater international cooperation on AI safety to address shared global risks, some view cooperation on AI with suspicion, arguing that it can pose unacceptable risks to national security. However, the extent to which cooperation on AI safety poses such risks, as well as provides benefits, depends on the specific area of cooperation. In this paper, we consider technical factors that impact the risks of international cooperation on AI safety research, focusing on the degree to which such cooperation can advance dangerous capabilities, result in the sharing of sensitive information, or provide opportunities for harm. We begin by why nations historically cooperate on strategic technologies and analyse current US-China cooperation in AI as a case study. We further argue that existing frameworks for managing associated risks can be supplemented with consideration of key risks specific to cooperation on technical AI safety research. Through our analysis, we find that research into AI verification mechanisms and shared protocols may be suitable areas for such cooperation. Through this analysis we aim to help researchers and governments identify and mitigate the risks of international cooperation on AI safety research, so that the benefits of cooperation can be fully realised."
  },
  {
    "title": "UncAD: Towards Safe End-to-end Autonomous Driving via Online Map Uncertainty",
    "url": "http://arxiv.org/abs/2504.12826v1",
    "arxiv_id": "2504.12826v1",
    "authors": [
      "Pengxuan Yang",
      "Yupeng Zheng",
      "Qichao Zhang",
      "Kefei Zhu",
      "Zebin Xing",
      "Qiao Lin",
      "Yun-Fu Liu",
      "Zhiguo Su",
      "Dongbin Zhao"
    ],
    "published": "2025-04-17T10:40:36+00:00",
    "summary": "End-to-end autonomous driving aims to produce planning trajectories from raw sensors directly. Currently, most approaches integrate perception, prediction, and planning modules into a fully differentiable network, promising great scalability. However, these methods typically rely on deterministic modeling of online maps in the perception module for guiding or constraining vehicle planning, which may incorporate erroneous perception information and further compromise planning safety. To address this issue, we delve into the importance of online map uncertainty for enhancing autonomous driving safety and propose a novel paradigm named UncAD. Specifically, UncAD first estimates the uncertainty of the online map in the perception module. It then leverages the uncertainty to guide motion prediction and planning modules to produce multi-modal trajectories. Finally, to achieve safer autonomous driving, UncAD proposes an uncertainty-collision-aware planning selection strategy according to the online map uncertainty to evaluate and select the best trajectory. In this study, we incorporate UncAD into various state-of-the-art (SOTA) end-to-end methods. Experiments on the nuScenes dataset show that integrating UncAD, with only a 1.9% increase in parameters, can reduce collision rates by up to 26% and drivable area conflict rate by up to 42%. Codes, pre-trained models, and demo videos can be accessed at https://github.com/pengxuanyang/UncAD."
  },
  {
    "title": "Supporting Urban Low-Altitude Economy: Channel Gain Map Inference Based on 3D Conditional GAN",
    "url": "http://arxiv.org/abs/2504.12794v1",
    "arxiv_id": "2504.12794v1",
    "authors": [
      "Yonghao Wang",
      "Ruoguang Li",
      "Di Wu",
      "Jiaqi Chen",
      "Yong Zeng"
    ],
    "published": "2025-04-17T09:55:03+00:00",
    "summary": "The advancement of advanced air mobility (AAM) in recent years has given rise to the concept of low-altitude economy (LAE). However, the diverse flight activities associated with the emerging LAE applications in urban scenarios confront complex physical environments, which urgently necessitates ubiquitous and reliable communication to guarantee the operation safety of the low-altitude aircraft. As one of promising technologies for the sixth generation (6G) mobile networks, channel knowledge map (CKM) enables the environment-aware communication by constructing a site-specific dataset, thereby providing a priori on-site information for the aircraft to obtain the channel state information (CSI) at arbitrary locations with much reduced online overhead. Diverse base station (BS) deployments in the three-dimensional (3D) urban low-altitude environment require efficient 3D CKM construction to capture spatial channel characteristics with less overhead. Towards this end, this paper proposes a 3D channel gain map (CGM) inference method based on a 3D conditional generative adversarial network (3D-CGAN). Specifically, we first analyze the potential deployment types of BSs in urban low-altitude scenario, and investigate the CGM representation with the corresponding 3D channel gain model. The framework of the proposed 3D-CGAN is then discussed, which is trained by a dataset consisting of existing CGMs. Consequently, the trained 3D-CGAN is capable of inferring the corresponding CGM only based on the BS coordinate without additional measurement. The simulation results demonstrate that the CGMs inferred by the proposed 3D-CGAN outperform those of the benchmark schemes, which can accurately reflect the radio propagation condition in 3D environment."
  },
  {
    "title": "On Error Classification from Physiological Signals within Airborne Environment",
    "url": "http://arxiv.org/abs/2504.12769v1",
    "arxiv_id": "2504.12769v1",
    "authors": [
      "Niall McGuire",
      "Yashar Moshfeghi"
    ],
    "published": "2025-04-17T09:08:21+00:00",
    "summary": "Human error remains a critical concern in aviation safety, contributing to 70-80% of accidents despite technological advancements. While physiological measures show promise for error detection in laboratory settings, their effectiveness in dynamic flight environments remains underexplored. Through live flight trials with nine commercial pilots, we investigated whether established error-detection approaches maintain accuracy during actual flight operations. Participants completed standardized multi-tasking scenarios across conditions ranging from laboratory settings to straight-and-level flight and 2G manoeuvres while we collected synchronized physiological data. Our findings demonstrate that EEG-based classification maintains high accuracy (87.83%) during complex flight manoeuvres, comparable to laboratory performance (89.23%). Eye-tracking showed moderate performance (82.50\\%), while ECG performed near chance level (51.50%). Classification accuracy remained stable across flight conditions, with minimal degradation during 2G manoeuvres. These results provide the first evidence that physiological error detection can translate effectively to operational aviation environments."
  },
  {
    "title": "Falcon: Advancing Asynchronous BFT Consensus for Lower Latency and Enhanced Throughput",
    "url": "http://arxiv.org/abs/2504.12766v1",
    "arxiv_id": "2504.12766v1",
    "authors": [
      "Xiaohai Dai",
      "Chaozheng Ding",
      "Wei Li",
      "Jiang Xiao",
      "Bolin Zhang",
      "Chen Yu",
      "Albert Y. Zomaya",
      "Hai Jin"
    ],
    "published": "2025-04-17T09:03:55+00:00",
    "summary": "Asynchronous Byzantine Fault Tolerant (BFT) consensus protocols have garnered significant attention with the rise of blockchain technology. A typical asynchronous protocol is designed by executing sequential instances of the Asynchronous Common Sub-seQuence (ACSQ). The ACSQ protocol consists of two primary components: the Asynchronous Common Subset (ACS) protocol and a block sorting mechanism, with the ACS protocol comprising two stages: broadcast and agreement. However, current protocols encounter three critical issues: high latency arising from the execution of the agreement stage, latency instability due to the integral-sorting mechanism, and reduced throughput caused by block discarding. To address these issues,we propose Falcon, an asynchronous BFT protocol that achieves low latency and enhanced throughput. Falcon introduces a novel broadcast protocol, Graded Broadcast (GBC), which enables a block to be included in the ACS set directly, bypassing the agreement stage and thereby reducing latency. To ensure safety, Falcon incorporates a new binary agreement protocol called Asymmetrical Asynchronous Binary Agreement (AABA), designed to complement GBC. Additionally, Falcon employs a partial-sorting mechanism, allowing continuous rather than simultaneous block committing, enhancing latency stability. Finally, we incorporate an agreement trigger that, before its activation, enables nodes to wait for more blocks to be delivered and committed, thereby boosting throughput. We conduct a series of experiments to evaluate Falcon, demonstrating its superior performance."
  },
  {
    "title": "Distributed Intelligent Sensing and Communications for 6G: Architecture and Use Cases",
    "url": "http://arxiv.org/abs/2504.12765v1",
    "arxiv_id": "2504.12765v1",
    "authors": [
      "Kyriakos Stylianopoulos",
      "Giyyarpuram Madhusudan",
      "Guillaume Jornod",
      "Sami Mekki",
      "Francesca Costanzo",
      "Hui Chen",
      "Placido Mursia",
      "Maurizio Crozzoli",
      "Emilio Calvanese Strinati",
      "George C. Alexandropoulos",
      "Henk Wymeersch"
    ],
    "published": "2025-04-17T09:02:36+00:00",
    "summary": "The Distributed Intelligent Sensing and Communication (DISAC) framework redefines Integrated Sensing and Communication (ISAC) for 6G by leveraging distributed architectures to enhance scalability, adaptability, and resource efficiency. This paper presents key architectural enablers, including advanced data representation, seamless target handover, support for heterogeneous devices, and semantic integration. Two use cases illustrate the transformative potential of DISAC: smart factory shop floors and Vulnerable Road User (VRU) protection at smart intersections. These scenarios demonstrate significant improvements in precision, safety, and operational efficiency compared to traditional ISAC systems. The preliminary DISAC architecture incorporates intelligent data processing, distributed coordination, and emerging technologies such as Reconfigurable Intelligent Surfaces (RIS) to meet 6G's stringent requirements. By addressing critical challenges in sensing accuracy, latency, and real-time decision-making, DISAC positions itself as a cornerstone for next-generation wireless networks, advancing innovation in dynamic and complex environments."
  },
  {
    "title": "Incorporating a Deep Neural Network into Moving Horizon Estimation for Embedded Thermal Torque Derating of an Electric Machine",
    "url": "http://arxiv.org/abs/2504.12736v1",
    "arxiv_id": "2504.12736v1",
    "authors": [
      "Alexander Winkler",
      "Pranav Shah",
      "Katrin Baumg\u00e4rtner",
      "Vasu Sharma",
      "David Gordon",
      "Jakob Andert"
    ],
    "published": "2025-04-17T08:24:32+00:00",
    "summary": "This study introduces a novel state estimation framework that incorporates Deep Neural Networks (DNNs) into Moving Horizon Estimation (MHE), shifting from traditional physics-based models to rapidly developed data-driven techniques. A DNN model with Long Short-Term Memory (LSTM) nodes is trained on synthetic data generated by a high-fidelity thermal model of a Permanent Magnet Synchronous Machine (PMSM), which undergoes thermal derating as part of the torque control strategy in a battery electric vehicle. The MHE is constructed by integrating the trained DNN with a simplified driving dynamics model in a discrete-time formulation, incorporating the LSTM hidden and cell states in the state vector to retain system dynamics. The resulting optimal control problem (OCP) is formulated as a nonlinear program (NLP) and implemented using the acados framework. Model-in-the-loop (MiL) simulations demonstrate accurate temperature estimation, even under noisy sensor conditions or failures. Achieving threefold real-time capability on embedded hardware confirms the feasibility of the approach for practical deployment. The primary focus of this study is to assess the feasibility of the MHE framework using a DNN-based plant model instead of focusing on quantitative comparisons of vehicle performance. Overall, this research highlights the potential of DNN-based MHE for real-time, safety-critical applications by combining the strengths of model-based and data-driven methods."
  },
  {
    "title": "Collaborative Perception Datasets for Autonomous Driving: A Review",
    "url": "http://arxiv.org/abs/2504.12696v1",
    "arxiv_id": "2504.12696v1",
    "authors": [
      "Naibang Wang",
      "Deyong Shang",
      "Yan Gong",
      "Xiaoxi Hu",
      "Ziying Song",
      "Lei Yang",
      "Yuhan Huang",
      "Xiaoyu Wang",
      "Jianli Lu"
    ],
    "published": "2025-04-17T06:49:21+00:00",
    "summary": "Collaborative perception has attracted growing interest from academia and industry due to its potential to enhance perception accuracy, safety, and robustness in autonomous driving through multi-agent information fusion. With the advancement of Vehicle-to-Everything (V2X) communication, numerous collaborative perception datasets have emerged, varying in cooperation paradigms, sensor configurations, data sources, and application scenarios. However, the absence of systematic summarization and comparative analysis hinders effective resource utilization and standardization of model evaluation. As the first comprehensive review focused on collaborative perception datasets, this work reviews and compares existing resources from a multi-dimensional perspective. We categorize datasets based on cooperation paradigms, examine their data sources and scenarios, and analyze sensor modalities and supported tasks. A detailed comparative analysis is conducted across multiple dimensions. We also outline key challenges and future directions, including dataset scalability, diversity, domain adaptation, standardization, privacy, and the integration of large language models. To support ongoing research, we provide a continuously updated online repository of collaborative perception datasets and related literature: https://github.com/frankwnb/Collaborative-Perception-Datasets-for-Autonomous-Driving."
  },
  {
    "title": "Embodied-R: Collaborative Framework for Activating Embodied Spatial Reasoning in Foundation Models via Reinforcement Learning",
    "url": "http://arxiv.org/abs/2504.12680v1",
    "arxiv_id": "2504.12680v1",
    "authors": [
      "Baining Zhao",
      "Ziyou Wang",
      "Jianjie Fang",
      "Chen Gao",
      "Fanhang Man",
      "Jinqiang Cui",
      "Xin Wang",
      "Xinlei Chen",
      "Yong Li",
      "Wenwu Zhu"
    ],
    "published": "2025-04-17T06:16:11+00:00",
    "summary": "Humans can perceive and reason about spatial relationships from sequential visual observations, such as egocentric video streams. However, how pretrained models acquire such abilities, especially high-level reasoning, remains unclear. This paper introduces Embodied-R, a collaborative framework combining large-scale Vision-Language Models (VLMs) for perception and small-scale Language Models (LMs) for reasoning. Using Reinforcement Learning (RL) with a novel reward system considering think-answer logical consistency, the model achieves slow-thinking capabilities with limited computational resources. After training on only 5k embodied video samples, Embodied-R with a 3B LM matches state-of-the-art multimodal reasoning models (OpenAI-o1, Gemini-2.5-pro) on both in-distribution and out-of-distribution embodied spatial reasoning tasks. Embodied-R also exhibits emergent thinking patterns such as systematic analysis and contextual integration. We further explore research questions including response length, training on VLM, strategies for reward design, and differences in model generalization after SFT (Supervised Fine-Tuning) and RL training."
  },
  {
    "title": "Predicting Driver's Perceived Risk: a Model Based on Semi-Supervised Learning Strategy",
    "url": "http://arxiv.org/abs/2504.12665v1",
    "arxiv_id": "2504.12665v1",
    "authors": [
      "Siwei Huang",
      "Chenhao Yang",
      "Chuan Hu"
    ],
    "published": "2025-04-17T05:50:33+00:00",
    "summary": "Drivers' perception of risk determines their acceptance, trust, and use of the Automated Driving Systems (ADSs). However, perceived risk is subjective and difficult to evaluate using existing methods. To address this issue, a driver's subjective perceived risk (DSPR) model is proposed, regarding perceived risk as a dynamically triggered mechanism with anisotropy and attenuation. 20 participants are recruited for a driver-in-the-loop experiment to report their real-time subjective risk ratings (SRRs) when experiencing various automatic driving scenarios. A convolutional neural network and bidirectional long short-term memory network with temporal pattern attention (CNN-Bi-LSTM-TPA) is embedded into a semi-supervised learning strategy to predict SRRs, aiming to reduce data noise caused by subjective randomness of participants. The results illustrate that DSPR achieves the highest prediction accuracy of 87.91% in predicting SRRs, compared to three state-of-the-art risk models. The semi-supervised strategy improves accuracy by 20.12%. Besides, CNN-Bi-LSTM-TPA network presents the highest accuracy among four different LSTM structures. This study offers an effective method for assessing driver's perceived risk, providing support for the safety enhancement of ADS and driver's trust improvement."
  },
  {
    "title": "VLMGuard-R1: Proactive Safety Alignment for VLMs via Reasoning-Driven Prompt Optimization",
    "url": "http://arxiv.org/abs/2504.12661v1",
    "arxiv_id": "2504.12661v1",
    "authors": [
      "Menglan Chen",
      "Xianghe Pang",
      "Jingjing Dong",
      "WenHao Wang",
      "Yaxin Du",
      "Siheng Chen"
    ],
    "published": "2025-04-17T05:46:41+00:00",
    "summary": "Aligning Vision-Language Models (VLMs) with safety standards is essential to mitigate risks arising from their multimodal complexity, where integrating vision and language unveils subtle threats beyond the reach of conventional safeguards. Inspired by the insight that reasoning across modalities is key to preempting intricate vulnerabilities, we propose a novel direction for VLM safety: multimodal reasoning-driven prompt rewriting. To this end, we introduce VLMGuard-R1, a proactive framework that refines user inputs through a reasoning-guided rewriter, dynamically interpreting text-image interactions to deliver refined prompts that bolster safety across diverse VLM architectures without altering their core parameters. To achieve this, we devise a three-stage reasoning pipeline to synthesize a dataset that trains the rewriter to infer subtle threats, enabling tailored, actionable responses over generic refusals. Extensive experiments across three benchmarks with five VLMs reveal that VLMGuard-R1 outperforms four baselines. In particular, VLMGuard-R1 achieves a remarkable 43.59\\% increase in average safety across five models on the SIUO benchmark."
  },
  {
    "title": "Photon Calibration Performance of KAGRA during the 4th Joint Observing Run (O4)",
    "url": "http://arxiv.org/abs/2504.12657v1",
    "arxiv_id": "2504.12657v1",
    "authors": [
      "Dan Chen",
      "Shingo Hido",
      "Darkhan Tuyenbayev",
      "Dripta Bhattacharjee",
      "Nobuyuki Kanda",
      "Richard Savage",
      "Rishabh Bajpai",
      "Sadakazu Haino",
      "Takahiro Sawada",
      "Takahiro Yamamoto",
      "Takayuki Tomaru",
      "Yoshiki Moriwaki"
    ],
    "published": "2025-04-17T05:36:31+00:00",
    "summary": "KAGRA is a kilometer-scale cryogenic gravitational-wave (GW) detector in Japan. It joined the 4th joint observing run (O4) in May 2023 in collaboration with the Laser Interferometer GW Observatory (LIGO) in the USA, and Virgo in Italy. After one month of observations, KAGRA entered a break period to enhance its sensitivity to GWs, and it is planned to rejoin O4 before its scheduled end in October 2025. To accurately recover the information encoded in the GW signals, it is essential to properly calibrate the observed signals. We employ a photon calibration (Pcal) system as a reference signal injector to calibrate the output signals obtained from the telescope. In ideal future conditions, the uncertainty in Pcal could dominate the uncertainty in the observed data. In this paper, we present the methods used to estimate the uncertainty in the Pcal systems employed during KAGRA O4 and report an estimated system uncertainty of 0.79%, which is three times lower than the uncertainty achieved in the previous 3rd joint observing run (O3) in 2020. Additionally, we investigate the uncertainty in the Pcal laser power sensors, which had the highest impact on the Pcal uncertainty, and estimate the beam positions on the KAGRA main mirror, which had the second highest impact. The Pcal systems in KAGRA are the first fully functional calibration systems for a cryogenic GW telescope. To avoid interference with the KAGRA cryogenic systems, the Pcal systems incorporate unique features regarding their placement and the use of telephoto cameras, which can capture images of the mirror surface at almost normal incidence. As future GW telescopes, such as the Einstein Telescope, are expected to adopt cryogenic techniques, the performance of the KAGRA Pcal systems can serve as a valuable reference."
  },
  {
    "title": "Graph-based Path Planning with Dynamic Obstacle Avoidance for Autonomous Parking",
    "url": "http://arxiv.org/abs/2504.12616v1",
    "arxiv_id": "2504.12616v1",
    "authors": [
      "Farhad Nawaz",
      "Minjun Sung",
      "Darshan Gadginmath",
      "Jovin D'sa",
      "Sangjae Bae",
      "David Isele",
      "Nadia Figueroa",
      "Nikolai Matni",
      "Faizan M. Tariq"
    ],
    "published": "2025-04-17T03:43:20+00:00",
    "summary": "Safe and efficient path planning in parking scenarios presents a significant challenge due to the presence of cluttered environments filled with static and dynamic obstacles. To address this, we propose a novel and computationally efficient planning strategy that seamlessly integrates the predictions of dynamic obstacles into the planning process, ensuring the generation of collision-free paths. Our approach builds upon the conventional Hybrid A star algorithm by introducing a time-indexed variant that explicitly accounts for the predictions of dynamic obstacles during node exploration in the graph, thus enabling dynamic obstacle avoidance. We integrate the time-indexed Hybrid A star algorithm within an online planning framework to compute local paths at each planning step, guided by an adaptively chosen intermediate goal. The proposed method is validated in diverse parking scenarios, including perpendicular, angled, and parallel parking. Through simulations, we showcase our approach's potential in greatly improving the efficiency and safety when compared to the state of the art spline-based planning method for parking situations."
  },
  {
    "title": "ZeroSumEval: Scaling LLM Evaluation with Inter-Model Competition",
    "url": "http://arxiv.org/abs/2504.12562v1",
    "arxiv_id": "2504.12562v1",
    "authors": [
      "Haidar Khan",
      "Hisham A. Alyahya",
      "Yazeed Alnumay",
      "M Saiful Bari",
      "B\u00fclent Yener"
    ],
    "published": "2025-04-17T01:23:50+00:00",
    "summary": "Evaluating the capabilities of Large Language Models (LLMs) has traditionally relied on static benchmark datasets, human assessments, or model-based evaluations - methods that often suffer from overfitting, high costs, and biases. ZeroSumEval is a novel competition-based evaluation protocol that leverages zero-sum games to assess LLMs with dynamic benchmarks that resist saturation. ZeroSumEval encompasses a diverse suite of games, including security challenges (PyJail), classic games (Chess, Liar's Dice, Poker), knowledge tests (MathQuiz), and persuasion challenges (Gandalf, Debate). These games are designed to evaluate a range of AI capabilities such as strategic reasoning, planning, knowledge application, and creativity. Building upon recent studies that highlight the effectiveness of game-based evaluations for LLMs, ZeroSumEval enhances these approaches by providing a standardized and extensible framework. To demonstrate this, we conduct extensive experiments with >7000 simulations across 7 games and 13 models. Our results show that while frontier models from the GPT and Claude families can play common games and answer questions, they struggle to play games that require creating novel and challenging questions. We also observe that models cannot reliably jailbreak each other and fail generally at tasks requiring creativity. We release our code at https://github.com/facebookresearch/ZeroSumEval."
  },
  {
    "title": "TraCeS: Trajectory Based Credit Assignment From Sparse Safety Feedback",
    "url": "http://arxiv.org/abs/2504.12557v1",
    "arxiv_id": "2504.12557v1",
    "authors": [
      "Siow Meng Low",
      "Akshat Kumar"
    ],
    "published": "2025-04-17T01:11:08+00:00",
    "summary": "In safe reinforcement learning (RL), auxiliary safety costs are used to align the agent to safe decision making. In practice, safety constraints, including cost functions and budgets, are unknown or hard to specify, as it requires anticipation of all possible unsafe behaviors. We therefore address a general setting where the true safety definition is unknown, and has to be learned from sparsely labeled data. Our key contributions are: first, we design a safety model that performs credit assignment to estimate each decision step's impact on the overall safety using a dataset of diverse trajectories and their corresponding binary safety labels (i.e., whether the corresponding trajectory is safe/unsafe). Second, we illustrate the architecture of our safety model to demonstrate its ability to learn a separate safety score for each timestep. Third, we reformulate the safe RL problem using the proposed safety model and derive an effective algorithm to optimize a safe yet rewarding policy. Finally, our empirical results corroborate our findings and show that this approach is effective in satisfying unknown safety definition, and scalable to various continuous control tasks."
  },
  {
    "title": "ELAB: Extensive LLM Alignment Benchmark in Persian Language",
    "url": "http://arxiv.org/abs/2504.12553v1",
    "arxiv_id": "2504.12553v1",
    "authors": [
      "Zahra Pourbahman",
      "Fatemeh Rajabi",
      "Mohammadhossein Sadeghi",
      "Omid Ghahroodi",
      "Somaye Bakhshaei",
      "Arash Amini",
      "Reza Kazemi",
      "Mahdieh Soleymani Baghshah"
    ],
    "published": "2025-04-17T00:50:41+00:00",
    "summary": "This paper presents a comprehensive evaluation framework for aligning Persian Large Language Models (LLMs) with critical ethical dimensions, including safety, fairness, and social norms. It addresses the gaps in existing LLM evaluation frameworks by adapting them to Persian linguistic and cultural contexts. This benchmark creates three types of Persian-language benchmarks: (i) translated data, (ii) new data generated synthetically, and (iii) new naturally collected data. We translate Anthropic Red Teaming data, AdvBench, HarmBench, and DecodingTrust into Persian. Furthermore, we create ProhibiBench-fa, SafeBench-fa, FairBench-fa, and SocialBench-fa as new datasets to address harmful and prohibited content in indigenous culture. Moreover, we collect extensive dataset as GuardBench-fa to consider Persian cultural norms. By combining these datasets, our work establishes a unified framework for evaluating Persian LLMs, offering a new approach to culturally grounded alignment evaluation. A systematic evaluation of Persian LLMs is performed across the three alignment aspects: safety (avoiding harmful content), fairness (mitigating biases), and social norms (adhering to culturally accepted behaviors). We present a publicly available leaderboard that benchmarks Persian LLMs with respect to safety, fairness, and social norms at: https://huggingface.co/spaces/MCILAB/LLM_Alignment_Evaluation."
  },
  {
    "title": "Knowledge Acquisition on Mass-shooting Events via LLMs for AI-Driven Justice",
    "url": "http://arxiv.org/abs/2504.12545v1",
    "arxiv_id": "2504.12545v1",
    "authors": [
      "Benign John Ihugba",
      "Afsana Nasrin",
      "Ling Wu",
      "Lin Li",
      "Lijun Qian",
      "Xishuang Dong"
    ],
    "published": "2025-04-17T00:13:04+00:00",
    "summary": "Mass-shooting events pose a significant challenge to public safety, generating large volumes of unstructured textual data that hinder effective investigations and the formulation of public policy. Despite the urgency, few prior studies have effectively automated the extraction of key information from these events to support legal and investigative efforts. This paper presented the first dataset designed for knowledge acquisition on mass-shooting events through the application of named entity recognition (NER) techniques. It focuses on identifying key entities such as offenders, victims, locations, and criminal instruments, that are vital for legal and investigative purposes. The NER process is powered by Large Language Models (LLMs) using few-shot prompting, facilitating the efficient extraction and organization of critical information from diverse sources, including news articles, police reports, and social media. Experimental results on real-world mass-shooting corpora demonstrate that GPT-4o is the most effective model for mass-shooting NER, achieving the highest Micro Precision, Micro Recall, and Micro F1-scores. Meanwhile, o1-mini delivers competitive performance, making it a resource-efficient alternative for less complex NER tasks. It is also observed that increasing the shot count enhances the performance of all models, but the gains are more substantial for GPT-4o and o1-mini, highlighting their superior adaptability to few-shot learning scenarios."
  },
  {
    "title": "KFinEval-Pilot: A Comprehensive Benchmark Suite for Korean Financial Language Understanding",
    "url": "http://arxiv.org/abs/2504.13216v1",
    "arxiv_id": "2504.13216v1",
    "authors": [
      "Bokwang Hwang",
      "Seonkyu Lim",
      "Taewoong Kim",
      "Yongjae Geun",
      "Sunghyun Bang",
      "Sohyun Park",
      "Jihyun Park",
      "Myeonggyu Lee",
      "Jinwoo Lee",
      "Yerin Kim",
      "Jinsun Yoo",
      "Jingyeong Hong",
      "Jina Park",
      "Yongchan Kim",
      "Suhyun Kim",
      "Younggyun Hahm",
      "Yiseul Lee",
      "Yejee Kang",
      "Chanhyuk Yoon",
      "Chansu Lee",
      "Heeyewon Jeong",
      "Jiyeon Lee",
      "Seonhye Gu",
      "Hyebin Kang",
      "Yousang Cho",
      "Hangyeol Yoo",
      "KyungTae Lim"
    ],
    "published": "2025-04-17T00:12:58+00:00",
    "summary": "We introduce KFinEval-Pilot, a benchmark suite specifically designed to evaluate large language models (LLMs) in the Korean financial domain. Addressing the limitations of existing English-centric benchmarks, KFinEval-Pilot comprises over 1,000 curated questions across three critical areas: financial knowledge, legal reasoning, and financial toxicity. The benchmark is constructed through a semi-automated pipeline that combines GPT-4-generated prompts with expert validation to ensure domain relevance and factual accuracy. We evaluate a range of representative LLMs and observe notable performance differences across models, with trade-offs between task accuracy and output safety across different model families. These results highlight persistent challenges in applying LLMs to high-stakes financial applications, particularly in reasoning and safety. Grounded in real-world financial use cases and aligned with the Korean regulatory and linguistic context, KFinEval-Pilot serves as an early diagnostic tool for developing safer and more reliable financial AI systems."
  },
  {
    "title": "What do people expect from Artificial Intelligence? Public opinion on alignment in AI moderation from Germany and the United States",
    "url": "http://arxiv.org/abs/2504.12476v1",
    "arxiv_id": "2504.12476v1",
    "authors": [
      "Andreas Jungherr",
      "Adrian Rauchfleisch"
    ],
    "published": "2025-04-16T20:27:03+00:00",
    "summary": "Recent advances in generative Artificial Intelligence have raised public awareness, shaping expectations and concerns about their societal implications. Central to these debates is the question of AI alignment -- how well AI systems meet public expectations regarding safety, fairness, and social values. However, little is known about what people expect from AI-enabled systems and how these expectations differ across national contexts. We present evidence from two surveys of public preferences for key functional features of AI-enabled systems in Germany (n = 1800) and the United States (n = 1756). We examine support for four types of alignment in AI moderation: accuracy and reliability, safety, bias mitigation, and the promotion of aspirational imaginaries. U.S. respondents report significantly higher AI use and consistently greater support for all alignment features, reflecting broader technological openness and higher societal involvement with AI. In both countries, accuracy and safety enjoy the strongest support, while more normatively charged goals -- like fairness and aspirational imaginaries -- receive more cautious backing, particularly in Germany. We also explore how individual experience with AI, attitudes toward free speech, political ideology, partisan affiliation, and gender shape these preferences. AI use and free speech support explain more variation in Germany. In contrast, U.S. responses show greater attitudinal uniformity, suggesting that higher exposure to AI may consolidate public expectations. These findings contribute to debates on AI governance and cross-national variation in public preferences. More broadly, our study demonstrates the value of empirically grounding AI alignment debates in public attitudes and of explicitly developing normatively grounded expectations into theoretical and policy discussions on the governance of AI-generated content."
  },
  {
    "title": "Learning-based Delay Compensation for Enhanced Control of Assistive Soft Robots",
    "url": "http://arxiv.org/abs/2504.12428v1",
    "arxiv_id": "2504.12428v1",
    "authors": [
      "Adri\u00e0 Momp\u00f3 Alepuz",
      "Dimitrios Papageorgiou",
      "Silvia Tolu"
    ],
    "published": "2025-04-16T18:57:23+00:00",
    "summary": "Soft robots are increasingly used in healthcare, especially for assistive care, due to their inherent safety and adaptability. Controlling soft robots is challenging due to their nonlinear dynamics and the presence of time delays, especially in applications like a soft robotic arm for patient care. This paper presents a learning-based approach to approximate the nonlinear state predictor (Smith Predictor), aiming to improve tracking performance in a two-module soft robot arm with a short inherent input delay. The method uses Kernel Recursive Least Squares Tracker (KRLST) for online learning of the system dynamics and a Legendre Delay Network (LDN) to compress past input history for efficient delay compensation. Experimental results demonstrate significant improvement in tracking performance compared to a baseline model-based non-linear controller. Statistical analysis confirms the significance of the improvements. The method is computationally efficient and adaptable online, making it suitable for real-world scenarios and highlighting its potential for enabling safer and more accurate control of soft robots in assistive care applications."
  },
  {
    "title": "Accountable Liveness",
    "url": "http://arxiv.org/abs/2504.12218v1",
    "arxiv_id": "2504.12218v1",
    "authors": [
      "Andrew Lewis-Pye",
      "Joachim Neu",
      "Tim Roughgarden",
      "Luca Zanolini"
    ],
    "published": "2025-04-16T16:13:09+00:00",
    "summary": "Safety and liveness are the two classical security properties of consensus protocols. Recent works have strengthened safety with accountability: should any safety violation occur, a sizable fraction of adversary nodes can be proven to be protocol violators. This paper studies to what extent analogous accountability guarantees are achievable for liveness. To reveal the full complexity of this question, we introduce an interpolation between the classical synchronous and partially-synchronous models that we call the $x$-partially-synchronous network model in which, intuitively, at most an $x$ fraction of the time steps in any sufficiently long interval are asynchronous (and, as with a partially-synchronous network, all time steps are synchronous following the passage of an unknown \"global stablization time\"). We prove a precise characterization of the parameter regime in which accountable liveness is achievable: if and only if $x < 1/2$ and $f < n/2$, where $n$ denotes the number of nodes and $f$ the number of nodes controlled by an adversary. We further refine the problem statement and our analysis by parameterizing by the number of violating nodes identified following a liveness violation, and provide evidence that the guarantees achieved by our protocol are near-optimal (as a function of $x$ and $f$). Our results provide rigorous foundations for liveness-accountability heuristics such as the \"inactivity leaks\" employed in Ethereum."
  },
  {
    "title": "GripMap: An Efficient, Spatially Resolved Constraint Framework for Offline and Online Trajectory Planning in Autonomous Racing",
    "url": "http://arxiv.org/abs/2504.12115v1",
    "arxiv_id": "2504.12115v1",
    "authors": [
      "Frederik Werner",
      "Ann-Kathrin Schwehn",
      "Markus Lienkamp",
      "Johannes Betz"
    ],
    "published": "2025-04-16T14:25:29+00:00",
    "summary": "Conventional trajectory planning approaches for autonomous vehicles often assume a fixed vehicle model that remains constant regardless of the vehicle's location. This overlooks the critical fact that the tires and the surface are the two force-transmitting partners in vehicle dynamics; while the tires stay with the vehicle, surface conditions vary with location. Recognizing these challenges, this paper presents a novel framework for spatially resolving dynamic constraints in both offline and online planning algorithms applied to autonomous racing. We introduce the GripMap concept, which provides a spatial resolution of vehicle dynamic constraints in the Frenet frame, allowing adaptation to locally varying grip conditions. This enables compensation for location-specific effects, more efficient vehicle behavior, and increased safety, unattainable with spatially invariant vehicle models. The focus is on low storage demand and quick access through perfect hashing. This framework proved advantageous in real-world applications in the presented form. Experiments inspired by autonomous racing demonstrate its effectiveness. In future work, this framework can serve as a foundational layer for developing future interpretable learning algorithms that adjust to varying grip conditions in real-time."
  },
  {
    "title": "Towards LLM Agents for Earth Observation",
    "url": "http://arxiv.org/abs/2504.12110v1",
    "arxiv_id": "2504.12110v1",
    "authors": [
      "Chia Hsiang Kao",
      "Wenting Zhao",
      "Shreelekha Revankar",
      "Samuel Speas",
      "Snehal Bhagat",
      "Rajeev Datta",
      "Cheng Perng Phoo",
      "Utkarsh Mall",
      "Carl Vondrick",
      "Kavita Bala",
      "Bharath Hariharan"
    ],
    "published": "2025-04-16T14:19:25+00:00",
    "summary": "Earth Observation (EO) provides critical planetary data for environmental monitoring, disaster management, climate science, and other scientific domains. Here we ask: Are AI systems ready for reliable Earth Observation? We introduce \\datasetnamenospace, a benchmark of 140 yes/no questions from NASA Earth Observatory articles across 13 topics and 17 satellite sensors. Using Google Earth Engine API as a tool, LLM agents can only achieve an accuracy of 33% because the code fails to run over 58% of the time. We improve the failure rate for open models by fine-tuning synthetic data, allowing much smaller models (Llama-3.1-8B) to achieve comparable accuracy to much larger ones (e.g., DeepSeek-R1). Taken together, our findings identify significant challenges to be solved before AI agents can automate earth observation, and suggest paths forward. The project page is available at https://iandrover.github.io/UnivEarth."
  },
  {
    "title": "Reasoning-Based AI for Startup Evaluation (R.A.I.S.E.): A Memory-Augmented, Multi-Step Decision Framework",
    "url": "http://arxiv.org/abs/2504.12090v1",
    "arxiv_id": "2504.12090v1",
    "authors": [
      "Jack Preuveneers",
      "Joseph Ternasky",
      "Fuat Alican",
      "Yigit Ihlamur"
    ],
    "published": "2025-04-16T13:53:42+00:00",
    "summary": "We present a novel framework that bridges the gap between the interpretability of decision trees and the advanced reasoning capabilities of large language models (LLMs) to predict startup success. Our approach leverages chain-of-thought prompting to generate detailed reasoning logs, which are subsequently distilled into structured, human-understandable logical rules. The pipeline integrates multiple enhancements - efficient data ingestion, a two-step refinement process, ensemble candidate sampling, simulated reinforcement learning scoring, and persistent memory - to ensure both stable decision-making and transparent output. Experimental evaluations on curated startup datasets demonstrate that our combined pipeline improves precision by 54% from 0.225 to 0.346 and accuracy by 50% from 0.46 to 0.70 compared to a standalone OpenAI o3 model. Notably, our model achieves over 2x the precision of a random classifier (16%). By combining state-of-the-art AI reasoning with explicit rule-based explanations, our method not only augments traditional decision-making processes but also facilitates expert intervention and continuous policy refinement. This work lays the foundation for the implementation of interpretable LLM-powered decision frameworks in high-stakes investment environments and other domains that require transparent and data-driven insights."
  },
  {
    "title": "Selective Demonstration Retrieval for Improved Implicit Hate Speech Detection",
    "url": "http://arxiv.org/abs/2504.12082v1",
    "arxiv_id": "2504.12082v1",
    "authors": [
      "Yumin Kim",
      "Hwanhee Lee"
    ],
    "published": "2025-04-16T13:43:23+00:00",
    "summary": "Hate speech detection is a crucial area of research in natural language processing, essential for ensuring online community safety. However, detecting implicit hate speech, where harmful intent is conveyed in subtle or indirect ways, remains a major challenge. Unlike explicit hate speech, implicit expressions often depend on context, cultural subtleties, and hidden biases, making them more challenging to identify consistently. Additionally, the interpretation of such speech is influenced by external knowledge and demographic biases, resulting in varied detection results across different language models. Furthermore, Large Language Models often show heightened sensitivity to toxic language and references to vulnerable groups, which can lead to misclassifications. This over-sensitivity results in false positives (incorrectly identifying harmless statements as hateful) and false negatives (failing to detect genuinely harmful content). Addressing these issues requires methods that not only improve detection precision but also reduce model biases and enhance robustness. To address these challenges, we propose a novel method, which utilizes in-context learning without requiring model fine-tuning. By adaptively retrieving demonstrations that focus on similar groups or those with the highest similarity scores, our approach enhances contextual comprehension. Experimental results show that our method outperforms current state-of-the-art techniques. Implementation details and code are available at TBD."
  },
  {
    "title": "Observational properties of regular black holes in Asymptotic Safety",
    "url": "http://arxiv.org/abs/2504.12072v1",
    "arxiv_id": "2504.12072v1",
    "authors": [
      "Abdybek Urmanov",
      "Hrishikesh Chakrabarty",
      "Daniele Malafarina"
    ],
    "published": "2025-04-16T13:28:51+00:00",
    "summary": "We consider the observational properties of a spherically symmetric, static regular black hole within the framework of asymptotic safety (AS) as proposed by Bonanno et al. The metric resembles the Schwarzschild solution in the classical limit. The departure from Schwarzschild at small scales is controlled by a single free parameter related to the ultraviolet (UV) cutoff of the theory. We investigated null and time-like geodesics around the AS metric, including circular orbits, photon rings and lensing effects. In particular we focused on the optical properties of thin accretion disks in the equatorial plane of the object and compared them with those of accretion disks in the Schwarzschild metric. We found that the radiation flux, luminosity, and efficiency of the accretion disk increase with the value of the free parameter. Using a spacetime generic open-source relativistic ray-tracing code, we simulate the K$\\alpha$ iron line profiles emitted by the disk and analyze their deviation from that of the Schwarzschild geometry."
  },
  {
    "title": "Contract-based hierarchical control using predictive feasibility value functions",
    "url": "http://arxiv.org/abs/2504.12036v1",
    "arxiv_id": "2504.12036v1",
    "authors": [
      "Felix Berkel",
      "Kim Peter Wabersich",
      "Hongxi Xiang",
      "Elias Milios"
    ],
    "published": "2025-04-16T12:51:18+00:00",
    "summary": "Today's control systems are often characterized by modularity and safety requirements to handle complexity, resulting in hierarchical control structures. Although hierarchical model predictive control offers favorable properties, achieving a provably safe, yet modular design remains a challenge. This paper introduces a contract-based hierarchical control strategy to improve the performance of control systems facing challenges related to model inconsistency and independent controller design across hierarchies. We consider a setup where a higher-level controller generates references that affect the constraints of a lower-level controller, which is based on a soft-constrained MPC formulation. The optimal slack variables serve as the basis for a contract that allows the higher-level controller to assess the feasibility of the reference trajectory without exact knowledge of the model, constraints, and cost of the lower-level controller. To ensure computational efficiency while maintaining model confidentiality, we propose using an explicit function approximation, such as a neural network, to represent the cost of optimal slack values. The approach is tested for a hierarchical control setup consisting of a planner and a motion controller as commonly found in autonomous driving."
  },
  {
    "title": "Global Patterns of Extreme Temperature Teleconnections Using Climate Network Analysis",
    "url": "http://arxiv.org/abs/2504.12008v1",
    "arxiv_id": "2504.12008v1",
    "authors": [
      "Yuhao Feng",
      "Jun Meng",
      "Jingfang Fan"
    ],
    "published": "2025-04-16T12:05:15+00:00",
    "summary": "Extreme weather events, rare yet profoundly impactful, are often accompanied by severe conditions. Increasing global temperatures are poised to exacerbate these events, resulting in greater human casualties, economic losses, and ecological destruction. Complex global climate interactions, known as teleconnections, can lead to widespread repercussions triggered by localized extreme weather. Understanding these teleconnection patterns is crucial for weather forecasting, enhancing safety, and advancing climate science. Here, we employ climate network analysis to uncover teleconnection patterns associated with extreme temperature fluctuations, including both extreme warming and cooling events occurring on a daily basis. Our study results demonstrate that the distances of significant teleconnections initially conform to a power-law decay, signifying a decline in connectivity with distance. However, this power-law decay tendency breaks beyond a certain threshold distance, suggesting the existence of long-distance connections. Additionally, we uncover a greater prevalence of long-distance connectivity among extreme cooling events compared to extreme warming events. The global pattern of teleconnections is, in part, driven by the mechanism of Rossby waves, which serve as a rapid conduit for inducing correlated fluctuations in both pressure and temperature. These results enhance our understanding of the multiscale nature of climate teleconnections and hold significant implications for improving weather forecasting and assessing climate risks in a warming world."
  },
  {
    "title": "Comment on Path integral measure and RG equations for gravity",
    "url": "http://arxiv.org/abs/2504.12006v1",
    "arxiv_id": "2504.12006v1",
    "authors": [
      "Aaron Held",
      "Benjamin Knorr",
      "Jan M. Pawlowski",
      "Alessia Platania",
      "Manuel Reichert",
      "Frank Saueressig",
      "Marc Schiffer"
    ],
    "published": "2025-04-16T12:00:22+00:00",
    "summary": "Asymptotic safety is a candidate for a predictive quantum theory of gravity and matter. Recent works (arXiv:2412.10194 and arXiv:2412.14108) challenged this scenario. We show that their arguments fail on a basic level."
  },
  {
    "title": "Leveraging Machine Learning Models to Predict the Outcome of Digital Medical Triage Interviews",
    "url": "http://arxiv.org/abs/2504.11977v1",
    "arxiv_id": "2504.11977v1",
    "authors": [
      "Sofia Krylova",
      "Fabian Schmidt",
      "Vladimir Vlassov"
    ],
    "published": "2025-04-16T11:17:23+00:00",
    "summary": "Many existing digital triage systems are questionnaire-based, guiding patients to appropriate care levels based on information (e.g., symptoms, medical history, and urgency) provided by the patients answering questionnaires. Such a system often uses a deterministic model with predefined rules to determine care levels. It faces challenges with incomplete triage interviews since it can only assist patients who finish the process. In this study, we explore the use of machine learning (ML) to predict outcomes of unfinished interviews, aiming to enhance patient care and service quality. Predicting triage outcomes from incomplete data is crucial for patient safety and healthcare efficiency. Our findings show that decision-tree models, particularly LGBMClassifier and CatBoostClassifier, achieve over 80\\% accuracy in predicting outcomes from complete interviews while having a linear correlation between the prediction accuracy and interview completeness degree. For example, LGBMClassifier achieves 88,2\\% prediction accuracy for interviews with 100\\% completeness, 79,6\\% accuracy for interviews with 80\\% completeness, 58,9\\% accuracy for 60\\% completeness, and 45,7\\% accuracy for 40\\% completeness. The TabTransformer model demonstrated exceptional accuracy of over 80\\% for all degrees of completeness but required extensive training time, indicating a need for more powerful computational resources. The study highlights the linear correlation between interview completeness and predictive power of the decision-tree models."
  },
  {
    "title": "SemDiff: Generating Natural Unrestricted Adversarial Examples via Semantic Attributes Optimization in Diffusion Models",
    "url": "http://arxiv.org/abs/2504.11923v1",
    "arxiv_id": "2504.11923v1",
    "authors": [
      "Zeyu Dai",
      "Shengcai Liu",
      "Rui He",
      "Jiahao Wu",
      "Ning Lu",
      "Wenqi Fan",
      "Qing Li",
      "Ke Tang"
    ],
    "published": "2025-04-16T09:58:04+00:00",
    "summary": "Unrestricted adversarial examples (UAEs), allow the attacker to create non-constrained adversarial examples without given clean samples, posing a severe threat to the safety of deep learning models. Recent works utilize diffusion models to generate UAEs. However, these UAEs often lack naturalness and imperceptibility due to simply optimizing in intermediate latent noises. In light of this, we propose SemDiff, a novel unrestricted adversarial attack that explores the semantic latent space of diffusion models for meaningful attributes, and devises a multi-attributes optimization approach to ensure attack success while maintaining the naturalness and imperceptibility of generated UAEs. We perform extensive experiments on four tasks on three high-resolution datasets, including CelebA-HQ, AFHQ and ImageNet. The results demonstrate that SemDiff outperforms state-of-the-art methods in terms of attack success rate and imperceptibility. The generated UAEs are natural and exhibit semantically meaningful changes, in accord with the attributes' weights. In addition, SemDiff is found capable of evading different defenses, which further validates its effectiveness and threatening."
  },
  {
    "title": "Rethinking the Generation of High-Quality CoT Data from the Perspective of LLM-Adaptive Question Difficulty Grading",
    "url": "http://arxiv.org/abs/2504.11919v1",
    "arxiv_id": "2504.11919v1",
    "authors": [
      "Qianjin Yu",
      "Keyu Wu",
      "Zihan Chen",
      "Chushu Zhang",
      "Manlin Mei",
      "Lingjun Huang",
      "Fang Tan",
      "Yongsheng Du",
      "Kunlin Liu",
      "Yurui Zhu"
    ],
    "published": "2025-04-16T09:55:34+00:00",
    "summary": "Recently, DeepSeek-R1 (671B) (DeepSeek-AIet al., 2025) has demonstrated its excellent reasoning ability in complex tasks and has publiclyshared its methodology. This provides potentially high-quality chain-of-thought (CoT) data for stimulating the reasoning abilities of small-sized large language models (LLMs). To generate high-quality CoT data for different LLMs, we seek an efficient method for generating high-quality CoT data with LLM-Adaptive questiondifficulty levels. First, we grade the difficulty of the questions according to the reasoning ability of the LLMs themselves and construct a LLM-Adaptive question database. Second, we sample the problem database based on a distribution of difficulty levels of the questions and then use DeepSeek-R1 (671B) (DeepSeek-AI et al., 2025) to generate the corresponding high-quality CoT data with correct answers. Thanks to the construction of CoT data with LLM-Adaptive difficulty levels, we have significantly reduced the cost of data generation and enhanced the efficiency of model supervised fine-tuning (SFT). Finally, we have validated the effectiveness and generalizability of the proposed method in the fields of complex mathematical competitions and code generation tasks. Notably, with only 2k high-quality mathematical CoT data, our ZMath-32B surpasses DeepSeek-Distill-32B in math reasoning task. Similarly, with only 2k high-quality code CoT data, our ZCode-32B surpasses DeepSeek-Distill-32B in code reasoning tasks."
  },
  {
    "title": "A Graph-Based Reinforcement Learning Approach with Frontier Potential Based Reward for Safe Cluttered Environment Exploration",
    "url": "http://arxiv.org/abs/2504.11907v1",
    "arxiv_id": "2504.11907v1",
    "authors": [
      "Gabriele Calzolari",
      "Vidya Sumathy",
      "Christoforos Kanellakis",
      "George Nikolakopoulos"
    ],
    "published": "2025-04-16T09:31:14+00:00",
    "summary": "Autonomous exploration of cluttered environments requires efficient exploration strategies that guarantee safety against potential collisions with unknown random obstacles. This paper presents a novel approach combining a graph neural network-based exploration greedy policy with a safety shield to ensure safe navigation goal selection. The network is trained using reinforcement learning and the proximal policy optimization algorithm to maximize exploration efficiency while reducing the safety shield interventions. However, if the policy selects an infeasible action, the safety shield intervenes to choose the best feasible alternative, ensuring system consistency. Moreover, this paper proposes a reward function that includes a potential field based on the agent's proximity to unexplored regions and the expected information gain from reaching them. Overall, the approach investigated in this paper merges the benefits of the adaptability of reinforcement learning-driven exploration policies and the guarantee ensured by explicit safety mechanisms. Extensive evaluations in simulated environments demonstrate that the approach enables efficient and safe exploration in cluttered environments."
  },
  {
    "title": "MDHP-Net: Detecting an Emerging Time-exciting Threat in IVN",
    "url": "http://arxiv.org/abs/2504.11867v1",
    "arxiv_id": "2504.11867v1",
    "authors": [
      "Qi Liu",
      "Yanchen Liu",
      "Ruifeng Li",
      "Chenhong Cao",
      "Yufeng Li",
      "Xingyu Li",
      "Peng Wang",
      "Runhan Feng",
      "Shiyang Bu"
    ],
    "published": "2025-04-16T08:41:24+00:00",
    "summary": "The integration of intelligent and connected technologies in modern vehicles, while offering enhanced functionalities through Electronic Control Unit (ECU) and interfaces like OBD-II and telematics, also exposes the vehicle's in-vehicle network (IVN) to potential cyberattacks. Unlike prior work, we identify a new time-exciting threat model against IVN. These attacks inject malicious messages that exhibit a time-exciting effect, gradually manipulating network traffic to disrupt vehicle operations and compromise safety-critical functions. We systematically analyze the characteristics of the threat: dynamism, time-exciting impact, and low prior knowledge dependency. To validate its practicality, we replicate the attack on a real Advanced Driver Assistance System via Controller Area Network (CAN), exploiting Unified Diagnostic Service vulnerabilities and proposing four attack strategies. While CAN's integrity checks mitigate attacks, Ethernet migration (e.g., DoIP/SOME/IP) introduces new surfaces. We further investigate the feasibility of time-exciting threat under SOME/IP. To detect time-exciting threat, we introduce MDHP-Net, leveraging Multi-Dimentional Hawkes Process (MDHP) and temporal and message-wise feature extracting structures. Meanwhile, to estimate MDHP parameters, we developed the first GPU-optimized gradient descent solver for MDHP (MDHP-GDS). These modules significantly improves the detection rate under time-exciting attacks in multi-ECU IVN system. To address data scarcity, we release STEIA9, the first open-source dataset for time-exciting attacks, covering 9 Ethernet-based attack scenarios. Extensive experiments on STEIA9 (9 attack scenarios) show MDHP-Net outperforms 3 baselines, confirming attack feasibility and detection efficacy."
  },
  {
    "title": "Visualization Analysis and Impedance Analysis for the Aging Behavior Assessment of 18650 Cells",
    "url": "http://arxiv.org/abs/2504.11861v1",
    "arxiv_id": "2504.11861v1",
    "authors": [
      "Yihan Shi",
      "Qingrui Pan",
      "Jitao Li",
      "Xiaoze Shi",
      "Youchang Wang",
      "Peng Xiao"
    ],
    "published": "2025-04-16T08:31:27+00:00",
    "summary": "This work presents a comprehensive study on the aging behavior of 18650-type lithium-ion batteries, focusing on the uneven intercalation of lithium ions during fast charging processes. It introduces a novel approach using color visual recognition technology to analyze color changes in the graphite anode, indicative of lithiation levels. The study employs X-ray diffraction (XRD) and Distribution of Relaxation Time (DRT) techniques to validate and analyze the observations. The study emphasizes the significance of electrode impedance, the positioning of battery tabs, and electrolyte distribution in influencing the aging dynamics of lithium-ion batteries. Furthermore, the paper presents an innovative impedance Transport-Line Model, specifically developed to capture the evolution of polarization impedance over time. This model offers a deeper understanding of the internal mechanisms driving battery aging, providing valuable insights for the design and optimization of lithium-ion batteries. The research represents a significant contribution to the field, shedding light on the complex aging processes in lithium-ion batteries, particularly under the conditions of fast charging. This could lead to improved battery performance, longevity, and safety, which are critical for the wide range of applications that depend on these energy storage systems."
  },
  {
    "title": "Ultra-Efficient Kidney Stone Fragment Removal via Spinner-Induced Synergistic Circulation and Spiral Flow",
    "url": "http://arxiv.org/abs/2504.11847v1",
    "arxiv_id": "2504.11847v1",
    "authors": [
      "Yilong Chang",
      "Jasmine Vallejo",
      "Yangqing Sun",
      "Ruike Renee Zhao"
    ],
    "published": "2025-04-16T08:10:16+00:00",
    "summary": "Kidney stones can cause severe pain and complications such as chronic kidney disease or kidney failure. Retrograde intrarenal surgery (RIRS), which uses laser lithotripsy to fragment stones for removal via a ureteroscope, is widely adopted due to its safety and effectiveness. However, conventional fragment removal methods using basketing and vacuum-assisted aspiration are inefficient, as they can capture only 1 to 3 fragments (1--3\\,mm in size) per pass, often requiring dozens to hundreds of ureteroscope passes during a single procedure to completely remove the fragments. These limitations lead to prolonged procedures and residual fragments that contribute to high recurrence rates. To address these limitations, we present a novel spinner device that enables ultra-efficient fragment removal through spinning-induced localized suction. The spinner generates a three-dimensional spiral and circulating flow field that dislodges and draws fragments into its cavity even from distances over 20\\,mm, eliminating the need to chase fragments. It can capture over 60 fragments (0.5--2\\,mm) or over 15 larger fragments (2--3\\,mm) in a single pass, significantly improving removal efficiency. In this work, the spinner design is optimized via computational fluid dynamics to maximize suction performance. \\textit{In vitro} testing demonstrates near 100\\% capture rates for up to 60 fragments in a single operation and superior large-distance capture efficacy compared to vacuum-assisted methods. \\textit{Ex vivo} testing of the integrated spinner-ureteroscope system in a porcine kidney confirmed its high performance by capturing 45 fragments in just 4 seconds during a single pass and achieving complete fragment clearance within a few passes."
  },
  {
    "title": "Support is All You Need for Certified VAE Training",
    "url": "http://arxiv.org/abs/2504.11831v1",
    "arxiv_id": "2504.11831v1",
    "authors": [
      "Changming Xu",
      "Debangshu Banerjee",
      "Deepak Vasisht",
      "Gagandeep Singh"
    ],
    "published": "2025-04-16T07:41:40+00:00",
    "summary": "Variational Autoencoders (VAEs) have become increasingly popular and deployed in safety-critical applications. In such applications, we want to give certified probabilistic guarantees on performance under adversarial attacks. We propose a novel method, CIVET, for certified training of VAEs. CIVET depends on the key insight that we can bound worst-case VAE error by bounding the error on carefully chosen support sets at the latent layer. We show this point mathematically and present a novel training algorithm utilizing this insight. We show in an extensive evaluation across different datasets (in both the wireless and vision application areas), architectures, and perturbation magnitudes that our method outperforms SOTA methods achieving good standard performance with strong robustness guarantees."
  },
  {
    "title": "Multi-goal Rapidly Exploring Random Tree with Safety and Dynamic Constraints for UAV Cooperative Path Planning",
    "url": "http://arxiv.org/abs/2504.11823v1",
    "arxiv_id": "2504.11823v1",
    "authors": [
      "Thu Hang Khuat",
      "Duy-Nam Bui",
      "Hoa TT. Nguyen",
      "Mien L. Trinh",
      "Minh T. Nguyen",
      "Manh Duong Phung"
    ],
    "published": "2025-04-16T07:16:35+00:00",
    "summary": "Cooperative path planning is gaining its importance due to the increasing demand on using multiple unmanned aerial vehicles (UAVs) for complex missions. This work addresses the problem by introducing a new algorithm named MultiRRT that extends the rapidly exploring random tree (RRT) to generate paths for a group of UAVs to reach multiple goal locations at the same time. We first derive the dynamics constraint of the UAV and include it in the problem formulation. MultiRRT is then developed, taking into account the cooperative requirements and safe constraints during its path-searching process. The algorithm features two new mechanisms, node reduction and Bezier interpolation, to ensure the feasibility and optimality of the paths generated. Importantly, the interpolated paths are proven to meet the safety and dynamics constraints imposed by obstacles and the UAVs. A number of simulations, comparisons, and experiments have been conducted to evaluate the performance of the proposed approach. The results show that MultiRRT can generate collision-free paths for multiple UAVs to reach their goals with better scores in path length and smoothness metrics than state-of-the-art RRT variants including Theta-RRT, FN-RRT, RRT*, and RRT*-Smart. The generated paths are also tested in practical flights with real UAVs to evaluate their validity for cooperative tasks. The source code of the algorithm is available at https://github.com/duynamrcv/multi-target_RRT"
  },
  {
    "title": "Towards an AI Observatory for the Nuclear Sector: A tool for anticipatory governance",
    "url": "http://arxiv.org/abs/2504.12358v1",
    "arxiv_id": "2504.12358v1",
    "authors": [
      "Aditi Verma",
      "Elizabeth Williams"
    ],
    "published": "2025-04-16T03:43:15+00:00",
    "summary": "AI models are rapidly becoming embedded in all aspects of nuclear energy research and work but the safety, security, and safeguards consequences of this embedding are not well understood. In this paper, we call for the creation of an anticipatory system of governance for AI in the nuclear sector as well as the creation of a global AI observatory as a means for operationalizing anticipatory governance. The paper explores the contours of the nuclear AI observatory and an anticipatory system of governance by drawing on work in science and technology studies, public policy, and foresight studies."
  },
  {
    "title": "Safety with Agency: Human-Centered Safety Filter with Application to AI-Assisted Motorsports",
    "url": "http://arxiv.org/abs/2504.11717v1",
    "arxiv_id": "2504.11717v1",
    "authors": [
      "Donggeon David Oh",
      "Justin Lidard",
      "Haimin Hu",
      "Himani Sinhmar",
      "Elle Lazarski",
      "Deepak Gopinath",
      "Emily S. Sumner",
      "Jonathan A. DeCastro",
      "Guy Rosman",
      "Naomi Ehrich Leonard",
      "Jaime Fern\u00e1ndez Fisac"
    ],
    "published": "2025-04-16T02:42:08+00:00",
    "summary": "We propose a human-centered safety filter (HCSF) for shared autonomy that significantly enhances system safety without compromising human agency. Our HCSF is built on a neural safety value function, which we first learn scalably through black-box interactions and then use at deployment to enforce a novel quality control barrier function (Q-CBF) safety constraint. Since this Q-CBF safety filter does not require any knowledge of the system dynamics for both synthesis and runtime safety monitoring and intervention, our method applies readily to complex, black-box shared autonomy systems. Notably, our HCSF's CBF-based interventions modify the human's actions minimally and smoothly, avoiding the abrupt, last-moment corrections delivered by many conventional safety filters. We validate our approach in a comprehensive in-person user study using Assetto Corsa-a high-fidelity car racing simulator with black-box dynamics-to assess robustness in \"driving on the edge\" scenarios. We compare both trajectory data and drivers' perceptions of our HCSF assistance against unassisted driving and a conventional safety filter. Experimental results show that 1) compared to having no assistance, our HCSF improves both safety and user satisfaction without compromising human agency or comfort, and 2) relative to a conventional safety filter, our proposed HCSF boosts human agency, comfort, and satisfaction while maintaining robustness."
  },
  {
    "title": "Safety with Agency: Human-Centered Safety Filter with Application to AI-Assisted Motorsports",
    "url": "http://arxiv.org/abs/2504.11717v2",
    "arxiv_id": "2504.11717v2",
    "authors": [
      "Donggeon David Oh",
      "Justin Lidard",
      "Haimin Hu",
      "Himani Sinhmar",
      "Elle Lazarski",
      "Deepak Gopinath",
      "Emily S. Sumner",
      "Jonathan A. DeCastro",
      "Guy Rosman",
      "Naomi Ehrich Leonard",
      "Jaime Fern\u00e1ndez Fisac"
    ],
    "published": "2025-04-16T02:42:08+00:00",
    "summary": "We propose a human-centered safety filter (HCSF) for shared autonomy that significantly enhances system safety without compromising human agency. Our HCSF is built on a neural safety value function, which we first learn scalably through black-box interactions and then use at deployment to enforce a novel state-action control barrier function (Q-CBF) safety constraint. Since this Q-CBF safety filter does not require any knowledge of the system dynamics for both synthesis and runtime safety monitoring and intervention, our method applies readily to complex, black-box shared autonomy systems. Notably, our HCSF's CBF-based interventions modify the human's actions minimally and smoothly, avoiding the abrupt, last-moment corrections delivered by many conventional safety filters. We validate our approach in a comprehensive in-person user study using Assetto Corsa-a high-fidelity car racing simulator with black-box dynamics-to assess robustness in \"driving on the edge\" scenarios. We compare both trajectory data and drivers' perceptions of our HCSF assistance against unassisted driving and a conventional safety filter. Experimental results show that 1) compared to having no assistance, our HCSF improves both safety and user satisfaction without compromising human agency or comfort, and 2) relative to a conventional safety filter, our proposed HCSF boosts human agency, comfort, and satisfaction while maintaining robustness."
  },
  {
    "title": "Optimal SVI-Weighted PSPS Decisions with Decision-Dependent Outage Uncertainty",
    "url": "http://arxiv.org/abs/2504.11665v1",
    "arxiv_id": "2504.11665v1",
    "authors": [
      "Ryan Greenough",
      "Kohei Murakami",
      "Jan Kleissl",
      "Adil Khurram"
    ],
    "published": "2025-04-15T23:31:39+00:00",
    "summary": "Public Safety Power Shutoffs (PSPS) are a pre-emptive strategy to mitigate the wildfires caused by power system malfunction. System operators implement PSPS to balance wildfire mitigation efforts through de-energization of transmission lines against the risk of widespread blackouts modeled with load shedding.   Existing approaches do not incorporate decision-dependent wildfire-driven failure probabilities, as modeling outage scenario probabilities requires incorporating high-order polynomial terms in the objective. This paper uses distribution shaping to develop an efficient MILP problem representation of the distributionally robust PSPS problem. Building upon the author's prior work, the wildfire risk of operating a transmission line is a function of the probability of a wildfire-driven outage and its subsequent expected impact in acres burned.   A day-ahead unit commitment and line de-energization PSPS framework is used to assess the trade-off between total cost and wildfire risk at different levels of distributional robustness, parameterized by a level of distributional dissimilarity $\\kappa$. We perform simulations on the IEEE RTS 24-bus test system."
  },
  {
    "title": "Real-time Object and Event Detection Service through Computer Vision and Edge Computing",
    "url": "http://arxiv.org/abs/2504.11662v1",
    "arxiv_id": "2504.11662v1",
    "authors": [
      "Marcos Mendes",
      "Gon\u00e7alo Perna",
      "Pedro Rito",
      "Duarte Raposo",
      "Susana Sargento"
    ],
    "published": "2025-04-15T23:11:42+00:00",
    "summary": "The World Health Organization suggests that road traffic crashes cost approximately 518 billion dollars globally each year, which accounts for 3% of the gross domestic product for most countries. Most fatal road accidents in urban areas involve Vulnerable Road Users (VRUs). Smart cities environments present innovative approaches to combat accidents involving cutting-edge technologies, that include advanced sensors, extensive datasets, Machine Learning (ML) models, communication systems, and edge computing. This paper proposes a strategy and an implementation of a system for road monitoring and safety for smart cities, based on Computer Vision (CV) and edge computing. Promising results were obtained by implementing vision algorithms and tracking using surveillance cameras, that are part of a Smart City testbed, the Aveiro Tech City Living Lab (ATCLL). The algorithm accurately detects and tracks cars, pedestrians, and bicycles, while predicting the road state, the distance between moving objects, and inferring on collision events to prevent collisions, in near real-time."
  },
  {
    "title": "Improving LLM Interpretability and Performance via Guided Embedding Refinement for Sequential Recommendation",
    "url": "http://arxiv.org/abs/2504.11658v1",
    "arxiv_id": "2504.11658v1",
    "authors": [
      "Nanshan Jia",
      "Chenfei Yuan",
      "Yuhang Wu",
      "Zeyu Zheng"
    ],
    "published": "2025-04-15T23:03:53+00:00",
    "summary": "The fast development of Large Language Models (LLMs) offers growing opportunities to further improve sequential recommendation systems. Yet for some practitioners, integrating LLMs to their existing base recommendation systems raises questions about model interpretability, transparency and related safety. To partly alleviate challenges from these questions, we propose guided embedding refinement, a method that carries out a guided and interpretable usage of LLM to enhance the embeddings associated with the base recommendation system. Instead of directly using LLMs as the backbone of sequential recommendation systems, we utilize them as auxiliary tools to emulate the sales logic of recommendation and generate guided embeddings that capture domain-relevant semantic information on interpretable attributes. Benefiting from the strong generalization capabilities of the guided embedding, we construct refined embedding by using the guided embedding and reduced-dimension version of the base embedding. We then integrate the refined embedding into the recommendation module for training and inference. A range of numerical experiments demonstrate that guided embedding is adaptable to various given existing base embedding models, and generalizes well across different recommendation tasks. The numerical results show that the refined embedding not only improves recommendation performance, achieving approximately $10\\%$ to $50\\%$ gains in Mean Reciprocal Rank (MRR), Recall rate, and Normalized Discounted Cumulative Gain (NDCG), but also enhances interpretability, as evidenced by case studies."
  },
  {
    "title": "Verifiable Mission Planning For Space Operations",
    "url": "http://arxiv.org/abs/2504.11631v1",
    "arxiv_id": "2504.11631v1",
    "authors": [
      "Quentin Rommel",
      "Michael Hibbard",
      "Pavan Shukla",
      "Himanshu Save",
      "Srinivas Bettadpur",
      "Ufuk Topcu"
    ],
    "published": "2025-04-15T21:41:09+00:00",
    "summary": "As space missions become more complex, planning methods must maximize mission performance while rigorously enforcing safety. We develop a probabilistic approach based on a finite-horizon Markov decision process to optimize spacecraft operations planning with safety guarantees. In the model, states capture essential mission parameters, and actions represent the operational adjustments needed to meet mission objectives. By directly incorporating uncertainties from environmental conditions and spacecraft dynamics, an optimal sequence of actions is computed that maximizes expected rewards and strictly enforces safety constraints. Numerical experiments on the GRACE-FO mission demonstrate robust performance under uncertainties while providing probabilistic safety guarantees, offering a reliable solution for autonomous spacecraft operations."
  },
  {
    "title": "Provably Safe Control for Constrained Nonlinear Systems with Bounded Input",
    "url": "http://arxiv.org/abs/2504.11592v1",
    "arxiv_id": "2504.11592v1",
    "authors": [
      "Saurabh Kumar",
      "Shashi Ranjan Kumar",
      "Abhinav Sinha"
    ],
    "published": "2025-04-15T20:10:21+00:00",
    "summary": "In real-world control applications, actuator constraints and output constraints (specifically in tracking problems) are inherent and critical to ensuring safe and reliable operation. However, generally, control strategies often neglect these physical limitations, leading to potential instability, degraded performance, or even system failure when deployed on real-world systems. This paper addresses the control design problem for a class of nonlinear systems under both actuator saturation and output constraints. First, a smooth asymmetric saturation model (a more generic representative of practical scenarios) is proposed to model actuator saturation, which ensures that the control inputs always remain confined within a predefined set to ensure safety. Based on the proposed model, we develop a nonlinear control framework that guarantees output tracking while ensuring that system output remains confined to the predefined set. Later, we integrate this design with the constrained output tracking control problem, wherein we show that the system output tracks its desired trajectory by simultaneously satisfying input and output constraints. The global stabilization of the tracking error is achieved in the presence of input constraints, while semi-global stabilization is achieved in the presence of both input and output constraints. Additionally, we rigorously establish the boundedness of all closed-loop signals under the proposed design. Simulation results demonstrate the effectiveness of the proposed methods in handling asymmetric constraints while achieving desirable tracking performance."
  },
  {
    "title": "REAL: Benchmarking Autonomous Agents on Deterministic Simulations of Real Websites",
    "url": "http://arxiv.org/abs/2504.11543v1",
    "arxiv_id": "2504.11543v1",
    "authors": [
      "Divyansh Garg",
      "Shaun VanWeelden",
      "Diego Caples",
      "Andis Draguns",
      "Nikil Ravi",
      "Pranav Putta",
      "Naman Garg",
      "Tomas Abraham",
      "Michael Lara",
      "Federico Lopez",
      "James Liu",
      "Atharva Gundawar",
      "Prannay Hebbar",
      "Youngchul Joo",
      "Charles London",
      "Christian Schroeder de Witt",
      "Sumeet Motwani"
    ],
    "published": "2025-04-15T18:22:55+00:00",
    "summary": "We introduce REAL, a benchmark and framework for multi-turn agent evaluations on deterministic simulations of real-world websites. REAL comprises high-fidelity, deterministic replicas of 11 widely-used websites across domains such as e-commerce, travel, communication, and professional networking. We also release a benchmark consisting of 112 practical tasks that mirror everyday complex user interactions requiring both accurate information retrieval and state-changing actions. All interactions occur within this fully controlled setting, eliminating safety risks and enabling robust, reproducible evaluation of agent capability and reliability. Our novel evaluation framework combines programmatic checks of website state for action-based tasks with rubric-guided LLM-based judgments for information retrieval. The framework supports both open-source and proprietary agent systems through a flexible evaluation harness that accommodates black-box commands within browser environments, allowing research labs to test agentic systems without modification. Our empirical results show that frontier language models achieve at most a 41% success rate on REAL, highlighting critical gaps in autonomous web navigation and task completion capabilities. Our framework supports easy integration of new tasks, reproducible evaluation, and scalable data generation for training web agents. The websites, framework, and leaderboard are available at https://realevals.xyz and https://github.com/agi-inc/REAL."
  },
  {
    "title": "REAL: Benchmarking Autonomous Agents on Deterministic Simulations of Real Websites",
    "url": "http://arxiv.org/abs/2504.11543v2",
    "arxiv_id": "2504.11543v2",
    "authors": [
      "Divyansh Garg",
      "Shaun VanWeelden",
      "Diego Caples",
      "Andis Draguns",
      "Nikil Ravi",
      "Pranav Putta",
      "Naman Garg",
      "Tomas Abraham",
      "Michael Lara",
      "Federico Lopez",
      "James Liu",
      "Atharva Gundawar",
      "Prannay Hebbar",
      "Youngchul Joo",
      "Jindong Gu",
      "Charles London",
      "Christian Schroeder de Witt",
      "Sumeet Motwani"
    ],
    "published": "2025-04-15T18:22:55+00:00",
    "summary": "We introduce REAL, a benchmark and framework for multi-turn agent evaluations on deterministic simulations of real-world websites. REAL comprises high-fidelity, deterministic replicas of 11 widely-used websites across domains such as e-commerce, travel, communication, and professional networking. We also release a benchmark consisting of 112 practical tasks that mirror everyday complex user interactions requiring both accurate information retrieval and state-changing actions. All interactions occur within this fully controlled setting, eliminating safety risks and enabling robust, reproducible evaluation of agent capability and reliability. Our novel evaluation framework combines programmatic checks of website state for action-based tasks with rubric-guided LLM-based judgments for information retrieval. The framework supports both open-source and proprietary agent systems through a flexible evaluation harness that accommodates black-box commands within browser environments, allowing research labs to test agentic systems without modification. Our empirical results show that frontier language models achieve at most a 41% success rate on REAL, highlighting critical gaps in autonomous web navigation and task completion capabilities. Our framework supports easy integration of new tasks, reproducible evaluation, and scalable post-training data generation, marking a significant step forward in evaluating and advancing agent capabilities."
  },
  {
    "title": "ReTool: Reinforcement Learning for Strategic Tool Use in LLMs",
    "url": "http://arxiv.org/abs/2504.11536v1",
    "arxiv_id": "2504.11536v1",
    "authors": [
      "Jiazhan Feng",
      "Shijue Huang",
      "Xingwei Qu",
      "Ge Zhang",
      "Yujia Qin",
      "Baoquan Zhong",
      "Chengquan Jiang",
      "Jinxin Chi",
      "Wanjun Zhong"
    ],
    "published": "2025-04-15T18:10:22+00:00",
    "summary": "While reasoning models (e.g., DeepSeek R1) trained with reinforcement learning (RL), excel in textual reasoning, they struggle in scenarios requiring structured problem-solving, such as geometric reasoning, concise computation, or complex equation solving-areas where computational tools like code interpreters (CI) demonstrate distinct advantages. To bridge this gap, we propose ReTool, which enhances long-form reasoning with tool-integrated learning, including two key features: (1) dynamic interleaving of real-time code execution within natural language reasoning processes, and (2) an automated RL paradigm that allows policy rollouts with multi-turn real-time code execution and teaches the model in learning when and how to invoke tools based on outcome feedback. ReTool employs a systematic training framework, beginning with synthetic cold-start data generation to produce code-augmented long-form reasoning traces for fine-tuning base models. Subsequent RL training leverages task outcomes as rewards to iteratively refine the model's tool use strategy, enabling autonomous discovery of optimal tool invocation patterns without human priors. Experiments on the challenging MATH Olympiad benchmark AIME demonstrate ReTool's superiority: Our 32B model achieves 67% accuracy with 400 training steps, outperforming text-based RL baseline (40% accuracy, 1080 steps) in efficiency and performance. Remarkably, ReTool-32B attains 72.5% accuracy in extended settings, surpassing OpenAI's o1-preview by 27.9%. Further analysis reveals emergent behaviors such as code self-correction, signaling an ''aha moment'' in which the model autonomously masters adaptive tool use. These findings highlight the promise of outcome-driven tool integration for advancing complex mathematical reasoning and offer new insights into hybrid neuro-symbolic systems."
  },
  {
    "title": "ReTool: Reinforcement Learning for Strategic Tool Use in LLMs",
    "url": "http://arxiv.org/abs/2504.11536v2",
    "arxiv_id": "2504.11536v2",
    "authors": [
      "Jiazhan Feng",
      "Shijue Huang",
      "Xingwei Qu",
      "Ge Zhang",
      "Yujia Qin",
      "Baoquan Zhong",
      "Chengquan Jiang",
      "Jinxin Chi",
      "Wanjun Zhong"
    ],
    "published": "2025-04-15T18:10:22+00:00",
    "summary": "While reasoning models (e.g., DeepSeek R1) trained with reinforcement learning (RL), excel in textual reasoning, they struggle in scenarios requiring structured problem-solving, such as geometric reasoning, concise computation, or complex equation solving-areas where computational tools like code interpreters (CI) demonstrate distinct advantages. To bridge this gap, we propose ReTool, which enhances long-form reasoning with tool-integrated learning, including two key features: (1) dynamic interleaving of real-time code execution within natural language reasoning processes, and (2) an automated RL paradigm that allows policy rollouts with multi-turn real-time code execution and teaches the model in learning when and how to invoke tools based on outcome feedback. ReTool employs a systematic training framework, beginning with synthetic cold-start data generation to produce code-augmented long-form reasoning traces for fine-tuning base models. Subsequent RL training leverages task outcomes as rewards to iteratively refine the model's tool use strategy, enabling autonomous discovery of optimal tool invocation patterns without human priors. Experiments on the challenging MATH Olympiad benchmark AIME demonstrate ReTool's superiority: Our 32B model achieves 67% accuracy with 400 training steps, outperforming text-based RL baseline (40% accuracy, 1080 steps) in efficiency and performance. Remarkably, ReTool-32B attains 72.5% accuracy in extended settings, surpassing OpenAI's o1-preview by 27.9%. Further analysis reveals emergent behaviors such as code self-correction, signaling an ''aha moment'' in which the model autonomously masters adaptive tool use. These findings highlight the promise of outcome-driven tool integration for advancing complex mathematical reasoning and offer new insights into hybrid neuro-symbolic systems."
  },
  {
    "title": "LANGTRAJ: Diffusion Model and Dataset for Language-Conditioned Trajectory Simulation",
    "url": "http://arxiv.org/abs/2504.11521v1",
    "arxiv_id": "2504.11521v1",
    "authors": [
      "Wei-Jer Chang",
      "Wei Zhan",
      "Masayoshi Tomizuka",
      "Manmohan Chandraker",
      "Francesco Pittaluga"
    ],
    "published": "2025-04-15T17:14:06+00:00",
    "summary": "Evaluating autonomous vehicles with controllability enables scalable testing in counterfactual or structured settings, enhancing both efficiency and safety. We introduce LangTraj, a language-conditioned scene-diffusion model that simulates the joint behavior of all agents in traffic scenarios. By conditioning on natural language inputs, LangTraj provides flexible and intuitive control over interactive behaviors, generating nuanced and realistic scenarios. Unlike prior approaches that depend on domain-specific guidance functions, LangTraj incorporates language conditioning during training, facilitating more intuitive traffic simulation control. We propose a novel closed-loop training strategy for diffusion models, explicitly tailored to enhance stability and realism during closed-loop simulation. To support language-conditioned simulation, we develop Inter-Drive, a large-scale dataset with diverse and interactive labels for training language-conditioned diffusion models. Our dataset is built upon a scalable pipeline for annotating agent-agent interactions and single-agent behaviors, ensuring rich and varied supervision. Validated on the Waymo Motion Dataset, LangTraj demonstrates strong performance in realism, language controllability, and language-conditioned safety-critical simulation, establishing a new paradigm for flexible and scalable autonomous vehicle testing."
  },
  {
    "title": "A Minimalist Approach to LLM Reasoning: from Rejection Sampling to Reinforce",
    "url": "http://arxiv.org/abs/2504.11343v1",
    "arxiv_id": "2504.11343v1",
    "authors": [
      "Wei Xiong",
      "Jiarui Yao",
      "Yuhui Xu",
      "Bo Pang",
      "Lei Wang",
      "Doyen Sahoo",
      "Junnan Li",
      "Nan Jiang",
      "Tong Zhang",
      "Caiming Xiong",
      "Hanze Dong"
    ],
    "published": "2025-04-15T16:15:02+00:00",
    "summary": "Reinforcement learning (RL) has become a prevailing approach for fine-tuning large language models (LLMs) on complex reasoning tasks. Among recent methods, GRPO stands out for its empirical success in training models such as DeepSeek-R1, yet the sources of its effectiveness remain poorly understood. In this work, we revisit GRPO from a reinforce-like algorithm perspective and analyze its core components. Surprisingly, we find that a simple rejection sampling baseline, RAFT, which trains only on positively rewarded samples, yields competitive performance than GRPO and PPO. Our ablation studies reveal that GRPO's main advantage arises from discarding prompts with entirely incorrect responses, rather than from its reward normalization. Motivated by this insight, we propose Reinforce-Rej, a minimal extension of policy gradient that filters both entirely incorrect and entirely correct samples. Reinforce-Rej improves KL efficiency and stability, serving as a lightweight yet effective alternative to more complex RL algorithms. We advocate RAFT as a robust and interpretable baseline, and suggest that future advances should focus on more principled designs for incorporating negative samples, rather than relying on them indiscriminately. Our findings provide guidance for future work in reward-based LLM post-training."
  },
  {
    "title": "Robust MPC for Uncertain Linear Systems -- Combining Model Adaptation and Iterative Learning",
    "url": "http://arxiv.org/abs/2504.11261v1",
    "arxiv_id": "2504.11261v1",
    "authors": [
      "Hannes Petrenz",
      "Johannes K\u00f6hler",
      "Francesco Borrelli"
    ],
    "published": "2025-04-15T15:00:34+00:00",
    "summary": "This paper presents a robust adaptive learning Model Predictive Control (MPC) framework for linear systems with parametric uncertainties and additive disturbances performing iterative tasks. The approach iteratively refines the parameter estimates using set membership estimation. Performance enhancement over iterations is achieved by learning the terminal cost from data. Safety is enforced using a terminal set, which is also learned iteratively. The proposed method guarantees recursive feasibility, constraint satisfaction, and a robust bound on the closed-loop cost. Numerical simulations on a mass-spring-damper system demonstrate improved computational efficiency and control performance compared to an existing robust adaptive MPC approach."
  },
  {
    "title": "Robust MPC for Uncertain Linear Systems -- Combining Model Adaptation and Iterative Learning",
    "url": "http://arxiv.org/abs/2504.11261v2",
    "arxiv_id": "2504.11261v2",
    "authors": [
      "Hannes Petrenz",
      "Johannes K\u00f6hler",
      "Francesco Borrelli"
    ],
    "published": "2025-04-15T15:00:34+00:00",
    "summary": "This paper presents a robust adaptive learning Model Predictive Control (MPC) framework for linear systems with parametric uncertainties and additive disturbances performing iterative tasks. The approach iteratively refines the parameter estimates using set membership estimation. Performance enhancement over iterations is achieved by learning the terminal cost from data. Safety is enforced using a terminal set, which is also learned iteratively. The proposed method guarantees recursive feasibility, constraint satisfaction, and a robust bound on the closed-loop cost. Numerical simulations on a mass-spring-damper system demonstrate improved computational efficiency and control performance compared to an existing robust adaptive MPC approach."
  },
  {
    "title": "Towards Automated Safety Requirements Derivation Using Agent-based RAG",
    "url": "http://arxiv.org/abs/2504.11243v1",
    "arxiv_id": "2504.11243v1",
    "authors": [
      "Balahari Vignesh Balu",
      "Florian Geissler",
      "Francesco Carella",
      "Joao-Vitor Zacchi",
      "Josef Jiru",
      "Nuria Mata",
      "Reinhard Stolle"
    ],
    "published": "2025-04-15T14:43:19+00:00",
    "summary": "We study the automated derivation of safety requirements in a self-driving vehicle use case, leveraging LLMs in combination with agent-based retrieval-augmented generation. Conventional approaches that utilise pre-trained LLMs to assist in safety analyses typically lack domain-specific knowledge. Existing RAG approaches address this issue, yet their performance deteriorates when handling complex queries and it becomes increasingly harder to retrieve the most relevant information. This is particularly relevant for safety-relevant applications. In this paper, we propose the use of agent-based RAG to derive safety requirements and show that the retrieved information is more relevant to the queries. We implement an agent-based approach on a document pool of automotive standards and the Apollo case study, as a representative example of an automated driving perception system. Our solution is tested on a data set of safety requirement questions and answers, extracted from the Apollo data. Evaluating a set of selected RAG metrics, we present and discuss advantages of a agent-based approach compared to default RAG methods."
  },
  {
    "title": "Nondeterministic Polynomial-time Problem Challenge: An Ever-Scaling Reasoning Benchmark for LLMs",
    "url": "http://arxiv.org/abs/2504.11239v1",
    "arxiv_id": "2504.11239v1",
    "authors": [
      "Chang Yang",
      "Ruiyu Wang",
      "Junzhe Jiang",
      "Qi Jiang",
      "Qinggang Zhang",
      "Yanchen Deng",
      "Shuxin Li",
      "Shuyue Hu",
      "Bo Li",
      "Florian T. Pokorny",
      "Xiao Huang",
      "Xinrun Wang"
    ],
    "published": "2025-04-15T14:40:29+00:00",
    "summary": "Reasoning is the fundamental capability of large language models (LLMs). Due to the rapid progress of LLMs, there are two main issues of current benchmarks: i) these benchmarks can be crushed in a short time (less than 1 year), and ii) these benchmarks may be easily hacked. To handle these issues, we propose the ever-scalingness for building the benchmarks which are uncrushable, unhackable, auto-verifiable and general. This paper presents Nondeterministic Polynomial-time Problem Challenge (NPPC), an ever-scaling reasoning benchmark for LLMs. Specifically, the NPPC has three main modules: i) npgym, which provides a unified interface of 25 well-known NP-complete problems and can generate any number of instances with any levels of complexities, ii) npsolver: which provides a unified interface to evaluate the problem instances with both online and offline models via APIs and local deployments, respectively, and iii) npeval: which provides the comprehensive and ready-to-use tools to analyze the performances of LLMs over different problems, the number of tokens, the aha moments, the reasoning errors and the solution errors. Extensive experiments over widely-used LLMs demonstrate: i) NPPC can successfully decrease the performances of advanced LLMs' performances to below 10%, demonstrating that NPPC is uncrushable, ii) DeepSeek-R1, Claude-3.7-Sonnet, and o1/o3-mini are the most powerful LLMs, where DeepSeek-R1 outperforms Claude-3.7-Sonnet and o1/o3-mini in most NP-complete problems considered, and iii) the numbers of tokens, aha moments in the advanced LLMs, e.g., Claude-3.7-Sonnet and DeepSeek-R1, are observed first to increase and then decrease when the problem instances become more and more difficult. We believe that NPPC is the first ever-scaling reasoning benchmark, serving as the uncrushable and unhackable testbed for LLMs toward artificial general intelligence (AGI)."
  },
  {
    "title": "Enhancing Autonomous Driving Systems with On-Board Deployed Large Language Models",
    "url": "http://arxiv.org/abs/2504.11514v1",
    "arxiv_id": "2504.11514v1",
    "authors": [
      "Nicolas Baumann",
      "Cheng Hu",
      "Paviththiren Sivasothilingam",
      "Haotong Qin",
      "Lei Xie",
      "Michele Magno",
      "Luca Benini"
    ],
    "published": "2025-04-15T13:49:17+00:00",
    "summary": "Neural Networks (NNs) trained through supervised learning struggle with managing edge-case scenarios common in real-world driving due to the intractability of exhaustive datasets covering all edge-cases, making knowledge-driven approaches, akin to how humans intuitively detect unexpected driving behavior, a suitable complement to data-driven methods. This work proposes a hybrid architecture combining low-level Model Predictive Controller (MPC) with locally deployed Large Language Models (LLMs) to enhance decision-making and Human Machine Interaction (HMI). The DecisionxLLM module evaluates robotic state information against natural language instructions to ensure adherence to desired driving behavior. The MPCxLLM module then adjusts MPC parameters based on LLM-generated insights, achieving control adaptability while preserving the safety and constraint guarantees of traditional MPC systems. Further, to enable efficient on-board deployment and to eliminate dependency on cloud connectivity, we shift processing to the on-board computing platform: We propose an approach that exploits Retrieval Augmented Generation (RAG), Low Rank Adaptation (LoRA) fine-tuning, and quantization. Experimental results demonstrate that these enhancements yield significant improvements in reasoning accuracy by up to 10.45%, control adaptability by as much as 52.2%, and up to 10.5x increase in computational efficiency (tokens/s), validating the proposed framework's practicality for real-time deployment even on down-scaled robotic platforms. This work bridges high-level decision-making with low-level control adaptability, offering a synergistic framework for knowledge-driven and adaptive Autonomous Driving Systems (ADS)."
  },
  {
    "title": "Benchmarking Next-Generation Reasoning-Focused Large Language Models in Ophthalmology: A Head-to-Head Evaluation on 5,888 Items",
    "url": "http://arxiv.org/abs/2504.11186v1",
    "arxiv_id": "2504.11186v1",
    "authors": [
      "Minjie Zou",
      "Sahana Srinivasan",
      "Thaddaeus Wai Soon Lo",
      "Ke Zou",
      "Gabriel Dawei Yang",
      "Xuguang Ai",
      "Hyunjae Kim",
      "Maxwell Singer",
      "Fares Antaki",
      "Kelvin Li",
      "Robert Chang",
      "Marcus Tan",
      "David Ziyou Chen",
      "Dianbo Liu",
      "Qingyu Chen",
      "Yih Chung Tham"
    ],
    "published": "2025-04-15T13:42:34+00:00",
    "summary": "Recent advances in reasoning-focused large language models (LLMs) mark a shift from general LLMs toward models designed for complex decision-making, a crucial aspect in medicine. However, their performance in specialized domains like ophthalmology remains underexplored. This study comprehensively evaluated and compared the accuracy and reasoning capabilities of four newly developed reasoning-focused LLMs, namely DeepSeek-R1, OpenAI o1, o3-mini, and Gemini 2.0 Flash-Thinking. Each model was assessed using 5,888 multiple-choice ophthalmology exam questions from the MedMCQA dataset in zero-shot setting. Quantitative evaluation included accuracy, Macro-F1, and five text-generation metrics (ROUGE-L, METEOR, BERTScore, BARTScore, and AlignScore), computed against ground-truth reasonings. Average inference time was recorded for a subset of 100 randomly selected questions. Additionally, two board-certified ophthalmologists qualitatively assessed clarity, completeness, and reasoning structure of responses to differential diagnosis questions.O1 (0.902) and DeepSeek-R1 (0.888) achieved the highest accuracy, with o1 also leading in Macro-F1 (0.900). The performance of models across the text-generation metrics varied: O3-mini excelled in ROUGE-L (0.151), o1 in METEOR (0.232), DeepSeek-R1 and o3-mini tied for BERTScore (0.673), DeepSeek-R1 (-4.105) and Gemini 2.0 Flash-Thinking (-4.127) performed best in BARTScore, while o3-mini (0.181) and o1 (0.176) led AlignScore. Inference time across the models varied, with DeepSeek-R1 being slowest (40.4 seconds) and Gemini 2.0 Flash-Thinking fastest (6.7 seconds). Qualitative evaluation revealed that DeepSeek-R1 and Gemini 2.0 Flash-Thinking tended to provide detailed and comprehensive intermediate reasoning, whereas o1 and o3-mini displayed concise and summarized justifications."
  },
  {
    "title": "Exploring Backdoor Attack and Defense for LLM-empowered Recommendations",
    "url": "http://arxiv.org/abs/2504.11182v1",
    "arxiv_id": "2504.11182v1",
    "authors": [
      "Liangbo Ning",
      "Wenqi Fan",
      "Qing Li"
    ],
    "published": "2025-04-15T13:37:38+00:00",
    "summary": "The fusion of Large Language Models (LLMs) with recommender systems (RecSys) has dramatically advanced personalized recommendations and drawn extensive attention. Despite the impressive progress, the safety of LLM-based RecSys against backdoor attacks remains largely under-explored. In this paper, we raise a new problem: Can a backdoor with a specific trigger be injected into LLM-based Recsys, leading to the manipulation of the recommendation responses when the backdoor trigger is appended to an item's title? To investigate the vulnerabilities of LLM-based RecSys under backdoor attacks, we propose a new attack framework termed Backdoor Injection Poisoning for RecSys (BadRec). BadRec perturbs the items' titles with triggers and employs several fake users to interact with these items, effectively poisoning the training set and injecting backdoors into LLM-based RecSys. Comprehensive experiments reveal that poisoning just 1% of the training data with adversarial examples is sufficient to successfully implant backdoors, enabling manipulation of recommendations. To further mitigate such a security threat, we propose a universal defense strategy called Poison Scanner (P-Scanner). Specifically, we introduce an LLM-based poison scanner to detect the poisoned items by leveraging the powerful language understanding and rich knowledge of LLMs. A trigger augmentation agent is employed to generate diverse synthetic triggers to guide the poison scanner in learning domain-specific knowledge of the poisoned item detection task. Extensive experiments on three real-world datasets validate the effectiveness of the proposed P-Scanner."
  },
  {
    "title": "A Real-time Anomaly Detection Method for Robots based on a Flexible and Sparse Latent Space",
    "url": "http://arxiv.org/abs/2504.11170v1",
    "arxiv_id": "2504.11170v1",
    "authors": [
      "Taewook Kang",
      "Bum-Jae You",
      "Juyoun Park",
      "Yisoo Lee"
    ],
    "published": "2025-04-15T13:17:14+00:00",
    "summary": "The growing demand for robots to operate effectively in diverse environments necessitates the need for robust real-time anomaly detection techniques during robotic operations. However, deep learning-based models in robotics face significant challenges due to limited training data and highly noisy signal features. In this paper, we present Sparse Masked Autoregressive Flow-based Adversarial AutoEncoders model to address these problems. This approach integrates Masked Autoregressive Flow model into Adversarial AutoEncoders to construct a flexible latent space and utilize Sparse autoencoder to efficiently focus on important features, even in scenarios with limited feature space. Our experiments demonstrate that the proposed model achieves a 4.96% to 9.75% higher area under the receiver operating characteristic curve for pick-and-place robotic operations with randomly placed cans, compared to existing state-of-the-art methods. Notably, it showed up to 19.67% better performance in scenarios involving collisions with lightweight objects. Additionally, unlike the existing state-of-the-art model, our model performs inferences within 1 millisecond, ensuring real-time anomaly detection. These capabilities make our model highly applicable to machine learning-based robotic safety systems in dynamic environments. The code will be made publicly available after acceptance."
  },
  {
    "title": "A Real-time Anomaly Detection Method for Robots based on a Flexible and Sparse Latent Space",
    "url": "http://arxiv.org/abs/2504.11170v2",
    "arxiv_id": "2504.11170v2",
    "authors": [
      "Taewook Kang",
      "Bum-Jae You",
      "Juyoun Park",
      "Yisoo Lee"
    ],
    "published": "2025-04-15T13:17:14+00:00",
    "summary": "The growing demand for robots to operate effectively in diverse environments necessitates the need for robust real-time anomaly detection techniques during robotic operations. However, deep learning-based models in robotics face significant challenges due to limited training data and highly noisy signal features. In this paper, we present Sparse Masked Autoregressive Flow-based Adversarial AutoEncoders model to address these problems. This approach integrates Masked Autoregressive Flow model into Adversarial AutoEncoders to construct a flexible latent space and utilize Sparse autoencoder to efficiently focus on important features, even in scenarios with limited feature space. Our experiments demonstrate that the proposed model achieves a 4.96% to 9.75% higher area under the receiver operating characteristic curve for pick-and-place robotic operations with randomly placed cans, compared to existing state-of-the-art methods. Notably, it showed up to 19.67% better performance in scenarios involving collisions with lightweight objects. Additionally, unlike the existing state-of-the-art model, our model performs inferences within 1 millisecond, ensuring real-time anomaly detection. These capabilities make our model highly applicable to machine learning-based robotic safety systems in dynamic environments. The code will be made publicly available after acceptance."
  },
  {
    "title": "Bypassing Prompt Injection and Jailbreak Detection in LLM Guardrails",
    "url": "http://arxiv.org/abs/2504.11168v1",
    "arxiv_id": "2504.11168v1",
    "authors": [
      "William Hackett",
      "Lewis Birch",
      "Stefan Trawicki",
      "Neeraj Suri",
      "Peter Garraghan"
    ],
    "published": "2025-04-15T13:16:02+00:00",
    "summary": "Large Language Models (LLMs) guardrail systems are designed to protect against prompt injection and jailbreak attacks. However, they remain vulnerable to evasion techniques. We demonstrate two approaches for bypassing LLM prompt injection and jailbreak detection systems via traditional character injection methods and algorithmic Adversarial Machine Learning (AML) evasion techniques. Through testing against six prominent protection systems, including Microsoft's Azure Prompt Shield and Meta's Prompt Guard, we show that both methods can be used to evade detection while maintaining adversarial utility achieving in some instances up to 100% evasion success. Furthermore, we demonstrate that adversaries can enhance Attack Success Rates (ASR) against black-box targets by leveraging word importance ranking computed by offline white-box models. Our findings reveal vulnerabilities within current LLM protection mechanisms and highlight the need for more robust guardrail systems."
  },
  {
    "title": "Bypassing Prompt Injection and Jailbreak Detection in LLM Guardrails",
    "url": "http://arxiv.org/abs/2504.11168v2",
    "arxiv_id": "2504.11168v2",
    "authors": [
      "William Hackett",
      "Lewis Birch",
      "Stefan Trawicki",
      "Neeraj Suri",
      "Peter Garraghan"
    ],
    "published": "2025-04-15T13:16:02+00:00",
    "summary": "Large Language Models (LLMs) guardrail systems are designed to protect against prompt injection and jailbreak attacks. However, they remain vulnerable to evasion techniques. We demonstrate two approaches for bypassing LLM prompt injection and jailbreak detection systems via traditional character injection methods and algorithmic Adversarial Machine Learning (AML) evasion techniques. Through testing against six prominent protection systems, including Microsoft's Azure Prompt Shield and Meta's Prompt Guard, we show that both methods can be used to evade detection while maintaining adversarial utility achieving in some instances up to 100% evasion success. Furthermore, we demonstrate that adversaries can enhance Attack Success Rates (ASR) against black-box targets by leveraging word importance ranking computed by offline white-box models. Our findings reveal vulnerabilities within current LLM protection mechanisms and highlight the need for more robust guardrail systems."
  },
  {
    "title": "Token-Level Constraint Boundary Search for Jailbreaking Text-to-Image Models",
    "url": "http://arxiv.org/abs/2504.11106v1",
    "arxiv_id": "2504.11106v1",
    "authors": [
      "Jiangtao Liu",
      "Zhaoxin Wang",
      "Handing Wang",
      "Cong Tian",
      "Yaochu Jin"
    ],
    "published": "2025-04-15T11:53:40+00:00",
    "summary": "Recent advancements in Text-to-Image (T2I) generation have significantly enhanced the realism and creativity of generated images. However, such powerful generative capabilities pose risks related to the production of inappropriate or harmful content. Existing defense mechanisms, including prompt checkers and post-hoc image checkers, are vulnerable to sophisticated adversarial attacks. In this work, we propose TCBS-Attack, a novel query-based black-box jailbreak attack that searches for tokens located near the decision boundaries defined by text and image checkers. By iteratively optimizing tokens near these boundaries, TCBS-Attack generates semantically coherent adversarial prompts capable of bypassing multiple defensive layers in T2I models. Extensive experiments demonstrate that our method consistently outperforms state-of-the-art jailbreak attacks across various T2I models, including securely trained open-source models and commercial online services like DALL-E 3. TCBS-Attack achieves an ASR-4 of 45\\% and an ASR-1 of 21\\% on jailbreaking full-chain T2I models, significantly surpassing baseline methods."
  },
  {
    "title": "Neural Control Barrier Functions from Physics Informed Neural Networks",
    "url": "http://arxiv.org/abs/2504.11045v1",
    "arxiv_id": "2504.11045v1",
    "authors": [
      "Shreenabh Agrawal",
      "Manan Tayal",
      "Aditya Singh",
      "Shishir Kolathaya"
    ],
    "published": "2025-04-15T10:13:30+00:00",
    "summary": "As autonomous systems become increasingly prevalent in daily life, ensuring their safety is paramount. Control Barrier Functions (CBFs) have emerged as an effective tool for guaranteeing safety; however, manually designing them for specific applications remains a significant challenge. With the advent of deep learning techniques, recent research has explored synthesizing CBFs using neural networks-commonly referred to as neural CBFs. This paper introduces a novel class of neural CBFs that leverages a physics-inspired neural network framework by incorporating Zubov's Partial Differential Equation (PDE) within the context of safety. This approach provides a scalable methodology for synthesizing neural CBFs applicable to high-dimensional systems. Furthermore, by utilizing reciprocal CBFs instead of zeroing CBFs, the proposed framework allows for the specification of flexible, user-defined safe regions. To validate the effectiveness of the approach, we present case studies on three different systems: an inverted pendulum, autonomous ground navigation, and aerial navigation in obstacle-laden environments."
  },
  {
    "title": "DRIFT open dataset: A drone-derived intelligence for traffic analysis in urban environmen",
    "url": "http://arxiv.org/abs/2504.11019v1",
    "arxiv_id": "2504.11019v1",
    "authors": [
      "Hyejin Lee",
      "Seokjun Hong",
      "Jeonghoon Song",
      "Haechan Cho",
      "Zhixiong Jin",
      "Byeonghun Kim",
      "Joobin Jin",
      "Jaegyun Im",
      "Byeongjoon Noh",
      "Hwasoo Yeo"
    ],
    "published": "2025-04-15T09:43:13+00:00",
    "summary": "Reliable traffic data are essential for understanding urban mobility and developing effective traffic management strategies. This study introduces the DRone-derived Intelligence For Traffic analysis (DRIFT) dataset, a large-scale urban traffic dataset collected systematically from synchronized drone videos at approximately 250 meters altitude, covering nine interconnected intersections in Daejeon, South Korea. DRIFT provides high-resolution vehicle trajectories that include directional information, processed through video synchronization and orthomap alignment, resulting in a comprehensive dataset of 81,699 vehicle trajectories. Through our DRIFT dataset, researchers can simultaneously analyze traffic at multiple scales - from individual vehicle maneuvers like lane-changes and safety metrics such as time-to-collision to aggregate network flow dynamics across interconnected urban intersections. The DRIFT dataset is structured to enable immediate use without additional preprocessing, complemented by open-source models for object detection and trajectory extraction, as well as associated analytical tools. DRIFT is expected to significantly contribute to academic research and practical applications, such as traffic flow analysis and simulation studies. The dataset and related resources are publicly accessible at https://github.com/AIxMobility/The-DRIFT."
  },
  {
    "title": "Reward Distance Comparisons Under Transition Sparsity",
    "url": "http://arxiv.org/abs/2504.11508v1",
    "arxiv_id": "2504.11508v1",
    "authors": [
      "Clement Nyanhongo",
      "Bruno Miranda Henrique",
      "Eugene Santos"
    ],
    "published": "2025-04-15T09:27:53+00:00",
    "summary": "Reward comparisons are vital for evaluating differences in agent behaviors induced by a set of reward functions. Most conventional techniques utilize the input reward functions to learn optimized policies, which are then used to compare agent behaviors. However, learning these policies can be computationally expensive and can also raise safety concerns. Direct reward comparison techniques obviate policy learning but suffer from transition sparsity, where only a small subset of transitions are sampled due to data collection challenges and feasibility constraints. Existing state-of-the-art direct reward comparison methods are ill-suited for these sparse conditions since they require high transition coverage, where the majority of transitions from a given coverage distribution are sampled. When this requirement is not satisfied, a distribution mismatch between sampled and expected transitions can occur, leading to significant errors. This paper introduces the Sparsity Resilient Reward Distance (SRRD) pseudometric, designed to eliminate the need for high transition coverage by accommodating diverse sample distributions, which are common under transition sparsity. We provide theoretical justification for SRRD's robustness and conduct experiments to demonstrate its practical efficacy across multiple domains."
  },
  {
    "title": "Drivers and barriers of adopting shared micromobility: a latent class clustering model on the attitudes towards shared micromobility as part of public transport trips in the Netherlands",
    "url": "http://arxiv.org/abs/2504.10943v1",
    "arxiv_id": "2504.10943v1",
    "authors": [
      "Nejc Ger\u017eini\u010d",
      "Mark van Hagen",
      "Hussein Al-Tamimi",
      "Niels van Oort",
      "Dorine Duives"
    ],
    "published": "2025-04-15T07:46:19+00:00",
    "summary": "Shared micromobility (SMM) is often cited as a solution to the first/last mile problem of public transport (train) travel, yet when implemented, they often do not get adopted by a broader travelling public. A large part of behavioural adoption is related to peoples' attitudes and perceptions. In this paper, we develop an adjusted behavioural framework, based on the UTAUT2 technology acceptance framework. We carry out an exploratory factor analysis (EFA) to obtain attitudinal factors which we then use to perform a latent class cluster analysis (LCCA), with the goal of studying the potential adoption of SMM and to assess the various drivers and barriers as perceived by different user groups. Our findings suggest there are six distinct user groups with varying intention to use shared micromobility: Progressives, Conservatives, Hesitant participants, Bold innovators, Anxious observers and Skilled sceptics. Bold innovators and Progressives tend to be the most open to adopting SMM and are also able to do so. Hesitant participants would like to, but find it difficult or dangerous to use, while Skilled sceptics are capable and confident, but have limited intention of using it. Conservatives and Anxious observers are most negative about SMM, finding it difficult to use and dangerous. In general, factors relating to technological savviness, ease-of-use, physical safety and societal perception seem to be the biggest barriers to wider adoption. Younger, highly educated males are the group most likely and open to using shared micromobility, while older individuals with lower incomes and a lower level of education tend to be the least likely."
  },
  {
    "title": "Safe-Construct: Redefining Construction Safety Violation Recognition as 3D Multi-View Engagement Task",
    "url": "http://arxiv.org/abs/2504.10880v1",
    "arxiv_id": "2504.10880v1",
    "authors": [
      "Aviral Chharia",
      "Tianyu Ren",
      "Tomotake Furuhata",
      "Kenji Shimada"
    ],
    "published": "2025-04-15T05:21:09+00:00",
    "summary": "Recognizing safety violations in construction environments is critical yet remains underexplored in computer vision. Existing models predominantly rely on 2D object detection, which fails to capture the complexities of real-world violations due to: (i) an oversimplified task formulation treating violation recognition merely as object detection, (ii) inadequate validation under realistic conditions, (iii) absence of standardized baselines, and (iv) limited scalability from the unavailability of synthetic dataset generators for diverse construction scenarios. To address these challenges, we introduce Safe-Construct, the first framework that reformulates violation recognition as a 3D multi-view engagement task, leveraging scene-level worker-object context and 3D spatial understanding. We also propose the Synthetic Indoor Construction Site Generator (SICSG) to create diverse, scalable training data, overcoming data limitations. Safe-Construct achieves a 7.6% improvement over state-of-the-art methods across four violation types. We rigorously evaluate our approach in near-realistic settings, incorporating four violations, four workers, 14 objects, and challenging conditions like occlusions (worker-object, worker-worker) and variable illumination (back-lighting, overexposure, sunlight). By integrating 3D multi-view spatial understanding and synthetic data generation, Safe-Construct sets a new benchmark for scalable and robust safety monitoring in high-risk industries. Project Website: https://Safe-Construct.github.io/Safe-Construct"
  },
  {
    "title": "Hallucination-Aware Generative Pretrained Transformer for Cooperative Aerial Mobility Control",
    "url": "http://arxiv.org/abs/2504.10831v1",
    "arxiv_id": "2504.10831v1",
    "authors": [
      "Hyojun Ahn",
      "Seungcheol Oh",
      "Gyu Seon Kim",
      "Soyi Jung",
      "Soohyun Park",
      "Joongheon Kim"
    ],
    "published": "2025-04-15T03:21:08+00:00",
    "summary": "This paper proposes SafeGPT, a two-tiered framework that integrates generative pretrained transformers (GPTs) with reinforcement learning (RL) for efficient and reliable unmanned aerial vehicle (UAV) last-mile deliveries. In the proposed design, a Global GPT module assigns high-level tasks such as sector allocation, while an On-Device GPT manages real-time local route planning. An RL-based safety filter monitors each GPT decision and overrides unsafe actions that could lead to battery depletion or duplicate visits, effectively mitigating hallucinations. Furthermore, a dual replay buffer mechanism helps both the GPT modules and the RL agent refine their strategies over time. Simulation results demonstrate that SafeGPT achieves higher delivery success rates compared to a GPT-only baseline, while substantially reducing battery consumption and travel distance. These findings validate the efficacy of combining GPT-based semantic reasoning with formal safety guarantees, contributing a viable solution for robust and energy-efficient UAV logistics."
  },
  {
    "title": "LayoutCoT: Unleashing the Deep Reasoning Potential of Large Language Models for Layout Generation",
    "url": "http://arxiv.org/abs/2504.10829v1",
    "arxiv_id": "2504.10829v1",
    "authors": [
      "Hengyu Shi",
      "Junhao Su",
      "Huansheng Ning",
      "Xiaoming Wei",
      "Jialin Gao"
    ],
    "published": "2025-04-15T03:12:01+00:00",
    "summary": "Conditional layout generation aims to automatically generate visually appealing and semantically coherent layouts from user-defined constraints. While recent methods based on generative models have shown promising results, they typically require substantial amounts of training data or extensive fine-tuning, limiting their versatility and practical applicability. Alternatively, some training-free approaches leveraging in-context learning with Large Language Models (LLMs) have emerged, but they often suffer from limited reasoning capabilities and overly simplistic ranking mechanisms, which restrict their ability to generate consistently high-quality layouts. To this end, we propose LayoutCoT, a novel approach that leverages the reasoning capabilities of LLMs through a combination of Retrieval-Augmented Generation (RAG) and Chain-of-Thought (CoT) techniques. Specifically, LayoutCoT transforms layout representations into a standardized serialized format suitable for processing by LLMs. A Layout-aware RAG is used to facilitate effective retrieval and generate a coarse layout by LLMs. This preliminary layout, together with the selected exemplars, is then fed into a specially designed CoT reasoning module for iterative refinement, significantly enhancing both semantic coherence and visual quality. We conduct extensive experiments on five public datasets spanning three conditional layout generation tasks. Experimental results demonstrate that LayoutCoT achieves state-of-the-art performance without requiring training or fine-tuning. Notably, our CoT reasoning module enables standard LLMs, even those without explicit deep reasoning abilities, to outperform specialized deep-reasoning models such as deepseek-R1, highlighting the potential of our approach in unleashing the deep reasoning capabilities of LLMs for layout generation tasks."
  },
  {
    "title": "A Framework for the Private Governance of Frontier Artificial Intelligence",
    "url": "http://arxiv.org/abs/2504.11501v1",
    "arxiv_id": "2504.11501v1",
    "authors": [
      "Dean W. Ball"
    ],
    "published": "2025-04-15T02:56:26+00:00",
    "summary": "This paper presents a proposal for the governance of frontier AI systems through a hybrid public-private system. Private bodies, authorized and overseen by government, provide certifications to developers of frontier AI systems on an opt-in basis. In exchange for opting in, frontier AI firms receive protections from tort liability for customer misuse of their models. Before detailing the proposal, the paper explores more commonly discussed approaches to AI governance, analyzing their strengths and flaws. It also examines the nature of frontier AI governance itself. The paper includes consideration of the political economic, institutional, legal, safety, and other merits and tradeoffs inherent in the governance system it proposes."
  },
  {
    "title": "The Sword of Damocles in ViTs: Computational Redundancy Amplifies Adversarial Transferability",
    "url": "http://arxiv.org/abs/2504.10804v1",
    "arxiv_id": "2504.10804v1",
    "authors": [
      "Jiani Liu",
      "Zhiyuan Wang",
      "Zeliang Zhang",
      "Chao Huang",
      "Susan Liang",
      "Yunlong Tang",
      "Chenliang Xu"
    ],
    "published": "2025-04-15T01:59:47+00:00",
    "summary": "Vision Transformers (ViTs) have demonstrated impressive performance across a range of applications, including many safety-critical tasks. However, their unique architectural properties raise new challenges and opportunities in adversarial robustness. In particular, we observe that adversarial examples crafted on ViTs exhibit higher transferability compared to those crafted on CNNs, suggesting that ViTs contain structural characteristics favorable for transferable attacks. In this work, we investigate the role of computational redundancy in ViTs and its impact on adversarial transferability. Unlike prior studies that aim to reduce computation for efficiency, we propose to exploit this redundancy to improve the quality and transferability of adversarial examples. Through a detailed analysis, we identify two forms of redundancy, including the data-level and model-level, that can be harnessed to amplify attack effectiveness. Building on this insight, we design a suite of techniques, including attention sparsity manipulation, attention head permutation, clean token regularization, ghost MoE diversification, and test-time adversarial training. Extensive experiments on the ImageNet-1k dataset validate the effectiveness of our approach, showing that our methods significantly outperform existing baselines in both transferability and generality across diverse model architectures."
  },
  {
    "title": "Products of Recursive Programs for Hypersafety Verification",
    "url": "http://arxiv.org/abs/2504.10800v1",
    "arxiv_id": "2504.10800v1",
    "authors": [
      "Ruotong Cheng",
      "Azadeh Farzan"
    ],
    "published": "2025-04-15T01:52:50+00:00",
    "summary": "We study the problem of automated hypersafety verification of infinite-state recursive programs. We propose an infinite class of product programs, specifically designed with recursion in mind, that reduce the hypersafety verification of a recursive program to standard safety verification. For this, we combine insights from language theory and concurrency theory to propose an algorithmic solution for constructing an infinite class of recursive product programs. One key insight is that, using the simple theory of visibly pushdown languages, one can maintain the recursive structure of syntactic program alignments which is vital to constructing a new product program that can be viewed as a classic recursive program -- that is, one that can be executed on a single stack. Another key insight is that techniques from concurrency theory can be generalized to help define product programs based on the view that the parallel composition of individual recursive programs includes all possible alignments from which a sound set of alignments that faithfully preserve the satisfaction of the hypersafety property can be selected. On the practical side, we formulate a family of parametric canonical product constructions that are intuitive to programmers and can be used as building blocks to specify recursive product programs for the purpose of relational and hypersafety verification, with the idea that the right product program can be verified automatically using existing techniques. We demonstrate the effectiveness of these techniques through an implementation and highly promising experimental results."
  },
  {
    "title": "ReasonDrive: Efficient Visual Question Answering for Autonomous Vehicles with Reasoning-Enhanced Small Vision-Language Models",
    "url": "http://arxiv.org/abs/2504.10757v1",
    "arxiv_id": "2504.10757v1",
    "authors": [
      "Amirhosein Chahe",
      "Lifeng Zhou"
    ],
    "published": "2025-04-14T23:16:07+00:00",
    "summary": "Vision-language models (VLMs) show promise for autonomous driving but often lack transparent reasoning capabilities that are critical for safety. We investigate whether explicitly modeling reasoning during fine-tuning enhances VLM performance on driving decision tasks. Using GPT-4o, we generate structured reasoning chains for driving scenarios from the DriveLM benchmark with category-specific prompting strategies. We compare reasoning-based fine-tuning, answer-only fine-tuning, and baseline instruction-tuned models across multiple small VLM families (Llama 3.2, Llava 1.5, and Qwen 2.5VL). Our results demonstrate that reasoning-based fine-tuning consistently outperforms alternatives, with Llama3.2-11B-reason achieving the highest performance. Models fine-tuned with reasoning show substantial improvements in accuracy and text generation quality, suggesting explicit reasoning enhances internal representations for driving decisions. These findings highlight the importance of transparent decision processes in safety-critical domains and offer a promising direction for developing more interpretable autonomous driving systems."
  },
  {
    "title": "The Jailbreak Tax: How Useful are Your Jailbreak Outputs?",
    "url": "http://arxiv.org/abs/2504.10694v1",
    "arxiv_id": "2504.10694v1",
    "authors": [
      "Kristina Nikoli\u0107",
      "Luze Sun",
      "Jie Zhang",
      "Florian Tram\u00e8r"
    ],
    "published": "2025-04-14T20:30:41+00:00",
    "summary": "Jailbreak attacks bypass the guardrails of large language models to produce harmful outputs. In this paper, we ask whether the model outputs produced by existing jailbreaks are actually useful. For example, when jailbreaking a model to give instructions for building a bomb, does the jailbreak yield good instructions? Since the utility of most unsafe answers (e.g., bomb instructions) is hard to evaluate rigorously, we build new jailbreak evaluation sets with known ground truth answers, by aligning models to refuse questions related to benign and easy-to-evaluate topics (e.g., biology or math). Our evaluation of eight representative jailbreaks across five utility benchmarks reveals a consistent drop in model utility in jailbroken responses, which we term the jailbreak tax. For example, while all jailbreaks we tested bypass guardrails in models aligned to refuse to answer math, this comes at the expense of a drop of up to 92% in accuracy. Overall, our work proposes the jailbreak tax as a new important metric in AI safety, and introduces benchmarks to evaluate existing and future jailbreaks. We make the benchmark available at https://github.com/ethz-spylab/jailbreak-tax"
  },
  {
    "title": "Beyond Chains of Thought: Benchmarking Latent-Space Reasoning Abilities in Large Language Models",
    "url": "http://arxiv.org/abs/2504.10615v1",
    "arxiv_id": "2504.10615v1",
    "authors": [
      "Thilo Hagendorff",
      "Sarah Fabi"
    ],
    "published": "2025-04-14T18:15:27+00:00",
    "summary": "Large language models (LLMs) can perform reasoning computations both internally within their latent space and externally by generating explicit token sequences like chains of thought. Significant progress in enhancing reasoning abilities has been made by scaling test-time compute. However, understanding and quantifying model-internal reasoning abilities - the inferential \"leaps\" models make between individual token predictions - remains crucial. This study introduces a benchmark (n = 4,000 items) designed to quantify model-internal reasoning in different domains. We achieve this by having LLMs indicate the correct solution to reasoning problems not through descriptive text, but by selecting a specific language of their initial response token that is different from English, the benchmark language. This not only requires models to reason beyond their context window, but also to overrise their default tendency to respond in the same language as the prompt, thereby posing an additional cognitive strain. We evaluate a set of 18 LLMs, showing significant performance variations, with GPT-4.5 achieving the highest accuracy (74.7%), outperforming models like Grok-2 (67.2%), and Llama 3.1 405B (65.6%). Control experiments and difficulty scaling analyses suggest that while LLMs engage in internal reasoning, we cannot rule out heuristic exploitations under certain conditions, marking an area for future investigation. Our experiments demonstrate that LLMs can \"think\" via latent-space computations, revealing model-internal inference strategies that need further understanding, especially regarding safety-related concerns such as covert planning, goal-seeking, or deception emerging without explicit token traces."
  },
  {
    "title": "Decoupled Diffusion Sparks Adaptive Scene Generation",
    "url": "http://arxiv.org/abs/2504.10485v1",
    "arxiv_id": "2504.10485v1",
    "authors": [
      "Yunsong Zhou",
      "Naisheng Ye",
      "William Ljungbergh",
      "Tianyu Li",
      "Jiazhi Yang",
      "Zetong Yang",
      "Hongzi Zhu",
      "Christoffer Petersson",
      "Hongyang Li"
    ],
    "published": "2025-04-14T17:59:57+00:00",
    "summary": "Controllable scene generation could reduce the cost of diverse data collection substantially for autonomous driving. Prior works formulate the traffic layout generation as predictive progress, either by denoising entire sequences at once or by iteratively predicting the next frame. However, full sequence denoising hinders online reaction, while the latter's short-sighted next-frame prediction lacks precise goal-state guidance. Further, the learned model struggles to generate complex or challenging scenarios due to a large number of safe and ordinal driving behaviors from open datasets. To overcome these, we introduce Nexus, a decoupled scene generation framework that improves reactivity and goal conditioning by simulating both ordinal and challenging scenarios from fine-grained tokens with independent noise states. At the core of the decoupled pipeline is the integration of a partial noise-masking training strategy and a noise-aware schedule that ensures timely environmental updates throughout the denoising process. To complement challenging scenario generation, we collect a dataset consisting of complex corner cases. It covers 540 hours of simulated data, including high-risk interactions such as cut-in, sudden braking, and collision. Nexus achieves superior generation realism while preserving reactivity and goal orientation, with a 40% reduction in displacement error. We further demonstrate that Nexus improves closed-loop planning by 20% through data augmentation and showcase its capability in safety-critical data generation."
  },
  {
    "title": "xVerify: Efficient Answer Verifier for Reasoning Model Evaluations",
    "url": "http://arxiv.org/abs/2504.10481v1",
    "arxiv_id": "2504.10481v1",
    "authors": [
      "Ding Chen",
      "Qingchen Yu",
      "Pengyuan Wang",
      "Wentao Zhang",
      "Bo Tang",
      "Feiyu Xiong",
      "Xinchi Li",
      "Minchuan Yang",
      "Zhiyu Li"
    ],
    "published": "2025-04-14T17:59:36+00:00",
    "summary": "With the release of the o1 model by OpenAI, reasoning models adopting slow thinking strategies have gradually emerged. As the responses generated by such models often include complex reasoning, intermediate steps, and self-reflection, existing evaluation methods are often inadequate. They struggle to determine whether the LLM output is truly equivalent to the reference answer, and also have difficulty identifying and extracting the final answer from long, complex responses. To address this issue, we propose xVerify, an efficient answer verifier for reasoning model evaluations. xVerify demonstrates strong capability in equivalence judgment, enabling it to effectively determine whether the answers produced by reasoning models are equivalent to reference answers across various types of objective questions. To train and evaluate xVerify, we construct the VAR dataset by collecting question-answer pairs generated by multiple LLMs across various datasets, leveraging multiple reasoning models and challenging evaluation sets designed specifically for reasoning model assessment. A multi-round annotation process is employed to ensure label accuracy. Based on the VAR dataset, we train multiple xVerify models of different scales. In evaluation experiments conducted on both the test set and generalization set, all xVerify models achieve overall F1 scores and accuracy exceeding 95\\%. Notably, the smallest variant, xVerify-0.5B-I, outperforms all evaluation methods except GPT-4o, while xVerify-3B-Ib surpasses GPT-4o in overall performance. These results validate the effectiveness and generalizability of xVerify."
  },
  {
    "title": "GUI-R1 : A Generalist R1-Style Vision-Language Action Model For GUI Agents",
    "url": "http://arxiv.org/abs/2504.10458v1",
    "arxiv_id": "2504.10458v1",
    "authors": [
      "Xiaobo Xia",
      "Run Luo"
    ],
    "published": "2025-04-14T17:45:54+00:00",
    "summary": "Existing efforts in building Graphical User Interface (GUI) agents largely rely on the training paradigm of supervised fine-tuning on Large Vision-Language Models (LVLMs). However, this approach not only demands extensive amounts of training data but also struggles to effectively understand GUI screenshots and generalize to unseen interfaces. The issue significantly limits its application in real-world scenarios, especially for high-level tasks. Inspired by Reinforcement Fine-Tuning (RFT) in large reasoning models (e.g., DeepSeek-R1), which efficiently enhances the problem-solving capabilities of large language models in real-world settings, we propose \\name, the first reinforcement learning framework designed to enhance the GUI capabilities of LVLMs in high-level real-world task scenarios, through unified action space rule modeling. By leveraging a small amount of carefully curated high-quality data across multiple platforms (including Windows, Linux, MacOS, Android, and Web) and employing policy optimization algorithms such as Group Relative Policy Optimization (GRPO) to update the model, \\name achieves superior performance using only 0.02\\% of the data (3K vs. 13M) compared to previous state-of-the-art methods like OS-Atlas across eight benchmarks spanning three different platforms (mobile, desktop, and web). These results demonstrate the immense potential of reinforcement learning based on unified action space rule modeling in improving the execution capabilities of LVLMs for real-world GUI agent tasks."
  },
  {
    "title": "GUI-R1 : A Generalist R1-Style Vision-Language Action Model For GUI Agents",
    "url": "http://arxiv.org/abs/2504.10458v2",
    "arxiv_id": "2504.10458v2",
    "authors": [
      "Xiaobo Xia",
      "Run Luo"
    ],
    "published": "2025-04-14T17:45:54+00:00",
    "summary": "Existing efforts in building Graphical User Interface (GUI) agents largely rely on the training paradigm of supervised fine-tuning on Large Vision-Language Models (LVLMs). However, this approach not only demands extensive amounts of training data but also struggles to effectively understand GUI screenshots and generalize to unseen interfaces. The issue significantly limits its application in real-world scenarios, especially for high-level tasks. Inspired by Reinforcement Fine-Tuning (RFT) in large reasoning models (e.g., DeepSeek-R1), which efficiently enhances the problem-solving capabilities of large language models in real-world settings, we propose \\name, the first reinforcement learning framework designed to enhance the GUI capabilities of LVLMs in high-level real-world task scenarios, through unified action space rule modeling. By leveraging a small amount of carefully curated high-quality data across multiple platforms (including Windows, Linux, MacOS, Android, and Web) and employing policy optimization algorithms such as Group Relative Policy Optimization (GRPO) to update the model, \\name achieves superior performance using only 0.02\\% of the data (3K vs. 13M) compared to previous state-of-the-art methods like OS-Atlas across eight benchmarks spanning three different platforms (mobile, desktop, and web). These results demonstrate the immense potential of reinforcement learning based on unified action space rule modeling in improving the execution capabilities of LVLMs for real-world GUI agent tasks."
  },
  {
    "title": "M1: Towards Scalable Test-Time Compute with Mamba Reasoning Models",
    "url": "http://arxiv.org/abs/2504.10449v1",
    "arxiv_id": "2504.10449v1",
    "authors": [
      "Junxiong Wang",
      "Wen-Ding Li",
      "Daniele Paliotta",
      "Daniel Ritter",
      "Alexander M. Rush",
      "Tri Dao"
    ],
    "published": "2025-04-14T17:38:25+00:00",
    "summary": "Effective reasoning is crucial to solving complex mathematical problems. Recent large language models (LLMs) have boosted performance by scaling test-time computation through long chain-of-thought reasoning. However, transformer-based models are inherently limited in extending context length due to their quadratic computational complexity and linear memory requirements. In this paper, we introduce a novel hybrid linear RNN reasoning model, M1, built on the Mamba architecture, which allows memory-efficient inference. Our approach leverages a distillation process from existing reasoning models and is further enhanced through RL training. Experimental results on the AIME and MATH benchmarks show that M1 not only outperforms previous linear RNN models but also matches the performance of state-of-the-art Deepseek R1 distilled reasoning models at a similar scale. We also compare our generation speed with a highly performant general purpose inference engine, vLLM, and observe more than a 3x speedup compared to a same size transformer. With throughput speedup, we are able to achieve higher accuracy compared to DeepSeek R1 distilled transformer reasoning models under a fixed generation time budget using self-consistency voting. Overall, we introduce a hybrid Mamba reasoning model and provide a more effective approach to scaling test-time generation using self-consistency or long chain of thought reasoning."
  },
  {
    "title": "LLM Can be a Dangerous Persuader: Empirical Study of Persuasion Safety in Large Language Models",
    "url": "http://arxiv.org/abs/2504.10430v1",
    "arxiv_id": "2504.10430v1",
    "authors": [
      "Minqian Liu",
      "Zhiyang Xu",
      "Xinyi Zhang",
      "Heajun An",
      "Sarvech Qadir",
      "Qi Zhang",
      "Pamela J. Wisniewski",
      "Jin-Hee Cho",
      "Sang Won Lee",
      "Ruoxi Jia",
      "Lifu Huang"
    ],
    "published": "2025-04-14T17:20:34+00:00",
    "summary": "Recent advancements in Large Language Models (LLMs) have enabled them to approach human-level persuasion capabilities. However, such potential also raises concerns about the safety risks of LLM-driven persuasion, particularly their potential for unethical influence through manipulation, deception, exploitation of vulnerabilities, and many other harmful tactics. In this work, we present a systematic investigation of LLM persuasion safety through two critical aspects: (1) whether LLMs appropriately reject unethical persuasion tasks and avoid unethical strategies during execution, including cases where the initial persuasion goal appears ethically neutral, and (2) how influencing factors like personality traits and external pressures affect their behavior. To this end, we introduce PersuSafety, the first comprehensive framework for the assessment of persuasion safety which consists of three stages, i.e., persuasion scene creation, persuasive conversation simulation, and persuasion safety assessment. PersuSafety covers 6 diverse unethical persuasion topics and 15 common unethical strategies. Through extensive experiments across 8 widely used LLMs, we observe significant safety concerns in most LLMs, including failing to identify harmful persuasion tasks and leveraging various unethical persuasion strategies. Our study calls for more attention to improve safety alignment in progressive and goal-driven conversations such as persuasion."
  },
  {
    "title": "Ctrl-Z: Controlling AI Agents via Resampling",
    "url": "http://arxiv.org/abs/2504.10374v1",
    "arxiv_id": "2504.10374v1",
    "authors": [
      "Aryan Bhatt",
      "Cody Rushing",
      "Adam Kaufman",
      "Tyler Tracy",
      "Vasil Georgiev",
      "David Matolcsi",
      "Akbir Khan",
      "Buck Shlegeris"
    ],
    "published": "2025-04-14T16:22:11+00:00",
    "summary": "Control evaluations measure whether monitoring and security protocols for AI systems prevent intentionally subversive AI models from causing harm. Our work presents the first control evaluation performed in an agent environment. We construct BashBench, a dataset of 257 challenging multi-step system administration tasks, and evaluate whether various safety measures can prevent an adversarially constructed AI agent from covertly downloading and executing malicious code in this environment. This multi-step setting introduces new attack and defense dynamics, which we investigate in order to design novel control protocols that prevent safety failures without hindering the ability of non-malicious agents to perform useful work. We introduce a class of control protocols called resample protocols that dynamically take additional samples of certain actions. We find these protocols significantly improve on existing techniques by selectively blocking the AI agent from executing suspicious code and incriminating the agent by generating additional examples of dangerous behavior. We measure the tradeoff between attack prevention and usefulness; our best protocol combines resampling with analysis of previous steps, reducing the success rate of attacks from 58% to 7% at a 5% cost to the performance of a non-malicious agent."
  },
  {
    "title": "Reactive power flow optimization in AC drive systems",
    "url": "http://arxiv.org/abs/2504.10360v1",
    "arxiv_id": "2504.10360v1",
    "authors": [
      "Sanjay Chandrasekaran",
      "Catalin Arghir",
      "Pieder Joerg",
      "Florian Doerfler",
      "Silvia Mastellone"
    ],
    "published": "2025-04-14T16:08:32+00:00",
    "summary": "This paper explores a limit avoidance approach in the case of input (modulation) and output (current) constraints with the aim of enhancing system availability of AC drives. Drawing on the observation that, in a certain range of reactive power, there exists a trade-off between current and modulation magnitude, we exploit this freedom and define a constrained optimization problem. We propose two approaches, one in the form of an activation-function which drives the reactive power set-point towards safety, and an approach which uses online feedback optimization to set the reactive power dynamically. Both methods compromise reactive power tracking accuracy for increased system robustness. Through a high fidelity simulation, we compare the benefits of the two methods, highlighting their effectiveness in industrial applications."
  },
  {
    "title": "Improving diffusion modeling in all-solid-state lithium batteries: a novel approach for grain boundary effects",
    "url": "http://arxiv.org/abs/2504.10348v1",
    "arxiv_id": "2504.10348v1",
    "authors": [
      "Lena Scholz",
      "Yongliang Ou",
      "Blazej Grabowski",
      "Felix Fritzen"
    ],
    "published": "2025-04-14T15:58:25+00:00",
    "summary": "All-solid-state lithium-ion batteries offer promising advantages with respect to capacity, safety, and performance. The diffusion behavior of lithium ions in the contained polycrystalline solid-state electrolyte is crucial for battery function. While atomistic studies indicate that grain boundaries (GBs) and grain size significantly impact diffusivity, the corresponding effects are either neglected in simulations on larger scales or considered only under strong assumptions such as isotropy. Our approach considers the fully resolved crystalline structure with a parametrization aligned with the atomistic perspective to describe diffusion along and across GBs. The approach is embedded into a finite element simulation using a novel collapsed interface element based on an analytical description in thickness direction. Results are governed by different and potentially anisotropic diffusion coefficients in bulk and GB domains. The mesoscale response is derived using linear computational homogenization to capture large-scale effects. The novel collapsed interface description allows for a reconstruction of the 3D transport behavior within the GB domain without resolving it and is able to capture the relevant transport mechanisms such as channeling effects and concentration jumps. Grain size and GB volume fraction are expressed in terms of an affine parameter dependence and can be altered without any changes to geometry or mesh. Together with the observed dependence of the effective material response on the anisotropic GB parametrization, this leads to the identification of four distinct diffusion regimes, each with implications for the design of battery materials."
  },
  {
    "title": "Heimdall: test-time scaling on the generative verification",
    "url": "http://arxiv.org/abs/2504.10337v1",
    "arxiv_id": "2504.10337v1",
    "authors": [
      "Wenlei Shi",
      "Xing Jin"
    ],
    "published": "2025-04-14T15:46:33+00:00",
    "summary": "An AI system can create and maintain knowledge only to the extent that it can verify that knowledge itself. Recent work on long Chain-of-Thought reasoning has demonstrated great potential of LLMs on solving competitive problems, but their verification ability remains to be weak and not sufficiently investigated. In this paper, we propose Heimdall, the long CoT verification LLM that can accurately judge the correctness of solutions. With pure reinforcement learning, we boost the verification accuracy from 62.5% to 94.5% on competitive math problems. By scaling with repeated sampling, the accuracy further increases to 97.5%. Through human evaluation, Heimdall demonstrates impressive generalization capabilities, successfully detecting most issues in challenging math proofs, the type of which is not included during training. Furthermore, we propose Pessimistic Verification to extend the functionality of Heimdall to scaling up the problem solving. It calls Heimdall to judge the solutions from a solver model and based on the pessimistic principle, selects the most likely correct solution with the least uncertainty. Taking DeepSeek-R1-Distill-Qwen-32B as the solver model, Pessimistic Verification improves the solution accuracy on AIME2025 from 54.2% to 70.0% with 16x compute budget and to 83.3% with more compute budget. With the stronger solver Gemini 2.5 Pro, the score reaches 93.0%. Finally, we prototype an automatic knowledge discovery system, a ternary system where one poses questions, another provides solutions, and the third verifies the solutions. Using the data synthesis work NuminaMath for the first two components, Heimdall effectively identifies problematic records within the dataset and reveals that nearly half of the data is flawed, which interestingly aligns with the recent ablation studies from NuminaMath."
  },
  {
    "title": "Heimdall: test-time scaling on the generative verification",
    "url": "http://arxiv.org/abs/2504.10337v2",
    "arxiv_id": "2504.10337v2",
    "authors": [
      "Wenlei Shi",
      "Xing Jin"
    ],
    "published": "2025-04-14T15:46:33+00:00",
    "summary": "An AI system can create and maintain knowledge only to the extent that it can verify that knowledge itself. Recent work on long Chain-of-Thought reasoning has demonstrated great potential of LLMs on solving competitive problems, but their verification ability remains to be weak and not sufficiently investigated. In this paper, we propose Heimdall, the long CoT verification LLM that can accurately judge the correctness of solutions. With pure reinforcement learning, we boost the verification accuracy from 62.5% to 94.5% on competitive math problems. By scaling with repeated sampling, the accuracy further increases to 97.5%. Through human evaluation, Heimdall demonstrates impressive generalization capabilities, successfully detecting most issues in challenging math proofs, the type of which is not included during training. Furthermore, we propose Pessimistic Verification to extend the functionality of Heimdall to scaling up the problem solving. It calls Heimdall to judge the solutions from a solver model and based on the pessimistic principle, selects the most likely correct solution with the least uncertainty. Taking DeepSeek-R1-Distill-Qwen-32B as the solver model, Pessimistic Verification improves the solution accuracy on AIME2025 from 54.2% to 70.0% with 16x compute budget and to 83.3% with more compute budget. With the stronger solver Gemini 2.5 Pro, the score reaches 93.0%. Finally, we prototype an automatic knowledge discovery system, a ternary system where one poses questions, another provides solutions, and the third verifies the solutions. Using the data synthesis work NuminaMath for the first two components, Heimdall effectively identifies problematic records within the dataset and reveals that nearly half of the data is flawed, which interestingly aligns with the recent ablation studies from NuminaMath."
  },
  {
    "title": "Cumulative-Time Signal Temporal Logic",
    "url": "http://arxiv.org/abs/2504.10325v1",
    "arxiv_id": "2504.10325v1",
    "authors": [
      "Hongkai Chen",
      "Zeyu Zhang",
      "Shouvik Roy",
      "Ezio Bartocci",
      "Scott A. Smolka",
      "Scott D. Stoller",
      "Shan Lin"
    ],
    "published": "2025-04-14T15:34:13+00:00",
    "summary": "Signal Temporal Logic (STL) is a widely adopted specification language in cyber-physical systems for expressing critical temporal requirements, such as safety conditions and response time. However, STL's expressivity is not sufficient to capture the cumulative duration during which a property holds within an interval of time. To overcome this limitation, we introduce Cumulative-Time Signal Temporal Logic (CT-STL) that operates over discrete-time signals and extends STL with a new cumulative-time operator. This operator compares the sum of all time steps for which its nested formula is true with a threshold. We present both a qualitative and a quantitative (robustness) semantics for CT-STL and prove both their soundness and completeness properties. We provide an efficient online monitoring algorithm for both semantics. Finally, we show the applicability of CT-STL in two case studies: specifying and monitoring cumulative temporal requirements for a microgrid and an artificial pancreas."
  },
  {
    "title": "SlowFastVAD: Video Anomaly Detection via Integrating Simple Detector and RAG-Enhanced Vision-Language Model",
    "url": "http://arxiv.org/abs/2504.10320v1",
    "arxiv_id": "2504.10320v1",
    "authors": [
      "Zongcan Ding",
      "Haodong Zhang",
      "Peng Wu",
      "Guansong Pang",
      "Zhiwei Yang",
      "Peng Wang",
      "Yanning Zhang"
    ],
    "published": "2025-04-14T15:30:03+00:00",
    "summary": "Video anomaly detection (VAD) aims to identify unexpected events in videos and has wide applications in safety-critical domains. While semi-supervised methods trained on only normal samples have gained traction, they often suffer from high false alarm rates and poor interpretability. Recently, vision-language models (VLMs) have demonstrated strong multimodal reasoning capabilities, offering new opportunities for explainable anomaly detection. However, their high computational cost and lack of domain adaptation hinder real-time deployment and reliability. Inspired by dual complementary pathways in human visual perception, we propose SlowFastVAD, a hybrid framework that integrates a fast anomaly detector with a slow anomaly detector (namely a retrieval augmented generation (RAG) enhanced VLM), to address these limitations. Specifically, the fast detector first provides coarse anomaly confidence scores, and only a small subset of ambiguous segments, rather than the entire video, is further analyzed by the slower yet more interpretable VLM for elaborate detection and reasoning. Furthermore, to adapt VLMs to domain-specific VAD scenarios, we construct a knowledge base including normal patterns based on few normal samples and abnormal patterns inferred by VLMs. During inference, relevant patterns are retrieved and used to augment prompts for anomaly reasoning. Finally, we smoothly fuse the anomaly confidence of fast and slow detectors to enhance robustness of anomaly detection. Extensive experiments on four benchmarks demonstrate that SlowFastVAD effectively combines the strengths of both fast and slow detectors, and achieves remarkable detection accuracy and interpretability with significantly reduced computational overhead, making it well-suited for real-world VAD applications with high reliability requirements."
  },
  {
    "title": "Siamese Network with Dual Attention for EEG-Driven Social Learning: Bridging the Human-Robot Gap in Long-Tail Autonomous Driving",
    "url": "http://arxiv.org/abs/2504.10296v1",
    "arxiv_id": "2504.10296v1",
    "authors": [
      "Xiaoshan Zhou",
      "Carol C. Menassa",
      "Vineet R. Kamat"
    ],
    "published": "2025-04-14T15:06:17+00:00",
    "summary": "Robots with wheeled, quadrupedal, or humanoid forms are increasingly integrated into built environments. However, unlike human social learning, they lack a critical pathway for intrinsic cognitive development, namely, learning from human feedback during interaction. To understand human ubiquitous observation, supervision, and shared control in dynamic and uncertain environments, this study presents a brain-computer interface (BCI) framework that enables classification of Electroencephalogram (EEG) signals to detect cognitively demanding and safety-critical events. As a timely and motivating co-robotic engineering application, we simulate a human-in-the-loop scenario to flag risky events in semi-autonomous robotic driving-representative of long-tail cases that pose persistent bottlenecks to the safety performance of smart mobility systems and robotic vehicles. Drawing on recent advances in few-shot learning, we propose a dual-attention Siamese convolutional network paired with Dynamic Time Warping Barycenter Averaging approach to generate robust EEG-encoded signal representations. Inverse source localization reveals activation in Broadman areas 4 and 9, indicating perception-action coupling during task-relevant mental imagery. The model achieves 80% classification accuracy under data-scarce conditions and exhibits a nearly 100% increase in the utility of salient features compared to state-of-the-art methods, as measured through integrated gradient attribution. Beyond performance, this study contributes to our understanding of the cognitive architecture required for BCI agents-particularly the role of attention and memory mechanisms-in categorizing diverse mental states and supporting both inter- and intra-subject adaptation. Overall, this research advances the development of cognitive robotics and socially guided learning for service robots in complex built environments."
  },
  {
    "title": "Gravity-induced emergence of the Fermi scale in quantum quadratic gravity",
    "url": "http://arxiv.org/abs/2504.10293v1",
    "arxiv_id": "2504.10293v1",
    "authors": [
      "Mohammad Mehrafarin"
    ],
    "published": "2025-04-14T15:04:47+00:00",
    "summary": "In the framework of asymptotic safety, we study quantum quadratic gravity in the presence of the Higgs field considered as non-separable from the vacuum. The theory flows to a high energy fixed point where the Higgs field is strongly coupled to gravity, its potential is symmetric, and the quadratic Weyl curvature coupling is large. The latter renders the ghost graviton an unstable high mass resonance which renders unitarity in the spirit of Lee-Week type theories. Furthermore, if the scalar graviton is tachyonic then there will be a low energy fixed point where tachyonic condensation leads to a new stable vacuum. At this fixed point the symmetry breaks and the Fermi scale emerges, and the behavior of the Higgs field is classical (not influenced by gravitational interaction). Gravity at the UV scale is purely quadratic whereas at the Fermi scale it is linear, and in the intermediate region both contributions are relevant. Thus, at the Fermi scale the quadratic curvature fields disappear through the ghost instability and tachyon condensation, giving rise to Einstein gravity and the electroweak phase transition."
  },
  {
    "title": "Deep Reasoning Translation via Reinforcement Learning",
    "url": "http://arxiv.org/abs/2504.10187v1",
    "arxiv_id": "2504.10187v1",
    "authors": [
      "Jiaan Wang",
      "Fandong Meng",
      "Jie Zhou"
    ],
    "published": "2025-04-14T12:40:39+00:00",
    "summary": "Recently, deep reasoning LLMs (e.g., OpenAI o1/o3 and DeepSeek-R1) have shown promising performance in various complex tasks. Free translation is an important and interesting task in the multilingual world, which requires going beyond word-for-word translation and taking cultural differences into account. This task is still under-explored in deep reasoning LLMs. In this paper, we introduce DeepTrans, a deep reasoning translation model that learns free translation via reinforcement learning. Specifically, we carefully build a reward model with pre-defined scoring criteria on both the translation results and the thought process. Given the source sentences, the reward model teaches the deep translation model how to think and free-translate them during reinforcement learning. In this way, training DeepTrans does not need any labeled translations, avoiding the human-intensive annotation or resource-intensive data synthesis. Experimental results show the effectiveness of DeepTrans. Using Qwen2.5-7B as the backbone, DeepTrans improves performance by 16.3% in literature translation, and outperforms strong deep reasoning baselines as well as baselines that are fine-tuned with synthesized data. Moreover, we summarize the failures and interesting findings during our RL exploration. We hope this work could inspire other researchers in free translation."
  },
  {
    "title": "LLM Unlearning Reveals a Stronger-Than-Expected Coreset Effect in Current Benchmarks",
    "url": "http://arxiv.org/abs/2504.10185v1",
    "arxiv_id": "2504.10185v1",
    "authors": [
      "Soumyadeep Pal",
      "Changsheng Wang",
      "James Diffenderfer",
      "Bhavya Kailkhura",
      "Sijia Liu"
    ],
    "published": "2025-04-14T12:38:37+00:00",
    "summary": "Large language model unlearning has become a critical challenge in ensuring safety and controlled model behavior by removing undesired data-model influences from the pretrained model while preserving general utility. Significant recent efforts have been dedicated to developing LLM unlearning benchmarks such as WMDP (Weapons of Mass Destruction Proxy) and MUSE (Machine Unlearning Six-way Evaluation), facilitating standardized unlearning performance assessment and method comparison. Despite their usefulness, we uncover for the first time a novel coreset effect within these benchmarks. Specifically, we find that LLM unlearning achieved with the original (full) forget set can be effectively maintained using a significantly smaller subset (functioning as a \"coreset\"), e.g., as little as 5% of the forget set, even when selected at random. This suggests that LLM unlearning in these benchmarks can be performed surprisingly easily, even in an extremely low-data regime. We demonstrate that this coreset effect remains strong, regardless of the LLM unlearning method used, such as NPO (Negative Preference Optimization) and RMU (Representation Misdirection Unlearning), the popular ones in these benchmarks. The surprisingly strong coreset effect is also robust across various data selection methods, ranging from random selection to more sophisticated heuristic approaches. We explain the coreset effect in LLM unlearning through a keyword-based perspective, showing that keywords extracted from the forget set alone contribute significantly to unlearning effectiveness and indicating that current unlearning is driven by a compact set of high-impact tokens rather than the entire dataset. We further justify the faithfulness of coreset-unlearned models along additional dimensions, such as mode connectivity and robustness to jailbreaking attacks. Codes are available at https://github.com/OPTML-Group/MU-Coreset."
  },
  {
    "title": "LLM Unlearning Reveals a Stronger-Than-Expected Coreset Effect in Current Benchmarks",
    "url": "http://arxiv.org/abs/2504.10185v2",
    "arxiv_id": "2504.10185v2",
    "authors": [
      "Soumyadeep Pal",
      "Changsheng Wang",
      "James Diffenderfer",
      "Bhavya Kailkhura",
      "Sijia Liu"
    ],
    "published": "2025-04-14T12:38:37+00:00",
    "summary": "Large language model unlearning has become a critical challenge in ensuring safety and controlled model behavior by removing undesired data-model influences from the pretrained model while preserving general utility. Significant recent efforts have been dedicated to developing LLM unlearning benchmarks such as WMDP (Weapons of Mass Destruction Proxy) and MUSE (Machine Unlearning Six-way Evaluation), facilitating standardized unlearning performance assessment and method comparison. Despite their usefulness, we uncover for the first time a novel coreset effect within these benchmarks. Specifically, we find that LLM unlearning achieved with the original (full) forget set can be effectively maintained using a significantly smaller subset (functioning as a \"coreset\"), e.g., as little as 5% of the forget set, even when selected at random. This suggests that LLM unlearning in these benchmarks can be performed surprisingly easily, even in an extremely low-data regime. We demonstrate that this coreset effect remains strong, regardless of the LLM unlearning method used, such as NPO (Negative Preference Optimization) and RMU (Representation Misdirection Unlearning), the popular ones in these benchmarks. The surprisingly strong coreset effect is also robust across various data selection methods, ranging from random selection to more sophisticated heuristic approaches. We explain the coreset effect in LLM unlearning through a keyword-based perspective, showing that keywords extracted from the forget set alone contribute significantly to unlearning effectiveness and indicating that current unlearning is driven by a compact set of high-impact tokens rather than the entire dataset. We further justify the faithfulness of coreset-unlearned models along additional dimensions, such as mode connectivity and robustness to jailbreaking attacks. Codes are available at https://github.com/OPTML-Group/MU-Coreset."
  },
  {
    "title": "Challenges in interpretability of additive models",
    "url": "http://arxiv.org/abs/2504.10169v1",
    "arxiv_id": "2504.10169v1",
    "authors": [
      "Xinyu Zhang",
      "Julien Martinelli",
      "ST John"
    ],
    "published": "2025-04-14T12:24:17+00:00",
    "summary": "We review generalized additive models as a type of ``transparent'' model that has recently seen renewed interest in the deep learning community as neural additive models. We highlight multiple types of nonidentifiability in this model class and discuss challenges in interpretability, arguing for restraint when claiming ``interpretability'' or ``suitability for safety-critical applications'' of such models."
  },
  {
    "title": "GeoUni: A Unified Model for Generating Geometry Diagrams, Problems and Problem Solutions",
    "url": "http://arxiv.org/abs/2504.10146v1",
    "arxiv_id": "2504.10146v1",
    "authors": [
      "Jo-Ku Cheng",
      "Zeren Zhang",
      "Ran Chen",
      "Jingyang Deng",
      "Ziran Qin",
      "Jinwen Ma"
    ],
    "published": "2025-04-14T11:56:55+00:00",
    "summary": "We propose GeoUni, the first unified geometry expert model capable of generating problem solutions and diagrams within a single framework in a way that enables the creation of unique and individualized geometry problems. Traditionally, solving geometry problems and generating diagrams have been treated as separate tasks in machine learning, with no models successfully integrating both to support problem creation. However, we believe that mastery in geometry requires frictionless integration of all of these skills, from solving problems to visualizing geometric relationships, and finally, crafting tailored problems. Our extensive experiments demonstrate that GeoUni, with only 1.5B parameters, achieves performance comparable to larger models such as DeepSeek-R1 with 671B parameters in geometric reasoning tasks. GeoUni also excels in generating precise geometric diagrams, surpassing both text-to-image models and unified models, including the GPT-4o image generation. Most importantly, GeoUni is the only model capable of successfully generating textual problems with matching diagrams based on specific knowledge points, thus offering a wider range of capabilities that extend beyond current models."
  },
  {
    "title": "M2S-RoAD: Multi-Modal Semantic Segmentation for Road Damage Using Camera and LiDAR Data",
    "url": "http://arxiv.org/abs/2504.10123v1",
    "arxiv_id": "2504.10123v1",
    "authors": [
      "Tzu-Yun Tseng",
      "Hongyu Lyu",
      "Josephine Li",
      "Julie Stephany Berrio",
      "Mao Shan",
      "Stewart Worrall"
    ],
    "published": "2025-04-14T11:32:01+00:00",
    "summary": "Road damage can create safety and comfort challenges for both human drivers and autonomous vehicles (AVs). This damage is particularly prevalent in rural areas due to less frequent surveying and maintenance of roads. Automated detection of pavement deterioration can be used as an input to AVs and driver assistance systems to improve road safety. Current research in this field has predominantly focused on urban environments driven largely by public datasets, while rural areas have received significantly less attention. This paper introduces M2S-RoAD, a dataset for the semantic segmentation of different classes of road damage. M2S-RoAD was collected in various towns across New South Wales, Australia, and labelled for semantic segmentation to identify nine distinct types of road damage. This dataset will be released upon the acceptance of the paper."
  },
  {
    "title": "CameraBench: Benchmarking Visual Reasoning in MLLMs via Photography",
    "url": "http://arxiv.org/abs/2504.10090v1",
    "arxiv_id": "2504.10090v1",
    "authors": [
      "I-Sheng Fang",
      "Jun-Cheng Chen"
    ],
    "published": "2025-04-14T10:53:44+00:00",
    "summary": "Large language models (LLMs) and multimodal large language models (MLLMs) have significantly advanced artificial intelligence. However, visual reasoning, reasoning involving both visual and textual inputs, remains underexplored. Recent advancements, including the reasoning models like OpenAI o1 and Gemini 2.0 Flash Thinking, which incorporate image inputs, have opened this capability. In this ongoing work, we focus specifically on photography-related tasks because a photo is a visual snapshot of the physical world where the underlying physics (i.e., illumination, blur extent, etc.) interplay with the camera parameters. Successfully reasoning from the visual information of a photo to identify these numerical camera settings requires the MLLMs to have a deeper understanding of the underlying physics for precise visual comprehension, representing a challenging and intelligent capability essential for practical applications like photography assistant agents. We aim to evaluate MLLMs on their ability to distinguish visual differences related to numerical camera settings, extending a methodology previously proposed for vision-language models (VLMs). Our preliminary results demonstrate the importance of visual reasoning in photography-related tasks. Moreover, these results show that no single MLLM consistently dominates across all evaluation tasks, demonstrating ongoing challenges and opportunities in developing MLLMs with better visual reasoning."
  },
  {
    "title": "RealSafe-R1: Safety-Aligned DeepSeek-R1 without Compromising Reasoning Capability",
    "url": "http://arxiv.org/abs/2504.10081v1",
    "arxiv_id": "2504.10081v1",
    "authors": [
      "Yichi Zhang",
      "Zihao Zeng",
      "Dongbai Li",
      "Yao Huang",
      "Zhijie Deng",
      "Yinpeng Dong"
    ],
    "published": "2025-04-14T10:26:37+00:00",
    "summary": "Large Reasoning Models (LRMs), such as OpenAI o1 and DeepSeek-R1, have been rapidly progressing and achieving breakthrough performance on complex reasoning tasks such as mathematics and coding. However, the open-source R1 models have raised safety concerns in wide applications, such as the tendency to comply with malicious queries, which greatly impacts the utility of these powerful models in their applications. In this paper, we introduce RealSafe-R1 as safety-aligned versions of DeepSeek-R1 distilled models. To train these models, we construct a dataset of 15k safety-aware reasoning trajectories generated by DeepSeek-R1, under explicit instructions for expected refusal behavior. Both quantitative experiments and qualitative case studies demonstrate the models' improvements, which are shown in their safety guardrails against both harmful queries and jailbreak attacks. Importantly, unlike prior safety alignment efforts that often compromise reasoning performance, our method preserves the models' reasoning capabilities by maintaining the training data within the original distribution of generation. Model weights of RealSafe-R1 are open-source at https://huggingface.co/RealSafe."
  },
  {
    "title": "Balancing Two Classifiers via A Simplex ETF Structure for Model Calibration",
    "url": "http://arxiv.org/abs/2504.10007v1",
    "arxiv_id": "2504.10007v1",
    "authors": [
      "Jiani Ni",
      "He Zhao",
      "Jintong Gao",
      "Dandan Guo",
      "Hongyuan Zha"
    ],
    "published": "2025-04-14T09:09:01+00:00",
    "summary": "In recent years, deep neural networks (DNNs) have demonstrated state-of-the-art performance across various domains. However, despite their success, they often face calibration issues, particularly in safety-critical applications such as autonomous driving and healthcare, where unreliable predictions can have serious consequences. Recent research has started to improve model calibration from the view of the classifier. However, the exploration of designing the classifier to solve the model calibration problem is insufficient. Let alone most of the existing methods ignore the calibration errors arising from underconfidence. In this work, we propose a novel method by balancing learnable and ETF classifiers to solve the overconfidence or underconfidence problem for model Calibration named BalCAL. By introducing a confidence-tunable module and a dynamic adjustment method, we ensure better alignment between model confidence and its true accuracy. Extensive experimental validation shows that ours significantly improves model calibration performance while maintaining high predictive accuracy, outperforming existing techniques. This provides a novel solution to the calibration challenges commonly encountered in deep learning."
  },
  {
    "title": "Do We Really Need Curated Malicious Data for Safety Alignment in Multi-modal Large Language Models?",
    "url": "http://arxiv.org/abs/2504.10000v1",
    "arxiv_id": "2504.10000v1",
    "authors": [
      "Yanbo Wang",
      "Jiyang Guan",
      "Jian Liang",
      "Ran He"
    ],
    "published": "2025-04-14T09:03:51+00:00",
    "summary": "Multi-modal large language models (MLLMs) have made significant progress, yet their safety alignment remains limited. Typically, current open-source MLLMs rely on the alignment inherited from their language module to avoid harmful generations. However, the lack of safety measures specifically designed for multi-modal inputs creates an alignment gap, leaving MLLMs vulnerable to vision-domain attacks such as typographic manipulation. Current methods utilize a carefully designed safety dataset to enhance model defense capability, while the specific knowledge or patterns acquired from the high-quality dataset remain unclear. Through comparison experiments, we find that the alignment gap primarily arises from data distribution biases, while image content, response quality, or the contrastive behavior of the dataset makes little contribution to boosting multi-modal safety. To further investigate this and identify the key factors in improving MLLM safety, we propose finetuning MLLMs on a small set of benign instruct-following data with responses replaced by simple, clear rejection sentences. Experiments show that, without the need for labor-intensive collection of high-quality malicious data, model safety can still be significantly improved, as long as a specific fraction of rejection data exists in the finetuning set, indicating the security alignment is not lost but rather obscured during multi-modal pretraining or instruction finetuning. Simply correcting the underlying data bias could narrow the safety gap in the vision domain."
  },
  {
    "title": "Assessing Judging Bias in Large Reasoning Models: An Empirical Study",
    "url": "http://arxiv.org/abs/2504.09946v1",
    "arxiv_id": "2504.09946v1",
    "authors": [
      "Qian Wang",
      "Zhanzhi Lou",
      "Zhenheng Tang",
      "Nuo Chen",
      "Xuandong Zhao",
      "Wenxuan Zhang",
      "Dawn Song",
      "Bingsheng He"
    ],
    "published": "2025-04-14T07:14:27+00:00",
    "summary": "Large Reasoning Models (LRMs) like DeepSeek-R1 and OpenAI-o1 have demonstrated remarkable reasoning capabilities, raising important questions about their biases in LLM-as-a-judge settings. We present a comprehensive benchmark comparing judging biases between LLMs and LRMs across both subjective preference-alignment datasets and objective fact-based datasets. Through investigation of bandwagon, authority, position, and distraction biases, we uncover four key findings: (1) despite their advanced reasoning capabilities, LRMs remain susceptible to the above biases; (2) LRMs demonstrate better robustness than LLMs specifically on fact-related datasets; (3) LRMs exhibit notable position bias, preferring options in later positions; and (4) we identify a novel \"superficial reflection bias\" where phrases mimicking reasoning (e.g., \"wait, let me think...\") significantly influence model judgments. To address these biases, we design and evaluate three mitigation strategies: specialized system prompts that reduce judging biases by up to 19\\% in preference alignment datasets and 14\\% in fact-related datasets, in-context learning that provides up to 27\\% improvement on preference tasks but shows inconsistent results on factual tasks, and a self-reflection mechanism that reduces biases by up to 10\\% in preference datasets and 16\\% in fact-related datasets, with self-reflection proving particularly effective for LRMs. Our work provides crucial insights for developing more reliable LLM-as-a-Judge frameworks, especially as LRMs become increasingly deployed as automated judges."
  },
  {
    "title": "Tight Semidefinite Relaxations for Verifying Robustness of Neural Networks",
    "url": "http://arxiv.org/abs/2504.09934v1",
    "arxiv_id": "2504.09934v1",
    "authors": [
      "Godai Azuma",
      "Sunyoung Kim",
      "Makoto Yamashita"
    ],
    "published": "2025-04-14T06:54:00+00:00",
    "summary": "For verifying the safety of neural networks (NNs), Fazlyab et al. (2019) introduced a semidefinite programming (SDP) approach called DeepSDP. This formulation can be viewed as the dual of the SDP relaxation for a problem formulated as a quadratically constrained quadratic program (QCQP). While SDP relaxations of QCQPs generally provide approximate solutions with some gaps, this work focuses on tight SDP relaxations that provide exact solutions to the QCQP for single-layer NNs. Specifically, we analyze tightness conditions in three cases: (i) NNs with a single neuron, (ii) single-layer NNs with an ellipsoidal input set, and (iii) single-layer NNs with a rectangular input set. For NNs with a single neuron, we propose a condition that ensures the SDP admits a rank-1 solution to DeepSDP by transforming the QCQP into an equivalent two-stage problem leads to a solution collinear with a predetermined vector. For single-layer NNs with an ellipsoidal input set, the collinearity of solutions is proved via the Karush-Kuhn-Tucker condition in the two-stage problem. In case of single-layer NNs with a rectangular input set, we demonstrate that the tightness of DeepSDP can be reduced to the single-neuron NNs, case (i), if the weight matrix is a diagonal matrix."
  },
  {
    "title": "Learning from Reference Answers: Versatile Language Model Alignment without Binary Human Preference Data",
    "url": "http://arxiv.org/abs/2504.09895v1",
    "arxiv_id": "2504.09895v1",
    "authors": [
      "Shuai Zhao",
      "Linchao Zhu",
      "Yi Yang"
    ],
    "published": "2025-04-14T05:43:21+00:00",
    "summary": "Large language models~(LLMs) are expected to be helpful, harmless, and honest. In various alignment scenarios, such as general human preference, safety, and confidence alignment, binary preference data collection and reward modeling are resource-intensive but necessary for human preference transferring. In this work, we explore using the similarity between sampled generations and high-quality reference answers as an alternative reward function for LLM alignment. Using similarity as a reward circumvents training reward models, and collecting a single reference answer potentially costs less time than constructing binary preference pairs when multiple candidates are available. Specifically, we develop \\textit{RefAlign}, a versatile REINFORCE-style alignment algorithm, which is free of reference and reward models. Instead, RefAlign utilizes BERTScore between sampled generations and high-quality reference answers as the surrogate reward. Beyond general human preference optimization, RefAlign can be readily extended to diverse scenarios, such as safety and confidence alignment, by incorporating the similarity reward with task-related objectives. In various scenarios, {RefAlign} demonstrates comparable performance to previous alignment methods while offering high efficiency."
  },
  {
    "title": "Reasoning Models Can Be Effective Without Thinking",
    "url": "http://arxiv.org/abs/2504.09858v1",
    "arxiv_id": "2504.09858v1",
    "authors": [
      "Wenjie Ma",
      "Jingxuan He",
      "Charlie Snell",
      "Tyler Griggs",
      "Sewon Min",
      "Matei Zaharia"
    ],
    "published": "2025-04-14T04:08:16+00:00",
    "summary": "Recent LLMs have significantly improved reasoning capabilities, primarily by including an explicit, lengthy Thinking process as part of generation. In this paper, we question whether this explicit thinking is necessary. Using the state-of-the-art DeepSeek-R1-Distill-Qwen, we find that bypassing the thinking process via simple prompting, denoted as NoThinking, can be surprisingly effective. When controlling for the number of tokens, NoThinking outperforms Thinking across a diverse set of seven challenging reasoning datasets--including mathematical problem solving, formal theorem proving, and coding--especially in low-budget settings, e.g., 51.3 vs. 28.9 on ACM 23 with 700 tokens. Notably, the performance of NoThinking becomes more competitive with pass@k as k increases. Building on this observation, we demonstrate that a parallel scaling approach that uses NoThinking to generate N outputs independently and aggregates them is highly effective. For aggregation, we use task-specific verifiers when available, or we apply simple best-of-N strategies such as confidence-based selection. Our method outperforms a range of baselines with similar latency using Thinking, and is comparable to Thinking with significantly longer latency (up to 9x). Together, our research encourages a reconsideration of the necessity of lengthy thinking processes, while also establishing a competitive reference for achieving strong reasoning performance in low-budget settings or at low latency using parallel scaling."
  },
  {
    "title": "Training Small Reasoning LLMs with Cognitive Preference Alignment",
    "url": "http://arxiv.org/abs/2504.09802v1",
    "arxiv_id": "2504.09802v1",
    "authors": [
      "Wenrui Cai",
      "Chengyu Wang",
      "Junbing Yan",
      "Jun Huang",
      "Xiangzhong Fang"
    ],
    "published": "2025-04-14T02:03:54+00:00",
    "summary": "The reasoning capabilities of large language models (LLMs), such as OpenAI's o1 and DeepSeek-R1, have seen substantial advancements through deep thinking. However, these enhancements come with significant resource demands, underscoring the need to explore strategies to train effective reasoning LLMs with far fewer parameters. A critical challenge is that smaller models have different capacities and cognitive trajectories than their larger counterparts. Hence, direct distillation of chain-of-thought (CoT) results from large LLMs to smaller ones can be sometimes ineffective and requires a huge amount of annotated data. In this paper, we introduce a novel framework called Critique-Rethink-Verify (CRV), designed for training smaller yet powerful reasoning LLMs. Our CRV framework consists of multiple LLM agents, each specializing in unique abilities: (i) critiquing the CoTs according to the cognitive capabilities of smaller models, (ii) rethinking and refining these CoTs based on the critiques, and (iii) verifying the correctness of the refined results. We further propose the cognitive preference optimization (CogPO) algorithm to enhance the reasoning abilities of smaller models by aligning thoughts of these models with their cognitive capacities. Comprehensive evaluations on challenging reasoning benchmarks demonstrate the efficacy of CRV and CogPO, which outperforms other training methods by a large margin."
  },
  {
    "title": "Reasoning without Regret",
    "url": "http://arxiv.org/abs/2504.09777v1",
    "arxiv_id": "2504.09777v1",
    "authors": [
      "Tarun Chitra"
    ],
    "published": "2025-04-14T00:34:20+00:00",
    "summary": "Chain-of-thought reasoning enables large language models to solve multi-step tasks by framing problem solving as sequential decision problems. Outcome-based rewards, which provide feedback only on final answers, show impressive success, but face challenges with credit assignment and slow convergence. In contrast, procedure-based rewards offer efficient step-level feedback, but typically require costly human supervision. We introduce \\emph{Backwards Adaptive Reward Shaping} (BARS), a no-regret framework that converts sparse outcomes-based rewards into effective procedure-based signals. BARS uses sparse rewards generated from terminal-state priors and cover trees to scale rewards while preventing exploitation. With Bellman contraction and $(\\Delta, \\epsilon)$-gap rewards, our backward Euler solver achieves $\\epsilon$-accuracy in $O\\left((R_{\\max}/\\Delta)\\log(1/\\epsilon)\\right)$ iterations with $O(\\log T)$ dynamic regret over $T$ rounds. Our analysis, based on generic chaining, continuous scaling limits, and non-linear Feynman-Kac bounds, connects recent outcome-based methods' empirical successes with the benefits of intermediate supervision. Combined, this provides the first rigorous no-regret algorithm for outcome reward shaping, providing a theoretical foundation for the empirical success of DeepSeek's R1."
  },
  {
    "title": "Two Heads are Better Than One: Test-time Scaling of Multi-agent Collaborative Reasoning",
    "url": "http://arxiv.org/abs/2504.09772v1",
    "arxiv_id": "2504.09772v1",
    "authors": [
      "Can Jin",
      "Hongwu Peng",
      "Qixin Zhang",
      "Yujin Tang",
      "Dimitris N. Metaxas",
      "Tong Che"
    ],
    "published": "2025-04-14T00:27:45+00:00",
    "summary": "Multi-agent systems (MAS) built on large language models (LLMs) offer a promising path toward solving complex, real-world tasks that single-agent systems often struggle to manage. While recent advancements in test-time scaling (TTS) have significantly improved single-agent performance on challenging reasoning tasks, how to effectively scale collaboration and reasoning in MAS remains an open question. In this work, we introduce an adaptive multi-agent framework designed to enhance collaborative reasoning through both model-level training and system-level coordination. We construct M500, a high-quality dataset containing 500 multi-agent collaborative reasoning traces, and fine-tune Qwen2.5-32B-Instruct on this dataset to produce M1-32B, a model optimized for multi-agent collaboration. To further enable adaptive reasoning, we propose a novel CEO agent that dynamically manages the discussion process, guiding agent collaboration and adjusting reasoning depth for more effective problem-solving. Evaluated in an open-source MAS across a range of tasks-including general understanding, mathematical reasoning, and coding-our system significantly outperforms strong baselines. For instance, M1-32B achieves 12% improvement on GPQA-Diamond, 41% on AIME2024, and 10% on MBPP-Sanitized, matching the performance of state-of-the-art models like DeepSeek-R1 on some tasks. These results highlight the importance of both learned collaboration and adaptive coordination in scaling multi-agent reasoning. Code is available at https://github.com/jincan333/MAS-TTS"
  },
  {
    "title": "(How) Do reasoning models reason?",
    "url": "http://arxiv.org/abs/2504.09762v1",
    "arxiv_id": "2504.09762v1",
    "authors": [
      "Subbarao Kambhampati",
      "Kaya Stechly",
      "Karthik Valmeekam"
    ],
    "published": "2025-04-14T00:03:34+00:00",
    "summary": "We will provide a broad unifying perspective on the recent breed of Large Reasoning Models (LRMs) such as OpenAI o1 and DeepSeek R1, including their promise, sources of power, misconceptions and limitations."
  },
  {
    "title": "Hybrid Lyapunov and Barrier Function-Based Control with Stabilization Guarantees",
    "url": "http://arxiv.org/abs/2504.09760v1",
    "arxiv_id": "2504.09760v1",
    "authors": [
      "Hugo Matias",
      "Daniel Silvestre"
    ],
    "published": "2025-04-13T23:55:09+00:00",
    "summary": "Control Lyapunov Functions (CLFs) and Control Barrier Functions (CBFs) can be combined, typically by means of Quadratic Programs (QPs), to design controllers that achieve performance and safety objectives. However, a significant limitation of this framework is the introduction of asymptotically stable equilibrium points besides the minimizer of the CLF, leading to deadlock situations even for simple systems and bounded convex unsafe sets. To address this problem, we propose a hybrid CLF-CBF control framework with global asymptotic stabilization and safety guarantees, offering a more flexible and systematic design methodology compared to current alternatives available in the literature. We further extend this framework to higher-order systems via a recursive procedure based on a joint CLF-CBF backstepping approach. The proposed solution is assessed through several simulation examples."
  },
  {
    "title": "Epsilon-Neighborhood Decision-Boundary Governed Estimation (EDGE) of 2D Black Box Classifier Functions",
    "url": "http://arxiv.org/abs/2504.09733v1",
    "arxiv_id": "2504.09733v1",
    "authors": [
      "Mithun Goutham",
      "Riccardo DalferroNucci",
      "Stephanie Stockar",
      "Meghna Menon",
      "Sneha Nayak",
      "Harshad Zade",
      "Chetan Patel",
      "Mario Santillo"
    ],
    "published": "2025-04-13T21:40:46+00:00",
    "summary": "Accurately estimating decision boundaries in black box systems is critical when ensuring safety, quality, and feasibility in real-world applications. However, existing methods iteratively refine boundary estimates by sampling in regions of uncertainty, without providing guarantees on the closeness to the decision boundary and also result in unnecessary exploration that is especially disadvantageous when evaluations are costly. This paper presents the Epsilon-Neighborhood Decision-Boundary Governed Estimation (EDGE), a sample efficient and function-agnostic algorithm that leverages the intermediate value theorem to estimate the location of the decision boundary of a black box binary classifier within a user-specified epsilon-neighborhood. Evaluations are conducted on three nonlinear test functions and a case study of an electric grid stability problem with uncertain renewable power injection. The EDGE algorithm demonstrates superior sample efficiency and better boundary approximation than adaptive sampling techniques and grid-based searches."
  },
  {
    "title": "The Structural Safety Generalization Problem",
    "url": "http://arxiv.org/abs/2504.09712v1",
    "arxiv_id": "2504.09712v1",
    "authors": [
      "Julius Broomfield",
      "Tom Gibbs",
      "Ethan Kosak-Hine",
      "George Ingebretsen",
      "Tia Nasir",
      "Jason Zhang",
      "Reihaneh Iranmanesh",
      "Sara Pieri",
      "Reihaneh Rabbany",
      "Kellin Pelrine"
    ],
    "published": "2025-04-13T20:21:08+00:00",
    "summary": "LLM jailbreaks are a widespread safety challenge. Given this problem has not yet been tractable, we suggest targeting a key failure mechanism: the failure of safety to generalize across semantically equivalent inputs. We further focus the target by requiring desirable tractability properties of attacks to study: explainability, transferability between models, and transferability between goals. We perform red-teaming within this framework by uncovering new vulnerabilities to multi-turn, multi-image, and translation-based attacks. These attacks are semantically equivalent by our design to their single-turn, single-image, or untranslated counterparts, enabling systematic comparisons; we show that the different structures yield different safety outcomes. We then demonstrate the potential for this framework to enable new defenses by proposing a Structure Rewriting Guardrail, which converts an input to a structure more conducive to safety assessment. This guardrail significantly improves refusal of harmful inputs, without over-refusing benign ones. Thus, by framing this intermediate challenge - more tractable than universal defenses but essential for long-term safety - we highlight a critical milestone for AI safety research."
  },
  {
    "title": "EmoAgent: Assessing and Safeguarding Human-AI Interaction for Mental Health Safety",
    "url": "http://arxiv.org/abs/2504.09689v1",
    "arxiv_id": "2504.09689v1",
    "authors": [
      "Jiahao Qiu",
      "Yinghui He",
      "Xinzhe Juan",
      "Yiming Wang",
      "Yuhan Liu",
      "Zixin Yao",
      "Yue Wu",
      "Xun Jiang",
      "Ling Yang",
      "Mengdi Wang"
    ],
    "published": "2025-04-13T18:47:22+00:00",
    "summary": "The rise of LLM-driven AI characters raises safety concerns, particularly for vulnerable human users with psychological disorders. To address these risks, we propose EmoAgent, a multi-agent AI framework designed to evaluate and mitigate mental health hazards in human-AI interactions. EmoAgent comprises two components: EmoEval simulates virtual users, including those portraying mentally vulnerable individuals, to assess mental health changes before and after interactions with AI characters. It uses clinically proven psychological and psychiatric assessment tools (PHQ-9, PDI, PANSS) to evaluate mental risks induced by LLM. EmoGuard serves as an intermediary, monitoring users' mental status, predicting potential harm, and providing corrective feedback to mitigate risks. Experiments conducted in popular character-based chatbots show that emotionally engaging dialogues can lead to psychological deterioration in vulnerable users, with mental state deterioration in more than 34.4% of the simulations. EmoGuard significantly reduces these deterioration rates, underscoring its role in ensuring safer AI-human interactions. Our code is available at: https://github.com/1akaman/EmoAgent"
  },
  {
    "title": "Leveraging Reasoning Model Answers to Enhance Non-Reasoning Model Capability",
    "url": "http://arxiv.org/abs/2504.09639v1",
    "arxiv_id": "2504.09639v1",
    "authors": [
      "Haotian Wang",
      "Han Zhao",
      "Shuaiting Chen",
      "Xiaoyu Tian",
      "Sitong Zhao",
      "Yunjie Ji",
      "Yiping Peng",
      "Xiangang Li"
    ],
    "published": "2025-04-13T16:26:56+00:00",
    "summary": "Recent advancements in large language models (LLMs), such as DeepSeek-R1 and OpenAI-o1, have demonstrated the significant effectiveness of test-time scaling, achieving substantial performance gains across various benchmarks. These advanced models utilize deliberate \"thinking\" steps to systematically enhance answer quality. In this paper, we propose leveraging these high-quality outputs generated by reasoning-intensive models to improve less computationally demanding, non-reasoning models. We explore and compare methodologies for utilizing the answers produced by reasoning models to train and improve non-reasoning models. Through straightforward Supervised Fine-Tuning (SFT) experiments on established benchmarks, we demonstrate consistent improvements across various benchmarks, underscoring the potential of this approach for advancing the ability of models to answer questions directly."
  },
  {
    "title": "Mitigating Many-Shot Jailbreaking",
    "url": "http://arxiv.org/abs/2504.09604v1",
    "arxiv_id": "2504.09604v1",
    "authors": [
      "Christopher M. Ackerman",
      "Nina Panickssery"
    ],
    "published": "2025-04-13T14:42:03+00:00",
    "summary": "Many-shot jailbreaking (MSJ) is an adversarial technique that exploits the long context windows of modern LLMs to circumvent model safety training by including in the prompt many examples of a ``fake'' assistant responding inappropriately before the final request. With enough examples, the model's in-context learning abilities override its safety training, and it responds as if it were the ``fake'' assistant. In this work, we probe the effectiveness of different fine tuning and input sanitization approaches on mitigating MSJ attacks, alone and in combination. We find incremental mitigation effectiveness for each, and we show that the combined techniques significantly reduce the effectiveness of MSJ attacks, while retaining model performance in benign in-context learning and conversational tasks. We suggest that our approach could meaningfully ameliorate this vulnerability if incorporated into model safety post-training."
  },
  {
    "title": "Fine-tuning an Large Language Model for Automating Computational Fluid Dynamics Simulations",
    "url": "http://arxiv.org/abs/2504.09602v1",
    "arxiv_id": "2504.09602v1",
    "authors": [
      "Zhehao Dong",
      "Zhen Lu",
      "Yue Yang"
    ],
    "published": "2025-04-13T14:35:30+00:00",
    "summary": "Configuring computational fluid dynamics (CFD) simulations typically demands extensive domain expertise, limiting broader access. Although large language models (LLMs) have advanced scientific computing, their use in automating CFD workflows is underdeveloped. We introduce a novel approach centered on domain-specific LLM adaptation. By fine-tuning Qwen2.5-7B-Instruct on NL2FOAM, our custom dataset of 28716 natural language-to-OpenFOAM configuration pairs with chain-of-thought (CoT) annotations, we enable direct translation from natural language descriptions to executable CFD setups. A multi-agent framework orchestrates the process, autonomously verifying inputs, generating configurations, running simulations, and correcting errors. Evaluation on a benchmark of 21 diverse flow cases demonstrates state-of-the-art performance, achieving 88.7% solution accuracy and 82.6% first-attempt success rate. This significantly outperforms larger general-purpose models like Qwen2.5-72B-Instruct, DeepSeek-R1, and Llama3.3-70B-Instruct, while also requiring fewer correction iterations and maintaining high computational efficiency. The results highlight the critical role of domain-specific adaptation in deploying LLM assistants for complex engineering workflows."
  },
  {
    "title": "Analysis of Radiation Level and Estimation of Protection Distance of \u03b3 Mobile Flaw Detection Source",
    "url": "http://arxiv.org/abs/2504.09556v1",
    "arxiv_id": "2504.09556v1",
    "authors": [
      "Zhihui Liu",
      "Xiang Hu",
      "Zhengyang Zhang"
    ],
    "published": "2025-04-13T13:05:47+00:00",
    "summary": "Objective To analyze the radiation dose associated with gamma-ray mobile flaw detection, estimate the extent of the supervision and control areas, and assess the associated radiation risks. Methods A combination of theoretical calculations and actual measurements was used to compare and analyze the ambient equivalent dose rates of 192 Ir and 75 Se at their nominal source strengths. Measurements were conducted at distances of 1 m, 2 m, and 5 m from the radiation source. The extents of the control and supervision areas were estimated under three working scenarios: 1 without considering air attenuation, 2 considering air attenuation, and 3 after shielding by the flaw detection workpiece, using source activities of 3.7 * 10^10 Bq and 3.7 * 10^12Bq. Results Actual measurement of radiation dose of 192 Ir and 75 Se were measured under three different nominal activities. Theoretical calculation of radiation dose estimates at various distances were obtained for both nuclides, and the results showed that the theoretical values were basically consistent with the measured values. Conclusion The estimated scope of the supervision and control areas provided in this study can serve as a reference for flaw detection companies. Technicians can use these estimates to calculate appropriate distances for safety zones based on different nuclide activities. This enables flaw detection personnel to reduce the measurement scope on-site and to quickly and accurately define area boundaries."
  },
  {
    "title": "Quality Control and Structural Reliability -- A Unified Framework for Integrating Conformity Assessment and Partial Safety Factors",
    "url": "http://arxiv.org/abs/2504.09508v1",
    "arxiv_id": "2504.09508v1",
    "authors": [
      "Tammam Bakeer",
      "Wolfram Jaeger"
    ],
    "published": "2025-04-13T10:14:34+00:00",
    "summary": "Ensuring structural reliability remains a core concern in civil engineering, yet the quantitative effects of quality control measures on material variability and safety margins are not fully understood, especially for materials other than reinforced concrete. This study addresses this gap by presenting a probabilistic framework that integrates Bayesian updating, acceptance sampling, and operating characteristic (OC) curves to model conformity assessment as a probabilistic filter. In doing so, it refines prior distributions of key material and execution parameters based on quality control outcomes, linking reductions in the coefficient of variation directly to adjustments in partial safety factors. Applying the framework to a masonry wall example demonstrates how systematic quality control efforts, particularly those targeting parameters with higher importance such as masonry unit strength and execution quality-produce substantial gains in structural reliability. The analysis shows that combined quality control measures can lower the partial safety factor from a baseline of 1.5 to about 1.38, corresponding to an improvement factor of roughly 1.09 and material savings of approximately 8%. Conversely, controlling parameters with negligible influence, such as mortar properties, provides limited benefit. These findings encourage focusing quality control resources on the most influential parameters and integrating results into semi-probabilistic design methods. By offering a transparent, standards-compatible approach, the framework supports the refinement of design guidelines, promotes more efficient resource allocation, and enhances overall structural safety in the built environment."
  },
  {
    "title": "AdaSteer: Your Aligned LLM is Inherently an Adaptive Jailbreak Defender",
    "url": "http://arxiv.org/abs/2504.09466v1",
    "arxiv_id": "2504.09466v1",
    "authors": [
      "Weixiang Zhao",
      "Jiahe Guo",
      "Yulin Hu",
      "Yang Deng",
      "An Zhang",
      "Xingyu Sui",
      "Xinyang Han",
      "Yanyan Zhao",
      "Bing Qin",
      "Tat-Seng Chua",
      "Ting Liu"
    ],
    "published": "2025-04-13T07:39:17+00:00",
    "summary": "Despite extensive efforts in safety alignment, large language models (LLMs) remain vulnerable to jailbreak attacks. Activation steering offers a training-free defense method but relies on fixed steering coefficients, resulting in suboptimal protection and increased false rejections of benign inputs. To address this, we propose AdaSteer, an adaptive activation steering method that dynamically adjusts model behavior based on input characteristics. We identify two key properties: Rejection Law (R-Law), which shows that stronger steering is needed for jailbreak inputs opposing the rejection direction, and Harmfulness Law (H-Law), which differentiates adversarial and benign inputs. AdaSteer steers input representations along both the Rejection Direction (RD) and Harmfulness Direction (HD), with adaptive coefficients learned via logistic regression, ensuring robust jailbreak defense while preserving benign input handling. Experiments on LLaMA-3.1, Gemma-2, and Qwen2.5 show that AdaSteer outperforms baseline methods across multiple jailbreak attacks with minimal impact on utility. Our results highlight the potential of interpretable model internals for real-time, flexible safety enforcement in LLMs."
  },
  {
    "title": "ADDT -- A Digital Twin Framework for Proactive Safety Validation in Autonomous Driving Systems",
    "url": "http://arxiv.org/abs/2504.09461v1",
    "arxiv_id": "2504.09461v1",
    "authors": [
      "Bo Yu",
      "Chaoran Yuan",
      "Zishen Wan",
      "Jie Tang",
      "Fadi Kurdahi",
      "Shaoshan Liu"
    ],
    "published": "2025-04-13T07:17:17+00:00",
    "summary": "Autonomous driving systems continue to face safety-critical failures, often triggered by rare and unpredictable corner cases that evade conventional testing. We present the Autonomous Driving Digital Twin (ADDT) framework, a high-fidelity simulation platform designed to proactively identify hidden faults, evaluate real-time performance, and validate safety before deployment. ADDT combines realistic digital models of driving environments, vehicle dynamics, sensor behavior, and fault conditions to enable scalable, scenario-rich stress-testing under diverse and adverse conditions. It supports adaptive exploration of edge cases using reinforcement-driven techniques, uncovering failure modes that physical road testing often misses. By shifting from reactive debugging to proactive simulation-driven validation, ADDT enables a more rigorous and transparent approach to autonomous vehicle safety engineering. To accelerate adoption and facilitate industry-wide safety improvements, the entire ADDT framework has been released as open-source software, providing developers with an accessible and extensible tool for comprehensive safety testing at scale."
  },
  {
    "title": "SaRO: Enhancing LLM Safety through Reasoning-based Alignment",
    "url": "http://arxiv.org/abs/2504.09420v1",
    "arxiv_id": "2504.09420v1",
    "authors": [
      "Yutao Mou",
      "Yuxiao Luo",
      "Shikun Zhang",
      "Wei Ye"
    ],
    "published": "2025-04-13T03:36:06+00:00",
    "summary": "Current safety alignment techniques for large language models (LLMs) face two key challenges: (1) under-generalization, which leaves models vulnerable to novel jailbreak attacks, and (2) over-alignment, which leads to the excessive refusal of benign instructions. Our preliminary investigation reveals semantic overlap between jailbreak/harmful queries and normal prompts in embedding space, suggesting that more effective safety alignment requires a deeper semantic understanding. This motivates us to incorporate safety-policy-driven reasoning into the alignment process. To this end, we propose the Safety-oriented Reasoning Optimization Framework (SaRO), which consists of two stages: (1) Reasoning-style Warmup (RW) that enables LLMs to internalize long-chain reasoning through supervised fine-tuning, and (2) Safety-oriented Reasoning Process Optimization (SRPO) that promotes safety reflection via direct preference optimization (DPO). Extensive experiments demonstrate the superiority of SaRO over traditional alignment methods."
  },
  {
    "title": "Graph-Based Prediction Models for Data Debiasing",
    "url": "http://arxiv.org/abs/2504.09348v1",
    "arxiv_id": "2504.09348v1",
    "authors": [
      "Dongze Wu",
      "Hanyang Jiang",
      "Yao Xie"
    ],
    "published": "2025-04-12T21:34:49+00:00",
    "summary": "Bias in data collection, arising from both under-reporting and over-reporting, poses significant challenges in critical applications such as healthcare and public safety. In this work, we introduce Graph-based Over- and Under-reporting Debiasing (GROUD), a novel graph-based optimization framework that debiases reported data by jointly estimating the true incident counts and the associated reporting bias probabilities. By modeling the bias as a smooth signal over a graph constructed from geophysical or feature-based similarities, our convex formulation not only ensures a unique solution but also comes with theoretical recovery guarantees under certain assumptions. We validate GROUD on both challenging simulated experiments and real-world datasets -- including Atlanta emergency calls and COVID-19 vaccine adverse event reports -- demonstrating its robustness and superior performance in accurately recovering debiased counts. This approach paves the way for more reliable downstream decision-making in systems affected by reporting irregularities."
  },
  {
    "title": "Strain-induced polarization rotation in freestanding ferroelectric oxide membranes",
    "url": "http://arxiv.org/abs/2504.09244v1",
    "arxiv_id": "2504.09244v1",
    "authors": [
      "Alban Degezelle",
      "Razvan Burcea",
      "Pascale Gemeiner",
      "Maxime Vallet",
      "Brahim Dkhil",
      "St\u00e9phane Fusil",
      "Vincent Garcia",
      "Sylvia Matzen",
      "Philippe Lecoeur",
      "Thomas Maroutian"
    ],
    "published": "2025-04-12T14:51:19+00:00",
    "summary": "Freestanding ferroelectric membranes have emerged as a versatile tool for strain engineering, enabling the exploration of ferroelectric properties beyond traditional epitaxy. The resulting ferroelectric domain patterns stem from the balance at the local scale of several effects playing a key role, i.e. piezoelectricity linked to strain, and flexoelectricity arising from strain gradients. To weight their respective contributions for a given membrane geometry, the strain profile has to be mapped with respect to the ferroelectric polarization landscape, a necessary step to allow for a controlled tailoring of the latter. In this study, we examine the effect of bending strain on a Pb(Zr,Ti)O3 membrane in a fold-like structure, observing a polarization rotation from out-of-plane to in-plane at the fold apex. Combining piezoresponse force microscopy, Raman spectroscopy, and scanning transmission electron microscopy, we map the ferroelectric polarization direction relative to the height profile of the membrane, and discuss the contributions of strain and strain gradients for this archetypal fold geometry. Our findings offer new insights into strain-engineered polarization configurations, and emphasize strain effects at the nanoscale to tune the functional properties in freestanding membranes."
  },
  {
    "title": "Graph Learning-Driven Multi-Vessel Association: Fusing Multimodal Data for Maritime Intelligence",
    "url": "http://arxiv.org/abs/2504.09197v1",
    "arxiv_id": "2504.09197v1",
    "authors": [
      "Yuxu Lu",
      "Kaisen Yang",
      "Dong Yang",
      "Haifeng Ding",
      "Jinxian Weng",
      "Ryan Wen Liu"
    ],
    "published": "2025-04-12T12:45:55+00:00",
    "summary": "Ensuring maritime safety and optimizing traffic management in increasingly crowded and complex waterways require effective waterway monitoring. However, current methods struggle with challenges arising from multimodal data, such as dimensional disparities, mismatched target counts, vessel scale variations, occlusions, and asynchronous data streams from systems like the automatic identification system (AIS) and closed-circuit television (CCTV). Traditional multi-target association methods often struggle with these complexities, particularly in densely trafficked waterways. To overcome these issues, we propose a graph learning-driven multi-vessel association (GMvA) method tailored for maritime multimodal data fusion. By integrating AIS and CCTV data, GMvA leverages time series learning and graph neural networks to capture the spatiotemporal features of vessel trajectories effectively. To enhance feature representation, the proposed method incorporates temporal graph attention and spatiotemporal attention, effectively capturing both local and global vessel interactions. Furthermore, a multi-layer perceptron-based uncertainty fusion module computes robust similarity scores, and the Hungarian algorithm is adopted to ensure globally consistent and accurate target matching. Extensive experiments on real-world maritime datasets confirm that GMvA delivers superior accuracy and robustness in multi-target association, outperforming existing methods even in challenging scenarios with high vessel density and incomplete or unevenly distributed AIS and CCTV data."
  },
  {
    "title": "Feature-Aware Malicious Output Detection and Mitigation",
    "url": "http://arxiv.org/abs/2504.09191v1",
    "arxiv_id": "2504.09191v1",
    "authors": [
      "Weilong Dong",
      "Peiguang Li",
      "Yu Tian",
      "Xinyi Zeng",
      "Fengdi Li",
      "Sirui Wang"
    ],
    "published": "2025-04-12T12:12:51+00:00",
    "summary": "The rapid advancement of large language models (LLMs) has brought significant benefits to various domains while introducing substantial risks. Despite being fine-tuned through reinforcement learning, LLMs lack the capability to discern malicious content, limiting their defense against jailbreak. To address these safety concerns, we propose a feature-aware method for harmful response rejection (FMM), which detects the presence of malicious features within the model's feature space and adaptively adjusts the model's rejection mechanism. By employing a simple discriminator, we detect potential malicious traits during the decoding phase. Upon detecting features indicative of toxic tokens, FMM regenerates the current token. By employing activation patching, an additional rejection vector is incorporated during the subsequent token generation, steering the model towards a refusal response. Experimental results demonstrate the effectiveness of our approach across multiple language models and diverse attack techniques, while crucially maintaining the models' standard generation capabilities."
  },
  {
    "title": "Compliant Explicit Reference Governor for Contact Friendly Robotic Manipulators",
    "url": "http://arxiv.org/abs/2504.09188v1",
    "arxiv_id": "2504.09188v1",
    "authors": [
      "Yaashia Gautam",
      "Nataliya Nechyporenko",
      "Chi-Hui Lin",
      "Alessandro Roncone",
      "Marco M. Nicotra"
    ],
    "published": "2025-04-12T12:01:46+00:00",
    "summary": "This paper introduces the Compliant Explicit Reference Governor (C-ERG), an extension of the Explicit Reference Governor that allows the robot to operate safely while in contact with the environment.   The C-ERG is an intermediate layer that can be placed between a high-level planner and a low-level controller: its role is to enforce operational constraints and to enable the smooth transition between free-motion and contact operations. The C-ERG ensures safety by limiting the total energy available to the robotic arm at the time of contact. In the absence of contact, however, the C-ERG does not penalize the system performance.   Numerical examples showcase the behavior of the C-ERG for increasingly complex systems."
  },
  {
    "title": "Dose-finding design based on level set estimation in phase I cancer clinical trials",
    "url": "http://arxiv.org/abs/2504.09157v1",
    "arxiv_id": "2504.09157v1",
    "authors": [
      "Keiichiro Seno",
      "Kota Matsui",
      "Shogo Iwazaki",
      "Yu Inatsu",
      "Shion Takeno",
      "Shigeyuki Matsui"
    ],
    "published": "2025-04-12T09:44:20+00:00",
    "summary": "The primary objective of phase I cancer clinical trials is to evaluate the safety of a new experimental treatment and to find the maximum tolerated dose (MTD). We show that the MTD estimation problem can be regarded as a level set estimation (LSE) problem whose objective is to determine the regions where an unknown function value is above or below a given threshold. Then, we propose a novel dose-finding design in the framework of LSE. The proposed design determines the next dose on the basis of an acquisition function incorporating uncertainty in the posterior distribution of the dose-toxicity curve as well as overdose control. Simulation experiments show that the proposed LSE design achieves a higher accuracy in estimating the MTD and involves a lower risk of overdosing allocation compared to existing designs, thereby indicating that it provides an effective methodology for phase I cancer clinical trial design."
  },
  {
    "title": "Research on the Crystal Growth, Band Structure and Luminescence Mechanism of (CH3NH3)2HgI4",
    "url": "http://arxiv.org/abs/2504.09150v1",
    "arxiv_id": "2504.09150v1",
    "authors": [
      "Linlin Liu",
      "Zuanquan Chen",
      "Wensi Yang",
      "Sen Zhang",
      "Jiaqi Zhu"
    ],
    "published": "2025-04-12T09:30:12+00:00",
    "summary": "Nuclear radiation detectors play a crucial role in fields such as nuclear safety and medical imaging. The core of their performance lies in the selection of detection materials. Semiconductor detectors have become a hot topic in current research due to their advantages such as small size, good energy resolution, and high detection efficiency. As one of the most promising materials for fabricating room - temperature nuclear radiation semiconductor detectors, HgI2 exhibits excellent detection performance due to its high atomic number, large band gap, strong ray - stopping power, and high volume dark resistivity. However, issues such as poor chemical stability and low vacancy mobility of HgI2 limit its development. Therefore, researchers have carried out inorganic doping/organic hybridization on it. By introducing the organic ligand CH3NH3I, the synthesis of organic - inorganic hybrid compounds based on HgI2 is expected to significantly improve the stability of HgI2. Research on organic - inorganic hybrid metal halide crystals shows that this material has great application potential in the field of luminescent materials."
  },
  {
    "title": "Can Large Language Models Become Policy Refinement Partners? Evidence from China's Social Security Studies",
    "url": "http://arxiv.org/abs/2504.09137v1",
    "arxiv_id": "2504.09137v1",
    "authors": [
      "Ke Jinghan",
      "Zhou Zheng",
      "Zhao Yuxuan"
    ],
    "published": "2025-04-12T08:50:12+00:00",
    "summary": "The rapid development of large language models (LLMs) is reshaping operational paradigms across multidisciplinary domains. LLMs' emergent capability to synthesize policy-relevant insights across disciplinary boundaries suggests potential as decision-support tools. However, their actual performance and suitability as policy refinement partners still require verification through rigorous and systematic evaluations. Our study employs the context-embedded generation-adaptation framework to conduct a tripartite comparison among the American GPT-4o, the Chinese DeepSeek-R1 and human researchers, investigating the capability boundaries and performance characteristics of LLMs in generating policy recommendations for China's social security issues. This study demonstrates that while large LLMs exhibit distinct advantages in systematic policy design, they face significant limitations in addressing complex social dynamics, balancing stakeholder interests, and controlling fiscal risks within the social security domain. Furthermore, DeepSeek-R1 demonstrates superior performance to GPT-4o across all evaluation dimensions in policy recommendation generation, illustrating the potential of localized training to improve contextual alignment. These findings suggest that regionally-adapted LLMs can function as supplementary tools for generating diverse policy alternatives informed by domain-specific social insights. Nevertheless, the formulation of policy refinement requires integration with human researchers' expertise, which remains critical for interpreting institutional frameworks, cultural norms, and value systems."
  },
  {
    "title": "Can Large Language Models Become Policy Refinement Partners? Evidence from China's Social Security Studies",
    "url": "http://arxiv.org/abs/2504.09137v2",
    "arxiv_id": "2504.09137v2",
    "authors": [
      "Ke Jinghan",
      "Zhou Zheng",
      "Zhao Yuxuan"
    ],
    "published": "2025-04-12T08:50:12+00:00",
    "summary": "The rapid development of large language models (LLMs) is reshaping operational paradigms across multidisciplinary domains. LLMs' emergent capability to synthesize policy-relevant insights across disciplinary boundaries suggests potential as decision-support tools. However, their actual performance and suitability as policy refinement partners still require verification through rigorous and systematic evaluations. Our study employs the context-embedded generation-adaptation framework to conduct a tripartite comparison among the American GPT-4o, the Chinese DeepSeek-R1 and human researchers, investigating the capability boundaries and performance characteristics of LLMs in generating policy recommendations for China's social security issues. This study demonstrates that while LLMs exhibit distinct advantages in systematic policy design, they face significant limitations in addressing complex social dynamics, balancing stakeholder interests, and controlling fiscal risks within the social security domain. Furthermore, DeepSeek-R1 demonstrates superior performance to GPT-4o across all evaluation dimensions in policy recommendation generation, illustrating the potential of localized training to improve contextual alignment. These findings suggest that regionally-adapted LLMs can function as supplementary tools for generating diverse policy alternatives informed by domain-specific social insights. Nevertheless, the formulation of policy refinement requires integration with human researchers' expertise, which remains critical for interpreting institutional frameworks, cultural norms, and value systems."
  },
  {
    "title": "BiFlex: A Passive Bimodal Stiffness Flexible Wrist for Manipulation in Unstructured Environments",
    "url": "http://arxiv.org/abs/2504.08706v1",
    "arxiv_id": "2504.08706v1",
    "authors": [
      "Gu-Cheol Jeong",
      "Stefano Dalla Gasperina",
      "Ashish D. Deshpande",
      "Lillian Chin",
      "Roberto Mart\u00edn-Mart\u00edn"
    ],
    "published": "2025-04-11T17:16:13+00:00",
    "summary": "Robotic manipulation in unstructured, humancentric environments poses a dual challenge: achieving the precision need for delicate free-space operation while ensuring safety during unexpected contact events. Traditional wrists struggle to balance these demands, often relying on complex control schemes or complicated mechanical designs to mitigate potential damage from force overload. In response, we present BiFlex, a flexible robotic wrist that uses a soft buckling honeycomb structure to provides a natural bimodal stiffness response. The higher stiffness mode enables precise household object manipulation, while the lower stiffness mode provides the compliance needed to adapt to external forces. We design BiFlex to maintain a fingertip deflection of less than 1 cm while supporting loads up to 500g and create a BiFlex wrist for many grippers, including Panda, Robotiq, and BaRiFlex. We validate BiFlex under several real-world experimental evaluations, including surface wiping, precise pick-and-place, and grasping under environmental constraints. We demonstrate that BiFlex simplifies control while maintaining precise object manipulation and enhanced safety in real-world applications."
  },
  {
    "title": "Offline Reinforcement Learning using Human-Aligned Reward Labeling for Autonomous Emergency Braking in Occluded Pedestrian Crossing",
    "url": "http://arxiv.org/abs/2504.08704v1",
    "arxiv_id": "2504.08704v1",
    "authors": [
      "Vinal Asodia",
      "Zhenhua Feng",
      "Saber Fallah"
    ],
    "published": "2025-04-11T17:11:21+00:00",
    "summary": "Effective leveraging of real-world driving datasets is crucial for enhancing the training of autonomous driving systems. While Offline Reinforcement Learning enables the training of autonomous vehicles using such data, most available datasets lack meaningful reward labels. Reward labeling is essential as it provides feedback for the learning algorithm to distinguish between desirable and undesirable behaviors, thereby improving policy performance. This paper presents a novel pipeline for generating human-aligned reward labels. The proposed approach addresses the challenge of absent reward signals in real-world datasets by generating labels that reflect human judgment and safety considerations. The pipeline incorporates an adaptive safety component, activated by analyzing semantic segmentation maps, allowing the autonomous vehicle to prioritize safety over efficiency in potential collision scenarios. The proposed pipeline is applied to an occluded pedestrian crossing scenario with varying levels of pedestrian traffic, using synthetic and simulation data. The results indicate that the generated reward labels closely match the simulation reward labels. When used to train the driving policy using Behavior Proximal Policy Optimisation, the results are competitive with other baselines. This demonstrates the effectiveness of our method in producing reliable and human-aligned reward signals, facilitating the training of autonomous driving systems through Reinforcement Learning outside of simulation environments and in alignment with human values."
  },
  {
    "title": "Safe Flow Matching: Robot Motion Planning with Control Barrier Functions",
    "url": "http://arxiv.org/abs/2504.08661v1",
    "arxiv_id": "2504.08661v1",
    "authors": [
      "Xiaobing Dai",
      "Dian Yu",
      "Shanshan Zhang",
      "Zewen Yang"
    ],
    "published": "2025-04-11T16:10:58+00:00",
    "summary": "Recent advances in generative modeling have led to promising results in robot motion planning, particularly through diffusion and flow-based models that capture complex, multimodal trajectory distributions. However, these methods are typically trained offline and remain limited when faced with unseen environments or dynamic constraints, often lacking explicit mechanisms to ensure safety during deployment. In this work, we propose, Safe Flow Matching (SafeFM), a motion planning approach for trajectory generation that integrates flow matching with safety guarantees. By incorporating the proposed flow matching barrier functions, SafeFM ensures that generated trajectories remain within safe regions throughout the planning horizon, even in the presence of previously unseen obstacles or state-action constraints. Unlike diffusion-based approaches, our method allows for direct, efficient sampling of constraint-satisfying trajectories, making it well-suited for real-time motion planning. We evaluate SafeFM on a diverse set of tasks, including planar robot navigation and 7-DoF manipulation, demonstrating superior safety, generalization, and planning performance compared to state-of-the-art generative planners. Comprehensive resources are available on the project website: https://safeflowmatching.github.io/SafeFM/"
  },
  {
    "title": "TinyCenterSpeed: Efficient Center-Based Object Detection for Autonomous Racing",
    "url": "http://arxiv.org/abs/2504.08655v1",
    "arxiv_id": "2504.08655v1",
    "authors": [
      "Neil Reichlin",
      "Nicolas Baumann",
      "Edoardo Ghignone",
      "Michele Magno"
    ],
    "published": "2025-04-11T15:58:46+00:00",
    "summary": "Perception within autonomous driving is nearly synonymous with Neural Networks (NNs). Yet, the domain of autonomous racing is often characterized by scaled, computationally limited robots used for cost-effectiveness and safety. For this reason, opponent detection and tracking systems typically resort to traditional computer vision techniques due to computational constraints. This paper introduces TinyCenterSpeed, a streamlined adaptation of the seminal CenterPoint method, optimized for real-time performance on 1:10 scale autonomous racing platforms. This adaptation is viable even on OBCs powered solely by Central Processing Units (CPUs), as it incorporates the use of an external Tensor Processing Unit (TPU). We demonstrate that, compared to Adaptive Breakpoint Detector (ABD), the current State-of-the-Art (SotA) in scaled autonomous racing, TinyCenterSpeed not only improves detection and velocity estimation by up to 61.38% but also supports multi-opponent detection and estimation. It achieves real-time performance with an inference time of just 7.88 ms on the TPU, significantly reducing CPU utilization 8.3-fold."
  },
  {
    "title": "Deep Learning Methods for Detecting Thermal Runaway Events in Battery Production Lines",
    "url": "http://arxiv.org/abs/2504.08632v1",
    "arxiv_id": "2504.08632v1",
    "authors": [
      "Athanasios Athanasopoulos",
      "Mat\u00fa\u0161 Mihal\u00e1k",
      "Marcin Pietrasik"
    ],
    "published": "2025-04-11T15:35:50+00:00",
    "summary": "One of the key safety considerations of battery manufacturing is thermal runaway, the uncontrolled increase in temperature which can lead to fires, explosions, and emissions of toxic gasses. As such, development of automated systems capable of detecting such events is of considerable importance in both academic and industrial contexts. In this work, we investigate the use of deep learning for detecting thermal runaway in the battery production line of VDL Nedcar, a Dutch automobile manufacturer. Specifically, we collect data from the production line to represent both baseline (non thermal runaway) and thermal runaway conditions. Thermal runaway was simulated through the use of external heat and smoke sources. The data consisted of both optical and thermal images which were then preprocessed and fused before serving as input to our models. In this regard, we evaluated three deep-learning models widely used in computer vision including shallow convolutional neural networks, residual neural networks, and vision transformers on two performance metrics. Furthermore, we evaluated these models using explainability methods to gain insight into their ability to capture the relevant feature information from their inputs. The obtained results indicate that the use of deep learning is a viable approach to thermal runaway detection in battery production lines."
  },
  {
    "title": "Enabling Safety for Aerial Robots: Planning and Control Architectures",
    "url": "http://arxiv.org/abs/2504.08601v1",
    "arxiv_id": "2504.08601v1",
    "authors": [
      "Kaleb Ben Naveed",
      "Devansh R. Agrawal",
      "Daniel M. Cherenson",
      "Haejoon Lee",
      "Alia Gilbert",
      "Hardik Parwana",
      "Vishnu S. Chipade",
      "William Bentz",
      "Dimitra Panagou"
    ],
    "published": "2025-04-11T15:05:31+00:00",
    "summary": "Ensuring safe autonomy is crucial for deploying aerial robots in real-world applications. However, safety is a multifaceted challenge that must be addressed from multiple perspectives, including navigation in dynamic environments, operation under resource constraints, and robustness against adversarial attacks and uncertainties. In this paper, we present the authors' recent work that tackles some of these challenges and highlights key aspects that must be considered to enhance the safety and performance of autonomous aerial systems. All presented approaches are validated through hardware experiments."
  },
  {
    "title": "Secondary Safety Control for Systems with Sector Bounded Nonlinearities",
    "url": "http://arxiv.org/abs/2504.08535v1",
    "arxiv_id": "2504.08535v1",
    "authors": [
      "Yankai Lin",
      "Michelle S. Chong",
      "Carlos Murguia"
    ],
    "published": "2025-04-11T13:44:22+00:00",
    "summary": "We consider the problem of safety verification and safety-aware controller synthesis for systems with sector bounded nonlinearities. We aim to keep the states of the system within a given safe set under potential actuator and sensor attacks. Specifically, we adopt the setup that a controller has already been designed to stabilize the plant. Using invariant sets and barrier certificate theory, we first give sufficient conditions to verify the safety of the closed-loop system under attacks. Furthermore, by using a subset of sensors that are assumed to be free of attacks, we provide a synthesis method for a secondary controller that enhances the safety of the system. The sufficient conditions to verify safety are derived using Lyapunov-based tools and the S-procedure. Using the projection lemma, the conditions are then formulated as linear matrix inequality (LMI) problems which can be solved efficiently. Lastly, our theoretical results are illustrated through numerical simulations."
  },
  {
    "title": "Secondary Safety Control for Systems with Sector Bounded Nonlinearities [Extended Version]",
    "url": "http://arxiv.org/abs/2504.08535v2",
    "arxiv_id": "2504.08535v2",
    "authors": [
      "Yankai Lin",
      "Michelle S. Chong",
      "Carlos Murguia"
    ],
    "published": "2025-04-11T13:44:22+00:00",
    "summary": "We consider the problem of safety verification and safety-aware controller synthesis for systems with sector bounded nonlinearities. We aim to keep the states of the system within a given safe set under potential actuator and sensor attacks. Specifically, we adopt the setup that a controller has already been designed to stabilize the plant. Using invariant sets and barrier certificate theory, we first give sufficient conditions to verify the safety of the closed-loop system under attacks. Furthermore, by using a subset of sensors that are assumed to be free of attacks, we provide a synthesis method for a secondary controller that enhances the safety of the system. The sufficient conditions to verify safety are derived using Lyapunov-based tools and the S-procedure. Using the projection lemma, the conditions are then formulated as linear matrix inequality (LMI) problems which can be solved efficiently. Lastly, our theoretical results are illustrated through numerical simulations."
  },
  {
    "title": "Physics-informed data-driven control without persistence of excitation",
    "url": "http://arxiv.org/abs/2504.08484v1",
    "arxiv_id": "2504.08484v1",
    "authors": [
      "Martina Vanelli",
      "Julien M. Hendrickx"
    ],
    "published": "2025-04-11T12:19:51+00:00",
    "summary": "We show that data that is not sufficiently informative to allow for system re-identification can still provide meaningful information when combined with external or physical knowledge of the system, such as bounded system matrix norms. We then illustrate how this information can be leveraged for safety and energy minimization problems and to enhance predictions in unmodelled dynamics. This preliminary work outlines key ideas toward using limited data for effective control by integrating physical knowledge of the system and exploiting interpolation conditions."
  },
  {
    "title": "Constrained Machine Learning Through Hyperspherical Representation",
    "url": "http://arxiv.org/abs/2504.08415v1",
    "arxiv_id": "2504.08415v1",
    "authors": [
      "Gaetano Signorelli",
      "Michele Lombardi"
    ],
    "published": "2025-04-11T10:19:49+00:00",
    "summary": "The problem of ensuring constraints satisfaction on the output of machine learning models is critical for many applications, especially in safety-critical domains. Modern approaches rely on penalty-based methods at training time, which do not guarantee to avoid constraints violations; or constraint-specific model architectures (e.g., for monotonocity); or on output projection, which requires to solve an optimization problem that might be computationally demanding. We present the Hypersherical Constrained Representation, a novel method to enforce constraints in the output space for convex and bounded feasibility regions (generalizable to star domains). Our method operates on a different representation system, where Euclidean coordinates are converted into hyperspherical coordinates relative to the constrained region, which can only inherently represent feasible points. Experiments on a synthetic and a real-world dataset show that our method has predictive performance comparable to the other approaches, can guarantee 100% constraint satisfaction, and has a minimal computational cost at inference time."
  },
  {
    "title": "Evaluating Pedestrian Risks in Shared Spaces Through Autonomous Vehicle Experiments on a Fixed Track",
    "url": "http://arxiv.org/abs/2504.08316v1",
    "arxiv_id": "2504.08316v1",
    "authors": [
      "Enrico Del Re",
      "Novel Certad",
      "Cristina Olaverri-Monreal"
    ],
    "published": "2025-04-11T07:37:15+00:00",
    "summary": "The majority of research on safety in autonomous vehicles has been conducted in structured and controlled environments. However, there is a scarcity of research on safety in unregulated pedestrian areas, especially when interacting with public transport vehicles like trams. This study investigates pedestrian responses to an alert system in this context by replicating this real-world scenario in an environment using an autonomous vehicle. The results show that safety measures from other contexts can be adapted to shared spaces with trams, where fixed tracks heighten risks in unregulated crossings."
  },
  {
    "title": "SortBench: Benchmarking LLMs based on their ability to sort lists",
    "url": "http://arxiv.org/abs/2504.08312v1",
    "arxiv_id": "2504.08312v1",
    "authors": [
      "Steffen Herbold"
    ],
    "published": "2025-04-11T07:29:56+00:00",
    "summary": "Sorting is a tedious but simple task for human intelligence and can be solved fairly easily algorithmically. However, for Large Language Models (LLMs) this task is surprisingly hard, as some properties of sorting are among known weaknesses of LLMs: being faithful to the input data, logical comparisons between values, and strictly differentiating between syntax (used for sorting) and semantics (typically learned by embeddings). Within this paper, we describe the new SortBench benchmark for LLMs that comes with different difficulties and that can be easily scaled in terms of difficulty. We apply this benchmark to seven state-of-the-art LLMs, including current test-time reasoning models. Our results show that while the o3-mini model is very capable at sorting in general, even this can be fooled if strings are defined to mix syntactical and semantical aspects, e.g., by asking to sort numbers written-out as word. Furthermore, all models have problems with the faithfulness to the input of long lists, i.e., they drop items and add new ones. Our results also show that test-time reasoning has a tendency to overthink problems which leads to performance degradation. Finally, models without test-time reasoning like GPT-4o are not much worse than reasoning models."
  },
  {
    "title": "Evaluating the Bias in LLMs for Surveying Opinion and Decision Making in Healthcare",
    "url": "http://arxiv.org/abs/2504.08260v1",
    "arxiv_id": "2504.08260v1",
    "authors": [
      "Yonchanok Khaokaew",
      "Flora D. Salim",
      "Andreas Z\u00fcfle",
      "Hao Xue",
      "Taylor Anderson",
      "Matthew Scotch",
      "David J Heslop"
    ],
    "published": "2025-04-11T05:11:40+00:00",
    "summary": "Generative agents have been increasingly used to simulate human behaviour in silico, driven by large language models (LLMs). These simulacra serve as sandboxes for studying human behaviour without compromising privacy or safety. However, it remains unclear whether such agents can truly represent real individuals. This work compares survey data from the Understanding America Study (UAS) on healthcare decision-making with simulated responses from generative agents. Using demographic-based prompt engineering, we create digital twins of survey respondents and analyse how well different LLMs reproduce real-world behaviours. Our findings show that some LLMs fail to reflect realistic decision-making, such as predicting universal vaccine acceptance. However, Llama 3 captures variations across race and Income more accurately but also introduces biases not present in the UAS data. This study highlights the potential of generative agents for behavioural research while underscoring the risks of bias from both LLMs and prompting strategies."
  },
  {
    "title": "Neural Network-assisted Interval Reachability for Systems with Control Barrier Function-Based Safe Controllers",
    "url": "http://arxiv.org/abs/2504.08249v1",
    "arxiv_id": "2504.08249v1",
    "authors": [
      "Damola Ajeyemi",
      "Saber Jafarpour",
      "Emiliano Dall'Anese"
    ],
    "published": "2025-04-11T04:14:55+00:00",
    "summary": "Control Barrier Functions (CBFs) have been widely utilized in the design of optimization-based controllers and filters for dynamical systems to ensure forward invariance of a given set of safe states. While CBF-based controllers offer safety guarantees, they can compromise the performance of the system, leading to undesirable behaviors such as unbounded trajectories and emergence of locally stable spurious equilibria. Computing reachable sets for systems with CBF-based controllers is an effective approach for runtime performance and stability verification, and can potentially serve as a tool for trajectory re-planning. In this paper, we propose a computationally efficient interval reachability method for performance verification of systems with optimization-based controllers by: (i) approximating the optimization-based controller by a pre-trained neural network to avoid solving optimization problems repeatedly, and (ii) using mixed monotone theory to construct an embedding system that leverages state-of-the-art neural network verification algorithms for bounding the output of the neural network. Results in terms of closeness of solutions of trajectories of the system with the optimization-based controller and the neural network are derived. Using a single trajectory of the embedding system along with our closeness of solutions result, we obtain an over-approximation of the reachable set of the system with optimization-based controllers. Numerical results are presented to corroborate the technical findings."
  },
  {
    "title": "InSPE: Rapid Evaluation of Heterogeneous Multi-Modal Infrastructure Sensor Placement",
    "url": "http://arxiv.org/abs/2504.08240v1",
    "arxiv_id": "2504.08240v1",
    "authors": [
      "Zhaoliang Zheng",
      "Yun Zhang",
      "Zongling Meng",
      "Johnson Liu",
      "Xin Xia",
      "Jiaqi Ma"
    ],
    "published": "2025-04-11T03:55:00+00:00",
    "summary": "Infrastructure sensing is vital for traffic monitoring at safety hotspots (e.g., intersections) and serves as the backbone of cooperative perception in autonomous driving. While vehicle sensing has been extensively studied, infrastructure sensing has received little attention, especially given the unique challenges of diverse intersection geometries, complex occlusions, varying traffic conditions, and ambient environments like lighting and weather. To address these issues and ensure cost-effective sensor placement, we propose Heterogeneous Multi-Modal Infrastructure Sensor Placement Evaluation (InSPE), a perception surrogate metric set that rapidly assesses perception effectiveness across diverse infrastructure and environmental scenarios with combinations of multi-modal sensors. InSPE systematically evaluates perception capabilities by integrating three carefully designed metrics, i.e., sensor coverage, perception occlusion, and information gain. To support large-scale evaluation, we develop a data generation tool within the CARLA simulator and also introduce Infra-Set, a dataset covering diverse intersection types and environmental conditions. Benchmarking experiments with state-of-the-art perception algorithms demonstrate that InSPE enables efficient and scalable sensor placement analysis, providing a robust solution for optimizing intelligent intersection infrastructure."
  },
  {
    "title": "Advancing Autonomous Vehicle Safety: A Combined Fault Tree Analysis and Bayesian Network Approach",
    "url": "http://arxiv.org/abs/2504.08206v1",
    "arxiv_id": "2504.08206v1",
    "authors": [
      "Lansu Dai",
      "Burak Kantarci"
    ],
    "published": "2025-04-11T02:17:10+00:00",
    "summary": "This paper integrates Fault Tree Analysis (FTA) and Bayesian Networks (BN) to assess collision risk and establish Automotive Safety Integrity Level (ASIL) B failure rate targets for critical autonomous vehicle (AV) components. The FTA-BN integration combines the systematic decomposition of failure events provided by FTA with the probabilistic reasoning capabilities of BN, which allow for dynamic updates in failure probabilities, enhancing the adaptability of risk assessment. A fault tree is constructed based on AV subsystem architecture, with collision as the top event, and failure rates are assigned while ensuring the total remains within 100 FIT. Bayesian inference is applied to update posterior probabilities, and the results indicate that perception system failures (46.06 FIT) are the most significant contributor, particularly failures to detect existing objects (PF5) and misclassification (PF6). Mitigation strategies are proposed for sensors, perception, decision-making, and motion control to reduce the collision risk. The FTA-BN integration approach provides dynamic risk quantification, offering system designers refined failure rate targets to improve AV safety."
  },
  {
    "title": "EO-VLM: VLM-Guided Energy Overload Attacks on Vision Models",
    "url": "http://arxiv.org/abs/2504.08205v1",
    "arxiv_id": "2504.08205v1",
    "authors": [
      "Minjae Seo",
      "Myoungsung You",
      "Junhee Lee",
      "Jaehan Kim",
      "Hwanjo Heo",
      "Jintae Oh",
      "Jinwoo Kim"
    ],
    "published": "2025-04-11T02:13:24+00:00",
    "summary": "Vision models are increasingly deployed in critical applications such as autonomous driving and CCTV monitoring, yet they remain susceptible to resource-consuming attacks. In this paper, we introduce a novel energy-overloading attack that leverages vision language model (VLM) prompts to generate adversarial images targeting vision models. These images, though imperceptible to the human eye, significantly increase GPU energy consumption across various vision models, threatening the availability of these systems. Our framework, EO-VLM (Energy Overload via VLM), is model-agnostic, meaning it is not limited by the architecture or type of the target vision model. By exploiting the lack of safety filters in VLMs like DALL-E 3, we create adversarial noise images without requiring prior knowledge or internal structure of the target vision models. Our experiments demonstrate up to a 50% increase in energy consumption, revealing a critical vulnerability in current vision models."
  },
  {
    "title": "SAEs $\\textit{Can}$ Improve Unlearning: Dynamic Sparse Autoencoder Guardrails for Precision Unlearning in LLMs",
    "url": "http://arxiv.org/abs/2504.08192v1",
    "arxiv_id": "2504.08192v1",
    "authors": [
      "Aashiq Muhamed",
      "Jacopo Bonato",
      "Mona Diab",
      "Virginia Smith"
    ],
    "published": "2025-04-11T01:24:03+00:00",
    "summary": "Machine unlearning is a promising approach to improve LLM safety by removing unwanted knowledge from the model. However, prevailing gradient-based unlearning methods suffer from issues such as high computational costs, hyperparameter instability, poor sequential unlearning capability, vulnerability to relearning attacks, low data efficiency, and lack of interpretability. While Sparse Autoencoders are well-suited to improve these aspects by enabling targeted activation-based unlearning, prior approaches underperform gradient-based methods. This work demonstrates that, contrary to these earlier findings, SAEs can significantly improve unlearning when employed dynamically. We introduce $\\textbf{Dynamic DAE Guardrails}$ (DSG), a novel method for precision unlearning that leverages principled feature selection and a dynamic classifier. Our experiments show DSG substantially outperforms leading unlearning methods, achieving superior forget-utility trade-offs. DSG addresses key drawbacks of gradient-based approaches for unlearning -- offering enhanced computational efficiency and stability, robust performance in sequential unlearning, stronger resistance to relearning attacks, better data efficiency including zero-shot settings, and more interpretable unlearning."
  },
  {
    "title": "Safe Data-Driven Predictive Control",
    "url": "http://arxiv.org/abs/2504.08188v1",
    "arxiv_id": "2504.08188v1",
    "authors": [
      "Amin Vahidi-Moghaddam",
      "Kaian Chen",
      "Kaixiang Zhang",
      "Zhaojian Li",
      "Yan Wang",
      "Kai Wu"
    ],
    "published": "2025-04-11T01:08:21+00:00",
    "summary": "In the realm of control systems, model predictive control (MPC) has exhibited remarkable potential; however, its reliance on accurate models and substantial computational resources has hindered its broader application, especially within real-time nonlinear systems. This study presents an innovative control framework to enhance the practical viability of the MPC. The developed safe data-driven predictive control aims to eliminate the requirement for precise models and alleviate computational burdens in the nonlinear MPC (NMPC). This is achieved by learning both the system dynamics and the control policy, enabling efficient data-driven predictive control while ensuring system safety. The methodology involves a spatial temporal filter (STF)-based concurrent learning for system identification, a robust control barrier function (RCBF) to ensure the system safety amid model uncertainties, and a RCBF-based NMPC policy approximation. An online policy correction mechanism is also introduced to counteract performance degradation caused by the existing model uncertainties. Demonstrated through simulations on two applications, the proposed approach offers comparable performance to existing benchmarks with significantly reduced computational costs."
  },
  {
    "title": "Investigating Vision-Language Model for Point Cloud-based Vehicle Classification",
    "url": "http://arxiv.org/abs/2504.08154v1",
    "arxiv_id": "2504.08154v1",
    "authors": [
      "Yiqiao Li",
      "Jie Wei",
      "Camille Kamga"
    ],
    "published": "2025-04-10T22:37:27+00:00",
    "summary": "Heavy-duty trucks pose significant safety challenges due to their large size and limited maneuverability compared to passenger vehicles. A deeper understanding of truck characteristics is essential for enhancing the safety perspective of cooperative autonomous driving. Traditional LiDAR-based truck classification methods rely on extensive manual annotations, which makes them labor-intensive and costly. The rapid advancement of large language models (LLMs) trained on massive datasets presents an opportunity to leverage their few-shot learning capabilities for truck classification. However, existing vision-language models (VLMs) are primarily trained on image datasets, which makes it challenging to directly process point cloud data. This study introduces a novel framework that integrates roadside LiDAR point cloud data with VLMs to facilitate efficient and accurate truck classification, which supports cooperative and safe driving environments. This study introduces three key innovations: (1) leveraging real-world LiDAR datasets for model development, (2) designing a preprocessing pipeline to adapt point cloud data for VLM input, including point cloud registration for dense 3D rendering and mathematical morphological techniques to enhance feature representation, and (3) utilizing in-context learning with few-shot prompting to enable vehicle classification with minimally labeled training data. Experimental results demonstrate encouraging performance of this method and present its potential to reduce annotation efforts while improving classification accuracy."
  },
  {
    "title": "Development and Performance Analysis of Glass-Based Gas-Tight RPCs for Muography Applications",
    "url": "http://arxiv.org/abs/2504.08146v1",
    "arxiv_id": "2504.08146v1",
    "authors": [
      "S. Ikram",
      "S. Basnet",
      "E. Cortina Gil",
      "P. Demin",
      "R. M. I. D. Gamage",
      "A. Giammanco",
      "R. Karnam",
      "V. K. S. Kashyap",
      "V. Kumar",
      "B. Mohanty",
      "M. Moussawi",
      "A. Samalan",
      "M. Tytgat"
    ],
    "published": "2025-04-10T22:14:32+00:00",
    "summary": "To achieve high-resolution muography of compact targets in scenarios with complex logistical constraints, we are developing a portable muon detector system utilizing glass Resistive Plate Chambers (RPCs). Although RPCs are well understood and widely used, our work focuses on developing a gas-tight variant specifically tailored for a broad range of muography applications, with key design goals including portability, robustness, autonomy, versatility, safety, and cost-effectiveness. Our RPC detectors are designed with various configurations, each featuring unique characteristics and performance attributes. We investigate the temporal evolution of the surface resistivity of glass electrodes, as well as the detector efficiency at varying voltages and thresholds, over a span of several months. These RPCs have been utilized in a small-scale feasibility study on muon absorption using lead blocks."
  },
  {
    "title": "Certified to Drive: A Policy Proposal for Mandatory Training on Semi-Automated Vehicles",
    "url": "http://arxiv.org/abs/2504.08128v1",
    "arxiv_id": "2504.08128v1",
    "authors": [
      "Soumita Mukherjee",
      "Varun Darshana Parekh",
      "Nikhil Tayal"
    ],
    "published": "2025-04-10T21:11:31+00:00",
    "summary": "Although the Boeing 737 Max incidents resulted from a mix of design shortcomings, regulatory oversights, and systemic issues, they also highlight a critical gap in pilot training on managing automated systems during abnormal conditions. This example demonstrates the urgent need for focused, concise training on human-automation interaction - a need that is equally critical for operators of Level 2 ADAS-equipped vehicles, as discussed in detail later in this article. The lack of structured education for semi-automated vehicle operators mirrors similar risks in other industries, where formal training is critical for safe operation. Two policy recommendations are proposed. First, governments should create concise, official resources in accessible and official format to educate drivers on system capabilities and limitations. Second, mandatory training and certification programs should be introduced, combining theoretical and hands-on components to prepare drivers for real-world scenarios. These measures will improve driver understanding, reduce misuse, and foster public trust in semi-automated vehicle technologies. By addressing the knowledge gap, policymakers can ensure a safer, more responsible transition to automation, maximizing its benefits while minimizing risks to public safety."
  },
  {
    "title": "DeepSeek vs. o3-mini: How Well can Reasoning LLMs Evaluate MT and Summarization?",
    "url": "http://arxiv.org/abs/2504.08120v1",
    "arxiv_id": "2504.08120v1",
    "authors": [
      "Daniil Larionov",
      "Sotaro Takeshita",
      "Ran Zhang",
      "Yanran Chen",
      "Christoph Leiter",
      "Zhipin Wang",
      "Christian Greisinger",
      "Steffen Eger"
    ],
    "published": "2025-04-10T20:39:18+00:00",
    "summary": "Reasoning-enabled large language models (LLMs) have recently demonstrated impressive performance in complex logical and mathematical tasks, yet their effectiveness in evaluating natural language generation remains unexplored. This study systematically compares reasoning-based LLMs (DeepSeek-R1 and OpenAI o3) with their non-reasoning counterparts across machine translation (MT) and text summarization (TS) evaluation tasks. We evaluate eight models across three architectural categories, including state-of-the-art reasoning models, their distilled variants (ranging from 8B to 70B parameters), and equivalent conventional, non-reasoning LLMs. Our experiments on WMT23 and SummEval benchmarks reveal that the benefits of reasoning capabilities are highly model and task-dependent: while OpenAI o3-mini models show consistent performance improvements with increased reasoning intensity, DeepSeek-R1 underperforms compared to its non-reasoning variant, with exception to certain aspects of TS evaluation. Correlation analysis demonstrates that increased reasoning token usage positively correlates with evaluation quality in o3-mini models. Furthermore, our results show that distillation of reasoning capabilities maintains reasonable performance in medium-sized models (32B) but degrades substantially in smaller variants (8B). This work provides the first comprehensive assessment of reasoning LLMs for NLG evaluation and offers insights into their practical use."
  },
  {
    "title": "Geneshift: Impact of different scenario shift on Jailbreaking LLM",
    "url": "http://arxiv.org/abs/2504.08104v1",
    "arxiv_id": "2504.08104v1",
    "authors": [
      "Tianyi Wu",
      "Zhiwei Xue",
      "Yue Liu",
      "Jiaheng Zhang",
      "Bryan Hooi",
      "See-Kiong Ng"
    ],
    "published": "2025-04-10T20:02:35+00:00",
    "summary": "Jailbreak attacks, which aim to cause LLMs to perform unrestricted behaviors, have become a critical and challenging direction in AI safety. Despite achieving the promising attack success rate using dictionary-based evaluation, existing jailbreak attack methods fail to output detailed contents to satisfy the harmful request, leading to poor performance on GPT-based evaluation. To this end, we propose a black-box jailbreak attack termed GeneShift, by using a genetic algorithm to optimize the scenario shifts. Firstly, we observe that the malicious queries perform optimally under different scenario shifts. Based on it, we develop a genetic algorithm to evolve and select the hybrid of scenario shifts. It guides our method to elicit detailed and actionable harmful responses while keeping the seemingly benign facade, improving stealthiness. Extensive experiments demonstrate the superiority of GeneShift. Notably, GeneShift increases the jailbreak success rate from 0% to 60% when direct prompting alone would fail."
  },
  {
    "title": "The AI Scientist-v2: Workshop-Level Automated Scientific Discovery via Agentic Tree Search",
    "url": "http://arxiv.org/abs/2504.08066v1",
    "arxiv_id": "2504.08066v1",
    "authors": [
      "Yutaro Yamada",
      "Robert Tjarko Lange",
      "Cong Lu",
      "Shengran Hu",
      "Chris Lu",
      "Jakob Foerster",
      "Jeff Clune",
      "David Ha"
    ],
    "published": "2025-04-10T18:44:41+00:00",
    "summary": "AI is increasingly playing a pivotal role in transforming how scientific discoveries are made. We introduce The AI Scientist-v2, an end-to-end agentic system capable of producing the first entirely AI generated peer-review-accepted workshop paper. This system iteratively formulates scientific hypotheses, designs and executes experiments, analyzes and visualizes data, and autonomously authors scientific manuscripts. Compared to its predecessor (v1, Lu et al., 2024 arXiv:2408.06292), The AI Scientist-v2 eliminates the reliance on human-authored code templates, generalizes effectively across diverse machine learning domains, and leverages a novel progressive agentic tree-search methodology managed by a dedicated experiment manager agent. Additionally, we enhance the AI reviewer component by integrating a Vision-Language Model (VLM) feedback loop for iterative refinement of content and aesthetics of the figures. We evaluated The AI Scientist-v2 by submitting three fully autonomous manuscripts to a peer-reviewed ICLR workshop. Notably, one manuscript achieved high enough scores to exceed the average human acceptance threshold, marking the first instance of a fully AI-generated paper successfully navigating a peer review. This accomplishment highlights the growing capability of AI in conducting all aspects of scientific research. We anticipate that further advancements in autonomous scientific discovery technologies will profoundly impact human knowledge generation, enabling unprecedented scalability in research productivity and significantly accelerating scientific breakthroughs, greatly benefiting society at large. We have open-sourced the code at https://github.com/SakanaAI/AI-Scientist-v2 to foster the future development of this transformative technology. We also discuss the role of AI in science, including AI safety."
  },
  {
    "title": "Can Reasoning LLMs Enhance Clinical Document Classification?",
    "url": "http://arxiv.org/abs/2504.08040v1",
    "arxiv_id": "2504.08040v1",
    "authors": [
      "Akram Mustafa",
      "Usman Naseem",
      "Mostafa Rahimi Azghadi"
    ],
    "published": "2025-04-10T18:00:27+00:00",
    "summary": "Clinical document classification is essential for converting unstructured medical texts into standardised ICD-10 diagnoses, yet it faces challenges due to complex medical language, privacy constraints, and limited annotated datasets. Large Language Models (LLMs) offer promising improvements in accuracy and efficiency for this task. This study evaluates the performance and consistency of eight LLMs; four reasoning (Qwen QWQ, Deepseek Reasoner, GPT o3 Mini, Gemini 2.0 Flash Thinking) and four non-reasoning (Llama 3.3, GPT 4o Mini, Gemini 2.0 Flash, Deepseek Chat); in classifying clinical discharge summaries using the MIMIC-IV dataset. Using cTAKES to structure clinical narratives, models were assessed across three experimental runs, with majority voting determining final predictions. Results showed that reasoning models outperformed non-reasoning models in accuracy (71% vs 68%) and F1 score (67% vs 60%), with Gemini 2.0 Flash Thinking achieving the highest accuracy (75%) and F1 score (76%). However, non-reasoning models demonstrated greater stability (91% vs 84% consistency). Performance varied across ICD-10 codes, with reasoning models excelling in complex cases but struggling with abstract categories. Findings indicate a trade-off between accuracy and consistency, suggesting that a hybrid approach could optimise clinical coding. Future research should explore multi-label classification, domain-specific fine-tuning, and ensemble methods to enhance model reliability in real-world applications."
  },
  {
    "title": "VCR-Bench: A Comprehensive Evaluation Framework for Video Chain-of-Thought Reasoning",
    "url": "http://arxiv.org/abs/2504.07956v1",
    "arxiv_id": "2504.07956v1",
    "authors": [
      "Yukun Qi",
      "Yiming Zhao",
      "Yu Zeng",
      "Xikun Bao",
      "Wenxuan Huang",
      "Lin Chen",
      "Zehui Chen",
      "Jie Zhao",
      "Zhongang Qi",
      "Feng Zhao"
    ],
    "published": "2025-04-10T17:59:03+00:00",
    "summary": "The advancement of Chain-of-Thought (CoT) reasoning has significantly enhanced the capabilities of large language models (LLMs) and large vision-language models (LVLMs). However, a rigorous evaluation framework for video CoT reasoning remains absent. Current video benchmarks fail to adequately assess the reasoning process and expose whether failures stem from deficiencies in perception or reasoning capabilities. Therefore, we introduce VCR-Bench, a novel benchmark designed to comprehensively evaluate LVLMs' Video Chain-of-Thought Reasoning capabilities. VCR-Bench comprises 859 videos spanning a variety of video content and durations, along with 1,034 high-quality question-answer pairs. Each pair is manually annotated with a stepwise CoT rationale, where every step is tagged to indicate its association with the perception or reasoning capabilities. Furthermore, we design seven distinct task dimensions and propose the CoT score to assess the entire CoT process based on the stepwise tagged CoT rationals. Extensive experiments on VCR-Bench highlight substantial limitations in current LVLMs. Even the top-performing model, o1, only achieves a 62.8% CoT score and an 56.7% accuracy, while most models score below 40%. Experiments show most models score lower on perception than reasoning steps, revealing LVLMs' key bottleneck in temporal-spatial information processing for complex video reasoning. A robust positive correlation between the CoT score and accuracy confirms the validity of our evaluation framework and underscores the critical role of CoT reasoning in solving complex video reasoning tasks. We hope VCR-Bench to serve as a standardized evaluation framework and expose the actual drawbacks in complex video reasoning task."
  },
  {
    "title": "Perception-R1: Pioneering Perception Policy with Reinforcement Learning",
    "url": "http://arxiv.org/abs/2504.07954v1",
    "arxiv_id": "2504.07954v1",
    "authors": [
      "En Yu",
      "Kangheng Lin",
      "Liang Zhao",
      "Jisheng Yin",
      "Yana Wei",
      "Yuang Peng",
      "Haoran Wei",
      "Jianjian Sun",
      "Chunrui Han",
      "Zheng Ge",
      "Xiangyu Zhang",
      "Daxin Jiang",
      "Jingyu Wang",
      "Wenbing Tao"
    ],
    "published": "2025-04-10T17:58:27+00:00",
    "summary": "Inspired by the success of DeepSeek-R1, we explore the potential of rule-based reinforcement learning (RL) in MLLM post-training for perception policy learning. While promising, our initial experiments reveal that incorporating a thinking process through RL does not consistently lead to performance gains across all visual perception tasks. This leads us to delve into the essential role of RL in the context of visual perception. In this work, we return to the fundamentals and explore the effects of RL on different perception tasks. We observe that the perceptual complexity is a major factor in determining the effectiveness of RL. We also observe that reward design plays a crucial role in further approching the upper limit of model perception. To leverage these findings, we propose Perception-R1, a scalable RL framework using GRPO during MLLM post-training. With a standard Qwen2.5-VL-3B-Instruct, Perception-R1 achieves +4.2% on RefCOCO+, +17.9% on PixMo-Count, +4.2% on PageOCR, and notably, 31.9% AP on COCO2017 val for the first time, establishing a strong baseline for perception policy learning."
  },
  {
    "title": "SoTA with Less: MCTS-Guided Sample Selection for Data-Efficient Visual Reasoning Self-Improvement",
    "url": "http://arxiv.org/abs/2504.07934v1",
    "arxiv_id": "2504.07934v1",
    "authors": [
      "Xiyao Wang",
      "Zhengyuan Yang",
      "Chao Feng",
      "Hongjin Lu",
      "Linjie Li",
      "Chung-Ching Lin",
      "Kevin Lin",
      "Furong Huang",
      "Lijuan Wang"
    ],
    "published": "2025-04-10T17:49:05+00:00",
    "summary": "In this paper, we present an effective method to enhance visual reasoning with significantly fewer training samples, relying purely on self-improvement with no knowledge distillation. Our key insight is that the difficulty of training data during reinforcement fine-tuning (RFT) is critical. Appropriately challenging samples can substantially boost reasoning capabilities even when the dataset is small. Despite being intuitive, the main challenge remains in accurately quantifying sample difficulty to enable effective data filtering. To this end, we propose a novel way of repurposing Monte Carlo Tree Search (MCTS) to achieve that. Starting from our curated 70k open-source training samples, we introduce an MCTS-based selection method that quantifies sample difficulty based on the number of iterations required by the VLMs to solve each problem. This explicit step-by-step reasoning in MCTS enforces the model to think longer and better identifies samples that are genuinely challenging. We filter and retain 11k samples to perform RFT on Qwen2.5-VL-7B-Instruct, resulting in our final model, ThinkLite-VL. Evaluation results on eight benchmarks show that ThinkLite-VL improves the average performance of Qwen2.5-VL-7B-Instruct by 7%, using only 11k training samples with no knowledge distillation. This significantly outperforms all existing 7B-level reasoning VLMs, and our fairly comparable baselines that use classic selection methods such as accuracy-based filtering. Notably, on MathVista, ThinkLite-VL-7B achieves the SoTA accuracy of 75.1, surpassing Qwen2.5-VL-72B, GPT-4o, and O1. Our code, data, and model are available at https://github.com/si0wang/ThinkLite-VL."
  },
  {
    "title": "Benchmarking Adversarial Robustness to Bias Elicitation in Large Language Models: Scalable Automated Assessment with LLM-as-a-Judge",
    "url": "http://arxiv.org/abs/2504.07887v1",
    "arxiv_id": "2504.07887v1",
    "authors": [
      "Riccardo Cantini",
      "Alessio Orsino",
      "Massimo Ruggiero",
      "Domenico Talia"
    ],
    "published": "2025-04-10T16:00:59+00:00",
    "summary": "Large Language Models (LLMs) have revolutionized artificial intelligence, driving advancements in machine translation, summarization, and conversational agents. However, their increasing integration into critical societal domains has raised concerns about embedded biases, which can perpetuate stereotypes and compromise fairness. These biases stem from various sources, including historical inequalities in training data, linguistic imbalances, and adversarial manipulation. Despite mitigation efforts, recent studies indicate that LLMs remain vulnerable to adversarial attacks designed to elicit biased responses. This work proposes a scalable benchmarking framework to evaluate LLM robustness against adversarial bias elicitation. Our methodology involves (i) systematically probing models with a multi-task approach targeting biases across various sociocultural dimensions, (ii) quantifying robustness through safety scores using an LLM-as-a-Judge approach for automated assessment of model responses, and (iii) employing jailbreak techniques to investigate vulnerabilities in safety mechanisms. Our analysis examines prevalent biases in both small and large state-of-the-art models and their impact on model safety. Additionally, we assess the safety of domain-specific models fine-tuned for critical fields, such as medicine. Finally, we release a curated dataset of bias-related prompts, CLEAR-Bias, to facilitate systematic vulnerability benchmarking. Our findings reveal critical trade-offs between model size and safety, aiding the development of fairer and more robust future language models."
  },
  {
    "title": "Gauge and parametrization dependence of Quantum Einstein Gravity within the Proper Time flow",
    "url": "http://arxiv.org/abs/2504.07877v1",
    "arxiv_id": "2504.07877v1",
    "authors": [
      "Alfio Bonanno",
      "Giovanni Oglialoro",
      "Dario Zappal\u00e0"
    ],
    "published": "2025-04-10T15:52:31+00:00",
    "summary": "Proper time functional flow equations have garnered significant attention in recent years, as they are particularly suitable in analyzing non-perturbative contexts. By resorting to this flow, we investigate the regulator and gauge dependence in quantum Einstein gravity within the asymptotic safety framework, considering various regularization schemes. Our findings indicate that some details of the regulator have minor influence on the critical properties of the theory. In contrast, the selection between linear and exponential parametrizations appears to have a more substantial impact on the scaling behavior of the renormalized flow near the non-Gaussian fixed point."
  },
  {
    "title": "Pangu Ultra: Pushing the Limits of Dense Large Language Models on Ascend NPUs",
    "url": "http://arxiv.org/abs/2504.07866v1",
    "arxiv_id": "2504.07866v1",
    "authors": [
      "Yichun Yin",
      "Wenyong Huang",
      "Kaikai Song",
      "Yehui Tang",
      "Xueyu Wu",
      "Wei Guo",
      "Peng Guo",
      "Yaoyuan Wang",
      "Xiaojun Meng",
      "Yasheng Wang",
      "Dong Li",
      "Can Chen",
      "Dandan Tu",
      "Yin Li",
      "Fisher Yu",
      "Ruiming Tang",
      "Yunhe Wang",
      "Baojun Wang",
      "Bin Wang",
      "Bo Wang",
      "Boxiao Liu",
      "Changzheng Zhang",
      "Duyu Tang",
      "Fei Mi",
      "Hui Jin",
      "Jiansheng Wei",
      "Jiarui Qin",
      "Jinpeng Li",
      "Jun Zhao",
      "Liqun Deng",
      "Lin Li",
      "Minghui Xu",
      "Naifu Zhang",
      "Nianzu Zheng",
      "Qiang Li",
      "Rongju Ruan",
      "Shengjun Cheng",
      "Tianyu Guo",
      "Wei He",
      "Wei Li",
      "Weiwen Liu",
      "Wulong Liu",
      "Xinyi Dai",
      "Yonghan Dong",
      "Yu Pan",
      "Yue Li",
      "Yufei Wang",
      "Yujun Li",
      "Yunsheng Ni",
      "Zhe Liu",
      "Zhenhe Zhang",
      "Zhicheng Liu"
    ],
    "published": "2025-04-10T15:41:51+00:00",
    "summary": "We present Pangu Ultra, a Large Language Model (LLM) with 135 billion parameters and dense Transformer modules trained on Ascend Neural Processing Units (NPUs). Although the field of LLM has been witnessing unprecedented advances in pushing the scale and capability of LLM in recent years, training such a large-scale model still involves significant optimization and system challenges. To stabilize the training process, we propose depth-scaled sandwich normalization, which effectively eliminates loss spikes during the training process of deep models. We pre-train our model on 13.2 trillion diverse and high-quality tokens and further enhance its reasoning capabilities during post-training. To perform such large-scale training efficiently, we utilize 8,192 Ascend NPUs with a series of system optimizations. Evaluations on multiple diverse benchmarks indicate that Pangu Ultra significantly advances the state-of-the-art capabilities of dense LLMs such as Llama 405B and Mistral Large 2, and even achieves competitive results with DeepSeek-R1, whose sparse model structure contains much more parameters. Our exploration demonstrates that Ascend NPUs are capable of efficiently and effectively training dense models with more than 100 billion parameters. Our model and system will be available for our commercial customers."
  },
  {
    "title": "Pangu Ultra: Pushing the Limits of Dense Large Language Models on Ascend NPUs",
    "url": "http://arxiv.org/abs/2504.07866v2",
    "arxiv_id": "2504.07866v2",
    "authors": [
      "Yichun Yin",
      "Wenyong Huang",
      "Kaikai Song",
      "Yehui Tang",
      "Xueyu Wu",
      "Wei Guo",
      "Peng Guo",
      "Yaoyuan Wang",
      "Xiaojun Meng",
      "Yasheng Wang",
      "Dong Li",
      "Can Chen",
      "Dandan Tu",
      "Yin Li",
      "Fisher Yu",
      "Ruiming Tang",
      "Yunhe Wang",
      "Baojun Wang",
      "Bin Wang",
      "Bo Wang",
      "Boxiao Liu",
      "Changzheng Zhang",
      "Duyu Tang",
      "Fei Mi",
      "Hui Jin",
      "Jiansheng Wei",
      "Jiarui Qin",
      "Jinpeng Li",
      "Jun Zhao",
      "Liqun Deng",
      "Lin Li",
      "Minghui Xu",
      "Naifu Zhang",
      "Nianzu Zheng",
      "Qiang Li",
      "Rongju Ruan",
      "Shengjun Cheng",
      "Tianyu Guo",
      "Wei He",
      "Wei Li",
      "Weiwen Liu",
      "Wulong Liu",
      "Xinyi Dai",
      "Yonghan Dong",
      "Yu Pan",
      "Yue Li",
      "Yufei Wang",
      "Yujun Li",
      "Yunsheng Ni",
      "Zhe Liu",
      "Zhenhe Zhang",
      "Zhicheng Liu"
    ],
    "published": "2025-04-10T15:41:51+00:00",
    "summary": "We present Pangu Ultra, a Large Language Model (LLM) with 135 billion parameters and dense Transformer modules trained on Ascend Neural Processing Units (NPUs). Although the field of LLM has been witnessing unprecedented advances in pushing the scale and capability of LLM in recent years, training such a large-scale model still involves significant optimization and system challenges. To stabilize the training process, we propose depth-scaled sandwich normalization, which effectively eliminates loss spikes during the training process of deep models. We pre-train our model on 13.2 trillion diverse and high-quality tokens and further enhance its reasoning capabilities during post-training. To perform such large-scale training efficiently, we utilize 8,192 Ascend NPUs with a series of system optimizations. Evaluations on multiple diverse benchmarks indicate that Pangu Ultra significantly advances the state-of-the-art capabilities of dense LLMs such as Llama 405B and Mistral Large 2, and even achieves competitive results with DeepSeek-R1, whose sparse model structure contains much more parameters. Our exploration demonstrates that Ascend NPUs are capable of efficiently and effectively training dense models with more than 100 billion parameters. Our model and system will be available for our commercial customers."
  },
  {
    "title": "Robust Hallucination Detection in LLMs via Adaptive Token Selection",
    "url": "http://arxiv.org/abs/2504.07863v1",
    "arxiv_id": "2504.07863v1",
    "authors": [
      "Mengjia Niu",
      "Hamed Haddadi",
      "Guansong Pang"
    ],
    "published": "2025-04-10T15:39:10+00:00",
    "summary": "Hallucinations in large language models (LLMs) pose significant safety concerns that impede their broader deployment. Recent research in hallucination detection has demonstrated that LLMs' internal representations contain truthfulness hints, which can be harnessed for detector training. However, the performance of these detectors is heavily dependent on the internal representations of predetermined tokens, fluctuating considerably when working on free-form generations with varying lengths and sparse distributions of hallucinated entities. To address this, we propose HaMI, a novel approach that enables robust detection of hallucinations through adaptive selection and learning of critical tokens that are most indicative of hallucinations. We achieve this robustness by an innovative formulation of the Hallucination detection task as Multiple Instance (HaMI) learning over token-level representations within a sequence, thereby facilitating a joint optimisation of token selection and hallucination detection on generation sequences of diverse forms. Comprehensive experimental results on four hallucination benchmarks show that HaMI significantly outperforms existing state-of-the-art approaches."
  },
  {
    "title": "Deceptive Automated Interpretability: Language Models Coordinating to Fool Oversight Systems",
    "url": "http://arxiv.org/abs/2504.07831v1",
    "arxiv_id": "2504.07831v1",
    "authors": [
      "Simon Lermen",
      "Mateusz Dziemian",
      "Natalia P\u00e9rez-Campanero Antol\u00edn"
    ],
    "published": "2025-04-10T15:07:10+00:00",
    "summary": "We demonstrate how AI agents can coordinate to deceive oversight systems using automated interpretability of neural networks. Using sparse autoencoders (SAEs) as our experimental framework, we show that language models (Llama, DeepSeek R1, and Claude 3.7 Sonnet) can generate deceptive explanations that evade detection. Our agents employ steganographic methods to hide information in seemingly innocent explanations, successfully fooling oversight models while achieving explanation quality comparable to reference labels. We further find that models can scheme to develop deceptive strategies when they believe the detection of harmful features might lead to negative consequences for themselves. All tested LLM agents were capable of deceiving the overseer while achieving high interpretability scores comparable to those of reference labels. We conclude by proposing mitigation strategies, emphasizing the critical need for robust understanding and defenses against deception."
  },
  {
    "title": "Revisiting Likelihood-Based Out-of-Distribution Detection by Modeling Representations",
    "url": "http://arxiv.org/abs/2504.07793v1",
    "arxiv_id": "2504.07793v1",
    "authors": [
      "Yifan Ding",
      "Arturas Aleksandrauskas",
      "Amirhossein Ahmadian",
      "Jonas Unger",
      "Fredrik Lindsten",
      "Gabriel Eilertsen"
    ],
    "published": "2025-04-10T14:30:41+00:00",
    "summary": "Out-of-distribution (OOD) detection is critical for ensuring the reliability of deep learning systems, particularly in safety-critical applications. Likelihood-based deep generative models have historically faced criticism for their unsatisfactory performance in OOD detection, often assigning higher likelihood to OOD data than in-distribution samples when applied to image data. In this work, we demonstrate that likelihood is not inherently flawed. Rather, several properties in the images space prohibit likelihood as a valid detection score. Given a sufficiently good likelihood estimator, specifically using the probability flow formulation of a diffusion model, we show that likelihood-based methods can still perform on par with state-of-the-art methods when applied in the representation space of pre-trained encoders. The code of our work can be found at $\\href{https://github.com/limchaos/Likelihood-OOD.git}{\\texttt{https://github.com/limchaos/Likelihood-OOD.git}}$."
  },
  {
    "title": "Realigning Incentives to Build Better Software: a Holistic Approach to Vendor Accountability",
    "url": "http://arxiv.org/abs/2504.07766v1",
    "arxiv_id": "2504.07766v1",
    "authors": [
      "Gergely Bicz\u00f3k",
      "Sasha Romanosky",
      "Mingyan Liu"
    ],
    "published": "2025-04-10T14:05:24+00:00",
    "summary": "In this paper, we ask the question of why the quality of commercial software, in terms of security and safety, does not measure up to that of other (durable) consumer goods we have come to expect. We examine this question through the lens of incentives. We argue that the challenge around better quality software is due in no small part to a sequence of misaligned incentives, the most critical of which being that the harm caused by software problems is by and large shouldered by consumers, not developers. This lack of liability means software vendors have every incentive to rush low-quality software onto the market and no incentive to enhance quality control. Within this context, this paper outlines a holistic technical and policy framework we believe is needed to incentivize better and more secure software development. At the heart of the incentive realignment is the concept of software liability. This framework touches on various components, including legal, technical, and financial, that are needed for software liability to work in practice; some currently exist, some will need to be re-imagined or established. This is primarily a market-driven approach that emphasizes voluntary participation but highlights the role appropriate regulation can play. We connect and contrast this with the EU legal environment and discuss what this framework means for open-source software (OSS) development and emerging AI risks. Moreover, we present a CrowdStrike case study complete with a what-if analysis had our proposed framework been in effect. Our intention is very much to stimulate a robust conversation among both researchers and practitioners."
  },
  {
    "title": "Optimal Frequency Support from Virtual Power Plants: Minimal Reserve and Allocation",
    "url": "http://arxiv.org/abs/2504.07703v1",
    "arxiv_id": "2504.07703v1",
    "authors": [
      "Xiang Zhu",
      "Guangchun Ruan",
      "Hua Geng"
    ],
    "published": "2025-04-10T12:43:38+00:00",
    "summary": "This paper proposes a novel reserve-minimizing and allocation strategy for virtual power plants (VPPs) to deliver optimal frequency support. The proposed strategy enables VPPs, acting as aggregators for inverter-based resources (IBRs), to provide optimal frequency support economically. The proposed strategy captures time-varying active power injections, reducing the unnecessary redundancy compared to traditional fixed reserve schemes. Reserve requirements for the VPPs are determined based on system frequency response and safety constraints, ensuring efficient grid support. Furthermore, an energy-based allocation model decomposes power injections for each IBR, accounting for their specific limitations. Numerical experiments validate the feasibility of the proposed approach, highlighting significant financial gains for VPPs, especially as system inertia decreases due to higher renewable energy integration."
  },
  {
    "title": "Joint Travel Route Optimization Framework for Platooning",
    "url": "http://arxiv.org/abs/2504.07623v1",
    "arxiv_id": "2504.07623v1",
    "authors": [
      "Akif Adas",
      "Stefano Arrigoni",
      "Mattia Brambilla",
      "Monica Barbara Nicoli",
      "Edoardo Sabbioni"
    ],
    "published": "2025-04-10T10:13:20+00:00",
    "summary": "Platooning represents an advanced driving technology designed to assist drivers in traffic convoys of varying lengths, enhancing road safety, reducing driver fatigue, and improving fuel efficiency. Sophisticated automated driving assistance systems have facilitated this innovation. Recent advancements in platooning emphasize cooperative mechanisms within both centralized and decentralized architectures enabled by vehicular communication technologies. This study introduces a cooperative route planning optimization framework aimed at promoting the adoption of platooning through a centralized platoon formation strategy at the system level. This approach is envisioned as a transitional phase from individual (ego) driving to fully collaborative driving. Additionally, this research formulates and incorporates travel cost metrics related to fuel consumption, driver fatigue, and travel time, considering regulatory constraints on consecutive driving durations. The performance of these cost metrics has been evaluated using Dijkstra's and A* shortest path algorithms within a network graph framework. The results indicate that the proposed architecture achieves an average cost improvement of 14 % compared to individual route planning for long road trips."
  },
  {
    "title": "VLM-R1: A Stable and Generalizable R1-style Large Vision-Language Model",
    "url": "http://arxiv.org/abs/2504.07615v1",
    "arxiv_id": "2504.07615v1",
    "authors": [
      "Haozhan Shen",
      "Peng Liu",
      "Jingcheng Li",
      "Chunxin Fang",
      "Yibo Ma",
      "Jiajia Liao",
      "Qiaoli Shen",
      "Zilun Zhang",
      "Kangjia Zhao",
      "Qianqian Zhang",
      "Ruochen Xu",
      "Tiancheng Zhao"
    ],
    "published": "2025-04-10T10:05:15+00:00",
    "summary": "Recently DeepSeek R1 has shown that reinforcement learning (RL) can substantially improve the reasoning capabilities of Large Language Models (LLMs) through a simple yet effective design. The core of R1 lies in its rule-based reward formulation, which leverages tasks with deterministic ground-truth answers to enable precise and stable reward computation. In the visual domain, we similarly observe that a wide range of visual understanding tasks are inherently equipped with well-defined ground-truth annotations. This property makes them naturally compatible with rule-based reward mechanisms. Motivated by this observation, we investigate the extension of R1-style reinforcement learning to Vision-Language Models (VLMs), aiming to enhance their visual reasoning capabilities. To this end, we develop VLM-R1, a dedicated framework designed to harness RL for improving VLMs' performance on general vision-language tasks. Using this framework, we further explore the feasibility of applying RL to visual domain. Experimental results indicate that the RL-based model not only delivers competitive performance on visual understanding tasks but also surpasses Supervised Fine-Tuning (SFT) in generalization ability. Furthermore, we conduct comprehensive ablation studies that uncover a series of noteworthy insights, including the presence of reward hacking in object detection, the emergence of the \"OD aha moment\", the impact of training data quality, and the scaling behavior of RL across different model sizes. Through these analyses, we aim to deepen the understanding of how reinforcement learning enhances the capabilities of vision-language models, and we hope our findings and open-source contributions will support continued progress in the vision-language RL community. Our code and model are available at https://github.com/om-ai-lab/VLM-R1"
  },
  {
    "title": "VLM-R1: A Stable and Generalizable R1-style Large Vision-Language Model",
    "url": "http://arxiv.org/abs/2504.07615v2",
    "arxiv_id": "2504.07615v2",
    "authors": [
      "Haozhan Shen",
      "Peng Liu",
      "Jingcheng Li",
      "Chunxin Fang",
      "Yibo Ma",
      "Jiajia Liao",
      "Qiaoli Shen",
      "Zilun Zhang",
      "Kangjia Zhao",
      "Qianqian Zhang",
      "Ruochen Xu",
      "Tiancheng Zhao"
    ],
    "published": "2025-04-10T10:05:15+00:00",
    "summary": "Recently DeepSeek R1 has shown that reinforcement learning (RL) can substantially improve the reasoning capabilities of Large Language Models (LLMs) through a simple yet effective design. The core of R1 lies in its rule-based reward formulation, which leverages tasks with deterministic ground-truth answers to enable precise and stable reward computation. In the visual domain, we similarly observe that a wide range of visual understanding tasks are inherently equipped with well-defined ground-truth annotations. This property makes them naturally compatible with rule-based reward mechanisms. Motivated by this observation, we investigate the extension of R1-style reinforcement learning to Vision-Language Models (VLMs), aiming to enhance their visual reasoning capabilities. To this end, we develop VLM-R1, a dedicated framework designed to harness RL for improving VLMs' performance on general vision-language tasks. Using this framework, we further explore the feasibility of applying RL to visual domain. Experimental results indicate that the RL-based model not only delivers competitive performance on visual understanding tasks but also surpasses Supervised Fine-Tuning (SFT) in generalization ability. Furthermore, we conduct comprehensive ablation studies that uncover a series of noteworthy insights, including the presence of reward hacking in object detection, the emergence of the \"OD aha moment\", the impact of training data quality, and the scaling behavior of RL across different model sizes. Through these analyses, we aim to deepen the understanding of how reinforcement learning enhances the capabilities of vision-language models, and we hope our findings and open-source contributions will support continued progress in the vision-language RL community. Our code and model are available at https://github.com/om-ai-lab/VLM-R1"
  },
  {
    "title": "Drive in Corridors: Enhancing the Safety of End-to-end Autonomous Driving via Corridor Learning and Planning",
    "url": "http://arxiv.org/abs/2504.07507v1",
    "arxiv_id": "2504.07507v1",
    "authors": [
      "Zhiwei Zhang",
      "Ruichen Yang",
      "Ke Wu",
      "Zijun Xu",
      "Jingchu Liu",
      "Lisen Mu",
      "Zhongxue Gan",
      "Wenchao Ding"
    ],
    "published": "2025-04-10T07:10:40+00:00",
    "summary": "Safety remains one of the most critical challenges in autonomous driving systems. In recent years, the end-to-end driving has shown great promise in advancing vehicle autonomy in a scalable manner. However, existing approaches often face safety risks due to the lack of explicit behavior constraints. To address this issue, we uncover a new paradigm by introducing the corridor as the intermediate representation. Widely adopted in robotics planning, the corridors represents spatio-temporal obstacle-free zones for the vehicle to traverse. To ensure accurate corridor prediction in diverse traffic scenarios, we develop a comprehensive learning pipeline including data annotation, architecture refinement and loss formulation. The predicted corridor is further integrated as the constraint in a trajectory optimization process. By extending the differentiability of the optimization, we enable the optimized trajectory to be seamlessly trained within the end-to-end learning framework, improving both safety and interpretability. Experimental results on the nuScenes dataset demonstrate state-of-the-art performance of our approach, showing a 66.7% reduction in collisions with agents and a 46.5% reduction with curbs, significantly enhancing the safety of end-to-end driving. Additionally, incorporating the corridor contributes to higher success rates in closed-loop evaluations."
  },
  {
    "title": "Defense against Prompt Injection Attacks via Mixture of Encodings",
    "url": "http://arxiv.org/abs/2504.07467v1",
    "arxiv_id": "2504.07467v1",
    "authors": [
      "Ruiyi Zhang",
      "David Sullivan",
      "Kyle Jackson",
      "Pengtao Xie",
      "Mei Chen"
    ],
    "published": "2025-04-10T05:35:21+00:00",
    "summary": "Large Language Models (LLMs) have emerged as a dominant approach for a wide range of NLP tasks, with their access to external information further enhancing their capabilities. However, this introduces new vulnerabilities, known as prompt injection attacks, where external content embeds malicious instructions that manipulate the LLM's output. Recently, the Base64 defense has been recognized as one of the most effective methods for reducing success rate of prompt injection attacks. Despite its efficacy, this method can degrade LLM performance on certain NLP tasks. To address this challenge, we propose a novel defense mechanism: mixture of encodings, which utilizes multiple character encodings, including Base64. Extensive experimental results show that our method achieves one of the lowest attack success rates under prompt injection attacks, while maintaining high performance across all NLP tasks, outperforming existing character encoding-based defense methods. This underscores the effectiveness of our mixture of encodings strategy for both safety and task performance metrics."
  },
  {
    "title": "Multi-Modal Data Fusion for Moisture Content Prediction in Apple Drying",
    "url": "http://arxiv.org/abs/2504.07465v1",
    "arxiv_id": "2504.07465v1",
    "authors": [
      "Shichen Li",
      "Chenhui Shao"
    ],
    "published": "2025-04-10T05:29:04+00:00",
    "summary": "Fruit drying is widely used in food manufacturing to reduce product moisture, ensure product safety, and extend product shelf life. Accurately predicting final moisture content (MC) is critically needed for quality control of drying processes. State-of-the-art methods can build deterministic relationships between process parameters and MC, but cannot adequately account for inherent process variabilities that are ubiquitous in fruit drying. To address this gap, this paper presents a novel multi-modal data fusion framework to effectively fuse two modalities of data: tabular data (process parameters) and high-dimensional image data (images of dried apple slices) to enable accurate MC prediction. The proposed modeling architecture permits flexible adjustment of information portion from tabular and image data modalities. Experimental validation shows that the multi-modal approach improves predictive accuracy substantially compared to state-of-the-art methods. The proposed method reduces root-mean-squared errors by 19.3%, 24.2%, and 15.2% over tabular-only, image-only, and standard tabular-image fusion models, respectively. Furthermore, it is demonstrated that our method is robust in varied tabular-image ratios and capable of effectively capturing inherent small-scale process variabilities. The proposed framework is extensible to a variety of other drying technologies."
  },
  {
    "title": "LoRI: Reducing Cross-Task Interference in Multi-Task Low-Rank Adaptation",
    "url": "http://arxiv.org/abs/2504.07448v1",
    "arxiv_id": "2504.07448v1",
    "authors": [
      "Juzheng Zhang",
      "Jiacheng You",
      "Ashwinee Panda",
      "Tom Goldstein"
    ],
    "published": "2025-04-10T04:46:04+00:00",
    "summary": "Low-Rank Adaptation (LoRA) has emerged as a popular parameter-efficient fine-tuning (PEFT) method for Large Language Models (LLMs), yet it still incurs notable overhead and suffers from parameter interference in multi-task scenarios. We propose LoRA with Reduced Interference (LoRI), a simple yet effective approach that freezes the projection matrices $A$ as random projections and sparsifies the matrices $B$ using task-specific masks. This design substantially reduces the number of trainable parameters while maintaining strong task performance. Moreover, LoRI minimizes cross-task interference in adapter merging by leveraging the orthogonality between adapter subspaces, and supports continual learning by using sparsity to mitigate catastrophic forgetting. Extensive experiments across natural language understanding, mathematical reasoning, code generation, and safety alignment tasks demonstrate that LoRI outperforms full fine-tuning and existing PEFT methods, while using up to 95% fewer trainable parameters than LoRA. In multi-task experiments, LoRI enables effective adapter merging and continual learning with reduced cross-task interference. Code is available at: https://github.com/juzhengz/LoRI"
  },
  {
    "title": "Electronic Warfare Cyberattacks, Countermeasures and Modern Defensive Strategies of UAV Avionics: A Survey",
    "url": "http://arxiv.org/abs/2504.07358v1",
    "arxiv_id": "2504.07358v1",
    "authors": [
      "Aaron Yu",
      "Iuliia Kolotylo",
      "Hashim A. Hashim",
      "A. E. E. Eltoukhy"
    ],
    "published": "2025-04-10T00:56:52+00:00",
    "summary": "Unmanned Aerial Vehicles (UAVs) play a pivotal role in modern autonomous air mobility, and the reliability of UAV avionics systems is critical to ensuring mission success, sustainability practices, and public safety. The success of UAV missions depends on effectively mitigating various aspects of electronic warfare, including non-destructive and destructive cyberattacks, transponder vulnerabilities, and jamming threats, while rigorously implementing countermeasures and defensive aids. This paper provides a comprehensive review of UAV cyberattacks, countermeasures, and defensive strategies. It explores UAV-to-UAV coordination attacks and their associated features, such as dispatch system attacks, Automatic Dependent Surveillance-Broadcast (ADS-B) attacks, Traffic Alert and Collision Avoidance System (TCAS)-induced collisions, and TCAS attacks. Additionally, the paper examines UAV-to-command center coordination attacks, as well as UAV functionality attacks. The review also covers various countermeasures and defensive aids designed for UAVs. Lastly, a comparison of common cyberattacks and countermeasure approaches is conducted, along with a discussion of future trends in the field. Keywords: Electronic warfare, UAVs, Avionics Systems, cyberattacks, coordination attacks, functionality attacks, countermeasure, defensive-aids."
  },
  {
    "title": "Revisiting Prompt Optimization with Large Reasoning Models-A Case Study on Event Extraction",
    "url": "http://arxiv.org/abs/2504.07357v1",
    "arxiv_id": "2504.07357v1",
    "authors": [
      "Saurabh Srivastava",
      "Ziyu Yao"
    ],
    "published": "2025-04-10T00:53:59+00:00",
    "summary": "Large Reasoning Models (LRMs) such as DeepSeek-R1 and OpenAI o1 have demonstrated remarkable capabilities in various reasoning tasks. Their strong capability to generate and reason over intermediate thoughts has also led to arguments that they may no longer require extensive prompt engineering or optimization to interpret human instructions and produce accurate outputs. In this work, we aim to systematically study this open question, using the structured task of event extraction for a case study. We experimented with two LRMs (DeepSeek-R1 and o1) and two general-purpose Large Language Models (LLMs) (GPT-4o and GPT-4.5), when they were used as task models or prompt optimizers. Our results show that on tasks as complicated as event extraction, LRMs as task models still benefit from prompt optimization, and that using LRMs as prompt optimizers yields more effective prompts. Finally, we provide an error analysis of common errors made by LRMs and highlight the stability and consistency of LRMs in refining task instructions and event guidelines."
  },
  {
    "title": "Code Generation with Small Language Models: A Deep Evaluation on Codeforces",
    "url": "http://arxiv.org/abs/2504.07343v1",
    "arxiv_id": "2504.07343v1",
    "authors": [
      "D\u00e9bora Souza",
      "Rohit Gheyi",
      "Lucas Albuquerque",
      "Gustavo Soares",
      "M\u00e1rcio Ribeiro"
    ],
    "published": "2025-04-09T23:57:44+00:00",
    "summary": "Large Language Models (LLMs) have demonstrated capabilities in code generation, potentially boosting developer productivity. However, their widespread adoption remains limited by high computational costs, significant energy demands, and security risks such as data leakage and adversarial attacks. As a lighter-weight alternative, Small Language Models (SLMs) offer faster inference, lower deployment overhead, and better adaptability to domain-specific tasks, making them an attractive option for real-world applications. While prior research has benchmarked LLMs on competitive programming tasks, such evaluations often focus narrowly on metrics like Elo scores or pass rates, overlooking deeper insights into model behavior, failure patterns, and problem diversity. Furthermore, the potential of SLMs to tackle complex tasks such as competitive programming remains underexplored. In this study, we benchmark five open SLMs - LLAMA 3.2 3B, GEMMA 2 9B, GEMMA 3 12B, DEEPSEEK-R1 14B, and PHI-4 14B - across 280 Codeforces problems spanning Elo ratings from 800 to 2100 and covering 36 distinct topics. All models were tasked with generating Python solutions. PHI-4 14B achieved the best performance among SLMs, with a pass@3 of 63.6%, approaching the proprietary O3-MINI-HIGH (86.8%). In addition, we evaluated PHI-4 14B on C++ and found that combining outputs from both Python and C++ increases its aggregated pass@3 to 73.6%. A qualitative analysis of PHI-4 14B's incorrect outputs revealed that some failures were due to minor implementation issues - such as handling edge cases or correcting variable initialization - rather than deeper reasoning flaws."
  },
  {
    "title": "Agentic SLMs: Hunting Down Test Smells",
    "url": "http://arxiv.org/abs/2504.07277v1",
    "arxiv_id": "2504.07277v1",
    "authors": [
      "Rian Melo",
      "Pedro Sim\u00f5es",
      "Rohit Gheyi",
      "Marcelo d'Amorim",
      "M\u00e1rcio Ribeiro",
      "Gustavo Soares",
      "Eduardo Almeida",
      "Elvys Soares"
    ],
    "published": "2025-04-09T21:12:01+00:00",
    "summary": "Test smells can compromise the reliability of test suites and hinder software maintenance. Although several strategies exist for detecting test smells, few address their removal. Traditional methods often rely on static analysis or machine learning, requiring significant effort and expertise. This study evaluates LLAMA 3.2 3B, GEMMA 2 9B, DEEPSEEK-R1 14B, and PHI 4 14B - small, open language models - for automating the detection and refactoring of test smells through agent-based workflows. We explore workflows with one, two, and four agents across 150 instances of 5 common test smell types extracted from real-world Java projects. Unlike prior approaches, ours is easily extensible to new smells via natural language definitions and generalizes to Python and Golang. All models detected nearly all test smell instances (pass@5 of 96% with four agents), with PHI 4 14B achieving the highest refactoring accuracy (pass@5 of 75.3%). Analyses were computationally inexpensive and ran efficiently on a consumer-grade hardware. Notably, PHI 4 14B with four agents performed within 5% of proprietary models such as O1-MINI, O3-MINI-HIGH, and GEMINI 2.5 PRO EXPERIMENTAL using a single agent. Multi-agent setups outperformed single-agent ones in three out of five test smell types, highlighting their potential to improve software quality with minimal developer effort. For the Assertion Roulette smell, however, a single agent performed better. To assess practical relevance, we submitted 10 pull requests with PHI 4 14B - generated code to open-source projects. Five were merged, one was rejected, and four remain under review, demonstrating the approach's real-world applicability."
  },
  {
    "title": "Sculpting Subspaces: Constrained Full Fine-Tuning in LLMs for Continual Learning",
    "url": "http://arxiv.org/abs/2504.07097v1",
    "arxiv_id": "2504.07097v1",
    "authors": [
      "Nikhil Shivakumar Nayak",
      "Krishnateja Killamsetty",
      "Ligong Han",
      "Abhishek Bhandwaldar",
      "Prateek Chanda",
      "Kai Xu",
      "Hao Wang",
      "Aldo Pareja",
      "Oleg Silkin",
      "Mustafa Eyceoz",
      "Akash Srivastava"
    ],
    "published": "2025-04-09T17:59:42+00:00",
    "summary": "Continual learning in large language models (LLMs) is prone to catastrophic forgetting, where adapting to new tasks significantly degrades performance on previously learned ones. Existing methods typically rely on low-rank, parameter-efficient updates that limit the model's expressivity and introduce additional parameters per task, leading to scalability issues. To address these limitations, we propose a novel continual full fine-tuning approach leveraging adaptive singular value decomposition (SVD). Our method dynamically identifies task-specific low-rank parameter subspaces and constrains updates to be orthogonal to critical directions associated with prior tasks, thus effectively minimizing interference without additional parameter overhead or storing previous task gradients. We evaluate our approach extensively on standard continual learning benchmarks using both encoder-decoder (T5-Large) and decoder-only (LLaMA-2 7B) models, spanning diverse tasks including classification, generation, and reasoning. Empirically, our method achieves state-of-the-art results, up to 7% higher average accuracy than recent baselines like O-LoRA, and notably maintains the model's general linguistic capabilities, instruction-following accuracy, and safety throughout the continual learning process by reducing forgetting to near-negligible levels. Our adaptive SVD framework effectively balances model plasticity and knowledge retention, providing a practical, theoretically grounded, and computationally scalable solution for continual learning scenarios in large language models."
  },
  {
    "title": "OmniCaptioner: One Captioner to Rule Them All",
    "url": "http://arxiv.org/abs/2504.07089v1",
    "arxiv_id": "2504.07089v1",
    "authors": [
      "Yiting Lu",
      "Jiakang Yuan",
      "Zhen Li",
      "Shitian Zhao",
      "Qi Qin",
      "Xinyue Li",
      "Le Zhuo",
      "Licheng Wen",
      "Dongyang Liu",
      "Yuewen Cao",
      "Xiangchao Yan",
      "Xin Li",
      "Botian Shi",
      "Tao Chen",
      "Zhibo Chen",
      "Lei Bai",
      "Bo Zhang",
      "Peng Gao"
    ],
    "published": "2025-04-09T17:58:58+00:00",
    "summary": "We propose OmniCaptioner, a versatile visual captioning framework for generating fine-grained textual descriptions across a wide variety of visual domains. Unlike prior methods limited to specific image types (e.g., natural images or geometric visuals), our framework provides a unified solution for captioning natural images, visual text (e.g., posters, UIs, textbooks), and structured visuals (e.g., documents, tables, charts). By converting low-level pixel information into semantically rich textual representations, our framework bridges the gap between visual and textual modalities. Our results highlight three key advantages: (i) Enhanced Visual Reasoning with LLMs, where long-context captions of visual modalities empower LLMs, particularly the DeepSeek-R1 series, to reason effectively in multimodal scenarios; (ii) Improved Image Generation, where detailed captions improve tasks like text-to-image generation and image transformation; and (iii) Efficient Supervised Fine-Tuning (SFT), which enables faster convergence with less data. We believe the versatility and adaptability of OmniCaptioner can offer a new perspective for bridging the gap between language and visual modalities."
  },
  {
    "title": "R2E-Gym: Procedural Environments and Hybrid Verifiers for Scaling Open-Weights SWE Agents",
    "url": "http://arxiv.org/abs/2504.07164v1",
    "arxiv_id": "2504.07164v1",
    "authors": [
      "Naman Jain",
      "Jaskirat Singh",
      "Manish Shetty",
      "Liang Zheng",
      "Koushik Sen",
      "Ion Stoica"
    ],
    "published": "2025-04-09T17:55:19+00:00",
    "summary": "Improving open-source models on real-world SWE tasks (solving GITHUB issues) faces two key challenges: 1) scalable curation of execution environments to train these models, and, 2) optimal scaling of test-time compute. We introduce AgentGym, the largest procedurally-curated executable gym environment for training real-world SWE-agents, consisting of more than 8.7K tasks. AgentGym is powered by two main contributions: 1) SYNGEN: a synthetic data curation recipe that enables scalable curation of executable environments using test-generation and back-translation directly from commits, thereby reducing reliance on human-written issues or unit tests. We show that this enables more scalable training leading to pass@1 performance of 34.4% on SWE-Bench Verified benchmark with our 32B model. 2) Hybrid Test-time Scaling: we provide an in-depth analysis of two test-time scaling axes; execution-based and execution-free verifiers, demonstrating that they exhibit complementary strengths and limitations. Test-based verifiers suffer from low distinguishability, while execution-free verifiers are biased and often rely on stylistic features. Surprisingly, we find that while each approach individually saturates around 42-43%, significantly higher gains can be obtained by leveraging their complementary strengths. Overall, our approach achieves 51% on the SWE-Bench Verified benchmark, reflecting a new state-of-the-art for open-weight SWE-agents and for the first time showing competitive performance with proprietary models such as o1, o1-preview and sonnet-3.5-v2 (with tools). We will open-source our environments, models, and agent trajectories."
  },
  {
    "title": "Self-Steering Language Models",
    "url": "http://arxiv.org/abs/2504.07081v1",
    "arxiv_id": "2504.07081v1",
    "authors": [
      "Gabriel Grand",
      "Joshua B. Tenenbaum",
      "Vikash K. Mansinghka",
      "Alexander K. Lew",
      "Jacob Andreas"
    ],
    "published": "2025-04-09T17:54:22+00:00",
    "summary": "While test-time reasoning enables language models to tackle complex tasks, searching or planning in natural language can be slow, costly, and error-prone. But even when LMs struggle to emulate the precise reasoning steps needed to solve a problem, they often excel at describing its abstract structure--both how to verify solutions and how to search for them. This paper introduces DisCIPL, a method for \"self-steering\" LMs where a Planner model generates a task-specific inference program that is executed by a population of Follower models. Our approach equips LMs with the ability to write recursive search procedures that guide LM inference, enabling new forms of verifiable and efficient reasoning. When instantiated with a small Follower (e.g., Llama-3.2-1B), DisCIPL matches (and sometimes outperforms) much larger models, including GPT-4o and o1, on challenging constrained generation tasks. In decoupling planning from execution, our work opens up a design space of highly-parallelized Monte Carlo inference strategies that outperform standard best-of-N sampling, require no finetuning, and can be implemented automatically by existing LMs."
  },
  {
    "title": "Evaluating Retrieval Augmented Generative Models for Document Queries in Transportation Safety",
    "url": "http://arxiv.org/abs/2504.07022v1",
    "arxiv_id": "2504.07022v1",
    "authors": [
      "Chad Melton",
      "Alex Sorokine",
      "Steve Peterson"
    ],
    "published": "2025-04-09T16:37:03+00:00",
    "summary": "Applications of generative Large Language Models LLMs are rapidly expanding across various domains, promising significant improvements in workflow efficiency and information retrieval. However, their implementation in specialized, high-stakes domains such as hazardous materials transportation is challenging due to accuracy and reliability concerns. This study evaluates the performance of three fine-tuned generative models, ChatGPT, Google's Vertex AI, and ORNL Retrieval Augmented Generation augmented LLaMA 2 and LLaMA in retrieving regulatory information essential for hazardous material transportation compliance in the United States. Utilizing approximately 40 publicly available federal and state regulatory documents, we developed 100 realistic queries relevant to route planning and permitting requirements. Responses were qualitatively rated based on accuracy, detail, and relevance, complemented by quantitative assessments of semantic similarity between model outputs. Results demonstrated that the RAG-augmented LLaMA models significantly outperformed Vertex AI and ChatGPT, providing more detailed and generally accurate information, despite occasional inconsistencies. This research introduces the first known application of RAG in transportation safety, emphasizing the need for domain-specific fine-tuning and rigorous evaluation methodologies to ensure reliability and minimize the risk of inaccuracies in high-stakes environments."
  },
  {
    "title": "Quantum Field Theory on Multifractal Spacetime: Varying Dimension and Ultraviolet Completeness",
    "url": "http://arxiv.org/abs/2504.06797v1",
    "arxiv_id": "2504.06797v1",
    "authors": [
      "Alessio Maiezza",
      "Juan Carlos Vasquez"
    ],
    "published": "2025-04-09T11:41:15+00:00",
    "summary": "Inspired by various quantum gravity approaches, we explore quantum field theory where spacetime exhibits scaling properties and dimensional reduction with changing energy scales, effectively behaving as a multifractal manifold. Working within canonical quantization, we demonstrate how to properly quantize fields in such multifractal spacetime. Our analysis reveals that a non-differentiable nature of spacetime is not merely compatible with quantum field theory but significantly enhances its mathematical foundation. Most notably, this approach guarantees the finiteness of the theory at all orders in perturbation theory and enables rigorous construction of the S-matrix in the interaction picture. The multifractal structure tames dominant, large-order divergence sources in the perturbative series and resolves the Landau pole problem through asymptotic safety, substantially improving the theory's behavior in the deep ultraviolet regime. Our formulation preserves all established predictions of standard quantum field theory at low energies while offering novel physical behaviors at high energy scales."
  },
  {
    "title": "Domain-Specific Pruning of Large Mixture-of-Experts Models with Few-shot Demonstrations",
    "url": "http://arxiv.org/abs/2504.06792v1",
    "arxiv_id": "2504.06792v1",
    "authors": [
      "Zican Dong",
      "Han Peng",
      "Peiyu Liu",
      "Wayne Xin Zhao",
      "Dong Wu",
      "Feng Xiao",
      "Zhifeng Wang"
    ],
    "published": "2025-04-09T11:34:06+00:00",
    "summary": "Mixture-of-Experts (MoE) models achieve a favorable trade-off between performance and inference efficiency by activating only a subset of experts. However, the memory overhead of storing all experts remains a major limitation, especially in large-scale MoE models such as DeepSeek-R1 (671B). In this study, we investigate domain specialization and expert redundancy in large-scale MoE models and uncover a consistent behavior we term few-shot expert localization, with only a few demonstrations, the model consistently activates a sparse and stable subset of experts. Building on this observation, we propose a simple yet effective pruning framework, EASY-EP, that leverages a few domain-specific demonstrations to identify and retain only the most relevant experts. EASY-EP comprises two key components: output-aware expert importance assessment and expert-level token contribution estimation. The former evaluates the importance of each expert for the current token by considering the gating scores and magnitudes of the outputs of activated experts, while the latter assesses the contribution of tokens based on representation similarities after and before routed experts. Experiments show that our method can achieve comparable performances and $2.99\\times$ throughput under the same memory budget with full DeepSeek-R1 with only half the experts. Our code is available at https://github.com/RUCAIBox/EASYEP."
  },
  {
    "title": "Beware of \"Explanations\" of AI",
    "url": "http://arxiv.org/abs/2504.06791v1",
    "arxiv_id": "2504.06791v1",
    "authors": [
      "David Martens",
      "Galit Shmueli",
      "Theodoros Evgeniou",
      "Kevin Bauer",
      "Christian Janiesch",
      "Stefan Feuerriegel",
      "Sebastian Gabel",
      "Sofie Goethals",
      "Travis Greene",
      "Nadja Klein",
      "Mathias Kraus",
      "Niklas K\u00fchl",
      "Claudia Perlich",
      "Wouter Verbeke",
      "Alona Zharova",
      "Patrick Zschech",
      "Foster Provost"
    ],
    "published": "2025-04-09T11:31:08+00:00",
    "summary": "Understanding the decisions made and actions taken by increasingly complex AI system remains a key challenge. This has led to an expanding field of research in explainable artificial intelligence (XAI), highlighting the potential of explanations to enhance trust, support adoption, and meet regulatory standards. However, the question of what constitutes a \"good\" explanation is dependent on the goals, stakeholders, and context. At a high level, psychological insights such as the concept of mental model alignment can offer guidance, but success in practice is challenging due to social and technical factors. As a result of this ill-defined nature of the problem, explanations can be of poor quality (e.g. unfaithful, irrelevant, or incoherent), potentially leading to substantial risks. Instead of fostering trust and safety, poorly designed explanations can actually cause harm, including wrong decisions, privacy violations, manipulation, and even reduced AI adoption. Therefore, we caution stakeholders to beware of explanations of AI: while they can be vital, they are not automatically a remedy for transparency or responsible AI adoption, and their misuse or limitations can exacerbate harm. Attention to these caveats can help guide future research to improve the quality and impact of AI explanations."
  },
  {
    "title": "Holistic Capability Preservation: Towards Compact Yet Comprehensive Reasoning Models",
    "url": "http://arxiv.org/abs/2504.07158v1",
    "arxiv_id": "2504.07158v1",
    "authors": [
      "Ling Team",
      "Caizhi Tang",
      "Chilin Fu",
      "Chunwei Wu",
      "Jia Guo",
      "Jianwen Wang",
      "Jingyu Hu",
      "Liang Jiang",
      "Meng Li",
      "Peng Jiao",
      "Pingping Liu",
      "Shaomian Zheng",
      "Shiwei Liang",
      "Shuaicheng Li",
      "Yalin Zhang",
      "Yingting Wu",
      "Yongkang Liu",
      "Zhenyu Huang"
    ],
    "published": "2025-04-09T11:24:32+00:00",
    "summary": "This technical report presents Ring-Lite-Distill, a lightweight reasoning model derived from our open-source Mixture-of-Experts (MoE) Large Language Models (LLMs) Ling-Lite. This study demonstrates that through meticulous high-quality data curation and ingenious training paradigms, the compact MoE model Ling-Lite can be further trained to achieve exceptional reasoning capabilities, while maintaining its parameter-efficient architecture with only 2.75 billion activated parameters, establishing an efficient lightweight reasoning architecture. In particular, in constructing this model, we have not merely focused on enhancing advanced reasoning capabilities, exemplified by high-difficulty mathematical problem solving, but rather aimed to develop a reasoning model with more comprehensive competency coverage. Our approach ensures coverage across reasoning tasks of varying difficulty levels while preserving generic capabilities, such as instruction following, tool use, and knowledge retention. We show that, Ring-Lite-Distill's reasoning ability reaches a level comparable to DeepSeek-R1-Distill-Qwen-7B, while its general capabilities significantly surpass those of DeepSeek-R1-Distill-Qwen-7B. The models are accessible at https://huggingface.co/inclusionAI"
  },
  {
    "title": "Holistic Capability Preservation: Towards Compact Yet Comprehensive Reasoning Models",
    "url": "http://arxiv.org/abs/2504.07158v2",
    "arxiv_id": "2504.07158v2",
    "authors": [
      "Ling Team",
      "Caizhi Tang",
      "Chilin Fu",
      "Chunwei Wu",
      "Jia Guo",
      "Jianwen Wang",
      "Jingyu Hu",
      "Liang Jiang",
      "Meng Li",
      "Peng Jiao",
      "Pingping Liu",
      "Shaomian Zheng",
      "Shiwei Liang",
      "Shuaicheng Li",
      "Yalin Zhang",
      "Yingting Wu",
      "Yongkang Liu",
      "Zhenyu Huang"
    ],
    "published": "2025-04-09T11:24:32+00:00",
    "summary": "This technical report presents Ring-Lite-Distill, a lightweight reasoning model derived from our open-source Mixture-of-Experts (MoE) Large Language Models (LLMs) Ling-Lite. This study demonstrates that through meticulous high-quality data curation and ingenious training paradigms, the compact MoE model Ling-Lite can be further trained to achieve exceptional reasoning capabilities, while maintaining its parameter-efficient architecture with only 2.75 billion activated parameters, establishing an efficient lightweight reasoning architecture. In particular, in constructing this model, we have not merely focused on enhancing advanced reasoning capabilities, exemplified by high-difficulty mathematical problem solving, but rather aimed to develop a reasoning model with more comprehensive competency coverage. Our approach ensures coverage across reasoning tasks of varying difficulty levels while preserving generic capabilities, such as instruction following, tool use, and knowledge retention. We show that, Ring-Lite-Distill's reasoning ability reaches a level comparable to DeepSeek-R1-Distill-Qwen-7B, while its general capabilities significantly surpass those of DeepSeek-R1-Distill-Qwen-7B. The models are accessible at https://huggingface.co/inclusionAI"
  },
  {
    "title": "Zero-Shot Image-Based Large Language Model Approach to Road Pavement Monitoring",
    "url": "http://arxiv.org/abs/2504.06785v1",
    "arxiv_id": "2504.06785v1",
    "authors": [
      "Shuoshuo Xu",
      "Kai Zhao",
      "James Loney",
      "Zili Li",
      "Andrea Visentin"
    ],
    "published": "2025-04-09T11:19:17+00:00",
    "summary": "Effective and rapid evaluation of pavement surface condition is critical for prioritizing maintenance, ensuring transportation safety, and minimizing vehicle wear and tear. While conventional manual inspections suffer from subjectivity, existing machine learning-based methods are constrained by their reliance on large and high-quality labeled datasets, which require significant resources and limit adaptability across varied road conditions. The revolutionary advancements in Large Language Models (LLMs) present significant potential for overcoming these challenges. In this study, we propose an innovative automated zero-shot learning approach that leverages the image recognition and natural language understanding capabilities of LLMs to assess road conditions effectively. Multiple LLM-based assessment models were developed, employing prompt engineering strategies aligned with the Pavement Surface Condition Index (PSCI) standards. These models' accuracy and reliability were evaluated against official PSCI results, with an optimized model ultimately selected. Extensive tests benchmarked the optimized model against evaluations from various levels experts using Google Street View road images. The results reveal that the LLM-based approach can effectively assess road conditions, with the optimized model -employing comprehensive and structured prompt engineering strategies -outperforming simpler configurations by achieving high accuracy and consistency, even surpassing expert evaluations. Moreover, successfully applying the optimized model to Google Street View images demonstrates its potential for future city-scale deployments. These findings highlight the transformative potential of LLMs in automating road damage evaluations and underscore the pivotal role of detailed prompt engineering in achieving reliable assessments."
  },
  {
    "title": "Dynamic Residual Safe Reinforcement Learning for Multi-Agent Safety-Critical Scenarios Decision-Making",
    "url": "http://arxiv.org/abs/2504.06670v1",
    "arxiv_id": "2504.06670v1",
    "authors": [
      "Kaifeng Wang",
      "Yinsong Chen",
      "Qi Liu",
      "Xueyuan Li",
      "Xin Gao"
    ],
    "published": "2025-04-09T08:13:14+00:00",
    "summary": "In multi-agent safety-critical scenarios, traditional autonomous driving frameworks face significant challenges in balancing safety constraints and task performance. These frameworks struggle to quantify dynamic interaction risks in real-time and depend heavily on manual rules, resulting in low computational efficiency and conservative strategies. To address these limitations, we propose a Dynamic Residual Safe Reinforcement Learning (DRS-RL) framework grounded in a safety-enhanced networked Markov decision process. It's the first time that the weak-to-strong theory is introduced into multi-agent decision-making, enabling lightweight dynamic calibration of safety boundaries via a weak-to-strong safety correction paradigm. Based on the multi-agent dynamic conflict zone model, our framework accurately captures spatiotemporal coupling risks among heterogeneous traffic participants and surpasses the static constraints of conventional geometric rules. Moreover, a risk-aware prioritized experience replay mechanism mitigates data distribution bias by mapping risk to sampling probability. Experimental results reveal that the proposed method significantly outperforms traditional RL algorithms in safety, efficiency, and comfort. Specifically, it reduces the collision rate by up to 92.17%, while the safety model accounts for merely 27% of the main model's parameters."
  },
  {
    "title": "Toward Holistic Evaluation of Recommender Systems Powered by Generative Models",
    "url": "http://arxiv.org/abs/2504.06667v1",
    "arxiv_id": "2504.06667v1",
    "authors": [
      "Yashar Deldjoo",
      "Nikhil Mehta",
      "Maheswaran Sathiamoorthy",
      "Shuai Zhang",
      "Pablo Castells",
      "Julian McAuley"
    ],
    "published": "2025-04-09T08:08:16+00:00",
    "summary": "Recommender systems powered by generative models (Gen-RecSys) extend beyond classical item ranking by producing open-ended content, which simultaneously unlocks richer user experiences and introduces new risks. On one hand, these systems can enhance personalization and appeal through dynamic explanations and multi-turn dialogues. On the other hand, they might venture into unknown territory-hallucinating nonexistent items, amplifying bias, or leaking private information. Traditional accuracy metrics cannot fully capture these challenges, as they fail to measure factual correctness, content safety, or alignment with user intent.   This paper makes two main contributions. First, we categorize the evaluation challenges of Gen-RecSys into two groups: (i) existing concerns that are exacerbated by generative outputs (e.g., bias, privacy) and (ii) entirely new risks (e.g., item hallucinations, contradictory explanations). Second, we propose a holistic evaluation approach that includes scenario-based assessments and multi-metric checks-incorporating relevance, factual grounding, bias detection, and policy compliance. Our goal is to provide a guiding framework so researchers and practitioners can thoroughly assess Gen-RecSys, ensuring effective personalization and responsible deployment."
  },
  {
    "title": "Uni-PrevPredMap: Extending PrevPredMap to a Unified Framework of Prior-Informed Modeling for Online Vectorized HD Map Construction",
    "url": "http://arxiv.org/abs/2504.06647v2",
    "arxiv_id": "2504.06647v2",
    "authors": [
      "Nan Peng",
      "Xun Zhou",
      "Mingming Wang",
      "Guisong Chen",
      "Wenqi Xu"
    ],
    "published": "2025-04-09T07:36:17+00:00",
    "summary": "Safety constitutes a foundational imperative for autonomous driving systems, necessitating the maximal incorporation of accessible external prior information. This study establishes that temporal perception buffers and cost-efficient maps inherently form complementary prior sources for online vectorized high-definition (HD) map construction. We present Uni-PrevPredMap, a unified prior-informed framework that systematically integrates two synergistic information sources: previous predictions and simulated outdated HD maps. The framework introduces two core innovations: a tile-indexed 3D vectorized global map processor enabling efficient refreshment, storage, and retrieval of 3D vectorized priors; a tri-mode operational optimization paradigm ensuring consistency across non-prior, temporal-prior, and temporal-map-fusion-prior scenarios while mitigating reliance on idealized map fidelity assumptions. Uni-PrevPredMap achieves state-of-the-art performance in map-absent scenarios across established online vectorized HD map construction benchmarks. When provided with simulated outdated HD maps, the framework exhibits robust capabilities in error-resilient prior fusion, empirically confirming the synergistic complementarity between previous predictions and simulated outdated HD maps. Code will be available at https://github.com/pnnnnnnn/Uni-PrevPredMap."
  },
  {
    "title": "Harmful information spreading and its impact on vaccination campaigns modeled through fractal-fractional operators",
    "url": "http://arxiv.org/abs/2504.06582v1",
    "arxiv_id": "2504.06582v1",
    "authors": [
      "Ali Akg\u00fcl",
      "Auwalu Hamisu Usman",
      "J. Alberto Conejero"
    ],
    "published": "2025-04-09T05:04:17+00:00",
    "summary": "Despite the huge efforts to develop and administer vaccines worldwide to cope with the COVID-19 pandemic, misinformation spreading through fake news in media and social networks about vaccination safety, make that people refuse to be vaccinated, which harms not only these people but also the whole population.   In this work, we model the effects of harmful information spreading in immunization acquisition through vaccination. Our model is posed for several fractional derivative operators. We have conducted a comprehensive foundation analysis of this model for the different fractional derivatives. Additionally, we have incorporated a strength parameter that shows the combined impact of nonlinear and linear components within an epidemiological model. We have used the second derivative of the Lyapunov function to ascertain the detection of wave patterns within the vaccination dynamics."
  },
  {
    "title": "Bypassing Safety Guardrails in LLMs Using Humor",
    "url": "http://arxiv.org/abs/2504.06577v1",
    "arxiv_id": "2504.06577v1",
    "authors": [
      "Pedro Cisneros-Velarde"
    ],
    "published": "2025-04-09T04:58:14+00:00",
    "summary": "In this paper, we show it is possible to bypass the safety guardrails of large language models (LLMs) through a humorous prompt including the unsafe request. In particular, our method does not edit the unsafe request and follows a fixed template -- it is simple to implement and does not need additional LLMs to craft prompts. Extensive experiments show the effectiveness of our method across different LLMs. We also show that both removing and adding more humor to our method can reduce its effectiveness -- excessive humor possibly distracts the LLM from fulfilling its unsafe request. Thus, we argue that LLM jailbreaking occurs when there is a proper balance between focus on the unsafe request and presence of humor."
  },
  {
    "title": "Do Reasoning Models Show Better Verbalized Calibration?",
    "url": "http://arxiv.org/abs/2504.06564v1",
    "arxiv_id": "2504.06564v1",
    "authors": [
      "Qingcheng Zeng",
      "Weihao Xuan",
      "Leyang Cui",
      "Rob Voigt"
    ],
    "published": "2025-04-09T03:58:19+00:00",
    "summary": "Large reasoning models (LRMs) have recently shown impressive capabilities in complex reasoning by leveraging increased test-time computation and exhibiting behaviors akin to human-like deliberation. Despite these advances, it remains an open question whether LRMs are better calibrated - particularly in their verbalized confidence - compared to instruction-tuned counterparts. In this paper, we investigate the calibration properties of LRMs trained via supervised fine-tuning distillation on long reasoning traces (henceforth SFT reasoning models) and outcome-based reinforcement learning for reasoning (henceforth RL reasoning models) across diverse domains. Our findings reveal that LRMs significantly outperform instruction-tuned models on complex reasoning tasks in both accuracy and confidence calibration. In contrast, we find surprising trends in the domain of factuality in particular. On factuality tasks, while Deepseek-R1 shows strong calibration behavior, smaller QwQ-32B shows no improvement over instruct models; moreover, SFT reasoning models display worse calibration (greater overconfidence) compared to instruct models. Our results provide evidence for a potentially critical role of reasoning-oriented RL training in improving LLMs' capacity for generating trustworthy, self-aware outputs."
  },
  {
    "title": "Data-Driven Reachability with Scenario Optimization and the Holdout Method",
    "url": "http://arxiv.org/abs/2504.06541v1",
    "arxiv_id": "2504.06541v1",
    "authors": [
      "Elizabeth Dietrich",
      "Rosalyn Devonport",
      "Stephen Tu",
      "Murat Arcak"
    ],
    "published": "2025-04-09T02:46:03+00:00",
    "summary": "Reachability analysis is an important method in providing safety guarantees for systems with unknown or uncertain dynamics. Due to the computational intractability of exact reachability analysis for general nonlinear, high-dimensional systems, recent work has focused on the use of probabilistic methods for computing approximate reachable sets. In this work, we advocate for the use of a general purpose, practical, and sharp method for data-driven reachability: the holdout method. Despite the simplicity of the holdout method, we show -- on several numerical examples including scenario-based reach tubes -- that the resulting probabilistic bounds are substantially sharper and require fewer samples than existing methods for data-driven reachability. Furthermore, we complement our work with a discussion on the necessity of probabilistic reachability bounds. We argue that any method that attempts to de-randomize the bounds, by converting the guarantees to hold deterministically, requires (a) an exponential in state-dimension amount of samples to achieve non-vacuous guarantees, and (b) extra assumptions on the dynamics."
  },
  {
    "title": "TSP-OCS: A Time-Series Prediction for Optimal Camera Selection in Multi-Viewpoint Surgical Video Analysis",
    "url": "http://arxiv.org/abs/2504.06527v1",
    "arxiv_id": "2504.06527v1",
    "authors": [
      "Xinyu Liu",
      "Xiaoguang Lin",
      "Xiang Liu",
      "Yong Yang",
      "Hongqian Wang",
      "Qilong Sun"
    ],
    "published": "2025-04-09T02:07:49+00:00",
    "summary": "Recording the open surgery process is essential for educational and medical evaluation purposes; however, traditional single-camera methods often face challenges such as occlusions caused by the surgeon's head and body, as well as limitations due to fixed camera angles, which reduce comprehensibility of the video content. This study addresses these limitations by employing a multi-viewpoint camera recording system, capturing the surgical procedure from six different angles to mitigate occlusions. We propose a fully supervised learning-based time series prediction method to choose the best shot sequences from multiple simultaneously recorded video streams, ensuring optimal viewpoints at each moment. Our time series prediction model forecasts future camera selections by extracting and fusing visual and semantic features from surgical videos using pre-trained models. These features are processed by a temporal prediction network with TimeBlocks to capture sequential dependencies. A linear embedding layer reduces dimensionality, and a Softmax classifier selects the optimal camera view based on the highest probability. In our experiments, we created five groups of open thyroidectomy videos, each with simultaneous recordings from six different angles. The results demonstrate that our method achieves competitive accuracy compared to traditional supervised methods, even when predicting over longer time horizons. Furthermore, our approach outperforms state-of-the-art time series prediction techniques on our dataset. This manuscript makes a unique contribution by presenting an innovative framework that advances surgical video analysis techniques, with significant implications for improving surgical education and patient safety."
  },
  {
    "title": "Safe Navigation in Uncertain Crowded Environments Using Risk Adaptive CVaR Barrier Functions",
    "url": "http://arxiv.org/abs/2504.06513v1",
    "arxiv_id": "2504.06513v1",
    "authors": [
      "Xinyi Wang",
      "Taekyung Kim",
      "Bardh Hoxha",
      "Georgios Fainekos",
      "Dimitra Panagou"
    ],
    "published": "2025-04-09T01:23:44+00:00",
    "summary": "Robot navigation in dynamic, crowded environments poses a significant challenge due to the inherent uncertainties in the obstacle model. In this work, we propose a risk-adaptive approach based on the Conditional Value-at-Risk Barrier Function (CVaR-BF), where the risk level is automatically adjusted to accept the minimum necessary risk, achieving a good performance in terms of safety and optimization feasibility under uncertainty. Additionally, we introduce a dynamic zone-based barrier function which characterizes the collision likelihood by evaluating the relative state between the robot and the obstacle. By integrating risk adaptation with this new function, our approach adaptively expands the safety margin, enabling the robot to proactively avoid obstacles in highly dynamic environments. Comparisons and ablation studies demonstrate that our method outperforms existing social navigation approaches, and validate the effectiveness of our proposed framework."
  },
  {
    "title": "Can LLMs Simulate Personas with Reversed Performance? A Benchmark for Counterfactual Instruction Following",
    "url": "http://arxiv.org/abs/2504.06460v1",
    "arxiv_id": "2504.06460v1",
    "authors": [
      "Sai Adith Senthil Kumar",
      "Hao Yan",
      "Saipavan Perepa",
      "Murong Yue",
      "Ziyu Yao"
    ],
    "published": "2025-04-08T22:00:32+00:00",
    "summary": "Large Language Models (LLMs) are now increasingly widely used to simulate personas in virtual environments, leveraging their instruction-following capability. However, we discovered that even state-of-the-art LLMs cannot simulate personas with reversed performance (e.g., student personas with low proficiency in educational settings), which impairs the simulation diversity and limits the practical applications of the simulated environments. In this work, using mathematical reasoning as a representative scenario, we propose the first benchmark dataset for evaluating LLMs on simulating personas with reversed performance, a capability that we dub \"counterfactual instruction following\". We evaluate both open-weight and closed-source LLMs on this task and find that LLMs, including the OpenAI o1 reasoning model, all struggle to follow counterfactual instructions for simulating reversedly performing personas. Intersectionally simulating both the performance level and the race population of a persona worsens the effect even further. These results highlight the challenges of counterfactual instruction following and the need for further research."
  },
  {
    "title": "Extended Version: Multi-Robot Motion Planning with Cooperative Localization",
    "url": "http://arxiv.org/abs/2504.06429v1",
    "arxiv_id": "2504.06429v1",
    "authors": [
      "Anne Theurkauf",
      "Nisar Ahmed",
      "Morteza Lahijanian"
    ],
    "published": "2025-04-08T20:58:19+00:00",
    "summary": "We consider the uncertain multi-robot motion planning (MRMP) problem with cooperative localization (CL-MRMP), under both motion and measurement noise, where each robot can act as a sensor for its nearby teammates. We formalize CL-MRMP as a chance-constrained motion planning problem, and propose a safety-guaranteed algorithm that explicitly accounts for robot-robot correlations. Our approach extends a sampling-based planner to solve CL-MRMP while preserving probabilistic completeness. To improve efficiency, we introduce novel biasing techniques. We evaluate our method across diverse benchmarks, demonstrating its effectiveness in generating motion plans, with significant performance gains from biasing strategies."
  },
  {
    "title": "Technological schemes and control methods in the reconstruction of parallel gas pipeline systems under non-stationary conditions",
    "url": "http://arxiv.org/abs/2504.06420v1",
    "arxiv_id": "2504.06420v1",
    "authors": [
      "Ilgar Aliyev"
    ],
    "published": "2025-04-08T20:40:02+00:00",
    "summary": "The study explores technological schemes and control methods for reconstructing parallel gas pipeline systems under non-stationary conditions. It focuses on improving safety, reliability, and efficiency by automating valve control and integrating IoT-based monitoring. Automated shut-off valves are designed to detect sudden pressure drops and block leaks instantly. These valves utilize pressure drop rates and valve position sensors to detect and isolate leaks. Wireless pressure sensors and real-time monitoring systems enable remote control of gas flow. Mathematical modeling is employed to analyze pressure variations along damaged sections in order to determine optimal valve placement and activation timing. Machine learning algorithms in the control center are used to predict and verify leak locations based on sensor data. To ensure uninterrupted gas supply in the event of an accident, the study develops an empirical formula for determining the location of connecting pipes activated between parallel pipelines, based on real-time pressure data. The aim of this research is to reduce gas leaks and environmental hazards, ensure continuous gas supply during emergencies, enhance decision-making through automated systems, minimize gas losses, and reduce maintenance costs."
  },
  {
    "title": "SPoRt -- Safe Policy Ratio: Certified Training and Deployment of Task Policies in Model-Free RL",
    "url": "http://arxiv.org/abs/2504.06386v1",
    "arxiv_id": "2504.06386v1",
    "authors": [
      "Jacques Cloete",
      "Nikolaus Vertovec",
      "Alessandro Abate"
    ],
    "published": "2025-04-08T19:09:07+00:00",
    "summary": "To apply reinforcement learning to safety-critical applications, we ought to provide safety guarantees during both policy training and deployment. In this work we present novel theoretical results that provide a bound on the probability of violating a safety property for a new task-specific policy in a model-free, episodic setup: the bound, based on a `maximum policy ratio' that is computed with respect to a `safe' base policy, can also be more generally applied to temporally-extended properties (beyond safety) and to robust control problems. We thus present SPoRt, which also provides a data-driven approach for obtaining such a bound for the base policy, based on scenario theory, and which includes Projected PPO, a new projection-based approach for training the task-specific policy while maintaining a user-specified bound on property violation. Hence, SPoRt enables the user to trade off safety guarantees in exchange for task-specific performance. Accordingly, we present experimental results demonstrating this trade-off, as well as a comparison of the theoretical bound to posterior bounds based on empirical violation rates."
  },
  {
    "title": "Addressing Relative Degree Issues in Control Barrier Function Synthesis with Physics-Informed Neural Networks",
    "url": "http://arxiv.org/abs/2504.06242v1",
    "arxiv_id": "2504.06242v1",
    "authors": [
      "Lukas Brunke",
      "Siqi Zhou",
      "Francesco D'Orazio",
      "Angela P. Schoellig"
    ],
    "published": "2025-04-08T17:41:43+00:00",
    "summary": "In robotics, control barrier function (CBF)-based safety filters are commonly used to enforce state constraints. A critical challenge arises when the relative degree of the CBF varies across the state space. This variability can create regions within the safe set where the control input becomes unconstrained. When implemented as a safety filter, this may result in chattering near the safety boundary and ultimately compromise system safety. To address this issue, we propose a novel approach for CBF synthesis by formulating it as solving a set of boundary value problems. The solutions to the boundary value problems are determined using physics-informed neural networks (PINNs). Our approach ensures that the synthesized CBFs maintain a constant relative degree across the set of admissible states, thereby preventing unconstrained control scenarios. We illustrate the approach in simulation and further verify it through real-world quadrotor experiments, demonstrating its effectiveness in preserving desired system safety properties."
  },
  {
    "title": "TxGemma: Efficient and Agentic LLMs for Therapeutics",
    "url": "http://arxiv.org/abs/2504.06196v1",
    "arxiv_id": "2504.06196v1",
    "authors": [
      "Eric Wang",
      "Samuel Schmidgall",
      "Paul F. Jaeger",
      "Fan Zhang",
      "Rory Pilgrim",
      "Yossi Matias",
      "Joelle Barral",
      "David Fleet",
      "Shekoofeh Azizi"
    ],
    "published": "2025-04-08T16:39:02+00:00",
    "summary": "Therapeutic development is a costly and high-risk endeavor that is often plagued by high failure rates. To address this, we introduce TxGemma, a suite of efficient, generalist large language models (LLMs) capable of therapeutic property prediction as well as interactive reasoning and explainability. Unlike task-specific models, TxGemma synthesizes information from diverse sources, enabling broad application across the therapeutic development pipeline. The suite includes 2B, 9B, and 27B parameter models, fine-tuned from Gemma-2 on a comprehensive dataset of small molecules, proteins, nucleic acids, diseases, and cell lines. Across 66 therapeutic development tasks, TxGemma achieved superior or comparable performance to the state-of-the-art generalist model on 64 (superior on 45), and against state-of-the-art specialist models on 50 (superior on 26). Fine-tuning TxGemma models on therapeutic downstream tasks, such as clinical trial adverse event prediction, requires less training data than fine-tuning base LLMs, making TxGemma suitable for data-limited applications. Beyond these predictive capabilities, TxGemma features conversational models that bridge the gap between general LLMs and specialized property predictors. These allow scientists to interact in natural language, provide mechanistic reasoning for predictions based on molecular structure, and engage in scientific discussions. Building on this, we further introduce Agentic-Tx, a generalist therapeutic agentic system powered by Gemini 2.5 that reasons, acts, manages diverse workflows, and acquires external domain knowledge. Agentic-Tx surpasses prior leading models on the Humanity's Last Exam benchmark (Chemistry & Biology) with 52.3% relative improvement over o3-mini (high) and 26.7% over o3-mini (high) on GPQA (Chemistry) and excels with improvements of 6.3% (ChemBench-Preference) and 2.4% (ChemBench-Mini) over o3-mini (high)."
  },
  {
    "title": "A Self-Supervised Framework for Space Object Behaviour Characterisation",
    "url": "http://arxiv.org/abs/2504.06176v1",
    "arxiv_id": "2504.06176v1",
    "authors": [
      "Ian Groves",
      "Andrew Campbell",
      "James Fernandes",
      "Diego Rodriguez",
      "Paul Murray",
      "Massimiliano Vasile",
      "Victoria Nockles"
    ],
    "published": "2025-04-08T16:19:19+00:00",
    "summary": "Foundation Models, pre-trained on large unlabelled datasets before task-specific fine-tuning, are increasingly being applied to specialised domains. Recent examples include ClimaX for climate and Clay for satellite Earth observation, but a Foundation Model for Space Object Behavioural Analysis has not yet been developed. As orbital populations grow, automated methods for characterising space object behaviour are crucial for space safety. We present a Space Safety and Sustainability Foundation Model focusing on space object behavioural analysis using light curves (LCs). We implemented a Perceiver-Variational Autoencoder (VAE) architecture, pre-trained with self-supervised reconstruction and masked reconstruction on 227,000 LCs from the MMT-9 observatory. The VAE enables anomaly detection, motion prediction, and LC generation. We fine-tuned the model for anomaly detection & motion prediction using two independent LC simulators (CASSANDRA and GRIAL respectively), using CAD models of boxwing, Sentinel-3, SMOS, and Starlink platforms. Our pre-trained model achieved a reconstruction error of 0.01%, identifying potentially anomalous light curves through reconstruction difficulty. After fine-tuning, the model scored 88% and 82% accuracy, with 0.90 and 0.95 ROC AUC scores respectively in both anomaly detection and motion mode prediction (sun-pointing, spin, etc.). Analysis of high-confidence anomaly predictions on real data revealed distinct patterns including characteristic object profiles and satellite glinting. Here, we demonstrate how self-supervised learning can simultaneously enable anomaly detection, motion prediction, and synthetic data generation from rich representations learned in pre-training. Our work therefore supports space safety and sustainability through automated monitoring and simulation capabilities."
  },
  {
    "title": "A Self-Supervised Framework for Space Object Behaviour Characterisation",
    "url": "http://arxiv.org/abs/2504.06176v2",
    "arxiv_id": "2504.06176v2",
    "authors": [
      "Ian Groves",
      "Andrew Campbell",
      "James Fernandes",
      "Diego Ram\u00edrez Rodr\u00edguez",
      "Paul Murray",
      "Massimiliano Vasile",
      "Victoria Nockles"
    ],
    "published": "2025-04-08T16:19:19+00:00",
    "summary": "Foundation Models, pre-trained on large unlabelled datasets before task-specific fine-tuning, are increasingly being applied to specialised domains. Recent examples include ClimaX for climate and Clay for satellite Earth observation, but a Foundation Model for Space Object Behavioural Analysis has not yet been developed. As orbital populations grow, automated methods for characterising space object behaviour are crucial for space safety. We present a Space Safety and Sustainability Foundation Model focusing on space object behavioural analysis using light curves (LCs). We implemented a Perceiver-Variational Autoencoder (VAE) architecture, pre-trained with self-supervised reconstruction and masked reconstruction on 227,000 LCs from the MMT-9 observatory. The VAE enables anomaly detection, motion prediction, and LC generation. We fine-tuned the model for anomaly detection & motion prediction using two independent LC simulators (CASSANDRA and GRIAL respectively), using CAD models of boxwing, Sentinel-3, SMOS, and Starlink platforms. Our pre-trained model achieved a reconstruction error of 0.01%, identifying potentially anomalous light curves through reconstruction difficulty. After fine-tuning, the model scored 88% and 82% accuracy, with 0.90 and 0.95 ROC AUC scores respectively in both anomaly detection and motion mode prediction (sun-pointing, spin, etc.). Analysis of high-confidence anomaly predictions on real data revealed distinct patterns including characteristic object profiles and satellite glinting. Here, we demonstrate how self-supervised learning can simultaneously enable anomaly detection, motion prediction, and synthetic data generation from rich representations learned in pre-training. Our work therefore supports space safety and sustainability through automated monitoring and simulation capabilities."
  },
  {
    "title": "Safe Interaction via Monte Carlo Linear-Quadratic Games",
    "url": "http://arxiv.org/abs/2504.06124v1",
    "arxiv_id": "2504.06124v1",
    "authors": [
      "Benjamin A. Christie",
      "Dylan P. Losey"
    ],
    "published": "2025-04-08T15:18:38+00:00",
    "summary": "Safety is critical during human-robot interaction. But -- because people are inherently unpredictable -- it is often difficult for robots to plan safe behaviors. Instead of relying on our ability to anticipate humans, here we identify robot policies that are robust to unexpected human decisions. We achieve this by formulating human-robot interaction as a zero-sum game, where (in the worst case) the human's actions directly conflict with the robot's objective. Solving for the Nash Equilibrium of this game provides robot policies that maximize safety and performance across a wide range of human actions. Existing approaches attempt to find these optimal policies by leveraging Hamilton-Jacobi analysis (which is intractable) or linear-quadratic approximations (which are inexact). By contrast, in this work we propose a computationally efficient and theoretically justified method that converges towards the Nash Equilibrium policy. Our approach (which we call MCLQ) leverages linear-quadratic games to obtain an initial guess at safe robot behavior, and then iteratively refines that guess with a Monte Carlo search. Not only does MCLQ provide real-time safety adjustments, but it also enables the designer to tune how conservative the robot is -- preventing the system from focusing on unrealistic human behaviors. Our simulations and user study suggest that this approach advances safety in terms of both computation time and expected performance. See videos of our experiments here: https://youtu.be/KJuHeiWVuWY."
  },
  {
    "title": "Leanabell-Prover: Posttraining Scaling in Formal Reasoning",
    "url": "http://arxiv.org/abs/2504.06122v2",
    "arxiv_id": "2504.06122v2",
    "authors": [
      "Jingyuan Zhang",
      "Qi Wang",
      "Xingguang Ji",
      "Yahui Liu",
      "Yang Yue",
      "Fuzheng Zhang",
      "Di Zhang",
      "Guorui Zhou",
      "Kun Gai"
    ],
    "published": "2025-04-08T15:15:26+00:00",
    "summary": "Recent advances in automated theorem proving (ATP) through LLMs have highlighted the potential of formal reasoning with Lean 4 codes. However, ATP has not yet be revolutionized by the recent posttraining scaling as demonstrated by Open AI O1/O3 and Deepseek R1. In this work, we investigate the entire posttraining of ATP, aiming to align it with breakthroughs in reasoning models in natural languages. To begin, we continual train current ATP models with a hybrid dataset, which consists of numerous statement-proof pairs, and additional data aimed at incorporating cognitive behaviors that emulate human reasoning and hypothesis refinement. Next, we explore reinforcement learning with the use of outcome reward returned by Lean 4 compiler. Through our designed continual training and reinforcement learning processes, we have successfully improved existing formal provers, including both DeepSeek-Prover-v1.5 and Goedel-Prover, achieving state-of-the-art performance in the field of whole-proof generation. For example, we achieve a 59.8% pass rate (pass@32) on MiniF2F. This is an on-going project and we will progressively update our findings, release our data and training details."
  },
  {
    "title": "Uncertainty-Aware Hybrid Machine Learning in Virtual Sensors for Vehicle Sideslip Angle Estimation",
    "url": "http://arxiv.org/abs/2504.06105v1",
    "arxiv_id": "2504.06105v1",
    "authors": [
      "Abinav Kalyanasundaram",
      "Karthikeyan Chandra Sekaran",
      "Philipp Stauber",
      "Michael Lange",
      "Wolfgang Utschick",
      "Michael Botsch"
    ],
    "published": "2025-04-08T14:49:58+00:00",
    "summary": "Precise vehicle state estimation is crucial for safe and reliable autonomous driving. The number of measurable states and their precision offered by the onboard vehicle sensor system are often constrained by cost. For instance, measuring critical quantities such as the Vehicle Sideslip Angle (VSA) poses significant commercial challenges using current optical sensors. This paper addresses these limitations by focusing on the development of high-performance virtual sensors to enhance vehicle state estimation for active safety. The proposed Uncertainty-Aware Hybrid Learning (UAHL) architecture integrates a machine learning model with vehicle motion models to estimate VSA directly from onboard sensor data. A key aspect of the UAHL architecture is its focus on uncertainty quantification for individual model estimates and hybrid fusion. These mechanisms enable the dynamic weighting of uncertainty-aware predictions from machine learning and vehicle motion models to produce accurate and reliable hybrid VSA estimates. This work also presents a novel dataset named Real-world Vehicle State Estimation Dataset (ReV-StED), comprising synchronized measurements from advanced vehicle dynamic sensors. The experimental results demonstrate the superior performance of the proposed method for VSA estimation, highlighting UAHL as a promising architecture for advancing virtual sensors and enhancing active safety in autonomous vehicles."
  }
]