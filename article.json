[
  {
    "title": "BiFlex: A Passive Bimodal Stiffness Flexible Wrist for Manipulation in Unstructured Environments",
    "url": "http://arxiv.org/abs/2504.08706v1",
    "arxiv_id": "2504.08706v1",
    "authors": [
      "Gu-Cheol Jeong",
      "Stefano Dalla Gasperina",
      "Ashish D. Deshpande",
      "Lillian Chin",
      "Roberto Mart\u00edn-Mart\u00edn"
    ],
    "published": "2025-04-11T17:16:13+00:00",
    "summary": "Robotic manipulation in unstructured, humancentric environments poses a dual challenge: achieving the precision need for delicate free-space operation while ensuring safety during unexpected contact events. Traditional wrists struggle to balance these demands, often relying on complex control schemes or complicated mechanical designs to mitigate potential damage from force overload. In response, we present BiFlex, a flexible robotic wrist that uses a soft buckling honeycomb structure to provides a natural bimodal stiffness response. The higher stiffness mode enables precise household object manipulation, while the lower stiffness mode provides the compliance needed to adapt to external forces. We design BiFlex to maintain a fingertip deflection of less than 1 cm while supporting loads up to 500g and create a BiFlex wrist for many grippers, including Panda, Robotiq, and BaRiFlex. We validate BiFlex under several real-world experimental evaluations, including surface wiping, precise pick-and-place, and grasping under environmental constraints. We demonstrate that BiFlex simplifies control while maintaining precise object manipulation and enhanced safety in real-world applications."
  },
  {
    "title": "Offline Reinforcement Learning using Human-Aligned Reward Labeling for Autonomous Emergency Braking in Occluded Pedestrian Crossing",
    "url": "http://arxiv.org/abs/2504.08704v1",
    "arxiv_id": "2504.08704v1",
    "authors": [
      "Vinal Asodia",
      "Zhenhua Feng",
      "Saber Fallah"
    ],
    "published": "2025-04-11T17:11:21+00:00",
    "summary": "Effective leveraging of real-world driving datasets is crucial for enhancing the training of autonomous driving systems. While Offline Reinforcement Learning enables the training of autonomous vehicles using such data, most available datasets lack meaningful reward labels. Reward labeling is essential as it provides feedback for the learning algorithm to distinguish between desirable and undesirable behaviors, thereby improving policy performance. This paper presents a novel pipeline for generating human-aligned reward labels. The proposed approach addresses the challenge of absent reward signals in real-world datasets by generating labels that reflect human judgment and safety considerations. The pipeline incorporates an adaptive safety component, activated by analyzing semantic segmentation maps, allowing the autonomous vehicle to prioritize safety over efficiency in potential collision scenarios. The proposed pipeline is applied to an occluded pedestrian crossing scenario with varying levels of pedestrian traffic, using synthetic and simulation data. The results indicate that the generated reward labels closely match the simulation reward labels. When used to train the driving policy using Behavior Proximal Policy Optimisation, the results are competitive with other baselines. This demonstrates the effectiveness of our method in producing reliable and human-aligned reward signals, facilitating the training of autonomous driving systems through Reinforcement Learning outside of simulation environments and in alignment with human values."
  },
  {
    "title": "Safe Flow Matching: Robot Motion Planning with Control Barrier Functions",
    "url": "http://arxiv.org/abs/2504.08661v1",
    "arxiv_id": "2504.08661v1",
    "authors": [
      "Xiaobing Dai",
      "Dian Yu",
      "Shanshan Zhang",
      "Zewen Yang"
    ],
    "published": "2025-04-11T16:10:58+00:00",
    "summary": "Recent advances in generative modeling have led to promising results in robot motion planning, particularly through diffusion and flow-based models that capture complex, multimodal trajectory distributions. However, these methods are typically trained offline and remain limited when faced with unseen environments or dynamic constraints, often lacking explicit mechanisms to ensure safety during deployment. In this work, we propose, Safe Flow Matching (SafeFM), a motion planning approach for trajectory generation that integrates flow matching with safety guarantees. By incorporating the proposed flow matching barrier functions, SafeFM ensures that generated trajectories remain within safe regions throughout the planning horizon, even in the presence of previously unseen obstacles or state-action constraints. Unlike diffusion-based approaches, our method allows for direct, efficient sampling of constraint-satisfying trajectories, making it well-suited for real-time motion planning. We evaluate SafeFM on a diverse set of tasks, including planar robot navigation and 7-DoF manipulation, demonstrating superior safety, generalization, and planning performance compared to state-of-the-art generative planners. Comprehensive resources are available on the project website: https://safeflowmatching.github.io/SafeFM/"
  },
  {
    "title": "TinyCenterSpeed: Efficient Center-Based Object Detection for Autonomous Racing",
    "url": "http://arxiv.org/abs/2504.08655v1",
    "arxiv_id": "2504.08655v1",
    "authors": [
      "Neil Reichlin",
      "Nicolas Baumann",
      "Edoardo Ghignone",
      "Michele Magno"
    ],
    "published": "2025-04-11T15:58:46+00:00",
    "summary": "Perception within autonomous driving is nearly synonymous with Neural Networks (NNs). Yet, the domain of autonomous racing is often characterized by scaled, computationally limited robots used for cost-effectiveness and safety. For this reason, opponent detection and tracking systems typically resort to traditional computer vision techniques due to computational constraints. This paper introduces TinyCenterSpeed, a streamlined adaptation of the seminal CenterPoint method, optimized for real-time performance on 1:10 scale autonomous racing platforms. This adaptation is viable even on OBCs powered solely by Central Processing Units (CPUs), as it incorporates the use of an external Tensor Processing Unit (TPU). We demonstrate that, compared to Adaptive Breakpoint Detector (ABD), the current State-of-the-Art (SotA) in scaled autonomous racing, TinyCenterSpeed not only improves detection and velocity estimation by up to 61.38% but also supports multi-opponent detection and estimation. It achieves real-time performance with an inference time of just 7.88 ms on the TPU, significantly reducing CPU utilization 8.3-fold."
  },
  {
    "title": "Deep Learning Methods for Detecting Thermal Runaway Events in Battery Production Lines",
    "url": "http://arxiv.org/abs/2504.08632v1",
    "arxiv_id": "2504.08632v1",
    "authors": [
      "Athanasios Athanasopoulos",
      "Mat\u00fa\u0161 Mihal\u00e1k",
      "Marcin Pietrasik"
    ],
    "published": "2025-04-11T15:35:50+00:00",
    "summary": "One of the key safety considerations of battery manufacturing is thermal runaway, the uncontrolled increase in temperature which can lead to fires, explosions, and emissions of toxic gasses. As such, development of automated systems capable of detecting such events is of considerable importance in both academic and industrial contexts. In this work, we investigate the use of deep learning for detecting thermal runaway in the battery production line of VDL Nedcar, a Dutch automobile manufacturer. Specifically, we collect data from the production line to represent both baseline (non thermal runaway) and thermal runaway conditions. Thermal runaway was simulated through the use of external heat and smoke sources. The data consisted of both optical and thermal images which were then preprocessed and fused before serving as input to our models. In this regard, we evaluated three deep-learning models widely used in computer vision including shallow convolutional neural networks, residual neural networks, and vision transformers on two performance metrics. Furthermore, we evaluated these models using explainability methods to gain insight into their ability to capture the relevant feature information from their inputs. The obtained results indicate that the use of deep learning is a viable approach to thermal runaway detection in battery production lines."
  },
  {
    "title": "Enabling Safety for Aerial Robots: Planning and Control Architectures",
    "url": "http://arxiv.org/abs/2504.08601v1",
    "arxiv_id": "2504.08601v1",
    "authors": [
      "Kaleb Ben Naveed",
      "Devansh R. Agrawal",
      "Daniel M. Cherenson",
      "Haejoon Lee",
      "Alia Gilbert",
      "Hardik Parwana",
      "Vishnu S. Chipade",
      "William Bentz",
      "Dimitra Panagou"
    ],
    "published": "2025-04-11T15:05:31+00:00",
    "summary": "Ensuring safe autonomy is crucial for deploying aerial robots in real-world applications. However, safety is a multifaceted challenge that must be addressed from multiple perspectives, including navigation in dynamic environments, operation under resource constraints, and robustness against adversarial attacks and uncertainties. In this paper, we present the authors' recent work that tackles some of these challenges and highlights key aspects that must be considered to enhance the safety and performance of autonomous aerial systems. All presented approaches are validated through hardware experiments."
  },
  {
    "title": "Secondary Safety Control for Systems with Sector Bounded Nonlinearities",
    "url": "http://arxiv.org/abs/2504.08535v1",
    "arxiv_id": "2504.08535v1",
    "authors": [
      "Yankai Lin",
      "Michelle S. Chong",
      "Carlos Murguia"
    ],
    "published": "2025-04-11T13:44:22+00:00",
    "summary": "We consider the problem of safety verification and safety-aware controller synthesis for systems with sector bounded nonlinearities. We aim to keep the states of the system within a given safe set under potential actuator and sensor attacks. Specifically, we adopt the setup that a controller has already been designed to stabilize the plant. Using invariant sets and barrier certificate theory, we first give sufficient conditions to verify the safety of the closed-loop system under attacks. Furthermore, by using a subset of sensors that are assumed to be free of attacks, we provide a synthesis method for a secondary controller that enhances the safety of the system. The sufficient conditions to verify safety are derived using Lyapunov-based tools and the S-procedure. Using the projection lemma, the conditions are then formulated as linear matrix inequality (LMI) problems which can be solved efficiently. Lastly, our theoretical results are illustrated through numerical simulations."
  },
  {
    "title": "Secondary Safety Control for Systems with Sector Bounded Nonlinearities [Extended Version]",
    "url": "http://arxiv.org/abs/2504.08535v2",
    "arxiv_id": "2504.08535v2",
    "authors": [
      "Yankai Lin",
      "Michelle S. Chong",
      "Carlos Murguia"
    ],
    "published": "2025-04-11T13:44:22+00:00",
    "summary": "We consider the problem of safety verification and safety-aware controller synthesis for systems with sector bounded nonlinearities. We aim to keep the states of the system within a given safe set under potential actuator and sensor attacks. Specifically, we adopt the setup that a controller has already been designed to stabilize the plant. Using invariant sets and barrier certificate theory, we first give sufficient conditions to verify the safety of the closed-loop system under attacks. Furthermore, by using a subset of sensors that are assumed to be free of attacks, we provide a synthesis method for a secondary controller that enhances the safety of the system. The sufficient conditions to verify safety are derived using Lyapunov-based tools and the S-procedure. Using the projection lemma, the conditions are then formulated as linear matrix inequality (LMI) problems which can be solved efficiently. Lastly, our theoretical results are illustrated through numerical simulations."
  },
  {
    "title": "Physics-informed data-driven control without persistence of excitation",
    "url": "http://arxiv.org/abs/2504.08484v1",
    "arxiv_id": "2504.08484v1",
    "authors": [
      "Martina Vanelli",
      "Julien M. Hendrickx"
    ],
    "published": "2025-04-11T12:19:51+00:00",
    "summary": "We show that data that is not sufficiently informative to allow for system re-identification can still provide meaningful information when combined with external or physical knowledge of the system, such as bounded system matrix norms. We then illustrate how this information can be leveraged for safety and energy minimization problems and to enhance predictions in unmodelled dynamics. This preliminary work outlines key ideas toward using limited data for effective control by integrating physical knowledge of the system and exploiting interpolation conditions."
  },
  {
    "title": "Constrained Machine Learning Through Hyperspherical Representation",
    "url": "http://arxiv.org/abs/2504.08415v1",
    "arxiv_id": "2504.08415v1",
    "authors": [
      "Gaetano Signorelli",
      "Michele Lombardi"
    ],
    "published": "2025-04-11T10:19:49+00:00",
    "summary": "The problem of ensuring constraints satisfaction on the output of machine learning models is critical for many applications, especially in safety-critical domains. Modern approaches rely on penalty-based methods at training time, which do not guarantee to avoid constraints violations; or constraint-specific model architectures (e.g., for monotonocity); or on output projection, which requires to solve an optimization problem that might be computationally demanding. We present the Hypersherical Constrained Representation, a novel method to enforce constraints in the output space for convex and bounded feasibility regions (generalizable to star domains). Our method operates on a different representation system, where Euclidean coordinates are converted into hyperspherical coordinates relative to the constrained region, which can only inherently represent feasible points. Experiments on a synthetic and a real-world dataset show that our method has predictive performance comparable to the other approaches, can guarantee 100% constraint satisfaction, and has a minimal computational cost at inference time."
  },
  {
    "title": "Evaluating Pedestrian Risks in Shared Spaces Through Autonomous Vehicle Experiments on a Fixed Track",
    "url": "http://arxiv.org/abs/2504.08316v1",
    "arxiv_id": "2504.08316v1",
    "authors": [
      "Enrico Del Re",
      "Novel Certad",
      "Cristina Olaverri-Monreal"
    ],
    "published": "2025-04-11T07:37:15+00:00",
    "summary": "The majority of research on safety in autonomous vehicles has been conducted in structured and controlled environments. However, there is a scarcity of research on safety in unregulated pedestrian areas, especially when interacting with public transport vehicles like trams. This study investigates pedestrian responses to an alert system in this context by replicating this real-world scenario in an environment using an autonomous vehicle. The results show that safety measures from other contexts can be adapted to shared spaces with trams, where fixed tracks heighten risks in unregulated crossings."
  },
  {
    "title": "SortBench: Benchmarking LLMs based on their ability to sort lists",
    "url": "http://arxiv.org/abs/2504.08312v1",
    "arxiv_id": "2504.08312v1",
    "authors": [
      "Steffen Herbold"
    ],
    "published": "2025-04-11T07:29:56+00:00",
    "summary": "Sorting is a tedious but simple task for human intelligence and can be solved fairly easily algorithmically. However, for Large Language Models (LLMs) this task is surprisingly hard, as some properties of sorting are among known weaknesses of LLMs: being faithful to the input data, logical comparisons between values, and strictly differentiating between syntax (used for sorting) and semantics (typically learned by embeddings). Within this paper, we describe the new SortBench benchmark for LLMs that comes with different difficulties and that can be easily scaled in terms of difficulty. We apply this benchmark to seven state-of-the-art LLMs, including current test-time reasoning models. Our results show that while the o3-mini model is very capable at sorting in general, even this can be fooled if strings are defined to mix syntactical and semantical aspects, e.g., by asking to sort numbers written-out as word. Furthermore, all models have problems with the faithfulness to the input of long lists, i.e., they drop items and add new ones. Our results also show that test-time reasoning has a tendency to overthink problems which leads to performance degradation. Finally, models without test-time reasoning like GPT-4o are not much worse than reasoning models."
  },
  {
    "title": "Evaluating the Bias in LLMs for Surveying Opinion and Decision Making in Healthcare",
    "url": "http://arxiv.org/abs/2504.08260v1",
    "arxiv_id": "2504.08260v1",
    "authors": [
      "Yonchanok Khaokaew",
      "Flora D. Salim",
      "Andreas Z\u00fcfle",
      "Hao Xue",
      "Taylor Anderson",
      "Matthew Scotch",
      "David J Heslop"
    ],
    "published": "2025-04-11T05:11:40+00:00",
    "summary": "Generative agents have been increasingly used to simulate human behaviour in silico, driven by large language models (LLMs). These simulacra serve as sandboxes for studying human behaviour without compromising privacy or safety. However, it remains unclear whether such agents can truly represent real individuals. This work compares survey data from the Understanding America Study (UAS) on healthcare decision-making with simulated responses from generative agents. Using demographic-based prompt engineering, we create digital twins of survey respondents and analyse how well different LLMs reproduce real-world behaviours. Our findings show that some LLMs fail to reflect realistic decision-making, such as predicting universal vaccine acceptance. However, Llama 3 captures variations across race and Income more accurately but also introduces biases not present in the UAS data. This study highlights the potential of generative agents for behavioural research while underscoring the risks of bias from both LLMs and prompting strategies."
  },
  {
    "title": "Neural Network-assisted Interval Reachability for Systems with Control Barrier Function-Based Safe Controllers",
    "url": "http://arxiv.org/abs/2504.08249v1",
    "arxiv_id": "2504.08249v1",
    "authors": [
      "Damola Ajeyemi",
      "Saber Jafarpour",
      "Emiliano Dall'Anese"
    ],
    "published": "2025-04-11T04:14:55+00:00",
    "summary": "Control Barrier Functions (CBFs) have been widely utilized in the design of optimization-based controllers and filters for dynamical systems to ensure forward invariance of a given set of safe states. While CBF-based controllers offer safety guarantees, they can compromise the performance of the system, leading to undesirable behaviors such as unbounded trajectories and emergence of locally stable spurious equilibria. Computing reachable sets for systems with CBF-based controllers is an effective approach for runtime performance and stability verification, and can potentially serve as a tool for trajectory re-planning. In this paper, we propose a computationally efficient interval reachability method for performance verification of systems with optimization-based controllers by: (i) approximating the optimization-based controller by a pre-trained neural network to avoid solving optimization problems repeatedly, and (ii) using mixed monotone theory to construct an embedding system that leverages state-of-the-art neural network verification algorithms for bounding the output of the neural network. Results in terms of closeness of solutions of trajectories of the system with the optimization-based controller and the neural network are derived. Using a single trajectory of the embedding system along with our closeness of solutions result, we obtain an over-approximation of the reachable set of the system with optimization-based controllers. Numerical results are presented to corroborate the technical findings."
  },
  {
    "title": "InSPE: Rapid Evaluation of Heterogeneous Multi-Modal Infrastructure Sensor Placement",
    "url": "http://arxiv.org/abs/2504.08240v1",
    "arxiv_id": "2504.08240v1",
    "authors": [
      "Zhaoliang Zheng",
      "Yun Zhang",
      "Zongling Meng",
      "Johnson Liu",
      "Xin Xia",
      "Jiaqi Ma"
    ],
    "published": "2025-04-11T03:55:00+00:00",
    "summary": "Infrastructure sensing is vital for traffic monitoring at safety hotspots (e.g., intersections) and serves as the backbone of cooperative perception in autonomous driving. While vehicle sensing has been extensively studied, infrastructure sensing has received little attention, especially given the unique challenges of diverse intersection geometries, complex occlusions, varying traffic conditions, and ambient environments like lighting and weather. To address these issues and ensure cost-effective sensor placement, we propose Heterogeneous Multi-Modal Infrastructure Sensor Placement Evaluation (InSPE), a perception surrogate metric set that rapidly assesses perception effectiveness across diverse infrastructure and environmental scenarios with combinations of multi-modal sensors. InSPE systematically evaluates perception capabilities by integrating three carefully designed metrics, i.e., sensor coverage, perception occlusion, and information gain. To support large-scale evaluation, we develop a data generation tool within the CARLA simulator and also introduce Infra-Set, a dataset covering diverse intersection types and environmental conditions. Benchmarking experiments with state-of-the-art perception algorithms demonstrate that InSPE enables efficient and scalable sensor placement analysis, providing a robust solution for optimizing intelligent intersection infrastructure."
  },
  {
    "title": "Advancing Autonomous Vehicle Safety: A Combined Fault Tree Analysis and Bayesian Network Approach",
    "url": "http://arxiv.org/abs/2504.08206v1",
    "arxiv_id": "2504.08206v1",
    "authors": [
      "Lansu Dai",
      "Burak Kantarci"
    ],
    "published": "2025-04-11T02:17:10+00:00",
    "summary": "This paper integrates Fault Tree Analysis (FTA) and Bayesian Networks (BN) to assess collision risk and establish Automotive Safety Integrity Level (ASIL) B failure rate targets for critical autonomous vehicle (AV) components. The FTA-BN integration combines the systematic decomposition of failure events provided by FTA with the probabilistic reasoning capabilities of BN, which allow for dynamic updates in failure probabilities, enhancing the adaptability of risk assessment. A fault tree is constructed based on AV subsystem architecture, with collision as the top event, and failure rates are assigned while ensuring the total remains within 100 FIT. Bayesian inference is applied to update posterior probabilities, and the results indicate that perception system failures (46.06 FIT) are the most significant contributor, particularly failures to detect existing objects (PF5) and misclassification (PF6). Mitigation strategies are proposed for sensors, perception, decision-making, and motion control to reduce the collision risk. The FTA-BN integration approach provides dynamic risk quantification, offering system designers refined failure rate targets to improve AV safety."
  },
  {
    "title": "EO-VLM: VLM-Guided Energy Overload Attacks on Vision Models",
    "url": "http://arxiv.org/abs/2504.08205v1",
    "arxiv_id": "2504.08205v1",
    "authors": [
      "Minjae Seo",
      "Myoungsung You",
      "Junhee Lee",
      "Jaehan Kim",
      "Hwanjo Heo",
      "Jintae Oh",
      "Jinwoo Kim"
    ],
    "published": "2025-04-11T02:13:24+00:00",
    "summary": "Vision models are increasingly deployed in critical applications such as autonomous driving and CCTV monitoring, yet they remain susceptible to resource-consuming attacks. In this paper, we introduce a novel energy-overloading attack that leverages vision language model (VLM) prompts to generate adversarial images targeting vision models. These images, though imperceptible to the human eye, significantly increase GPU energy consumption across various vision models, threatening the availability of these systems. Our framework, EO-VLM (Energy Overload via VLM), is model-agnostic, meaning it is not limited by the architecture or type of the target vision model. By exploiting the lack of safety filters in VLMs like DALL-E 3, we create adversarial noise images without requiring prior knowledge or internal structure of the target vision models. Our experiments demonstrate up to a 50% increase in energy consumption, revealing a critical vulnerability in current vision models."
  },
  {
    "title": "SAEs $\\textit{Can}$ Improve Unlearning: Dynamic Sparse Autoencoder Guardrails for Precision Unlearning in LLMs",
    "url": "http://arxiv.org/abs/2504.08192v1",
    "arxiv_id": "2504.08192v1",
    "authors": [
      "Aashiq Muhamed",
      "Jacopo Bonato",
      "Mona Diab",
      "Virginia Smith"
    ],
    "published": "2025-04-11T01:24:03+00:00",
    "summary": "Machine unlearning is a promising approach to improve LLM safety by removing unwanted knowledge from the model. However, prevailing gradient-based unlearning methods suffer from issues such as high computational costs, hyperparameter instability, poor sequential unlearning capability, vulnerability to relearning attacks, low data efficiency, and lack of interpretability. While Sparse Autoencoders are well-suited to improve these aspects by enabling targeted activation-based unlearning, prior approaches underperform gradient-based methods. This work demonstrates that, contrary to these earlier findings, SAEs can significantly improve unlearning when employed dynamically. We introduce $\\textbf{Dynamic DAE Guardrails}$ (DSG), a novel method for precision unlearning that leverages principled feature selection and a dynamic classifier. Our experiments show DSG substantially outperforms leading unlearning methods, achieving superior forget-utility trade-offs. DSG addresses key drawbacks of gradient-based approaches for unlearning -- offering enhanced computational efficiency and stability, robust performance in sequential unlearning, stronger resistance to relearning attacks, better data efficiency including zero-shot settings, and more interpretable unlearning."
  },
  {
    "title": "Safe Data-Driven Predictive Control",
    "url": "http://arxiv.org/abs/2504.08188v1",
    "arxiv_id": "2504.08188v1",
    "authors": [
      "Amin Vahidi-Moghaddam",
      "Kaian Chen",
      "Kaixiang Zhang",
      "Zhaojian Li",
      "Yan Wang",
      "Kai Wu"
    ],
    "published": "2025-04-11T01:08:21+00:00",
    "summary": "In the realm of control systems, model predictive control (MPC) has exhibited remarkable potential; however, its reliance on accurate models and substantial computational resources has hindered its broader application, especially within real-time nonlinear systems. This study presents an innovative control framework to enhance the practical viability of the MPC. The developed safe data-driven predictive control aims to eliminate the requirement for precise models and alleviate computational burdens in the nonlinear MPC (NMPC). This is achieved by learning both the system dynamics and the control policy, enabling efficient data-driven predictive control while ensuring system safety. The methodology involves a spatial temporal filter (STF)-based concurrent learning for system identification, a robust control barrier function (RCBF) to ensure the system safety amid model uncertainties, and a RCBF-based NMPC policy approximation. An online policy correction mechanism is also introduced to counteract performance degradation caused by the existing model uncertainties. Demonstrated through simulations on two applications, the proposed approach offers comparable performance to existing benchmarks with significantly reduced computational costs."
  },
  {
    "title": "Investigating Vision-Language Model for Point Cloud-based Vehicle Classification",
    "url": "http://arxiv.org/abs/2504.08154v1",
    "arxiv_id": "2504.08154v1",
    "authors": [
      "Yiqiao Li",
      "Jie Wei",
      "Camille Kamga"
    ],
    "published": "2025-04-10T22:37:27+00:00",
    "summary": "Heavy-duty trucks pose significant safety challenges due to their large size and limited maneuverability compared to passenger vehicles. A deeper understanding of truck characteristics is essential for enhancing the safety perspective of cooperative autonomous driving. Traditional LiDAR-based truck classification methods rely on extensive manual annotations, which makes them labor-intensive and costly. The rapid advancement of large language models (LLMs) trained on massive datasets presents an opportunity to leverage their few-shot learning capabilities for truck classification. However, existing vision-language models (VLMs) are primarily trained on image datasets, which makes it challenging to directly process point cloud data. This study introduces a novel framework that integrates roadside LiDAR point cloud data with VLMs to facilitate efficient and accurate truck classification, which supports cooperative and safe driving environments. This study introduces three key innovations: (1) leveraging real-world LiDAR datasets for model development, (2) designing a preprocessing pipeline to adapt point cloud data for VLM input, including point cloud registration for dense 3D rendering and mathematical morphological techniques to enhance feature representation, and (3) utilizing in-context learning with few-shot prompting to enable vehicle classification with minimally labeled training data. Experimental results demonstrate encouraging performance of this method and present its potential to reduce annotation efforts while improving classification accuracy."
  },
  {
    "title": "Development and Performance Analysis of Glass-Based Gas-Tight RPCs for Muography Applications",
    "url": "http://arxiv.org/abs/2504.08146v1",
    "arxiv_id": "2504.08146v1",
    "authors": [
      "S. Ikram",
      "S. Basnet",
      "E. Cortina Gil",
      "P. Demin",
      "R. M. I. D. Gamage",
      "A. Giammanco",
      "R. Karnam",
      "V. K. S. Kashyap",
      "V. Kumar",
      "B. Mohanty",
      "M. Moussawi",
      "A. Samalan",
      "M. Tytgat"
    ],
    "published": "2025-04-10T22:14:32+00:00",
    "summary": "To achieve high-resolution muography of compact targets in scenarios with complex logistical constraints, we are developing a portable muon detector system utilizing glass Resistive Plate Chambers (RPCs). Although RPCs are well understood and widely used, our work focuses on developing a gas-tight variant specifically tailored for a broad range of muography applications, with key design goals including portability, robustness, autonomy, versatility, safety, and cost-effectiveness. Our RPC detectors are designed with various configurations, each featuring unique characteristics and performance attributes. We investigate the temporal evolution of the surface resistivity of glass electrodes, as well as the detector efficiency at varying voltages and thresholds, over a span of several months. These RPCs have been utilized in a small-scale feasibility study on muon absorption using lead blocks."
  },
  {
    "title": "Certified to Drive: A Policy Proposal for Mandatory Training on Semi-Automated Vehicles",
    "url": "http://arxiv.org/abs/2504.08128v1",
    "arxiv_id": "2504.08128v1",
    "authors": [
      "Soumita Mukherjee",
      "Varun Darshana Parekh",
      "Nikhil Tayal"
    ],
    "published": "2025-04-10T21:11:31+00:00",
    "summary": "Although the Boeing 737 Max incidents resulted from a mix of design shortcomings, regulatory oversights, and systemic issues, they also highlight a critical gap in pilot training on managing automated systems during abnormal conditions. This example demonstrates the urgent need for focused, concise training on human-automation interaction - a need that is equally critical for operators of Level 2 ADAS-equipped vehicles, as discussed in detail later in this article. The lack of structured education for semi-automated vehicle operators mirrors similar risks in other industries, where formal training is critical for safe operation. Two policy recommendations are proposed. First, governments should create concise, official resources in accessible and official format to educate drivers on system capabilities and limitations. Second, mandatory training and certification programs should be introduced, combining theoretical and hands-on components to prepare drivers for real-world scenarios. These measures will improve driver understanding, reduce misuse, and foster public trust in semi-automated vehicle technologies. By addressing the knowledge gap, policymakers can ensure a safer, more responsible transition to automation, maximizing its benefits while minimizing risks to public safety."
  },
  {
    "title": "DeepSeek vs. o3-mini: How Well can Reasoning LLMs Evaluate MT and Summarization?",
    "url": "http://arxiv.org/abs/2504.08120v1",
    "arxiv_id": "2504.08120v1",
    "authors": [
      "Daniil Larionov",
      "Sotaro Takeshita",
      "Ran Zhang",
      "Yanran Chen",
      "Christoph Leiter",
      "Zhipin Wang",
      "Christian Greisinger",
      "Steffen Eger"
    ],
    "published": "2025-04-10T20:39:18+00:00",
    "summary": "Reasoning-enabled large language models (LLMs) have recently demonstrated impressive performance in complex logical and mathematical tasks, yet their effectiveness in evaluating natural language generation remains unexplored. This study systematically compares reasoning-based LLMs (DeepSeek-R1 and OpenAI o3) with their non-reasoning counterparts across machine translation (MT) and text summarization (TS) evaluation tasks. We evaluate eight models across three architectural categories, including state-of-the-art reasoning models, their distilled variants (ranging from 8B to 70B parameters), and equivalent conventional, non-reasoning LLMs. Our experiments on WMT23 and SummEval benchmarks reveal that the benefits of reasoning capabilities are highly model and task-dependent: while OpenAI o3-mini models show consistent performance improvements with increased reasoning intensity, DeepSeek-R1 underperforms compared to its non-reasoning variant, with exception to certain aspects of TS evaluation. Correlation analysis demonstrates that increased reasoning token usage positively correlates with evaluation quality in o3-mini models. Furthermore, our results show that distillation of reasoning capabilities maintains reasonable performance in medium-sized models (32B) but degrades substantially in smaller variants (8B). This work provides the first comprehensive assessment of reasoning LLMs for NLG evaluation and offers insights into their practical use."
  },
  {
    "title": "Geneshift: Impact of different scenario shift on Jailbreaking LLM",
    "url": "http://arxiv.org/abs/2504.08104v1",
    "arxiv_id": "2504.08104v1",
    "authors": [
      "Tianyi Wu",
      "Zhiwei Xue",
      "Yue Liu",
      "Jiaheng Zhang",
      "Bryan Hooi",
      "See-Kiong Ng"
    ],
    "published": "2025-04-10T20:02:35+00:00",
    "summary": "Jailbreak attacks, which aim to cause LLMs to perform unrestricted behaviors, have become a critical and challenging direction in AI safety. Despite achieving the promising attack success rate using dictionary-based evaluation, existing jailbreak attack methods fail to output detailed contents to satisfy the harmful request, leading to poor performance on GPT-based evaluation. To this end, we propose a black-box jailbreak attack termed GeneShift, by using a genetic algorithm to optimize the scenario shifts. Firstly, we observe that the malicious queries perform optimally under different scenario shifts. Based on it, we develop a genetic algorithm to evolve and select the hybrid of scenario shifts. It guides our method to elicit detailed and actionable harmful responses while keeping the seemingly benign facade, improving stealthiness. Extensive experiments demonstrate the superiority of GeneShift. Notably, GeneShift increases the jailbreak success rate from 0% to 60% when direct prompting alone would fail."
  },
  {
    "title": "The AI Scientist-v2: Workshop-Level Automated Scientific Discovery via Agentic Tree Search",
    "url": "http://arxiv.org/abs/2504.08066v1",
    "arxiv_id": "2504.08066v1",
    "authors": [
      "Yutaro Yamada",
      "Robert Tjarko Lange",
      "Cong Lu",
      "Shengran Hu",
      "Chris Lu",
      "Jakob Foerster",
      "Jeff Clune",
      "David Ha"
    ],
    "published": "2025-04-10T18:44:41+00:00",
    "summary": "AI is increasingly playing a pivotal role in transforming how scientific discoveries are made. We introduce The AI Scientist-v2, an end-to-end agentic system capable of producing the first entirely AI generated peer-review-accepted workshop paper. This system iteratively formulates scientific hypotheses, designs and executes experiments, analyzes and visualizes data, and autonomously authors scientific manuscripts. Compared to its predecessor (v1, Lu et al., 2024 arXiv:2408.06292), The AI Scientist-v2 eliminates the reliance on human-authored code templates, generalizes effectively across diverse machine learning domains, and leverages a novel progressive agentic tree-search methodology managed by a dedicated experiment manager agent. Additionally, we enhance the AI reviewer component by integrating a Vision-Language Model (VLM) feedback loop for iterative refinement of content and aesthetics of the figures. We evaluated The AI Scientist-v2 by submitting three fully autonomous manuscripts to a peer-reviewed ICLR workshop. Notably, one manuscript achieved high enough scores to exceed the average human acceptance threshold, marking the first instance of a fully AI-generated paper successfully navigating a peer review. This accomplishment highlights the growing capability of AI in conducting all aspects of scientific research. We anticipate that further advancements in autonomous scientific discovery technologies will profoundly impact human knowledge generation, enabling unprecedented scalability in research productivity and significantly accelerating scientific breakthroughs, greatly benefiting society at large. We have open-sourced the code at https://github.com/SakanaAI/AI-Scientist-v2 to foster the future development of this transformative technology. We also discuss the role of AI in science, including AI safety."
  },
  {
    "title": "Can Reasoning LLMs Enhance Clinical Document Classification?",
    "url": "http://arxiv.org/abs/2504.08040v1",
    "arxiv_id": "2504.08040v1",
    "authors": [
      "Akram Mustafa",
      "Usman Naseem",
      "Mostafa Rahimi Azghadi"
    ],
    "published": "2025-04-10T18:00:27+00:00",
    "summary": "Clinical document classification is essential for converting unstructured medical texts into standardised ICD-10 diagnoses, yet it faces challenges due to complex medical language, privacy constraints, and limited annotated datasets. Large Language Models (LLMs) offer promising improvements in accuracy and efficiency for this task. This study evaluates the performance and consistency of eight LLMs; four reasoning (Qwen QWQ, Deepseek Reasoner, GPT o3 Mini, Gemini 2.0 Flash Thinking) and four non-reasoning (Llama 3.3, GPT 4o Mini, Gemini 2.0 Flash, Deepseek Chat); in classifying clinical discharge summaries using the MIMIC-IV dataset. Using cTAKES to structure clinical narratives, models were assessed across three experimental runs, with majority voting determining final predictions. Results showed that reasoning models outperformed non-reasoning models in accuracy (71% vs 68%) and F1 score (67% vs 60%), with Gemini 2.0 Flash Thinking achieving the highest accuracy (75%) and F1 score (76%). However, non-reasoning models demonstrated greater stability (91% vs 84% consistency). Performance varied across ICD-10 codes, with reasoning models excelling in complex cases but struggling with abstract categories. Findings indicate a trade-off between accuracy and consistency, suggesting that a hybrid approach could optimise clinical coding. Future research should explore multi-label classification, domain-specific fine-tuning, and ensemble methods to enhance model reliability in real-world applications."
  },
  {
    "title": "VCR-Bench: A Comprehensive Evaluation Framework for Video Chain-of-Thought Reasoning",
    "url": "http://arxiv.org/abs/2504.07956v1",
    "arxiv_id": "2504.07956v1",
    "authors": [
      "Yukun Qi",
      "Yiming Zhao",
      "Yu Zeng",
      "Xikun Bao",
      "Wenxuan Huang",
      "Lin Chen",
      "Zehui Chen",
      "Jie Zhao",
      "Zhongang Qi",
      "Feng Zhao"
    ],
    "published": "2025-04-10T17:59:03+00:00",
    "summary": "The advancement of Chain-of-Thought (CoT) reasoning has significantly enhanced the capabilities of large language models (LLMs) and large vision-language models (LVLMs). However, a rigorous evaluation framework for video CoT reasoning remains absent. Current video benchmarks fail to adequately assess the reasoning process and expose whether failures stem from deficiencies in perception or reasoning capabilities. Therefore, we introduce VCR-Bench, a novel benchmark designed to comprehensively evaluate LVLMs' Video Chain-of-Thought Reasoning capabilities. VCR-Bench comprises 859 videos spanning a variety of video content and durations, along with 1,034 high-quality question-answer pairs. Each pair is manually annotated with a stepwise CoT rationale, where every step is tagged to indicate its association with the perception or reasoning capabilities. Furthermore, we design seven distinct task dimensions and propose the CoT score to assess the entire CoT process based on the stepwise tagged CoT rationals. Extensive experiments on VCR-Bench highlight substantial limitations in current LVLMs. Even the top-performing model, o1, only achieves a 62.8% CoT score and an 56.7% accuracy, while most models score below 40%. Experiments show most models score lower on perception than reasoning steps, revealing LVLMs' key bottleneck in temporal-spatial information processing for complex video reasoning. A robust positive correlation between the CoT score and accuracy confirms the validity of our evaluation framework and underscores the critical role of CoT reasoning in solving complex video reasoning tasks. We hope VCR-Bench to serve as a standardized evaluation framework and expose the actual drawbacks in complex video reasoning task."
  },
  {
    "title": "Perception-R1: Pioneering Perception Policy with Reinforcement Learning",
    "url": "http://arxiv.org/abs/2504.07954v1",
    "arxiv_id": "2504.07954v1",
    "authors": [
      "En Yu",
      "Kangheng Lin",
      "Liang Zhao",
      "Jisheng Yin",
      "Yana Wei",
      "Yuang Peng",
      "Haoran Wei",
      "Jianjian Sun",
      "Chunrui Han",
      "Zheng Ge",
      "Xiangyu Zhang",
      "Daxin Jiang",
      "Jingyu Wang",
      "Wenbing Tao"
    ],
    "published": "2025-04-10T17:58:27+00:00",
    "summary": "Inspired by the success of DeepSeek-R1, we explore the potential of rule-based reinforcement learning (RL) in MLLM post-training for perception policy learning. While promising, our initial experiments reveal that incorporating a thinking process through RL does not consistently lead to performance gains across all visual perception tasks. This leads us to delve into the essential role of RL in the context of visual perception. In this work, we return to the fundamentals and explore the effects of RL on different perception tasks. We observe that the perceptual complexity is a major factor in determining the effectiveness of RL. We also observe that reward design plays a crucial role in further approching the upper limit of model perception. To leverage these findings, we propose Perception-R1, a scalable RL framework using GRPO during MLLM post-training. With a standard Qwen2.5-VL-3B-Instruct, Perception-R1 achieves +4.2% on RefCOCO+, +17.9% on PixMo-Count, +4.2% on PageOCR, and notably, 31.9% AP on COCO2017 val for the first time, establishing a strong baseline for perception policy learning."
  },
  {
    "title": "SoTA with Less: MCTS-Guided Sample Selection for Data-Efficient Visual Reasoning Self-Improvement",
    "url": "http://arxiv.org/abs/2504.07934v1",
    "arxiv_id": "2504.07934v1",
    "authors": [
      "Xiyao Wang",
      "Zhengyuan Yang",
      "Chao Feng",
      "Hongjin Lu",
      "Linjie Li",
      "Chung-Ching Lin",
      "Kevin Lin",
      "Furong Huang",
      "Lijuan Wang"
    ],
    "published": "2025-04-10T17:49:05+00:00",
    "summary": "In this paper, we present an effective method to enhance visual reasoning with significantly fewer training samples, relying purely on self-improvement with no knowledge distillation. Our key insight is that the difficulty of training data during reinforcement fine-tuning (RFT) is critical. Appropriately challenging samples can substantially boost reasoning capabilities even when the dataset is small. Despite being intuitive, the main challenge remains in accurately quantifying sample difficulty to enable effective data filtering. To this end, we propose a novel way of repurposing Monte Carlo Tree Search (MCTS) to achieve that. Starting from our curated 70k open-source training samples, we introduce an MCTS-based selection method that quantifies sample difficulty based on the number of iterations required by the VLMs to solve each problem. This explicit step-by-step reasoning in MCTS enforces the model to think longer and better identifies samples that are genuinely challenging. We filter and retain 11k samples to perform RFT on Qwen2.5-VL-7B-Instruct, resulting in our final model, ThinkLite-VL. Evaluation results on eight benchmarks show that ThinkLite-VL improves the average performance of Qwen2.5-VL-7B-Instruct by 7%, using only 11k training samples with no knowledge distillation. This significantly outperforms all existing 7B-level reasoning VLMs, and our fairly comparable baselines that use classic selection methods such as accuracy-based filtering. Notably, on MathVista, ThinkLite-VL-7B achieves the SoTA accuracy of 75.1, surpassing Qwen2.5-VL-72B, GPT-4o, and O1. Our code, data, and model are available at https://github.com/si0wang/ThinkLite-VL."
  },
  {
    "title": "Benchmarking Adversarial Robustness to Bias Elicitation in Large Language Models: Scalable Automated Assessment with LLM-as-a-Judge",
    "url": "http://arxiv.org/abs/2504.07887v1",
    "arxiv_id": "2504.07887v1",
    "authors": [
      "Riccardo Cantini",
      "Alessio Orsino",
      "Massimo Ruggiero",
      "Domenico Talia"
    ],
    "published": "2025-04-10T16:00:59+00:00",
    "summary": "Large Language Models (LLMs) have revolutionized artificial intelligence, driving advancements in machine translation, summarization, and conversational agents. However, their increasing integration into critical societal domains has raised concerns about embedded biases, which can perpetuate stereotypes and compromise fairness. These biases stem from various sources, including historical inequalities in training data, linguistic imbalances, and adversarial manipulation. Despite mitigation efforts, recent studies indicate that LLMs remain vulnerable to adversarial attacks designed to elicit biased responses. This work proposes a scalable benchmarking framework to evaluate LLM robustness against adversarial bias elicitation. Our methodology involves (i) systematically probing models with a multi-task approach targeting biases across various sociocultural dimensions, (ii) quantifying robustness through safety scores using an LLM-as-a-Judge approach for automated assessment of model responses, and (iii) employing jailbreak techniques to investigate vulnerabilities in safety mechanisms. Our analysis examines prevalent biases in both small and large state-of-the-art models and their impact on model safety. Additionally, we assess the safety of domain-specific models fine-tuned for critical fields, such as medicine. Finally, we release a curated dataset of bias-related prompts, CLEAR-Bias, to facilitate systematic vulnerability benchmarking. Our findings reveal critical trade-offs between model size and safety, aiding the development of fairer and more robust future language models."
  },
  {
    "title": "Gauge and parametrization dependence of Quantum Einstein Gravity within the Proper Time flow",
    "url": "http://arxiv.org/abs/2504.07877v1",
    "arxiv_id": "2504.07877v1",
    "authors": [
      "Alfio Bonanno",
      "Giovanni Oglialoro",
      "Dario Zappal\u00e0"
    ],
    "published": "2025-04-10T15:52:31+00:00",
    "summary": "Proper time functional flow equations have garnered significant attention in recent years, as they are particularly suitable in analyzing non-perturbative contexts. By resorting to this flow, we investigate the regulator and gauge dependence in quantum Einstein gravity within the asymptotic safety framework, considering various regularization schemes. Our findings indicate that some details of the regulator have minor influence on the critical properties of the theory. In contrast, the selection between linear and exponential parametrizations appears to have a more substantial impact on the scaling behavior of the renormalized flow near the non-Gaussian fixed point."
  },
  {
    "title": "Pangu Ultra: Pushing the Limits of Dense Large Language Models on Ascend NPUs",
    "url": "http://arxiv.org/abs/2504.07866v1",
    "arxiv_id": "2504.07866v1",
    "authors": [
      "Yichun Yin",
      "Wenyong Huang",
      "Kaikai Song",
      "Yehui Tang",
      "Xueyu Wu",
      "Wei Guo",
      "Peng Guo",
      "Yaoyuan Wang",
      "Xiaojun Meng",
      "Yasheng Wang",
      "Dong Li",
      "Can Chen",
      "Dandan Tu",
      "Yin Li",
      "Fisher Yu",
      "Ruiming Tang",
      "Yunhe Wang",
      "Baojun Wang",
      "Bin Wang",
      "Bo Wang",
      "Boxiao Liu",
      "Changzheng Zhang",
      "Duyu Tang",
      "Fei Mi",
      "Hui Jin",
      "Jiansheng Wei",
      "Jiarui Qin",
      "Jinpeng Li",
      "Jun Zhao",
      "Liqun Deng",
      "Lin Li",
      "Minghui Xu",
      "Naifu Zhang",
      "Nianzu Zheng",
      "Qiang Li",
      "Rongju Ruan",
      "Shengjun Cheng",
      "Tianyu Guo",
      "Wei He",
      "Wei Li",
      "Weiwen Liu",
      "Wulong Liu",
      "Xinyi Dai",
      "Yonghan Dong",
      "Yu Pan",
      "Yue Li",
      "Yufei Wang",
      "Yujun Li",
      "Yunsheng Ni",
      "Zhe Liu",
      "Zhenhe Zhang",
      "Zhicheng Liu"
    ],
    "published": "2025-04-10T15:41:51+00:00",
    "summary": "We present Pangu Ultra, a Large Language Model (LLM) with 135 billion parameters and dense Transformer modules trained on Ascend Neural Processing Units (NPUs). Although the field of LLM has been witnessing unprecedented advances in pushing the scale and capability of LLM in recent years, training such a large-scale model still involves significant optimization and system challenges. To stabilize the training process, we propose depth-scaled sandwich normalization, which effectively eliminates loss spikes during the training process of deep models. We pre-train our model on 13.2 trillion diverse and high-quality tokens and further enhance its reasoning capabilities during post-training. To perform such large-scale training efficiently, we utilize 8,192 Ascend NPUs with a series of system optimizations. Evaluations on multiple diverse benchmarks indicate that Pangu Ultra significantly advances the state-of-the-art capabilities of dense LLMs such as Llama 405B and Mistral Large 2, and even achieves competitive results with DeepSeek-R1, whose sparse model structure contains much more parameters. Our exploration demonstrates that Ascend NPUs are capable of efficiently and effectively training dense models with more than 100 billion parameters. Our model and system will be available for our commercial customers."
  },
  {
    "title": "Pangu Ultra: Pushing the Limits of Dense Large Language Models on Ascend NPUs",
    "url": "http://arxiv.org/abs/2504.07866v2",
    "arxiv_id": "2504.07866v2",
    "authors": [
      "Yichun Yin",
      "Wenyong Huang",
      "Kaikai Song",
      "Yehui Tang",
      "Xueyu Wu",
      "Wei Guo",
      "Peng Guo",
      "Yaoyuan Wang",
      "Xiaojun Meng",
      "Yasheng Wang",
      "Dong Li",
      "Can Chen",
      "Dandan Tu",
      "Yin Li",
      "Fisher Yu",
      "Ruiming Tang",
      "Yunhe Wang",
      "Baojun Wang",
      "Bin Wang",
      "Bo Wang",
      "Boxiao Liu",
      "Changzheng Zhang",
      "Duyu Tang",
      "Fei Mi",
      "Hui Jin",
      "Jiansheng Wei",
      "Jiarui Qin",
      "Jinpeng Li",
      "Jun Zhao",
      "Liqun Deng",
      "Lin Li",
      "Minghui Xu",
      "Naifu Zhang",
      "Nianzu Zheng",
      "Qiang Li",
      "Rongju Ruan",
      "Shengjun Cheng",
      "Tianyu Guo",
      "Wei He",
      "Wei Li",
      "Weiwen Liu",
      "Wulong Liu",
      "Xinyi Dai",
      "Yonghan Dong",
      "Yu Pan",
      "Yue Li",
      "Yufei Wang",
      "Yujun Li",
      "Yunsheng Ni",
      "Zhe Liu",
      "Zhenhe Zhang",
      "Zhicheng Liu"
    ],
    "published": "2025-04-10T15:41:51+00:00",
    "summary": "We present Pangu Ultra, a Large Language Model (LLM) with 135 billion parameters and dense Transformer modules trained on Ascend Neural Processing Units (NPUs). Although the field of LLM has been witnessing unprecedented advances in pushing the scale and capability of LLM in recent years, training such a large-scale model still involves significant optimization and system challenges. To stabilize the training process, we propose depth-scaled sandwich normalization, which effectively eliminates loss spikes during the training process of deep models. We pre-train our model on 13.2 trillion diverse and high-quality tokens and further enhance its reasoning capabilities during post-training. To perform such large-scale training efficiently, we utilize 8,192 Ascend NPUs with a series of system optimizations. Evaluations on multiple diverse benchmarks indicate that Pangu Ultra significantly advances the state-of-the-art capabilities of dense LLMs such as Llama 405B and Mistral Large 2, and even achieves competitive results with DeepSeek-R1, whose sparse model structure contains much more parameters. Our exploration demonstrates that Ascend NPUs are capable of efficiently and effectively training dense models with more than 100 billion parameters. Our model and system will be available for our commercial customers."
  },
  {
    "title": "Robust Hallucination Detection in LLMs via Adaptive Token Selection",
    "url": "http://arxiv.org/abs/2504.07863v1",
    "arxiv_id": "2504.07863v1",
    "authors": [
      "Mengjia Niu",
      "Hamed Haddadi",
      "Guansong Pang"
    ],
    "published": "2025-04-10T15:39:10+00:00",
    "summary": "Hallucinations in large language models (LLMs) pose significant safety concerns that impede their broader deployment. Recent research in hallucination detection has demonstrated that LLMs' internal representations contain truthfulness hints, which can be harnessed for detector training. However, the performance of these detectors is heavily dependent on the internal representations of predetermined tokens, fluctuating considerably when working on free-form generations with varying lengths and sparse distributions of hallucinated entities. To address this, we propose HaMI, a novel approach that enables robust detection of hallucinations through adaptive selection and learning of critical tokens that are most indicative of hallucinations. We achieve this robustness by an innovative formulation of the Hallucination detection task as Multiple Instance (HaMI) learning over token-level representations within a sequence, thereby facilitating a joint optimisation of token selection and hallucination detection on generation sequences of diverse forms. Comprehensive experimental results on four hallucination benchmarks show that HaMI significantly outperforms existing state-of-the-art approaches."
  },
  {
    "title": "Deceptive Automated Interpretability: Language Models Coordinating to Fool Oversight Systems",
    "url": "http://arxiv.org/abs/2504.07831v1",
    "arxiv_id": "2504.07831v1",
    "authors": [
      "Simon Lermen",
      "Mateusz Dziemian",
      "Natalia P\u00e9rez-Campanero Antol\u00edn"
    ],
    "published": "2025-04-10T15:07:10+00:00",
    "summary": "We demonstrate how AI agents can coordinate to deceive oversight systems using automated interpretability of neural networks. Using sparse autoencoders (SAEs) as our experimental framework, we show that language models (Llama, DeepSeek R1, and Claude 3.7 Sonnet) can generate deceptive explanations that evade detection. Our agents employ steganographic methods to hide information in seemingly innocent explanations, successfully fooling oversight models while achieving explanation quality comparable to reference labels. We further find that models can scheme to develop deceptive strategies when they believe the detection of harmful features might lead to negative consequences for themselves. All tested LLM agents were capable of deceiving the overseer while achieving high interpretability scores comparable to those of reference labels. We conclude by proposing mitigation strategies, emphasizing the critical need for robust understanding and defenses against deception."
  },
  {
    "title": "Revisiting Likelihood-Based Out-of-Distribution Detection by Modeling Representations",
    "url": "http://arxiv.org/abs/2504.07793v1",
    "arxiv_id": "2504.07793v1",
    "authors": [
      "Yifan Ding",
      "Arturas Aleksandrauskas",
      "Amirhossein Ahmadian",
      "Jonas Unger",
      "Fredrik Lindsten",
      "Gabriel Eilertsen"
    ],
    "published": "2025-04-10T14:30:41+00:00",
    "summary": "Out-of-distribution (OOD) detection is critical for ensuring the reliability of deep learning systems, particularly in safety-critical applications. Likelihood-based deep generative models have historically faced criticism for their unsatisfactory performance in OOD detection, often assigning higher likelihood to OOD data than in-distribution samples when applied to image data. In this work, we demonstrate that likelihood is not inherently flawed. Rather, several properties in the images space prohibit likelihood as a valid detection score. Given a sufficiently good likelihood estimator, specifically using the probability flow formulation of a diffusion model, we show that likelihood-based methods can still perform on par with state-of-the-art methods when applied in the representation space of pre-trained encoders. The code of our work can be found at $\\href{https://github.com/limchaos/Likelihood-OOD.git}{\\texttt{https://github.com/limchaos/Likelihood-OOD.git}}$."
  },
  {
    "title": "Realigning Incentives to Build Better Software: a Holistic Approach to Vendor Accountability",
    "url": "http://arxiv.org/abs/2504.07766v1",
    "arxiv_id": "2504.07766v1",
    "authors": [
      "Gergely Bicz\u00f3k",
      "Sasha Romanosky",
      "Mingyan Liu"
    ],
    "published": "2025-04-10T14:05:24+00:00",
    "summary": "In this paper, we ask the question of why the quality of commercial software, in terms of security and safety, does not measure up to that of other (durable) consumer goods we have come to expect. We examine this question through the lens of incentives. We argue that the challenge around better quality software is due in no small part to a sequence of misaligned incentives, the most critical of which being that the harm caused by software problems is by and large shouldered by consumers, not developers. This lack of liability means software vendors have every incentive to rush low-quality software onto the market and no incentive to enhance quality control. Within this context, this paper outlines a holistic technical and policy framework we believe is needed to incentivize better and more secure software development. At the heart of the incentive realignment is the concept of software liability. This framework touches on various components, including legal, technical, and financial, that are needed for software liability to work in practice; some currently exist, some will need to be re-imagined or established. This is primarily a market-driven approach that emphasizes voluntary participation but highlights the role appropriate regulation can play. We connect and contrast this with the EU legal environment and discuss what this framework means for open-source software (OSS) development and emerging AI risks. Moreover, we present a CrowdStrike case study complete with a what-if analysis had our proposed framework been in effect. Our intention is very much to stimulate a robust conversation among both researchers and practitioners."
  },
  {
    "title": "Optimal Frequency Support from Virtual Power Plants: Minimal Reserve and Allocation",
    "url": "http://arxiv.org/abs/2504.07703v1",
    "arxiv_id": "2504.07703v1",
    "authors": [
      "Xiang Zhu",
      "Guangchun Ruan",
      "Hua Geng"
    ],
    "published": "2025-04-10T12:43:38+00:00",
    "summary": "This paper proposes a novel reserve-minimizing and allocation strategy for virtual power plants (VPPs) to deliver optimal frequency support. The proposed strategy enables VPPs, acting as aggregators for inverter-based resources (IBRs), to provide optimal frequency support economically. The proposed strategy captures time-varying active power injections, reducing the unnecessary redundancy compared to traditional fixed reserve schemes. Reserve requirements for the VPPs are determined based on system frequency response and safety constraints, ensuring efficient grid support. Furthermore, an energy-based allocation model decomposes power injections for each IBR, accounting for their specific limitations. Numerical experiments validate the feasibility of the proposed approach, highlighting significant financial gains for VPPs, especially as system inertia decreases due to higher renewable energy integration."
  },
  {
    "title": "Joint Travel Route Optimization Framework for Platooning",
    "url": "http://arxiv.org/abs/2504.07623v1",
    "arxiv_id": "2504.07623v1",
    "authors": [
      "Akif Adas",
      "Stefano Arrigoni",
      "Mattia Brambilla",
      "Monica Barbara Nicoli",
      "Edoardo Sabbioni"
    ],
    "published": "2025-04-10T10:13:20+00:00",
    "summary": "Platooning represents an advanced driving technology designed to assist drivers in traffic convoys of varying lengths, enhancing road safety, reducing driver fatigue, and improving fuel efficiency. Sophisticated automated driving assistance systems have facilitated this innovation. Recent advancements in platooning emphasize cooperative mechanisms within both centralized and decentralized architectures enabled by vehicular communication technologies. This study introduces a cooperative route planning optimization framework aimed at promoting the adoption of platooning through a centralized platoon formation strategy at the system level. This approach is envisioned as a transitional phase from individual (ego) driving to fully collaborative driving. Additionally, this research formulates and incorporates travel cost metrics related to fuel consumption, driver fatigue, and travel time, considering regulatory constraints on consecutive driving durations. The performance of these cost metrics has been evaluated using Dijkstra's and A* shortest path algorithms within a network graph framework. The results indicate that the proposed architecture achieves an average cost improvement of 14 % compared to individual route planning for long road trips."
  },
  {
    "title": "VLM-R1: A Stable and Generalizable R1-style Large Vision-Language Model",
    "url": "http://arxiv.org/abs/2504.07615v1",
    "arxiv_id": "2504.07615v1",
    "authors": [
      "Haozhan Shen",
      "Peng Liu",
      "Jingcheng Li",
      "Chunxin Fang",
      "Yibo Ma",
      "Jiajia Liao",
      "Qiaoli Shen",
      "Zilun Zhang",
      "Kangjia Zhao",
      "Qianqian Zhang",
      "Ruochen Xu",
      "Tiancheng Zhao"
    ],
    "published": "2025-04-10T10:05:15+00:00",
    "summary": "Recently DeepSeek R1 has shown that reinforcement learning (RL) can substantially improve the reasoning capabilities of Large Language Models (LLMs) through a simple yet effective design. The core of R1 lies in its rule-based reward formulation, which leverages tasks with deterministic ground-truth answers to enable precise and stable reward computation. In the visual domain, we similarly observe that a wide range of visual understanding tasks are inherently equipped with well-defined ground-truth annotations. This property makes them naturally compatible with rule-based reward mechanisms. Motivated by this observation, we investigate the extension of R1-style reinforcement learning to Vision-Language Models (VLMs), aiming to enhance their visual reasoning capabilities. To this end, we develop VLM-R1, a dedicated framework designed to harness RL for improving VLMs' performance on general vision-language tasks. Using this framework, we further explore the feasibility of applying RL to visual domain. Experimental results indicate that the RL-based model not only delivers competitive performance on visual understanding tasks but also surpasses Supervised Fine-Tuning (SFT) in generalization ability. Furthermore, we conduct comprehensive ablation studies that uncover a series of noteworthy insights, including the presence of reward hacking in object detection, the emergence of the \"OD aha moment\", the impact of training data quality, and the scaling behavior of RL across different model sizes. Through these analyses, we aim to deepen the understanding of how reinforcement learning enhances the capabilities of vision-language models, and we hope our findings and open-source contributions will support continued progress in the vision-language RL community. Our code and model are available at https://github.com/om-ai-lab/VLM-R1"
  },
  {
    "title": "VLM-R1: A Stable and Generalizable R1-style Large Vision-Language Model",
    "url": "http://arxiv.org/abs/2504.07615v2",
    "arxiv_id": "2504.07615v2",
    "authors": [
      "Haozhan Shen",
      "Peng Liu",
      "Jingcheng Li",
      "Chunxin Fang",
      "Yibo Ma",
      "Jiajia Liao",
      "Qiaoli Shen",
      "Zilun Zhang",
      "Kangjia Zhao",
      "Qianqian Zhang",
      "Ruochen Xu",
      "Tiancheng Zhao"
    ],
    "published": "2025-04-10T10:05:15+00:00",
    "summary": "Recently DeepSeek R1 has shown that reinforcement learning (RL) can substantially improve the reasoning capabilities of Large Language Models (LLMs) through a simple yet effective design. The core of R1 lies in its rule-based reward formulation, which leverages tasks with deterministic ground-truth answers to enable precise and stable reward computation. In the visual domain, we similarly observe that a wide range of visual understanding tasks are inherently equipped with well-defined ground-truth annotations. This property makes them naturally compatible with rule-based reward mechanisms. Motivated by this observation, we investigate the extension of R1-style reinforcement learning to Vision-Language Models (VLMs), aiming to enhance their visual reasoning capabilities. To this end, we develop VLM-R1, a dedicated framework designed to harness RL for improving VLMs' performance on general vision-language tasks. Using this framework, we further explore the feasibility of applying RL to visual domain. Experimental results indicate that the RL-based model not only delivers competitive performance on visual understanding tasks but also surpasses Supervised Fine-Tuning (SFT) in generalization ability. Furthermore, we conduct comprehensive ablation studies that uncover a series of noteworthy insights, including the presence of reward hacking in object detection, the emergence of the \"OD aha moment\", the impact of training data quality, and the scaling behavior of RL across different model sizes. Through these analyses, we aim to deepen the understanding of how reinforcement learning enhances the capabilities of vision-language models, and we hope our findings and open-source contributions will support continued progress in the vision-language RL community. Our code and model are available at https://github.com/om-ai-lab/VLM-R1"
  },
  {
    "title": "Drive in Corridors: Enhancing the Safety of End-to-end Autonomous Driving via Corridor Learning and Planning",
    "url": "http://arxiv.org/abs/2504.07507v1",
    "arxiv_id": "2504.07507v1",
    "authors": [
      "Zhiwei Zhang",
      "Ruichen Yang",
      "Ke Wu",
      "Zijun Xu",
      "Jingchu Liu",
      "Lisen Mu",
      "Zhongxue Gan",
      "Wenchao Ding"
    ],
    "published": "2025-04-10T07:10:40+00:00",
    "summary": "Safety remains one of the most critical challenges in autonomous driving systems. In recent years, the end-to-end driving has shown great promise in advancing vehicle autonomy in a scalable manner. However, existing approaches often face safety risks due to the lack of explicit behavior constraints. To address this issue, we uncover a new paradigm by introducing the corridor as the intermediate representation. Widely adopted in robotics planning, the corridors represents spatio-temporal obstacle-free zones for the vehicle to traverse. To ensure accurate corridor prediction in diverse traffic scenarios, we develop a comprehensive learning pipeline including data annotation, architecture refinement and loss formulation. The predicted corridor is further integrated as the constraint in a trajectory optimization process. By extending the differentiability of the optimization, we enable the optimized trajectory to be seamlessly trained within the end-to-end learning framework, improving both safety and interpretability. Experimental results on the nuScenes dataset demonstrate state-of-the-art performance of our approach, showing a 66.7% reduction in collisions with agents and a 46.5% reduction with curbs, significantly enhancing the safety of end-to-end driving. Additionally, incorporating the corridor contributes to higher success rates in closed-loop evaluations."
  },
  {
    "title": "Defense against Prompt Injection Attacks via Mixture of Encodings",
    "url": "http://arxiv.org/abs/2504.07467v1",
    "arxiv_id": "2504.07467v1",
    "authors": [
      "Ruiyi Zhang",
      "David Sullivan",
      "Kyle Jackson",
      "Pengtao Xie",
      "Mei Chen"
    ],
    "published": "2025-04-10T05:35:21+00:00",
    "summary": "Large Language Models (LLMs) have emerged as a dominant approach for a wide range of NLP tasks, with their access to external information further enhancing their capabilities. However, this introduces new vulnerabilities, known as prompt injection attacks, where external content embeds malicious instructions that manipulate the LLM's output. Recently, the Base64 defense has been recognized as one of the most effective methods for reducing success rate of prompt injection attacks. Despite its efficacy, this method can degrade LLM performance on certain NLP tasks. To address this challenge, we propose a novel defense mechanism: mixture of encodings, which utilizes multiple character encodings, including Base64. Extensive experimental results show that our method achieves one of the lowest attack success rates under prompt injection attacks, while maintaining high performance across all NLP tasks, outperforming existing character encoding-based defense methods. This underscores the effectiveness of our mixture of encodings strategy for both safety and task performance metrics."
  },
  {
    "title": "Multi-Modal Data Fusion for Moisture Content Prediction in Apple Drying",
    "url": "http://arxiv.org/abs/2504.07465v1",
    "arxiv_id": "2504.07465v1",
    "authors": [
      "Shichen Li",
      "Chenhui Shao"
    ],
    "published": "2025-04-10T05:29:04+00:00",
    "summary": "Fruit drying is widely used in food manufacturing to reduce product moisture, ensure product safety, and extend product shelf life. Accurately predicting final moisture content (MC) is critically needed for quality control of drying processes. State-of-the-art methods can build deterministic relationships between process parameters and MC, but cannot adequately account for inherent process variabilities that are ubiquitous in fruit drying. To address this gap, this paper presents a novel multi-modal data fusion framework to effectively fuse two modalities of data: tabular data (process parameters) and high-dimensional image data (images of dried apple slices) to enable accurate MC prediction. The proposed modeling architecture permits flexible adjustment of information portion from tabular and image data modalities. Experimental validation shows that the multi-modal approach improves predictive accuracy substantially compared to state-of-the-art methods. The proposed method reduces root-mean-squared errors by 19.3%, 24.2%, and 15.2% over tabular-only, image-only, and standard tabular-image fusion models, respectively. Furthermore, it is demonstrated that our method is robust in varied tabular-image ratios and capable of effectively capturing inherent small-scale process variabilities. The proposed framework is extensible to a variety of other drying technologies."
  },
  {
    "title": "LoRI: Reducing Cross-Task Interference in Multi-Task Low-Rank Adaptation",
    "url": "http://arxiv.org/abs/2504.07448v1",
    "arxiv_id": "2504.07448v1",
    "authors": [
      "Juzheng Zhang",
      "Jiacheng You",
      "Ashwinee Panda",
      "Tom Goldstein"
    ],
    "published": "2025-04-10T04:46:04+00:00",
    "summary": "Low-Rank Adaptation (LoRA) has emerged as a popular parameter-efficient fine-tuning (PEFT) method for Large Language Models (LLMs), yet it still incurs notable overhead and suffers from parameter interference in multi-task scenarios. We propose LoRA with Reduced Interference (LoRI), a simple yet effective approach that freezes the projection matrices $A$ as random projections and sparsifies the matrices $B$ using task-specific masks. This design substantially reduces the number of trainable parameters while maintaining strong task performance. Moreover, LoRI minimizes cross-task interference in adapter merging by leveraging the orthogonality between adapter subspaces, and supports continual learning by using sparsity to mitigate catastrophic forgetting. Extensive experiments across natural language understanding, mathematical reasoning, code generation, and safety alignment tasks demonstrate that LoRI outperforms full fine-tuning and existing PEFT methods, while using up to 95% fewer trainable parameters than LoRA. In multi-task experiments, LoRI enables effective adapter merging and continual learning with reduced cross-task interference. Code is available at: https://github.com/juzhengz/LoRI"
  },
  {
    "title": "Electronic Warfare Cyberattacks, Countermeasures and Modern Defensive Strategies of UAV Avionics: A Survey",
    "url": "http://arxiv.org/abs/2504.07358v1",
    "arxiv_id": "2504.07358v1",
    "authors": [
      "Aaron Yu",
      "Iuliia Kolotylo",
      "Hashim A. Hashim",
      "A. E. E. Eltoukhy"
    ],
    "published": "2025-04-10T00:56:52+00:00",
    "summary": "Unmanned Aerial Vehicles (UAVs) play a pivotal role in modern autonomous air mobility, and the reliability of UAV avionics systems is critical to ensuring mission success, sustainability practices, and public safety. The success of UAV missions depends on effectively mitigating various aspects of electronic warfare, including non-destructive and destructive cyberattacks, transponder vulnerabilities, and jamming threats, while rigorously implementing countermeasures and defensive aids. This paper provides a comprehensive review of UAV cyberattacks, countermeasures, and defensive strategies. It explores UAV-to-UAV coordination attacks and their associated features, such as dispatch system attacks, Automatic Dependent Surveillance-Broadcast (ADS-B) attacks, Traffic Alert and Collision Avoidance System (TCAS)-induced collisions, and TCAS attacks. Additionally, the paper examines UAV-to-command center coordination attacks, as well as UAV functionality attacks. The review also covers various countermeasures and defensive aids designed for UAVs. Lastly, a comparison of common cyberattacks and countermeasure approaches is conducted, along with a discussion of future trends in the field. Keywords: Electronic warfare, UAVs, Avionics Systems, cyberattacks, coordination attacks, functionality attacks, countermeasure, defensive-aids."
  },
  {
    "title": "Revisiting Prompt Optimization with Large Reasoning Models-A Case Study on Event Extraction",
    "url": "http://arxiv.org/abs/2504.07357v1",
    "arxiv_id": "2504.07357v1",
    "authors": [
      "Saurabh Srivastava",
      "Ziyu Yao"
    ],
    "published": "2025-04-10T00:53:59+00:00",
    "summary": "Large Reasoning Models (LRMs) such as DeepSeek-R1 and OpenAI o1 have demonstrated remarkable capabilities in various reasoning tasks. Their strong capability to generate and reason over intermediate thoughts has also led to arguments that they may no longer require extensive prompt engineering or optimization to interpret human instructions and produce accurate outputs. In this work, we aim to systematically study this open question, using the structured task of event extraction for a case study. We experimented with two LRMs (DeepSeek-R1 and o1) and two general-purpose Large Language Models (LLMs) (GPT-4o and GPT-4.5), when they were used as task models or prompt optimizers. Our results show that on tasks as complicated as event extraction, LRMs as task models still benefit from prompt optimization, and that using LRMs as prompt optimizers yields more effective prompts. Finally, we provide an error analysis of common errors made by LRMs and highlight the stability and consistency of LRMs in refining task instructions and event guidelines."
  },
  {
    "title": "Code Generation with Small Language Models: A Deep Evaluation on Codeforces",
    "url": "http://arxiv.org/abs/2504.07343v1",
    "arxiv_id": "2504.07343v1",
    "authors": [
      "D\u00e9bora Souza",
      "Rohit Gheyi",
      "Lucas Albuquerque",
      "Gustavo Soares",
      "M\u00e1rcio Ribeiro"
    ],
    "published": "2025-04-09T23:57:44+00:00",
    "summary": "Large Language Models (LLMs) have demonstrated capabilities in code generation, potentially boosting developer productivity. However, their widespread adoption remains limited by high computational costs, significant energy demands, and security risks such as data leakage and adversarial attacks. As a lighter-weight alternative, Small Language Models (SLMs) offer faster inference, lower deployment overhead, and better adaptability to domain-specific tasks, making them an attractive option for real-world applications. While prior research has benchmarked LLMs on competitive programming tasks, such evaluations often focus narrowly on metrics like Elo scores or pass rates, overlooking deeper insights into model behavior, failure patterns, and problem diversity. Furthermore, the potential of SLMs to tackle complex tasks such as competitive programming remains underexplored. In this study, we benchmark five open SLMs - LLAMA 3.2 3B, GEMMA 2 9B, GEMMA 3 12B, DEEPSEEK-R1 14B, and PHI-4 14B - across 280 Codeforces problems spanning Elo ratings from 800 to 2100 and covering 36 distinct topics. All models were tasked with generating Python solutions. PHI-4 14B achieved the best performance among SLMs, with a pass@3 of 63.6%, approaching the proprietary O3-MINI-HIGH (86.8%). In addition, we evaluated PHI-4 14B on C++ and found that combining outputs from both Python and C++ increases its aggregated pass@3 to 73.6%. A qualitative analysis of PHI-4 14B's incorrect outputs revealed that some failures were due to minor implementation issues - such as handling edge cases or correcting variable initialization - rather than deeper reasoning flaws."
  },
  {
    "title": "Agentic SLMs: Hunting Down Test Smells",
    "url": "http://arxiv.org/abs/2504.07277v1",
    "arxiv_id": "2504.07277v1",
    "authors": [
      "Rian Melo",
      "Pedro Sim\u00f5es",
      "Rohit Gheyi",
      "Marcelo d'Amorim",
      "M\u00e1rcio Ribeiro",
      "Gustavo Soares",
      "Eduardo Almeida",
      "Elvys Soares"
    ],
    "published": "2025-04-09T21:12:01+00:00",
    "summary": "Test smells can compromise the reliability of test suites and hinder software maintenance. Although several strategies exist for detecting test smells, few address their removal. Traditional methods often rely on static analysis or machine learning, requiring significant effort and expertise. This study evaluates LLAMA 3.2 3B, GEMMA 2 9B, DEEPSEEK-R1 14B, and PHI 4 14B - small, open language models - for automating the detection and refactoring of test smells through agent-based workflows. We explore workflows with one, two, and four agents across 150 instances of 5 common test smell types extracted from real-world Java projects. Unlike prior approaches, ours is easily extensible to new smells via natural language definitions and generalizes to Python and Golang. All models detected nearly all test smell instances (pass@5 of 96% with four agents), with PHI 4 14B achieving the highest refactoring accuracy (pass@5 of 75.3%). Analyses were computationally inexpensive and ran efficiently on a consumer-grade hardware. Notably, PHI 4 14B with four agents performed within 5% of proprietary models such as O1-MINI, O3-MINI-HIGH, and GEMINI 2.5 PRO EXPERIMENTAL using a single agent. Multi-agent setups outperformed single-agent ones in three out of five test smell types, highlighting their potential to improve software quality with minimal developer effort. For the Assertion Roulette smell, however, a single agent performed better. To assess practical relevance, we submitted 10 pull requests with PHI 4 14B - generated code to open-source projects. Five were merged, one was rejected, and four remain under review, demonstrating the approach's real-world applicability."
  },
  {
    "title": "Sculpting Subspaces: Constrained Full Fine-Tuning in LLMs for Continual Learning",
    "url": "http://arxiv.org/abs/2504.07097v1",
    "arxiv_id": "2504.07097v1",
    "authors": [
      "Nikhil Shivakumar Nayak",
      "Krishnateja Killamsetty",
      "Ligong Han",
      "Abhishek Bhandwaldar",
      "Prateek Chanda",
      "Kai Xu",
      "Hao Wang",
      "Aldo Pareja",
      "Oleg Silkin",
      "Mustafa Eyceoz",
      "Akash Srivastava"
    ],
    "published": "2025-04-09T17:59:42+00:00",
    "summary": "Continual learning in large language models (LLMs) is prone to catastrophic forgetting, where adapting to new tasks significantly degrades performance on previously learned ones. Existing methods typically rely on low-rank, parameter-efficient updates that limit the model's expressivity and introduce additional parameters per task, leading to scalability issues. To address these limitations, we propose a novel continual full fine-tuning approach leveraging adaptive singular value decomposition (SVD). Our method dynamically identifies task-specific low-rank parameter subspaces and constrains updates to be orthogonal to critical directions associated with prior tasks, thus effectively minimizing interference without additional parameter overhead or storing previous task gradients. We evaluate our approach extensively on standard continual learning benchmarks using both encoder-decoder (T5-Large) and decoder-only (LLaMA-2 7B) models, spanning diverse tasks including classification, generation, and reasoning. Empirically, our method achieves state-of-the-art results, up to 7% higher average accuracy than recent baselines like O-LoRA, and notably maintains the model's general linguistic capabilities, instruction-following accuracy, and safety throughout the continual learning process by reducing forgetting to near-negligible levels. Our adaptive SVD framework effectively balances model plasticity and knowledge retention, providing a practical, theoretically grounded, and computationally scalable solution for continual learning scenarios in large language models."
  },
  {
    "title": "OmniCaptioner: One Captioner to Rule Them All",
    "url": "http://arxiv.org/abs/2504.07089v1",
    "arxiv_id": "2504.07089v1",
    "authors": [
      "Yiting Lu",
      "Jiakang Yuan",
      "Zhen Li",
      "Shitian Zhao",
      "Qi Qin",
      "Xinyue Li",
      "Le Zhuo",
      "Licheng Wen",
      "Dongyang Liu",
      "Yuewen Cao",
      "Xiangchao Yan",
      "Xin Li",
      "Botian Shi",
      "Tao Chen",
      "Zhibo Chen",
      "Lei Bai",
      "Bo Zhang",
      "Peng Gao"
    ],
    "published": "2025-04-09T17:58:58+00:00",
    "summary": "We propose OmniCaptioner, a versatile visual captioning framework for generating fine-grained textual descriptions across a wide variety of visual domains. Unlike prior methods limited to specific image types (e.g., natural images or geometric visuals), our framework provides a unified solution for captioning natural images, visual text (e.g., posters, UIs, textbooks), and structured visuals (e.g., documents, tables, charts). By converting low-level pixel information into semantically rich textual representations, our framework bridges the gap between visual and textual modalities. Our results highlight three key advantages: (i) Enhanced Visual Reasoning with LLMs, where long-context captions of visual modalities empower LLMs, particularly the DeepSeek-R1 series, to reason effectively in multimodal scenarios; (ii) Improved Image Generation, where detailed captions improve tasks like text-to-image generation and image transformation; and (iii) Efficient Supervised Fine-Tuning (SFT), which enables faster convergence with less data. We believe the versatility and adaptability of OmniCaptioner can offer a new perspective for bridging the gap between language and visual modalities."
  },
  {
    "title": "R2E-Gym: Procedural Environments and Hybrid Verifiers for Scaling Open-Weights SWE Agents",
    "url": "http://arxiv.org/abs/2504.07164v1",
    "arxiv_id": "2504.07164v1",
    "authors": [
      "Naman Jain",
      "Jaskirat Singh",
      "Manish Shetty",
      "Liang Zheng",
      "Koushik Sen",
      "Ion Stoica"
    ],
    "published": "2025-04-09T17:55:19+00:00",
    "summary": "Improving open-source models on real-world SWE tasks (solving GITHUB issues) faces two key challenges: 1) scalable curation of execution environments to train these models, and, 2) optimal scaling of test-time compute. We introduce AgentGym, the largest procedurally-curated executable gym environment for training real-world SWE-agents, consisting of more than 8.7K tasks. AgentGym is powered by two main contributions: 1) SYNGEN: a synthetic data curation recipe that enables scalable curation of executable environments using test-generation and back-translation directly from commits, thereby reducing reliance on human-written issues or unit tests. We show that this enables more scalable training leading to pass@1 performance of 34.4% on SWE-Bench Verified benchmark with our 32B model. 2) Hybrid Test-time Scaling: we provide an in-depth analysis of two test-time scaling axes; execution-based and execution-free verifiers, demonstrating that they exhibit complementary strengths and limitations. Test-based verifiers suffer from low distinguishability, while execution-free verifiers are biased and often rely on stylistic features. Surprisingly, we find that while each approach individually saturates around 42-43%, significantly higher gains can be obtained by leveraging their complementary strengths. Overall, our approach achieves 51% on the SWE-Bench Verified benchmark, reflecting a new state-of-the-art for open-weight SWE-agents and for the first time showing competitive performance with proprietary models such as o1, o1-preview and sonnet-3.5-v2 (with tools). We will open-source our environments, models, and agent trajectories."
  },
  {
    "title": "Self-Steering Language Models",
    "url": "http://arxiv.org/abs/2504.07081v1",
    "arxiv_id": "2504.07081v1",
    "authors": [
      "Gabriel Grand",
      "Joshua B. Tenenbaum",
      "Vikash K. Mansinghka",
      "Alexander K. Lew",
      "Jacob Andreas"
    ],
    "published": "2025-04-09T17:54:22+00:00",
    "summary": "While test-time reasoning enables language models to tackle complex tasks, searching or planning in natural language can be slow, costly, and error-prone. But even when LMs struggle to emulate the precise reasoning steps needed to solve a problem, they often excel at describing its abstract structure--both how to verify solutions and how to search for them. This paper introduces DisCIPL, a method for \"self-steering\" LMs where a Planner model generates a task-specific inference program that is executed by a population of Follower models. Our approach equips LMs with the ability to write recursive search procedures that guide LM inference, enabling new forms of verifiable and efficient reasoning. When instantiated with a small Follower (e.g., Llama-3.2-1B), DisCIPL matches (and sometimes outperforms) much larger models, including GPT-4o and o1, on challenging constrained generation tasks. In decoupling planning from execution, our work opens up a design space of highly-parallelized Monte Carlo inference strategies that outperform standard best-of-N sampling, require no finetuning, and can be implemented automatically by existing LMs."
  },
  {
    "title": "Evaluating Retrieval Augmented Generative Models for Document Queries in Transportation Safety",
    "url": "http://arxiv.org/abs/2504.07022v1",
    "arxiv_id": "2504.07022v1",
    "authors": [
      "Chad Melton",
      "Alex Sorokine",
      "Steve Peterson"
    ],
    "published": "2025-04-09T16:37:03+00:00",
    "summary": "Applications of generative Large Language Models LLMs are rapidly expanding across various domains, promising significant improvements in workflow efficiency and information retrieval. However, their implementation in specialized, high-stakes domains such as hazardous materials transportation is challenging due to accuracy and reliability concerns. This study evaluates the performance of three fine-tuned generative models, ChatGPT, Google's Vertex AI, and ORNL Retrieval Augmented Generation augmented LLaMA 2 and LLaMA in retrieving regulatory information essential for hazardous material transportation compliance in the United States. Utilizing approximately 40 publicly available federal and state regulatory documents, we developed 100 realistic queries relevant to route planning and permitting requirements. Responses were qualitatively rated based on accuracy, detail, and relevance, complemented by quantitative assessments of semantic similarity between model outputs. Results demonstrated that the RAG-augmented LLaMA models significantly outperformed Vertex AI and ChatGPT, providing more detailed and generally accurate information, despite occasional inconsistencies. This research introduces the first known application of RAG in transportation safety, emphasizing the need for domain-specific fine-tuning and rigorous evaluation methodologies to ensure reliability and minimize the risk of inaccuracies in high-stakes environments."
  },
  {
    "title": "Quantum Field Theory on Multifractal Spacetime: Varying Dimension and Ultraviolet Completeness",
    "url": "http://arxiv.org/abs/2504.06797v1",
    "arxiv_id": "2504.06797v1",
    "authors": [
      "Alessio Maiezza",
      "Juan Carlos Vasquez"
    ],
    "published": "2025-04-09T11:41:15+00:00",
    "summary": "Inspired by various quantum gravity approaches, we explore quantum field theory where spacetime exhibits scaling properties and dimensional reduction with changing energy scales, effectively behaving as a multifractal manifold. Working within canonical quantization, we demonstrate how to properly quantize fields in such multifractal spacetime. Our analysis reveals that a non-differentiable nature of spacetime is not merely compatible with quantum field theory but significantly enhances its mathematical foundation. Most notably, this approach guarantees the finiteness of the theory at all orders in perturbation theory and enables rigorous construction of the S-matrix in the interaction picture. The multifractal structure tames dominant, large-order divergence sources in the perturbative series and resolves the Landau pole problem through asymptotic safety, substantially improving the theory's behavior in the deep ultraviolet regime. Our formulation preserves all established predictions of standard quantum field theory at low energies while offering novel physical behaviors at high energy scales."
  },
  {
    "title": "Domain-Specific Pruning of Large Mixture-of-Experts Models with Few-shot Demonstrations",
    "url": "http://arxiv.org/abs/2504.06792v1",
    "arxiv_id": "2504.06792v1",
    "authors": [
      "Zican Dong",
      "Han Peng",
      "Peiyu Liu",
      "Wayne Xin Zhao",
      "Dong Wu",
      "Feng Xiao",
      "Zhifeng Wang"
    ],
    "published": "2025-04-09T11:34:06+00:00",
    "summary": "Mixture-of-Experts (MoE) models achieve a favorable trade-off between performance and inference efficiency by activating only a subset of experts. However, the memory overhead of storing all experts remains a major limitation, especially in large-scale MoE models such as DeepSeek-R1 (671B). In this study, we investigate domain specialization and expert redundancy in large-scale MoE models and uncover a consistent behavior we term few-shot expert localization, with only a few demonstrations, the model consistently activates a sparse and stable subset of experts. Building on this observation, we propose a simple yet effective pruning framework, EASY-EP, that leverages a few domain-specific demonstrations to identify and retain only the most relevant experts. EASY-EP comprises two key components: output-aware expert importance assessment and expert-level token contribution estimation. The former evaluates the importance of each expert for the current token by considering the gating scores and magnitudes of the outputs of activated experts, while the latter assesses the contribution of tokens based on representation similarities after and before routed experts. Experiments show that our method can achieve comparable performances and $2.99\\times$ throughput under the same memory budget with full DeepSeek-R1 with only half the experts. Our code is available at https://github.com/RUCAIBox/EASYEP."
  },
  {
    "title": "Beware of \"Explanations\" of AI",
    "url": "http://arxiv.org/abs/2504.06791v1",
    "arxiv_id": "2504.06791v1",
    "authors": [
      "David Martens",
      "Galit Shmueli",
      "Theodoros Evgeniou",
      "Kevin Bauer",
      "Christian Janiesch",
      "Stefan Feuerriegel",
      "Sebastian Gabel",
      "Sofie Goethals",
      "Travis Greene",
      "Nadja Klein",
      "Mathias Kraus",
      "Niklas K\u00fchl",
      "Claudia Perlich",
      "Wouter Verbeke",
      "Alona Zharova",
      "Patrick Zschech",
      "Foster Provost"
    ],
    "published": "2025-04-09T11:31:08+00:00",
    "summary": "Understanding the decisions made and actions taken by increasingly complex AI system remains a key challenge. This has led to an expanding field of research in explainable artificial intelligence (XAI), highlighting the potential of explanations to enhance trust, support adoption, and meet regulatory standards. However, the question of what constitutes a \"good\" explanation is dependent on the goals, stakeholders, and context. At a high level, psychological insights such as the concept of mental model alignment can offer guidance, but success in practice is challenging due to social and technical factors. As a result of this ill-defined nature of the problem, explanations can be of poor quality (e.g. unfaithful, irrelevant, or incoherent), potentially leading to substantial risks. Instead of fostering trust and safety, poorly designed explanations can actually cause harm, including wrong decisions, privacy violations, manipulation, and even reduced AI adoption. Therefore, we caution stakeholders to beware of explanations of AI: while they can be vital, they are not automatically a remedy for transparency or responsible AI adoption, and their misuse or limitations can exacerbate harm. Attention to these caveats can help guide future research to improve the quality and impact of AI explanations."
  },
  {
    "title": "Holistic Capability Preservation: Towards Compact Yet Comprehensive Reasoning Models",
    "url": "http://arxiv.org/abs/2504.07158v1",
    "arxiv_id": "2504.07158v1",
    "authors": [
      "Ling Team",
      "Caizhi Tang",
      "Chilin Fu",
      "Chunwei Wu",
      "Jia Guo",
      "Jianwen Wang",
      "Jingyu Hu",
      "Liang Jiang",
      "Meng Li",
      "Peng Jiao",
      "Pingping Liu",
      "Shaomian Zheng",
      "Shiwei Liang",
      "Shuaicheng Li",
      "Yalin Zhang",
      "Yingting Wu",
      "Yongkang Liu",
      "Zhenyu Huang"
    ],
    "published": "2025-04-09T11:24:32+00:00",
    "summary": "This technical report presents Ring-Lite-Distill, a lightweight reasoning model derived from our open-source Mixture-of-Experts (MoE) Large Language Models (LLMs) Ling-Lite. This study demonstrates that through meticulous high-quality data curation and ingenious training paradigms, the compact MoE model Ling-Lite can be further trained to achieve exceptional reasoning capabilities, while maintaining its parameter-efficient architecture with only 2.75 billion activated parameters, establishing an efficient lightweight reasoning architecture. In particular, in constructing this model, we have not merely focused on enhancing advanced reasoning capabilities, exemplified by high-difficulty mathematical problem solving, but rather aimed to develop a reasoning model with more comprehensive competency coverage. Our approach ensures coverage across reasoning tasks of varying difficulty levels while preserving generic capabilities, such as instruction following, tool use, and knowledge retention. We show that, Ring-Lite-Distill's reasoning ability reaches a level comparable to DeepSeek-R1-Distill-Qwen-7B, while its general capabilities significantly surpass those of DeepSeek-R1-Distill-Qwen-7B. The models are accessible at https://huggingface.co/inclusionAI"
  },
  {
    "title": "Holistic Capability Preservation: Towards Compact Yet Comprehensive Reasoning Models",
    "url": "http://arxiv.org/abs/2504.07158v2",
    "arxiv_id": "2504.07158v2",
    "authors": [
      "Ling Team",
      "Caizhi Tang",
      "Chilin Fu",
      "Chunwei Wu",
      "Jia Guo",
      "Jianwen Wang",
      "Jingyu Hu",
      "Liang Jiang",
      "Meng Li",
      "Peng Jiao",
      "Pingping Liu",
      "Shaomian Zheng",
      "Shiwei Liang",
      "Shuaicheng Li",
      "Yalin Zhang",
      "Yingting Wu",
      "Yongkang Liu",
      "Zhenyu Huang"
    ],
    "published": "2025-04-09T11:24:32+00:00",
    "summary": "This technical report presents Ring-Lite-Distill, a lightweight reasoning model derived from our open-source Mixture-of-Experts (MoE) Large Language Models (LLMs) Ling-Lite. This study demonstrates that through meticulous high-quality data curation and ingenious training paradigms, the compact MoE model Ling-Lite can be further trained to achieve exceptional reasoning capabilities, while maintaining its parameter-efficient architecture with only 2.75 billion activated parameters, establishing an efficient lightweight reasoning architecture. In particular, in constructing this model, we have not merely focused on enhancing advanced reasoning capabilities, exemplified by high-difficulty mathematical problem solving, but rather aimed to develop a reasoning model with more comprehensive competency coverage. Our approach ensures coverage across reasoning tasks of varying difficulty levels while preserving generic capabilities, such as instruction following, tool use, and knowledge retention. We show that, Ring-Lite-Distill's reasoning ability reaches a level comparable to DeepSeek-R1-Distill-Qwen-7B, while its general capabilities significantly surpass those of DeepSeek-R1-Distill-Qwen-7B. The models are accessible at https://huggingface.co/inclusionAI"
  },
  {
    "title": "Zero-Shot Image-Based Large Language Model Approach to Road Pavement Monitoring",
    "url": "http://arxiv.org/abs/2504.06785v1",
    "arxiv_id": "2504.06785v1",
    "authors": [
      "Shuoshuo Xu",
      "Kai Zhao",
      "James Loney",
      "Zili Li",
      "Andrea Visentin"
    ],
    "published": "2025-04-09T11:19:17+00:00",
    "summary": "Effective and rapid evaluation of pavement surface condition is critical for prioritizing maintenance, ensuring transportation safety, and minimizing vehicle wear and tear. While conventional manual inspections suffer from subjectivity, existing machine learning-based methods are constrained by their reliance on large and high-quality labeled datasets, which require significant resources and limit adaptability across varied road conditions. The revolutionary advancements in Large Language Models (LLMs) present significant potential for overcoming these challenges. In this study, we propose an innovative automated zero-shot learning approach that leverages the image recognition and natural language understanding capabilities of LLMs to assess road conditions effectively. Multiple LLM-based assessment models were developed, employing prompt engineering strategies aligned with the Pavement Surface Condition Index (PSCI) standards. These models' accuracy and reliability were evaluated against official PSCI results, with an optimized model ultimately selected. Extensive tests benchmarked the optimized model against evaluations from various levels experts using Google Street View road images. The results reveal that the LLM-based approach can effectively assess road conditions, with the optimized model -employing comprehensive and structured prompt engineering strategies -outperforming simpler configurations by achieving high accuracy and consistency, even surpassing expert evaluations. Moreover, successfully applying the optimized model to Google Street View images demonstrates its potential for future city-scale deployments. These findings highlight the transformative potential of LLMs in automating road damage evaluations and underscore the pivotal role of detailed prompt engineering in achieving reliable assessments."
  },
  {
    "title": "Dynamic Residual Safe Reinforcement Learning for Multi-Agent Safety-Critical Scenarios Decision-Making",
    "url": "http://arxiv.org/abs/2504.06670v1",
    "arxiv_id": "2504.06670v1",
    "authors": [
      "Kaifeng Wang",
      "Yinsong Chen",
      "Qi Liu",
      "Xueyuan Li",
      "Xin Gao"
    ],
    "published": "2025-04-09T08:13:14+00:00",
    "summary": "In multi-agent safety-critical scenarios, traditional autonomous driving frameworks face significant challenges in balancing safety constraints and task performance. These frameworks struggle to quantify dynamic interaction risks in real-time and depend heavily on manual rules, resulting in low computational efficiency and conservative strategies. To address these limitations, we propose a Dynamic Residual Safe Reinforcement Learning (DRS-RL) framework grounded in a safety-enhanced networked Markov decision process. It's the first time that the weak-to-strong theory is introduced into multi-agent decision-making, enabling lightweight dynamic calibration of safety boundaries via a weak-to-strong safety correction paradigm. Based on the multi-agent dynamic conflict zone model, our framework accurately captures spatiotemporal coupling risks among heterogeneous traffic participants and surpasses the static constraints of conventional geometric rules. Moreover, a risk-aware prioritized experience replay mechanism mitigates data distribution bias by mapping risk to sampling probability. Experimental results reveal that the proposed method significantly outperforms traditional RL algorithms in safety, efficiency, and comfort. Specifically, it reduces the collision rate by up to 92.17%, while the safety model accounts for merely 27% of the main model's parameters."
  },
  {
    "title": "Toward Holistic Evaluation of Recommender Systems Powered by Generative Models",
    "url": "http://arxiv.org/abs/2504.06667v1",
    "arxiv_id": "2504.06667v1",
    "authors": [
      "Yashar Deldjoo",
      "Nikhil Mehta",
      "Maheswaran Sathiamoorthy",
      "Shuai Zhang",
      "Pablo Castells",
      "Julian McAuley"
    ],
    "published": "2025-04-09T08:08:16+00:00",
    "summary": "Recommender systems powered by generative models (Gen-RecSys) extend beyond classical item ranking by producing open-ended content, which simultaneously unlocks richer user experiences and introduces new risks. On one hand, these systems can enhance personalization and appeal through dynamic explanations and multi-turn dialogues. On the other hand, they might venture into unknown territory-hallucinating nonexistent items, amplifying bias, or leaking private information. Traditional accuracy metrics cannot fully capture these challenges, as they fail to measure factual correctness, content safety, or alignment with user intent.   This paper makes two main contributions. First, we categorize the evaluation challenges of Gen-RecSys into two groups: (i) existing concerns that are exacerbated by generative outputs (e.g., bias, privacy) and (ii) entirely new risks (e.g., item hallucinations, contradictory explanations). Second, we propose a holistic evaluation approach that includes scenario-based assessments and multi-metric checks-incorporating relevance, factual grounding, bias detection, and policy compliance. Our goal is to provide a guiding framework so researchers and practitioners can thoroughly assess Gen-RecSys, ensuring effective personalization and responsible deployment."
  },
  {
    "title": "Uni-PrevPredMap: Extending PrevPredMap to a Unified Framework of Prior-Informed Modeling for Online Vectorized HD Map Construction",
    "url": "http://arxiv.org/abs/2504.06647v2",
    "arxiv_id": "2504.06647v2",
    "authors": [
      "Nan Peng",
      "Xun Zhou",
      "Mingming Wang",
      "Guisong Chen",
      "Wenqi Xu"
    ],
    "published": "2025-04-09T07:36:17+00:00",
    "summary": "Safety constitutes a foundational imperative for autonomous driving systems, necessitating the maximal incorporation of accessible external prior information. This study establishes that temporal perception buffers and cost-efficient maps inherently form complementary prior sources for online vectorized high-definition (HD) map construction. We present Uni-PrevPredMap, a unified prior-informed framework that systematically integrates two synergistic information sources: previous predictions and simulated outdated HD maps. The framework introduces two core innovations: a tile-indexed 3D vectorized global map processor enabling efficient refreshment, storage, and retrieval of 3D vectorized priors; a tri-mode operational optimization paradigm ensuring consistency across non-prior, temporal-prior, and temporal-map-fusion-prior scenarios while mitigating reliance on idealized map fidelity assumptions. Uni-PrevPredMap achieves state-of-the-art performance in map-absent scenarios across established online vectorized HD map construction benchmarks. When provided with simulated outdated HD maps, the framework exhibits robust capabilities in error-resilient prior fusion, empirically confirming the synergistic complementarity between previous predictions and simulated outdated HD maps. Code will be available at https://github.com/pnnnnnnn/Uni-PrevPredMap."
  },
  {
    "title": "Harmful information spreading and its impact on vaccination campaigns modeled through fractal-fractional operators",
    "url": "http://arxiv.org/abs/2504.06582v1",
    "arxiv_id": "2504.06582v1",
    "authors": [
      "Ali Akg\u00fcl",
      "Auwalu Hamisu Usman",
      "J. Alberto Conejero"
    ],
    "published": "2025-04-09T05:04:17+00:00",
    "summary": "Despite the huge efforts to develop and administer vaccines worldwide to cope with the COVID-19 pandemic, misinformation spreading through fake news in media and social networks about vaccination safety, make that people refuse to be vaccinated, which harms not only these people but also the whole population.   In this work, we model the effects of harmful information spreading in immunization acquisition through vaccination. Our model is posed for several fractional derivative operators. We have conducted a comprehensive foundation analysis of this model for the different fractional derivatives. Additionally, we have incorporated a strength parameter that shows the combined impact of nonlinear and linear components within an epidemiological model. We have used the second derivative of the Lyapunov function to ascertain the detection of wave patterns within the vaccination dynamics."
  },
  {
    "title": "Bypassing Safety Guardrails in LLMs Using Humor",
    "url": "http://arxiv.org/abs/2504.06577v1",
    "arxiv_id": "2504.06577v1",
    "authors": [
      "Pedro Cisneros-Velarde"
    ],
    "published": "2025-04-09T04:58:14+00:00",
    "summary": "In this paper, we show it is possible to bypass the safety guardrails of large language models (LLMs) through a humorous prompt including the unsafe request. In particular, our method does not edit the unsafe request and follows a fixed template -- it is simple to implement and does not need additional LLMs to craft prompts. Extensive experiments show the effectiveness of our method across different LLMs. We also show that both removing and adding more humor to our method can reduce its effectiveness -- excessive humor possibly distracts the LLM from fulfilling its unsafe request. Thus, we argue that LLM jailbreaking occurs when there is a proper balance between focus on the unsafe request and presence of humor."
  },
  {
    "title": "Do Reasoning Models Show Better Verbalized Calibration?",
    "url": "http://arxiv.org/abs/2504.06564v1",
    "arxiv_id": "2504.06564v1",
    "authors": [
      "Qingcheng Zeng",
      "Weihao Xuan",
      "Leyang Cui",
      "Rob Voigt"
    ],
    "published": "2025-04-09T03:58:19+00:00",
    "summary": "Large reasoning models (LRMs) have recently shown impressive capabilities in complex reasoning by leveraging increased test-time computation and exhibiting behaviors akin to human-like deliberation. Despite these advances, it remains an open question whether LRMs are better calibrated - particularly in their verbalized confidence - compared to instruction-tuned counterparts. In this paper, we investigate the calibration properties of LRMs trained via supervised fine-tuning distillation on long reasoning traces (henceforth SFT reasoning models) and outcome-based reinforcement learning for reasoning (henceforth RL reasoning models) across diverse domains. Our findings reveal that LRMs significantly outperform instruction-tuned models on complex reasoning tasks in both accuracy and confidence calibration. In contrast, we find surprising trends in the domain of factuality in particular. On factuality tasks, while Deepseek-R1 shows strong calibration behavior, smaller QwQ-32B shows no improvement over instruct models; moreover, SFT reasoning models display worse calibration (greater overconfidence) compared to instruct models. Our results provide evidence for a potentially critical role of reasoning-oriented RL training in improving LLMs' capacity for generating trustworthy, self-aware outputs."
  },
  {
    "title": "Data-Driven Reachability with Scenario Optimization and the Holdout Method",
    "url": "http://arxiv.org/abs/2504.06541v1",
    "arxiv_id": "2504.06541v1",
    "authors": [
      "Elizabeth Dietrich",
      "Rosalyn Devonport",
      "Stephen Tu",
      "Murat Arcak"
    ],
    "published": "2025-04-09T02:46:03+00:00",
    "summary": "Reachability analysis is an important method in providing safety guarantees for systems with unknown or uncertain dynamics. Due to the computational intractability of exact reachability analysis for general nonlinear, high-dimensional systems, recent work has focused on the use of probabilistic methods for computing approximate reachable sets. In this work, we advocate for the use of a general purpose, practical, and sharp method for data-driven reachability: the holdout method. Despite the simplicity of the holdout method, we show -- on several numerical examples including scenario-based reach tubes -- that the resulting probabilistic bounds are substantially sharper and require fewer samples than existing methods for data-driven reachability. Furthermore, we complement our work with a discussion on the necessity of probabilistic reachability bounds. We argue that any method that attempts to de-randomize the bounds, by converting the guarantees to hold deterministically, requires (a) an exponential in state-dimension amount of samples to achieve non-vacuous guarantees, and (b) extra assumptions on the dynamics."
  },
  {
    "title": "TSP-OCS: A Time-Series Prediction for Optimal Camera Selection in Multi-Viewpoint Surgical Video Analysis",
    "url": "http://arxiv.org/abs/2504.06527v1",
    "arxiv_id": "2504.06527v1",
    "authors": [
      "Xinyu Liu",
      "Xiaoguang Lin",
      "Xiang Liu",
      "Yong Yang",
      "Hongqian Wang",
      "Qilong Sun"
    ],
    "published": "2025-04-09T02:07:49+00:00",
    "summary": "Recording the open surgery process is essential for educational and medical evaluation purposes; however, traditional single-camera methods often face challenges such as occlusions caused by the surgeon's head and body, as well as limitations due to fixed camera angles, which reduce comprehensibility of the video content. This study addresses these limitations by employing a multi-viewpoint camera recording system, capturing the surgical procedure from six different angles to mitigate occlusions. We propose a fully supervised learning-based time series prediction method to choose the best shot sequences from multiple simultaneously recorded video streams, ensuring optimal viewpoints at each moment. Our time series prediction model forecasts future camera selections by extracting and fusing visual and semantic features from surgical videos using pre-trained models. These features are processed by a temporal prediction network with TimeBlocks to capture sequential dependencies. A linear embedding layer reduces dimensionality, and a Softmax classifier selects the optimal camera view based on the highest probability. In our experiments, we created five groups of open thyroidectomy videos, each with simultaneous recordings from six different angles. The results demonstrate that our method achieves competitive accuracy compared to traditional supervised methods, even when predicting over longer time horizons. Furthermore, our approach outperforms state-of-the-art time series prediction techniques on our dataset. This manuscript makes a unique contribution by presenting an innovative framework that advances surgical video analysis techniques, with significant implications for improving surgical education and patient safety."
  },
  {
    "title": "Safe Navigation in Uncertain Crowded Environments Using Risk Adaptive CVaR Barrier Functions",
    "url": "http://arxiv.org/abs/2504.06513v1",
    "arxiv_id": "2504.06513v1",
    "authors": [
      "Xinyi Wang",
      "Taekyung Kim",
      "Bardh Hoxha",
      "Georgios Fainekos",
      "Dimitra Panagou"
    ],
    "published": "2025-04-09T01:23:44+00:00",
    "summary": "Robot navigation in dynamic, crowded environments poses a significant challenge due to the inherent uncertainties in the obstacle model. In this work, we propose a risk-adaptive approach based on the Conditional Value-at-Risk Barrier Function (CVaR-BF), where the risk level is automatically adjusted to accept the minimum necessary risk, achieving a good performance in terms of safety and optimization feasibility under uncertainty. Additionally, we introduce a dynamic zone-based barrier function which characterizes the collision likelihood by evaluating the relative state between the robot and the obstacle. By integrating risk adaptation with this new function, our approach adaptively expands the safety margin, enabling the robot to proactively avoid obstacles in highly dynamic environments. Comparisons and ablation studies demonstrate that our method outperforms existing social navigation approaches, and validate the effectiveness of our proposed framework."
  },
  {
    "title": "Can LLMs Simulate Personas with Reversed Performance? A Benchmark for Counterfactual Instruction Following",
    "url": "http://arxiv.org/abs/2504.06460v1",
    "arxiv_id": "2504.06460v1",
    "authors": [
      "Sai Adith Senthil Kumar",
      "Hao Yan",
      "Saipavan Perepa",
      "Murong Yue",
      "Ziyu Yao"
    ],
    "published": "2025-04-08T22:00:32+00:00",
    "summary": "Large Language Models (LLMs) are now increasingly widely used to simulate personas in virtual environments, leveraging their instruction-following capability. However, we discovered that even state-of-the-art LLMs cannot simulate personas with reversed performance (e.g., student personas with low proficiency in educational settings), which impairs the simulation diversity and limits the practical applications of the simulated environments. In this work, using mathematical reasoning as a representative scenario, we propose the first benchmark dataset for evaluating LLMs on simulating personas with reversed performance, a capability that we dub \"counterfactual instruction following\". We evaluate both open-weight and closed-source LLMs on this task and find that LLMs, including the OpenAI o1 reasoning model, all struggle to follow counterfactual instructions for simulating reversedly performing personas. Intersectionally simulating both the performance level and the race population of a persona worsens the effect even further. These results highlight the challenges of counterfactual instruction following and the need for further research."
  },
  {
    "title": "Extended Version: Multi-Robot Motion Planning with Cooperative Localization",
    "url": "http://arxiv.org/abs/2504.06429v1",
    "arxiv_id": "2504.06429v1",
    "authors": [
      "Anne Theurkauf",
      "Nisar Ahmed",
      "Morteza Lahijanian"
    ],
    "published": "2025-04-08T20:58:19+00:00",
    "summary": "We consider the uncertain multi-robot motion planning (MRMP) problem with cooperative localization (CL-MRMP), under both motion and measurement noise, where each robot can act as a sensor for its nearby teammates. We formalize CL-MRMP as a chance-constrained motion planning problem, and propose a safety-guaranteed algorithm that explicitly accounts for robot-robot correlations. Our approach extends a sampling-based planner to solve CL-MRMP while preserving probabilistic completeness. To improve efficiency, we introduce novel biasing techniques. We evaluate our method across diverse benchmarks, demonstrating its effectiveness in generating motion plans, with significant performance gains from biasing strategies."
  },
  {
    "title": "Technological schemes and control methods in the reconstruction of parallel gas pipeline systems under non-stationary conditions",
    "url": "http://arxiv.org/abs/2504.06420v1",
    "arxiv_id": "2504.06420v1",
    "authors": [
      "Ilgar Aliyev"
    ],
    "published": "2025-04-08T20:40:02+00:00",
    "summary": "The study explores technological schemes and control methods for reconstructing parallel gas pipeline systems under non-stationary conditions. It focuses on improving safety, reliability, and efficiency by automating valve control and integrating IoT-based monitoring. Automated shut-off valves are designed to detect sudden pressure drops and block leaks instantly. These valves utilize pressure drop rates and valve position sensors to detect and isolate leaks. Wireless pressure sensors and real-time monitoring systems enable remote control of gas flow. Mathematical modeling is employed to analyze pressure variations along damaged sections in order to determine optimal valve placement and activation timing. Machine learning algorithms in the control center are used to predict and verify leak locations based on sensor data. To ensure uninterrupted gas supply in the event of an accident, the study develops an empirical formula for determining the location of connecting pipes activated between parallel pipelines, based on real-time pressure data. The aim of this research is to reduce gas leaks and environmental hazards, ensure continuous gas supply during emergencies, enhance decision-making through automated systems, minimize gas losses, and reduce maintenance costs."
  },
  {
    "title": "SPoRt -- Safe Policy Ratio: Certified Training and Deployment of Task Policies in Model-Free RL",
    "url": "http://arxiv.org/abs/2504.06386v1",
    "arxiv_id": "2504.06386v1",
    "authors": [
      "Jacques Cloete",
      "Nikolaus Vertovec",
      "Alessandro Abate"
    ],
    "published": "2025-04-08T19:09:07+00:00",
    "summary": "To apply reinforcement learning to safety-critical applications, we ought to provide safety guarantees during both policy training and deployment. In this work we present novel theoretical results that provide a bound on the probability of violating a safety property for a new task-specific policy in a model-free, episodic setup: the bound, based on a `maximum policy ratio' that is computed with respect to a `safe' base policy, can also be more generally applied to temporally-extended properties (beyond safety) and to robust control problems. We thus present SPoRt, which also provides a data-driven approach for obtaining such a bound for the base policy, based on scenario theory, and which includes Projected PPO, a new projection-based approach for training the task-specific policy while maintaining a user-specified bound on property violation. Hence, SPoRt enables the user to trade off safety guarantees in exchange for task-specific performance. Accordingly, we present experimental results demonstrating this trade-off, as well as a comparison of the theoretical bound to posterior bounds based on empirical violation rates."
  },
  {
    "title": "Addressing Relative Degree Issues in Control Barrier Function Synthesis with Physics-Informed Neural Networks",
    "url": "http://arxiv.org/abs/2504.06242v1",
    "arxiv_id": "2504.06242v1",
    "authors": [
      "Lukas Brunke",
      "Siqi Zhou",
      "Francesco D'Orazio",
      "Angela P. Schoellig"
    ],
    "published": "2025-04-08T17:41:43+00:00",
    "summary": "In robotics, control barrier function (CBF)-based safety filters are commonly used to enforce state constraints. A critical challenge arises when the relative degree of the CBF varies across the state space. This variability can create regions within the safe set where the control input becomes unconstrained. When implemented as a safety filter, this may result in chattering near the safety boundary and ultimately compromise system safety. To address this issue, we propose a novel approach for CBF synthesis by formulating it as solving a set of boundary value problems. The solutions to the boundary value problems are determined using physics-informed neural networks (PINNs). Our approach ensures that the synthesized CBFs maintain a constant relative degree across the set of admissible states, thereby preventing unconstrained control scenarios. We illustrate the approach in simulation and further verify it through real-world quadrotor experiments, demonstrating its effectiveness in preserving desired system safety properties."
  },
  {
    "title": "TxGemma: Efficient and Agentic LLMs for Therapeutics",
    "url": "http://arxiv.org/abs/2504.06196v1",
    "arxiv_id": "2504.06196v1",
    "authors": [
      "Eric Wang",
      "Samuel Schmidgall",
      "Paul F. Jaeger",
      "Fan Zhang",
      "Rory Pilgrim",
      "Yossi Matias",
      "Joelle Barral",
      "David Fleet",
      "Shekoofeh Azizi"
    ],
    "published": "2025-04-08T16:39:02+00:00",
    "summary": "Therapeutic development is a costly and high-risk endeavor that is often plagued by high failure rates. To address this, we introduce TxGemma, a suite of efficient, generalist large language models (LLMs) capable of therapeutic property prediction as well as interactive reasoning and explainability. Unlike task-specific models, TxGemma synthesizes information from diverse sources, enabling broad application across the therapeutic development pipeline. The suite includes 2B, 9B, and 27B parameter models, fine-tuned from Gemma-2 on a comprehensive dataset of small molecules, proteins, nucleic acids, diseases, and cell lines. Across 66 therapeutic development tasks, TxGemma achieved superior or comparable performance to the state-of-the-art generalist model on 64 (superior on 45), and against state-of-the-art specialist models on 50 (superior on 26). Fine-tuning TxGemma models on therapeutic downstream tasks, such as clinical trial adverse event prediction, requires less training data than fine-tuning base LLMs, making TxGemma suitable for data-limited applications. Beyond these predictive capabilities, TxGemma features conversational models that bridge the gap between general LLMs and specialized property predictors. These allow scientists to interact in natural language, provide mechanistic reasoning for predictions based on molecular structure, and engage in scientific discussions. Building on this, we further introduce Agentic-Tx, a generalist therapeutic agentic system powered by Gemini 2.5 that reasons, acts, manages diverse workflows, and acquires external domain knowledge. Agentic-Tx surpasses prior leading models on the Humanity's Last Exam benchmark (Chemistry & Biology) with 52.3% relative improvement over o3-mini (high) and 26.7% over o3-mini (high) on GPQA (Chemistry) and excels with improvements of 6.3% (ChemBench-Preference) and 2.4% (ChemBench-Mini) over o3-mini (high)."
  },
  {
    "title": "A Self-Supervised Framework for Space Object Behaviour Characterisation",
    "url": "http://arxiv.org/abs/2504.06176v1",
    "arxiv_id": "2504.06176v1",
    "authors": [
      "Ian Groves",
      "Andrew Campbell",
      "James Fernandes",
      "Diego Rodriguez",
      "Paul Murray",
      "Massimiliano Vasile",
      "Victoria Nockles"
    ],
    "published": "2025-04-08T16:19:19+00:00",
    "summary": "Foundation Models, pre-trained on large unlabelled datasets before task-specific fine-tuning, are increasingly being applied to specialised domains. Recent examples include ClimaX for climate and Clay for satellite Earth observation, but a Foundation Model for Space Object Behavioural Analysis has not yet been developed. As orbital populations grow, automated methods for characterising space object behaviour are crucial for space safety. We present a Space Safety and Sustainability Foundation Model focusing on space object behavioural analysis using light curves (LCs). We implemented a Perceiver-Variational Autoencoder (VAE) architecture, pre-trained with self-supervised reconstruction and masked reconstruction on 227,000 LCs from the MMT-9 observatory. The VAE enables anomaly detection, motion prediction, and LC generation. We fine-tuned the model for anomaly detection & motion prediction using two independent LC simulators (CASSANDRA and GRIAL respectively), using CAD models of boxwing, Sentinel-3, SMOS, and Starlink platforms. Our pre-trained model achieved a reconstruction error of 0.01%, identifying potentially anomalous light curves through reconstruction difficulty. After fine-tuning, the model scored 88% and 82% accuracy, with 0.90 and 0.95 ROC AUC scores respectively in both anomaly detection and motion mode prediction (sun-pointing, spin, etc.). Analysis of high-confidence anomaly predictions on real data revealed distinct patterns including characteristic object profiles and satellite glinting. Here, we demonstrate how self-supervised learning can simultaneously enable anomaly detection, motion prediction, and synthetic data generation from rich representations learned in pre-training. Our work therefore supports space safety and sustainability through automated monitoring and simulation capabilities."
  },
  {
    "title": "A Self-Supervised Framework for Space Object Behaviour Characterisation",
    "url": "http://arxiv.org/abs/2504.06176v2",
    "arxiv_id": "2504.06176v2",
    "authors": [
      "Ian Groves",
      "Andrew Campbell",
      "James Fernandes",
      "Diego Ram\u00edrez Rodr\u00edguez",
      "Paul Murray",
      "Massimiliano Vasile",
      "Victoria Nockles"
    ],
    "published": "2025-04-08T16:19:19+00:00",
    "summary": "Foundation Models, pre-trained on large unlabelled datasets before task-specific fine-tuning, are increasingly being applied to specialised domains. Recent examples include ClimaX for climate and Clay for satellite Earth observation, but a Foundation Model for Space Object Behavioural Analysis has not yet been developed. As orbital populations grow, automated methods for characterising space object behaviour are crucial for space safety. We present a Space Safety and Sustainability Foundation Model focusing on space object behavioural analysis using light curves (LCs). We implemented a Perceiver-Variational Autoencoder (VAE) architecture, pre-trained with self-supervised reconstruction and masked reconstruction on 227,000 LCs from the MMT-9 observatory. The VAE enables anomaly detection, motion prediction, and LC generation. We fine-tuned the model for anomaly detection & motion prediction using two independent LC simulators (CASSANDRA and GRIAL respectively), using CAD models of boxwing, Sentinel-3, SMOS, and Starlink platforms. Our pre-trained model achieved a reconstruction error of 0.01%, identifying potentially anomalous light curves through reconstruction difficulty. After fine-tuning, the model scored 88% and 82% accuracy, with 0.90 and 0.95 ROC AUC scores respectively in both anomaly detection and motion mode prediction (sun-pointing, spin, etc.). Analysis of high-confidence anomaly predictions on real data revealed distinct patterns including characteristic object profiles and satellite glinting. Here, we demonstrate how self-supervised learning can simultaneously enable anomaly detection, motion prediction, and synthetic data generation from rich representations learned in pre-training. Our work therefore supports space safety and sustainability through automated monitoring and simulation capabilities."
  },
  {
    "title": "Safe Interaction via Monte Carlo Linear-Quadratic Games",
    "url": "http://arxiv.org/abs/2504.06124v1",
    "arxiv_id": "2504.06124v1",
    "authors": [
      "Benjamin A. Christie",
      "Dylan P. Losey"
    ],
    "published": "2025-04-08T15:18:38+00:00",
    "summary": "Safety is critical during human-robot interaction. But -- because people are inherently unpredictable -- it is often difficult for robots to plan safe behaviors. Instead of relying on our ability to anticipate humans, here we identify robot policies that are robust to unexpected human decisions. We achieve this by formulating human-robot interaction as a zero-sum game, where (in the worst case) the human's actions directly conflict with the robot's objective. Solving for the Nash Equilibrium of this game provides robot policies that maximize safety and performance across a wide range of human actions. Existing approaches attempt to find these optimal policies by leveraging Hamilton-Jacobi analysis (which is intractable) or linear-quadratic approximations (which are inexact). By contrast, in this work we propose a computationally efficient and theoretically justified method that converges towards the Nash Equilibrium policy. Our approach (which we call MCLQ) leverages linear-quadratic games to obtain an initial guess at safe robot behavior, and then iteratively refines that guess with a Monte Carlo search. Not only does MCLQ provide real-time safety adjustments, but it also enables the designer to tune how conservative the robot is -- preventing the system from focusing on unrealistic human behaviors. Our simulations and user study suggest that this approach advances safety in terms of both computation time and expected performance. See videos of our experiments here: https://youtu.be/KJuHeiWVuWY."
  },
  {
    "title": "Leanabell-Prover: Posttraining Scaling in Formal Reasoning",
    "url": "http://arxiv.org/abs/2504.06122v2",
    "arxiv_id": "2504.06122v2",
    "authors": [
      "Jingyuan Zhang",
      "Qi Wang",
      "Xingguang Ji",
      "Yahui Liu",
      "Yang Yue",
      "Fuzheng Zhang",
      "Di Zhang",
      "Guorui Zhou",
      "Kun Gai"
    ],
    "published": "2025-04-08T15:15:26+00:00",
    "summary": "Recent advances in automated theorem proving (ATP) through LLMs have highlighted the potential of formal reasoning with Lean 4 codes. However, ATP has not yet be revolutionized by the recent posttraining scaling as demonstrated by Open AI O1/O3 and Deepseek R1. In this work, we investigate the entire posttraining of ATP, aiming to align it with breakthroughs in reasoning models in natural languages. To begin, we continual train current ATP models with a hybrid dataset, which consists of numerous statement-proof pairs, and additional data aimed at incorporating cognitive behaviors that emulate human reasoning and hypothesis refinement. Next, we explore reinforcement learning with the use of outcome reward returned by Lean 4 compiler. Through our designed continual training and reinforcement learning processes, we have successfully improved existing formal provers, including both DeepSeek-Prover-v1.5 and Goedel-Prover, achieving state-of-the-art performance in the field of whole-proof generation. For example, we achieve a 59.8% pass rate (pass@32) on MiniF2F. This is an on-going project and we will progressively update our findings, release our data and training details."
  },
  {
    "title": "Uncertainty-Aware Hybrid Machine Learning in Virtual Sensors for Vehicle Sideslip Angle Estimation",
    "url": "http://arxiv.org/abs/2504.06105v1",
    "arxiv_id": "2504.06105v1",
    "authors": [
      "Abinav Kalyanasundaram",
      "Karthikeyan Chandra Sekaran",
      "Philipp Stauber",
      "Michael Lange",
      "Wolfgang Utschick",
      "Michael Botsch"
    ],
    "published": "2025-04-08T14:49:58+00:00",
    "summary": "Precise vehicle state estimation is crucial for safe and reliable autonomous driving. The number of measurable states and their precision offered by the onboard vehicle sensor system are often constrained by cost. For instance, measuring critical quantities such as the Vehicle Sideslip Angle (VSA) poses significant commercial challenges using current optical sensors. This paper addresses these limitations by focusing on the development of high-performance virtual sensors to enhance vehicle state estimation for active safety. The proposed Uncertainty-Aware Hybrid Learning (UAHL) architecture integrates a machine learning model with vehicle motion models to estimate VSA directly from onboard sensor data. A key aspect of the UAHL architecture is its focus on uncertainty quantification for individual model estimates and hybrid fusion. These mechanisms enable the dynamic weighting of uncertainty-aware predictions from machine learning and vehicle motion models to produce accurate and reliable hybrid VSA estimates. This work also presents a novel dataset named Real-world Vehicle State Estimation Dataset (ReV-StED), comprising synchronized measurements from advanced vehicle dynamic sensors. The experimental results demonstrate the superior performance of the proposed method for VSA estimation, highlighting UAHL as a promising architecture for advancing virtual sensors and enhancing active safety in autonomous vehicles."
  }
]