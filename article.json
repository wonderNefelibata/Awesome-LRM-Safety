[
  {
    "title": "Integrating Learning-Based Manipulation and Physics-Based Locomotion for Whole-Body Badminton Robot Control",
    "url": "http://arxiv.org/abs/2504.17771v1",
    "arxiv_id": "2504.17771v1",
    "authors": [
      "Haochen Wang",
      "Zhiwei Shi",
      "Chengxi Zhu",
      "Yafei Qiao",
      "Cheng Zhang",
      "Fan Yang",
      "Pengjie Ren",
      "Lan Lu",
      "Dong Xuan"
    ],
    "published": "2025-04-24T17:46:29+00:00",
    "summary": "Learning-based methods, such as imitation learning (IL) and reinforcement learning (RL), can produce excel control policies over challenging agile robot tasks, such as sports robot. However, no existing work has harmonized learning-based policy with model-based methods to reduce training complexity and ensure the safety and stability for agile badminton robot control. In this paper, we introduce \\ourmethod, a novel hybrid control system for agile badminton robots. Specifically, we propose a model-based strategy for chassis locomotion which provides a base for arm policy. We introduce a physics-informed ``IL+RL'' training framework for learning-based arm policy. In this train framework, a model-based strategy with privileged information is used to guide arm policy training during both IL and RL phases. In addition, we train the critic model during IL phase to alleviate the performance drop issue when transitioning from IL to RL. We present results on our self-engineered badminton robot, achieving 94.5% success rate against the serving machine and 90.7% success rate against human players. Our system can be easily generalized to other agile mobile manipulation tasks such as agile catching and table tennis. Our project website: https://dreamstarring.github.io/HAMLET/."
  },
  {
    "title": "Conformal Segmentation in Industrial Surface Defect Detection with Statistical Guarantees",
    "url": "http://arxiv.org/abs/2504.17721v1",
    "arxiv_id": "2504.17721v1",
    "authors": [
      "Cheng Shen",
      "Yuewei Liu"
    ],
    "published": "2025-04-24T16:33:56+00:00",
    "summary": "In industrial settings, surface defects on steel can significantly compromise its service life and elevate potential safety risks. Traditional defect detection methods predominantly rely on manual inspection, which suffers from low efficiency and high costs. Although automated defect detection approaches based on Convolutional Neural Networks(e.g., Mask R-CNN) have advanced rapidly, their reliability remains challenged due to data annotation uncertainties during deep model training and overfitting issues. These limitations may lead to detection deviations when processing the given new test samples, rendering automated detection processes unreliable. To address this challenge, we first evaluate the detection model's practical performance through calibration data that satisfies the independent and identically distributed (i.i.d) condition with test data. Specifically, we define a loss function for each calibration sample to quantify detection error rates, such as the complement of recall rate and false discovery rate. Subsequently, we derive a statistically rigorous threshold based on a user-defined risk level to identify high-probability defective pixels in test images, thereby constructing prediction sets (e.g., defect regions). This methodology ensures that the expected error rate (mean error rate) on the test set remains strictly bounced by the predefined risk level. Additionally, we observe a negative correlation between the average prediction set size and the risk level on the test set, establishing a statistically rigorous metric for assessing detection model uncertainty. Furthermore, our study demonstrates robust and efficient control over the expected test set error rate across varying calibration-to-test partitioning ratios, validating the method's adaptability and operational effectiveness."
  },
  {
    "title": "Safety in Large Reasoning Models: A Survey",
    "url": "http://arxiv.org/abs/2504.17704v1",
    "arxiv_id": "2504.17704v1",
    "authors": [
      "Cheng Wang",
      "Yue Liu",
      "Baolong Li",
      "Duzhen Zhang",
      "Zhongzhi Li",
      "Junfeng Fang"
    ],
    "published": "2025-04-24T16:11:01+00:00",
    "summary": "Large Reasoning Models (LRMs) have exhibited extraordinary prowess in tasks like mathematics and coding, leveraging their advanced reasoning capabilities. Nevertheless, as these capabilities progress, significant concerns regarding their vulnerabilities and safety have arisen, which can pose challenges to their deployment and application in real-world settings. This paper presents a comprehensive survey of LRMs, meticulously exploring and summarizing the newly emerged safety risks, attacks, and defense strategies. By organizing these elements into a detailed taxonomy, this work aims to offer a clear and structured understanding of the current safety landscape of LRMs, facilitating future research and development to enhance the security and reliability of these powerful models."
  },
  {
    "title": "Using mathematical models of heart cells to assess the safety of new pharmaceutical drugs",
    "url": "http://arxiv.org/abs/2504.17694v1",
    "arxiv_id": "2504.17694v1",
    "authors": [
      "Gary R. Mirams"
    ],
    "published": "2025-04-24T16:03:06+00:00",
    "summary": "Many drugs have been withdrawn from the market worldwide, at a cost of billions of dollars, because of patient fatalities due to them unexpectedly disturbing heart rhythm. Even drugs for ailments as mild as hay fever have been withdrawn due to an unacceptable increase in risk of these heart rhythm disturbances. Consequently, the whole pharmaceutical industry expends a huge effort in checking all new drugs for any unwanted side effects on the heart. The predominant root cause has been identified as drug molecules blocking ionic current flows in the heart. Block of individual types of ionic currents can now be measured experimentally at an early stage of drug development, and this is the standard screening approach for a number of ion currents in many large pharmaceutical companies. However, clinical risk is a complex function of the degree of block of many different types of cardiac ion currents, and this is difficult to understand by looking at results of these screens independently. By using ordinary differential equation models for the electrical activity of heart cells (electrophysiology models) we can integrate information from different types of currents, to predict the effect on whole heart cells and subsequent risk of side effects. The resulting simulations can provide a more accurate summary of the risk of a drug earlier in development and hence more cheaply than the pre-existing approaches."
  },
  {
    "title": "Data-Driven Calibration of Prediction Sets in Large Vision-Language Models Based on Inductive Conformal Prediction",
    "url": "http://arxiv.org/abs/2504.17671v1",
    "arxiv_id": "2504.17671v1",
    "authors": [
      "Yuanchang Ye",
      "Weiyan Wen"
    ],
    "published": "2025-04-24T15:39:46+00:00",
    "summary": "This study addresses the critical challenge of hallucination mitigation in Large Vision-Language Models (LVLMs) for Visual Question Answering (VQA) tasks through a Split Conformal Prediction (SCP) framework. While LVLMs excel in multi-modal reasoning, their outputs often exhibit hallucinated content with high confidence, posing risks in safety-critical applications. We propose a model-agnostic uncertainty quantification method that integrates dynamic threshold calibration and cross-modal consistency verification. By partitioning data into calibration and test sets, the framework computes nonconformity scores to construct prediction sets with statistical guarantees under user-defined risk levels ($\\alpha$). Key innovations include: (1) rigorous control of \\textbf{marginal coverage} to ensure empirical error rates remain strictly below $\\alpha$; (2) dynamic adjustment of prediction set sizes inversely with $\\alpha$, filtering low-confidence outputs; (3) elimination of prior distribution assumptions and retraining requirements. Evaluations on benchmarks (ScienceQA, MMMU) with eight LVLMs demonstrate that SCP enforces theoretical guarantees across all $\\alpha$ values. The framework achieves stable performance across varying calibration-to-test split ratios, underscoring its robustness for real-world deployment in healthcare, autonomous systems, and other safety-sensitive domains. This work bridges the gap between theoretical reliability and practical applicability in multi-modal AI systems, offering a scalable solution for hallucination detection and uncertainty-aware decision-making."
  },
  {
    "title": "Data-Driven Calibration of Prediction Sets in Large Vision-Language Models Based on Inductive Conformal Prediction",
    "url": "http://arxiv.org/abs/2504.17671v2",
    "arxiv_id": "2504.17671v2",
    "authors": [
      "Yuanchang Ye",
      "Weiyan Wen"
    ],
    "published": "2025-04-24T15:39:46+00:00",
    "summary": "This study addresses the critical challenge of hallucination mitigation in Large Vision-Language Models (LVLMs) for Visual Question Answering (VQA) tasks through a Split Conformal Prediction (SCP) framework. While LVLMs excel in multi-modal reasoning, their outputs often exhibit hallucinated content with high confidence, posing risks in safety-critical applications. We propose a model-agnostic uncertainty quantification method that integrates dynamic threshold calibration and cross-modal consistency verification. By partitioning data into calibration and test sets, the framework computes nonconformity scores to construct prediction sets with statistical guarantees under user-defined risk levels ($\\alpha$). Key innovations include: (1) rigorous control of \\textbf{marginal coverage} to ensure empirical error rates remain strictly below $\\alpha$; (2) dynamic adjustment of prediction set sizes inversely with $\\alpha$, filtering low-confidence outputs; (3) elimination of prior distribution assumptions and retraining requirements. Evaluations on benchmarks (ScienceQA, MMMU) with eight LVLMs demonstrate that SCP enforces theoretical guarantees across all $\\alpha$ values. The framework achieves stable performance across varying calibration-to-test split ratios, underscoring its robustness for real-world deployment in healthcare, autonomous systems, and other safety-sensitive domains. This work bridges the gap between theoretical reliability and practical applicability in multi-modal AI systems, offering a scalable solution for hallucination detection and uncertainty-aware decision-making."
  },
  {
    "title": "Unifying Complementarity Constraints and Control Barrier Functions for Safe Whole-Body Robot Control",
    "url": "http://arxiv.org/abs/2504.17647v1",
    "arxiv_id": "2504.17647v1",
    "authors": [
      "Rafael I. Cabral Muchacho",
      "Riddhiman Laha",
      "Florian T. Pokorny",
      "Luis F. C. Figueredo",
      "Nilanjan Chakraborty"
    ],
    "published": "2025-04-24T15:17:26+00:00",
    "summary": "Safety-critical whole-body robot control demands reactive methods that ensure collision avoidance in real-time. Complementarity constraints and control barrier functions (CBF) have emerged as core tools for ensuring such safety constraints, and each represents a well-developed field. Despite addressing similar problems, their connection remains largely unexplored. This paper bridges this gap by formally proving the equivalence between these two methodologies for sampled-data, first-order systems, considering both single and multiple constraint scenarios. By demonstrating this equivalence, we provide a unified perspective on these techniques. This unification has theoretical and practical implications, facilitating the cross-application of robustness guarantees and algorithmic improvements between complementarity and CBF frameworks. We discuss these synergistic benefits and motivate future work in the comparison of the methods in more general cases."
  },
  {
    "title": "Portability of Optimizations from SC to TSO",
    "url": "http://arxiv.org/abs/2504.17646v1",
    "arxiv_id": "2504.17646v1",
    "authors": [
      "Akshay Gopalakrishnan",
      "Clark Verbrugge"
    ],
    "published": "2025-04-24T15:16:17+00:00",
    "summary": "It is well recognized that the safety of compiler optimizations is at risk in a concurrent context. Existing approaches primarily rely on context-free thread-local guarantees, and prohibit optimizations that introduce a data-race. However, compilers utilize global context-specific information, exposing safe optimizations that may violate such guarantees as well as introduce a race. Such optimizations need to individually be proven safe for each language model. An alternate approach to this would be proving them safe for an intuitive model (like interleaving semantics), and then determine their portability across other concurrent models. In this paper, we address this problem of porting across models of concurrency. We first identify a global guarantee on optimizations portable from Sequential Consistency (SC) to Total Store Order (TSO). Our guarantee is in the form of constraints specifying the syntactic changes an optimization must not incur. We then show these constraints correlate to prohibiting the introduction of triangular races, a subset of data-race relevant to TSO. We conclude by showing how such race inducing optimizations relate to porting across Strong Release Acquire (SRA), a known causally consistent memory model."
  },
  {
    "title": "Safe to Stay: Psychological Safety Sustains Participation in Pull-based Open Source Projects",
    "url": "http://arxiv.org/abs/2504.17510v1",
    "arxiv_id": "2504.17510v1",
    "authors": [
      "Emeralda Sesari",
      "Federica Sarro",
      "Ayushi Rastogi"
    ],
    "published": "2025-04-24T12:54:30+00:00",
    "summary": "Psychological safety is the belief that team members can speak up or make mistakes without fear of negative consequences. While it is recognized as important in traditional software teams, its role in open-source development remains understudied. Yet, open-source contributors often collaborate without formal roles or structures, where interpersonal relationship can make or break participation. In this study, we examine whether team-level psychological safety, inferred from code review activities, is associated with contributors' continued participation in open-source projects. Code review is a central and collaborative activity in modern software development, which offers a rich context for observing team interactions. Based on 60,684 pull requests, we construct a psychological safety index using cues such as merge decisions, comment activity, interaction diversity, and mentions. We analyze its relationship with contributors' short-term (after 1 year) and long-term (after 4-5 years) sustained participation using three logistic regression models. Our findings show that contributors are more likely to remain active in repositories with higher levels of psychological safety. Psychological safety is positively associated with both short-term and future sustained participation. However, when prior participation is included, it becomes the stronger predictor of future sustained participation, while the effect of psychological safety becomes smaller. This study introduces a scalable approach to study psychological safety through pull request data and provides new evidence that it matters in open-source development."
  },
  {
    "title": "Assessing the Capability of Large Language Models for Domain-Specific Ontology Generation",
    "url": "http://arxiv.org/abs/2504.17402v1",
    "arxiv_id": "2504.17402v1",
    "authors": [
      "Anna Sofia Lippolis",
      "Mohammad Javad Saeedizade",
      "Robin Keskisarkka",
      "Aldo Gangemi",
      "Eva Blomqvist",
      "Andrea Giovanni Nuzzolese"
    ],
    "published": "2025-04-24T09:47:14+00:00",
    "summary": "Large Language Models (LLMs) have shown significant potential for ontology engineering. However, it is still unclear to what extent they are applicable to the task of domain-specific ontology generation. In this study, we explore the application of LLMs for automated ontology generation and evaluate their performance across different domains. Specifically, we investigate the generalizability of two state-of-the-art LLMs, DeepSeek and o1-preview, both equipped with reasoning capabilities, by generating ontologies from a set of competency questions (CQs) and related user stories. Our experimental setup comprises six distinct domains carried out in existing ontology engineering projects and a total of 95 curated CQs designed to test the models' reasoning for ontology engineering. Our findings show that with both LLMs, the performance of the experiments is remarkably consistent across all domains, indicating that these methods are capable of generalizing ontology generation tasks irrespective of the domain. These results highlight the potential of LLM-based approaches in achieving scalable and domain-agnostic ontology construction and lay the groundwork for further research into enhancing automated reasoning and knowledge representation techniques."
  },
  {
    "title": "Highly Accurate and Diverse Traffic Data: The DeepScenario Open 3D Dataset",
    "url": "http://arxiv.org/abs/2504.17371v1",
    "arxiv_id": "2504.17371v1",
    "authors": [
      "Oussema Dhaouadi",
      "Johannes Meier",
      "Luca Wahl",
      "Jacques Kaiser",
      "Luca Scalerandi",
      "Nick Wandelburg",
      "Zhuolun Zhou",
      "Nijanthan Berinpanathan",
      "Holger Banzhaf",
      "Daniel Cremers"
    ],
    "published": "2025-04-24T08:43:48+00:00",
    "summary": "Accurate 3D trajectory data is crucial for advancing autonomous driving. Yet, traditional datasets are usually captured by fixed sensors mounted on a car and are susceptible to occlusion. Additionally, such an approach can precisely reconstruct the dynamic environment in the close vicinity of the measurement vehicle only, while neglecting objects that are further away. In this paper, we introduce the DeepScenario Open 3D Dataset (DSC3D), a high-quality, occlusion-free dataset of 6 degrees of freedom bounding box trajectories acquired through a novel monocular camera drone tracking pipeline. Our dataset includes more than 175,000 trajectories of 14 types of traffic participants and significantly exceeds existing datasets in terms of diversity and scale, containing many unprecedented scenarios such as complex vehicle-pedestrian interaction on highly populated urban streets and comprehensive parking maneuvers from entry to exit. DSC3D dataset was captured in five various locations in Europe and the United States and include: a parking lot, a crowded inner-city, a steep urban intersection, a federal highway, and a suburban intersection. Our 3D trajectory dataset aims to enhance autonomous driving systems by providing detailed environmental 3D representations, which could lead to improved obstacle interactions and safety. We demonstrate its utility across multiple applications including motion prediction, motion planning, scenario mining, and generative reactive traffic agents. Our interactive online visualization platform and the complete dataset are publicly available at app.deepscenario.com, facilitating research in motion prediction, behavior modeling, and safety validation."
  },
  {
    "title": "Highly Accurate and Diverse Traffic Data: The DeepScenario Open 3D Dataset",
    "url": "http://arxiv.org/abs/2504.17371v2",
    "arxiv_id": "2504.17371v2",
    "authors": [
      "Oussema Dhaouadi",
      "Johannes Meier",
      "Luca Wahl",
      "Jacques Kaiser",
      "Luca Scalerandi",
      "Nick Wandelburg",
      "Zhuolun Zhou",
      "Nijanthan Berinpanathan",
      "Holger Banzhaf",
      "Daniel Cremers"
    ],
    "published": "2025-04-24T08:43:48+00:00",
    "summary": "Accurate 3D trajectory data is crucial for advancing autonomous driving. Yet, traditional datasets are usually captured by fixed sensors mounted on a car and are susceptible to occlusion. Additionally, such an approach can precisely reconstruct the dynamic environment in the close vicinity of the measurement vehicle only, while neglecting objects that are further away. In this paper, we introduce the DeepScenario Open 3D Dataset (DSC3D), a high-quality, occlusion-free dataset of 6 degrees of freedom bounding box trajectories acquired through a novel monocular camera drone tracking pipeline. Our dataset includes more than 175,000 trajectories of 14 types of traffic participants and significantly exceeds existing datasets in terms of diversity and scale, containing many unprecedented scenarios such as complex vehicle-pedestrian interaction on highly populated urban streets and comprehensive parking maneuvers from entry to exit. DSC3D dataset was captured in five various locations in Europe and the United States and include: a parking lot, a crowded inner-city, a steep urban intersection, a federal highway, and a suburban intersection. Our 3D trajectory dataset aims to enhance autonomous driving systems by providing detailed environmental 3D representations, which could lead to improved obstacle interactions and safety. We demonstrate its utility across multiple applications including motion prediction, motion planning, scenario mining, and generative reactive traffic agents. Our interactive online visualization platform and the complete dataset are publicly available at https://app.deepscenario.com, facilitating research in motion prediction, behavior modeling, and safety validation."
  },
  {
    "title": "AUTHENTICATION: Identifying Rare Failure Modes in Autonomous Vehicle Perception Systems using Adversarially Guided Diffusion Models",
    "url": "http://arxiv.org/abs/2504.17179v1",
    "arxiv_id": "2504.17179v1",
    "authors": [
      "Mohammad Zarei",
      "Melanie A Jutras",
      "Eliana Evans",
      "Mike Tan",
      "Omid Aaramoon"
    ],
    "published": "2025-04-24T01:31:13+00:00",
    "summary": "Autonomous Vehicles (AVs) rely on artificial intelligence (AI) to accurately detect objects and interpret their surroundings. However, even when trained using millions of miles of real-world data, AVs are often unable to detect rare failure modes (RFMs). The problem of RFMs is commonly referred to as the \"long-tail challenge\", due to the distribution of data including many instances that are very rarely seen. In this paper, we present a novel approach that utilizes advanced generative and explainable AI techniques to aid in understanding RFMs. Our methods can be used to enhance the robustness and reliability of AVs when combined with both downstream model training and testing. We extract segmentation masks for objects of interest (e.g., cars) and invert them to create environmental masks. These masks, combined with carefully crafted text prompts, are fed into a custom diffusion model. We leverage the Stable Diffusion inpainting model guided by adversarial noise optimization to generate images containing diverse environments designed to evade object detection models and expose vulnerabilities in AI systems. Finally, we produce natural language descriptions of the generated RFMs that can guide developers and policymakers to improve the safety and reliability of AV systems."
  },
  {
    "title": "Opt-ODENet: A Neural ODE Framework with Differentiable QP Layers for Safe and Stable Control Design (longer version)",
    "url": "http://arxiv.org/abs/2504.17139v1",
    "arxiv_id": "2504.17139v1",
    "authors": [
      "Keyan Miao",
      "Liqun Zhao",
      "Han Wang",
      "Konstantinos Gatsis",
      "Antonis Papachristodoulou"
    ],
    "published": "2025-04-23T23:09:37+00:00",
    "summary": "Designing controllers that achieve task objectives while ensuring safety is a key challenge in control systems. This work introduces Opt-ODENet, a Neural ODE framework with a differentiable Quadratic Programming (QP) optimization layer to enforce constraints as hard requirements. Eliminating the reliance on nominal controllers or large datasets, our framework solves the optimal control problem directly using Neural ODEs. Stability and convergence are ensured through Control Lyapunov Functions (CLFs) in the loss function, while Control Barrier Functions (CBFs) embedded in the QP layer enforce real-time safety. By integrating the differentiable QP layer with Neural ODEs, we demonstrate compatibility with the adjoint method for gradient computation, enabling the learning of the CBF class-$\\mathcal{K}$ function and control network parameters. Experiments validate its effectiveness in balancing safety and performance."
  },
  {
    "title": "Steering the CensorShip: Uncovering Representation Vectors for LLM \"Thought\" Control",
    "url": "http://arxiv.org/abs/2504.17130v1",
    "arxiv_id": "2504.17130v1",
    "authors": [
      "Hannah Cyberey",
      "David Evans"
    ],
    "published": "2025-04-23T22:47:30+00:00",
    "summary": "Large language models (LLMs) have transformed the way we access information. These models are often tuned to refuse to comply with requests that are considered harmful and to produce responses that better align with the preferences of those who control the models. To understand how this \"censorship\" works. We use representation engineering techniques to study open-weights safety-tuned models. We present a method for finding a refusal--compliance vector that detects and controls the level of censorship in model outputs. We also analyze recent reasoning LLMs, distilled from DeepSeek-R1, and uncover an additional dimension of censorship through \"thought suppression\". We show a similar approach can be used to find a vector that suppresses the model's reasoning process, allowing us to remove censorship by applying the negative multiples of this vector"
  },
  {
    "title": "Peer-Aware Cost Estimation in Nonlinear General-Sum Dynamic Games for Mutual Learning and Intent Inference",
    "url": "http://arxiv.org/abs/2504.17129v1",
    "arxiv_id": "2504.17129v1",
    "authors": [
      "Seyed Yousef Soltanian",
      "Wenlong Zhang"
    ],
    "published": "2025-04-23T22:47:20+00:00",
    "summary": "Human-robot interactions can be modeled as incomplete-information general-sum dynamic games since the objective functions of both agents are not explicitly known to each other. However, solving for equilibrium policies for such games presents a major challenge, especially if the games involve nonlinear underlying dynamics. To simplify the problem, existing work often assumes that one agent is an expert with complete information about its peer, which can lead to biased estimates and failures in coordination. To address this challenge, we propose a nonlinear peer-aware cost estimation (N-PACE) algorithm for general-sum dynamic games. In N-PACE, using iterative linear quadratic (LQ) approximation of the nonlinear general-sum game, each agent explicitly models the learning dynamics of its peer agent while inferring their objective functions, leading to unbiased fast learning in inferring the unknown objective function of the peer agent, which is critical for task completion and safety assurance. Additionally, we demonstrate how N-PACE enables \\textbf{intent communication} in such multi-agent systems by explicitly modeling the peer's learning dynamics."
  },
  {
    "title": "Discovering the Precursors of Traffic Breakdowns Using Spatiotemporal Graph Attribution Networks",
    "url": "http://arxiv.org/abs/2504.17109v1",
    "arxiv_id": "2504.17109v1",
    "authors": [
      "Zhaobin Mo",
      "Xiangyi Liao",
      "Dominik A. Karbowski",
      "Yanbing Wang"
    ],
    "published": "2025-04-23T21:40:23+00:00",
    "summary": "Understanding and predicting the precursors of traffic breakdowns is critical for improving road safety and traffic flow management. This paper presents a novel approach combining spatiotemporal graph neural networks (ST-GNNs) with Shapley values to identify and interpret traffic breakdown precursors. By extending Shapley explanation methods to a spatiotemporal setting, our proposed method bridges the gap between black-box neural network predictions and interpretable causes. We demonstrate the method on the Interstate-24 data, and identify that road topology and abrupt braking are major factors that lead to traffic breakdowns."
  },
  {
    "title": "Safety Pretraining: Toward the Next Generation of Safe AI",
    "url": "http://arxiv.org/abs/2504.16980v1",
    "arxiv_id": "2504.16980v1",
    "authors": [
      "Pratyush Maini",
      "Sachin Goyal",
      "Dylan Sam",
      "Alex Robey",
      "Yash Savani",
      "Yiding Jiang",
      "Andy Zou",
      "Zacharcy C. Lipton",
      "J. Zico Kolter"
    ],
    "published": "2025-04-23T17:58:08+00:00",
    "summary": "As large language models (LLMs) are increasingly deployed in high-stakes settings, the risk of generating harmful or toxic content remains a central challenge. Post-hoc alignment methods are brittle: once unsafe patterns are learned during pretraining, they are hard to remove. We present a data-centric pretraining framework that builds safety into the model from the start. Our contributions include: (i) a safety classifier trained on 10,000 GPT-4 labeled examples, used to filter 600B tokens; (ii) the largest synthetic safety dataset to date (100B tokens) generated via recontextualization of harmful web data; (iii) RefuseWeb and Moral Education datasets that convert harmful prompts into refusal dialogues and web-style educational material; (iv) Harmfulness-Tag annotations injected during pretraining to flag unsafe content and steer away inference from harmful generations; and (v) safety evaluations measuring base model behavior before instruction tuning. Our safety-pretrained models reduce attack success rates from 38.8% to 8.4% with no performance degradation on standard LLM safety benchmarks."
  },
  {
    "title": "Meta-Learning Online Dynamics Model Adaptation in Off-Road Autonomous Driving",
    "url": "http://arxiv.org/abs/2504.16923v1",
    "arxiv_id": "2504.16923v1",
    "authors": [
      "Jacob Levy",
      "Jason Gibson",
      "Bogdan Vlahov",
      "Erica Tevere",
      "Evangelos Theodorou",
      "David Fridovich-Keil",
      "Patrick Spieler"
    ],
    "published": "2025-04-23T17:51:36+00:00",
    "summary": "High-speed off-road autonomous driving presents unique challenges due to complex, evolving terrain characteristics and the difficulty of accurately modeling terrain-vehicle interactions. While dynamics models used in model-based control can be learned from real-world data, they often struggle to generalize to unseen terrain, making real-time adaptation essential. We propose a novel framework that combines a Kalman filter-based online adaptation scheme with meta-learned parameters to address these challenges. Offline meta-learning optimizes the basis functions along which adaptation occurs, as well as the adaptation parameters, while online adaptation dynamically adjusts the onboard dynamics model in real time for model-based control. We validate our approach through extensive experiments, including real-world testing on a full-scale autonomous off-road vehicle, demonstrating that our method outperforms baseline approaches in prediction accuracy, performance, and safety metrics, particularly in safety-critical scenarios. Our results underscore the effectiveness of meta-learned dynamics model adaptation, advancing the development of reliable autonomous systems capable of navigating diverse and unseen environments. Video is available at: https://youtu.be/cCKHHrDRQEA"
  },
  {
    "title": "Learning Verifiable Control Policies Using Relaxed Verification",
    "url": "http://arxiv.org/abs/2504.16879v1",
    "arxiv_id": "2504.16879v1",
    "authors": [
      "Puja Chaudhury",
      "Alexander Estornell",
      "Michael Everett"
    ],
    "published": "2025-04-23T16:54:35+00:00",
    "summary": "To provide safety guarantees for learning-based control systems, recent work has developed formal verification methods to apply after training ends. However, if the trained policy does not meet the specifications, or there is conservatism in the verification algorithm, establishing these guarantees may not be possible. Instead, this work proposes to perform verification throughout training to ultimately aim for policies whose properties can be evaluated throughout runtime with lightweight, relaxed verification algorithms. The approach is to use differentiable reachability analysis and incorporate new components into the loss function. Numerical experiments on a quadrotor model and unicycle model highlight the ability of this approach to lead to learned control policies that satisfy desired reach-avoid and invariance specifications."
  },
  {
    "title": "Hybrid Reinforcement Learning and Model Predictive Control for Adaptive Control of Hydrogen-Diesel Dual-Fuel Combustion",
    "url": "http://arxiv.org/abs/2504.16875v1",
    "arxiv_id": "2504.16875v1",
    "authors": [
      "Julian Bedei",
      "Murray McBain",
      "Charles Robert Koch",
      "Jakob Andert",
      "David Gordon"
    ],
    "published": "2025-04-23T16:51:49+00:00",
    "summary": "Reinforcement Learning (RL) and Machine Learning Integrated Model Predictive Control (ML-MPC) are promising approaches for optimizing hydrogen-diesel dual-fuel engine control, as they can effectively control multiple-input multiple-output systems and nonlinear processes. ML-MPC is advantageous for providing safe and optimal controls, ensuring the engine operates within predefined safety limits. In contrast, RL is distinguished by its adaptability to changing conditions through its learning-based approach. However, the practical implementation of either method alone poses challenges. RL requires high variance in control inputs during early learning phases, which can pose risks to the system by potentially executing unsafe actions, leading to mechanical damage. Conversely, ML-MPC relies on an accurate system model to generate optimal control inputs and has limited adaptability to system drifts, such as injector aging, which naturally occur in engine applications. To address these limitations, this study proposes a hybrid RL and ML-MPC approach that uses an ML-MPC framework while incorporating an RL agent to dynamically adjust the ML-MPC load tracking reference in response to changes in the environment. At the same time, the ML-MPC ensures that actions stay safe throughout the RL agent's exploration. To evaluate the effectiveness of this approach, fuel pressure is deliberately varied to introduce a model-plant mismatch between the ML-MPC and the engine test bench. The result of this mismatch is a root mean square error (RMSE) in indicated mean effective pressure of 0.57 bar when running the ML-MPC. The experimental results demonstrate that RL successfully adapts to changing boundary conditions by altering the tracking reference while ML-MPC ensures safe control inputs. The quantitative improvement in load tracking by implementing RL is an RSME of 0.44 bar."
  },
  {
    "title": "Improving Significant Wave Height Prediction Using Chronos Models",
    "url": "http://arxiv.org/abs/2504.16834v1",
    "arxiv_id": "2504.16834v1",
    "authors": [
      "Yilin Zhai",
      "Hongyuan Shi",
      "Chao Zhan",
      "Qing Wang",
      "Zaijin You",
      "Nan Wang"
    ],
    "published": "2025-04-23T15:56:28+00:00",
    "summary": "Accurate wave height prediction is critical for maritime safety and coastal resilience, yet conventional physics-based models and traditional machine learning methods face challenges in computational efficiency and nonlinear dynamics modeling. This study introduces Chronos, the first implementation of a large language model (LLM)-powered temporal architecture (Chronos) optimized for wave forecasting. Through advanced temporal pattern recognition applied to historical wave data from three strategically chosen marine zones in the Northwest Pacific basin, our framework achieves multimodal improvements: (1) 14.3% reduction in training time with 2.5x faster inference speed compared to PatchTST baselines, achieving 0.575 mean absolute scaled error (MASE) units; (2) superior short-term forecasting (1-24h) across comprehensive metrics; (3) sustained predictive leadership in extended-range forecasts (1-120h); and (4) demonstrated zero-shot capability maintaining median performance (rank 4/12) against specialized operational models. This LLM-enhanced temporal modeling paradigm establishes a new standard in wave prediction, offering both computationally efficient solutions and a transferable framework for complex geophysical systems modeling."
  },
  {
    "title": "Improving Significant Wave Height Prediction Using Chronos Models",
    "url": "http://arxiv.org/abs/2504.16834v2",
    "arxiv_id": "2504.16834v2",
    "authors": [
      "Yilin Zhai",
      "Hongyuan Shi",
      "Chao Zhan",
      "Qing Wang",
      "Zaijin You",
      "Nan Wang"
    ],
    "published": "2025-04-23T15:56:28+00:00",
    "summary": "Accurate wave height prediction is critical for maritime safety and coastal resilience, yet conventional physics-based models and traditional machine learning methods face challenges in computational efficiency and nonlinear dynamics modeling. This study introduces Chronos, the first implementation of a large language model (LLM)-powered temporal architecture (Chronos) optimized for wave forecasting. Through advanced temporal pattern recognition applied to historical wave data from three strategically chosen marine zones in the Northwest Pacific basin, our framework achieves multimodal improvements: (1) 14.3% reduction in training time with 2.5x faster inference speed compared to PatchTST baselines, achieving 0.575 mean absolute scaled error (MASE) units; (2) superior short-term forecasting (1-24h) across comprehensive metrics; (3) sustained predictive leadership in extended-range forecasts (1-120h); and (4) demonstrated zero-shot capability maintaining median performance (rank 4/12) against specialized operational models. This LLM-enhanced temporal modeling paradigm establishes a new standard in wave prediction, offering both computationally efficient solutions and a transferable framework for complex geophysical systems modeling."
  },
  {
    "title": "Association-Based Track-Before-Detect with Object Contribution Probabilities",
    "url": "http://arxiv.org/abs/2504.16814v1",
    "arxiv_id": "2504.16814v1",
    "authors": [
      "Thomas Kropfreiter",
      "Jason L. Williams",
      "Florian Meyer"
    ],
    "published": "2025-04-23T15:32:11+00:00",
    "summary": "Multiobject tracking provides situational awareness that enables new applications for modern convenience, applied ocean sciences, public safety, and homeland security. In many multiobject tracking applications, including radar and sonar tracking, after coherent prefiltering of the received signal, measurement data is typically structured in cells, where each cell represent, e.g., a different range and bearing value. While conventional detect-then-track (DTT) multiobject tracking approaches convert the cell-structured data within a detection phase into so-called point measurements in order to reduce the amount of data, track-before-detect (TBD) methods process the cell-structured data directly, avoiding a potential information loss. However, many TBD tracking methods are computationally intensive and achieve a reduced tracking accuracy when objects interact, i.e., when they come into close proximity. We here counteract these difficulties by introducing the concept of probabilistic object-to-cell contributions. As many conventional DTT methods, our approach uses a probabilistic association of objects with data cells, and a new object contribution model with corresponding object contribution probabilities to further associate cell contributions to objects that occupy the same data cell. Furthermore, to keep the computational complexity and filter runtimes low, we here use an efficient Poisson multi-Bernoulli filtering approach in combination with the application of belief propagation for fast probabilistic data association. We demonstrate numerically that our method achieves significantly increased tracking performance compared to state-of-the-art TBD tracking approaches, where performance differences are particularly pronounced when multiple objects interact."
  },
  {
    "title": "Evaluating the Impact of CT-to-RED Calibration Curves on Dosimetric Accuracy in Brain Radiotherapy Dose Distribution",
    "url": "http://arxiv.org/abs/2504.16805v1",
    "arxiv_id": "2504.16805v1",
    "authors": [
      "Islam G. Ali",
      "Wael M. Daabis",
      "Hossam Donya"
    ],
    "published": "2025-04-23T15:24:57+00:00",
    "summary": "Accurate dose calculation is crucial in radiotherapy, as tissue relative electron densities (RED) derived from CT scans play a vital role. This study investigated the impact of different CT-to-RED calibration curves on brain cancer treatment plans. Three calibration curves were compared: CIRS phantom-derived, Catphan phantom-derived, and the default curve in the Monaco Treatment Planning System. Ten volumetric modulated arc therapy (VMAT) plans were generated and recalculated using each curve. Dosimetric parameters for Planning Target Volume (PTV) and Organs at Risk (OARs) were analyzed. Results showed significant differences in PTV dose distribution between the CIRS-derived and default curves, while no significant differences were found between Catphan-derived and default curves. The CIRS-derived curve demonstrated superior performance in representing brain tissue electron densities. These findings emphasize the importance of using site-specific CT-to-RED calibration curves for accurate dose calculations in brain radiotherapy, potentially improving treatment safety and efficacy"
  },
  {
    "title": "Evaluating the Impact of CT-to-RED Calibration Curves on Dosimetric Accuracy in Brain Radiotherapy Dose Distribution",
    "url": "http://arxiv.org/abs/2504.16805v2",
    "arxiv_id": "2504.16805v2",
    "authors": [
      "Hossam Donya",
      "Duong Thanh Tai",
      "Islam G. Ali"
    ],
    "published": "2025-04-23T15:24:57+00:00",
    "summary": "Accurate dose calculation is crucial in radiotherapy, as tissue relative electron densities (RED) derived from CT scans play a vital role. This study investigated the impact of different CT-to-RED calibration curves on brain cancer treatment plans. Three calibration curves were compared: CIRS phantom-derived, Catphan phantom-derived, and the default curve in the Monaco Treatment Planning System. Ten volumetric modulated arc therapy (VMAT) plans were generated and recalculated using each curve. Dosimetric parameters for Planning Target Volume (PTV) and Organs at Risk (OARs) were analyzed. Results showed significant differences in PTV dose distribution between the CIRS-derived and default curves, while no significant differences were found between Catphan-derived and default curves. The CIRS-derived curve demonstrated superior performance in representing brain tissue electron densities. These findings emphasize the importance of using site-specific CT-to-RED calibration curves for accurate dose calculations in brain radiotherapy, potentially improving treatment safety and efficacy"
  },
  {
    "title": "Reduction of $\u03b5$-expanded Feynman integrals",
    "url": "http://arxiv.org/abs/2504.16766v1",
    "arxiv_id": "2504.16766v1",
    "authors": [
      "Yan-Qing Ma",
      "Cong-Hao Qin",
      "Ao Tan",
      "Kai Yan"
    ],
    "published": "2025-04-23T14:34:34+00:00",
    "summary": "Since Feynman integrals (FIs) at higher spacetime dimensions are free of infrared and collinear divergence--and their ultraviolet divergences can be systematically subtracted--this allows us to construct a wide range of locally finite Feynman integrals. Especially, we propose a method named $\\bar{R}$-operation to subtract out ultraviolet divergences that at the same time preserves infrared and collinear safety of the original FI. By expressing these locally finite FIs in terms of master integrals and imposing constraints on their $\\epsilon$-expanded forms, we reduce the $\\epsilon$-expanded master integrals to a minimal basis. We provide an automated package to identify such constraints, offering a tool useful for high-order perturbative computations."
  },
  {
    "title": "Frequency-Compensated Network for Daily Arctic Sea Ice Concentration Prediction",
    "url": "http://arxiv.org/abs/2504.16745v1",
    "arxiv_id": "2504.16745v1",
    "authors": [
      "Jialiang Zhang",
      "Feng Gao",
      "Yanhai Gan",
      "Junyu Dong",
      "Qian Du"
    ],
    "published": "2025-04-23T14:15:48+00:00",
    "summary": "Accurately forecasting sea ice concentration (SIC) in the Arctic is critical to global ecosystem health and navigation safety. However, current methods still is confronted with two challenges: 1) these methods rarely explore the long-term feature dependencies in the frequency domain. 2) they can hardly preserve the high-frequency details, and the changes in the marginal area of the sea ice cannot be accurately captured. To this end, we present a Frequency-Compensated Network (FCNet) for Arctic SIC prediction on a daily basis. In particular, we design a dual-branch network, including branches for frequency feature extraction and convolutional feature extraction. For frequency feature extraction, we design an adaptive frequency filter block, which integrates trainable layers with Fourier-based filters. By adding frequency features, the FCNet can achieve refined prediction of edges and details. For convolutional feature extraction, we propose a high-frequency enhancement block to separate high and low-frequency information. Moreover, high-frequency features are enhanced via channel-wise attention, and temporal attention unit is employed for low-frequency feature extraction to capture long-range sea ice changes. Extensive experiments are conducted on a satellite-derived daily SIC dataset, and the results verify the effectiveness of the proposed FCNet. Our codes and data will be made public available at: https://github.com/oucailab/FCNet ."
  },
  {
    "title": "DYNUS: Uncertainty-aware Trajectory Planner in Dynamic Unknown Environments",
    "url": "http://arxiv.org/abs/2504.16734v1",
    "arxiv_id": "2504.16734v1",
    "authors": [
      "Kota Kondo",
      "Mason Peterson",
      "Nicholas Rober",
      "Juan Rached Viso",
      "Lucas Jia",
      "Jialin Chen",
      "Harvey Merton",
      "Jonathan P. How"
    ],
    "published": "2025-04-23T14:05:04+00:00",
    "summary": "This paper introduces DYNUS, an uncertainty-aware trajectory planner designed for dynamic unknown environments. Operating in such settings presents many challenges -- most notably, because the agent cannot predict the ground-truth future paths of obstacles, a previously planned trajectory can become unsafe at any moment, requiring rapid replanning to avoid collisions.   Recently developed planners have used soft-constraint approaches to achieve the necessary fast computation times; however, these methods do not guarantee collision-free paths even with static obstacles. In contrast, hard-constraint methods ensure collision-free safety, but typically have longer computation times.   To address these issues, we propose three key contributions. First, the DYNUS Global Planner (DGP) and Temporal Safe Corridor Generation operate in spatio-temporal space and handle both static and dynamic obstacles in the 3D environment. Second, the Safe Planning Framework leverages a combination of exploratory, safe, and contingency trajectories to flexibly re-route when potential future collisions with dynamic obstacles are detected. Finally, the Fast Hard-Constraint Local Trajectory Formulation uses a variable elimination approach to reduce the problem size and enable faster computation by pre-computing dependencies between free and dependent variables while still ensuring collision-free trajectories.   We evaluated DYNUS in a variety of simulations, including dense forests, confined office spaces, cave systems, and dynamic environments. Our experiments show that DYNUS achieves a success rate of 100% and travel times that are approximately 25.0% faster than state-of-the-art methods. We also evaluated DYNUS on multiple platforms -- a quadrotor, a wheeled robot, and a quadruped -- in both simulation and hardware experiments."
  },
  {
    "title": "DYNUS: Uncertainty-aware Trajectory Planner in Dynamic Unknown Environments",
    "url": "http://arxiv.org/abs/2504.16734v2",
    "arxiv_id": "2504.16734v2",
    "authors": [
      "Kota Kondo",
      "Mason Peterson",
      "Nicholas Rober",
      "Juan Rached Viso",
      "Lucas Jia",
      "Jialin Chen",
      "Harvey Merton",
      "Jonathan P. How"
    ],
    "published": "2025-04-23T14:05:04+00:00",
    "summary": "This paper introduces DYNUS, an uncertainty-aware trajectory planner designed for dynamic unknown environments. Operating in such settings presents many challenges -- most notably, because the agent cannot predict the ground-truth future paths of obstacles, a previously planned trajectory can become unsafe at any moment, requiring rapid replanning to avoid collisions.   Recently developed planners have used soft-constraint approaches to achieve the necessary fast computation times; however, these methods do not guarantee collision-free paths even with static obstacles. In contrast, hard-constraint methods ensure collision-free safety, but typically have longer computation times.   To address these issues, we propose three key contributions. First, the DYNUS Global Planner (DGP) and Temporal Safe Corridor Generation operate in spatio-temporal space and handle both static and dynamic obstacles in the 3D environment. Second, the Safe Planning Framework leverages a combination of exploratory, safe, and contingency trajectories to flexibly re-route when potential future collisions with dynamic obstacles are detected. Finally, the Fast Hard-Constraint Local Trajectory Formulation uses a variable elimination approach to reduce the problem size and enable faster computation by pre-computing dependencies between free and dependent variables while still ensuring collision-free trajectories.   We evaluated DYNUS in a variety of simulations, including dense forests, confined office spaces, cave systems, and dynamic environments. Our experiments show that DYNUS achieves a success rate of 100% and travel times that are approximately 25.0% faster than state-of-the-art methods. We also evaluated DYNUS on multiple platforms -- a quadrotor, a wheeled robot, and a quadruped -- in both simulation and hardware experiments."
  },
  {
    "title": "Offline Robotic World Model: Learning Robotic Policies without a Physics Simulator",
    "url": "http://arxiv.org/abs/2504.16680v1",
    "arxiv_id": "2504.16680v1",
    "authors": [
      "Chenhao Li",
      "Andreas Krause",
      "Marco Hutter"
    ],
    "published": "2025-04-23T12:58:15+00:00",
    "summary": "Reinforcement Learning (RL) has demonstrated impressive capabilities in robotic control but remains challenging due to high sample complexity, safety concerns, and the sim-to-real gap. While offline RL eliminates the need for risky real-world exploration by learning from pre-collected data, it suffers from distributional shift, limiting policy generalization. Model-Based RL (MBRL) addresses this by leveraging predictive models for synthetic rollouts, yet existing approaches often lack robust uncertainty estimation, leading to compounding errors in offline settings. We introduce Offline Robotic World Model (RWM-O), a model-based approach that explicitly estimates epistemic uncertainty to improve policy learning without reliance on a physics simulator. By integrating these uncertainty estimates into policy optimization, our approach penalizes unreliable transitions, reducing overfitting to model errors and enhancing stability. Experimental results show that RWM-O improves generalization and safety, enabling policy learning purely from real-world data and advancing scalable, data-efficient RL for robotics."
  },
  {
    "title": "3D-1D modelling of cranial plate heating induced by low or medium frequency magnetic fields",
    "url": "http://arxiv.org/abs/2504.16600v1",
    "arxiv_id": "2504.16600v1",
    "authors": [
      "Alessandro Arduino",
      "Oriano Bottauscio",
      "Denise Grappein",
      "Stefano Scial\u00f3",
      "Fabio Vicini",
      "Umberto Zanovello",
      "Luca Zilberti"
    ],
    "published": "2025-04-23T10:29:53+00:00",
    "summary": "Safety assessment of patients with one-dimensionally structured passive implants, like cranial plates or stents, exposed to low or medium frequency magnetic fields, like those generated in magnetic resonance imaging or magnetic hyperthermia, can be challenging, because of the different length scales of the implant and the human body. Most of the methods used to estimate the heating induced near such implants neglect the presence of the metallic materials within the body, modeling the metal as thermal seeds. To overcome this limitation, a novel numerical approach that solves three-dimensional and one-dimensional coupled problems is proposed. This method leads to improved results by modelling the thermal diffusion through the highly conductive metallic implants. A comparison of the proposed method predictions with measurements performed on a cranial plate exposed to the magnetic field generated by a gradient coil system for magnetic resonance imaging is presented, showing an improved accuracy up to 25 % with respect to the method based on thermal seeds. The proposed method is finally applied to a magnetic hyperthermia case study in which a patient with a cranial plate is exposed to the magnetic field generated by a collar-type magnetic hyperthermia applicator for neck tumour treatment, predicting a temperature increase in proximity of the implant that is 10 % lower than the one overestimated by relying on thermal seeds."
  },
  {
    "title": "Streetscape Analysis with Generative AI (SAGAI): Vision-Language Assessment and Mapping of Urban Scenes",
    "url": "http://arxiv.org/abs/2504.16538v1",
    "arxiv_id": "2504.16538v1",
    "authors": [
      "Joan Perez",
      "Giovanni Fusco"
    ],
    "published": "2025-04-23T09:08:06+00:00",
    "summary": "Streetscapes are an essential component of urban space. Their assessment is presently either limited to morphometric properties of their mass skeleton or requires labor-intensive qualitative evaluations of visually perceived qualities. This paper introduces SAGAI: Streetscape Analysis with Generative Artificial Intelligence, a modular workflow for scoring street-level urban scenes using open-access data and vision-language models. SAGAI integrates OpenStreetMap geometries, Google Street View imagery, and a lightweight version of the LLaVA model to generate structured spatial indicators from images via customizable natural language prompts. The pipeline includes an automated mapping module that aggregates visual scores at both the point and street levels, enabling direct cartographic interpretation. It operates without task-specific training or proprietary software dependencies, supporting scalable and interpretable analysis of urban environments. Two exploratory case studies in Nice and Vienna illustrate SAGAI's capacity to produce geospatial outputs from vision-language inference. The initial results show strong performance for binary urban-rural scene classification, moderate precision in commercial feature detection, and lower estimates, but still informative, of sidewalk width. Fully deployable by any user, SAGAI can be easily adapted to a wide range of urban research themes, such as walkability, safety, or urban design, through prompt modification alone."
  },
  {
    "title": "SafeSpect: Safety-First Augmented Reality Heads-up Display for Drone Inspections",
    "url": "http://arxiv.org/abs/2504.16533v1",
    "arxiv_id": "2504.16533v1",
    "authors": [
      "Peisen Xu",
      "J\u00e9r\u00e9mie Garcia",
      "Wei Tsang Ooi",
      "Christophe Jouffrais"
    ],
    "published": "2025-04-23T08:59:05+00:00",
    "summary": "Current tablet-based interfaces for drone operations often impose a heavy cognitive load on pilots and reduce situational awareness by dividing attention between the video feed and the real world. To address these challenges, we designed a heads-up augmented reality (AR) interface that overlays in-situ information to support drone pilots in safety-critical tasks. Through participatory design workshops with professional pilots, we identified key features and developed an adaptive AR interface that dynamically switches between task and safety views to prevent information overload. We evaluated our prototype by creating a realistic building inspection task and comparing three interfaces: a 2D tablet, a static AR, and our adaptive AR design. A user study with 15 participants showed that the AR interface improved access to safety information, while the adaptive AR interface reduced cognitive load and enhanced situational awareness without compromising task performance. We offer design insights for developing safety-first heads-up AR interfaces."
  },
  {
    "title": "Amplified Vulnerabilities: Structured Jailbreak Attacks on LLM-based Multi-Agent Debate",
    "url": "http://arxiv.org/abs/2504.16489v1",
    "arxiv_id": "2504.16489v1",
    "authors": [
      "Senmao Qi",
      "Yifei Zou",
      "Peng Li",
      "Ziyi Lin",
      "Xiuzhen Cheng",
      "Dongxiao Yu"
    ],
    "published": "2025-04-23T08:01:50+00:00",
    "summary": "Multi-Agent Debate (MAD), leveraging collaborative interactions among Large Language Models (LLMs), aim to enhance reasoning capabilities in complex tasks. However, the security implications of their iterative dialogues and role-playing characteristics, particularly susceptibility to jailbreak attacks eliciting harmful content, remain critically underexplored. This paper systematically investigates the jailbreak vulnerabilities of four prominent MAD frameworks built upon leading commercial LLMs (GPT-4o, GPT-4, GPT-3.5-turbo, and DeepSeek) without compromising internal agents. We introduce a novel structured prompt-rewriting framework specifically designed to exploit MAD dynamics via narrative encapsulation, role-driven escalation, iterative refinement, and rhetorical obfuscation. Our extensive experiments demonstrate that MAD systems are inherently more vulnerable than single-agent setups. Crucially, our proposed attack methodology significantly amplifies this fragility, increasing average harmfulness from 28.14% to 80.34% and achieving attack success rates as high as 80% in certain scenarios. These findings reveal intrinsic vulnerabilities in MAD architectures and underscore the urgent need for robust, specialized defenses prior to real-world deployment."
  },
  {
    "title": "The Dodecacopter: a Versatile Multirotor System of Dodecahedron-Shaped Modules",
    "url": "http://arxiv.org/abs/2504.16475v1",
    "arxiv_id": "2504.16475v1",
    "authors": [
      "K\u00e9vin Garanger",
      "Thanakorn Khamvilai",
      "Jeremy Epps",
      "Eric Feron"
    ],
    "published": "2025-04-23T07:38:00+00:00",
    "summary": "With the promise of greater safety and adaptability, modular reconfigurable uncrewed air vehicles have been proposed as unique, versatile platforms holding the potential to replace multiple types of monolithic vehicles at once. State-of-the-art rigidly assembled modular vehicles are generally two-dimensional configurations in which the rotors are coplanar and assume the shape of a \"flight array\". We introduce the Dodecacopter, a new type of modular rotorcraft where all modules take the shape of a regular dodecahedron, allowing the creation of richer sets of configurations beyond flight arrays. In particular, we show how the chosen module design can be used to create three-dimensional and fully actuated configurations. We justify the relevance of these types of configurations in terms of their structural and actuation properties with various performance indicators. Given the broad range of configurations and capabilities that can be achieved with our proposed design, we formulate tractable optimization programs to find optimal configurations given structural and actuation constraints. Finally, a prototype of such a vehicle is presented along with results of performed flights in multiple configurations."
  },
  {
    "title": "ERASER: Efficient RTL FAult Simulation Framework with Trimmed Execution Redundancy",
    "url": "http://arxiv.org/abs/2504.16473v1",
    "arxiv_id": "2504.16473v1",
    "authors": [
      "Jiaping Tang",
      "Jianan Mu",
      "Silin Liu",
      "Zizhen Liu",
      "Feng Gu",
      "Xinyu Zhang",
      "Leyan Wang",
      "Shenwen Liang",
      "Jing Ye",
      "Huawei Li",
      "Xiaowei Li"
    ],
    "published": "2025-04-23T07:33:44+00:00",
    "summary": "As intelligent computing devices increasingly integrate into human life, ensuring the functional safety of the corresponding electronic chips becomes more critical. A key metric for functional safety is achieving a sufficient fault coverage. To meet this requirement, extensive time-consuming fault simulation of the RTL code is necessary during the chip design phase.The main overhead in RTL fault simulation comes from simulating behavioral nodes (always blocks). Due to the limited fault propagation capacity, fault simulation results often match the good simulation results for many behavioral nodes. A key strategy for accelerating RTL fault simulation is the identification and elimination of redundant simulations. Existing methods detect redundant executions by examining whether the fault inputs to each RTL node are consistent with the good inputs. However, we observe that this input comparison mechanism overlooks a significant amount of implicit redundant execution: although the fault inputs differ from the good inputs, the node's execution results remain unchanged. Our experiments reveal that this overlooked redundant execution constitutes nearly half of the total execution overhead of behavioral nodes, becoming a significant bottleneck in current RTL fault simulation. The underlying reason for this overlooked redundancy is that, in these cases, the true execution paths within the behavioral nodes are not affected by the changes in input values. In this work, we propose a behavior-level redundancy detection algorithm that focuses on the true execution paths. Building on the elimination of redundant executions, we further developed an efficient RTL fault simulation framework, Eraser.Experimental results show that compared to commercial tools, under the same fault coverage, our framework achieves a 3.9 $\\times$ improvement in simulation performance on average."
  },
  {
    "title": "iTFKAN: Interpretable Time Series Forecasting with Kolmogorov-Arnold Network",
    "url": "http://arxiv.org/abs/2504.16432v1",
    "arxiv_id": "2504.16432v1",
    "authors": [
      "Ziran Liang",
      "Rui An",
      "Wenqi Fan",
      "Yanghui Rao",
      "Yuxuan Liang"
    ],
    "published": "2025-04-23T05:34:49+00:00",
    "summary": "As time evolves, data within specific domains exhibit predictability that motivates time series forecasting to predict future trends from historical data. However, current deep forecasting methods can achieve promising performance but generally lack interpretability, hindering trustworthiness and practical deployment in safety-critical applications such as auto-driving and healthcare. In this paper, we propose a novel interpretable model, iTFKAN, for credible time series forecasting. iTFKAN enables further exploration of model decision rationales and underlying data patterns due to its interpretability achieved through model symbolization. Besides, iTFKAN develops two strategies, prior knowledge injection, and time-frequency synergy learning, to effectively guide model learning under complex intertwined time series data. Extensive experimental results demonstrated that iTFKAN can achieve promising forecasting performance while simultaneously possessing high interpretive capabilities."
  },
  {
    "title": "On Validating Angular Power Spectral Models for the Stochastic Gravitational-Wave Background Without Distributional Assumptions",
    "url": "http://arxiv.org/abs/2504.16959v1",
    "arxiv_id": "2504.16959v1",
    "authors": [
      "Xiangyu Zhang",
      "Erik Floden",
      "Hongru Zhao",
      "Sara Algeri",
      "Galin Jones",
      "Vuk Mandic",
      "Jesse Miller"
    ],
    "published": "2025-04-23T05:03:39+00:00",
    "summary": "It is demonstrated that estimators of the angular power spectrum commonly used for the stochastic gravitational-wave background (SGWB) lack a closed-form analytical expression for the likelihood function and, typically, cannot be accurately approximated by a Gaussian likelihood. Nevertheless, a robust statistical analysis can be performed by extending the framework outlined in \\cite{PRL} to enable the estimation and testing of angular power spectral models for the SGWB without specifying distributional assumptions. Here, the technical aspects of the method are discussed in detail. Moreover, a new, consistent estimator for the covariance of the angular power spectrum is derived. The proposed approach is applied to data from the third observing run (O3) of Advanced LIGO and Advanced Virgo."
  },
  {
    "title": "Anytime Safe Reinforcement Learning",
    "url": "http://arxiv.org/abs/2504.16417v1",
    "arxiv_id": "2504.16417v1",
    "authors": [
      "Pol Mestres",
      "Arnau Marzabal",
      "Jorge Cort\u00e9s"
    ],
    "published": "2025-04-23T04:51:31+00:00",
    "summary": "This paper considers the problem of solving constrained   reinforcement learning problems with anytime guarantees, meaning   that the algorithmic solution returns a safe policy regardless of   when it is terminated. Drawing inspiration from anytime constrained   optimization, we introduce Reinforcement Learning-based Safe   Gradient Flow (RL-SGF), an on-policy algorithm which employs   estimates of the value functions and their respective gradients   associated with the objective and safety constraints for the current   policy, and updates the policy parameters by solving a convex   quadratically constrained quadratic program. We show that if the   estimates are computed with a sufficiently large number of episodes   (for which we provide an explicit bound), safe policies are updated   to safe policies with a probability higher than a prescribed   tolerance. We also show that iterates asymptotically converge to a   neighborhood of a KKT point, whose size can be arbitrarily reduced   by refining the estimates of the value function and their gradients.   We illustrate the performance of RL-SGF in a navigation example."
  },
  {
    "title": "SILM: A Subjective Intent Based Low-Latency Framework for Multiple Traffic Participants Joint Trajectory Prediction",
    "url": "http://arxiv.org/abs/2504.16377v1",
    "arxiv_id": "2504.16377v1",
    "authors": [
      "Qu Weiming",
      "Wang Jia",
      "Du Jiawei",
      "Zhu Yuanhao",
      "Yu Jianfeng",
      "Xia Rui",
      "Cao Song",
      "Wu Xihong",
      "Luo Dingsheng"
    ],
    "published": "2025-04-23T02:56:34+00:00",
    "summary": "Trajectory prediction is a fundamental technology for advanced autonomous driving systems and represents one of the most challenging problems in the field of cognitive intelligence. Accurately predicting the future trajectories of each traffic participant is a prerequisite for building high safety and high reliability decision-making, planning, and control capabilities in autonomous driving. However, existing methods often focus solely on the motion of other traffic participants without considering the underlying intent behind that motion, which increases the uncertainty in trajectory prediction. Autonomous vehicles operate in real-time environments, meaning that trajectory prediction algorithms must be able to process data and generate predictions in real-time. While many existing methods achieve high accuracy, they often struggle to effectively handle heterogeneous traffic scenarios. In this paper, we propose a Subjective Intent-based Low-latency framework for Multiple traffic participants joint trajectory prediction. Our method explicitly incorporates the subjective intent of traffic participants based on their key points, and predicts the future trajectories jointly without map, which ensures promising performance while significantly reducing the prediction latency. Additionally, we introduce a novel dataset designed specifically for trajectory prediction. Related code and dataset will be available soon."
  },
  {
    "title": "The Safety-Privacy Tradeoff in Linear Bandits",
    "url": "http://arxiv.org/abs/2504.16371v1",
    "arxiv_id": "2504.16371v1",
    "authors": [
      "Arghavan Zibaie",
      "Spencer Hutchinson",
      "Ramtin Pedarsani",
      "Mahnoosh Alizadeh"
    ],
    "published": "2025-04-23T02:48:02+00:00",
    "summary": "We consider a collection of linear stochastic bandit problems, each modeling the random response of different agents to proposed interventions, coupled together by a global safety constraint. We assume a central coordinator must choose actions to play on each bandit with the objective of regret minimization, while also ensuring that the expected response of all agents satisfies the global safety constraints at each round, in spite of uncertainty about the bandits' parameters. The agents consider their observed responses to be private and in order to protect their sensitive information, the data sharing with the central coordinator is performed under local differential privacy (LDP). However, providing higher level of privacy to different agents would have consequences in terms of safety and regret. We formalize these tradeoffs by building on the notion of the sharpness of the safety set - a measure of how the geometric properties of the safe set affects the growth of regret - and propose a unilaterally unimprovable vector of privacy levels for different agents given a maximum regret budget."
  },
  {
    "title": "VeriFix: Verifying Your Fix Towards An Atomicity Violation",
    "url": "http://arxiv.org/abs/2504.16354v1",
    "arxiv_id": "2504.16354v1",
    "authors": [
      "Zhuang Li",
      "Qiuping Yi",
      "Jeff Huang"
    ],
    "published": "2025-04-23T02:11:07+00:00",
    "summary": "Atomicity violation is one of the most serious types of bugs in concurrent programs. Synchronizations are commonly used to enforce atomicity. However, it is very challenging to place synchronizations correctly and sufficiently   due to complex thread interactions and large input space. This paper presents \\textsf{VeriFix}, a new approach for verifying atomicity violation fixes. Given a buggy trace that exposes an atomicity violation and a corresponding fix, % in the form of locks, \\textsf{VeriFix} effectively verifies if the fix introduces sufficient synchronizations to repair the atomicity violation without introducing new deadlocks. The key idea is that \\textsf{VeriFix} transforms the fix verification problem into a property verification problem, in which both the observed atomicity violation and potential deadlocks are encoded as a safety property, and both the inputs and schedules are encoded as symbolic constraints. By reasoning the conjoined constraints with an SMT solver, \\textsf{VeriFix} systematically explores all reachable paths %from the whole schedule and input space and verifies if there exists a concrete \\textit{schedule+input} combination to manifest the intended atomicity or any new deadlocks. We have implemented and evaluated \\verifix\\ on a collection of real-world C/C++ programs. The result shows that \\textsf{VeriFix} significantly outperforms the state-of-the-art."
  },
  {
    "title": "Impact of Noise on LLM-Models Performance in Abstraction and Reasoning Corpus (ARC) Tasks with Model Temperature Considerations",
    "url": "http://arxiv.org/abs/2504.15903v1",
    "arxiv_id": "2504.15903v1",
    "authors": [
      "Nikhil Khandalkar",
      "Pavan Yadav",
      "Krishna Shinde",
      "Lokesh B. Ramegowda",
      "Rajarshi Das"
    ],
    "published": "2025-04-22T13:43:58+00:00",
    "summary": "Recent advancements in Large Language Models (LLMs) have generated growing interest in their structured reasoning capabilities, particularly in tasks involving abstraction and pattern recognition. The Abstraction and Reasoning Corpus (ARC) benchmark plays a crucial role in evaluating these capabilities by testing how well AI models generalize to novel problems. While GPT-4o demonstrates strong performance by solving all ARC tasks under zero-noise conditions, other models like DeepSeek R1 and LLaMA 3.2 fail to solve any, suggesting limitations in their ability to reason beyond simple pattern matching. To explore this gap, we systematically evaluate these models across different noise levels and temperature settings. Our results reveal that the introduction of noise consistently impairs model performance, regardless of architecture. This decline highlights a shared vulnerability: current LLMs, despite showing signs of abstract reasoning, remain highly sensitive to input perturbations. Such fragility raises concerns about their real-world applicability, where noise and uncertainty are common. By comparing how different model architectures respond to these challenges, we offer insights into the structural weaknesses of modern LLMs in reasoning tasks. This work underscores the need for developing more robust and adaptable AI systems capable of handling the ambiguity and variability inherent in real-world scenarios. Our findings aim to guide future research toward enhancing model generalization, robustness, and alignment with human-like cognitive flexibility."
  },
  {
    "title": "Impact of Noise on LLM-Models Performance in Abstraction and Reasoning Corpus (ARC) Tasks with Model Temperature Considerations",
    "url": "http://arxiv.org/abs/2504.15903v2",
    "arxiv_id": "2504.15903v2",
    "authors": [
      "Nikhil Khandalkar",
      "Pavan Yadav",
      "Krishna Shinde",
      "Lokesh B. Ramegowda",
      "Rajarshi Das"
    ],
    "published": "2025-04-22T13:43:58+00:00",
    "summary": "Recent advancements in Large Language Models (LLMs) have generated growing interest in their structured reasoning capabilities, particularly in tasks involving abstraction and pattern recognition. The Abstraction and Reasoning Corpus (ARC) benchmark plays a crucial role in evaluating these capabilities by testing how well AI models generalize to novel problems. While GPT-4o demonstrates strong performance by solving all ARC tasks under zero-noise conditions, other models like DeepSeek R1 and LLaMA 3.2 fail to solve any, suggesting limitations in their ability to reason beyond simple pattern matching. To explore this gap, we systematically evaluate these models across different noise levels and temperature settings. Our results reveal that the introduction of noise consistently impairs model performance, regardless of architecture. This decline highlights a shared vulnerability: current LLMs, despite showing signs of abstract reasoning, remain highly sensitive to input perturbations. Such fragility raises concerns about their real-world applicability, where noise and uncertainty are common. By comparing how different model architectures respond to these challenges, we offer insights into the structural weaknesses of modern LLMs in reasoning tasks. This work underscores the need for developing more robust and adaptable AI systems capable of handling the ambiguity and variability inherent in real-world scenarios. Our findings aim to guide future research toward enhancing model generalization, robustness, and alignment with human-like cognitive flexibility."
  },
  {
    "title": "SARI: Structured Audio Reasoning via Curriculum-Guided Reinforcement Learning",
    "url": "http://arxiv.org/abs/2504.15900v1",
    "arxiv_id": "2504.15900v1",
    "authors": [
      "Cheng Wen",
      "Tingwei Guo",
      "Shuaijiang Zhao",
      "Wei Zou",
      "Xiangang Li"
    ],
    "published": "2025-04-22T13:41:26+00:00",
    "summary": "Recent work shows that reinforcement learning(RL) can markedly sharpen the reasoning ability of large language models (LLMs) by prompting them to \"think before answering.\" Yet whether and how these gains transfer to audio-language reasoning remains largely unexplored. We extend the Group-Relative Policy Optimization (GRPO) framework from DeepSeek-R1 to a Large Audio-Language Model (LALM), and construct a 32k sample multiple-choice corpus. Using a two-stage regimen supervised fine-tuning on structured and unstructured chains-of-thought, followed by curriculum-guided GRPO, we systematically compare implicit vs. explicit, and structured vs. free form reasoning under identical architectures. Our structured audio reasoning model, SARI (Structured Audio Reasoning via Curriculum-Guided Reinforcement Learning), achieves a 16.35% improvement in average accuracy over the base model Qwen2-Audio-7B-Instruct. Furthermore, the variant built upon Qwen2.5-Omni reaches state-of-the-art performance of 67.08% on the MMAU test-mini benchmark. Ablation experiments show that on the base model we use: (i) SFT warm-up is important for stable RL training, (ii) structured chains yield more robust generalization than unstructured ones, and (iii) easy-to-hard curricula accelerate convergence and improve final performance. These findings demonstrate that explicit, structured reasoning and curriculum learning substantially enhances audio-language understanding."
  },
  {
    "title": "Dynamic Early Exit in Reasoning Models",
    "url": "http://arxiv.org/abs/2504.15895v1",
    "arxiv_id": "2504.15895v1",
    "authors": [
      "Chenxu Yang",
      "Qingyi Si",
      "Yongjie Duan",
      "Zheliang Zhu",
      "Chenyu Zhu",
      "Zheng Lin",
      "Li Cao",
      "Weiping Wang"
    ],
    "published": "2025-04-22T13:36:53+00:00",
    "summary": "Recent advances in large reasoning language models (LRLMs) rely on test-time scaling, which extends long chain-of-thought (CoT) generation to solve complex tasks. However, overthinking in long CoT not only slows down the efficiency of problem solving, but also risks accuracy loss due to the extremely detailed or redundant reasoning steps. We propose a simple yet effective method that allows LLMs to self-truncate CoT sequences by early exit during generation. Instead of relying on fixed heuristics, the proposed method monitors model behavior at potential reasoning transition points (e.g.,\"Wait\" tokens) and dynamically terminates the next reasoning chain's generation when the model exhibits high confidence in a trial answer. Our method requires no additional training and can be seamlessly integrated into existing o1-like reasoning LLMs. Experiments on multiple reasoning benchmarks MATH-500, AMC 2023, GPQA Diamond and AIME 2024 show that the proposed method is consistently effective on deepseek-series reasoning LLMs, reducing the length of CoT sequences by an average of 31% to 43% while improving accuracy by 1.7% to 5.7%."
  },
  {
    "title": "MS-Occ: Multi-Stage LiDAR-Camera Fusion for 3D Semantic Occupancy Prediction",
    "url": "http://arxiv.org/abs/2504.15888v1",
    "arxiv_id": "2504.15888v1",
    "authors": [
      "Zhiqiang Wei",
      "Lianqing Zheng",
      "Jianan Liu",
      "Tao Huang",
      "Qing-Long Han",
      "Wenwen Zhang",
      "Fengdeng Zhang"
    ],
    "published": "2025-04-22T13:33:26+00:00",
    "summary": "Accurate 3D semantic occupancy perception is essential for autonomous driving in complex environments with diverse and irregular objects. While vision-centric methods suffer from geometric inaccuracies, LiDAR-based approaches often lack rich semantic information. To address these limitations, MS-Occ, a novel multi-stage LiDAR-camera fusion framework which includes middle-stage fusion and late-stage fusion, is proposed, integrating LiDAR's geometric fidelity with camera-based semantic richness via hierarchical cross-modal fusion. The framework introduces innovations at two critical stages: (1) In the middle-stage feature fusion, the Gaussian-Geo module leverages Gaussian kernel rendering on sparse LiDAR depth maps to enhance 2D image features with dense geometric priors, and the Semantic-Aware module enriches LiDAR voxels with semantic context via deformable cross-attention; (2) In the late-stage voxel fusion, the Adaptive Fusion (AF) module dynamically balances voxel features across modalities, while the High Classification Confidence Voxel Fusion (HCCVF) module resolves semantic inconsistencies using self-attention-based refinement. Experiments on the nuScenes-OpenOccupancy benchmark show that MS-Occ achieves an Intersection over Union (IoU) of 32.1% and a mean IoU (mIoU) of 25.3%, surpassing the state-of-the-art by +0.7% IoU and +2.4% mIoU. Ablation studies further validate the contribution of each module, with substantial improvements in small-object perception, demonstrating the practical value of MS-Occ for safety-critical autonomous driving scenarios."
  },
  {
    "title": "Embedded Safe Reactive Navigation for Multirotors Systems using Control Barrier Functions",
    "url": "http://arxiv.org/abs/2504.15850v1",
    "arxiv_id": "2504.15850v1",
    "authors": [
      "Nazar Misyats",
      "Marvin Harms",
      "Morten Nissov",
      "Martin Jacquet",
      "Kostas Alexis"
    ],
    "published": "2025-04-22T12:45:11+00:00",
    "summary": "Aiming to promote the wide adoption of safety filters for autonomous aerial robots, this paper presents a safe control architecture designed for seamless integration into widely used open-source autopilots. Departing from methods that require consistent localization and mapping, we formalize the obstacle avoidance problem as a composite control barrier function constructed only from the online onboard range measurements. The proposed framework acts as a safety filter, modifying the acceleration references derived by the nominal position/velocity control loops, and is integrated into the PX4 autopilot stack. Experimental studies using a small multirotor aerial robot demonstrate the effectiveness and performance of the solution within dynamic maneuvering and unknown environments."
  },
  {
    "title": "TrustGeoGen: Scalable and Formal-Verified Data Engine for Trustworthy Multi-modal Geometric Problem Solving",
    "url": "http://arxiv.org/abs/2504.15780v1",
    "arxiv_id": "2504.15780v1",
    "authors": [
      "Daocheng Fu",
      "Zijun Chen",
      "Renqiu Xia",
      "Qi Liu",
      "Yuan Feng",
      "Hongbin Zhou",
      "Renrui Zhang",
      "Shiyang Feng",
      "Peng Gao",
      "Junchi Yan",
      "Botian Shi",
      "Bo Zhang",
      "Yu Qiao"
    ],
    "published": "2025-04-22T10:45:23+00:00",
    "summary": "Mathematical geometric problem solving (GPS) often requires effective integration of multimodal information and verifiable logical coherence. Despite the fast development of large language models in general problem solving, it remains unresolved regarding with both methodology and benchmarks, especially given the fact that exiting synthetic GPS benchmarks are often not self-verified and contain noise and self-contradicted information due to the illusion of LLMs. In this paper, we propose a scalable data engine called TrustGeoGen for problem generation, with formal verification to provide a principled benchmark, which we believe lays the foundation for the further development of methods for GPS. The engine synthesizes geometric data through four key innovations: 1) multimodal-aligned generation of diagrams, textual descriptions, and stepwise solutions; 2) formal verification ensuring rule-compliant reasoning paths; 3) a bootstrapping mechanism enabling complexity escalation via recursive state generation and 4) our devised GeoExplore series algorithms simultaneously produce multi-solution variants and self-reflective backtracking traces. By formal logical verification, TrustGeoGen produces GeoTrust-200K dataset with guaranteed modality integrity, along with GeoTrust-test testset. Experiments reveal the state-of-the-art models achieve only 49.17\\% accuracy on GeoTrust-test, demonstrating its evaluation stringency. Crucially, models trained on GeoTrust achieve OOD generalization on GeoQA, significantly reducing logical inconsistencies relative to pseudo-label annotated by OpenAI-o1. Our code is available at https://github.com/Alpha-Innovator/TrustGeoGen"
  },
  {
    "title": "Pose Optimization for Autonomous Driving Datasets using Neural Rendering Models",
    "url": "http://arxiv.org/abs/2504.15776v1",
    "arxiv_id": "2504.15776v1",
    "authors": [
      "Quentin Herau",
      "Nathan Piasco",
      "Moussab Bennehar",
      "Luis Rolado",
      "Dzmitry Tsishkou",
      "Bingbing Liu",
      "Cyrille Migniot",
      "Pascal Vasseur",
      "C\u00e9dric Demonceaux"
    ],
    "published": "2025-04-22T10:33:01+00:00",
    "summary": "Autonomous driving systems rely on accurate perception and localization of the ego car to ensure safety and reliability in challenging real-world driving scenarios. Public datasets play a vital role in benchmarking and guiding advancement in research by providing standardized resources for model development and evaluation. However, potential inaccuracies in sensor calibration and vehicle poses within these datasets can lead to erroneous evaluations of downstream tasks, adversely impacting the reliability and performance of the autonomous systems. To address this challenge, we propose a robust optimization method based on Neural Radiance Fields (NeRF) to refine sensor poses and calibration parameters, enhancing the integrity of dataset benchmarks. To validate improvement in accuracy of our optimized poses without ground truth, we present a thorough evaluation process, relying on reprojection metrics, Novel View Synthesis rendering quality, and geometric alignment. We demonstrate that our method achieves significant improvements in sensor pose accuracy. By optimizing these critical parameters, our approach not only improves the utility of existing datasets but also paves the way for more reliable autonomous driving models. To foster continued progress in this field, we make the optimized sensor poses publicly available, providing a valuable resource for the research community."
  },
  {
    "title": "Prediction of CO2 reduction reaction intermediates and products on transition metal-doped r-GeSe monolayers:A combined DFT and machine learning approach",
    "url": "http://arxiv.org/abs/2504.15710v1",
    "arxiv_id": "2504.15710v1",
    "authors": [
      "Xuxin Kang",
      "Wenjing Zhou",
      "Ziyuan Li",
      "Zhaoqin Chu",
      "Hanqin Yin",
      "Shan Gao",
      "Aijun Du",
      "Xiangmei Duan"
    ],
    "published": "2025-04-22T08:52:18+00:00",
    "summary": "The electrocatalytic CO2 reduction reaction (CO2RR) is a complex multi-proton-electron transfer process that generates a vast network of reaction intermediates. Accurate prediction of free energy changes (G) of these intermediates and products is essential for evaluating catalytic performance. We combined density functional theory (DFT) and machine learning (ML) to screen 25 single-atom catalysts (SACs) on defective r-GeSe monolayers for CO2 reduction to methanol, methane, and formic acid. Among nine ML models evaluated with 14 intrinsic and DFT-based features, the XGBoost performed best (R2 = 0.92 and MAE = 0.24 eV), aligning closely with DFT calculations and identifying Ni, Ru, and Rh@GeSe as prospective catalysts. Feature importance analysis in free energy and product predictions highlighted the significance of CO2 activation with O-C-O and IPC-O1 as the key attributes. Furthermore, by incorporating non-DFT-based features, rapid predictions became possible, and the XGBoost model retained its predictive performance with R2 = 0.89 and MAE = 0.29 eV. This accuracy was further validated using Ir@GeSe. Our work highlights effective SACs for CO2RR, and provides valuable insights for efficient catalyst design."
  },
  {
    "title": "Advancing Embodied Agent Security: From Safety Benchmarks to Input Moderation",
    "url": "http://arxiv.org/abs/2504.15699v1",
    "arxiv_id": "2504.15699v1",
    "authors": [
      "Ning Wang",
      "Zihan Yan",
      "Weiyang Li",
      "Chuan Ma",
      "He Chen",
      "Tao Xiang"
    ],
    "published": "2025-04-22T08:34:35+00:00",
    "summary": "Embodied agents exhibit immense potential across a multitude of domains, making the assurance of their behavioral safety a fundamental prerequisite for their widespread deployment. However, existing research predominantly concentrates on the security of general large language models, lacking specialized methodologies for establishing safety benchmarks and input moderation tailored to embodied agents. To bridge this gap, this paper introduces a novel input moderation framework, meticulously designed to safeguard embodied agents. This framework encompasses the entire pipeline, including taxonomy definition, dataset curation, moderator architecture, model training, and rigorous evaluation. Notably, we introduce EAsafetyBench, a meticulously crafted safety benchmark engineered to facilitate both the training and stringent assessment of moderators specifically designed for embodied agents. Furthermore, we propose Pinpoint, an innovative prompt-decoupled input moderation scheme that harnesses a masked attention mechanism to effectively isolate and mitigate the influence of functional prompts on moderation tasks. Extensive experiments conducted on diverse benchmark datasets and models validate the feasibility and efficacy of the proposed approach. The results demonstrate that our methodologies achieve an impressive average detection accuracy of 94.58%, surpassing the performance of existing state-of-the-art techniques, alongside an exceptional moderation processing time of merely 0.002 seconds per instance."
  },
  {
    "title": "Symbolic Runtime Verification and Adaptive Decision-Making for Robot-Assisted Dressing",
    "url": "http://arxiv.org/abs/2504.15666v1",
    "arxiv_id": "2504.15666v1",
    "authors": [
      "Yasmin Rafiq",
      "Gricel V\u00e1zquez",
      "Radu Calinescu",
      "Sanja Dogramadzi",
      "Robert M Hierons"
    ],
    "published": "2025-04-22T07:42:16+00:00",
    "summary": "We present a control framework for robot-assisted dressing that augments low-level hazard response with runtime monitoring and formal verification. A parametric discrete-time Markov chain (pDTMC) models the dressing process, while Bayesian inference dynamically updates this pDTMC's transition probabilities based on sensory and user feedback. Safety constraints from hazard analysis are expressed in probabilistic computation tree logic, and symbolically verified using a probabilistic model checker. We evaluate reachability, cost, and reward trade-offs for garment-snag mitigation and escalation, enabling real-time adaptation. Our approach provides a formal yet lightweight foundation for safety-aware, explainable robotic assistance."
  },
  {
    "title": "An ACO-MPC Framework for Energy-Efficient and Collision-Free Path Planning in Autonomous Maritime Navigation",
    "url": "http://arxiv.org/abs/2504.15611v1",
    "arxiv_id": "2504.15611v1",
    "authors": [
      "Yaoze Liu",
      "Zhen Tian",
      "Qifan Zhou",
      "Zixuan Huang",
      "Hongyu Sun"
    ],
    "published": "2025-04-22T06:09:54+00:00",
    "summary": "Automated driving on ramps presents significant challenges due to the need to balance both safety and efficiency during lane changes. This paper proposes an integrated planner for automated vehicles (AVs) on ramps, utilizing an unsatisfactory level metric for efficiency and arrow-cluster-based sampling for safety. The planner identifies optimal times for the AV to change lanes, taking into account the vehicle's velocity as a key factor in efficiency. Additionally, the integrated planner employs arrow-cluster-based sampling to evaluate collision risks and select an optimal lane-changing curve. Extensive simulations were conducted in a ramp scenario to verify the planner's efficient and safe performance. The results demonstrate that the proposed planner can effectively select an appropriate lane-changing time point and a safe lane-changing curve for AVs, without incurring any collisions during the maneuver."
  },
  {
    "title": "A Comprehensive Survey in LLM(-Agent) Full Stack Safety: Data, Training and Deployment",
    "url": "http://arxiv.org/abs/2504.15585v1",
    "arxiv_id": "2504.15585v1",
    "authors": [
      "Kun Wang",
      "Guibin Zhang",
      "Zhenhong Zhou",
      "Jiahao Wu",
      "Miao Yu",
      "Shiqian Zhao",
      "Chenlong Yin",
      "Jinhu Fu",
      "Yibo Yan",
      "Hanjun Luo",
      "Liang Lin",
      "Zhihao Xu",
      "Haolang Lu",
      "Xinye Cao",
      "Xinyun Zhou",
      "Weifei Jin",
      "Fanci Meng",
      "Junyuan Mao",
      "Hao Wu",
      "Minghe Wang",
      "Fan Zhang",
      "Junfeng Fang",
      "Chengwei Liu",
      "Yifan Zhang",
      "Qiankun Li",
      "Chongye Guo",
      "Yalan Qin",
      "Yi Ding",
      "Donghai Hong",
      "Jiaming Ji",
      "Xinfeng Li",
      "Yifan Jiang",
      "Dongxia Wang",
      "Yihao Huang",
      "Yufei Guo",
      "Jen-tse Huang",
      "Yanwei Yue",
      "Wenke Huang",
      "Guancheng Wan",
      "Tianlin Li",
      "Lei Bai",
      "Jie Zhang",
      "Qing Guo",
      "Jingyi Wang",
      "Tianlong Chen",
      "Joey Tianyi Zhou",
      "Xiaojun Jia",
      "Weisong Sun",
      "Cong Wu",
      "Jing Chen",
      "Xuming Hu",
      "Yiming Li",
      "Xiao Wang",
      "Ningyu Zhang",
      "Luu Anh Tuan",
      "Guowen Xu",
      "Tianwei Zhang",
      "Xingjun Ma",
      "Xiang Wang",
      "Bo An",
      "Jun Sun",
      "Mohit Bansal",
      "Shirui Pan",
      "Yuval Elovici",
      "Bhavya Kailkhura",
      "Bo Li",
      "Yaodong Yang",
      "Hongwei Li",
      "Wenyuan Xu",
      "Yizhou Sun",
      "Wei Wang",
      "Qing Li",
      "Ke Tang",
      "Yu-Gang Jiang",
      "Felix Juefei-Xu",
      "Hui Xiong",
      "Xiaofeng Wang",
      "Shuicheng Yan",
      "Dacheng Tao",
      "Philip S. Yu",
      "Qingsong Wen",
      "Yang Liu"
    ],
    "published": "2025-04-22T05:02:49+00:00",
    "summary": "The remarkable success of Large Language Models (LLMs) has illuminated a promising pathway toward achieving Artificial General Intelligence for both academic and industrial communities, owing to their unprecedented performance across various applications. As LLMs continue to gain prominence in both research and commercial domains, their security and safety implications have become a growing concern, not only for researchers and corporations but also for every nation. Currently, existing surveys on LLM safety primarily focus on specific stages of the LLM lifecycle, e.g., deployment phase or fine-tuning phase, lacking a comprehensive understanding of the entire \"lifechain\" of LLMs. To address this gap, this paper introduces, for the first time, the concept of \"full-stack\" safety to systematically consider safety issues throughout the entire process of LLM training, deployment, and eventual commercialization. Compared to the off-the-shelf LLM safety surveys, our work demonstrates several distinctive advantages: (I) Comprehensive Perspective. We define the complete LLM lifecycle as encompassing data preparation, pre-training, post-training, deployment and final commercialization. To our knowledge, this represents the first safety survey to encompass the entire lifecycle of LLMs. (II) Extensive Literature Support. Our research is grounded in an exhaustive review of over 800+ papers, ensuring comprehensive coverage and systematic organization of security issues within a more holistic understanding. (III) Unique Insights. Through systematic literature analysis, we have developed reliable roadmaps and perspectives for each chapter. Our work identifies promising research directions, including safety in data generation, alignment techniques, model editing, and LLM-based agent systems. These insights provide valuable guidance for researchers pursuing future work in this field."
  },
  {
    "title": "RiskNet: Interaction-Aware Risk Forecasting for Autonomous Driving in Long-Tail Scenarios",
    "url": "http://arxiv.org/abs/2504.15541v1",
    "arxiv_id": "2504.15541v1",
    "authors": [
      "Qichao Liu",
      "Heye Huang",
      "Shiyue Zhao",
      "Lei Shi",
      "Soyoung Ahn",
      "Xiaopeng Li"
    ],
    "published": "2025-04-22T02:36:54+00:00",
    "summary": "Ensuring the safety of autonomous vehicles (AVs) in long-tail scenarios remains a critical challenge, particularly under high uncertainty and complex multi-agent interactions. To address this, we propose RiskNet, an interaction-aware risk forecasting framework, which integrates deterministic risk modeling with probabilistic behavior prediction for comprehensive risk assessment. At its core, RiskNet employs a field-theoretic model that captures interactions among ego vehicle, surrounding agents, and infrastructure via interaction fields and force. This model supports multidimensional risk evaluation across diverse scenarios (highways, intersections, and roundabouts), and shows robustness under high-risk and long-tail settings. To capture the behavioral uncertainty, we incorporate a graph neural network (GNN)-based trajectory prediction module, which learns multi-modal future motion distributions. Coupled with the deterministic risk field, it enables dynamic, probabilistic risk inference across time, enabling proactive safety assessment under uncertainty. Evaluations on the highD, inD, and rounD datasets, spanning lane changes, turns, and complex merges, demonstrate that our method significantly outperforms traditional approaches (e.g., TTC, THW, RSS, NC Field) in terms of accuracy, responsiveness, and directional sensitivity, while maintaining strong generalization across scenarios. This framework supports real-time, scenario-adaptive risk forecasting and demonstrates strong generalization across uncertain driving environments. It offers a unified foundation for safety-critical decision-making in long-tail scenarios."
  },
  {
    "title": "Towards Resilience and Autonomy-based Approaches for Adolescents Online Safety",
    "url": "http://arxiv.org/abs/2504.15533v1",
    "arxiv_id": "2504.15533v1",
    "authors": [
      "Jinkyung Park",
      "Mamtaj Akter",
      "Naima Samreen Ali",
      "Zainab Agha",
      "Ashwaq Alsoubai",
      "Pamela Wisniewski"
    ],
    "published": "2025-04-22T02:23:48+00:00",
    "summary": "In this position paper, we discuss the paradigm shift that has emerged in the literature, suggesting to move away from restrictive and authoritarian parental mediation approaches to move toward resilient-based and privacy-preserving solutions to promote adolescents' online safety. We highlight the limitations of restrictive mediation strategies, which often induce a trade-off between teens' privacy and online safety, and call for more teen-centric frameworks that can empower teens to self-regulate while using the technology in meaningful ways. We also present an overview of empirical studies that conceptualized and examined resilience-based approaches to promoting the digital well-being of teens in a way to empower teens to be more resilient."
  },
  {
    "title": "T2VShield: Model-Agnostic Jailbreak Defense for Text-to-Video Models",
    "url": "http://arxiv.org/abs/2504.15512v1",
    "arxiv_id": "2504.15512v1",
    "authors": [
      "Siyuan Liang",
      "Jiayang Liu",
      "Jiecheng Zhai",
      "Tianmeng Fang",
      "Rongcheng Tu",
      "Aishan Liu",
      "Xiaochun Cao",
      "Dacheng Tao"
    ],
    "published": "2025-04-22T01:18:42+00:00",
    "summary": "The rapid development of generative artificial intelligence has made text to video models essential for building future multimodal world simulators. However, these models remain vulnerable to jailbreak attacks, where specially crafted prompts bypass safety mechanisms and lead to the generation of harmful or unsafe content. Such vulnerabilities undermine the reliability and security of simulation based applications. In this paper, we propose T2VShield, a comprehensive and model agnostic defense framework designed to protect text to video models from jailbreak threats. Our method systematically analyzes the input, model, and output stages to identify the limitations of existing defenses, including semantic ambiguities in prompts, difficulties in detecting malicious content in dynamic video outputs, and inflexible model centric mitigation strategies. T2VShield introduces a prompt rewriting mechanism based on reasoning and multimodal retrieval to sanitize malicious inputs, along with a multi scope detection module that captures local and global inconsistencies across time and modalities. The framework does not require access to internal model parameters and works with both open and closed source systems. Extensive experiments on five platforms show that T2VShield can reduce jailbreak success rates by up to 35 percent compared to strong baselines. We further develop a human centered audiovisual evaluation protocol to assess perceptual safety, emphasizing the importance of visual level defense in enhancing the trustworthiness of next generation multimodal simulators."
  },
  {
    "title": "Automatically Detecting Numerical Instability in Machine Learning Applications via Soft Assertions",
    "url": "http://arxiv.org/abs/2504.15507v1",
    "arxiv_id": "2504.15507v1",
    "authors": [
      "Shaila Sharmin",
      "Anwar Hossain Zahid",
      "Subhankar Bhattacharjee",
      "Chiamaka Igwilo",
      "Miryung Kim",
      "Wei Le"
    ],
    "published": "2025-04-22T00:55:33+00:00",
    "summary": "Machine learning (ML) applications have become an integral part of our lives. ML applications extensively use floating-point computation and involve very large/small numbers; thus, maintaining the numerical stability of such complex computations remains an important challenge. Numerical bugs can lead to system crashes, incorrect output, and wasted computing resources. In this paper, we introduce a novel idea, namely soft assertions (SA), to encode safety/error conditions for the places where numerical instability can occur. A soft assertion is an ML model automatically trained using the dataset obtained during unit testing of unstable functions. Given the values at the unstable function in an ML application, a soft assertion reports how to change these values in order to trigger the instability. We then use the output of soft assertions as signals to effectively mutate inputs to trigger numerical instability in ML applications. In the evaluation, we used the GRIST benchmark, a total of 79 programs, as well as 15 real-world ML applications from GitHub. We compared our tool with 5 state-of-the-art (SOTA) fuzzers. We found all the GRIST bugs and outperformed the baselines. We found 13 numerical bugs in real-world code, one of which had already been confirmed by the GitHub developers. While the baselines mostly found the bugs that report NaN and INF, our tool \\tool found numerical bugs with incorrect output. We showed one case where the Tumor Detection Model, trained on Brain MRI images, should have predicted \"tumor\", but instead, it incorrectly predicted \"no tumor\" due to the numerical bugs. Our replication package is located at https://figshare.com/s/6528d21ccd28bea94c32."
  },
  {
    "title": "Automatically Detecting Numerical Instability in Machine Learning Applications via Soft Assertions",
    "url": "http://arxiv.org/abs/2504.15507v2",
    "arxiv_id": "2504.15507v2",
    "authors": [
      "Shaila Sharmin",
      "Anwar Hossain Zahid",
      "Subhankar Bhattacharjee",
      "Chiamaka Igwilo",
      "Miryung Kim",
      "Wei Le"
    ],
    "published": "2025-04-22T00:55:33+00:00",
    "summary": "Machine learning (ML) applications have become an integral part of our lives. ML applications extensively use floating-point computation and involve very large/small numbers; thus, maintaining the numerical stability of such complex computations remains an important challenge. Numerical bugs can lead to system crashes, incorrect output, and wasted computing resources. In this paper, we introduce a novel idea, namely soft assertions (SA), to encode safety/error conditions for the places where numerical instability can occur. A soft assertion is an ML model automatically trained using the dataset obtained during unit testing of unstable functions. Given the values at the unstable function in an ML application, a soft assertion reports how to change these values in order to trigger the instability. We then use the output of soft assertions as signals to effectively mutate inputs to trigger numerical instability in ML applications. In the evaluation, we used the GRIST benchmark, a total of 79 programs, as well as 15 real-world ML applications from GitHub. We compared our tool with 5 state-of-the-art (SOTA) fuzzers. We found all the GRIST bugs and outperformed the baselines. We found 13 numerical bugs in real-world code, one of which had already been confirmed by the GitHub developers. While the baselines mostly found the bugs that report NaN and INF, our tool \\tool found numerical bugs with incorrect output. We showed one case where the Tumor Detection Model, trained on Brain MRI images, should have predicted \"tumor\", but instead, it incorrectly predicted \"no tumor\" due to the numerical bugs. Our replication package is located at https://figshare.com/s/6528d21ccd28bea94c32."
  },
  {
    "title": "Nearly Optimal Nonlinear Safe Control with BaS-SDRE",
    "url": "http://arxiv.org/abs/2504.15453v1",
    "arxiv_id": "2504.15453v1",
    "authors": [
      "Hassan Almubarak",
      "Maitham F. AL-Sunni",
      "Justin T. Dubbin",
      "Nader Sadegh",
      "John M. Dolan",
      "Evangelos A. Theodorou"
    ],
    "published": "2025-04-21T21:39:49+00:00",
    "summary": "The State-Dependent Riccati Equation (SDRE) approach has emerged as a systematic and effective means of designing nearly optimal nonlinear controllers. The Barrier States (BaS) embedding methodology was developed recently for safe multi-objective controls in which the safety condition is manifested as a state to be controlled along with other states of the system. The overall system, termed the safety embedded system, is highly nonlinear even if the original system is linear. This paper develops a nonlinear nearly optimal safe feedback control technique by combining the two strategies effectively. First, the BaS is derived in an extended linearization formulation to be subsequently used to form an extended safety embedded system. A new optimal control problem is formed thereafter, which is used to construct a safety embedded State-Dependent Riccati Equation, termed BaS-SDRE, whose solution approximates the solution of the optimal control problem's associated Hamilton-Jacobi-Bellman (HJB) equation. The BaS-SDRE is then solved online to synthesize the nearly optimal safe control. The proposed technique's efficacy is demonstrated on an unstable, constrained linear system that shows how the synthesized control reacts to nonlinearities near the unsafe region, a nonlinear flight control system with limited path angular velocity that exists due to structural and dynamic concerns, and a planar quadrotor system that navigates safely in a crowded environment."
  },
  {
    "title": "Virology Capabilities Test (VCT): A Multimodal Virology Q&A Benchmark",
    "url": "http://arxiv.org/abs/2504.16137v1",
    "arxiv_id": "2504.16137v1",
    "authors": [
      "Jasper G\u00f6tting",
      "Pedro Medeiros",
      "Jon G Sanders",
      "Nathaniel Li",
      "Long Phan",
      "Karam Elabd",
      "Lennart Justen",
      "Dan Hendrycks",
      "Seth Donoughe"
    ],
    "published": "2025-04-21T21:04:01+00:00",
    "summary": "We present the Virology Capabilities Test (VCT), a large language model (LLM) benchmark that measures the capability to troubleshoot complex virology laboratory protocols. Constructed from the inputs of dozens of PhD-level expert virologists, VCT consists of $322$ multimodal questions covering fundamental, tacit, and visual knowledge that is essential for practical work in virology laboratories. VCT is difficult: expert virologists with access to the internet score an average of $22.1\\%$ on questions specifically in their sub-areas of expertise. However, the most performant LLM, OpenAI's o3, reaches $43.8\\%$ accuracy, outperforming $94\\%$ of expert virologists even within their sub-areas of specialization. The ability to provide expert-level virology troubleshooting is inherently dual-use: it is useful for beneficial research, but it can also be misused. Therefore, the fact that publicly available models outperform virologists on VCT raises pressing governance considerations. We propose that the capability of LLMs to provide expert-level troubleshooting of dual-use virology work should be integrated into existing frameworks for handling dual-use technologies in the life sciences."
  },
  {
    "title": "Solving Multi-Agent Safe Optimal Control with Distributed Epigraph Form MARL",
    "url": "http://arxiv.org/abs/2504.15425v1",
    "arxiv_id": "2504.15425v1",
    "authors": [
      "Songyuan Zhang",
      "Oswin So",
      "Mitchell Black",
      "Zachary Serlin",
      "Chuchu Fan"
    ],
    "published": "2025-04-21T20:34:55+00:00",
    "summary": "Tasks for multi-robot systems often require the robots to collaborate and complete a team goal while maintaining safety. This problem is usually formalized as a constrained Markov decision process (CMDP), which targets minimizing a global cost and bringing the mean of constraint violation below a user-defined threshold. Inspired by real-world robotic applications, we define safety as zero constraint violation. While many safe multi-agent reinforcement learning (MARL) algorithms have been proposed to solve CMDPs, these algorithms suffer from unstable training in this setting. To tackle this, we use the epigraph form for constrained optimization to improve training stability and prove that the centralized epigraph form problem can be solved in a distributed fashion by each agent. This results in a novel centralized training distributed execution MARL algorithm named Def-MARL. Simulation experiments on 8 different tasks across 2 different simulators show that Def-MARL achieves the best overall performance, satisfies safety constraints, and maintains stable training. Real-world hardware experiments on Crazyflie quadcopters demonstrate the ability of Def-MARL to safely coordinate agents to complete complex collaborative tasks compared to other methods."
  },
  {
    "title": "Safety Embedded Adaptive Control Using Barrier States",
    "url": "http://arxiv.org/abs/2504.15423v1",
    "arxiv_id": "2504.15423v1",
    "authors": [
      "Maitham F. AL-Sunni",
      "Hassan Almubarak",
      "John M. Dolan"
    ],
    "published": "2025-04-21T20:29:38+00:00",
    "summary": "In this work, we explore the application of barrier states (BaS) in the realm of safe nonlinear adaptive control. Our proposed framework derives barrier states for systems with parametric uncertainty, which are augmented into the uncertain dynamical model. We employ an adaptive nonlinear control strategy based on a control Lyapunov functions approach to design a stabilizing controller for the augmented system. The developed theory shows that the controller ensures safe control actions for the original system while meeting specified performance objectives. We validate the effectiveness of our approach through simulations on diverse systems, including a planar quadrotor subject to unknown drag forces and an adaptive cruise control system, for which we provide comparisons with existing methodologies."
  },
  {
    "title": "$k$-Inductive and Interpolation-Inspired Barrier Certificates for Stochastic Dynamical Systems",
    "url": "http://arxiv.org/abs/2504.15412v1",
    "arxiv_id": "2504.15412v1",
    "authors": [
      "Mohammed Adib Oumer",
      "Vishnu Murali",
      "Majid Zamani"
    ],
    "published": "2025-04-21T19:41:43+00:00",
    "summary": "We introduce two notions of barrier certificates that use multiple functions to provide a lower bound on the probabilistic satisfaction of safety for stochastic dynamical systems. A barrier certificate for a stochastic dynamical system acts as a nonnegative supermartingale, and provides a lower bound on the probability that the system is safe. The promise of such certificates is that their search can be effectively automated. Typically, one may use optimization or SMT solvers to find such barrier certificates of a given fixed template. When such approaches fail, a typical approach is to instead change the template. We propose an alternative approach that we dub interpolation-inspired barrier certificates. An interpolation-inspired barrier certificate consists of a set of functions that jointly provide a lower bound on the probability of satisfying safety. We show how one may find such certificates of a fixed template, even when we fail to find standard barrier certificates of the same template. However, we note that such certificates still need to ensure a supermartingale guarantee for one function in the set. To address this challenge, we consider the use of $k$-induction with these interpolation-inspired certificates. The recent use of $k$-induction in barrier certificates allows one to relax the supermartingale requirement at every time step to a combination of a supermartingale requirement every $k$ steps and a $c$-martingale requirement for the intermediate steps. We provide a generic formulation of a barrier certificate that we dub $k$-inductive interpolation-inspired barrier certificate. The formulation allows for several combinations of interpolation and $k$-induction for barrier certificate. We present two examples among the possible combinations. We finally present sum-of-squares programming to synthesize this set of functions and demonstrate their utility in case studies."
  },
  {
    "title": "Multimodal Large Language Models for Enhanced Traffic Safety: A Comprehensive Review and Future Trends",
    "url": "http://arxiv.org/abs/2504.16134v1",
    "arxiv_id": "2504.16134v1",
    "authors": [
      "Mohammad Abu Tami",
      "Mohammed Elhenawy",
      "Huthaifa I. Ashqar"
    ],
    "published": "2025-04-21T18:48:35+00:00",
    "summary": "Traffic safety remains a critical global challenge, with traditional Advanced Driver-Assistance Systems (ADAS) often struggling in dynamic real-world scenarios due to fragmented sensor processing and susceptibility to adversarial conditions. This paper reviews the transformative potential of Multimodal Large Language Models (MLLMs) in addressing these limitations by integrating cross-modal data such as visual, spatial, and environmental inputs to enable holistic scene understanding. Through a comprehensive analysis of MLLM-based approaches, we highlight their capabilities in enhancing perception, decision-making, and adversarial robustness, while also examining the role of key datasets (e.g., KITTI, DRAMA, ML4RoadSafety) in advancing research. Furthermore, we outline future directions, including real-time edge deployment, causality-driven reasoning, and human-AI collaboration. By positioning MLLMs as a cornerstone for next-generation traffic safety systems, this review underscores their potential to revolutionize the field, offering scalable, context-aware solutions that proactively mitigate risks and improve overall road safety."
  },
  {
    "title": "A Conceptual Framework for AI-based Decision Systems in Critical Infrastructures",
    "url": "http://arxiv.org/abs/2504.16133v1",
    "arxiv_id": "2504.16133v1",
    "authors": [
      "Milad Leyli-abadi",
      "Ricardo J. Bessa",
      "Jan Viebahn",
      "Daniel Boos",
      "Clark Borst",
      "Alberto Castagna",
      "Ricardo Chavarriaga",
      "Mohamed Hassouna",
      "Bruno Lemetayer",
      "Giulia Leto",
      "Antoine Marot",
      "Maroua Meddeb",
      "Manuel Meyer",
      "Viola Schiaffonati",
      "Manuel Schneider",
      "Toni Waefler"
    ],
    "published": "2025-04-21T18:38:26+00:00",
    "summary": "The interaction between humans and AI in safety-critical systems presents a unique set of challenges that remain partially addressed by existing frameworks. These challenges stem from the complex interplay of requirements for transparency, trust, and explainability, coupled with the necessity for robust and safe decision-making. A framework that holistically integrates human and AI capabilities while addressing these concerns is notably required, bridging the critical gaps in designing, deploying, and maintaining safe and effective systems. This paper proposes a holistic conceptual framework for critical infrastructures by adopting an interdisciplinary approach. It integrates traditionally distinct fields such as mathematics, decision theory, computer science, philosophy, psychology, and cognitive engineering and draws on specialized engineering domains, particularly energy, mobility, and aeronautics. The flexibility in its adoption is also demonstrated through its instantiation on an already existing framework."
  },
  {
    "title": "Interpretable Locomotion Prediction in Construction Using a Memory-Driven LLM Agent With Chain-of-Thought Reasoning",
    "url": "http://arxiv.org/abs/2504.15263v1",
    "arxiv_id": "2504.15263v1",
    "authors": [
      "Ehsan Ahmadi",
      "Chao Wang"
    ],
    "published": "2025-04-21T17:45:21+00:00",
    "summary": "Construction tasks are inherently unpredictable, with dynamic environments and safety-critical demands posing significant risks to workers. Exoskeletons offer potential assistance but falter without accurate intent recognition across diverse locomotion modes. This paper presents a locomotion prediction agent leveraging Large Language Models (LLMs) augmented with memory systems, aimed at improving exoskeleton assistance in such settings. Using multimodal inputs - spoken commands and visual data from smart glasses - the agent integrates a Perception Module, Short-Term Memory (STM), Long-Term Memory (LTM), and Refinement Module to predict locomotion modes effectively. Evaluation reveals a baseline weighted F1-score of 0.73 without memory, rising to 0.81 with STM, and reaching 0.90 with both STM and LTM, excelling with vague and safety-critical commands. Calibration metrics, including a Brier Score drop from 0.244 to 0.090 and ECE from 0.222 to 0.044, affirm improved reliability. This framework supports safer, high-level human-exoskeleton collaboration, with promise for adaptive assistive systems in dynamic industries."
  },
  {
    "title": "Leveraging Language Models for Automated Patient Record Linkage",
    "url": "http://arxiv.org/abs/2504.15261v1",
    "arxiv_id": "2504.15261v1",
    "authors": [
      "Mohammad Beheshti",
      "Lovedeep Gondara",
      "Iris Zachary"
    ],
    "published": "2025-04-21T17:41:15+00:00",
    "summary": "Objective: Healthcare data fragmentation presents a major challenge for linking patient data, necessitating robust record linkage to integrate patient records from diverse sources. This study investigates the feasibility of leveraging language models for automated patient record linkage, focusing on two key tasks: blocking and matching. Materials and Methods: We utilized real-world healthcare data from the Missouri Cancer Registry and Research Center, linking patient records from two independent sources using probabilistic linkage as a baseline. A transformer-based model, RoBERTa, was fine-tuned for blocking using sentence embeddings. For matching, several language models were experimented under fine-tuned and zero-shot settings, assessing their performance against ground truth labels. Results: The fine-tuned blocking model achieved a 92% reduction in the number of candidate pairs while maintaining near-perfect recall. In the matching task, fine-tuned Mistral-7B achieved the best performance with only 6 incorrect predictions. Among zero-shot models, Mistral-Small-24B performed best, with a total of 55 incorrect predictions. Discussion: Fine-tuned language models achieved strong performance in patient record blocking and matching with minimal errors. However, they remain less accurate and efficient than a hybrid rule-based and probabilistic approach for blocking. Additionally, reasoning models like DeepSeek-R1 are impractical for large-scale record linkage due to high computational costs. Conclusion: This study highlights the potential of language models for automating patient record linkage, offering improved efficiency by eliminating the manual efforts required to perform patient record linkage. Overall, language models offer a scalable solution that can enhance data integration, reduce manual effort, and support disease surveillance and research."
  },
  {
    "title": "FlowReasoner: Reinforcing Query-Level Meta-Agents",
    "url": "http://arxiv.org/abs/2504.15257v1",
    "arxiv_id": "2504.15257v1",
    "authors": [
      "Hongcheng Gao",
      "Yue Liu",
      "Yufei He",
      "Longxu Dou",
      "Chao Du",
      "Zhijie Deng",
      "Bryan Hooi",
      "Min Lin",
      "Tianyu Pang"
    ],
    "published": "2025-04-21T17:35:42+00:00",
    "summary": "This paper proposes a query-level meta-agent named FlowReasoner to automate the design of query-level multi-agent systems, i.e., one system per user query. Our core idea is to incentivize a reasoning-based meta-agent via external execution feedback. Concretely, by distilling DeepSeek R1, we first endow the basic reasoning ability regarding the generation of multi-agent systems to FlowReasoner. Then, we further enhance it via reinforcement learning (RL) with external execution feedback. A multi-purpose reward is designed to guide the RL training from aspects of performance, complexity, and efficiency. In this manner, FlowReasoner is enabled to generate a personalized multi-agent system for each user query via deliberative reasoning. Experiments on both engineering and competition code benchmarks demonstrate the superiority of FlowReasoner. Remarkably, it surpasses o1-mini by 10.52% accuracy across three benchmarks. The code is available at https://github.com/sail-sg/FlowReasoner."
  },
  {
    "title": "CRUST-Bench: A Comprehensive Benchmark for C-to-safe-Rust Transpilation",
    "url": "http://arxiv.org/abs/2504.15254v1",
    "arxiv_id": "2504.15254v1",
    "authors": [
      "Anirudh Khatry",
      "Robert Zhang",
      "Jia Pan",
      "Ziteng Wang",
      "Qiaochu Chen",
      "Greg Durrett",
      "Isil Dillig"
    ],
    "published": "2025-04-21T17:33:33+00:00",
    "summary": "C-to-Rust transpilation is essential for modernizing legacy C code while enhancing safety and interoperability with modern Rust ecosystems. However, no dataset currently exists for evaluating whether a system can transpile C into safe Rust that passes a set of test cases. We introduce CRUST-Bench, a dataset of 100 C repositories, each paired with manually-written interfaces in safe Rust as well as test cases that can be used to validate correctness of the transpilation. By considering entire repositories rather than isolated functions, CRUST-Bench captures the challenges of translating complex projects with dependencies across multiple files. The provided Rust interfaces provide explicit specifications that ensure adherence to idiomatic, memory-safe Rust patterns, while the accompanying test cases enforce functional correctness. We evaluate state-of-the-art large language models (LLMs) on this task and find that safe and idiomatic Rust generation is still a challenging problem for various state-of-the-art methods and techniques. We also provide insights into the errors LLMs usually make in transpiling code from C to safe Rust. The best performing model, OpenAI o1, is able to solve only 15 tasks in a single-shot setting. Improvements on CRUST-Bench would lead to improved transpilation systems that can reason about complex scenarios and help in migrating legacy codebases from C into languages like Rust that ensure memory safety. You can find the dataset and code at https://github.com/anirudhkhatry/CRUST-bench."
  },
  {
    "title": "MR. Guard: Multilingual Reasoning Guardrail using Curriculum Learning",
    "url": "http://arxiv.org/abs/2504.15241v1",
    "arxiv_id": "2504.15241v1",
    "authors": [
      "Yahan Yang",
      "Soham Dan",
      "Shuo Li",
      "Dan Roth",
      "Insup Lee"
    ],
    "published": "2025-04-21T17:15:06+00:00",
    "summary": "Large Language Models (LLMs) are susceptible to adversarial attacks such as jailbreaking, which can elicit harmful or unsafe behaviors. This vulnerability is exacerbated in multilingual setting, where multilingual safety-aligned data are often limited. Thus, developing a guardrail capable of detecting and filtering unsafe content across diverse languages is critical for deploying LLMs in real-world applications. In this work, we propose an approach to build a multilingual guardrail with reasoning. Our method consists of: (1) synthetic multilingual data generation incorporating culturally and linguistically nuanced variants, (2) supervised fine-tuning, and (3) a curriculum-guided Group Relative Policy Optimization (GRPO) framework that further improves performance. Experimental results demonstrate that our multilingual guardrail consistently outperforms recent baselines across both in-domain and out-of-domain languages. The multilingual reasoning capability of our guardrail enables it to generate multilingual explanations, which are particularly useful for understanding language-specific risks and ambiguities in multilingual content moderation."
  },
  {
    "title": "A Genetic Fuzzy-Enabled Framework on Robotic Manipulation for In-Space Servicing",
    "url": "http://arxiv.org/abs/2504.15226v1",
    "arxiv_id": "2504.15226v1",
    "authors": [
      "Nathan Steffen",
      "Wilhelm Louw",
      "Nicholas Ernest",
      "Timothy Arnett",
      "Kelly Cohen"
    ],
    "published": "2025-04-21T16:57:56+00:00",
    "summary": "Automation of robotic systems for servicing in cislunar space is becoming extremely important as the number of satellites in orbit increases. Safety is critical in performing satellite maintenance, so the control techniques utilized must be trusted in addition to being highly efficient. In this work, Genetic Fuzzy Trees are combined with the widely used LQR control scheme via Thales' TrUE AI Toolkit to create a trusted and efficient controller for a two-degree-of-freedom planar robotic manipulator that would theoretically be used to perform satellite maintenance. It was found that Genetic Fuzzy-LQR is 18.5% more performant than optimal LQR on average, and that it is incredibly robust to uncertainty."
  },
  {
    "title": "Existing Industry Practice for the EU AI Act's General-Purpose AI Code of Practice Safety and Security Measures",
    "url": "http://arxiv.org/abs/2504.15181v1",
    "arxiv_id": "2504.15181v1",
    "authors": [
      "Lily Stelling",
      "Mick Yang",
      "Rokas Gipi\u0161kis",
      "Leon Staufer",
      "Ze Shen Chin",
      "Sim\u00e9on Campos",
      "Michael Chen"
    ],
    "published": "2025-04-21T15:44:01+00:00",
    "summary": "This report provides a detailed comparison between the measures proposed in the EU AI Act's General-Purpose AI (GPAI) Code of Practice (Third Draft) and current practices adopted by leading AI companies. As the EU moves toward enforcing binding obligations for GPAI model providers, the Code of Practice will be key to bridging legal requirements with concrete technical commitments. Our analysis focuses on the draft's Safety and Security section which is only relevant for the providers of the most advanced models (Commitments II.1-II.16) and excerpts from current public-facing documents quotes that are relevant to each individual measure.   We systematically reviewed different document types - including companies' frontier safety frameworks and model cards - from over a dozen companies, including OpenAI, Anthropic, Google DeepMind, Microsoft, Meta, Amazon, and others. This report is not meant to be an indication of legal compliance nor does it take any prescriptive viewpoint about the Code of Practice or companies' policies. Instead, it aims to inform the ongoing dialogue between regulators and GPAI model providers by surfacing evidence of precedent."
  },
  {
    "title": "C2RUST-BENCH: A Minimized, Representative Dataset for C-to-Rust Transpilation Evaluation",
    "url": "http://arxiv.org/abs/2504.15144v1",
    "arxiv_id": "2504.15144v1",
    "authors": [
      "Melih Sirlanci",
      "Carter Yagemann",
      "Zhiqiang Lin"
    ],
    "published": "2025-04-21T14:48:45+00:00",
    "summary": "Despite the effort in vulnerability detection over the last two decades, memory safety vulnerabilities continue to be a critical problem. Recent reports suggest that the key solution is to migrate to memory-safe languages. To this end, C-to-Rust transpilation becomes popular to resolve memory-safety issues in C programs. Recent works propose C-to-Rust transpilation frameworks; however, a comprehensive evaluation dataset is missing. Although one solution is to put together a large enough dataset, this increases the analysis time in automated frameworks as well as in manual efforts for some cases. In this work, we build a method to select functions from a large set to construct a minimized yet representative dataset to evaluate the C-to-Rust transpilation. We propose C2RUST-BENCH that contains 2,905 functions, which are representative of C-to-Rust transpilation, selected from 15,503 functions of real-world programs."
  },
  {
    "title": "EasyEdit2: An Easy-to-use Steering Framework for Editing Large Language Models",
    "url": "http://arxiv.org/abs/2504.15133v1",
    "arxiv_id": "2504.15133v1",
    "authors": [
      "Ziwen Xu",
      "Shuxun Wang",
      "Kewei Xu",
      "Haoming Xu",
      "Mengru Wang",
      "Xinle Deng",
      "Yunzhi Yao",
      "Guozhou Zheng",
      "Huajun Chen",
      "Ningyu Zhang"
    ],
    "published": "2025-04-21T14:33:55+00:00",
    "summary": "In this paper, we introduce EasyEdit2, a framework designed to enable plug-and-play adjustability for controlling Large Language Model (LLM) behaviors. EasyEdit2 supports a wide range of test-time interventions, including safety, sentiment, personality, reasoning patterns, factuality, and language features. Unlike its predecessor, EasyEdit2 features a new architecture specifically designed for seamless model steering. It comprises key modules such as the steering vector generator and the steering vector applier, which enable automatic generation and application of steering vectors to influence the model's behavior without modifying its parameters. One of the main advantages of EasyEdit2 is its ease of use-users do not need extensive technical knowledge. With just a single example, they can effectively guide and adjust the model's responses, making precise control both accessible and efficient. Empirically, we report model steering performance across different LLMs, demonstrating the effectiveness of these techniques. We have released the source code on GitHub at https://github.com/zjunlp/EasyEdit along with a demonstration notebook. In addition, we provide a demo video at https://zjunlp.github.io/project/EasyEdit2/video for a quick introduction."
  },
  {
    "title": "Safety Co-Option and Compromised National Security: The Self-Fulfilling Prophecy of Weakened AI Risk Thresholds",
    "url": "http://arxiv.org/abs/2504.15088v1",
    "arxiv_id": "2504.15088v1",
    "authors": [
      "Heidy Khlaaf",
      "Sarah Myers West"
    ],
    "published": "2025-04-21T13:20:56+00:00",
    "summary": "Risk thresholds provide a measure of the level of risk exposure that a society or individual is willing to withstand, ultimately shaping how we determine the safety of technological systems. Against the backdrop of the Cold War, the first risk analyses, such as those devised for nuclear systems, cemented societally accepted risk thresholds against which safety-critical and defense systems are now evaluated. But today, the appropriate risk tolerances for AI systems have yet to be agreed on by global governing efforts, despite the need for democratic deliberation regarding the acceptable levels of harm to human life. Absent such AI risk thresholds, AI technologists-primarily industry labs, as well as \"AI safety\" focused organizations-have instead advocated for risk tolerances skewed by a purported AI arms race and speculative \"existential\" risks, taking over the arbitration of risk determinations with life-or-death consequences, subverting democratic processes.   In this paper, we demonstrate how such approaches have allowed AI technologists to engage in \"safety revisionism,\" substituting traditional safety methods and terminology with ill-defined alternatives that vie for the accelerated adoption of military AI uses at the cost of lowered safety and security thresholds. We explore how the current trajectory for AI risk determination and evaluation for foundation model use within national security is poised for a race to the bottom, to the detriment of the US's national security interests. Safety-critical and defense systems must comply with assurance frameworks that are aligned with established risk thresholds, and foundation models are no exception. As such, development of evaluation frameworks for AI-based military systems must preserve the safety and security of US critical and defense infrastructure, and remain in alignment with international humanitarian law."
  },
  {
    "title": "RainbowPlus: Enhancing Adversarial Prompt Generation via Evolutionary Quality-Diversity Search",
    "url": "http://arxiv.org/abs/2504.15047v1",
    "arxiv_id": "2504.15047v1",
    "authors": [
      "Quy-Anh Dang",
      "Chris Ngo",
      "Truong-Son Hy"
    ],
    "published": "2025-04-21T12:04:57+00:00",
    "summary": "Large Language Models (LLMs) exhibit remarkable capabilities but are susceptible to adversarial prompts that exploit vulnerabilities to produce unsafe or biased outputs. Existing red-teaming methods often face scalability challenges, resource-intensive requirements, or limited diversity in attack strategies. We propose RainbowPlus, a novel red-teaming framework rooted in evolutionary computation, enhancing adversarial prompt generation through an adaptive quality-diversity (QD) search that extends classical evolutionary algorithms like MAP-Elites with innovations tailored for language models. By employing a multi-element archive to store diverse high-quality prompts and a comprehensive fitness function to evaluate multiple prompts concurrently, RainbowPlus overcomes the constraints of single-prompt archives and pairwise comparisons in prior QD methods like Rainbow Teaming. Experiments comparing RainbowPlus to QD methods across six benchmark datasets and four open-source LLMs demonstrate superior attack success rate (ASR) and diversity (Diverse-Score $\\approx 0.84$), generating up to 100 times more unique prompts (e.g., 10,418 vs. 100 for Ministral-8B-Instruct-2410). Against nine state-of-the-art methods on the HarmBench dataset with twelve LLMs (ten open-source, two closed-source), RainbowPlus achieves an average ASR of 81.1%, surpassing AutoDAN-Turbo by 3.9%, and is 9 times faster (1.45 vs. 13.50 hours). Our open-source implementation fosters further advancements in LLM safety, offering a scalable tool for vulnerability assessment. Code and resources are publicly available at https://github.com/knoveleng/rainbowplus, supporting reproducibility and future research in LLM red-teaming."
  },
  {
    "title": "aiXamine: LLM Safety and Security Simplified",
    "url": "http://arxiv.org/abs/2504.14985v1",
    "arxiv_id": "2504.14985v1",
    "authors": [
      "Fatih Deniz",
      "Dorde Popovic",
      "Yazan Boshmaf",
      "Euisuh Jeong",
      "Minhaj Ahmad",
      "Sanjay Chawla",
      "Issa Khalil"
    ],
    "published": "2025-04-21T09:26:05+00:00",
    "summary": "Evaluating Large Language Models (LLMs) for safety and security remains a complex task, often requiring users to navigate a fragmented landscape of ad hoc benchmarks, datasets, metrics, and reporting formats. To address this challenge, we present aiXamine, a comprehensive black-box evaluation platform for LLM safety and security. aiXamine integrates over 40 tests (i.e., benchmarks) organized into eight key services targeting specific dimensions of safety and security: adversarial robustness, code security, fairness and bias, hallucination, model and data privacy, out-of-distribution (OOD) robustness, over-refusal, and safety alignment. The platform aggregates the evaluation results into a single detailed report per model, providing a detailed breakdown of model performance, test examples, and rich visualizations. We used aiXamine to assess over 50 publicly available and proprietary LLMs, conducting over 2K examinations. Our findings reveal notable vulnerabilities in leading models, including susceptibility to adversarial attacks in OpenAI's GPT-4o, biased outputs in xAI's Grok-3, and privacy weaknesses in Google's Gemini 2.0. Additionally, we observe that open-source models can match or exceed proprietary models in specific services such as safety alignment, fairness and bias, and OOD robustness. Finally, we identify trade-offs between distillation strategies, model size, training methods, and architectural choices."
  },
  {
    "title": "Distributed Time-Varying Gaussian Regression via Kalman Filtering",
    "url": "http://arxiv.org/abs/2504.14900v1",
    "arxiv_id": "2504.14900v1",
    "authors": [
      "Nicola Taddei",
      "Riccardo Maggioni",
      "Jaap Eising",
      "Giulia De Pasquale",
      "Florian Dorfler"
    ],
    "published": "2025-04-21T07:12:05+00:00",
    "summary": "We consider the problem of learning time-varying functions in a distributed fashion, where agents collect local information to collaboratively achieve a shared estimate. This task is particularly relevant in control applications, whenever real-time and robust estimation of dynamic cost/reward functions in safety critical settings has to be performed. In this paper, we,adopt a finite-dimensional approximation of a Gaussian Process, corresponding to a Bayesian linear regression in an appropriate feature space, and propose a new algorithm, DistKP, to track the time-varying coefficients via a distributed Kalman filter. The proposed method works for arbitrary kernels and under weaker assumptions on the time-evolution of the function to learn compared to the literature. We validate our results using a simulation example in which a fleet of Unmanned Aerial Vehicles (UAVs) learns a dynamically changing wind field."
  },
  {
    "title": "Retrieval Augmented Generation Evaluation in the Era of Large Language Models: A Comprehensive Survey",
    "url": "http://arxiv.org/abs/2504.14891v1",
    "arxiv_id": "2504.14891v1",
    "authors": [
      "Aoran Gan",
      "Hao Yu",
      "Kai Zhang",
      "Qi Liu",
      "Wenyu Yan",
      "Zhenya Huang",
      "Shiwei Tong",
      "Guoping Hu"
    ],
    "published": "2025-04-21T06:39:47+00:00",
    "summary": "Recent advancements in Retrieval-Augmented Generation (RAG) have revolutionized natural language processing by integrating Large Language Models (LLMs) with external information retrieval, enabling accurate, up-to-date, and verifiable text generation across diverse applications. However, evaluating RAG systems presents unique challenges due to their hybrid architecture that combines retrieval and generation components, as well as their dependence on dynamic knowledge sources in the LLM era. In response, this paper provides a comprehensive survey of RAG evaluation methods and frameworks, systematically reviewing traditional and emerging evaluation approaches, for system performance, factual accuracy, safety, and computational efficiency in the LLM era. We also compile and categorize the RAG-specific datasets and evaluation frameworks, conducting a meta-analysis of evaluation practices in high-impact RAG research. To the best of our knowledge, this work represents the most comprehensive survey for RAG evaluation, bridging traditional and LLM-driven methods, and serves as a critical resource for advancing RAG development."
  },
  {
    "title": "ReCraft: Self-Contained Split, Merge, and Membership Change of Raft Protocol",
    "url": "http://arxiv.org/abs/2504.14802v1",
    "arxiv_id": "2504.14802v1",
    "authors": [
      "Kezhi Xiong",
      "Soonwon Moon",
      "Joshua Kang",
      "Bryant Curto",
      "Jieung Kim",
      "Ji-Yong Shin"
    ],
    "published": "2025-04-21T02:05:06+00:00",
    "summary": "Designing reconfiguration schemes for consensus protocols is challenging because subtle corner cases during reconfiguration could invalidate the correctness of the protocol. Thus, most systems that embed consensus protocols conservatively implement the reconfiguration and refrain from developing an efficient scheme. Existing implementations often stop the entire system during reconfiguration and rely on a centralized coordinator, which can become a single point of failure. We present ReCraft, a novel reconfiguration protocol for Raft, which supports multi- and single-cluster-level reconfigurations. ReCraft does not rely on external coordinators and blocks minimally. ReCraft enables the sharding of Raft clusters with split and merge reconfigurations and adds a membership change scheme that improves Raft. We prove the safety and liveness of ReCraft and demonstrate its efficiency through implementations in etcd."
  },
  {
    "title": "Safe Autonomous Environmental Contact for Soft Robots using Control Barrier Functions",
    "url": "http://arxiv.org/abs/2504.14755v1",
    "arxiv_id": "2504.14755v1",
    "authors": [
      "Akua K. Dickson",
      "Juan C. Pacheco Garcia",
      "Meredith L. Anderson",
      "Ran Jing",
      "Sarah Alizadeh-Shabdiz",
      "Audrey X. Wang",
      "Charles DeLorey",
      "Zach J. Patterson",
      "Andrew P. Sabelhaus"
    ],
    "published": "2025-04-20T22:31:55+00:00",
    "summary": "Robots built from soft materials will inherently apply lower environmental forces than their rigid counterparts, and therefore may be more suitable in sensitive settings with unintended contact. However, these robots' applied forces result from both their design and their control system in closed-loop, and therefore, ensuring bounds on these forces requires controller synthesis for safety as well. This article introduces the first feedback controller for a soft manipulator that formally meets a safety specification with respect to environmental contact. In our proof-of-concept setting, the robot's environment has known geometry and is deformable with a known elastic modulus. Our approach maps a bound on applied forces to a safe set of positions of the robot's tip via predicted deformations of the environment. Then, a quadratic program with Control Barrier Functions in its constraints is used to supervise a nominal feedback signal, verifiably maintaining the robot's tip within this safe set. Hardware experiments on a multi-segment soft pneumatic robot demonstrate that the proposed framework successfully constrains its environmental contact forces. This framework represents a fundamental shift in perspective on control and safety for soft robots, defining and implementing a formally verifiable logic specification on their pose and contact forces."
  },
  {
    "title": "Adaptive Field Effect Planner for Safe Interactive Autonomous Driving on Curved Roads",
    "url": "http://arxiv.org/abs/2504.14747v1",
    "arxiv_id": "2504.14747v1",
    "authors": [
      "Qinghao Li",
      "Zhen Tian",
      "Xiaodan Wang",
      "Jinming Yang",
      "Zhihao Lin"
    ],
    "published": "2025-04-20T21:41:19+00:00",
    "summary": "Autonomous driving has garnered significant attention for its potential to improve safety, traffic efficiency, and user convenience. However, the dynamic and complex nature of interactive driving poses significant challenges, including the need to navigate non-linear road geometries, handle dynamic obstacles, and meet stringent safety and comfort requirements. Traditional approaches, such as artificial potential fields (APF), often fall short in addressing these complexities independently, necessitating the development of integrated and adaptive frameworks. This paper presents a novel approach to autonomous vehicle navigation that integrates artificial potential fields, Frenet coordinates, and improved particle swarm optimization (IPSO). A dynamic risk field, adapted from traditional APF, is proposed to ensure interactive safety by quantifying risks and dynamically adjusting lane-changing intentions based on surrounding vehicle behavior. Frenet coordinates are utilized to simplify trajectory planning on non-straight roads, while an enhanced quintic polynomial trajectory generator ensures smooth and comfortable path transitions. Additionally, an IPSO algorithm optimizes trajectory selection in real time, balancing safety and user comfort within a feasible input range. The proposed framework is validated through extensive simulations and real-world scenarios, demonstrating its ability to navigate complex traffic environments, maintain safety margins, and generate smooth, dynamically feasible trajectories."
  },
  {
    "title": "Efficient and Safe Planner for Automated Driving on Ramps Considering Unsatisfication",
    "url": "http://arxiv.org/abs/2504.15320v1",
    "arxiv_id": "2504.15320v1",
    "authors": [
      "Qinghao Li",
      "Zhen Tian",
      "Xiaodan Wang",
      "Jinming Yang",
      "Zhihao Lin"
    ],
    "published": "2025-04-20T21:39:51+00:00",
    "summary": "Automated driving on ramps presents significant challenges due to the need to balance both safety and efficiency during lane changes. This paper proposes an integrated planner for automated vehicles (AVs) on ramps, utilizing an unsatisfactory level metric for efficiency and arrow-cluster-based sampling for safety. The planner identifies optimal times for the AV to change lanes, taking into account the vehicle's velocity as a key factor in efficiency. Additionally, the integrated planner employs arrow-cluster-based sampling to evaluate collision risks and select an optimal lane-changing curve. Extensive simulations were conducted in a ramp scenario to verify the planner's efficient and safe performance. The results demonstrate that the proposed planner can effectively select an appropriate lane-changing time point and a safe lane-changing curve for AVs, without incurring any collisions during the maneuver."
  },
  {
    "title": "Can We Ignore Labels In Out of Distribution Detection?",
    "url": "http://arxiv.org/abs/2504.14704v1",
    "arxiv_id": "2504.14704v1",
    "authors": [
      "Hong Yang",
      "Qi Yu",
      "Travis Desel"
    ],
    "published": "2025-04-20T18:37:51+00:00",
    "summary": "Out-of-distribution (OOD) detection methods have recently become more prominent, serving as a core element in safety-critical autonomous systems. One major purpose of OOD detection is to reject invalid inputs that could lead to unpredictable errors and compromise safety. Due to the cost of labeled data, recent works have investigated the feasibility of self-supervised learning (SSL) OOD detection, unlabeled OOD detection, and zero shot OOD detection. In this work, we identify a set of conditions for a theoretical guarantee of failure in unlabeled OOD detection algorithms from an information-theoretic perspective. These conditions are present in all OOD tasks dealing with real-world data: I) we provide theoretical proof of unlabeled OOD detection failure when there exists zero mutual information between the learning objective and the in-distribution labels, a.k.a. 'label blindness', II) we define a new OOD task - Adjacent OOD detection - that tests for label blindness and accounts for a previously ignored safety gap in all OOD detection benchmarks, and III) we perform experiments demonstrating that existing unlabeled OOD methods fail under conditions suggested by our label blindness theory and analyze the implications for future research in unlabeled OOD methods."
  },
  {
    "title": "A Byzantine Fault Tolerance Approach towards AI Safety",
    "url": "http://arxiv.org/abs/2504.14668v1",
    "arxiv_id": "2504.14668v1",
    "authors": [
      "John deVadoss",
      "Matthias Artzt"
    ],
    "published": "2025-04-20T16:18:06+00:00",
    "summary": "Ensuring that an AI system behaves reliably and as intended, especially in the presence of unexpected faults or adversarial conditions, is a complex challenge. Inspired by the field of Byzantine Fault Tolerance (BFT) from distributed computing, we explore a fault tolerance architecture for AI safety. By drawing an analogy between unreliable, corrupt, misbehaving or malicious AI artifacts and Byzantine nodes in a distributed system, we propose an architecture that leverages consensus mechanisms to enhance AI safety and reliability."
  },
  {
    "title": "BLACKOUT: Data-Oblivious Computation with Blinded Capabilities",
    "url": "http://arxiv.org/abs/2504.14654v1",
    "arxiv_id": "2504.14654v1",
    "authors": [
      "Hossam ElAtali",
      "Merve G\u00fclmez",
      "Thomas Nyman",
      "N. Asokan"
    ],
    "published": "2025-04-20T15:25:59+00:00",
    "summary": "Lack of memory-safety and exposure to side channels are two prominent, persistent challenges for the secure implementation of software. Memory-safe programming languages promise to significantly reduce the prevalence of memory-safety bugs, but make it more difficult to implement side-channel-resistant code. We aim to address both memory-safety and side-channel resistance by augmenting memory-safe hardware with the ability for data-oblivious programming. We describe an extension to the CHERI capability architecture to provide blinded capabilities that allow data-oblivious computation to be carried out by userspace tasks. We also present BLACKOUT, our realization of blinded capabilities on a FPGA softcore based on the speculative out-of-order CHERI-Toooba processor and extend the CHERI-enabled Clang/LLVM compiler and the CheriBSD operating system with support for blinded capabilities. BLACKOUT makes writing side-channel-resistant code easier by making non-data-oblivious operations via blinded capabilities explicitly fault. Through rigorous evaluation we show that BLACKOUT ensures memory operated on through blinded capabilities is securely allocated, used, and reclaimed and demonstrate that, in benchmarks comparable to those used by previous work, BLACKOUT imposes only a small performance degradation (1.5% geometric mean) compared to the baseline CHERI-Toooba processor."
  },
  {
    "title": "A Framework for Benchmarking and Aligning Task-Planning Safety in LLM-Based Embodied Agents",
    "url": "http://arxiv.org/abs/2504.14650v1",
    "arxiv_id": "2504.14650v1",
    "authors": [
      "Yuting Huang",
      "Leilei Ding",
      "Zhipeng Tang",
      "Tianfu Wang",
      "Xinrui Lin",
      "Wuyang Zhang",
      "Mingxiao Ma",
      "Yanyong Zhang"
    ],
    "published": "2025-04-20T15:12:14+00:00",
    "summary": "Large Language Models (LLMs) exhibit substantial promise in enhancing task-planning capabilities within embodied agents due to their advanced reasoning and comprehension. However, the systemic safety of these agents remains an underexplored frontier. In this study, we present Safe-BeAl, an integrated framework for the measurement (SafePlan-Bench) and alignment (Safe-Align) of LLM-based embodied agents' behaviors. SafePlan-Bench establishes a comprehensive benchmark for evaluating task-planning safety, encompassing 2,027 daily tasks and corresponding environments distributed across 8 distinct hazard categories (e.g., Fire Hazard). Our empirical analysis reveals that even in the absence of adversarial inputs or malicious intent, LLM-based agents can exhibit unsafe behaviors. To mitigate these hazards, we propose Safe-Align, a method designed to integrate physical-world safety knowledge into LLM-based embodied agents while maintaining task-specific performance. Experiments across a variety of settings demonstrate that Safe-BeAl provides comprehensive safety validation, improving safety by 8.55 - 15.22%, compared to embodied agents based on GPT-4, while ensuring successful task completion."
  },
  {
    "title": "Surrogate Fitness Metrics for Interpretable Reinforcement Learning",
    "url": "http://arxiv.org/abs/2504.14645v1",
    "arxiv_id": "2504.14645v1",
    "authors": [
      "Philipp Altmann",
      "C\u00e9line Davignon",
      "Maximilian Zorn",
      "Fabian Ritz",
      "Claudia Linnhoff-Popien",
      "Thomas Gabor"
    ],
    "published": "2025-04-20T15:01:19+00:00",
    "summary": "We employ an evolutionary optimization framework that perturbs initial states to generate informative and diverse policy demonstrations. A joint surrogate fitness function guides the optimization by combining local diversity, behavioral certainty, and global population diversity. To assess demonstration quality, we apply a set of evaluation metrics, including the reward-based optimality gap, fidelity interquartile means (IQMs), fitness composition analysis, and trajectory visualizations. Hyperparameter sensitivity is also examined to better understand the dynamics of trajectory optimization. Our findings demonstrate that optimizing trajectory selection via surrogate fitness metrics significantly improves interpretability of RL policies in both discrete and continuous environments. In gridworld domains, evaluations reveal significantly enhanced demonstration fidelities compared to random and ablated baselines. In continuous control, the proposed framework offers valuable insights, particularly for early-stage policies, while fidelity-based optimization proves more effective for mature policies. By refining and systematically analyzing surrogate fitness functions, this study advances the interpretability of RL models. The proposed improvements provide deeper insights into RL decision-making, benefiting applications in safety-critical and explainability-focused domains."
  },
  {
    "title": "Transferred plasma catheter for endotherapeutic applications: a parametric study of guided streamers dynamics",
    "url": "http://arxiv.org/abs/2504.14637v1",
    "arxiv_id": "2504.14637v1",
    "authors": [
      "M. Soulier",
      "T. Vacek",
      "K. Geraud",
      "T. Dufour"
    ],
    "published": "2025-04-20T14:29:57+00:00",
    "summary": "Non-thermal atmospheric pressure plasma jets (APPJs) are increasingly used in biomedical applications due to their low temperatures and ability to generate reactive oxygen and nitrogen species (RONS), making them suitable for sensitive environments like medical therapies. The transferred plasma catheter (TPC), a variant of APPJ, shows promise for endoscopic applications but requires precise control of plasma dynamics in confined spaces to ensure safety and efficacy. Despite extensive studies on guided streamers in traditional APPJs, there is limited understanding of streamer behavior in TPC configurations, particularly in challenging scenarios involving grounded metallic surfaces. This study examines the spatiotemporal dynamics of guided streamers generated by TPCs under varying gap distances to establish a robust framework for safe and effective plasma delivery in endoscopic settings. Combining electrical and optical diagnostics, the study characterizes streamer propagation, electric field profiles, and plasma-induced currents in a helium-driven TPC delivering cold plasma to a grounded metal target across gaps of 2 to 18 mm. Results show that streamers maintain charge stability and effectively interact with the target for gap distances below 12 mm, producing significant therapeutic currents. Beyond this threshold, propagation deteriorates due to recombination and reduced electric field intensity. For shorter gaps, counter-propagating waves and secondary streamer interactions are observed, while larger gaps lead to charge dissipation and reduced efficacy. These findings highlight the importance of optimizing gap distances for plasma-assisted endoscopic procedures and demonstrate the TPC's robustness in adverse conditions."
  },
  {
    "title": "a1: Steep Test-time Scaling Law via Environment Augmented Generation",
    "url": "http://arxiv.org/abs/2504.14597v1",
    "arxiv_id": "2504.14597v1",
    "authors": [
      "Lingrui Mei",
      "Shenghua Liu",
      "Yiwei Wang",
      "Baolong Bi",
      "Yuyao Ge",
      "Jun Wan",
      "Yurong Wu",
      "Xueqi Cheng"
    ],
    "published": "2025-04-20T12:55:59+00:00",
    "summary": "Large Language Models (LLMs) have made remarkable breakthroughs in reasoning, yet continue to struggle with hallucinations, logical errors, and inability to self-correct during complex multi-step tasks. Current approaches like chain-of-thought prompting offer limited reasoning capabilities that fail when precise step validation is required. We propose Environment Augmented Generation (EAG), a framework that enhances LLM reasoning through: (1) real-time environmental feedback validating each reasoning step, (2) dynamic branch exploration for investigating alternative solution paths when faced with errors, and (3) experience-based learning from successful reasoning trajectories. Unlike existing methods, EAG enables deliberate backtracking and strategic replanning through tight integration of execution feedback with branching exploration. Our a1-32B model achieves state-of-the-art performance among similar-sized models across all benchmarks, matching larger models like o1 on competition mathematics while outperforming comparable models by up to 24.4 percentage points. Analysis reveals EAG's distinctive scaling pattern: initial token investment in environment interaction yields substantial long-term performance dividends, with advantages amplifying proportionally to task complexity. EAG's theoretical framework demonstrates how environment interactivity and systematic branch exploration together establish a new paradigm for reliable machine reasoning, particularly for problems requiring precise multi-step calculation and logical verification."
  },
  {
    "title": "LLM-Enabled In-Context Learning for Data Collection Scheduling in UAV-assisted Sensor Networks",
    "url": "http://arxiv.org/abs/2504.14556v1",
    "arxiv_id": "2504.14556v1",
    "authors": [
      "Yousef Emami",
      "Hao Gao",
      "SeyedSina Nabavirazani",
      "Luis Almeida"
    ],
    "published": "2025-04-20T10:05:07+00:00",
    "summary": "Unmanned Aerial Vehicles (UAVs) are increasingly being used in various private and commercial applications, e.g. traffic control, package delivery, and Search and Rescue (SAR) operations. Machine Learning (ML) methods used in UAV-assisted Sensor Networks (UASNETs) and especially in Deep Reinforcement Learning (DRL) face challenges such as complex and lengthy model training, gaps between simulation and reality, and low sample efficiency, which conflict with the urgency of emergencies such as SAR operations. This paper proposes In-Context Learning (ICL)-based Data Collection Scheduling (ICLDC) scheme, as an alternative to DRL in emergencies. The UAV collects and transmits logged sensory data, to an LLM, to generate a task description in natural language, from which it obtains a data collection schedule to be executed by the UAV. The system continuously adapts by adding feedback to task descriptions and utilizing feedback for future decisions. This method is tested against jailbreaking attacks, where task description is manipulated to undermine network performance, highlighting the vulnerability of LLMs to such attacks. The proposed ICLDC outperforms the Maximum Channel Gain by reducing cumulative packet loss by approximately 56\\%. ICLDC presents a promising direction for intelligent scheduling and control in UAV-assisted data collection."
  },
  {
    "title": "Should Benevolent Deception be Allowed in EHMI? A Mechanism Explanation Based on Game Theory",
    "url": "http://arxiv.org/abs/2504.14539v1",
    "arxiv_id": "2504.14539v1",
    "authors": [
      "Linkun Liu",
      "Jian Sun",
      "Ye Tian"
    ],
    "published": "2025-04-20T09:03:28+00:00",
    "summary": "The application of external human-machine interface (EHMI) on autonomous vehicles (AVs) facilitates information exchange. Existing research fails to consider the impact of the sequence of actions, as well as the effects of EHMI applications and deception, raising the question of whether benevolent, well-intentioned deception should be permitted (i.e., misleading statements that are intended to benefit both parties). We established a game theory based EHMI information disclosure framework for AVs in this study. In considering benevolent deception, this framework divided the decision-making process into three stages, respectively encompassing three key questions: whether to disclose, when to disclose, and what type of intention information to disclose. The results show that theoretical advantages of deception exist in certain cases when AV expects to maximize the safety of the interaction. In 40 out of 484 cases (8.3%), safety can be enhanced through successful deception. Those successful deceptions fall into two categories: 1) In 28 of these cases, the straight-going AV expected the left-turning HV to yield, while HV exhibited lower speed and higher acceleration; 2) In 12 of these cases, AV expected HV to proceed first, while HV exhibited higher speed and lower acceleration. We also conducted a VR-based driving simulation experiment, and the results confirmed our conclusion. Additionally, we found that when participants had low trust in the EHMI, its use negatively impacted interaction efficiency instead. This study aims to analyze the mechanisms of EHMI information disclosure and contribute to the ongoing discourse on the ethical framework governing autonomous driving systems."
  },
  {
    "title": "Are Vision LLMs Road-Ready? A Comprehensive Benchmark for Safety-Critical Driving Video Understanding",
    "url": "http://arxiv.org/abs/2504.14526v1",
    "arxiv_id": "2504.14526v1",
    "authors": [
      "Tong Zeng",
      "Longfeng Wu",
      "Liang Shi",
      "Dawei Zhou",
      "Feng Guo"
    ],
    "published": "2025-04-20T07:50:44+00:00",
    "summary": "Vision Large Language Models (VLLMs) have demonstrated impressive capabilities in general visual tasks such as image captioning and visual question answering. However, their effectiveness in specialized, safety-critical domains like autonomous driving remains largely unexplored. Autonomous driving systems require sophisticated scene understanding in complex environments, yet existing multimodal benchmarks primarily focus on normal driving conditions, failing to adequately assess VLLMs' performance in safety-critical scenarios. To address this, we introduce DVBench, a pioneering benchmark designed to evaluate the performance of VLLMs in understanding safety-critical driving videos. Built around a hierarchical ability taxonomy that aligns with widely adopted frameworks for describing driving scenarios used in assessing highly automated driving systems, DVBench features 10,000 multiple-choice questions with human-annotated ground-truth answers, enabling a comprehensive evaluation of VLLMs' capabilities in perception and reasoning. Experiments on 14 SOTA VLLMs, ranging from 0.5B to 72B parameters, reveal significant performance gaps, with no model achieving over 40% accuracy, highlighting critical limitations in understanding complex driving scenarios. To probe adaptability, we fine-tuned selected models using domain-specific data from DVBench, achieving accuracy gains ranging from 5.24 to 10.94 percentage points, with relative improvements of up to 43.59%. This improvement underscores the necessity of targeted adaptation to bridge the gap between general-purpose VLLMs and mission-critical driving applications. DVBench establishes an essential evaluation framework and research roadmap for developing VLLMs that meet the safety and robustness requirements for real-world autonomous systems. We released the benchmark toolbox and the fine-tuned model at: https://github.com/tong-zeng/DVBench.git."
  },
  {
    "title": "CoLoTa: A Dataset for Entity-based Commonsense Reasoning over Long-Tail Knowledge",
    "url": "http://arxiv.org/abs/2504.14462v1",
    "arxiv_id": "2504.14462v1",
    "authors": [
      "Armin Toroghi",
      "Willis Guo",
      "Scott Sanner"
    ],
    "published": "2025-04-20T02:47:18+00:00",
    "summary": "The rise of Large Language Models (LLMs) has redefined the AI landscape, particularly due to their ability to encode factual and commonsense knowledge, and their outstanding performance in tasks requiring reasoning. Despite these advances, hallucinations and reasoning errors remain a significant barrier to their deployment in high-stakes settings. In this work, we observe that even the most prominent LLMs, such as OpenAI-o1, suffer from high rates of reasoning errors and hallucinations on tasks requiring commonsense reasoning over obscure, long-tail entities. To investigate this limitation, we present a new dataset for Commonsense reasoning over Long-Tail entities (CoLoTa), that consists of 3,300 queries from question answering and claim verification tasks and covers a diverse range of commonsense reasoning skills. We remark that CoLoTa can also serve as a Knowledge Graph Question Answering (KGQA) dataset since the support of knowledge required to answer its queries is present in the Wikidata knowledge graph. However, as opposed to existing KGQA benchmarks that merely focus on factoid questions, our CoLoTa queries also require commonsense reasoning. Our experiments with strong LLM-based KGQA methodologies indicate their severe inability to answer queries involving commonsense reasoning. Hence, we propose CoLoTa as a novel benchmark for assessing both (i) LLM commonsense reasoning capabilities and their robustness to hallucinations on long-tail entities and (ii) the commonsense reasoning capabilities of KGQA methods."
  },
  {
    "title": "Environmental Monitoring Requirements for the ngVLA",
    "url": "http://arxiv.org/abs/2504.14451v1",
    "arxiv_id": "2504.14451v1",
    "authors": [
      "T. K. Sridharan",
      "J. G. Mangum",
      "B. Butler"
    ],
    "published": "2025-04-20T01:58:41+00:00",
    "summary": "Measurement of environmental parameters is one of the basic requirements for the proper operation of a telescope. This memo is intended to provide guidance for the measurement accuracy requirements in the context of the ngVLA. It relies on previous work for ALMA (Mangum, 2001) and EVLA (Butler \\& Perley, 2008) and a review of the subject by Mangum \\& Wallace (2015). The local operational environment can be broadly divided into two categories: electromagnetic and physical. Meteorological parameters (weather) primarily constitute the physical environmental component and radio frequency interference (RFI) is the essential element of the electromagnetic environment. This memo focuses on the weather component and does not address the RFI, safety and physical infrastructure components. Under weather, the relevant topics are (1) the correction to pointing arising from refraction in the atmosphere (2) the different delays in the arrival times of signals at different antennas due to propagation in the atmosphere (3) monitoring weather parameters to provide operations support, e.g. in determining prevalence of precision or normal conditions, dynamic scheduling and the choice of antennas to constitute a sub-array with a given set of characteristics, among others, and (4) archival. Here we restrict ourselves to the first two topics which impact the data obtained and its calibration."
  },
  {
    "title": "Seeing Through Risk: A Symbolic Approximation of Prospect Theory",
    "url": "http://arxiv.org/abs/2504.14448v1",
    "arxiv_id": "2504.14448v1",
    "authors": [
      "Ali Arslan Yousaf",
      "Umair Rehman",
      "Muhammad Umair Danish"
    ],
    "published": "2025-04-20T01:44:54+00:00",
    "summary": "We propose a novel symbolic modeling framework for decision-making under risk that merges interpretability with the core insights of Prospect Theory. Our approach replaces opaque utility curves and probability weighting functions with transparent, effect-size-guided features. We mathematically formalize the method, demonstrate its ability to replicate well-known framing and loss-aversion phenomena, and provide an end-to-end empirical validation on synthetic datasets. The resulting model achieves competitive predictive performance while yielding clear coefficients mapped onto psychological constructs, making it suitable for applications ranging from AI safety to economic policy analysis."
  },
  {
    "title": "RedMulE-FT: A Reconfigurable Fault-Tolerant Matrix Multiplication Engine",
    "url": "http://arxiv.org/abs/2504.14399v1",
    "arxiv_id": "2504.14399v1",
    "authors": [
      "Philip Wiese",
      "Maurus Item",
      "Luca Bertaccini",
      "Yvan Tortorella",
      "Angelo Garofalo",
      "Luca Benini"
    ],
    "published": "2025-04-19T20:25:33+00:00",
    "summary": "As safety-critical applications increasingly rely on data-parallel floating-point computations, there is an increasing need for flexible and configurable fault tolerance in parallel floating-point accelerators such as tensor engines. While replication-based methods ensure reliability but incur high area and power costs, error correction codes lack the flexibility to trade off robustness against performance. This work presents RedMulE-FT, a runtime-configurable fault-tolerant extension of the RedMulE matrix multiplication accelerator, balancing fault tolerance, area overhead, and performance impacts. The fault tolerance mode is configured in a shadowed context register file before task execution. By combining replication with error-detecting codes to protect the data path, RedMulE-FT achieves an 11x uncorrected fault reduction with only 2.3% area overhead. Full protection extends to control signals, resulting in no functional errors after 1M injections during our extensive fault injection simulation campaign, with a total area overhead of 25.2% while maintaining a 500 MHz frequency in a 12 nm technology."
  },
  {
    "title": "The Geometry of Self-Verification in a Task-Specific Reasoning Model",
    "url": "http://arxiv.org/abs/2504.14379v1",
    "arxiv_id": "2504.14379v1",
    "authors": [
      "Andrew Lee",
      "Lihao Sun",
      "Chris Wendler",
      "Fernanda Vi\u00e9gas",
      "Martin Wattenberg"
    ],
    "published": "2025-04-19T18:40:51+00:00",
    "summary": "How do reasoning models verify their own answers? We study this question by training a model using DeepSeek R1's recipe on the CountDown task. We leverage the fact that preference tuning leads to mode collapse, resulting in a model that always produces highly structured and easily parse-able chain-of-thought sequences. With this setup, we do a top-down and bottom-up analysis to reverse-engineer how the model verifies its outputs. Our top-down analysis reveals Gated Linear Unit (GLU) weights encoding verification-related tokens, such as ``success'' or ``incorrect'', which activate according to the correctness of the model's reasoning steps. Our bottom-up analysis reveals that ``previous-token heads'' are mainly responsible for model verification. Our analyses meet in the middle: drawing inspiration from inter-layer communication channels, we use the identified GLU vectors to localize as few as three attention heads that can disable model verification, pointing to a necessary component of a potentially larger verification circuit."
  },
  {
    "title": "Manipulating Multimodal Agents via Cross-Modal Prompt Injection",
    "url": "http://arxiv.org/abs/2504.14348v1",
    "arxiv_id": "2504.14348v1",
    "authors": [
      "Le Wang",
      "Zonghao Ying",
      "Tianyuan Zhang",
      "Siyuan Liang",
      "Shengshan Hu",
      "Mingchuan Zhang",
      "Aishan Liu",
      "Xianglong Liu"
    ],
    "published": "2025-04-19T16:28:03+00:00",
    "summary": "The emergence of multimodal large language models has redefined the agent paradigm by integrating language and vision modalities with external data sources, enabling agents to better interpret human instructions and execute increasingly complex tasks. However, in this work, we identify a critical yet previously overlooked security vulnerability in multimodal agents: cross-modal prompt injection attacks. To exploit this vulnerability, we propose CrossInject, a novel attack framework in which attackers embed adversarial perturbations across multiple modalities to align with target malicious content, allowing external instructions to hijack the agent's decision-making process and execute unauthorized tasks. Our approach consists of two key components. First, we introduce Visual Latent Alignment, where we optimize adversarial features to the malicious instructions in the visual embedding space based on a text-to-image generative model, ensuring that adversarial images subtly encode cues for malicious task execution. Subsequently, we present Textual Guidance Enhancement, where a large language model is leveraged to infer the black-box defensive system prompt through adversarial meta prompting and generate an malicious textual command that steers the agent's output toward better compliance with attackers' requests. Extensive experiments demonstrate that our method outperforms existing injection attacks, achieving at least a +26.4% increase in attack success rates across diverse tasks. Furthermore, we validate our attack's effectiveness in real-world multimodal autonomous agents, highlighting its potential implications for safety-critical applications."
  },
  {
    "title": "Manipulating Multimodal Agents via Cross-Modal Prompt Injection",
    "url": "http://arxiv.org/abs/2504.14348v2",
    "arxiv_id": "2504.14348v2",
    "authors": [
      "Le Wang",
      "Zonghao Ying",
      "Tianyuan Zhang",
      "Siyuan Liang",
      "Shengshan Hu",
      "Mingchuan Zhang",
      "Aishan Liu",
      "Xianglong Liu"
    ],
    "published": "2025-04-19T16:28:03+00:00",
    "summary": "The emergence of multimodal large language models has redefined the agent paradigm by integrating language and vision modalities with external data sources, enabling agents to better interpret human instructions and execute increasingly complex tasks. However, in this work, we identify a critical yet previously overlooked security vulnerability in multimodal agents: cross-modal prompt injection attacks. To exploit this vulnerability, we propose CrossInject, a novel attack framework in which attackers embed adversarial perturbations across multiple modalities to align with target malicious content, allowing external instructions to hijack the agent's decision-making process and execute unauthorized tasks. Our approach consists of two key components. First, we introduce Visual Latent Alignment, where we optimize adversarial features to the malicious instructions in the visual embedding space based on a text-to-image generative model, ensuring that adversarial images subtly encode cues for malicious task execution. Subsequently, we present Textual Guidance Enhancement, where a large language model is leveraged to infer the black-box defensive system prompt through adversarial meta prompting and generate an malicious textual command that steers the agent's output toward better compliance with attackers' requests. Extensive experiments demonstrate that our method outperforms existing injection attacks, achieving at least a +26.4% increase in attack success rates across diverse tasks. Furthermore, we validate our attack's effectiveness in real-world multimodal autonomous agents, highlighting its potential implications for safety-critical applications."
  },
  {
    "title": "DLW-CI: A Dynamic Likelihood-Weighted Cooperative Infotaxis Approach for Multi-Source Search in Urban Environments Using Consumer Drone Networks",
    "url": "http://arxiv.org/abs/2504.14330v1",
    "arxiv_id": "2504.14330v1",
    "authors": [
      "Xiaoran Zhang",
      "Yatai Ji",
      "Yong Zhao",
      "Chuan Ai",
      "Bin Chen",
      "Zhengqiu Zhu"
    ],
    "published": "2025-04-19T15:44:09+00:00",
    "summary": "Consumer-grade drones equipped with low-cost sensors have emerged as a cornerstone of Autonomous Intelligent Systems (AISs) for environmental monitoring and hazardous substance detection in urban environments. However, existing research primarily addresses single-source search problems, overlooking the complexities of real-world urban scenarios where both the location and quantity of hazardous sources remain unknown. To address this issue, we propose the Dynamic Likelihood-Weighted Cooperative Infotaxis (DLW-CI) approach for consumer drone networks. Our approach enhances multi-drone collaboration in AISs by combining infotaxis (a cognitive search strategy) with optimized source term estimation and an innovative cooperative mechanism. Specifically, we introduce a novel source term estimation method that utilizes multiple parallel particle filters, with each filter dedicated to estimating the parameters of a potentially unknown source within the search scene. Furthermore, we develop a cooperative mechanism based on dynamic likelihood weights to prevent multiple drones from simultaneously estimating and searching for the same source, thus optimizing the energy efficiency and search coverage of the consumer AIS. Experimental results demonstrate that the DLW-CI approach significantly outperforms baseline methods regarding success rate, accuracy, and root mean square error, particularly in scenarios with relatively few sources, regardless of the presence of obstacles. Also, the effectiveness of the proposed approach is verified in a diffusion scenario generated by the computational fluid dynamics (CFD) model. Research findings indicate that our approach could improve source estimation accuracy and search efficiency by consumer drone-based AISs, making a valuable contribution to environmental safety monitoring applications within smart city infrastructure."
  },
  {
    "title": "Towards NSFW-Free Text-to-Image Generation via Safety-Constraint Direct Preference Optimization",
    "url": "http://arxiv.org/abs/2504.14290v1",
    "arxiv_id": "2504.14290v1",
    "authors": [
      "Shouwei Ruan",
      "Zhenyu Wu",
      "Yao Huang",
      "Ruochen Zhang",
      "Yitong Sun",
      "Caixin Kang",
      "Xingxing Wei"
    ],
    "published": "2025-04-19T13:26:46+00:00",
    "summary": "Ensuring the safety of generated content remains a fundamental challenge for Text-to-Image (T2I) generation. Existing studies either fail to guarantee complete safety under potentially harmful concepts or struggle to balance safety with generation quality. To address these issues, we propose Safety-Constrained Direct Preference Optimization (SC-DPO), a novel framework for safety alignment in T2I models. SC-DPO integrates safety constraints into the general human preference calibration, aiming to maximize the likelihood of generating human-preferred samples while minimizing the safety cost of the generated outputs. In SC-DPO, we introduce a safety cost model to accurately quantify harmful levels for images, and train it effectively using the proposed contrastive learning and cost anchoring objectives. To apply SC-DPO for effective T2I safety alignment, we constructed SCP-10K, a safety-constrained preference dataset containing rich harmful concepts, which blends safety-constrained preference pairs under both harmful and clean instructions, further mitigating the trade-off between safety and sample quality. Additionally, we propose a Dynamic Focusing Mechanism (DFM) for SC-DPO, promoting the model's learning of difficult preference pair samples. Extensive experiments demonstrate that SC-DPO outperforms existing methods, effectively defending against various NSFW content while maintaining optimal sample quality and human preference alignment. Additionally, SC-DPO exhibits resilience against adversarial prompts designed to generate harmful content."
  },
  {
    "title": "SRPO: A Cross-Domain Implementation of Large-Scale Reinforcement Learning on LLM",
    "url": "http://arxiv.org/abs/2504.14286v1",
    "arxiv_id": "2504.14286v1",
    "authors": [
      "Xiaojiang Zhang",
      "Jinghui Wang",
      "Zifei Cheng",
      "Wenhao Zhuang",
      "Zheng Lin",
      "Minglei Zhang",
      "Shaojie Wang",
      "Yinghan Cui",
      "Chao Wang",
      "Junyi Peng",
      "Shimiao Jiang",
      "Shiqi Kuang",
      "Shouyu Yin",
      "Chaohang Wen",
      "Haotian Zhang",
      "Bin Chen",
      "Bing Yu"
    ],
    "published": "2025-04-19T13:06:03+00:00",
    "summary": "Recent advances of reasoning models, exemplified by OpenAI's o1 and DeepSeek's R1, highlight the significant potential of Reinforcement Learning (RL) to enhance the reasoning capabilities of Large Language Models (LLMs). However, replicating these advancements across diverse domains remains challenging due to limited methodological transparency. In this work, we present two-Staged history-Resampling Policy Optimization (SRPO), which successfully surpasses the performance of DeepSeek-R1-Zero-32B on the AIME24 and LiveCodeBench benchmarks. SRPO achieves this using the same base model as DeepSeek (i.e. Qwen2.5-32B) and relies solely on RL, without prior Supervised Fine-Tuning (SFT). Building upon Group Relative Policy Optimization (GRPO), we introduce two key methodological innovations: (1) a two-stage cross-domain training paradigm designed to balance the development of mathematical reasoning and coding proficiency, and (2) History Resampling (HR), a technique to address ineffective samples. Our comprehensive experiments validate the effectiveness of our approach, dedicating to offer valuable insights into scaling LLM reasoning capabilities across diverse tasks."
  },
  {
    "title": "SRPO: A Cross-Domain Implementation of Large-Scale Reinforcement Learning on LLM",
    "url": "http://arxiv.org/abs/2504.14286v2",
    "arxiv_id": "2504.14286v2",
    "authors": [
      "Xiaojiang Zhang",
      "Jinghui Wang",
      "Zifei Cheng",
      "Wenhao Zhuang",
      "Zheng Lin",
      "Minglei Zhang",
      "Shaojie Wang",
      "Yinghan Cui",
      "Chao Wang",
      "Junyi Peng",
      "Shimiao Jiang",
      "Shiqi Kuang",
      "Shouyu Yin",
      "Chaohang Wen",
      "Haotian Zhang",
      "Bin Chen",
      "Bing Yu"
    ],
    "published": "2025-04-19T13:06:03+00:00",
    "summary": "Recent advances of reasoning models, exemplified by OpenAI's o1 and DeepSeek's R1, highlight the significant potential of Reinforcement Learning (RL) to enhance the reasoning capabilities of Large Language Models (LLMs). However, replicating these advancements across diverse domains remains challenging due to limited methodological transparency. In this work, we present two-Staged history-Resampling Policy Optimization (SRPO), which surpasses the performance of DeepSeek-R1-Zero-32B on the AIME24 and LiveCodeBench benchmarks. SRPO achieves this using the same base model as DeepSeek (i.e. Qwen2.5-32B), using only about 1/10 of the training steps required by DeepSeek-R1-Zero-32B, demonstrating superior efficiency. Building upon Group Relative Policy Optimization (GRPO), we introduce two key methodological innovations: (1) a two-stage cross-domain training paradigm designed to balance the development of mathematical reasoning and coding proficiency, and (2) History Resampling (HR), a technique to address ineffective samples. Our comprehensive experiments validate the effectiveness of our approach, offering valuable insights into scaling LLM reasoning capabilities across diverse tasks."
  },
  {
    "title": "Microscopic features of the effect of vehicle overacceleration on traffic flow",
    "url": "http://arxiv.org/abs/2504.14244v1",
    "arxiv_id": "2504.14244v1",
    "authors": [
      "Boris S. Kerner",
      "Sergey L. Klenov"
    ],
    "published": "2025-04-19T09:37:17+00:00",
    "summary": "Through the development of a microscopic deterministic model in the framework of three-phase traffic theory, microscopic features of vehicle overacceleration, which determines the occurrence of the metastability of free traffic flow at a bottleneck, have been revealed: (i) The greater the impact of vehicle overacceleration on free traffic flow at a bottleneck, the higher the maximum flow rate at which free flow can persist at the bottleneck, i.e., the better traffic breakdown can be avoided. (ii) There can be at least two mechanisms of overacceleration in road lane caused by safety acceleration at the bottleneck. (iii) Through a microscopic analysis of spatiotemporal competition between speed adaptation and vehicle acceleration behaviors, traffic conditions have been found at which safety acceleration in road lane or/and vehicle acceleration due to lane-changing on multi-lane road become overacceleration. (iv) There is spatiotemporal cooperation of different overacceleration mechanisms. (v) The stronger the overacceleration cooperation, the stronger the maintenance of free flow at the bottleneck due to overacceleration. (vi) On two-lane road, both speed adaptation and overacceleration in road lane can effect qualitatively on the overacceleration mechanism caused by lane-changing. These microscopic features of the effect of vehicle overacceleration on traffic flow are related to traffic flow consisting of human-driving or/and automated-driving vehicles."
  },
  {
    "title": "Know Me, Respond to Me: Benchmarking LLMs for Dynamic User Profiling and Personalized Responses at Scale",
    "url": "http://arxiv.org/abs/2504.14225v1",
    "arxiv_id": "2504.14225v1",
    "authors": [
      "Bowen Jiang",
      "Zhuoqun Hao",
      "Young-Min Cho",
      "Bryan Li",
      "Yuan Yuan",
      "Sihao Chen",
      "Lyle Ungar",
      "Camillo J. Taylor",
      "Dan Roth"
    ],
    "published": "2025-04-19T08:16:10+00:00",
    "summary": "Large Language Models (LLMs) have emerged as personalized assistants for users across a wide range of tasks -- from offering writing support to delivering tailored recommendations or consultations. Over time, the interaction history between a user and an LLM can provide extensive information about an individual's traits and preferences. However, open questions remain on how well LLMs today can effectively leverage such history to (1) internalize the user's inherent traits and preferences, (2) track how the user profiling and preferences evolve over time, and (3) generate personalized responses accordingly in new scenarios.   In this work, we introduce the PERSONAMEM benchmark. PERSONAMEM features curated user profiles with over 180 simulated user-LLM interaction histories, each containing up to 60 sessions of multi-turn conversations across 15 real-world tasks that require personalization. Given an in-situ user query, i.e. query issued by the user from the first-person perspective, we evaluate LLM chatbots' ability to identify the most suitable response according to the current state of the user's profile. We observe that current LLMs still struggle to recognize the dynamic evolution in users' profiles over time through direct prompting approaches. As a consequence, LLMs often fail to deliver responses that align with users' current situations and preferences, with frontier models such as GPT-4.1, o4-mini, GPT-4.5, o1, or Gemini-2.0 achieving only around 50% overall accuracy, suggesting room for improvement. We hope that PERSONAMEM, along with the user profile and conversation simulation pipeline, can facilitate future research in the development of truly user-aware chatbots. Code and data are available at github.com/bowen-upenn/PersonaMem."
  },
  {
    "title": "Amplify Initiative: Building A Localized Data Platform for Globalized AI",
    "url": "http://arxiv.org/abs/2504.14105v1",
    "arxiv_id": "2504.14105v1",
    "authors": [
      "Qazi Mamunur Rashid",
      "Erin van Liemt",
      "Tiffany Shih",
      "Amber Ebinama",
      "Karla Barrios Ramos",
      "Madhurima Maji",
      "Aishwarya Verma",
      "Charu Kalia",
      "Jamila Smith-Loud",
      "Joyce Nakatumba-Nabende",
      "Rehema Baguma",
      "Andrew Katumba",
      "Chodrine Mutebi",
      "Jagen Marvin",
      "Eric Peter Wairagala",
      "Mugizi Bruce",
      "Peter Oketta",
      "Lawrence Nderu",
      "Obichi Obiajunwa",
      "Abigail Oppong",
      "Michael Zimba",
      "Data Authors"
    ],
    "published": "2025-04-18T23:20:52+00:00",
    "summary": "Current AI models often fail to account for local context and language, given the predominance of English and Western internet content in their training data. This hinders the global relevance, usefulness, and safety of these models as they gain more users around the globe. Amplify Initiative, a data platform and methodology, leverages expert communities to collect diverse, high-quality data to address the limitations of these models. The platform is designed to enable co-creation of datasets, provide access to high-quality multilingual datasets, and offer recognition to data authors. This paper presents the approach to co-creating datasets with domain experts (e.g., health workers, teachers) through a pilot conducted in Sub-Saharan Africa (Ghana, Kenya, Malawi, Nigeria, and Uganda). In partnership with local researchers situated in these countries, the pilot demonstrated an end-to-end approach to co-creating data with 155 experts in sensitive domains (e.g., physicians, bankers, anthropologists, human and civil rights advocates). This approach, implemented with an Android app, resulted in an annotated dataset of 8,091 adversarial queries in seven languages (e.g., Luganda, Swahili, Chichewa), capturing nuanced and contextual information related to key themes such as misinformation and public interest topics. This dataset in turn can be used to evaluate models for their safety and cultural relevance within the context of these languages."
  },
  {
    "title": "LogicTree: Structured Proof Exploration for Coherent and Rigorous Logical Reasoning with Large Language Models",
    "url": "http://arxiv.org/abs/2504.14089v1",
    "arxiv_id": "2504.14089v1",
    "authors": [
      "Kang He",
      "Kaushik Roy"
    ],
    "published": "2025-04-18T22:10:02+00:00",
    "summary": "Large language models (LLMs) have achieved remarkable multi-step reasoning capabilities across various domains. However, LLMs still face distinct challenges in complex logical reasoning, as (1) proof-finding requires systematic exploration and the maintenance of logical coherence and (2) searching the right combination of premises at each reasoning step is inherently challenging in tasks with large premise space. To address this, we propose LogicTree, an inference-time modular framework employing algorithm-guided search to automate structured proof exploration and ensure logical coherence. Advancing beyond tree-of-thought (ToT), we incorporate caching mechanism into LogicTree to enable effective utilization of historical knowledge, preventing reasoning stagnation and minimizing redundancy. Furthermore, we address the combinatorial complexity of premise search by decomposing it into a linear process. The refined premise selection restricts subsequent inference to at most one derivation per step, enhancing reasoning granularity and enforcing strict step-by-step reasoning. Additionally, we introduce two LLM-free heuristics for premise prioritization, enabling strategic proof search. Experimental results on five datasets demonstrate that LogicTree optimally scales inference-time computation to achieve higher proof accuracy, surpassing chain-of-thought (CoT) and ToT with average gains of 23.6% and 12.5%, respectively, on GPT-4o. Moreover, within LogicTree, GPT-4o outperforms o3-mini by 7.6% on average."
  },
  {
    "title": "Infrared Vision Systems for Emergency Vehicle Driver Assistance in Low-Visibility Conditions",
    "url": "http://arxiv.org/abs/2504.14078v1",
    "arxiv_id": "2504.14078v1",
    "authors": [
      "M-Mahdi Naddaf-Sh",
      "Andrew Lee",
      "Kin Yen",
      "Eemon Amini",
      "Iman Soltani"
    ],
    "published": "2025-04-18T21:06:41+00:00",
    "summary": "This study investigates the potential of infrared (IR) camera technology to enhance driver safety for emergency vehicles operating in low-visibility conditions, particularly at night and in dense fog. Such environments significantly increase the risk of collisions, especially for tow trucks and snowplows that must remain operational in challenging conditions. Conventional driver assistance systems often struggle under these conditions due to limited visibility. In contrast, IR cameras, which detect the thermal signatures of obstacles, offer a promising alternative. The evaluation combines controlled laboratory experiments, real-world field tests, and surveys of emergency vehicle operators. In addition to assessing detection performance, the study examines the feasibility of retrofitting existing Department of Transportation (DoT) fleets with cost-effective IR-based driver assistance systems. Results underscore the utility of IR technology in enhancing driver awareness and provide data-driven recommendations for scalable deployment across legacy emergency vehicle fleets."
  },
  {
    "title": "Think Deep, Think Fast: Investigating Efficiency of Verifier-free Inference-time-scaling Methods",
    "url": "http://arxiv.org/abs/2504.14047v1",
    "arxiv_id": "2504.14047v1",
    "authors": [
      "Junlin Wang",
      "Shang Zhu",
      "Jon Saad-Falcon",
      "Ben Athiwaratkun",
      "Qingyang Wu",
      "Jue Wang",
      "Shuaiwen Leon Song",
      "Ce Zhang",
      "Bhuwan Dhingra",
      "James Zou"
    ],
    "published": "2025-04-18T19:32:55+00:00",
    "summary": "There is intense interest in investigating how inference time compute (ITC) (e.g. repeated sampling, refinements, etc) can improve large language model (LLM) capabilities. At the same time, recent breakthroughs in reasoning models, such as Deepseek-R1, unlock the opportunity for reinforcement learning to improve LLM reasoning skills. An in-depth understanding of how ITC interacts with reasoning across different models could provide important guidance on how to further advance the LLM frontier. This work conducts a comprehensive analysis of inference-time scaling methods for both reasoning and non-reasoning models on challenging reasoning tasks. Specifically, we focus our research on verifier-free inference time-scaling methods due to its generalizability without needing a reward model. We construct the Pareto frontier of quality and efficiency. We find that non-reasoning models, even with an extremely high inference budget, still fall substantially behind reasoning models. For reasoning models, majority voting proves to be a robust inference strategy, generally competitive or outperforming other more sophisticated ITC methods like best-of-N and sequential revisions, while the additional inference compute offers minimal improvements. We further perform in-depth analyses of the association of key response features (length and linguistic markers) with response quality, with which we can improve the existing ITC methods. We find that correct responses from reasoning models are typically shorter and have fewer hedging and thinking markers (but more discourse markers) than the incorrect responses."
  },
  {
    "title": "Multi-Stage Retrieval for Operational Technology Cybersecurity Compliance Using Large Language Models: A Railway Casestudy",
    "url": "http://arxiv.org/abs/2504.14044v1",
    "arxiv_id": "2504.14044v1",
    "authors": [
      "Regan Bolton",
      "Mohammadreza Sheikhfathollahi",
      "Simon Parkinson",
      "Dan Basher",
      "Howard Parkinson"
    ],
    "published": "2025-04-18T19:24:17+00:00",
    "summary": "Operational Technology Cybersecurity (OTCS) continues to be a dominant challenge for critical infrastructure such as railways. As these systems become increasingly vulnerable to malicious attacks due to digitalization, effective documentation and compliance processes are essential to protect these safety-critical systems. This paper proposes a novel system that leverages Large Language Models (LLMs) and multi-stage retrieval to enhance the compliance verification process against standards like IEC 62443 and the rail-specific IEC 63452. We first evaluate a Baseline Compliance Architecture (BCA) for answering OTCS compliance queries, then develop an extended approach called Parallel Compliance Architecture (PCA) that incorporates additional context from regulatory standards. Through empirical evaluation comparing OpenAI-gpt-4o and Claude-3.5-haiku models in these architectures, we demonstrate that the PCA significantly improves both correctness and reasoning quality in compliance verification. Our research establishes metrics for response correctness, logical reasoning, and hallucination detection, highlighting the strengths and limitations of using LLMs for compliance verification in railway cybersecurity. The results suggest that retrieval-augmented approaches can significantly improve the efficiency and accuracy of compliance assessments, particularly valuable in an industry facing a shortage of cybersecurity expertise."
  },
  {
    "title": "Statistical Analysis and End-to-End Performance Evaluation of Traffic Models for Automotive Data",
    "url": "http://arxiv.org/abs/2504.14017v1",
    "arxiv_id": "2504.14017v1",
    "authors": [
      "Marcello Bullo",
      "Amir Ashtari Gargari",
      "Paolo Testolina",
      "Michele Zorzi",
      "Marco Giordani"
    ],
    "published": "2025-04-18T18:12:08+00:00",
    "summary": "Autonomous driving is a major paradigm shift in transportation, with the potential to enhance safety, optimize traffic congestion, and reduce fuel consumption. Although autonomous vehicles rely on advanced sensors and on-board computing systems to navigate without human control, full awareness of the driving environment also requires a cooperative effort via Vehicle-To-Everything (V2X) communication. Specifically, vehicles send and receive sensor perceptions to/from other vehicles to extend perception beyond their own sensing range. However, transmitting large volumes of data can be challenging for current V2X communication technologies, so data compression represents a crucial solution to reduce the message size and link congestion. In this paper, we present a statistical characterization of automotive data, focusing on LiDAR sensors. Notably, we provide models for the size of both raw and compressed point clouds. The use of statistical traffic models offers several advantages compared to using real data, such as faster simulations, reduced storage requirements, and greater flexibility in the application design. Furthermore, statistical models can be used for understanding traffic patterns and analyzing statistics, which is crucial to design and optimize wireless networks. We validate our statistical models via a Kolmogorov-Smirnoff test implementing a Bootstrap Resampling scheme. Moreover, we show via ns-3 simulations that using statistical models yields comparable results in terms of latency and throughput compared to real data, which also demonstrates the accuracy of the models."
  },
  {
    "title": "DiffOG: Differentiable Policy Trajectory Optimization with Generalizability",
    "url": "http://arxiv.org/abs/2504.13807v1",
    "arxiv_id": "2504.13807v1",
    "authors": [
      "Zhengtong Xu",
      "Zichen Miao",
      "Qiang Qiu",
      "Zhe Zhang",
      "Yu She"
    ],
    "published": "2025-04-18T17:20:27+00:00",
    "summary": "Imitation learning-based visuomotor policies excel at manipulation tasks but often produce suboptimal action trajectories compared to model-based methods. Directly mapping camera data to actions via neural networks can result in jerky motions and difficulties in meeting critical constraints, compromising safety and robustness in real-world deployment. For tasks that require high robustness or strict adherence to constraints, ensuring trajectory quality is crucial. However, the lack of interpretability in neural networks makes it challenging to generate constraint-compliant actions in a controlled manner. This paper introduces differentiable policy trajectory optimization with generalizability (DiffOG), a learning-based trajectory optimization framework designed to enhance visuomotor policies. By leveraging the proposed differentiable formulation of trajectory optimization with transformer, DiffOG seamlessly integrates policies with a generalizable optimization layer. Visuomotor policies enhanced by DiffOG generate smoother, constraint-compliant action trajectories in a more interpretable way. DiffOG exhibits strong generalization capabilities and high flexibility. We evaluated DiffOG across 11 simulated tasks and 2 real-world tasks. The results demonstrate that DiffOG significantly enhances the trajectory quality of visuomotor policies while having minimal impact on policy performance, outperforming trajectory processing baselines such as greedy constraint clipping and penalty-based trajectory optimization. Furthermore, DiffOG achieves superior performance compared to existing constrained visuomotor policy."
  },
  {
    "title": "Meta-Learning and Knowledge Discovery based Physics-Informed Neural Network for Remaining Useful Life Prediction",
    "url": "http://arxiv.org/abs/2504.13797v1",
    "arxiv_id": "2504.13797v1",
    "authors": [
      "Yu Wang",
      "Shujie Liu",
      "Shuai Lv",
      "Gengshuo Liu"
    ],
    "published": "2025-04-18T16:58:38+00:00",
    "summary": "Predicting the remaining useful life (RUL) of rotating machinery is critical for industrial safety and maintenance, but existing methods struggle with scarce target-domain data and unclear degradation dynamics. We propose a Meta-Learning and Knowledge Discovery-based Physics-Informed Neural Network (MKDPINN) to address these challenges. The method first maps noisy sensor data to a low-dimensional hidden state space via a Hidden State Mapper (HSM). A Physics-Guided Regulator (PGR) then learns unknown nonlinear PDEs governing degradation evolution, embedding these physical constraints into the PINN framework. This integrates data-driven and physics-based approaches. The framework uses meta-learning, optimizing across source-domain meta-tasks to enable few-shot adaptation to new target tasks. Experiments on industrial data and the C-MAPSS benchmark show MKDPINN outperforms baselines in generalization and accuracy, proving its effectiveness for RUL prediction under data scarcity"
  },
  {
    "title": "Revisiting Uncertainty Quantification Evaluation in Language Models: Spurious Interactions with Response Length Bias Results",
    "url": "http://arxiv.org/abs/2504.13677v1",
    "arxiv_id": "2504.13677v1",
    "authors": [
      "Andrea Santilli",
      "Adam Golinski",
      "Michael Kirchhof",
      "Federico Danieli",
      "Arno Blaas",
      "Miao Xiong",
      "Luca Zappella",
      "Sinead Williamson"
    ],
    "published": "2025-04-18T13:13:42+00:00",
    "summary": "Uncertainty Quantification (UQ) in Language Models (LMs) is crucial for improving their safety and reliability. Evaluations often use performance metrics like AUROC to assess how well UQ methods (e.g., negative sequence probabilities) correlate with task correctness functions (e.g., ROUGE-L). In this paper, we show that commonly used correctness functions bias UQ evaluations by inflating the performance of certain UQ methods. We evaluate 7 correctness functions -- from lexical-based and embedding-based metrics to LLM-as-a-judge approaches -- across 4 datasets x 4 models x 6 UQ methods. Our analysis reveals that length biases in the errors of these correctness functions distort UQ assessments by interacting with length biases in UQ methods. We identify LLM-as-a-judge approaches as among the least length-biased choices and hence a potential solution to mitigate these biases."
  },
  {
    "title": "Enhancing Pothole Detection and Characterization: Integrated Segmentation and Depth Estimation in Road Anomaly Systems",
    "url": "http://arxiv.org/abs/2504.13648v1",
    "arxiv_id": "2504.13648v1",
    "authors": [
      "Uthman Baroudi",
      "Alala BaHamid",
      "Yasser Elalfy",
      "Ziad Al Alami"
    ],
    "published": "2025-04-18T11:59:38+00:00",
    "summary": "Road anomaly detection plays a crucial role in road maintenance and in enhancing the safety of both drivers and vehicles. Recent machine learning approaches for road anomaly detection have overcome the tedious and time-consuming process of manual analysis and anomaly counting; however, they often fall short in providing a complete characterization of road potholes. In this paper, we leverage transfer learning by adopting a pre-trained YOLOv8-seg model for the automatic characterization of potholes using digital images captured from a dashboard-mounted camera. Our work includes the creation of a novel dataset, comprising both images and their corresponding depth maps, collected from diverse road environments in Al-Khobar city and the KFUPM campus in Saudi Arabia. Our approach performs pothole detection and segmentation to precisely localize potholes and calculate their area. Subsequently, the segmented image is merged with its depth map to extract detailed depth information about the potholes. This integration of segmentation and depth data offers a more comprehensive characterization compared to previous deep learning-based road anomaly detection systems. Overall, this method not only has the potential to significantly enhance autonomous vehicle navigation by improving the detection and characterization of road hazards but also assists road maintenance authorities in responding more effectively to road damage."
  },
  {
    "title": "Thought Manipulation: External Thought Can Be Efficient for Large Reasoning Models",
    "url": "http://arxiv.org/abs/2504.13626v1",
    "arxiv_id": "2504.13626v1",
    "authors": [
      "Yule Liu",
      "Jingyi Zheng",
      "Zhen Sun",
      "Zifan Peng",
      "Wenhan Dong",
      "Zeyang Sha",
      "Shiwen Cui",
      "Weiqiang Wang",
      "Xinlei He"
    ],
    "published": "2025-04-18T11:07:19+00:00",
    "summary": "Recent advancements in large reasoning models (LRMs) have demonstrated the effectiveness of scaling test-time computation to enhance reasoning capabilities in multiple tasks. However, LRMs typically suffer from \"overthinking\" problems, where models generate significantly redundant reasoning steps while bringing limited performance gains. Existing work relies on fine-tuning to mitigate overthinking, which requires additional data, unconventional training setups, risky safety misalignment, and poor generalization.   Through empirical analysis, we reveal an important characteristic of LRM behaviors that placing external CoTs generated by smaller models between the thinking token ($\\texttt{<think>}$ and $\\texttt{</think>)}$ can effectively manipulate the model to generate fewer thoughts. Building on these insights, we propose a simple yet efficient pipeline, ThoughtMani, to enable LRMs to bypass unnecessary intermediate steps and reduce computational costs significantly. We conduct extensive experiments to validate the utility and efficiency of ThoughtMani. For instance, when applied to QwQ-32B on the LiveBench/Code dataset, ThoughtMani keeps the original performance and reduces output token counts by approximately 30%, with little overhead from the CoT generator. Furthermore, we find that ThoughtMani enhances safety alignment by an average of 10%. Since model vendors typically serve models of different sizes simultaneously, ThoughtMani provides an effective way to construct more efficient and accessible LRMs for real-world applications."
  },
  {
    "title": "Hysteresis-Aware Neural Network Modeling and Whole-Body Reinforcement Learning Control of Soft Robots",
    "url": "http://arxiv.org/abs/2504.13582v1",
    "arxiv_id": "2504.13582v1",
    "authors": [
      "Zongyuan Chen",
      "Yan Xia",
      "Jiayuan Liu",
      "Jijia Liu",
      "Wenhao Tang",
      "Jiayu Chen",
      "Feng Gao",
      "Longfei Ma",
      "Hongen Liao",
      "Yu Wang",
      "Chao Yu",
      "Boyu Zhang",
      "Fei Xing"
    ],
    "published": "2025-04-18T09:34:56+00:00",
    "summary": "Soft robots exhibit inherent compliance and safety, which makes them particularly suitable for applications requiring direct physical interaction with humans, such as surgical procedures. However, their nonlinear and hysteretic behavior, resulting from the properties of soft materials, presents substantial challenges for accurate modeling and control. In this study, we present a soft robotic system designed for surgical applications and propose a hysteresis-aware whole-body neural network model that accurately captures and predicts the soft robot's whole-body motion, including its hysteretic behavior. Building upon the high-precision dynamic model, we construct a highly parallel simulation environment for soft robot control and apply an on-policy reinforcement learning algorithm to efficiently train whole-body motion control strategies. Based on the trained control policy, we developed a soft robotic system for surgical applications and validated it through phantom-based laser ablation experiments in a physical environment. The results demonstrate that the hysteresis-aware modeling reduces the Mean Squared Error (MSE) by 84.95 percent compared to traditional modeling methods. The deployed control algorithm achieved a trajectory tracking error ranging from 0.126 to 0.250 mm on the real soft robot, highlighting its precision in real-world conditions. The proposed method showed strong performance in phantom-based surgical experiments and demonstrates its potential for complex scenarios, including future real-world clinical applications."
  },
  {
    "title": "DETAM: Defending LLMs Against Jailbreak Attacks via Targeted Attention Modification",
    "url": "http://arxiv.org/abs/2504.13562v1",
    "arxiv_id": "2504.13562v1",
    "authors": [
      "Yu Li",
      "Han Jiang",
      "Zhihua Wei"
    ],
    "published": "2025-04-18T09:02:12+00:00",
    "summary": "With the widespread adoption of Large Language Models (LLMs), jailbreak attacks have become an increasingly pressing safety concern. While safety-aligned LLMs can effectively defend against normal harmful queries, they remain vulnerable to such attacks. Existing defense methods primarily rely on fine-tuning or input modification, which often suffer from limited generalization and reduced utility. To address this, we introduce DETAM, a finetuning-free defense approach that improves the defensive capabilities against jailbreak attacks of LLMs via targeted attention modification. Specifically, we analyze the differences in attention scores between successful and unsuccessful defenses to identify the attention heads sensitive to jailbreak attacks. During inference, we reallocate attention to emphasize the user's core intention, minimizing interference from attack tokens. Our experimental results demonstrate that DETAM outperforms various baselines in jailbreak defense and exhibits robust generalization across different attacks and models, maintaining its effectiveness even on in-the-wild jailbreak data. Furthermore, in evaluating the model's utility, we incorporated over-defense datasets, which further validate the superior performance of our approach. The code will be released immediately upon acceptance."
  },
  {
    "title": "Risk-aware black-box portfolio construction using Bayesian optimization with adaptive weighted Lagrangian estimator",
    "url": "http://arxiv.org/abs/2504.13529v1",
    "arxiv_id": "2504.13529v1",
    "authors": [
      "Zinuo You",
      "John Cartlidge",
      "Karen Elliott",
      "Menghan Ge",
      "Daniel Gold"
    ],
    "published": "2025-04-18T07:40:24+00:00",
    "summary": "Existing portfolio management approaches are often black-box models due to safety and commercial issues in the industry. However, their performance can vary considerably whenever market conditions or internal trading strategies change. Furthermore, evaluating these non-transparent systems is expensive, where certain budgets limit observations of the systems. Therefore, optimizing performance while controlling the potential risk of these financial systems has become a critical challenge. This work presents a novel Bayesian optimization framework to optimize black-box portfolio management models under limited observations. In conventional Bayesian optimization settings, the objective function is to maximize the expectation of performance metrics. However, simply maximizing performance expectations leads to erratic optimization trajectories, which exacerbate risk accumulation in portfolio management. Meanwhile, this can lead to misalignment between the target distribution and the actual distribution of the black-box model. To mitigate this problem, we propose an adaptive weight Lagrangian estimator considering dual objective, which incorporates maximizing model performance and minimizing variance of model observations. Extensive experiments demonstrate the superiority of our approach over five backtest settings with three black-box stock portfolio management models. Ablation studies further verify the effectiveness of the proposed estimator."
  },
  {
    "title": "Monitor and Recover: A Paradigm for Future Research on Distribution Shift in Learning-Enabled Cyber-Physical Systems",
    "url": "http://arxiv.org/abs/2504.13484v1",
    "arxiv_id": "2504.13484v1",
    "authors": [
      "Vivian Lin",
      "Insup Lee"
    ],
    "published": "2025-04-18T05:48:35+00:00",
    "summary": "With the known vulnerability of neural networks to distribution shift, maintaining reliability in learning-enabled cyber-physical systems poses a salient challenge. In response, many existing methods adopt a detect and abstain methodology, aiming to detect distribution shift at inference time so that the learning-enabled component can abstain from decision-making. This approach, however, has limited use in real-world applications. We instead propose a monitor and recover paradigm as a promising direction for future research. This philosophy emphasizes 1) robust safety monitoring instead of distribution shift detection and 2) distribution shift recovery instead of abstention. We discuss two examples from our recent work."
  },
  {
    "title": "Safety Monitoring for Learning-Enabled Cyber-Physical Systems in Out-of-Distribution Scenarios",
    "url": "http://arxiv.org/abs/2504.13478v1",
    "arxiv_id": "2504.13478v1",
    "authors": [
      "Vivian Lin",
      "Ramneet Kaur",
      "Yahan Yang",
      "Souradeep Dutta",
      "Yiannis Kantaros",
      "Anirban Roy",
      "Susmit Jha",
      "Oleg Sokolsky",
      "Insup Lee"
    ],
    "published": "2025-04-18T05:42:37+00:00",
    "summary": "The safety of learning-enabled cyber-physical systems is compromised by the well-known vulnerabilities of deep neural networks to out-of-distribution (OOD) inputs. Existing literature has sought to monitor the safety of such systems by detecting OOD data. However, such approaches have limited utility, as the presence of an OOD input does not necessarily imply the violation of a desired safety property. We instead propose to directly monitor safety in a manner that is itself robust to OOD data. To this end, we predict violations of signal temporal logic safety specifications based on predicted future trajectories. Our safety monitor additionally uses a novel combination of adaptive conformal prediction and incremental learning. The former obtains probabilistic prediction guarantees even on OOD data, and the latter prevents overly conservative predictions. We evaluate the efficacy of the proposed approach in two case studies on safety monitoring: 1) predicting collisions of an F1Tenth car with static obstacles, and 2) predicting collisions of a race car with multiple dynamic obstacles. We find that adaptive conformal prediction obtains theoretical guarantees where other uncertainty quantification methods fail to do so. Additionally, combining adaptive conformal prediction and incremental learning for safety monitoring achieves high recall and timeliness while reducing loss in precision. We achieve these results even in OOD settings and outperform alternative methods."
  },
  {
    "title": "Testing the Fault-Tolerance of Multi-Sensor Fusion Perception in Autonomous Driving Systems",
    "url": "http://arxiv.org/abs/2504.13420v1",
    "arxiv_id": "2504.13420v1",
    "authors": [
      "Haoxiang Tian",
      "Wenqiang Ding",
      "Xingshuo Han",
      "Guoquan Wu",
      "An Guo",
      "Junqi Zhang. Wei Chen",
      "Jun Wei",
      "Tianwei Zhang"
    ],
    "published": "2025-04-18T02:37:55+00:00",
    "summary": "High-level Autonomous Driving Systems (ADSs), such as Google Waymo and Baidu Apollo, typically rely on multi-sensor fusion (MSF) based approaches to perceive their surroundings. This strategy increases perception robustness by combining the respective strengths of the camera and LiDAR and directly affects the safety-critical driving decisions of autonomous vehicles (AVs). However, in real-world autonomous driving scenarios, cameras and LiDAR are subject to various faults, which can probably significantly impact the decision-making and behaviors of ADSs. Existing MSF testing approaches only discovered corner cases that the MSF-based perception cannot accurately detected by MSF-based perception, while lacking research on how sensor faults affect the system-level behaviors of ADSs.   To address this gap, we conduct the first exploration of the fault tolerance of MSF perception-based ADS for sensor faults. In this paper, we systematically and comprehensively build fault models for cameras and LiDAR in AVs and inject them into the MSF perception-based ADS to test its behaviors in test scenarios. To effectively and efficiently explore the parameter spaces of sensor fault models, we design a feedback-guided differential fuzzer to discover the safety violations of MSF perception-based ADS caused by the injected sensor faults. We evaluate FADE on the representative and practical industrial ADS, Baidu Apollo. Our evaluation results demonstrate the effectiveness and efficiency of FADE, and we conclude some useful findings from the experimental results. To validate the findings in the physical world, we use a real Baidu Apollo 6.0 EDU autonomous vehicle to conduct the physical experiments, and the results show the practical significance of our findings."
  },
  {
    "title": "LangCoop: Collaborative Driving with Language",
    "url": "http://arxiv.org/abs/2504.13406v1",
    "arxiv_id": "2504.13406v1",
    "authors": [
      "Xiangbo Gao",
      "Yuheng Wu",
      "Rujia Wang",
      "Chenxi Liu",
      "Yang Zhou",
      "Zhengzhong Tu"
    ],
    "published": "2025-04-18T02:03:14+00:00",
    "summary": "Multi-agent collaboration holds great promise for enhancing the safety, reliability, and mobility of autonomous driving systems by enabling information sharing among multiple connected agents. However, existing multi-agent communication approaches are hindered by limitations of existing communication media, including high bandwidth demands, agent heterogeneity, and information loss. To address these challenges, we introduce LangCoop, a new paradigm for collaborative autonomous driving that leverages natural language as a compact yet expressive medium for inter-agent communication. LangCoop features two key innovations: Mixture Model Modular Chain-of-thought (M$^3$CoT) for structured zero-shot vision-language reasoning and Natural Language Information Packaging (LangPack) for efficiently packaging information into concise, language-based messages. Through extensive experiments conducted in the CARLA simulations, we demonstrate that LangCoop achieves a remarkable 96\\% reduction in communication bandwidth (< 2KB per message) compared to image-based communication, while maintaining competitive driving performance in the closed-loop evaluation."
  },
  {
    "title": "Towards a Multi-Agent Vision-Language System for Zero-Shot Novel Hazardous Object Detection for Autonomous Driving Safety",
    "url": "http://arxiv.org/abs/2504.13399v1",
    "arxiv_id": "2504.13399v1",
    "authors": [
      "Shashank Shriram",
      "Srinivasa Perisetla",
      "Aryan Keskar",
      "Harsha Krishnaswamy",
      "Tonko Emil Westerhof Bossen",
      "Andreas M\u00f8gelmose",
      "Ross Greer"
    ],
    "published": "2025-04-18T01:25:02+00:00",
    "summary": "Detecting anomalous hazards in visual data, particularly in video streams, is a critical challenge in autonomous driving. Existing models often struggle with unpredictable, out-of-label hazards due to their reliance on predefined object categories. In this paper, we propose a multimodal approach that integrates vision-language reasoning with zero-shot object detection to improve hazard identification and explanation. Our pipeline consists of a Vision-Language Model (VLM), a Large Language Model (LLM), in order to detect hazardous objects within a traffic scene. We refine object detection by incorporating OpenAI's CLIP model to match predicted hazards with bounding box annotations, improving localization accuracy. To assess model performance, we create a ground truth dataset by denoising and extending the foundational COOOL (Challenge-of-Out-of-Label) anomaly detection benchmark dataset with complete natural language descriptions for hazard annotations. We define a means of hazard detection and labeling evaluation on the extended dataset using cosine similarity. This evaluation considers the semantic similarity between the predicted hazard description and the annotated ground truth for each video. Additionally, we release a set of tools for structuring and managing large-scale hazard detection datasets. Our findings highlight the strengths and limitations of current vision-language-based approaches, offering insights into future improvements in autonomous hazard detection systems. Our models, scripts, and data can be found at https://github.com/mi3labucm/COOOLER.git"
  },
  {
    "title": "Leveraging Functional Encryption and Deep Learning for Privacy-Preserving Traffic Forecasting",
    "url": "http://arxiv.org/abs/2504.13267v1",
    "arxiv_id": "2504.13267v1",
    "authors": [
      "Isaac Adom",
      "Mohammmad Iqbal Hossain",
      "Hassan Mahmoud",
      "Ahmad Alsharif",
      "Mahmoud Nabil Mahmoud",
      "Yang Xiao"
    ],
    "published": "2025-04-17T18:21:55+00:00",
    "summary": "Over the past few years, traffic congestion has continuously plagued the nation's transportation system creating several negative impacts including longer travel times, increased pollution rates, and higher collision risks. To overcome these challenges, Intelligent Transportation Systems (ITS) aim to improve mobility and vehicular systems, ensuring higher levels of safety by utilizing cutting-edge technologies, sophisticated sensing capabilities, and innovative algorithms. Drivers' participatory sensing, current/future location reporting, and machine learning algorithms have considerably improved real-time congestion monitoring and future traffic management. However, each driver's sensitive spatiotemporal location information can create serious privacy concerns. To address these challenges, we propose in this paper a secure, privacy-preserving location reporting and traffic forecasting system that guarantees privacy protection of driver data while maintaining high traffic forecasting accuracy. Our novel k-anonymity scheme utilizes functional encryption to aggregate encrypted location information submitted by drivers while ensuring the privacy of driver location data. Additionally, using the aggregated encrypted location information as input, this research proposes a deep learning model that incorporates a Convolutional-Long Short-Term Memory (Conv-LSTM) module to capture spatial and short-term temporal features and a Bidirectional Long Short-Term Memory (Bi-LSTM) module to recover long-term periodic patterns for traffic forecasting. With extensive evaluation on real datasets, we demonstrate the effectiveness of the proposed scheme with less than 10% mean absolute error for a 60-minute forecasting horizon, all while protecting driver privacy."
  },
  {
    "title": "Generate, but Verify: Reducing Hallucination in Vision-Language Models with Retrospective Resampling",
    "url": "http://arxiv.org/abs/2504.13169v1",
    "arxiv_id": "2504.13169v1",
    "authors": [
      "Tsung-Han Wu",
      "Heekyung Lee",
      "Jiaxin Ge",
      "Joseph E. Gonzalez",
      "Trevor Darrell",
      "David M. Chan"
    ],
    "published": "2025-04-17T17:59:22+00:00",
    "summary": "Vision-Language Models (VLMs) excel at visual understanding but often suffer from visual hallucinations, where they generate descriptions of nonexistent objects, actions, or concepts, posing significant risks in safety-critical applications. Existing hallucination mitigation methods typically follow one of two paradigms: generation adjustment, which modifies decoding behavior to align text with visual inputs, and post-hoc verification, where external models assess and correct outputs. While effective, generation adjustment methods often rely on heuristics and lack correction mechanisms, while post-hoc verification is complicated, typically requiring multiple models and tending to reject outputs rather than refine them. In this work, we introduce REVERSE, a unified framework that integrates hallucination-aware training with on-the-fly self-verification. By leveraging a new hallucination-verification dataset containing over 1.3M semi-synthetic samples, along with a novel inference-time retrospective resampling technique, our approach enables VLMs to both detect hallucinations during generation and dynamically revise those hallucinations. Our evaluations show that REVERSE achieves state-of-the-art hallucination reduction, outperforming the best existing methods by up to 12% on CHAIR-MSCOCO and 28% on HaloQuest. Our dataset, model, and code are available at: https://reverse-vlm.github.io."
  },
  {
    "title": "Energy-Based Reward Models for Robust Language Model Alignment",
    "url": "http://arxiv.org/abs/2504.13134v1",
    "arxiv_id": "2504.13134v1",
    "authors": [
      "Anamika Lochab",
      "Ruqi Zhang"
    ],
    "published": "2025-04-17T17:47:15+00:00",
    "summary": "Reward models (RMs) are essential for aligning Large Language Models (LLMs) with human preferences. However, they often struggle with capturing complex human preferences and generalizing to unseen data. To address these challenges, we introduce Energy-Based Reward Model (EBRM), a lightweight post-hoc refinement framework that enhances RM robustness and generalization. EBRM models the reward distribution explicitly, capturing uncertainty in human preferences and mitigating the impact of noisy or misaligned annotations. It achieves this through conflict-aware data filtering, label-noise-aware contrastive training, and hybrid initialization. Notably, EBRM enhances RMs without retraining, making it computationally efficient and adaptable across different models and tasks. Empirical evaluations on RM benchmarks demonstrate significant improvements in both robustness and generalization, achieving up to a 5.97% improvement in safety-critical alignment tasks compared to standard RMs. Furthermore, reinforcement learning experiments confirm that our refined rewards enhance alignment quality, effectively delaying reward hacking. These results demonstrate our approach as a scalable and effective enhancement for existing RMs and alignment pipelines. The code is available at EBRM."
  },
  {
    "title": "LLMs Meet Finance: Fine-Tuning Foundation Models for the Open FinLLM Leaderboard",
    "url": "http://arxiv.org/abs/2504.13125v1",
    "arxiv_id": "2504.13125v1",
    "authors": [
      "Varun Rao",
      "Youran Sun",
      "Mahendra Kumar",
      "Tejas Mutneja",
      "Agastya Mukherjee",
      "Haizhao Yang"
    ],
    "published": "2025-04-17T17:42:02+00:00",
    "summary": "This paper investigates the application of large language models (LLMs) to financial tasks. We fine-tuned foundation models using the Open FinLLM Leaderboard as a benchmark. Building on Qwen2.5 and Deepseek-R1, we employed techniques including supervised fine-tuning (SFT), direct preference optimization (DPO), and reinforcement learning (RL) to enhance their financial capabilities. The fine-tuned models demonstrated substantial performance gains across a wide range of financial tasks. Moreover, we measured the data scaling law in the financial domain. Our work demonstrates the potential of large language models (LLMs) in financial applications."
  },
  {
    "title": "Accuracy is Not Agreement: Expert-Aligned Evaluation of Crash Narrative Classification Models",
    "url": "http://arxiv.org/abs/2504.13068v1",
    "arxiv_id": "2504.13068v1",
    "authors": [
      "Sudesh Ramesh Bhagat",
      "Ibne Farabi Shihab",
      "Anuj Sharma"
    ],
    "published": "2025-04-17T16:29:08+00:00",
    "summary": "This study explores the relationship between deep learning (DL) model accuracy and expert agreement in the classification of crash narratives. We evaluate five DL models -- including BERT variants, the Universal Sentence Encoder (USE), and a zero-shot classifier -- against expert-labeled data and narrative text. The analysis is further extended to four large language models (LLMs): GPT-4, LLaMA 3, Qwen, and Claude. Our results reveal a counterintuitive trend: models with higher technical accuracy often exhibit lower agreement with domain experts, whereas LLMs demonstrate greater expert alignment despite relatively lower accuracy scores. To quantify and interpret model-expert agreement, we employ Cohen's Kappa, Principal Component Analysis (PCA), and SHAP-based explainability techniques. Findings indicate that expert-aligned models tend to rely more on contextual and temporal language cues, rather than location-specific keywords. These results underscore that accuracy alone is insufficient for evaluating models in safety-critical NLP applications. We advocate for incorporating expert agreement as a complementary metric in model evaluation frameworks and highlight the promise of LLMs as interpretable, scalable tools for crash analysis pipelines."
  },
  {
    "title": "GraphAttack: Exploiting Representational Blindspots in LLM Safety Mechanisms",
    "url": "http://arxiv.org/abs/2504.13052v1",
    "arxiv_id": "2504.13052v1",
    "authors": [
      "Sinan He",
      "An Wang"
    ],
    "published": "2025-04-17T16:09:12+00:00",
    "summary": "Large Language Models (LLMs) have been equipped with safety mechanisms to prevent harmful outputs, but these guardrails can often be bypassed through \"jailbreak\" prompts. This paper introduces a novel graph-based approach to systematically generate jailbreak prompts through semantic transformations. We represent malicious prompts as nodes in a graph structure with edges denoting different transformations, leveraging Abstract Meaning Representation (AMR) and Resource Description Framework (RDF) to parse user goals into semantic components that can be manipulated to evade safety filters. We demonstrate a particularly effective exploitation vector by instructing LLMs to generate code that realizes the intent described in these semantic graphs, achieving success rates of up to 87% against leading commercial LLMs. Our analysis reveals that contextual framing and abstraction are particularly effective at circumventing safety measures, highlighting critical gaps in current safety alignment techniques that focus primarily on surface-level patterns. These findings provide insights for developing more robust safeguards against structured semantic attacks. Our research contributes both a theoretical framework and practical methodology for systematically stress-testing LLM safety mechanisms."
  },
  {
    "title": "QI-MPC: A Hybrid Quantum-Inspired Model Predictive Control for Learning Optimal Policies",
    "url": "http://arxiv.org/abs/2504.13041v1",
    "arxiv_id": "2504.13041v1",
    "authors": [
      "Muhammad Al-Zafar Khan",
      "Jamal Al-Karaki"
    ],
    "published": "2025-04-17T15:55:37+00:00",
    "summary": "In this paper, we present Quantum-Inspired Model Predictive Control (QIMPC), an approach that uses Variational Quantum Circuits (VQCs) to learn control polices in MPC problems. The viability of the approach is tested in five experiments: A target-tracking control strategy, energy-efficient building climate control, autonomous vehicular dynamics, the simple pendulum, and the compound pendulum. Three safety guarantees were established for the approach, and the experiments gave the motivation for two important theoretical results that, in essence, identify systems for which the approach works best."
  },
  {
    "title": "Safe Physics-Informed Machine Learning for Dynamics and Control",
    "url": "http://arxiv.org/abs/2504.12952v1",
    "arxiv_id": "2504.12952v1",
    "authors": [
      "Jan Drgona",
      "Truong X. Nghiem",
      "Thomas Beckers",
      "Mahyar Fazlyab",
      "Enrique Mallada",
      "Colin Jones",
      "Draguna Vrabie",
      "Steven L. Brunton",
      "Rolf Findeisen"
    ],
    "published": "2025-04-17T13:52:55+00:00",
    "summary": "This tutorial paper focuses on safe physics-informed machine learning in the context of dynamics and control, providing a comprehensive overview of how to integrate physical models and safety guarantees. As machine learning techniques enhance the modeling and control of complex dynamical systems, ensuring safety and stability remains a critical challenge, especially in safety-critical applications like autonomous vehicles, robotics, medical decision-making, and energy systems. We explore various approaches for embedding and ensuring safety constraints, such as structural priors, Lyapunov functions, Control Barrier Functions, predictive control, projections, and robust optimization techniques, ensuring that the learned models respect stability and safety criteria. Additionally, we delve into methods for uncertainty quantification and safety verification, including reachability analysis and neural network verification tools, which help validate that control policies remain within safe operating bounds even in uncertain environments. The paper includes illustrative examples demonstrating the implementation aspects of safe learning frameworks that combine the strengths of data-driven approaches with the rigor of physical principles, offering a path toward the safe control of complex dynamical systems."
  },
  {
    "title": "In Which Areas of Technical AI Safety Could Geopolitical Rivals Cooperate?",
    "url": "http://arxiv.org/abs/2504.12914v1",
    "arxiv_id": "2504.12914v1",
    "authors": [
      "Ben Bucknall",
      "Saad Siddiqui",
      "Lara Thurnherr",
      "Conor McGurk",
      "Ben Harack",
      "Anka Reuel",
      "Patricia Paskov",
      "Casey Mahoney",
      "S\u00f6ren Mindermann",
      "Scott Singer",
      "Vinay Hiremath",
      "Charbel-Rapha\u00ebl Segerie",
      "Oscar Delaney",
      "Alessandro Abate",
      "Fazl Barez",
      "Michael K. Cohen",
      "Philip Torr",
      "Ferenc Husz\u00e1r",
      "Anisoara Calinescu",
      "Gabriel Davis Jones",
      "Yoshua Bengio",
      "Robert Trager"
    ],
    "published": "2025-04-17T13:03:56+00:00",
    "summary": "International cooperation is common in AI research, including between geopolitical rivals. While many experts advocate for greater international cooperation on AI safety to address shared global risks, some view cooperation on AI with suspicion, arguing that it can pose unacceptable risks to national security. However, the extent to which cooperation on AI safety poses such risks, as well as provides benefits, depends on the specific area of cooperation. In this paper, we consider technical factors that impact the risks of international cooperation on AI safety research, focusing on the degree to which such cooperation can advance dangerous capabilities, result in the sharing of sensitive information, or provide opportunities for harm. We begin by why nations historically cooperate on strategic technologies and analyse current US-China cooperation in AI as a case study. We further argue that existing frameworks for managing associated risks can be supplemented with consideration of key risks specific to cooperation on technical AI safety research. Through our analysis, we find that research into AI verification mechanisms and shared protocols may be suitable areas for such cooperation. Through this analysis we aim to help researchers and governments identify and mitigate the risks of international cooperation on AI safety research, so that the benefits of cooperation can be fully realised."
  },
  {
    "title": "UncAD: Towards Safe End-to-end Autonomous Driving via Online Map Uncertainty",
    "url": "http://arxiv.org/abs/2504.12826v1",
    "arxiv_id": "2504.12826v1",
    "authors": [
      "Pengxuan Yang",
      "Yupeng Zheng",
      "Qichao Zhang",
      "Kefei Zhu",
      "Zebin Xing",
      "Qiao Lin",
      "Yun-Fu Liu",
      "Zhiguo Su",
      "Dongbin Zhao"
    ],
    "published": "2025-04-17T10:40:36+00:00",
    "summary": "End-to-end autonomous driving aims to produce planning trajectories from raw sensors directly. Currently, most approaches integrate perception, prediction, and planning modules into a fully differentiable network, promising great scalability. However, these methods typically rely on deterministic modeling of online maps in the perception module for guiding or constraining vehicle planning, which may incorporate erroneous perception information and further compromise planning safety. To address this issue, we delve into the importance of online map uncertainty for enhancing autonomous driving safety and propose a novel paradigm named UncAD. Specifically, UncAD first estimates the uncertainty of the online map in the perception module. It then leverages the uncertainty to guide motion prediction and planning modules to produce multi-modal trajectories. Finally, to achieve safer autonomous driving, UncAD proposes an uncertainty-collision-aware planning selection strategy according to the online map uncertainty to evaluate and select the best trajectory. In this study, we incorporate UncAD into various state-of-the-art (SOTA) end-to-end methods. Experiments on the nuScenes dataset show that integrating UncAD, with only a 1.9% increase in parameters, can reduce collision rates by up to 26% and drivable area conflict rate by up to 42%. Codes, pre-trained models, and demo videos can be accessed at https://github.com/pengxuanyang/UncAD."
  },
  {
    "title": "Supporting Urban Low-Altitude Economy: Channel Gain Map Inference Based on 3D Conditional GAN",
    "url": "http://arxiv.org/abs/2504.12794v1",
    "arxiv_id": "2504.12794v1",
    "authors": [
      "Yonghao Wang",
      "Ruoguang Li",
      "Di Wu",
      "Jiaqi Chen",
      "Yong Zeng"
    ],
    "published": "2025-04-17T09:55:03+00:00",
    "summary": "The advancement of advanced air mobility (AAM) in recent years has given rise to the concept of low-altitude economy (LAE). However, the diverse flight activities associated with the emerging LAE applications in urban scenarios confront complex physical environments, which urgently necessitates ubiquitous and reliable communication to guarantee the operation safety of the low-altitude aircraft. As one of promising technologies for the sixth generation (6G) mobile networks, channel knowledge map (CKM) enables the environment-aware communication by constructing a site-specific dataset, thereby providing a priori on-site information for the aircraft to obtain the channel state information (CSI) at arbitrary locations with much reduced online overhead. Diverse base station (BS) deployments in the three-dimensional (3D) urban low-altitude environment require efficient 3D CKM construction to capture spatial channel characteristics with less overhead. Towards this end, this paper proposes a 3D channel gain map (CGM) inference method based on a 3D conditional generative adversarial network (3D-CGAN). Specifically, we first analyze the potential deployment types of BSs in urban low-altitude scenario, and investigate the CGM representation with the corresponding 3D channel gain model. The framework of the proposed 3D-CGAN is then discussed, which is trained by a dataset consisting of existing CGMs. Consequently, the trained 3D-CGAN is capable of inferring the corresponding CGM only based on the BS coordinate without additional measurement. The simulation results demonstrate that the CGMs inferred by the proposed 3D-CGAN outperform those of the benchmark schemes, which can accurately reflect the radio propagation condition in 3D environment."
  },
  {
    "title": "On Error Classification from Physiological Signals within Airborne Environment",
    "url": "http://arxiv.org/abs/2504.12769v1",
    "arxiv_id": "2504.12769v1",
    "authors": [
      "Niall McGuire",
      "Yashar Moshfeghi"
    ],
    "published": "2025-04-17T09:08:21+00:00",
    "summary": "Human error remains a critical concern in aviation safety, contributing to 70-80% of accidents despite technological advancements. While physiological measures show promise for error detection in laboratory settings, their effectiveness in dynamic flight environments remains underexplored. Through live flight trials with nine commercial pilots, we investigated whether established error-detection approaches maintain accuracy during actual flight operations. Participants completed standardized multi-tasking scenarios across conditions ranging from laboratory settings to straight-and-level flight and 2G manoeuvres while we collected synchronized physiological data. Our findings demonstrate that EEG-based classification maintains high accuracy (87.83%) during complex flight manoeuvres, comparable to laboratory performance (89.23%). Eye-tracking showed moderate performance (82.50\\%), while ECG performed near chance level (51.50%). Classification accuracy remained stable across flight conditions, with minimal degradation during 2G manoeuvres. These results provide the first evidence that physiological error detection can translate effectively to operational aviation environments."
  },
  {
    "title": "Falcon: Advancing Asynchronous BFT Consensus for Lower Latency and Enhanced Throughput",
    "url": "http://arxiv.org/abs/2504.12766v1",
    "arxiv_id": "2504.12766v1",
    "authors": [
      "Xiaohai Dai",
      "Chaozheng Ding",
      "Wei Li",
      "Jiang Xiao",
      "Bolin Zhang",
      "Chen Yu",
      "Albert Y. Zomaya",
      "Hai Jin"
    ],
    "published": "2025-04-17T09:03:55+00:00",
    "summary": "Asynchronous Byzantine Fault Tolerant (BFT) consensus protocols have garnered significant attention with the rise of blockchain technology. A typical asynchronous protocol is designed by executing sequential instances of the Asynchronous Common Sub-seQuence (ACSQ). The ACSQ protocol consists of two primary components: the Asynchronous Common Subset (ACS) protocol and a block sorting mechanism, with the ACS protocol comprising two stages: broadcast and agreement. However, current protocols encounter three critical issues: high latency arising from the execution of the agreement stage, latency instability due to the integral-sorting mechanism, and reduced throughput caused by block discarding. To address these issues,we propose Falcon, an asynchronous BFT protocol that achieves low latency and enhanced throughput. Falcon introduces a novel broadcast protocol, Graded Broadcast (GBC), which enables a block to be included in the ACS set directly, bypassing the agreement stage and thereby reducing latency. To ensure safety, Falcon incorporates a new binary agreement protocol called Asymmetrical Asynchronous Binary Agreement (AABA), designed to complement GBC. Additionally, Falcon employs a partial-sorting mechanism, allowing continuous rather than simultaneous block committing, enhancing latency stability. Finally, we incorporate an agreement trigger that, before its activation, enables nodes to wait for more blocks to be delivered and committed, thereby boosting throughput. We conduct a series of experiments to evaluate Falcon, demonstrating its superior performance."
  },
  {
    "title": "Distributed Intelligent Sensing and Communications for 6G: Architecture and Use Cases",
    "url": "http://arxiv.org/abs/2504.12765v1",
    "arxiv_id": "2504.12765v1",
    "authors": [
      "Kyriakos Stylianopoulos",
      "Giyyarpuram Madhusudan",
      "Guillaume Jornod",
      "Sami Mekki",
      "Francesca Costanzo",
      "Hui Chen",
      "Placido Mursia",
      "Maurizio Crozzoli",
      "Emilio Calvanese Strinati",
      "George C. Alexandropoulos",
      "Henk Wymeersch"
    ],
    "published": "2025-04-17T09:02:36+00:00",
    "summary": "The Distributed Intelligent Sensing and Communication (DISAC) framework redefines Integrated Sensing and Communication (ISAC) for 6G by leveraging distributed architectures to enhance scalability, adaptability, and resource efficiency. This paper presents key architectural enablers, including advanced data representation, seamless target handover, support for heterogeneous devices, and semantic integration. Two use cases illustrate the transformative potential of DISAC: smart factory shop floors and Vulnerable Road User (VRU) protection at smart intersections. These scenarios demonstrate significant improvements in precision, safety, and operational efficiency compared to traditional ISAC systems. The preliminary DISAC architecture incorporates intelligent data processing, distributed coordination, and emerging technologies such as Reconfigurable Intelligent Surfaces (RIS) to meet 6G's stringent requirements. By addressing critical challenges in sensing accuracy, latency, and real-time decision-making, DISAC positions itself as a cornerstone for next-generation wireless networks, advancing innovation in dynamic and complex environments."
  },
  {
    "title": "Incorporating a Deep Neural Network into Moving Horizon Estimation for Embedded Thermal Torque Derating of an Electric Machine",
    "url": "http://arxiv.org/abs/2504.12736v1",
    "arxiv_id": "2504.12736v1",
    "authors": [
      "Alexander Winkler",
      "Pranav Shah",
      "Katrin Baumg\u00e4rtner",
      "Vasu Sharma",
      "David Gordon",
      "Jakob Andert"
    ],
    "published": "2025-04-17T08:24:32+00:00",
    "summary": "This study introduces a novel state estimation framework that incorporates Deep Neural Networks (DNNs) into Moving Horizon Estimation (MHE), shifting from traditional physics-based models to rapidly developed data-driven techniques. A DNN model with Long Short-Term Memory (LSTM) nodes is trained on synthetic data generated by a high-fidelity thermal model of a Permanent Magnet Synchronous Machine (PMSM), which undergoes thermal derating as part of the torque control strategy in a battery electric vehicle. The MHE is constructed by integrating the trained DNN with a simplified driving dynamics model in a discrete-time formulation, incorporating the LSTM hidden and cell states in the state vector to retain system dynamics. The resulting optimal control problem (OCP) is formulated as a nonlinear program (NLP) and implemented using the acados framework. Model-in-the-loop (MiL) simulations demonstrate accurate temperature estimation, even under noisy sensor conditions or failures. Achieving threefold real-time capability on embedded hardware confirms the feasibility of the approach for practical deployment. The primary focus of this study is to assess the feasibility of the MHE framework using a DNN-based plant model instead of focusing on quantitative comparisons of vehicle performance. Overall, this research highlights the potential of DNN-based MHE for real-time, safety-critical applications by combining the strengths of model-based and data-driven methods."
  },
  {
    "title": "Collaborative Perception Datasets for Autonomous Driving: A Review",
    "url": "http://arxiv.org/abs/2504.12696v1",
    "arxiv_id": "2504.12696v1",
    "authors": [
      "Naibang Wang",
      "Deyong Shang",
      "Yan Gong",
      "Xiaoxi Hu",
      "Ziying Song",
      "Lei Yang",
      "Yuhan Huang",
      "Xiaoyu Wang",
      "Jianli Lu"
    ],
    "published": "2025-04-17T06:49:21+00:00",
    "summary": "Collaborative perception has attracted growing interest from academia and industry due to its potential to enhance perception accuracy, safety, and robustness in autonomous driving through multi-agent information fusion. With the advancement of Vehicle-to-Everything (V2X) communication, numerous collaborative perception datasets have emerged, varying in cooperation paradigms, sensor configurations, data sources, and application scenarios. However, the absence of systematic summarization and comparative analysis hinders effective resource utilization and standardization of model evaluation. As the first comprehensive review focused on collaborative perception datasets, this work reviews and compares existing resources from a multi-dimensional perspective. We categorize datasets based on cooperation paradigms, examine their data sources and scenarios, and analyze sensor modalities and supported tasks. A detailed comparative analysis is conducted across multiple dimensions. We also outline key challenges and future directions, including dataset scalability, diversity, domain adaptation, standardization, privacy, and the integration of large language models. To support ongoing research, we provide a continuously updated online repository of collaborative perception datasets and related literature: https://github.com/frankwnb/Collaborative-Perception-Datasets-for-Autonomous-Driving."
  },
  {
    "title": "Embodied-R: Collaborative Framework for Activating Embodied Spatial Reasoning in Foundation Models via Reinforcement Learning",
    "url": "http://arxiv.org/abs/2504.12680v1",
    "arxiv_id": "2504.12680v1",
    "authors": [
      "Baining Zhao",
      "Ziyou Wang",
      "Jianjie Fang",
      "Chen Gao",
      "Fanhang Man",
      "Jinqiang Cui",
      "Xin Wang",
      "Xinlei Chen",
      "Yong Li",
      "Wenwu Zhu"
    ],
    "published": "2025-04-17T06:16:11+00:00",
    "summary": "Humans can perceive and reason about spatial relationships from sequential visual observations, such as egocentric video streams. However, how pretrained models acquire such abilities, especially high-level reasoning, remains unclear. This paper introduces Embodied-R, a collaborative framework combining large-scale Vision-Language Models (VLMs) for perception and small-scale Language Models (LMs) for reasoning. Using Reinforcement Learning (RL) with a novel reward system considering think-answer logical consistency, the model achieves slow-thinking capabilities with limited computational resources. After training on only 5k embodied video samples, Embodied-R with a 3B LM matches state-of-the-art multimodal reasoning models (OpenAI-o1, Gemini-2.5-pro) on both in-distribution and out-of-distribution embodied spatial reasoning tasks. Embodied-R also exhibits emergent thinking patterns such as systematic analysis and contextual integration. We further explore research questions including response length, training on VLM, strategies for reward design, and differences in model generalization after SFT (Supervised Fine-Tuning) and RL training."
  },
  {
    "title": "Predicting Driver's Perceived Risk: a Model Based on Semi-Supervised Learning Strategy",
    "url": "http://arxiv.org/abs/2504.12665v1",
    "arxiv_id": "2504.12665v1",
    "authors": [
      "Siwei Huang",
      "Chenhao Yang",
      "Chuan Hu"
    ],
    "published": "2025-04-17T05:50:33+00:00",
    "summary": "Drivers' perception of risk determines their acceptance, trust, and use of the Automated Driving Systems (ADSs). However, perceived risk is subjective and difficult to evaluate using existing methods. To address this issue, a driver's subjective perceived risk (DSPR) model is proposed, regarding perceived risk as a dynamically triggered mechanism with anisotropy and attenuation. 20 participants are recruited for a driver-in-the-loop experiment to report their real-time subjective risk ratings (SRRs) when experiencing various automatic driving scenarios. A convolutional neural network and bidirectional long short-term memory network with temporal pattern attention (CNN-Bi-LSTM-TPA) is embedded into a semi-supervised learning strategy to predict SRRs, aiming to reduce data noise caused by subjective randomness of participants. The results illustrate that DSPR achieves the highest prediction accuracy of 87.91% in predicting SRRs, compared to three state-of-the-art risk models. The semi-supervised strategy improves accuracy by 20.12%. Besides, CNN-Bi-LSTM-TPA network presents the highest accuracy among four different LSTM structures. This study offers an effective method for assessing driver's perceived risk, providing support for the safety enhancement of ADS and driver's trust improvement."
  },
  {
    "title": "VLMGuard-R1: Proactive Safety Alignment for VLMs via Reasoning-Driven Prompt Optimization",
    "url": "http://arxiv.org/abs/2504.12661v1",
    "arxiv_id": "2504.12661v1",
    "authors": [
      "Menglan Chen",
      "Xianghe Pang",
      "Jingjing Dong",
      "WenHao Wang",
      "Yaxin Du",
      "Siheng Chen"
    ],
    "published": "2025-04-17T05:46:41+00:00",
    "summary": "Aligning Vision-Language Models (VLMs) with safety standards is essential to mitigate risks arising from their multimodal complexity, where integrating vision and language unveils subtle threats beyond the reach of conventional safeguards. Inspired by the insight that reasoning across modalities is key to preempting intricate vulnerabilities, we propose a novel direction for VLM safety: multimodal reasoning-driven prompt rewriting. To this end, we introduce VLMGuard-R1, a proactive framework that refines user inputs through a reasoning-guided rewriter, dynamically interpreting text-image interactions to deliver refined prompts that bolster safety across diverse VLM architectures without altering their core parameters. To achieve this, we devise a three-stage reasoning pipeline to synthesize a dataset that trains the rewriter to infer subtle threats, enabling tailored, actionable responses over generic refusals. Extensive experiments across three benchmarks with five VLMs reveal that VLMGuard-R1 outperforms four baselines. In particular, VLMGuard-R1 achieves a remarkable 43.59\\% increase in average safety across five models on the SIUO benchmark."
  },
  {
    "title": "Photon Calibration Performance of KAGRA during the 4th Joint Observing Run (O4)",
    "url": "http://arxiv.org/abs/2504.12657v1",
    "arxiv_id": "2504.12657v1",
    "authors": [
      "Dan Chen",
      "Shingo Hido",
      "Darkhan Tuyenbayev",
      "Dripta Bhattacharjee",
      "Nobuyuki Kanda",
      "Richard Savage",
      "Rishabh Bajpai",
      "Sadakazu Haino",
      "Takahiro Sawada",
      "Takahiro Yamamoto",
      "Takayuki Tomaru",
      "Yoshiki Moriwaki"
    ],
    "published": "2025-04-17T05:36:31+00:00",
    "summary": "KAGRA is a kilometer-scale cryogenic gravitational-wave (GW) detector in Japan. It joined the 4th joint observing run (O4) in May 2023 in collaboration with the Laser Interferometer GW Observatory (LIGO) in the USA, and Virgo in Italy. After one month of observations, KAGRA entered a break period to enhance its sensitivity to GWs, and it is planned to rejoin O4 before its scheduled end in October 2025. To accurately recover the information encoded in the GW signals, it is essential to properly calibrate the observed signals. We employ a photon calibration (Pcal) system as a reference signal injector to calibrate the output signals obtained from the telescope. In ideal future conditions, the uncertainty in Pcal could dominate the uncertainty in the observed data. In this paper, we present the methods used to estimate the uncertainty in the Pcal systems employed during KAGRA O4 and report an estimated system uncertainty of 0.79%, which is three times lower than the uncertainty achieved in the previous 3rd joint observing run (O3) in 2020. Additionally, we investigate the uncertainty in the Pcal laser power sensors, which had the highest impact on the Pcal uncertainty, and estimate the beam positions on the KAGRA main mirror, which had the second highest impact. The Pcal systems in KAGRA are the first fully functional calibration systems for a cryogenic GW telescope. To avoid interference with the KAGRA cryogenic systems, the Pcal systems incorporate unique features regarding their placement and the use of telephoto cameras, which can capture images of the mirror surface at almost normal incidence. As future GW telescopes, such as the Einstein Telescope, are expected to adopt cryogenic techniques, the performance of the KAGRA Pcal systems can serve as a valuable reference."
  },
  {
    "title": "Graph-based Path Planning with Dynamic Obstacle Avoidance for Autonomous Parking",
    "url": "http://arxiv.org/abs/2504.12616v1",
    "arxiv_id": "2504.12616v1",
    "authors": [
      "Farhad Nawaz",
      "Minjun Sung",
      "Darshan Gadginmath",
      "Jovin D'sa",
      "Sangjae Bae",
      "David Isele",
      "Nadia Figueroa",
      "Nikolai Matni",
      "Faizan M. Tariq"
    ],
    "published": "2025-04-17T03:43:20+00:00",
    "summary": "Safe and efficient path planning in parking scenarios presents a significant challenge due to the presence of cluttered environments filled with static and dynamic obstacles. To address this, we propose a novel and computationally efficient planning strategy that seamlessly integrates the predictions of dynamic obstacles into the planning process, ensuring the generation of collision-free paths. Our approach builds upon the conventional Hybrid A star algorithm by introducing a time-indexed variant that explicitly accounts for the predictions of dynamic obstacles during node exploration in the graph, thus enabling dynamic obstacle avoidance. We integrate the time-indexed Hybrid A star algorithm within an online planning framework to compute local paths at each planning step, guided by an adaptively chosen intermediate goal. The proposed method is validated in diverse parking scenarios, including perpendicular, angled, and parallel parking. Through simulations, we showcase our approach's potential in greatly improving the efficiency and safety when compared to the state of the art spline-based planning method for parking situations."
  },
  {
    "title": "ZeroSumEval: Scaling LLM Evaluation with Inter-Model Competition",
    "url": "http://arxiv.org/abs/2504.12562v1",
    "arxiv_id": "2504.12562v1",
    "authors": [
      "Haidar Khan",
      "Hisham A. Alyahya",
      "Yazeed Alnumay",
      "M Saiful Bari",
      "B\u00fclent Yener"
    ],
    "published": "2025-04-17T01:23:50+00:00",
    "summary": "Evaluating the capabilities of Large Language Models (LLMs) has traditionally relied on static benchmark datasets, human assessments, or model-based evaluations - methods that often suffer from overfitting, high costs, and biases. ZeroSumEval is a novel competition-based evaluation protocol that leverages zero-sum games to assess LLMs with dynamic benchmarks that resist saturation. ZeroSumEval encompasses a diverse suite of games, including security challenges (PyJail), classic games (Chess, Liar's Dice, Poker), knowledge tests (MathQuiz), and persuasion challenges (Gandalf, Debate). These games are designed to evaluate a range of AI capabilities such as strategic reasoning, planning, knowledge application, and creativity. Building upon recent studies that highlight the effectiveness of game-based evaluations for LLMs, ZeroSumEval enhances these approaches by providing a standardized and extensible framework. To demonstrate this, we conduct extensive experiments with >7000 simulations across 7 games and 13 models. Our results show that while frontier models from the GPT and Claude families can play common games and answer questions, they struggle to play games that require creating novel and challenging questions. We also observe that models cannot reliably jailbreak each other and fail generally at tasks requiring creativity. We release our code at https://github.com/facebookresearch/ZeroSumEval."
  },
  {
    "title": "TraCeS: Trajectory Based Credit Assignment From Sparse Safety Feedback",
    "url": "http://arxiv.org/abs/2504.12557v1",
    "arxiv_id": "2504.12557v1",
    "authors": [
      "Siow Meng Low",
      "Akshat Kumar"
    ],
    "published": "2025-04-17T01:11:08+00:00",
    "summary": "In safe reinforcement learning (RL), auxiliary safety costs are used to align the agent to safe decision making. In practice, safety constraints, including cost functions and budgets, are unknown or hard to specify, as it requires anticipation of all possible unsafe behaviors. We therefore address a general setting where the true safety definition is unknown, and has to be learned from sparsely labeled data. Our key contributions are: first, we design a safety model that performs credit assignment to estimate each decision step's impact on the overall safety using a dataset of diverse trajectories and their corresponding binary safety labels (i.e., whether the corresponding trajectory is safe/unsafe). Second, we illustrate the architecture of our safety model to demonstrate its ability to learn a separate safety score for each timestep. Third, we reformulate the safe RL problem using the proposed safety model and derive an effective algorithm to optimize a safe yet rewarding policy. Finally, our empirical results corroborate our findings and show that this approach is effective in satisfying unknown safety definition, and scalable to various continuous control tasks."
  },
  {
    "title": "ELAB: Extensive LLM Alignment Benchmark in Persian Language",
    "url": "http://arxiv.org/abs/2504.12553v1",
    "arxiv_id": "2504.12553v1",
    "authors": [
      "Zahra Pourbahman",
      "Fatemeh Rajabi",
      "Mohammadhossein Sadeghi",
      "Omid Ghahroodi",
      "Somaye Bakhshaei",
      "Arash Amini",
      "Reza Kazemi",
      "Mahdieh Soleymani Baghshah"
    ],
    "published": "2025-04-17T00:50:41+00:00",
    "summary": "This paper presents a comprehensive evaluation framework for aligning Persian Large Language Models (LLMs) with critical ethical dimensions, including safety, fairness, and social norms. It addresses the gaps in existing LLM evaluation frameworks by adapting them to Persian linguistic and cultural contexts. This benchmark creates three types of Persian-language benchmarks: (i) translated data, (ii) new data generated synthetically, and (iii) new naturally collected data. We translate Anthropic Red Teaming data, AdvBench, HarmBench, and DecodingTrust into Persian. Furthermore, we create ProhibiBench-fa, SafeBench-fa, FairBench-fa, and SocialBench-fa as new datasets to address harmful and prohibited content in indigenous culture. Moreover, we collect extensive dataset as GuardBench-fa to consider Persian cultural norms. By combining these datasets, our work establishes a unified framework for evaluating Persian LLMs, offering a new approach to culturally grounded alignment evaluation. A systematic evaluation of Persian LLMs is performed across the three alignment aspects: safety (avoiding harmful content), fairness (mitigating biases), and social norms (adhering to culturally accepted behaviors). We present a publicly available leaderboard that benchmarks Persian LLMs with respect to safety, fairness, and social norms at: https://huggingface.co/spaces/MCILAB/LLM_Alignment_Evaluation."
  },
  {
    "title": "Knowledge Acquisition on Mass-shooting Events via LLMs for AI-Driven Justice",
    "url": "http://arxiv.org/abs/2504.12545v1",
    "arxiv_id": "2504.12545v1",
    "authors": [
      "Benign John Ihugba",
      "Afsana Nasrin",
      "Ling Wu",
      "Lin Li",
      "Lijun Qian",
      "Xishuang Dong"
    ],
    "published": "2025-04-17T00:13:04+00:00",
    "summary": "Mass-shooting events pose a significant challenge to public safety, generating large volumes of unstructured textual data that hinder effective investigations and the formulation of public policy. Despite the urgency, few prior studies have effectively automated the extraction of key information from these events to support legal and investigative efforts. This paper presented the first dataset designed for knowledge acquisition on mass-shooting events through the application of named entity recognition (NER) techniques. It focuses on identifying key entities such as offenders, victims, locations, and criminal instruments, that are vital for legal and investigative purposes. The NER process is powered by Large Language Models (LLMs) using few-shot prompting, facilitating the efficient extraction and organization of critical information from diverse sources, including news articles, police reports, and social media. Experimental results on real-world mass-shooting corpora demonstrate that GPT-4o is the most effective model for mass-shooting NER, achieving the highest Micro Precision, Micro Recall, and Micro F1-scores. Meanwhile, o1-mini delivers competitive performance, making it a resource-efficient alternative for less complex NER tasks. It is also observed that increasing the shot count enhances the performance of all models, but the gains are more substantial for GPT-4o and o1-mini, highlighting their superior adaptability to few-shot learning scenarios."
  },
  {
    "title": "KFinEval-Pilot: A Comprehensive Benchmark Suite for Korean Financial Language Understanding",
    "url": "http://arxiv.org/abs/2504.13216v1",
    "arxiv_id": "2504.13216v1",
    "authors": [
      "Bokwang Hwang",
      "Seonkyu Lim",
      "Taewoong Kim",
      "Yongjae Geun",
      "Sunghyun Bang",
      "Sohyun Park",
      "Jihyun Park",
      "Myeonggyu Lee",
      "Jinwoo Lee",
      "Yerin Kim",
      "Jinsun Yoo",
      "Jingyeong Hong",
      "Jina Park",
      "Yongchan Kim",
      "Suhyun Kim",
      "Younggyun Hahm",
      "Yiseul Lee",
      "Yejee Kang",
      "Chanhyuk Yoon",
      "Chansu Lee",
      "Heeyewon Jeong",
      "Jiyeon Lee",
      "Seonhye Gu",
      "Hyebin Kang",
      "Yousang Cho",
      "Hangyeol Yoo",
      "KyungTae Lim"
    ],
    "published": "2025-04-17T00:12:58+00:00",
    "summary": "We introduce KFinEval-Pilot, a benchmark suite specifically designed to evaluate large language models (LLMs) in the Korean financial domain. Addressing the limitations of existing English-centric benchmarks, KFinEval-Pilot comprises over 1,000 curated questions across three critical areas: financial knowledge, legal reasoning, and financial toxicity. The benchmark is constructed through a semi-automated pipeline that combines GPT-4-generated prompts with expert validation to ensure domain relevance and factual accuracy. We evaluate a range of representative LLMs and observe notable performance differences across models, with trade-offs between task accuracy and output safety across different model families. These results highlight persistent challenges in applying LLMs to high-stakes financial applications, particularly in reasoning and safety. Grounded in real-world financial use cases and aligned with the Korean regulatory and linguistic context, KFinEval-Pilot serves as an early diagnostic tool for developing safer and more reliable financial AI systems."
  },
  {
    "title": "What do people expect from Artificial Intelligence? Public opinion on alignment in AI moderation from Germany and the United States",
    "url": "http://arxiv.org/abs/2504.12476v1",
    "arxiv_id": "2504.12476v1",
    "authors": [
      "Andreas Jungherr",
      "Adrian Rauchfleisch"
    ],
    "published": "2025-04-16T20:27:03+00:00",
    "summary": "Recent advances in generative Artificial Intelligence have raised public awareness, shaping expectations and concerns about their societal implications. Central to these debates is the question of AI alignment -- how well AI systems meet public expectations regarding safety, fairness, and social values. However, little is known about what people expect from AI-enabled systems and how these expectations differ across national contexts. We present evidence from two surveys of public preferences for key functional features of AI-enabled systems in Germany (n = 1800) and the United States (n = 1756). We examine support for four types of alignment in AI moderation: accuracy and reliability, safety, bias mitigation, and the promotion of aspirational imaginaries. U.S. respondents report significantly higher AI use and consistently greater support for all alignment features, reflecting broader technological openness and higher societal involvement with AI. In both countries, accuracy and safety enjoy the strongest support, while more normatively charged goals -- like fairness and aspirational imaginaries -- receive more cautious backing, particularly in Germany. We also explore how individual experience with AI, attitudes toward free speech, political ideology, partisan affiliation, and gender shape these preferences. AI use and free speech support explain more variation in Germany. In contrast, U.S. responses show greater attitudinal uniformity, suggesting that higher exposure to AI may consolidate public expectations. These findings contribute to debates on AI governance and cross-national variation in public preferences. More broadly, our study demonstrates the value of empirically grounding AI alignment debates in public attitudes and of explicitly developing normatively grounded expectations into theoretical and policy discussions on the governance of AI-generated content."
  },
  {
    "title": "Learning-based Delay Compensation for Enhanced Control of Assistive Soft Robots",
    "url": "http://arxiv.org/abs/2504.12428v1",
    "arxiv_id": "2504.12428v1",
    "authors": [
      "Adri\u00e0 Momp\u00f3 Alepuz",
      "Dimitrios Papageorgiou",
      "Silvia Tolu"
    ],
    "published": "2025-04-16T18:57:23+00:00",
    "summary": "Soft robots are increasingly used in healthcare, especially for assistive care, due to their inherent safety and adaptability. Controlling soft robots is challenging due to their nonlinear dynamics and the presence of time delays, especially in applications like a soft robotic arm for patient care. This paper presents a learning-based approach to approximate the nonlinear state predictor (Smith Predictor), aiming to improve tracking performance in a two-module soft robot arm with a short inherent input delay. The method uses Kernel Recursive Least Squares Tracker (KRLST) for online learning of the system dynamics and a Legendre Delay Network (LDN) to compress past input history for efficient delay compensation. Experimental results demonstrate significant improvement in tracking performance compared to a baseline model-based non-linear controller. Statistical analysis confirms the significance of the improvements. The method is computationally efficient and adaptable online, making it suitable for real-world scenarios and highlighting its potential for enabling safer and more accurate control of soft robots in assistive care applications."
  },
  {
    "title": "Accountable Liveness",
    "url": "http://arxiv.org/abs/2504.12218v1",
    "arxiv_id": "2504.12218v1",
    "authors": [
      "Andrew Lewis-Pye",
      "Joachim Neu",
      "Tim Roughgarden",
      "Luca Zanolini"
    ],
    "published": "2025-04-16T16:13:09+00:00",
    "summary": "Safety and liveness are the two classical security properties of consensus protocols. Recent works have strengthened safety with accountability: should any safety violation occur, a sizable fraction of adversary nodes can be proven to be protocol violators. This paper studies to what extent analogous accountability guarantees are achievable for liveness. To reveal the full complexity of this question, we introduce an interpolation between the classical synchronous and partially-synchronous models that we call the $x$-partially-synchronous network model in which, intuitively, at most an $x$ fraction of the time steps in any sufficiently long interval are asynchronous (and, as with a partially-synchronous network, all time steps are synchronous following the passage of an unknown \"global stablization time\"). We prove a precise characterization of the parameter regime in which accountable liveness is achievable: if and only if $x < 1/2$ and $f < n/2$, where $n$ denotes the number of nodes and $f$ the number of nodes controlled by an adversary. We further refine the problem statement and our analysis by parameterizing by the number of violating nodes identified following a liveness violation, and provide evidence that the guarantees achieved by our protocol are near-optimal (as a function of $x$ and $f$). Our results provide rigorous foundations for liveness-accountability heuristics such as the \"inactivity leaks\" employed in Ethereum."
  },
  {
    "title": "GripMap: An Efficient, Spatially Resolved Constraint Framework for Offline and Online Trajectory Planning in Autonomous Racing",
    "url": "http://arxiv.org/abs/2504.12115v1",
    "arxiv_id": "2504.12115v1",
    "authors": [
      "Frederik Werner",
      "Ann-Kathrin Schwehn",
      "Markus Lienkamp",
      "Johannes Betz"
    ],
    "published": "2025-04-16T14:25:29+00:00",
    "summary": "Conventional trajectory planning approaches for autonomous vehicles often assume a fixed vehicle model that remains constant regardless of the vehicle's location. This overlooks the critical fact that the tires and the surface are the two force-transmitting partners in vehicle dynamics; while the tires stay with the vehicle, surface conditions vary with location. Recognizing these challenges, this paper presents a novel framework for spatially resolving dynamic constraints in both offline and online planning algorithms applied to autonomous racing. We introduce the GripMap concept, which provides a spatial resolution of vehicle dynamic constraints in the Frenet frame, allowing adaptation to locally varying grip conditions. This enables compensation for location-specific effects, more efficient vehicle behavior, and increased safety, unattainable with spatially invariant vehicle models. The focus is on low storage demand and quick access through perfect hashing. This framework proved advantageous in real-world applications in the presented form. Experiments inspired by autonomous racing demonstrate its effectiveness. In future work, this framework can serve as a foundational layer for developing future interpretable learning algorithms that adjust to varying grip conditions in real-time."
  },
  {
    "title": "Towards LLM Agents for Earth Observation",
    "url": "http://arxiv.org/abs/2504.12110v1",
    "arxiv_id": "2504.12110v1",
    "authors": [
      "Chia Hsiang Kao",
      "Wenting Zhao",
      "Shreelekha Revankar",
      "Samuel Speas",
      "Snehal Bhagat",
      "Rajeev Datta",
      "Cheng Perng Phoo",
      "Utkarsh Mall",
      "Carl Vondrick",
      "Kavita Bala",
      "Bharath Hariharan"
    ],
    "published": "2025-04-16T14:19:25+00:00",
    "summary": "Earth Observation (EO) provides critical planetary data for environmental monitoring, disaster management, climate science, and other scientific domains. Here we ask: Are AI systems ready for reliable Earth Observation? We introduce \\datasetnamenospace, a benchmark of 140 yes/no questions from NASA Earth Observatory articles across 13 topics and 17 satellite sensors. Using Google Earth Engine API as a tool, LLM agents can only achieve an accuracy of 33% because the code fails to run over 58% of the time. We improve the failure rate for open models by fine-tuning synthetic data, allowing much smaller models (Llama-3.1-8B) to achieve comparable accuracy to much larger ones (e.g., DeepSeek-R1). Taken together, our findings identify significant challenges to be solved before AI agents can automate earth observation, and suggest paths forward. The project page is available at https://iandrover.github.io/UnivEarth."
  },
  {
    "title": "Reasoning-Based AI for Startup Evaluation (R.A.I.S.E.): A Memory-Augmented, Multi-Step Decision Framework",
    "url": "http://arxiv.org/abs/2504.12090v1",
    "arxiv_id": "2504.12090v1",
    "authors": [
      "Jack Preuveneers",
      "Joseph Ternasky",
      "Fuat Alican",
      "Yigit Ihlamur"
    ],
    "published": "2025-04-16T13:53:42+00:00",
    "summary": "We present a novel framework that bridges the gap between the interpretability of decision trees and the advanced reasoning capabilities of large language models (LLMs) to predict startup success. Our approach leverages chain-of-thought prompting to generate detailed reasoning logs, which are subsequently distilled into structured, human-understandable logical rules. The pipeline integrates multiple enhancements - efficient data ingestion, a two-step refinement process, ensemble candidate sampling, simulated reinforcement learning scoring, and persistent memory - to ensure both stable decision-making and transparent output. Experimental evaluations on curated startup datasets demonstrate that our combined pipeline improves precision by 54% from 0.225 to 0.346 and accuracy by 50% from 0.46 to 0.70 compared to a standalone OpenAI o3 model. Notably, our model achieves over 2x the precision of a random classifier (16%). By combining state-of-the-art AI reasoning with explicit rule-based explanations, our method not only augments traditional decision-making processes but also facilitates expert intervention and continuous policy refinement. This work lays the foundation for the implementation of interpretable LLM-powered decision frameworks in high-stakes investment environments and other domains that require transparent and data-driven insights."
  },
  {
    "title": "Selective Demonstration Retrieval for Improved Implicit Hate Speech Detection",
    "url": "http://arxiv.org/abs/2504.12082v1",
    "arxiv_id": "2504.12082v1",
    "authors": [
      "Yumin Kim",
      "Hwanhee Lee"
    ],
    "published": "2025-04-16T13:43:23+00:00",
    "summary": "Hate speech detection is a crucial area of research in natural language processing, essential for ensuring online community safety. However, detecting implicit hate speech, where harmful intent is conveyed in subtle or indirect ways, remains a major challenge. Unlike explicit hate speech, implicit expressions often depend on context, cultural subtleties, and hidden biases, making them more challenging to identify consistently. Additionally, the interpretation of such speech is influenced by external knowledge and demographic biases, resulting in varied detection results across different language models. Furthermore, Large Language Models often show heightened sensitivity to toxic language and references to vulnerable groups, which can lead to misclassifications. This over-sensitivity results in false positives (incorrectly identifying harmless statements as hateful) and false negatives (failing to detect genuinely harmful content). Addressing these issues requires methods that not only improve detection precision but also reduce model biases and enhance robustness. To address these challenges, we propose a novel method, which utilizes in-context learning without requiring model fine-tuning. By adaptively retrieving demonstrations that focus on similar groups or those with the highest similarity scores, our approach enhances contextual comprehension. Experimental results show that our method outperforms current state-of-the-art techniques. Implementation details and code are available at TBD."
  },
  {
    "title": "Observational properties of regular black holes in Asymptotic Safety",
    "url": "http://arxiv.org/abs/2504.12072v1",
    "arxiv_id": "2504.12072v1",
    "authors": [
      "Abdybek Urmanov",
      "Hrishikesh Chakrabarty",
      "Daniele Malafarina"
    ],
    "published": "2025-04-16T13:28:51+00:00",
    "summary": "We consider the observational properties of a spherically symmetric, static regular black hole within the framework of asymptotic safety (AS) as proposed by Bonanno et al. The metric resembles the Schwarzschild solution in the classical limit. The departure from Schwarzschild at small scales is controlled by a single free parameter related to the ultraviolet (UV) cutoff of the theory. We investigated null and time-like geodesics around the AS metric, including circular orbits, photon rings and lensing effects. In particular we focused on the optical properties of thin accretion disks in the equatorial plane of the object and compared them with those of accretion disks in the Schwarzschild metric. We found that the radiation flux, luminosity, and efficiency of the accretion disk increase with the value of the free parameter. Using a spacetime generic open-source relativistic ray-tracing code, we simulate the K$\\alpha$ iron line profiles emitted by the disk and analyze their deviation from that of the Schwarzschild geometry."
  },
  {
    "title": "Contract-based hierarchical control using predictive feasibility value functions",
    "url": "http://arxiv.org/abs/2504.12036v1",
    "arxiv_id": "2504.12036v1",
    "authors": [
      "Felix Berkel",
      "Kim Peter Wabersich",
      "Hongxi Xiang",
      "Elias Milios"
    ],
    "published": "2025-04-16T12:51:18+00:00",
    "summary": "Today's control systems are often characterized by modularity and safety requirements to handle complexity, resulting in hierarchical control structures. Although hierarchical model predictive control offers favorable properties, achieving a provably safe, yet modular design remains a challenge. This paper introduces a contract-based hierarchical control strategy to improve the performance of control systems facing challenges related to model inconsistency and independent controller design across hierarchies. We consider a setup where a higher-level controller generates references that affect the constraints of a lower-level controller, which is based on a soft-constrained MPC formulation. The optimal slack variables serve as the basis for a contract that allows the higher-level controller to assess the feasibility of the reference trajectory without exact knowledge of the model, constraints, and cost of the lower-level controller. To ensure computational efficiency while maintaining model confidentiality, we propose using an explicit function approximation, such as a neural network, to represent the cost of optimal slack values. The approach is tested for a hierarchical control setup consisting of a planner and a motion controller as commonly found in autonomous driving."
  },
  {
    "title": "Global Patterns of Extreme Temperature Teleconnections Using Climate Network Analysis",
    "url": "http://arxiv.org/abs/2504.12008v1",
    "arxiv_id": "2504.12008v1",
    "authors": [
      "Yuhao Feng",
      "Jun Meng",
      "Jingfang Fan"
    ],
    "published": "2025-04-16T12:05:15+00:00",
    "summary": "Extreme weather events, rare yet profoundly impactful, are often accompanied by severe conditions. Increasing global temperatures are poised to exacerbate these events, resulting in greater human casualties, economic losses, and ecological destruction. Complex global climate interactions, known as teleconnections, can lead to widespread repercussions triggered by localized extreme weather. Understanding these teleconnection patterns is crucial for weather forecasting, enhancing safety, and advancing climate science. Here, we employ climate network analysis to uncover teleconnection patterns associated with extreme temperature fluctuations, including both extreme warming and cooling events occurring on a daily basis. Our study results demonstrate that the distances of significant teleconnections initially conform to a power-law decay, signifying a decline in connectivity with distance. However, this power-law decay tendency breaks beyond a certain threshold distance, suggesting the existence of long-distance connections. Additionally, we uncover a greater prevalence of long-distance connectivity among extreme cooling events compared to extreme warming events. The global pattern of teleconnections is, in part, driven by the mechanism of Rossby waves, which serve as a rapid conduit for inducing correlated fluctuations in both pressure and temperature. These results enhance our understanding of the multiscale nature of climate teleconnections and hold significant implications for improving weather forecasting and assessing climate risks in a warming world."
  },
  {
    "title": "Comment on Path integral measure and RG equations for gravity",
    "url": "http://arxiv.org/abs/2504.12006v1",
    "arxiv_id": "2504.12006v1",
    "authors": [
      "Aaron Held",
      "Benjamin Knorr",
      "Jan M. Pawlowski",
      "Alessia Platania",
      "Manuel Reichert",
      "Frank Saueressig",
      "Marc Schiffer"
    ],
    "published": "2025-04-16T12:00:22+00:00",
    "summary": "Asymptotic safety is a candidate for a predictive quantum theory of gravity and matter. Recent works (arXiv:2412.10194 and arXiv:2412.14108) challenged this scenario. We show that their arguments fail on a basic level."
  },
  {
    "title": "Leveraging Machine Learning Models to Predict the Outcome of Digital Medical Triage Interviews",
    "url": "http://arxiv.org/abs/2504.11977v1",
    "arxiv_id": "2504.11977v1",
    "authors": [
      "Sofia Krylova",
      "Fabian Schmidt",
      "Vladimir Vlassov"
    ],
    "published": "2025-04-16T11:17:23+00:00",
    "summary": "Many existing digital triage systems are questionnaire-based, guiding patients to appropriate care levels based on information (e.g., symptoms, medical history, and urgency) provided by the patients answering questionnaires. Such a system often uses a deterministic model with predefined rules to determine care levels. It faces challenges with incomplete triage interviews since it can only assist patients who finish the process. In this study, we explore the use of machine learning (ML) to predict outcomes of unfinished interviews, aiming to enhance patient care and service quality. Predicting triage outcomes from incomplete data is crucial for patient safety and healthcare efficiency. Our findings show that decision-tree models, particularly LGBMClassifier and CatBoostClassifier, achieve over 80\\% accuracy in predicting outcomes from complete interviews while having a linear correlation between the prediction accuracy and interview completeness degree. For example, LGBMClassifier achieves 88,2\\% prediction accuracy for interviews with 100\\% completeness, 79,6\\% accuracy for interviews with 80\\% completeness, 58,9\\% accuracy for 60\\% completeness, and 45,7\\% accuracy for 40\\% completeness. The TabTransformer model demonstrated exceptional accuracy of over 80\\% for all degrees of completeness but required extensive training time, indicating a need for more powerful computational resources. The study highlights the linear correlation between interview completeness and predictive power of the decision-tree models."
  },
  {
    "title": "SemDiff: Generating Natural Unrestricted Adversarial Examples via Semantic Attributes Optimization in Diffusion Models",
    "url": "http://arxiv.org/abs/2504.11923v1",
    "arxiv_id": "2504.11923v1",
    "authors": [
      "Zeyu Dai",
      "Shengcai Liu",
      "Rui He",
      "Jiahao Wu",
      "Ning Lu",
      "Wenqi Fan",
      "Qing Li",
      "Ke Tang"
    ],
    "published": "2025-04-16T09:58:04+00:00",
    "summary": "Unrestricted adversarial examples (UAEs), allow the attacker to create non-constrained adversarial examples without given clean samples, posing a severe threat to the safety of deep learning models. Recent works utilize diffusion models to generate UAEs. However, these UAEs often lack naturalness and imperceptibility due to simply optimizing in intermediate latent noises. In light of this, we propose SemDiff, a novel unrestricted adversarial attack that explores the semantic latent space of diffusion models for meaningful attributes, and devises a multi-attributes optimization approach to ensure attack success while maintaining the naturalness and imperceptibility of generated UAEs. We perform extensive experiments on four tasks on three high-resolution datasets, including CelebA-HQ, AFHQ and ImageNet. The results demonstrate that SemDiff outperforms state-of-the-art methods in terms of attack success rate and imperceptibility. The generated UAEs are natural and exhibit semantically meaningful changes, in accord with the attributes' weights. In addition, SemDiff is found capable of evading different defenses, which further validates its effectiveness and threatening."
  },
  {
    "title": "Rethinking the Generation of High-Quality CoT Data from the Perspective of LLM-Adaptive Question Difficulty Grading",
    "url": "http://arxiv.org/abs/2504.11919v1",
    "arxiv_id": "2504.11919v1",
    "authors": [
      "Qianjin Yu",
      "Keyu Wu",
      "Zihan Chen",
      "Chushu Zhang",
      "Manlin Mei",
      "Lingjun Huang",
      "Fang Tan",
      "Yongsheng Du",
      "Kunlin Liu",
      "Yurui Zhu"
    ],
    "published": "2025-04-16T09:55:34+00:00",
    "summary": "Recently, DeepSeek-R1 (671B) (DeepSeek-AIet al., 2025) has demonstrated its excellent reasoning ability in complex tasks and has publiclyshared its methodology. This provides potentially high-quality chain-of-thought (CoT) data for stimulating the reasoning abilities of small-sized large language models (LLMs). To generate high-quality CoT data for different LLMs, we seek an efficient method for generating high-quality CoT data with LLM-Adaptive questiondifficulty levels. First, we grade the difficulty of the questions according to the reasoning ability of the LLMs themselves and construct a LLM-Adaptive question database. Second, we sample the problem database based on a distribution of difficulty levels of the questions and then use DeepSeek-R1 (671B) (DeepSeek-AI et al., 2025) to generate the corresponding high-quality CoT data with correct answers. Thanks to the construction of CoT data with LLM-Adaptive difficulty levels, we have significantly reduced the cost of data generation and enhanced the efficiency of model supervised fine-tuning (SFT). Finally, we have validated the effectiveness and generalizability of the proposed method in the fields of complex mathematical competitions and code generation tasks. Notably, with only 2k high-quality mathematical CoT data, our ZMath-32B surpasses DeepSeek-Distill-32B in math reasoning task. Similarly, with only 2k high-quality code CoT data, our ZCode-32B surpasses DeepSeek-Distill-32B in code reasoning tasks."
  },
  {
    "title": "A Graph-Based Reinforcement Learning Approach with Frontier Potential Based Reward for Safe Cluttered Environment Exploration",
    "url": "http://arxiv.org/abs/2504.11907v1",
    "arxiv_id": "2504.11907v1",
    "authors": [
      "Gabriele Calzolari",
      "Vidya Sumathy",
      "Christoforos Kanellakis",
      "George Nikolakopoulos"
    ],
    "published": "2025-04-16T09:31:14+00:00",
    "summary": "Autonomous exploration of cluttered environments requires efficient exploration strategies that guarantee safety against potential collisions with unknown random obstacles. This paper presents a novel approach combining a graph neural network-based exploration greedy policy with a safety shield to ensure safe navigation goal selection. The network is trained using reinforcement learning and the proximal policy optimization algorithm to maximize exploration efficiency while reducing the safety shield interventions. However, if the policy selects an infeasible action, the safety shield intervenes to choose the best feasible alternative, ensuring system consistency. Moreover, this paper proposes a reward function that includes a potential field based on the agent's proximity to unexplored regions and the expected information gain from reaching them. Overall, the approach investigated in this paper merges the benefits of the adaptability of reinforcement learning-driven exploration policies and the guarantee ensured by explicit safety mechanisms. Extensive evaluations in simulated environments demonstrate that the approach enables efficient and safe exploration in cluttered environments."
  },
  {
    "title": "MDHP-Net: Detecting an Emerging Time-exciting Threat in IVN",
    "url": "http://arxiv.org/abs/2504.11867v1",
    "arxiv_id": "2504.11867v1",
    "authors": [
      "Qi Liu",
      "Yanchen Liu",
      "Ruifeng Li",
      "Chenhong Cao",
      "Yufeng Li",
      "Xingyu Li",
      "Peng Wang",
      "Runhan Feng",
      "Shiyang Bu"
    ],
    "published": "2025-04-16T08:41:24+00:00",
    "summary": "The integration of intelligent and connected technologies in modern vehicles, while offering enhanced functionalities through Electronic Control Unit (ECU) and interfaces like OBD-II and telematics, also exposes the vehicle's in-vehicle network (IVN) to potential cyberattacks. Unlike prior work, we identify a new time-exciting threat model against IVN. These attacks inject malicious messages that exhibit a time-exciting effect, gradually manipulating network traffic to disrupt vehicle operations and compromise safety-critical functions. We systematically analyze the characteristics of the threat: dynamism, time-exciting impact, and low prior knowledge dependency. To validate its practicality, we replicate the attack on a real Advanced Driver Assistance System via Controller Area Network (CAN), exploiting Unified Diagnostic Service vulnerabilities and proposing four attack strategies. While CAN's integrity checks mitigate attacks, Ethernet migration (e.g., DoIP/SOME/IP) introduces new surfaces. We further investigate the feasibility of time-exciting threat under SOME/IP. To detect time-exciting threat, we introduce MDHP-Net, leveraging Multi-Dimentional Hawkes Process (MDHP) and temporal and message-wise feature extracting structures. Meanwhile, to estimate MDHP parameters, we developed the first GPU-optimized gradient descent solver for MDHP (MDHP-GDS). These modules significantly improves the detection rate under time-exciting attacks in multi-ECU IVN system. To address data scarcity, we release STEIA9, the first open-source dataset for time-exciting attacks, covering 9 Ethernet-based attack scenarios. Extensive experiments on STEIA9 (9 attack scenarios) show MDHP-Net outperforms 3 baselines, confirming attack feasibility and detection efficacy."
  },
  {
    "title": "Visualization Analysis and Impedance Analysis for the Aging Behavior Assessment of 18650 Cells",
    "url": "http://arxiv.org/abs/2504.11861v1",
    "arxiv_id": "2504.11861v1",
    "authors": [
      "Yihan Shi",
      "Qingrui Pan",
      "Jitao Li",
      "Xiaoze Shi",
      "Youchang Wang",
      "Peng Xiao"
    ],
    "published": "2025-04-16T08:31:27+00:00",
    "summary": "This work presents a comprehensive study on the aging behavior of 18650-type lithium-ion batteries, focusing on the uneven intercalation of lithium ions during fast charging processes. It introduces a novel approach using color visual recognition technology to analyze color changes in the graphite anode, indicative of lithiation levels. The study employs X-ray diffraction (XRD) and Distribution of Relaxation Time (DRT) techniques to validate and analyze the observations. The study emphasizes the significance of electrode impedance, the positioning of battery tabs, and electrolyte distribution in influencing the aging dynamics of lithium-ion batteries. Furthermore, the paper presents an innovative impedance Transport-Line Model, specifically developed to capture the evolution of polarization impedance over time. This model offers a deeper understanding of the internal mechanisms driving battery aging, providing valuable insights for the design and optimization of lithium-ion batteries. The research represents a significant contribution to the field, shedding light on the complex aging processes in lithium-ion batteries, particularly under the conditions of fast charging. This could lead to improved battery performance, longevity, and safety, which are critical for the wide range of applications that depend on these energy storage systems."
  },
  {
    "title": "Ultra-Efficient Kidney Stone Fragment Removal via Spinner-Induced Synergistic Circulation and Spiral Flow",
    "url": "http://arxiv.org/abs/2504.11847v1",
    "arxiv_id": "2504.11847v1",
    "authors": [
      "Yilong Chang",
      "Jasmine Vallejo",
      "Yangqing Sun",
      "Ruike Renee Zhao"
    ],
    "published": "2025-04-16T08:10:16+00:00",
    "summary": "Kidney stones can cause severe pain and complications such as chronic kidney disease or kidney failure. Retrograde intrarenal surgery (RIRS), which uses laser lithotripsy to fragment stones for removal via a ureteroscope, is widely adopted due to its safety and effectiveness. However, conventional fragment removal methods using basketing and vacuum-assisted aspiration are inefficient, as they can capture only 1 to 3 fragments (1--3\\,mm in size) per pass, often requiring dozens to hundreds of ureteroscope passes during a single procedure to completely remove the fragments. These limitations lead to prolonged procedures and residual fragments that contribute to high recurrence rates. To address these limitations, we present a novel spinner device that enables ultra-efficient fragment removal through spinning-induced localized suction. The spinner generates a three-dimensional spiral and circulating flow field that dislodges and draws fragments into its cavity even from distances over 20\\,mm, eliminating the need to chase fragments. It can capture over 60 fragments (0.5--2\\,mm) or over 15 larger fragments (2--3\\,mm) in a single pass, significantly improving removal efficiency. In this work, the spinner design is optimized via computational fluid dynamics to maximize suction performance. \\textit{In vitro} testing demonstrates near 100\\% capture rates for up to 60 fragments in a single operation and superior large-distance capture efficacy compared to vacuum-assisted methods. \\textit{Ex vivo} testing of the integrated spinner-ureteroscope system in a porcine kidney confirmed its high performance by capturing 45 fragments in just 4 seconds during a single pass and achieving complete fragment clearance within a few passes."
  },
  {
    "title": "Support is All You Need for Certified VAE Training",
    "url": "http://arxiv.org/abs/2504.11831v1",
    "arxiv_id": "2504.11831v1",
    "authors": [
      "Changming Xu",
      "Debangshu Banerjee",
      "Deepak Vasisht",
      "Gagandeep Singh"
    ],
    "published": "2025-04-16T07:41:40+00:00",
    "summary": "Variational Autoencoders (VAEs) have become increasingly popular and deployed in safety-critical applications. In such applications, we want to give certified probabilistic guarantees on performance under adversarial attacks. We propose a novel method, CIVET, for certified training of VAEs. CIVET depends on the key insight that we can bound worst-case VAE error by bounding the error on carefully chosen support sets at the latent layer. We show this point mathematically and present a novel training algorithm utilizing this insight. We show in an extensive evaluation across different datasets (in both the wireless and vision application areas), architectures, and perturbation magnitudes that our method outperforms SOTA methods achieving good standard performance with strong robustness guarantees."
  },
  {
    "title": "Multi-goal Rapidly Exploring Random Tree with Safety and Dynamic Constraints for UAV Cooperative Path Planning",
    "url": "http://arxiv.org/abs/2504.11823v1",
    "arxiv_id": "2504.11823v1",
    "authors": [
      "Thu Hang Khuat",
      "Duy-Nam Bui",
      "Hoa TT. Nguyen",
      "Mien L. Trinh",
      "Minh T. Nguyen",
      "Manh Duong Phung"
    ],
    "published": "2025-04-16T07:16:35+00:00",
    "summary": "Cooperative path planning is gaining its importance due to the increasing demand on using multiple unmanned aerial vehicles (UAVs) for complex missions. This work addresses the problem by introducing a new algorithm named MultiRRT that extends the rapidly exploring random tree (RRT) to generate paths for a group of UAVs to reach multiple goal locations at the same time. We first derive the dynamics constraint of the UAV and include it in the problem formulation. MultiRRT is then developed, taking into account the cooperative requirements and safe constraints during its path-searching process. The algorithm features two new mechanisms, node reduction and Bezier interpolation, to ensure the feasibility and optimality of the paths generated. Importantly, the interpolated paths are proven to meet the safety and dynamics constraints imposed by obstacles and the UAVs. A number of simulations, comparisons, and experiments have been conducted to evaluate the performance of the proposed approach. The results show that MultiRRT can generate collision-free paths for multiple UAVs to reach their goals with better scores in path length and smoothness metrics than state-of-the-art RRT variants including Theta-RRT, FN-RRT, RRT*, and RRT*-Smart. The generated paths are also tested in practical flights with real UAVs to evaluate their validity for cooperative tasks. The source code of the algorithm is available at https://github.com/duynamrcv/multi-target_RRT"
  },
  {
    "title": "Towards an AI Observatory for the Nuclear Sector: A tool for anticipatory governance",
    "url": "http://arxiv.org/abs/2504.12358v1",
    "arxiv_id": "2504.12358v1",
    "authors": [
      "Aditi Verma",
      "Elizabeth Williams"
    ],
    "published": "2025-04-16T03:43:15+00:00",
    "summary": "AI models are rapidly becoming embedded in all aspects of nuclear energy research and work but the safety, security, and safeguards consequences of this embedding are not well understood. In this paper, we call for the creation of an anticipatory system of governance for AI in the nuclear sector as well as the creation of a global AI observatory as a means for operationalizing anticipatory governance. The paper explores the contours of the nuclear AI observatory and an anticipatory system of governance by drawing on work in science and technology studies, public policy, and foresight studies."
  },
  {
    "title": "Safety with Agency: Human-Centered Safety Filter with Application to AI-Assisted Motorsports",
    "url": "http://arxiv.org/abs/2504.11717v1",
    "arxiv_id": "2504.11717v1",
    "authors": [
      "Donggeon David Oh",
      "Justin Lidard",
      "Haimin Hu",
      "Himani Sinhmar",
      "Elle Lazarski",
      "Deepak Gopinath",
      "Emily S. Sumner",
      "Jonathan A. DeCastro",
      "Guy Rosman",
      "Naomi Ehrich Leonard",
      "Jaime Fern\u00e1ndez Fisac"
    ],
    "published": "2025-04-16T02:42:08+00:00",
    "summary": "We propose a human-centered safety filter (HCSF) for shared autonomy that significantly enhances system safety without compromising human agency. Our HCSF is built on a neural safety value function, which we first learn scalably through black-box interactions and then use at deployment to enforce a novel quality control barrier function (Q-CBF) safety constraint. Since this Q-CBF safety filter does not require any knowledge of the system dynamics for both synthesis and runtime safety monitoring and intervention, our method applies readily to complex, black-box shared autonomy systems. Notably, our HCSF's CBF-based interventions modify the human's actions minimally and smoothly, avoiding the abrupt, last-moment corrections delivered by many conventional safety filters. We validate our approach in a comprehensive in-person user study using Assetto Corsa-a high-fidelity car racing simulator with black-box dynamics-to assess robustness in \"driving on the edge\" scenarios. We compare both trajectory data and drivers' perceptions of our HCSF assistance against unassisted driving and a conventional safety filter. Experimental results show that 1) compared to having no assistance, our HCSF improves both safety and user satisfaction without compromising human agency or comfort, and 2) relative to a conventional safety filter, our proposed HCSF boosts human agency, comfort, and satisfaction while maintaining robustness."
  },
  {
    "title": "Safety with Agency: Human-Centered Safety Filter with Application to AI-Assisted Motorsports",
    "url": "http://arxiv.org/abs/2504.11717v2",
    "arxiv_id": "2504.11717v2",
    "authors": [
      "Donggeon David Oh",
      "Justin Lidard",
      "Haimin Hu",
      "Himani Sinhmar",
      "Elle Lazarski",
      "Deepak Gopinath",
      "Emily S. Sumner",
      "Jonathan A. DeCastro",
      "Guy Rosman",
      "Naomi Ehrich Leonard",
      "Jaime Fern\u00e1ndez Fisac"
    ],
    "published": "2025-04-16T02:42:08+00:00",
    "summary": "We propose a human-centered safety filter (HCSF) for shared autonomy that significantly enhances system safety without compromising human agency. Our HCSF is built on a neural safety value function, which we first learn scalably through black-box interactions and then use at deployment to enforce a novel state-action control barrier function (Q-CBF) safety constraint. Since this Q-CBF safety filter does not require any knowledge of the system dynamics for both synthesis and runtime safety monitoring and intervention, our method applies readily to complex, black-box shared autonomy systems. Notably, our HCSF's CBF-based interventions modify the human's actions minimally and smoothly, avoiding the abrupt, last-moment corrections delivered by many conventional safety filters. We validate our approach in a comprehensive in-person user study using Assetto Corsa-a high-fidelity car racing simulator with black-box dynamics-to assess robustness in \"driving on the edge\" scenarios. We compare both trajectory data and drivers' perceptions of our HCSF assistance against unassisted driving and a conventional safety filter. Experimental results show that 1) compared to having no assistance, our HCSF improves both safety and user satisfaction without compromising human agency or comfort, and 2) relative to a conventional safety filter, our proposed HCSF boosts human agency, comfort, and satisfaction while maintaining robustness."
  },
  {
    "title": "Optimal SVI-Weighted PSPS Decisions with Decision-Dependent Outage Uncertainty",
    "url": "http://arxiv.org/abs/2504.11665v1",
    "arxiv_id": "2504.11665v1",
    "authors": [
      "Ryan Greenough",
      "Kohei Murakami",
      "Jan Kleissl",
      "Adil Khurram"
    ],
    "published": "2025-04-15T23:31:39+00:00",
    "summary": "Public Safety Power Shutoffs (PSPS) are a pre-emptive strategy to mitigate the wildfires caused by power system malfunction. System operators implement PSPS to balance wildfire mitigation efforts through de-energization of transmission lines against the risk of widespread blackouts modeled with load shedding.   Existing approaches do not incorporate decision-dependent wildfire-driven failure probabilities, as modeling outage scenario probabilities requires incorporating high-order polynomial terms in the objective. This paper uses distribution shaping to develop an efficient MILP problem representation of the distributionally robust PSPS problem. Building upon the author's prior work, the wildfire risk of operating a transmission line is a function of the probability of a wildfire-driven outage and its subsequent expected impact in acres burned.   A day-ahead unit commitment and line de-energization PSPS framework is used to assess the trade-off between total cost and wildfire risk at different levels of distributional robustness, parameterized by a level of distributional dissimilarity $\\kappa$. We perform simulations on the IEEE RTS 24-bus test system."
  },
  {
    "title": "Real-time Object and Event Detection Service through Computer Vision and Edge Computing",
    "url": "http://arxiv.org/abs/2504.11662v1",
    "arxiv_id": "2504.11662v1",
    "authors": [
      "Marcos Mendes",
      "Gon\u00e7alo Perna",
      "Pedro Rito",
      "Duarte Raposo",
      "Susana Sargento"
    ],
    "published": "2025-04-15T23:11:42+00:00",
    "summary": "The World Health Organization suggests that road traffic crashes cost approximately 518 billion dollars globally each year, which accounts for 3% of the gross domestic product for most countries. Most fatal road accidents in urban areas involve Vulnerable Road Users (VRUs). Smart cities environments present innovative approaches to combat accidents involving cutting-edge technologies, that include advanced sensors, extensive datasets, Machine Learning (ML) models, communication systems, and edge computing. This paper proposes a strategy and an implementation of a system for road monitoring and safety for smart cities, based on Computer Vision (CV) and edge computing. Promising results were obtained by implementing vision algorithms and tracking using surveillance cameras, that are part of a Smart City testbed, the Aveiro Tech City Living Lab (ATCLL). The algorithm accurately detects and tracks cars, pedestrians, and bicycles, while predicting the road state, the distance between moving objects, and inferring on collision events to prevent collisions, in near real-time."
  },
  {
    "title": "Improving LLM Interpretability and Performance via Guided Embedding Refinement for Sequential Recommendation",
    "url": "http://arxiv.org/abs/2504.11658v1",
    "arxiv_id": "2504.11658v1",
    "authors": [
      "Nanshan Jia",
      "Chenfei Yuan",
      "Yuhang Wu",
      "Zeyu Zheng"
    ],
    "published": "2025-04-15T23:03:53+00:00",
    "summary": "The fast development of Large Language Models (LLMs) offers growing opportunities to further improve sequential recommendation systems. Yet for some practitioners, integrating LLMs to their existing base recommendation systems raises questions about model interpretability, transparency and related safety. To partly alleviate challenges from these questions, we propose guided embedding refinement, a method that carries out a guided and interpretable usage of LLM to enhance the embeddings associated with the base recommendation system. Instead of directly using LLMs as the backbone of sequential recommendation systems, we utilize them as auxiliary tools to emulate the sales logic of recommendation and generate guided embeddings that capture domain-relevant semantic information on interpretable attributes. Benefiting from the strong generalization capabilities of the guided embedding, we construct refined embedding by using the guided embedding and reduced-dimension version of the base embedding. We then integrate the refined embedding into the recommendation module for training and inference. A range of numerical experiments demonstrate that guided embedding is adaptable to various given existing base embedding models, and generalizes well across different recommendation tasks. The numerical results show that the refined embedding not only improves recommendation performance, achieving approximately $10\\%$ to $50\\%$ gains in Mean Reciprocal Rank (MRR), Recall rate, and Normalized Discounted Cumulative Gain (NDCG), but also enhances interpretability, as evidenced by case studies."
  },
  {
    "title": "Verifiable Mission Planning For Space Operations",
    "url": "http://arxiv.org/abs/2504.11631v1",
    "arxiv_id": "2504.11631v1",
    "authors": [
      "Quentin Rommel",
      "Michael Hibbard",
      "Pavan Shukla",
      "Himanshu Save",
      "Srinivas Bettadpur",
      "Ufuk Topcu"
    ],
    "published": "2025-04-15T21:41:09+00:00",
    "summary": "As space missions become more complex, planning methods must maximize mission performance while rigorously enforcing safety. We develop a probabilistic approach based on a finite-horizon Markov decision process to optimize spacecraft operations planning with safety guarantees. In the model, states capture essential mission parameters, and actions represent the operational adjustments needed to meet mission objectives. By directly incorporating uncertainties from environmental conditions and spacecraft dynamics, an optimal sequence of actions is computed that maximizes expected rewards and strictly enforces safety constraints. Numerical experiments on the GRACE-FO mission demonstrate robust performance under uncertainties while providing probabilistic safety guarantees, offering a reliable solution for autonomous spacecraft operations."
  },
  {
    "title": "Provably Safe Control for Constrained Nonlinear Systems with Bounded Input",
    "url": "http://arxiv.org/abs/2504.11592v1",
    "arxiv_id": "2504.11592v1",
    "authors": [
      "Saurabh Kumar",
      "Shashi Ranjan Kumar",
      "Abhinav Sinha"
    ],
    "published": "2025-04-15T20:10:21+00:00",
    "summary": "In real-world control applications, actuator constraints and output constraints (specifically in tracking problems) are inherent and critical to ensuring safe and reliable operation. However, generally, control strategies often neglect these physical limitations, leading to potential instability, degraded performance, or even system failure when deployed on real-world systems. This paper addresses the control design problem for a class of nonlinear systems under both actuator saturation and output constraints. First, a smooth asymmetric saturation model (a more generic representative of practical scenarios) is proposed to model actuator saturation, which ensures that the control inputs always remain confined within a predefined set to ensure safety. Based on the proposed model, we develop a nonlinear control framework that guarantees output tracking while ensuring that system output remains confined to the predefined set. Later, we integrate this design with the constrained output tracking control problem, wherein we show that the system output tracks its desired trajectory by simultaneously satisfying input and output constraints. The global stabilization of the tracking error is achieved in the presence of input constraints, while semi-global stabilization is achieved in the presence of both input and output constraints. Additionally, we rigorously establish the boundedness of all closed-loop signals under the proposed design. Simulation results demonstrate the effectiveness of the proposed methods in handling asymmetric constraints while achieving desirable tracking performance."
  },
  {
    "title": "REAL: Benchmarking Autonomous Agents on Deterministic Simulations of Real Websites",
    "url": "http://arxiv.org/abs/2504.11543v1",
    "arxiv_id": "2504.11543v1",
    "authors": [
      "Divyansh Garg",
      "Shaun VanWeelden",
      "Diego Caples",
      "Andis Draguns",
      "Nikil Ravi",
      "Pranav Putta",
      "Naman Garg",
      "Tomas Abraham",
      "Michael Lara",
      "Federico Lopez",
      "James Liu",
      "Atharva Gundawar",
      "Prannay Hebbar",
      "Youngchul Joo",
      "Charles London",
      "Christian Schroeder de Witt",
      "Sumeet Motwani"
    ],
    "published": "2025-04-15T18:22:55+00:00",
    "summary": "We introduce REAL, a benchmark and framework for multi-turn agent evaluations on deterministic simulations of real-world websites. REAL comprises high-fidelity, deterministic replicas of 11 widely-used websites across domains such as e-commerce, travel, communication, and professional networking. We also release a benchmark consisting of 112 practical tasks that mirror everyday complex user interactions requiring both accurate information retrieval and state-changing actions. All interactions occur within this fully controlled setting, eliminating safety risks and enabling robust, reproducible evaluation of agent capability and reliability. Our novel evaluation framework combines programmatic checks of website state for action-based tasks with rubric-guided LLM-based judgments for information retrieval. The framework supports both open-source and proprietary agent systems through a flexible evaluation harness that accommodates black-box commands within browser environments, allowing research labs to test agentic systems without modification. Our empirical results show that frontier language models achieve at most a 41% success rate on REAL, highlighting critical gaps in autonomous web navigation and task completion capabilities. Our framework supports easy integration of new tasks, reproducible evaluation, and scalable data generation for training web agents. The websites, framework, and leaderboard are available at https://realevals.xyz and https://github.com/agi-inc/REAL."
  },
  {
    "title": "REAL: Benchmarking Autonomous Agents on Deterministic Simulations of Real Websites",
    "url": "http://arxiv.org/abs/2504.11543v2",
    "arxiv_id": "2504.11543v2",
    "authors": [
      "Divyansh Garg",
      "Shaun VanWeelden",
      "Diego Caples",
      "Andis Draguns",
      "Nikil Ravi",
      "Pranav Putta",
      "Naman Garg",
      "Tomas Abraham",
      "Michael Lara",
      "Federico Lopez",
      "James Liu",
      "Atharva Gundawar",
      "Prannay Hebbar",
      "Youngchul Joo",
      "Jindong Gu",
      "Charles London",
      "Christian Schroeder de Witt",
      "Sumeet Motwani"
    ],
    "published": "2025-04-15T18:22:55+00:00",
    "summary": "We introduce REAL, a benchmark and framework for multi-turn agent evaluations on deterministic simulations of real-world websites. REAL comprises high-fidelity, deterministic replicas of 11 widely-used websites across domains such as e-commerce, travel, communication, and professional networking. We also release a benchmark consisting of 112 practical tasks that mirror everyday complex user interactions requiring both accurate information retrieval and state-changing actions. All interactions occur within this fully controlled setting, eliminating safety risks and enabling robust, reproducible evaluation of agent capability and reliability. Our novel evaluation framework combines programmatic checks of website state for action-based tasks with rubric-guided LLM-based judgments for information retrieval. The framework supports both open-source and proprietary agent systems through a flexible evaluation harness that accommodates black-box commands within browser environments, allowing research labs to test agentic systems without modification. Our empirical results show that frontier language models achieve at most a 41% success rate on REAL, highlighting critical gaps in autonomous web navigation and task completion capabilities. Our framework supports easy integration of new tasks, reproducible evaluation, and scalable post-training data generation, marking a significant step forward in evaluating and advancing agent capabilities."
  },
  {
    "title": "ReTool: Reinforcement Learning for Strategic Tool Use in LLMs",
    "url": "http://arxiv.org/abs/2504.11536v1",
    "arxiv_id": "2504.11536v1",
    "authors": [
      "Jiazhan Feng",
      "Shijue Huang",
      "Xingwei Qu",
      "Ge Zhang",
      "Yujia Qin",
      "Baoquan Zhong",
      "Chengquan Jiang",
      "Jinxin Chi",
      "Wanjun Zhong"
    ],
    "published": "2025-04-15T18:10:22+00:00",
    "summary": "While reasoning models (e.g., DeepSeek R1) trained with reinforcement learning (RL), excel in textual reasoning, they struggle in scenarios requiring structured problem-solving, such as geometric reasoning, concise computation, or complex equation solving-areas where computational tools like code interpreters (CI) demonstrate distinct advantages. To bridge this gap, we propose ReTool, which enhances long-form reasoning with tool-integrated learning, including two key features: (1) dynamic interleaving of real-time code execution within natural language reasoning processes, and (2) an automated RL paradigm that allows policy rollouts with multi-turn real-time code execution and teaches the model in learning when and how to invoke tools based on outcome feedback. ReTool employs a systematic training framework, beginning with synthetic cold-start data generation to produce code-augmented long-form reasoning traces for fine-tuning base models. Subsequent RL training leverages task outcomes as rewards to iteratively refine the model's tool use strategy, enabling autonomous discovery of optimal tool invocation patterns without human priors. Experiments on the challenging MATH Olympiad benchmark AIME demonstrate ReTool's superiority: Our 32B model achieves 67% accuracy with 400 training steps, outperforming text-based RL baseline (40% accuracy, 1080 steps) in efficiency and performance. Remarkably, ReTool-32B attains 72.5% accuracy in extended settings, surpassing OpenAI's o1-preview by 27.9%. Further analysis reveals emergent behaviors such as code self-correction, signaling an ''aha moment'' in which the model autonomously masters adaptive tool use. These findings highlight the promise of outcome-driven tool integration for advancing complex mathematical reasoning and offer new insights into hybrid neuro-symbolic systems."
  },
  {
    "title": "ReTool: Reinforcement Learning for Strategic Tool Use in LLMs",
    "url": "http://arxiv.org/abs/2504.11536v2",
    "arxiv_id": "2504.11536v2",
    "authors": [
      "Jiazhan Feng",
      "Shijue Huang",
      "Xingwei Qu",
      "Ge Zhang",
      "Yujia Qin",
      "Baoquan Zhong",
      "Chengquan Jiang",
      "Jinxin Chi",
      "Wanjun Zhong"
    ],
    "published": "2025-04-15T18:10:22+00:00",
    "summary": "While reasoning models (e.g., DeepSeek R1) trained with reinforcement learning (RL), excel in textual reasoning, they struggle in scenarios requiring structured problem-solving, such as geometric reasoning, concise computation, or complex equation solving-areas where computational tools like code interpreters (CI) demonstrate distinct advantages. To bridge this gap, we propose ReTool, which enhances long-form reasoning with tool-integrated learning, including two key features: (1) dynamic interleaving of real-time code execution within natural language reasoning processes, and (2) an automated RL paradigm that allows policy rollouts with multi-turn real-time code execution and teaches the model in learning when and how to invoke tools based on outcome feedback. ReTool employs a systematic training framework, beginning with synthetic cold-start data generation to produce code-augmented long-form reasoning traces for fine-tuning base models. Subsequent RL training leverages task outcomes as rewards to iteratively refine the model's tool use strategy, enabling autonomous discovery of optimal tool invocation patterns without human priors. Experiments on the challenging MATH Olympiad benchmark AIME demonstrate ReTool's superiority: Our 32B model achieves 67% accuracy with 400 training steps, outperforming text-based RL baseline (40% accuracy, 1080 steps) in efficiency and performance. Remarkably, ReTool-32B attains 72.5% accuracy in extended settings, surpassing OpenAI's o1-preview by 27.9%. Further analysis reveals emergent behaviors such as code self-correction, signaling an ''aha moment'' in which the model autonomously masters adaptive tool use. These findings highlight the promise of outcome-driven tool integration for advancing complex mathematical reasoning and offer new insights into hybrid neuro-symbolic systems."
  },
  {
    "title": "LANGTRAJ: Diffusion Model and Dataset for Language-Conditioned Trajectory Simulation",
    "url": "http://arxiv.org/abs/2504.11521v1",
    "arxiv_id": "2504.11521v1",
    "authors": [
      "Wei-Jer Chang",
      "Wei Zhan",
      "Masayoshi Tomizuka",
      "Manmohan Chandraker",
      "Francesco Pittaluga"
    ],
    "published": "2025-04-15T17:14:06+00:00",
    "summary": "Evaluating autonomous vehicles with controllability enables scalable testing in counterfactual or structured settings, enhancing both efficiency and safety. We introduce LangTraj, a language-conditioned scene-diffusion model that simulates the joint behavior of all agents in traffic scenarios. By conditioning on natural language inputs, LangTraj provides flexible and intuitive control over interactive behaviors, generating nuanced and realistic scenarios. Unlike prior approaches that depend on domain-specific guidance functions, LangTraj incorporates language conditioning during training, facilitating more intuitive traffic simulation control. We propose a novel closed-loop training strategy for diffusion models, explicitly tailored to enhance stability and realism during closed-loop simulation. To support language-conditioned simulation, we develop Inter-Drive, a large-scale dataset with diverse and interactive labels for training language-conditioned diffusion models. Our dataset is built upon a scalable pipeline for annotating agent-agent interactions and single-agent behaviors, ensuring rich and varied supervision. Validated on the Waymo Motion Dataset, LangTraj demonstrates strong performance in realism, language controllability, and language-conditioned safety-critical simulation, establishing a new paradigm for flexible and scalable autonomous vehicle testing."
  },
  {
    "title": "A Minimalist Approach to LLM Reasoning: from Rejection Sampling to Reinforce",
    "url": "http://arxiv.org/abs/2504.11343v1",
    "arxiv_id": "2504.11343v1",
    "authors": [
      "Wei Xiong",
      "Jiarui Yao",
      "Yuhui Xu",
      "Bo Pang",
      "Lei Wang",
      "Doyen Sahoo",
      "Junnan Li",
      "Nan Jiang",
      "Tong Zhang",
      "Caiming Xiong",
      "Hanze Dong"
    ],
    "published": "2025-04-15T16:15:02+00:00",
    "summary": "Reinforcement learning (RL) has become a prevailing approach for fine-tuning large language models (LLMs) on complex reasoning tasks. Among recent methods, GRPO stands out for its empirical success in training models such as DeepSeek-R1, yet the sources of its effectiveness remain poorly understood. In this work, we revisit GRPO from a reinforce-like algorithm perspective and analyze its core components. Surprisingly, we find that a simple rejection sampling baseline, RAFT, which trains only on positively rewarded samples, yields competitive performance than GRPO and PPO. Our ablation studies reveal that GRPO's main advantage arises from discarding prompts with entirely incorrect responses, rather than from its reward normalization. Motivated by this insight, we propose Reinforce-Rej, a minimal extension of policy gradient that filters both entirely incorrect and entirely correct samples. Reinforce-Rej improves KL efficiency and stability, serving as a lightweight yet effective alternative to more complex RL algorithms. We advocate RAFT as a robust and interpretable baseline, and suggest that future advances should focus on more principled designs for incorporating negative samples, rather than relying on them indiscriminately. Our findings provide guidance for future work in reward-based LLM post-training."
  },
  {
    "title": "Robust MPC for Uncertain Linear Systems -- Combining Model Adaptation and Iterative Learning",
    "url": "http://arxiv.org/abs/2504.11261v1",
    "arxiv_id": "2504.11261v1",
    "authors": [
      "Hannes Petrenz",
      "Johannes K\u00f6hler",
      "Francesco Borrelli"
    ],
    "published": "2025-04-15T15:00:34+00:00",
    "summary": "This paper presents a robust adaptive learning Model Predictive Control (MPC) framework for linear systems with parametric uncertainties and additive disturbances performing iterative tasks. The approach iteratively refines the parameter estimates using set membership estimation. Performance enhancement over iterations is achieved by learning the terminal cost from data. Safety is enforced using a terminal set, which is also learned iteratively. The proposed method guarantees recursive feasibility, constraint satisfaction, and a robust bound on the closed-loop cost. Numerical simulations on a mass-spring-damper system demonstrate improved computational efficiency and control performance compared to an existing robust adaptive MPC approach."
  },
  {
    "title": "Robust MPC for Uncertain Linear Systems -- Combining Model Adaptation and Iterative Learning",
    "url": "http://arxiv.org/abs/2504.11261v2",
    "arxiv_id": "2504.11261v2",
    "authors": [
      "Hannes Petrenz",
      "Johannes K\u00f6hler",
      "Francesco Borrelli"
    ],
    "published": "2025-04-15T15:00:34+00:00",
    "summary": "This paper presents a robust adaptive learning Model Predictive Control (MPC) framework for linear systems with parametric uncertainties and additive disturbances performing iterative tasks. The approach iteratively refines the parameter estimates using set membership estimation. Performance enhancement over iterations is achieved by learning the terminal cost from data. Safety is enforced using a terminal set, which is also learned iteratively. The proposed method guarantees recursive feasibility, constraint satisfaction, and a robust bound on the closed-loop cost. Numerical simulations on a mass-spring-damper system demonstrate improved computational efficiency and control performance compared to an existing robust adaptive MPC approach."
  },
  {
    "title": "Towards Automated Safety Requirements Derivation Using Agent-based RAG",
    "url": "http://arxiv.org/abs/2504.11243v1",
    "arxiv_id": "2504.11243v1",
    "authors": [
      "Balahari Vignesh Balu",
      "Florian Geissler",
      "Francesco Carella",
      "Joao-Vitor Zacchi",
      "Josef Jiru",
      "Nuria Mata",
      "Reinhard Stolle"
    ],
    "published": "2025-04-15T14:43:19+00:00",
    "summary": "We study the automated derivation of safety requirements in a self-driving vehicle use case, leveraging LLMs in combination with agent-based retrieval-augmented generation. Conventional approaches that utilise pre-trained LLMs to assist in safety analyses typically lack domain-specific knowledge. Existing RAG approaches address this issue, yet their performance deteriorates when handling complex queries and it becomes increasingly harder to retrieve the most relevant information. This is particularly relevant for safety-relevant applications. In this paper, we propose the use of agent-based RAG to derive safety requirements and show that the retrieved information is more relevant to the queries. We implement an agent-based approach on a document pool of automotive standards and the Apollo case study, as a representative example of an automated driving perception system. Our solution is tested on a data set of safety requirement questions and answers, extracted from the Apollo data. Evaluating a set of selected RAG metrics, we present and discuss advantages of a agent-based approach compared to default RAG methods."
  },
  {
    "title": "Nondeterministic Polynomial-time Problem Challenge: An Ever-Scaling Reasoning Benchmark for LLMs",
    "url": "http://arxiv.org/abs/2504.11239v1",
    "arxiv_id": "2504.11239v1",
    "authors": [
      "Chang Yang",
      "Ruiyu Wang",
      "Junzhe Jiang",
      "Qi Jiang",
      "Qinggang Zhang",
      "Yanchen Deng",
      "Shuxin Li",
      "Shuyue Hu",
      "Bo Li",
      "Florian T. Pokorny",
      "Xiao Huang",
      "Xinrun Wang"
    ],
    "published": "2025-04-15T14:40:29+00:00",
    "summary": "Reasoning is the fundamental capability of large language models (LLMs). Due to the rapid progress of LLMs, there are two main issues of current benchmarks: i) these benchmarks can be crushed in a short time (less than 1 year), and ii) these benchmarks may be easily hacked. To handle these issues, we propose the ever-scalingness for building the benchmarks which are uncrushable, unhackable, auto-verifiable and general. This paper presents Nondeterministic Polynomial-time Problem Challenge (NPPC), an ever-scaling reasoning benchmark for LLMs. Specifically, the NPPC has three main modules: i) npgym, which provides a unified interface of 25 well-known NP-complete problems and can generate any number of instances with any levels of complexities, ii) npsolver: which provides a unified interface to evaluate the problem instances with both online and offline models via APIs and local deployments, respectively, and iii) npeval: which provides the comprehensive and ready-to-use tools to analyze the performances of LLMs over different problems, the number of tokens, the aha moments, the reasoning errors and the solution errors. Extensive experiments over widely-used LLMs demonstrate: i) NPPC can successfully decrease the performances of advanced LLMs' performances to below 10%, demonstrating that NPPC is uncrushable, ii) DeepSeek-R1, Claude-3.7-Sonnet, and o1/o3-mini are the most powerful LLMs, where DeepSeek-R1 outperforms Claude-3.7-Sonnet and o1/o3-mini in most NP-complete problems considered, and iii) the numbers of tokens, aha moments in the advanced LLMs, e.g., Claude-3.7-Sonnet and DeepSeek-R1, are observed first to increase and then decrease when the problem instances become more and more difficult. We believe that NPPC is the first ever-scaling reasoning benchmark, serving as the uncrushable and unhackable testbed for LLMs toward artificial general intelligence (AGI)."
  },
  {
    "title": "Enhancing Autonomous Driving Systems with On-Board Deployed Large Language Models",
    "url": "http://arxiv.org/abs/2504.11514v1",
    "arxiv_id": "2504.11514v1",
    "authors": [
      "Nicolas Baumann",
      "Cheng Hu",
      "Paviththiren Sivasothilingam",
      "Haotong Qin",
      "Lei Xie",
      "Michele Magno",
      "Luca Benini"
    ],
    "published": "2025-04-15T13:49:17+00:00",
    "summary": "Neural Networks (NNs) trained through supervised learning struggle with managing edge-case scenarios common in real-world driving due to the intractability of exhaustive datasets covering all edge-cases, making knowledge-driven approaches, akin to how humans intuitively detect unexpected driving behavior, a suitable complement to data-driven methods. This work proposes a hybrid architecture combining low-level Model Predictive Controller (MPC) with locally deployed Large Language Models (LLMs) to enhance decision-making and Human Machine Interaction (HMI). The DecisionxLLM module evaluates robotic state information against natural language instructions to ensure adherence to desired driving behavior. The MPCxLLM module then adjusts MPC parameters based on LLM-generated insights, achieving control adaptability while preserving the safety and constraint guarantees of traditional MPC systems. Further, to enable efficient on-board deployment and to eliminate dependency on cloud connectivity, we shift processing to the on-board computing platform: We propose an approach that exploits Retrieval Augmented Generation (RAG), Low Rank Adaptation (LoRA) fine-tuning, and quantization. Experimental results demonstrate that these enhancements yield significant improvements in reasoning accuracy by up to 10.45%, control adaptability by as much as 52.2%, and up to 10.5x increase in computational efficiency (tokens/s), validating the proposed framework's practicality for real-time deployment even on down-scaled robotic platforms. This work bridges high-level decision-making with low-level control adaptability, offering a synergistic framework for knowledge-driven and adaptive Autonomous Driving Systems (ADS)."
  },
  {
    "title": "Benchmarking Next-Generation Reasoning-Focused Large Language Models in Ophthalmology: A Head-to-Head Evaluation on 5,888 Items",
    "url": "http://arxiv.org/abs/2504.11186v1",
    "arxiv_id": "2504.11186v1",
    "authors": [
      "Minjie Zou",
      "Sahana Srinivasan",
      "Thaddaeus Wai Soon Lo",
      "Ke Zou",
      "Gabriel Dawei Yang",
      "Xuguang Ai",
      "Hyunjae Kim",
      "Maxwell Singer",
      "Fares Antaki",
      "Kelvin Li",
      "Robert Chang",
      "Marcus Tan",
      "David Ziyou Chen",
      "Dianbo Liu",
      "Qingyu Chen",
      "Yih Chung Tham"
    ],
    "published": "2025-04-15T13:42:34+00:00",
    "summary": "Recent advances in reasoning-focused large language models (LLMs) mark a shift from general LLMs toward models designed for complex decision-making, a crucial aspect in medicine. However, their performance in specialized domains like ophthalmology remains underexplored. This study comprehensively evaluated and compared the accuracy and reasoning capabilities of four newly developed reasoning-focused LLMs, namely DeepSeek-R1, OpenAI o1, o3-mini, and Gemini 2.0 Flash-Thinking. Each model was assessed using 5,888 multiple-choice ophthalmology exam questions from the MedMCQA dataset in zero-shot setting. Quantitative evaluation included accuracy, Macro-F1, and five text-generation metrics (ROUGE-L, METEOR, BERTScore, BARTScore, and AlignScore), computed against ground-truth reasonings. Average inference time was recorded for a subset of 100 randomly selected questions. Additionally, two board-certified ophthalmologists qualitatively assessed clarity, completeness, and reasoning structure of responses to differential diagnosis questions.O1 (0.902) and DeepSeek-R1 (0.888) achieved the highest accuracy, with o1 also leading in Macro-F1 (0.900). The performance of models across the text-generation metrics varied: O3-mini excelled in ROUGE-L (0.151), o1 in METEOR (0.232), DeepSeek-R1 and o3-mini tied for BERTScore (0.673), DeepSeek-R1 (-4.105) and Gemini 2.0 Flash-Thinking (-4.127) performed best in BARTScore, while o3-mini (0.181) and o1 (0.176) led AlignScore. Inference time across the models varied, with DeepSeek-R1 being slowest (40.4 seconds) and Gemini 2.0 Flash-Thinking fastest (6.7 seconds). Qualitative evaluation revealed that DeepSeek-R1 and Gemini 2.0 Flash-Thinking tended to provide detailed and comprehensive intermediate reasoning, whereas o1 and o3-mini displayed concise and summarized justifications."
  },
  {
    "title": "Exploring Backdoor Attack and Defense for LLM-empowered Recommendations",
    "url": "http://arxiv.org/abs/2504.11182v1",
    "arxiv_id": "2504.11182v1",
    "authors": [
      "Liangbo Ning",
      "Wenqi Fan",
      "Qing Li"
    ],
    "published": "2025-04-15T13:37:38+00:00",
    "summary": "The fusion of Large Language Models (LLMs) with recommender systems (RecSys) has dramatically advanced personalized recommendations and drawn extensive attention. Despite the impressive progress, the safety of LLM-based RecSys against backdoor attacks remains largely under-explored. In this paper, we raise a new problem: Can a backdoor with a specific trigger be injected into LLM-based Recsys, leading to the manipulation of the recommendation responses when the backdoor trigger is appended to an item's title? To investigate the vulnerabilities of LLM-based RecSys under backdoor attacks, we propose a new attack framework termed Backdoor Injection Poisoning for RecSys (BadRec). BadRec perturbs the items' titles with triggers and employs several fake users to interact with these items, effectively poisoning the training set and injecting backdoors into LLM-based RecSys. Comprehensive experiments reveal that poisoning just 1% of the training data with adversarial examples is sufficient to successfully implant backdoors, enabling manipulation of recommendations. To further mitigate such a security threat, we propose a universal defense strategy called Poison Scanner (P-Scanner). Specifically, we introduce an LLM-based poison scanner to detect the poisoned items by leveraging the powerful language understanding and rich knowledge of LLMs. A trigger augmentation agent is employed to generate diverse synthetic triggers to guide the poison scanner in learning domain-specific knowledge of the poisoned item detection task. Extensive experiments on three real-world datasets validate the effectiveness of the proposed P-Scanner."
  },
  {
    "title": "A Real-time Anomaly Detection Method for Robots based on a Flexible and Sparse Latent Space",
    "url": "http://arxiv.org/abs/2504.11170v1",
    "arxiv_id": "2504.11170v1",
    "authors": [
      "Taewook Kang",
      "Bum-Jae You",
      "Juyoun Park",
      "Yisoo Lee"
    ],
    "published": "2025-04-15T13:17:14+00:00",
    "summary": "The growing demand for robots to operate effectively in diverse environments necessitates the need for robust real-time anomaly detection techniques during robotic operations. However, deep learning-based models in robotics face significant challenges due to limited training data and highly noisy signal features. In this paper, we present Sparse Masked Autoregressive Flow-based Adversarial AutoEncoders model to address these problems. This approach integrates Masked Autoregressive Flow model into Adversarial AutoEncoders to construct a flexible latent space and utilize Sparse autoencoder to efficiently focus on important features, even in scenarios with limited feature space. Our experiments demonstrate that the proposed model achieves a 4.96% to 9.75% higher area under the receiver operating characteristic curve for pick-and-place robotic operations with randomly placed cans, compared to existing state-of-the-art methods. Notably, it showed up to 19.67% better performance in scenarios involving collisions with lightweight objects. Additionally, unlike the existing state-of-the-art model, our model performs inferences within 1 millisecond, ensuring real-time anomaly detection. These capabilities make our model highly applicable to machine learning-based robotic safety systems in dynamic environments. The code will be made publicly available after acceptance."
  },
  {
    "title": "A Real-time Anomaly Detection Method for Robots based on a Flexible and Sparse Latent Space",
    "url": "http://arxiv.org/abs/2504.11170v2",
    "arxiv_id": "2504.11170v2",
    "authors": [
      "Taewook Kang",
      "Bum-Jae You",
      "Juyoun Park",
      "Yisoo Lee"
    ],
    "published": "2025-04-15T13:17:14+00:00",
    "summary": "The growing demand for robots to operate effectively in diverse environments necessitates the need for robust real-time anomaly detection techniques during robotic operations. However, deep learning-based models in robotics face significant challenges due to limited training data and highly noisy signal features. In this paper, we present Sparse Masked Autoregressive Flow-based Adversarial AutoEncoders model to address these problems. This approach integrates Masked Autoregressive Flow model into Adversarial AutoEncoders to construct a flexible latent space and utilize Sparse autoencoder to efficiently focus on important features, even in scenarios with limited feature space. Our experiments demonstrate that the proposed model achieves a 4.96% to 9.75% higher area under the receiver operating characteristic curve for pick-and-place robotic operations with randomly placed cans, compared to existing state-of-the-art methods. Notably, it showed up to 19.67% better performance in scenarios involving collisions with lightweight objects. Additionally, unlike the existing state-of-the-art model, our model performs inferences within 1 millisecond, ensuring real-time anomaly detection. These capabilities make our model highly applicable to machine learning-based robotic safety systems in dynamic environments. The code will be made publicly available after acceptance."
  },
  {
    "title": "Bypassing Prompt Injection and Jailbreak Detection in LLM Guardrails",
    "url": "http://arxiv.org/abs/2504.11168v1",
    "arxiv_id": "2504.11168v1",
    "authors": [
      "William Hackett",
      "Lewis Birch",
      "Stefan Trawicki",
      "Neeraj Suri",
      "Peter Garraghan"
    ],
    "published": "2025-04-15T13:16:02+00:00",
    "summary": "Large Language Models (LLMs) guardrail systems are designed to protect against prompt injection and jailbreak attacks. However, they remain vulnerable to evasion techniques. We demonstrate two approaches for bypassing LLM prompt injection and jailbreak detection systems via traditional character injection methods and algorithmic Adversarial Machine Learning (AML) evasion techniques. Through testing against six prominent protection systems, including Microsoft's Azure Prompt Shield and Meta's Prompt Guard, we show that both methods can be used to evade detection while maintaining adversarial utility achieving in some instances up to 100% evasion success. Furthermore, we demonstrate that adversaries can enhance Attack Success Rates (ASR) against black-box targets by leveraging word importance ranking computed by offline white-box models. Our findings reveal vulnerabilities within current LLM protection mechanisms and highlight the need for more robust guardrail systems."
  },
  {
    "title": "Bypassing Prompt Injection and Jailbreak Detection in LLM Guardrails",
    "url": "http://arxiv.org/abs/2504.11168v2",
    "arxiv_id": "2504.11168v2",
    "authors": [
      "William Hackett",
      "Lewis Birch",
      "Stefan Trawicki",
      "Neeraj Suri",
      "Peter Garraghan"
    ],
    "published": "2025-04-15T13:16:02+00:00",
    "summary": "Large Language Models (LLMs) guardrail systems are designed to protect against prompt injection and jailbreak attacks. However, they remain vulnerable to evasion techniques. We demonstrate two approaches for bypassing LLM prompt injection and jailbreak detection systems via traditional character injection methods and algorithmic Adversarial Machine Learning (AML) evasion techniques. Through testing against six prominent protection systems, including Microsoft's Azure Prompt Shield and Meta's Prompt Guard, we show that both methods can be used to evade detection while maintaining adversarial utility achieving in some instances up to 100% evasion success. Furthermore, we demonstrate that adversaries can enhance Attack Success Rates (ASR) against black-box targets by leveraging word importance ranking computed by offline white-box models. Our findings reveal vulnerabilities within current LLM protection mechanisms and highlight the need for more robust guardrail systems."
  },
  {
    "title": "Token-Level Constraint Boundary Search for Jailbreaking Text-to-Image Models",
    "url": "http://arxiv.org/abs/2504.11106v1",
    "arxiv_id": "2504.11106v1",
    "authors": [
      "Jiangtao Liu",
      "Zhaoxin Wang",
      "Handing Wang",
      "Cong Tian",
      "Yaochu Jin"
    ],
    "published": "2025-04-15T11:53:40+00:00",
    "summary": "Recent advancements in Text-to-Image (T2I) generation have significantly enhanced the realism and creativity of generated images. However, such powerful generative capabilities pose risks related to the production of inappropriate or harmful content. Existing defense mechanisms, including prompt checkers and post-hoc image checkers, are vulnerable to sophisticated adversarial attacks. In this work, we propose TCBS-Attack, a novel query-based black-box jailbreak attack that searches for tokens located near the decision boundaries defined by text and image checkers. By iteratively optimizing tokens near these boundaries, TCBS-Attack generates semantically coherent adversarial prompts capable of bypassing multiple defensive layers in T2I models. Extensive experiments demonstrate that our method consistently outperforms state-of-the-art jailbreak attacks across various T2I models, including securely trained open-source models and commercial online services like DALL-E 3. TCBS-Attack achieves an ASR-4 of 45\\% and an ASR-1 of 21\\% on jailbreaking full-chain T2I models, significantly surpassing baseline methods."
  },
  {
    "title": "Neural Control Barrier Functions from Physics Informed Neural Networks",
    "url": "http://arxiv.org/abs/2504.11045v1",
    "arxiv_id": "2504.11045v1",
    "authors": [
      "Shreenabh Agrawal",
      "Manan Tayal",
      "Aditya Singh",
      "Shishir Kolathaya"
    ],
    "published": "2025-04-15T10:13:30+00:00",
    "summary": "As autonomous systems become increasingly prevalent in daily life, ensuring their safety is paramount. Control Barrier Functions (CBFs) have emerged as an effective tool for guaranteeing safety; however, manually designing them for specific applications remains a significant challenge. With the advent of deep learning techniques, recent research has explored synthesizing CBFs using neural networks-commonly referred to as neural CBFs. This paper introduces a novel class of neural CBFs that leverages a physics-inspired neural network framework by incorporating Zubov's Partial Differential Equation (PDE) within the context of safety. This approach provides a scalable methodology for synthesizing neural CBFs applicable to high-dimensional systems. Furthermore, by utilizing reciprocal CBFs instead of zeroing CBFs, the proposed framework allows for the specification of flexible, user-defined safe regions. To validate the effectiveness of the approach, we present case studies on three different systems: an inverted pendulum, autonomous ground navigation, and aerial navigation in obstacle-laden environments."
  },
  {
    "title": "DRIFT open dataset: A drone-derived intelligence for traffic analysis in urban environmen",
    "url": "http://arxiv.org/abs/2504.11019v1",
    "arxiv_id": "2504.11019v1",
    "authors": [
      "Hyejin Lee",
      "Seokjun Hong",
      "Jeonghoon Song",
      "Haechan Cho",
      "Zhixiong Jin",
      "Byeonghun Kim",
      "Joobin Jin",
      "Jaegyun Im",
      "Byeongjoon Noh",
      "Hwasoo Yeo"
    ],
    "published": "2025-04-15T09:43:13+00:00",
    "summary": "Reliable traffic data are essential for understanding urban mobility and developing effective traffic management strategies. This study introduces the DRone-derived Intelligence For Traffic analysis (DRIFT) dataset, a large-scale urban traffic dataset collected systematically from synchronized drone videos at approximately 250 meters altitude, covering nine interconnected intersections in Daejeon, South Korea. DRIFT provides high-resolution vehicle trajectories that include directional information, processed through video synchronization and orthomap alignment, resulting in a comprehensive dataset of 81,699 vehicle trajectories. Through our DRIFT dataset, researchers can simultaneously analyze traffic at multiple scales - from individual vehicle maneuvers like lane-changes and safety metrics such as time-to-collision to aggregate network flow dynamics across interconnected urban intersections. The DRIFT dataset is structured to enable immediate use without additional preprocessing, complemented by open-source models for object detection and trajectory extraction, as well as associated analytical tools. DRIFT is expected to significantly contribute to academic research and practical applications, such as traffic flow analysis and simulation studies. The dataset and related resources are publicly accessible at https://github.com/AIxMobility/The-DRIFT."
  },
  {
    "title": "Reward Distance Comparisons Under Transition Sparsity",
    "url": "http://arxiv.org/abs/2504.11508v1",
    "arxiv_id": "2504.11508v1",
    "authors": [
      "Clement Nyanhongo",
      "Bruno Miranda Henrique",
      "Eugene Santos"
    ],
    "published": "2025-04-15T09:27:53+00:00",
    "summary": "Reward comparisons are vital for evaluating differences in agent behaviors induced by a set of reward functions. Most conventional techniques utilize the input reward functions to learn optimized policies, which are then used to compare agent behaviors. However, learning these policies can be computationally expensive and can also raise safety concerns. Direct reward comparison techniques obviate policy learning but suffer from transition sparsity, where only a small subset of transitions are sampled due to data collection challenges and feasibility constraints. Existing state-of-the-art direct reward comparison methods are ill-suited for these sparse conditions since they require high transition coverage, where the majority of transitions from a given coverage distribution are sampled. When this requirement is not satisfied, a distribution mismatch between sampled and expected transitions can occur, leading to significant errors. This paper introduces the Sparsity Resilient Reward Distance (SRRD) pseudometric, designed to eliminate the need for high transition coverage by accommodating diverse sample distributions, which are common under transition sparsity. We provide theoretical justification for SRRD's robustness and conduct experiments to demonstrate its practical efficacy across multiple domains."
  },
  {
    "title": "Drivers and barriers of adopting shared micromobility: a latent class clustering model on the attitudes towards shared micromobility as part of public transport trips in the Netherlands",
    "url": "http://arxiv.org/abs/2504.10943v1",
    "arxiv_id": "2504.10943v1",
    "authors": [
      "Nejc Ger\u017eini\u010d",
      "Mark van Hagen",
      "Hussein Al-Tamimi",
      "Niels van Oort",
      "Dorine Duives"
    ],
    "published": "2025-04-15T07:46:19+00:00",
    "summary": "Shared micromobility (SMM) is often cited as a solution to the first/last mile problem of public transport (train) travel, yet when implemented, they often do not get adopted by a broader travelling public. A large part of behavioural adoption is related to peoples' attitudes and perceptions. In this paper, we develop an adjusted behavioural framework, based on the UTAUT2 technology acceptance framework. We carry out an exploratory factor analysis (EFA) to obtain attitudinal factors which we then use to perform a latent class cluster analysis (LCCA), with the goal of studying the potential adoption of SMM and to assess the various drivers and barriers as perceived by different user groups. Our findings suggest there are six distinct user groups with varying intention to use shared micromobility: Progressives, Conservatives, Hesitant participants, Bold innovators, Anxious observers and Skilled sceptics. Bold innovators and Progressives tend to be the most open to adopting SMM and are also able to do so. Hesitant participants would like to, but find it difficult or dangerous to use, while Skilled sceptics are capable and confident, but have limited intention of using it. Conservatives and Anxious observers are most negative about SMM, finding it difficult to use and dangerous. In general, factors relating to technological savviness, ease-of-use, physical safety and societal perception seem to be the biggest barriers to wider adoption. Younger, highly educated males are the group most likely and open to using shared micromobility, while older individuals with lower incomes and a lower level of education tend to be the least likely."
  },
  {
    "title": "Safe-Construct: Redefining Construction Safety Violation Recognition as 3D Multi-View Engagement Task",
    "url": "http://arxiv.org/abs/2504.10880v1",
    "arxiv_id": "2504.10880v1",
    "authors": [
      "Aviral Chharia",
      "Tianyu Ren",
      "Tomotake Furuhata",
      "Kenji Shimada"
    ],
    "published": "2025-04-15T05:21:09+00:00",
    "summary": "Recognizing safety violations in construction environments is critical yet remains underexplored in computer vision. Existing models predominantly rely on 2D object detection, which fails to capture the complexities of real-world violations due to: (i) an oversimplified task formulation treating violation recognition merely as object detection, (ii) inadequate validation under realistic conditions, (iii) absence of standardized baselines, and (iv) limited scalability from the unavailability of synthetic dataset generators for diverse construction scenarios. To address these challenges, we introduce Safe-Construct, the first framework that reformulates violation recognition as a 3D multi-view engagement task, leveraging scene-level worker-object context and 3D spatial understanding. We also propose the Synthetic Indoor Construction Site Generator (SICSG) to create diverse, scalable training data, overcoming data limitations. Safe-Construct achieves a 7.6% improvement over state-of-the-art methods across four violation types. We rigorously evaluate our approach in near-realistic settings, incorporating four violations, four workers, 14 objects, and challenging conditions like occlusions (worker-object, worker-worker) and variable illumination (back-lighting, overexposure, sunlight). By integrating 3D multi-view spatial understanding and synthetic data generation, Safe-Construct sets a new benchmark for scalable and robust safety monitoring in high-risk industries. Project Website: https://Safe-Construct.github.io/Safe-Construct"
  },
  {
    "title": "Hallucination-Aware Generative Pretrained Transformer for Cooperative Aerial Mobility Control",
    "url": "http://arxiv.org/abs/2504.10831v1",
    "arxiv_id": "2504.10831v1",
    "authors": [
      "Hyojun Ahn",
      "Seungcheol Oh",
      "Gyu Seon Kim",
      "Soyi Jung",
      "Soohyun Park",
      "Joongheon Kim"
    ],
    "published": "2025-04-15T03:21:08+00:00",
    "summary": "This paper proposes SafeGPT, a two-tiered framework that integrates generative pretrained transformers (GPTs) with reinforcement learning (RL) for efficient and reliable unmanned aerial vehicle (UAV) last-mile deliveries. In the proposed design, a Global GPT module assigns high-level tasks such as sector allocation, while an On-Device GPT manages real-time local route planning. An RL-based safety filter monitors each GPT decision and overrides unsafe actions that could lead to battery depletion or duplicate visits, effectively mitigating hallucinations. Furthermore, a dual replay buffer mechanism helps both the GPT modules and the RL agent refine their strategies over time. Simulation results demonstrate that SafeGPT achieves higher delivery success rates compared to a GPT-only baseline, while substantially reducing battery consumption and travel distance. These findings validate the efficacy of combining GPT-based semantic reasoning with formal safety guarantees, contributing a viable solution for robust and energy-efficient UAV logistics."
  },
  {
    "title": "LayoutCoT: Unleashing the Deep Reasoning Potential of Large Language Models for Layout Generation",
    "url": "http://arxiv.org/abs/2504.10829v1",
    "arxiv_id": "2504.10829v1",
    "authors": [
      "Hengyu Shi",
      "Junhao Su",
      "Huansheng Ning",
      "Xiaoming Wei",
      "Jialin Gao"
    ],
    "published": "2025-04-15T03:12:01+00:00",
    "summary": "Conditional layout generation aims to automatically generate visually appealing and semantically coherent layouts from user-defined constraints. While recent methods based on generative models have shown promising results, they typically require substantial amounts of training data or extensive fine-tuning, limiting their versatility and practical applicability. Alternatively, some training-free approaches leveraging in-context learning with Large Language Models (LLMs) have emerged, but they often suffer from limited reasoning capabilities and overly simplistic ranking mechanisms, which restrict their ability to generate consistently high-quality layouts. To this end, we propose LayoutCoT, a novel approach that leverages the reasoning capabilities of LLMs through a combination of Retrieval-Augmented Generation (RAG) and Chain-of-Thought (CoT) techniques. Specifically, LayoutCoT transforms layout representations into a standardized serialized format suitable for processing by LLMs. A Layout-aware RAG is used to facilitate effective retrieval and generate a coarse layout by LLMs. This preliminary layout, together with the selected exemplars, is then fed into a specially designed CoT reasoning module for iterative refinement, significantly enhancing both semantic coherence and visual quality. We conduct extensive experiments on five public datasets spanning three conditional layout generation tasks. Experimental results demonstrate that LayoutCoT achieves state-of-the-art performance without requiring training or fine-tuning. Notably, our CoT reasoning module enables standard LLMs, even those without explicit deep reasoning abilities, to outperform specialized deep-reasoning models such as deepseek-R1, highlighting the potential of our approach in unleashing the deep reasoning capabilities of LLMs for layout generation tasks."
  },
  {
    "title": "A Framework for the Private Governance of Frontier Artificial Intelligence",
    "url": "http://arxiv.org/abs/2504.11501v1",
    "arxiv_id": "2504.11501v1",
    "authors": [
      "Dean W. Ball"
    ],
    "published": "2025-04-15T02:56:26+00:00",
    "summary": "This paper presents a proposal for the governance of frontier AI systems through a hybrid public-private system. Private bodies, authorized and overseen by government, provide certifications to developers of frontier AI systems on an opt-in basis. In exchange for opting in, frontier AI firms receive protections from tort liability for customer misuse of their models. Before detailing the proposal, the paper explores more commonly discussed approaches to AI governance, analyzing their strengths and flaws. It also examines the nature of frontier AI governance itself. The paper includes consideration of the political economic, institutional, legal, safety, and other merits and tradeoffs inherent in the governance system it proposes."
  },
  {
    "title": "The Sword of Damocles in ViTs: Computational Redundancy Amplifies Adversarial Transferability",
    "url": "http://arxiv.org/abs/2504.10804v1",
    "arxiv_id": "2504.10804v1",
    "authors": [
      "Jiani Liu",
      "Zhiyuan Wang",
      "Zeliang Zhang",
      "Chao Huang",
      "Susan Liang",
      "Yunlong Tang",
      "Chenliang Xu"
    ],
    "published": "2025-04-15T01:59:47+00:00",
    "summary": "Vision Transformers (ViTs) have demonstrated impressive performance across a range of applications, including many safety-critical tasks. However, their unique architectural properties raise new challenges and opportunities in adversarial robustness. In particular, we observe that adversarial examples crafted on ViTs exhibit higher transferability compared to those crafted on CNNs, suggesting that ViTs contain structural characteristics favorable for transferable attacks. In this work, we investigate the role of computational redundancy in ViTs and its impact on adversarial transferability. Unlike prior studies that aim to reduce computation for efficiency, we propose to exploit this redundancy to improve the quality and transferability of adversarial examples. Through a detailed analysis, we identify two forms of redundancy, including the data-level and model-level, that can be harnessed to amplify attack effectiveness. Building on this insight, we design a suite of techniques, including attention sparsity manipulation, attention head permutation, clean token regularization, ghost MoE diversification, and test-time adversarial training. Extensive experiments on the ImageNet-1k dataset validate the effectiveness of our approach, showing that our methods significantly outperform existing baselines in both transferability and generality across diverse model architectures."
  },
  {
    "title": "Products of Recursive Programs for Hypersafety Verification",
    "url": "http://arxiv.org/abs/2504.10800v1",
    "arxiv_id": "2504.10800v1",
    "authors": [
      "Ruotong Cheng",
      "Azadeh Farzan"
    ],
    "published": "2025-04-15T01:52:50+00:00",
    "summary": "We study the problem of automated hypersafety verification of infinite-state recursive programs. We propose an infinite class of product programs, specifically designed with recursion in mind, that reduce the hypersafety verification of a recursive program to standard safety verification. For this, we combine insights from language theory and concurrency theory to propose an algorithmic solution for constructing an infinite class of recursive product programs. One key insight is that, using the simple theory of visibly pushdown languages, one can maintain the recursive structure of syntactic program alignments which is vital to constructing a new product program that can be viewed as a classic recursive program -- that is, one that can be executed on a single stack. Another key insight is that techniques from concurrency theory can be generalized to help define product programs based on the view that the parallel composition of individual recursive programs includes all possible alignments from which a sound set of alignments that faithfully preserve the satisfaction of the hypersafety property can be selected. On the practical side, we formulate a family of parametric canonical product constructions that are intuitive to programmers and can be used as building blocks to specify recursive product programs for the purpose of relational and hypersafety verification, with the idea that the right product program can be verified automatically using existing techniques. We demonstrate the effectiveness of these techniques through an implementation and highly promising experimental results."
  },
  {
    "title": "ReasonDrive: Efficient Visual Question Answering for Autonomous Vehicles with Reasoning-Enhanced Small Vision-Language Models",
    "url": "http://arxiv.org/abs/2504.10757v1",
    "arxiv_id": "2504.10757v1",
    "authors": [
      "Amirhosein Chahe",
      "Lifeng Zhou"
    ],
    "published": "2025-04-14T23:16:07+00:00",
    "summary": "Vision-language models (VLMs) show promise for autonomous driving but often lack transparent reasoning capabilities that are critical for safety. We investigate whether explicitly modeling reasoning during fine-tuning enhances VLM performance on driving decision tasks. Using GPT-4o, we generate structured reasoning chains for driving scenarios from the DriveLM benchmark with category-specific prompting strategies. We compare reasoning-based fine-tuning, answer-only fine-tuning, and baseline instruction-tuned models across multiple small VLM families (Llama 3.2, Llava 1.5, and Qwen 2.5VL). Our results demonstrate that reasoning-based fine-tuning consistently outperforms alternatives, with Llama3.2-11B-reason achieving the highest performance. Models fine-tuned with reasoning show substantial improvements in accuracy and text generation quality, suggesting explicit reasoning enhances internal representations for driving decisions. These findings highlight the importance of transparent decision processes in safety-critical domains and offer a promising direction for developing more interpretable autonomous driving systems."
  },
  {
    "title": "The Jailbreak Tax: How Useful are Your Jailbreak Outputs?",
    "url": "http://arxiv.org/abs/2504.10694v1",
    "arxiv_id": "2504.10694v1",
    "authors": [
      "Kristina Nikoli\u0107",
      "Luze Sun",
      "Jie Zhang",
      "Florian Tram\u00e8r"
    ],
    "published": "2025-04-14T20:30:41+00:00",
    "summary": "Jailbreak attacks bypass the guardrails of large language models to produce harmful outputs. In this paper, we ask whether the model outputs produced by existing jailbreaks are actually useful. For example, when jailbreaking a model to give instructions for building a bomb, does the jailbreak yield good instructions? Since the utility of most unsafe answers (e.g., bomb instructions) is hard to evaluate rigorously, we build new jailbreak evaluation sets with known ground truth answers, by aligning models to refuse questions related to benign and easy-to-evaluate topics (e.g., biology or math). Our evaluation of eight representative jailbreaks across five utility benchmarks reveals a consistent drop in model utility in jailbroken responses, which we term the jailbreak tax. For example, while all jailbreaks we tested bypass guardrails in models aligned to refuse to answer math, this comes at the expense of a drop of up to 92% in accuracy. Overall, our work proposes the jailbreak tax as a new important metric in AI safety, and introduces benchmarks to evaluate existing and future jailbreaks. We make the benchmark available at https://github.com/ethz-spylab/jailbreak-tax"
  },
  {
    "title": "Beyond Chains of Thought: Benchmarking Latent-Space Reasoning Abilities in Large Language Models",
    "url": "http://arxiv.org/abs/2504.10615v1",
    "arxiv_id": "2504.10615v1",
    "authors": [
      "Thilo Hagendorff",
      "Sarah Fabi"
    ],
    "published": "2025-04-14T18:15:27+00:00",
    "summary": "Large language models (LLMs) can perform reasoning computations both internally within their latent space and externally by generating explicit token sequences like chains of thought. Significant progress in enhancing reasoning abilities has been made by scaling test-time compute. However, understanding and quantifying model-internal reasoning abilities - the inferential \"leaps\" models make between individual token predictions - remains crucial. This study introduces a benchmark (n = 4,000 items) designed to quantify model-internal reasoning in different domains. We achieve this by having LLMs indicate the correct solution to reasoning problems not through descriptive text, but by selecting a specific language of their initial response token that is different from English, the benchmark language. This not only requires models to reason beyond their context window, but also to overrise their default tendency to respond in the same language as the prompt, thereby posing an additional cognitive strain. We evaluate a set of 18 LLMs, showing significant performance variations, with GPT-4.5 achieving the highest accuracy (74.7%), outperforming models like Grok-2 (67.2%), and Llama 3.1 405B (65.6%). Control experiments and difficulty scaling analyses suggest that while LLMs engage in internal reasoning, we cannot rule out heuristic exploitations under certain conditions, marking an area for future investigation. Our experiments demonstrate that LLMs can \"think\" via latent-space computations, revealing model-internal inference strategies that need further understanding, especially regarding safety-related concerns such as covert planning, goal-seeking, or deception emerging without explicit token traces."
  },
  {
    "title": "Decoupled Diffusion Sparks Adaptive Scene Generation",
    "url": "http://arxiv.org/abs/2504.10485v1",
    "arxiv_id": "2504.10485v1",
    "authors": [
      "Yunsong Zhou",
      "Naisheng Ye",
      "William Ljungbergh",
      "Tianyu Li",
      "Jiazhi Yang",
      "Zetong Yang",
      "Hongzi Zhu",
      "Christoffer Petersson",
      "Hongyang Li"
    ],
    "published": "2025-04-14T17:59:57+00:00",
    "summary": "Controllable scene generation could reduce the cost of diverse data collection substantially for autonomous driving. Prior works formulate the traffic layout generation as predictive progress, either by denoising entire sequences at once or by iteratively predicting the next frame. However, full sequence denoising hinders online reaction, while the latter's short-sighted next-frame prediction lacks precise goal-state guidance. Further, the learned model struggles to generate complex or challenging scenarios due to a large number of safe and ordinal driving behaviors from open datasets. To overcome these, we introduce Nexus, a decoupled scene generation framework that improves reactivity and goal conditioning by simulating both ordinal and challenging scenarios from fine-grained tokens with independent noise states. At the core of the decoupled pipeline is the integration of a partial noise-masking training strategy and a noise-aware schedule that ensures timely environmental updates throughout the denoising process. To complement challenging scenario generation, we collect a dataset consisting of complex corner cases. It covers 540 hours of simulated data, including high-risk interactions such as cut-in, sudden braking, and collision. Nexus achieves superior generation realism while preserving reactivity and goal orientation, with a 40% reduction in displacement error. We further demonstrate that Nexus improves closed-loop planning by 20% through data augmentation and showcase its capability in safety-critical data generation."
  },
  {
    "title": "xVerify: Efficient Answer Verifier for Reasoning Model Evaluations",
    "url": "http://arxiv.org/abs/2504.10481v1",
    "arxiv_id": "2504.10481v1",
    "authors": [
      "Ding Chen",
      "Qingchen Yu",
      "Pengyuan Wang",
      "Wentao Zhang",
      "Bo Tang",
      "Feiyu Xiong",
      "Xinchi Li",
      "Minchuan Yang",
      "Zhiyu Li"
    ],
    "published": "2025-04-14T17:59:36+00:00",
    "summary": "With the release of the o1 model by OpenAI, reasoning models adopting slow thinking strategies have gradually emerged. As the responses generated by such models often include complex reasoning, intermediate steps, and self-reflection, existing evaluation methods are often inadequate. They struggle to determine whether the LLM output is truly equivalent to the reference answer, and also have difficulty identifying and extracting the final answer from long, complex responses. To address this issue, we propose xVerify, an efficient answer verifier for reasoning model evaluations. xVerify demonstrates strong capability in equivalence judgment, enabling it to effectively determine whether the answers produced by reasoning models are equivalent to reference answers across various types of objective questions. To train and evaluate xVerify, we construct the VAR dataset by collecting question-answer pairs generated by multiple LLMs across various datasets, leveraging multiple reasoning models and challenging evaluation sets designed specifically for reasoning model assessment. A multi-round annotation process is employed to ensure label accuracy. Based on the VAR dataset, we train multiple xVerify models of different scales. In evaluation experiments conducted on both the test set and generalization set, all xVerify models achieve overall F1 scores and accuracy exceeding 95\\%. Notably, the smallest variant, xVerify-0.5B-I, outperforms all evaluation methods except GPT-4o, while xVerify-3B-Ib surpasses GPT-4o in overall performance. These results validate the effectiveness and generalizability of xVerify."
  },
  {
    "title": "GUI-R1 : A Generalist R1-Style Vision-Language Action Model For GUI Agents",
    "url": "http://arxiv.org/abs/2504.10458v1",
    "arxiv_id": "2504.10458v1",
    "authors": [
      "Xiaobo Xia",
      "Run Luo"
    ],
    "published": "2025-04-14T17:45:54+00:00",
    "summary": "Existing efforts in building Graphical User Interface (GUI) agents largely rely on the training paradigm of supervised fine-tuning on Large Vision-Language Models (LVLMs). However, this approach not only demands extensive amounts of training data but also struggles to effectively understand GUI screenshots and generalize to unseen interfaces. The issue significantly limits its application in real-world scenarios, especially for high-level tasks. Inspired by Reinforcement Fine-Tuning (RFT) in large reasoning models (e.g., DeepSeek-R1), which efficiently enhances the problem-solving capabilities of large language models in real-world settings, we propose \\name, the first reinforcement learning framework designed to enhance the GUI capabilities of LVLMs in high-level real-world task scenarios, through unified action space rule modeling. By leveraging a small amount of carefully curated high-quality data across multiple platforms (including Windows, Linux, MacOS, Android, and Web) and employing policy optimization algorithms such as Group Relative Policy Optimization (GRPO) to update the model, \\name achieves superior performance using only 0.02\\% of the data (3K vs. 13M) compared to previous state-of-the-art methods like OS-Atlas across eight benchmarks spanning three different platforms (mobile, desktop, and web). These results demonstrate the immense potential of reinforcement learning based on unified action space rule modeling in improving the execution capabilities of LVLMs for real-world GUI agent tasks."
  },
  {
    "title": "GUI-R1 : A Generalist R1-Style Vision-Language Action Model For GUI Agents",
    "url": "http://arxiv.org/abs/2504.10458v2",
    "arxiv_id": "2504.10458v2",
    "authors": [
      "Xiaobo Xia",
      "Run Luo"
    ],
    "published": "2025-04-14T17:45:54+00:00",
    "summary": "Existing efforts in building Graphical User Interface (GUI) agents largely rely on the training paradigm of supervised fine-tuning on Large Vision-Language Models (LVLMs). However, this approach not only demands extensive amounts of training data but also struggles to effectively understand GUI screenshots and generalize to unseen interfaces. The issue significantly limits its application in real-world scenarios, especially for high-level tasks. Inspired by Reinforcement Fine-Tuning (RFT) in large reasoning models (e.g., DeepSeek-R1), which efficiently enhances the problem-solving capabilities of large language models in real-world settings, we propose \\name, the first reinforcement learning framework designed to enhance the GUI capabilities of LVLMs in high-level real-world task scenarios, through unified action space rule modeling. By leveraging a small amount of carefully curated high-quality data across multiple platforms (including Windows, Linux, MacOS, Android, and Web) and employing policy optimization algorithms such as Group Relative Policy Optimization (GRPO) to update the model, \\name achieves superior performance using only 0.02\\% of the data (3K vs. 13M) compared to previous state-of-the-art methods like OS-Atlas across eight benchmarks spanning three different platforms (mobile, desktop, and web). These results demonstrate the immense potential of reinforcement learning based on unified action space rule modeling in improving the execution capabilities of LVLMs for real-world GUI agent tasks."
  },
  {
    "title": "M1: Towards Scalable Test-Time Compute with Mamba Reasoning Models",
    "url": "http://arxiv.org/abs/2504.10449v1",
    "arxiv_id": "2504.10449v1",
    "authors": [
      "Junxiong Wang",
      "Wen-Ding Li",
      "Daniele Paliotta",
      "Daniel Ritter",
      "Alexander M. Rush",
      "Tri Dao"
    ],
    "published": "2025-04-14T17:38:25+00:00",
    "summary": "Effective reasoning is crucial to solving complex mathematical problems. Recent large language models (LLMs) have boosted performance by scaling test-time computation through long chain-of-thought reasoning. However, transformer-based models are inherently limited in extending context length due to their quadratic computational complexity and linear memory requirements. In this paper, we introduce a novel hybrid linear RNN reasoning model, M1, built on the Mamba architecture, which allows memory-efficient inference. Our approach leverages a distillation process from existing reasoning models and is further enhanced through RL training. Experimental results on the AIME and MATH benchmarks show that M1 not only outperforms previous linear RNN models but also matches the performance of state-of-the-art Deepseek R1 distilled reasoning models at a similar scale. We also compare our generation speed with a highly performant general purpose inference engine, vLLM, and observe more than a 3x speedup compared to a same size transformer. With throughput speedup, we are able to achieve higher accuracy compared to DeepSeek R1 distilled transformer reasoning models under a fixed generation time budget using self-consistency voting. Overall, we introduce a hybrid Mamba reasoning model and provide a more effective approach to scaling test-time generation using self-consistency or long chain of thought reasoning."
  },
  {
    "title": "LLM Can be a Dangerous Persuader: Empirical Study of Persuasion Safety in Large Language Models",
    "url": "http://arxiv.org/abs/2504.10430v1",
    "arxiv_id": "2504.10430v1",
    "authors": [
      "Minqian Liu",
      "Zhiyang Xu",
      "Xinyi Zhang",
      "Heajun An",
      "Sarvech Qadir",
      "Qi Zhang",
      "Pamela J. Wisniewski",
      "Jin-Hee Cho",
      "Sang Won Lee",
      "Ruoxi Jia",
      "Lifu Huang"
    ],
    "published": "2025-04-14T17:20:34+00:00",
    "summary": "Recent advancements in Large Language Models (LLMs) have enabled them to approach human-level persuasion capabilities. However, such potential also raises concerns about the safety risks of LLM-driven persuasion, particularly their potential for unethical influence through manipulation, deception, exploitation of vulnerabilities, and many other harmful tactics. In this work, we present a systematic investigation of LLM persuasion safety through two critical aspects: (1) whether LLMs appropriately reject unethical persuasion tasks and avoid unethical strategies during execution, including cases where the initial persuasion goal appears ethically neutral, and (2) how influencing factors like personality traits and external pressures affect their behavior. To this end, we introduce PersuSafety, the first comprehensive framework for the assessment of persuasion safety which consists of three stages, i.e., persuasion scene creation, persuasive conversation simulation, and persuasion safety assessment. PersuSafety covers 6 diverse unethical persuasion topics and 15 common unethical strategies. Through extensive experiments across 8 widely used LLMs, we observe significant safety concerns in most LLMs, including failing to identify harmful persuasion tasks and leveraging various unethical persuasion strategies. Our study calls for more attention to improve safety alignment in progressive and goal-driven conversations such as persuasion."
  },
  {
    "title": "Ctrl-Z: Controlling AI Agents via Resampling",
    "url": "http://arxiv.org/abs/2504.10374v1",
    "arxiv_id": "2504.10374v1",
    "authors": [
      "Aryan Bhatt",
      "Cody Rushing",
      "Adam Kaufman",
      "Tyler Tracy",
      "Vasil Georgiev",
      "David Matolcsi",
      "Akbir Khan",
      "Buck Shlegeris"
    ],
    "published": "2025-04-14T16:22:11+00:00",
    "summary": "Control evaluations measure whether monitoring and security protocols for AI systems prevent intentionally subversive AI models from causing harm. Our work presents the first control evaluation performed in an agent environment. We construct BashBench, a dataset of 257 challenging multi-step system administration tasks, and evaluate whether various safety measures can prevent an adversarially constructed AI agent from covertly downloading and executing malicious code in this environment. This multi-step setting introduces new attack and defense dynamics, which we investigate in order to design novel control protocols that prevent safety failures without hindering the ability of non-malicious agents to perform useful work. We introduce a class of control protocols called resample protocols that dynamically take additional samples of certain actions. We find these protocols significantly improve on existing techniques by selectively blocking the AI agent from executing suspicious code and incriminating the agent by generating additional examples of dangerous behavior. We measure the tradeoff between attack prevention and usefulness; our best protocol combines resampling with analysis of previous steps, reducing the success rate of attacks from 58% to 7% at a 5% cost to the performance of a non-malicious agent."
  },
  {
    "title": "Reactive power flow optimization in AC drive systems",
    "url": "http://arxiv.org/abs/2504.10360v1",
    "arxiv_id": "2504.10360v1",
    "authors": [
      "Sanjay Chandrasekaran",
      "Catalin Arghir",
      "Pieder Joerg",
      "Florian Doerfler",
      "Silvia Mastellone"
    ],
    "published": "2025-04-14T16:08:32+00:00",
    "summary": "This paper explores a limit avoidance approach in the case of input (modulation) and output (current) constraints with the aim of enhancing system availability of AC drives. Drawing on the observation that, in a certain range of reactive power, there exists a trade-off between current and modulation magnitude, we exploit this freedom and define a constrained optimization problem. We propose two approaches, one in the form of an activation-function which drives the reactive power set-point towards safety, and an approach which uses online feedback optimization to set the reactive power dynamically. Both methods compromise reactive power tracking accuracy for increased system robustness. Through a high fidelity simulation, we compare the benefits of the two methods, highlighting their effectiveness in industrial applications."
  },
  {
    "title": "Improving diffusion modeling in all-solid-state lithium batteries: a novel approach for grain boundary effects",
    "url": "http://arxiv.org/abs/2504.10348v1",
    "arxiv_id": "2504.10348v1",
    "authors": [
      "Lena Scholz",
      "Yongliang Ou",
      "Blazej Grabowski",
      "Felix Fritzen"
    ],
    "published": "2025-04-14T15:58:25+00:00",
    "summary": "All-solid-state lithium-ion batteries offer promising advantages with respect to capacity, safety, and performance. The diffusion behavior of lithium ions in the contained polycrystalline solid-state electrolyte is crucial for battery function. While atomistic studies indicate that grain boundaries (GBs) and grain size significantly impact diffusivity, the corresponding effects are either neglected in simulations on larger scales or considered only under strong assumptions such as isotropy. Our approach considers the fully resolved crystalline structure with a parametrization aligned with the atomistic perspective to describe diffusion along and across GBs. The approach is embedded into a finite element simulation using a novel collapsed interface element based on an analytical description in thickness direction. Results are governed by different and potentially anisotropic diffusion coefficients in bulk and GB domains. The mesoscale response is derived using linear computational homogenization to capture large-scale effects. The novel collapsed interface description allows for a reconstruction of the 3D transport behavior within the GB domain without resolving it and is able to capture the relevant transport mechanisms such as channeling effects and concentration jumps. Grain size and GB volume fraction are expressed in terms of an affine parameter dependence and can be altered without any changes to geometry or mesh. Together with the observed dependence of the effective material response on the anisotropic GB parametrization, this leads to the identification of four distinct diffusion regimes, each with implications for the design of battery materials."
  },
  {
    "title": "Heimdall: test-time scaling on the generative verification",
    "url": "http://arxiv.org/abs/2504.10337v1",
    "arxiv_id": "2504.10337v1",
    "authors": [
      "Wenlei Shi",
      "Xing Jin"
    ],
    "published": "2025-04-14T15:46:33+00:00",
    "summary": "An AI system can create and maintain knowledge only to the extent that it can verify that knowledge itself. Recent work on long Chain-of-Thought reasoning has demonstrated great potential of LLMs on solving competitive problems, but their verification ability remains to be weak and not sufficiently investigated. In this paper, we propose Heimdall, the long CoT verification LLM that can accurately judge the correctness of solutions. With pure reinforcement learning, we boost the verification accuracy from 62.5% to 94.5% on competitive math problems. By scaling with repeated sampling, the accuracy further increases to 97.5%. Through human evaluation, Heimdall demonstrates impressive generalization capabilities, successfully detecting most issues in challenging math proofs, the type of which is not included during training. Furthermore, we propose Pessimistic Verification to extend the functionality of Heimdall to scaling up the problem solving. It calls Heimdall to judge the solutions from a solver model and based on the pessimistic principle, selects the most likely correct solution with the least uncertainty. Taking DeepSeek-R1-Distill-Qwen-32B as the solver model, Pessimistic Verification improves the solution accuracy on AIME2025 from 54.2% to 70.0% with 16x compute budget and to 83.3% with more compute budget. With the stronger solver Gemini 2.5 Pro, the score reaches 93.0%. Finally, we prototype an automatic knowledge discovery system, a ternary system where one poses questions, another provides solutions, and the third verifies the solutions. Using the data synthesis work NuminaMath for the first two components, Heimdall effectively identifies problematic records within the dataset and reveals that nearly half of the data is flawed, which interestingly aligns with the recent ablation studies from NuminaMath."
  },
  {
    "title": "Heimdall: test-time scaling on the generative verification",
    "url": "http://arxiv.org/abs/2504.10337v2",
    "arxiv_id": "2504.10337v2",
    "authors": [
      "Wenlei Shi",
      "Xing Jin"
    ],
    "published": "2025-04-14T15:46:33+00:00",
    "summary": "An AI system can create and maintain knowledge only to the extent that it can verify that knowledge itself. Recent work on long Chain-of-Thought reasoning has demonstrated great potential of LLMs on solving competitive problems, but their verification ability remains to be weak and not sufficiently investigated. In this paper, we propose Heimdall, the long CoT verification LLM that can accurately judge the correctness of solutions. With pure reinforcement learning, we boost the verification accuracy from 62.5% to 94.5% on competitive math problems. By scaling with repeated sampling, the accuracy further increases to 97.5%. Through human evaluation, Heimdall demonstrates impressive generalization capabilities, successfully detecting most issues in challenging math proofs, the type of which is not included during training. Furthermore, we propose Pessimistic Verification to extend the functionality of Heimdall to scaling up the problem solving. It calls Heimdall to judge the solutions from a solver model and based on the pessimistic principle, selects the most likely correct solution with the least uncertainty. Taking DeepSeek-R1-Distill-Qwen-32B as the solver model, Pessimistic Verification improves the solution accuracy on AIME2025 from 54.2% to 70.0% with 16x compute budget and to 83.3% with more compute budget. With the stronger solver Gemini 2.5 Pro, the score reaches 93.0%. Finally, we prototype an automatic knowledge discovery system, a ternary system where one poses questions, another provides solutions, and the third verifies the solutions. Using the data synthesis work NuminaMath for the first two components, Heimdall effectively identifies problematic records within the dataset and reveals that nearly half of the data is flawed, which interestingly aligns with the recent ablation studies from NuminaMath."
  },
  {
    "title": "Cumulative-Time Signal Temporal Logic",
    "url": "http://arxiv.org/abs/2504.10325v1",
    "arxiv_id": "2504.10325v1",
    "authors": [
      "Hongkai Chen",
      "Zeyu Zhang",
      "Shouvik Roy",
      "Ezio Bartocci",
      "Scott A. Smolka",
      "Scott D. Stoller",
      "Shan Lin"
    ],
    "published": "2025-04-14T15:34:13+00:00",
    "summary": "Signal Temporal Logic (STL) is a widely adopted specification language in cyber-physical systems for expressing critical temporal requirements, such as safety conditions and response time. However, STL's expressivity is not sufficient to capture the cumulative duration during which a property holds within an interval of time. To overcome this limitation, we introduce Cumulative-Time Signal Temporal Logic (CT-STL) that operates over discrete-time signals and extends STL with a new cumulative-time operator. This operator compares the sum of all time steps for which its nested formula is true with a threshold. We present both a qualitative and a quantitative (robustness) semantics for CT-STL and prove both their soundness and completeness properties. We provide an efficient online monitoring algorithm for both semantics. Finally, we show the applicability of CT-STL in two case studies: specifying and monitoring cumulative temporal requirements for a microgrid and an artificial pancreas."
  },
  {
    "title": "SlowFastVAD: Video Anomaly Detection via Integrating Simple Detector and RAG-Enhanced Vision-Language Model",
    "url": "http://arxiv.org/abs/2504.10320v1",
    "arxiv_id": "2504.10320v1",
    "authors": [
      "Zongcan Ding",
      "Haodong Zhang",
      "Peng Wu",
      "Guansong Pang",
      "Zhiwei Yang",
      "Peng Wang",
      "Yanning Zhang"
    ],
    "published": "2025-04-14T15:30:03+00:00",
    "summary": "Video anomaly detection (VAD) aims to identify unexpected events in videos and has wide applications in safety-critical domains. While semi-supervised methods trained on only normal samples have gained traction, they often suffer from high false alarm rates and poor interpretability. Recently, vision-language models (VLMs) have demonstrated strong multimodal reasoning capabilities, offering new opportunities for explainable anomaly detection. However, their high computational cost and lack of domain adaptation hinder real-time deployment and reliability. Inspired by dual complementary pathways in human visual perception, we propose SlowFastVAD, a hybrid framework that integrates a fast anomaly detector with a slow anomaly detector (namely a retrieval augmented generation (RAG) enhanced VLM), to address these limitations. Specifically, the fast detector first provides coarse anomaly confidence scores, and only a small subset of ambiguous segments, rather than the entire video, is further analyzed by the slower yet more interpretable VLM for elaborate detection and reasoning. Furthermore, to adapt VLMs to domain-specific VAD scenarios, we construct a knowledge base including normal patterns based on few normal samples and abnormal patterns inferred by VLMs. During inference, relevant patterns are retrieved and used to augment prompts for anomaly reasoning. Finally, we smoothly fuse the anomaly confidence of fast and slow detectors to enhance robustness of anomaly detection. Extensive experiments on four benchmarks demonstrate that SlowFastVAD effectively combines the strengths of both fast and slow detectors, and achieves remarkable detection accuracy and interpretability with significantly reduced computational overhead, making it well-suited for real-world VAD applications with high reliability requirements."
  },
  {
    "title": "Siamese Network with Dual Attention for EEG-Driven Social Learning: Bridging the Human-Robot Gap in Long-Tail Autonomous Driving",
    "url": "http://arxiv.org/abs/2504.10296v1",
    "arxiv_id": "2504.10296v1",
    "authors": [
      "Xiaoshan Zhou",
      "Carol C. Menassa",
      "Vineet R. Kamat"
    ],
    "published": "2025-04-14T15:06:17+00:00",
    "summary": "Robots with wheeled, quadrupedal, or humanoid forms are increasingly integrated into built environments. However, unlike human social learning, they lack a critical pathway for intrinsic cognitive development, namely, learning from human feedback during interaction. To understand human ubiquitous observation, supervision, and shared control in dynamic and uncertain environments, this study presents a brain-computer interface (BCI) framework that enables classification of Electroencephalogram (EEG) signals to detect cognitively demanding and safety-critical events. As a timely and motivating co-robotic engineering application, we simulate a human-in-the-loop scenario to flag risky events in semi-autonomous robotic driving-representative of long-tail cases that pose persistent bottlenecks to the safety performance of smart mobility systems and robotic vehicles. Drawing on recent advances in few-shot learning, we propose a dual-attention Siamese convolutional network paired with Dynamic Time Warping Barycenter Averaging approach to generate robust EEG-encoded signal representations. Inverse source localization reveals activation in Broadman areas 4 and 9, indicating perception-action coupling during task-relevant mental imagery. The model achieves 80% classification accuracy under data-scarce conditions and exhibits a nearly 100% increase in the utility of salient features compared to state-of-the-art methods, as measured through integrated gradient attribution. Beyond performance, this study contributes to our understanding of the cognitive architecture required for BCI agents-particularly the role of attention and memory mechanisms-in categorizing diverse mental states and supporting both inter- and intra-subject adaptation. Overall, this research advances the development of cognitive robotics and socially guided learning for service robots in complex built environments."
  },
  {
    "title": "Gravity-induced emergence of the Fermi scale in quantum quadratic gravity",
    "url": "http://arxiv.org/abs/2504.10293v1",
    "arxiv_id": "2504.10293v1",
    "authors": [
      "Mohammad Mehrafarin"
    ],
    "published": "2025-04-14T15:04:47+00:00",
    "summary": "In the framework of asymptotic safety, we study quantum quadratic gravity in the presence of the Higgs field considered as non-separable from the vacuum. The theory flows to a high energy fixed point where the Higgs field is strongly coupled to gravity, its potential is symmetric, and the quadratic Weyl curvature coupling is large. The latter renders the ghost graviton an unstable high mass resonance which renders unitarity in the spirit of Lee-Week type theories. Furthermore, if the scalar graviton is tachyonic then there will be a low energy fixed point where tachyonic condensation leads to a new stable vacuum. At this fixed point the symmetry breaks and the Fermi scale emerges, and the behavior of the Higgs field is classical (not influenced by gravitational interaction). Gravity at the UV scale is purely quadratic whereas at the Fermi scale it is linear, and in the intermediate region both contributions are relevant. Thus, at the Fermi scale the quadratic curvature fields disappear through the ghost instability and tachyon condensation, giving rise to Einstein gravity and the electroweak phase transition."
  },
  {
    "title": "Deep Reasoning Translation via Reinforcement Learning",
    "url": "http://arxiv.org/abs/2504.10187v1",
    "arxiv_id": "2504.10187v1",
    "authors": [
      "Jiaan Wang",
      "Fandong Meng",
      "Jie Zhou"
    ],
    "published": "2025-04-14T12:40:39+00:00",
    "summary": "Recently, deep reasoning LLMs (e.g., OpenAI o1/o3 and DeepSeek-R1) have shown promising performance in various complex tasks. Free translation is an important and interesting task in the multilingual world, which requires going beyond word-for-word translation and taking cultural differences into account. This task is still under-explored in deep reasoning LLMs. In this paper, we introduce DeepTrans, a deep reasoning translation model that learns free translation via reinforcement learning. Specifically, we carefully build a reward model with pre-defined scoring criteria on both the translation results and the thought process. Given the source sentences, the reward model teaches the deep translation model how to think and free-translate them during reinforcement learning. In this way, training DeepTrans does not need any labeled translations, avoiding the human-intensive annotation or resource-intensive data synthesis. Experimental results show the effectiveness of DeepTrans. Using Qwen2.5-7B as the backbone, DeepTrans improves performance by 16.3% in literature translation, and outperforms strong deep reasoning baselines as well as baselines that are fine-tuned with synthesized data. Moreover, we summarize the failures and interesting findings during our RL exploration. We hope this work could inspire other researchers in free translation."
  },
  {
    "title": "LLM Unlearning Reveals a Stronger-Than-Expected Coreset Effect in Current Benchmarks",
    "url": "http://arxiv.org/abs/2504.10185v1",
    "arxiv_id": "2504.10185v1",
    "authors": [
      "Soumyadeep Pal",
      "Changsheng Wang",
      "James Diffenderfer",
      "Bhavya Kailkhura",
      "Sijia Liu"
    ],
    "published": "2025-04-14T12:38:37+00:00",
    "summary": "Large language model unlearning has become a critical challenge in ensuring safety and controlled model behavior by removing undesired data-model influences from the pretrained model while preserving general utility. Significant recent efforts have been dedicated to developing LLM unlearning benchmarks such as WMDP (Weapons of Mass Destruction Proxy) and MUSE (Machine Unlearning Six-way Evaluation), facilitating standardized unlearning performance assessment and method comparison. Despite their usefulness, we uncover for the first time a novel coreset effect within these benchmarks. Specifically, we find that LLM unlearning achieved with the original (full) forget set can be effectively maintained using a significantly smaller subset (functioning as a \"coreset\"), e.g., as little as 5% of the forget set, even when selected at random. This suggests that LLM unlearning in these benchmarks can be performed surprisingly easily, even in an extremely low-data regime. We demonstrate that this coreset effect remains strong, regardless of the LLM unlearning method used, such as NPO (Negative Preference Optimization) and RMU (Representation Misdirection Unlearning), the popular ones in these benchmarks. The surprisingly strong coreset effect is also robust across various data selection methods, ranging from random selection to more sophisticated heuristic approaches. We explain the coreset effect in LLM unlearning through a keyword-based perspective, showing that keywords extracted from the forget set alone contribute significantly to unlearning effectiveness and indicating that current unlearning is driven by a compact set of high-impact tokens rather than the entire dataset. We further justify the faithfulness of coreset-unlearned models along additional dimensions, such as mode connectivity and robustness to jailbreaking attacks. Codes are available at https://github.com/OPTML-Group/MU-Coreset."
  },
  {
    "title": "LLM Unlearning Reveals a Stronger-Than-Expected Coreset Effect in Current Benchmarks",
    "url": "http://arxiv.org/abs/2504.10185v2",
    "arxiv_id": "2504.10185v2",
    "authors": [
      "Soumyadeep Pal",
      "Changsheng Wang",
      "James Diffenderfer",
      "Bhavya Kailkhura",
      "Sijia Liu"
    ],
    "published": "2025-04-14T12:38:37+00:00",
    "summary": "Large language model unlearning has become a critical challenge in ensuring safety and controlled model behavior by removing undesired data-model influences from the pretrained model while preserving general utility. Significant recent efforts have been dedicated to developing LLM unlearning benchmarks such as WMDP (Weapons of Mass Destruction Proxy) and MUSE (Machine Unlearning Six-way Evaluation), facilitating standardized unlearning performance assessment and method comparison. Despite their usefulness, we uncover for the first time a novel coreset effect within these benchmarks. Specifically, we find that LLM unlearning achieved with the original (full) forget set can be effectively maintained using a significantly smaller subset (functioning as a \"coreset\"), e.g., as little as 5% of the forget set, even when selected at random. This suggests that LLM unlearning in these benchmarks can be performed surprisingly easily, even in an extremely low-data regime. We demonstrate that this coreset effect remains strong, regardless of the LLM unlearning method used, such as NPO (Negative Preference Optimization) and RMU (Representation Misdirection Unlearning), the popular ones in these benchmarks. The surprisingly strong coreset effect is also robust across various data selection methods, ranging from random selection to more sophisticated heuristic approaches. We explain the coreset effect in LLM unlearning through a keyword-based perspective, showing that keywords extracted from the forget set alone contribute significantly to unlearning effectiveness and indicating that current unlearning is driven by a compact set of high-impact tokens rather than the entire dataset. We further justify the faithfulness of coreset-unlearned models along additional dimensions, such as mode connectivity and robustness to jailbreaking attacks. Codes are available at https://github.com/OPTML-Group/MU-Coreset."
  },
  {
    "title": "Challenges in interpretability of additive models",
    "url": "http://arxiv.org/abs/2504.10169v1",
    "arxiv_id": "2504.10169v1",
    "authors": [
      "Xinyu Zhang",
      "Julien Martinelli",
      "ST John"
    ],
    "published": "2025-04-14T12:24:17+00:00",
    "summary": "We review generalized additive models as a type of ``transparent'' model that has recently seen renewed interest in the deep learning community as neural additive models. We highlight multiple types of nonidentifiability in this model class and discuss challenges in interpretability, arguing for restraint when claiming ``interpretability'' or ``suitability for safety-critical applications'' of such models."
  },
  {
    "title": "GeoUni: A Unified Model for Generating Geometry Diagrams, Problems and Problem Solutions",
    "url": "http://arxiv.org/abs/2504.10146v1",
    "arxiv_id": "2504.10146v1",
    "authors": [
      "Jo-Ku Cheng",
      "Zeren Zhang",
      "Ran Chen",
      "Jingyang Deng",
      "Ziran Qin",
      "Jinwen Ma"
    ],
    "published": "2025-04-14T11:56:55+00:00",
    "summary": "We propose GeoUni, the first unified geometry expert model capable of generating problem solutions and diagrams within a single framework in a way that enables the creation of unique and individualized geometry problems. Traditionally, solving geometry problems and generating diagrams have been treated as separate tasks in machine learning, with no models successfully integrating both to support problem creation. However, we believe that mastery in geometry requires frictionless integration of all of these skills, from solving problems to visualizing geometric relationships, and finally, crafting tailored problems. Our extensive experiments demonstrate that GeoUni, with only 1.5B parameters, achieves performance comparable to larger models such as DeepSeek-R1 with 671B parameters in geometric reasoning tasks. GeoUni also excels in generating precise geometric diagrams, surpassing both text-to-image models and unified models, including the GPT-4o image generation. Most importantly, GeoUni is the only model capable of successfully generating textual problems with matching diagrams based on specific knowledge points, thus offering a wider range of capabilities that extend beyond current models."
  },
  {
    "title": "M2S-RoAD: Multi-Modal Semantic Segmentation for Road Damage Using Camera and LiDAR Data",
    "url": "http://arxiv.org/abs/2504.10123v1",
    "arxiv_id": "2504.10123v1",
    "authors": [
      "Tzu-Yun Tseng",
      "Hongyu Lyu",
      "Josephine Li",
      "Julie Stephany Berrio",
      "Mao Shan",
      "Stewart Worrall"
    ],
    "published": "2025-04-14T11:32:01+00:00",
    "summary": "Road damage can create safety and comfort challenges for both human drivers and autonomous vehicles (AVs). This damage is particularly prevalent in rural areas due to less frequent surveying and maintenance of roads. Automated detection of pavement deterioration can be used as an input to AVs and driver assistance systems to improve road safety. Current research in this field has predominantly focused on urban environments driven largely by public datasets, while rural areas have received significantly less attention. This paper introduces M2S-RoAD, a dataset for the semantic segmentation of different classes of road damage. M2S-RoAD was collected in various towns across New South Wales, Australia, and labelled for semantic segmentation to identify nine distinct types of road damage. This dataset will be released upon the acceptance of the paper."
  },
  {
    "title": "CameraBench: Benchmarking Visual Reasoning in MLLMs via Photography",
    "url": "http://arxiv.org/abs/2504.10090v1",
    "arxiv_id": "2504.10090v1",
    "authors": [
      "I-Sheng Fang",
      "Jun-Cheng Chen"
    ],
    "published": "2025-04-14T10:53:44+00:00",
    "summary": "Large language models (LLMs) and multimodal large language models (MLLMs) have significantly advanced artificial intelligence. However, visual reasoning, reasoning involving both visual and textual inputs, remains underexplored. Recent advancements, including the reasoning models like OpenAI o1 and Gemini 2.0 Flash Thinking, which incorporate image inputs, have opened this capability. In this ongoing work, we focus specifically on photography-related tasks because a photo is a visual snapshot of the physical world where the underlying physics (i.e., illumination, blur extent, etc.) interplay with the camera parameters. Successfully reasoning from the visual information of a photo to identify these numerical camera settings requires the MLLMs to have a deeper understanding of the underlying physics for precise visual comprehension, representing a challenging and intelligent capability essential for practical applications like photography assistant agents. We aim to evaluate MLLMs on their ability to distinguish visual differences related to numerical camera settings, extending a methodology previously proposed for vision-language models (VLMs). Our preliminary results demonstrate the importance of visual reasoning in photography-related tasks. Moreover, these results show that no single MLLM consistently dominates across all evaluation tasks, demonstrating ongoing challenges and opportunities in developing MLLMs with better visual reasoning."
  },
  {
    "title": "RealSafe-R1: Safety-Aligned DeepSeek-R1 without Compromising Reasoning Capability",
    "url": "http://arxiv.org/abs/2504.10081v1",
    "arxiv_id": "2504.10081v1",
    "authors": [
      "Yichi Zhang",
      "Zihao Zeng",
      "Dongbai Li",
      "Yao Huang",
      "Zhijie Deng",
      "Yinpeng Dong"
    ],
    "published": "2025-04-14T10:26:37+00:00",
    "summary": "Large Reasoning Models (LRMs), such as OpenAI o1 and DeepSeek-R1, have been rapidly progressing and achieving breakthrough performance on complex reasoning tasks such as mathematics and coding. However, the open-source R1 models have raised safety concerns in wide applications, such as the tendency to comply with malicious queries, which greatly impacts the utility of these powerful models in their applications. In this paper, we introduce RealSafe-R1 as safety-aligned versions of DeepSeek-R1 distilled models. To train these models, we construct a dataset of 15k safety-aware reasoning trajectories generated by DeepSeek-R1, under explicit instructions for expected refusal behavior. Both quantitative experiments and qualitative case studies demonstrate the models' improvements, which are shown in their safety guardrails against both harmful queries and jailbreak attacks. Importantly, unlike prior safety alignment efforts that often compromise reasoning performance, our method preserves the models' reasoning capabilities by maintaining the training data within the original distribution of generation. Model weights of RealSafe-R1 are open-source at https://huggingface.co/RealSafe."
  },
  {
    "title": "Balancing Two Classifiers via A Simplex ETF Structure for Model Calibration",
    "url": "http://arxiv.org/abs/2504.10007v1",
    "arxiv_id": "2504.10007v1",
    "authors": [
      "Jiani Ni",
      "He Zhao",
      "Jintong Gao",
      "Dandan Guo",
      "Hongyuan Zha"
    ],
    "published": "2025-04-14T09:09:01+00:00",
    "summary": "In recent years, deep neural networks (DNNs) have demonstrated state-of-the-art performance across various domains. However, despite their success, they often face calibration issues, particularly in safety-critical applications such as autonomous driving and healthcare, where unreliable predictions can have serious consequences. Recent research has started to improve model calibration from the view of the classifier. However, the exploration of designing the classifier to solve the model calibration problem is insufficient. Let alone most of the existing methods ignore the calibration errors arising from underconfidence. In this work, we propose a novel method by balancing learnable and ETF classifiers to solve the overconfidence or underconfidence problem for model Calibration named BalCAL. By introducing a confidence-tunable module and a dynamic adjustment method, we ensure better alignment between model confidence and its true accuracy. Extensive experimental validation shows that ours significantly improves model calibration performance while maintaining high predictive accuracy, outperforming existing techniques. This provides a novel solution to the calibration challenges commonly encountered in deep learning."
  },
  {
    "title": "Do We Really Need Curated Malicious Data for Safety Alignment in Multi-modal Large Language Models?",
    "url": "http://arxiv.org/abs/2504.10000v1",
    "arxiv_id": "2504.10000v1",
    "authors": [
      "Yanbo Wang",
      "Jiyang Guan",
      "Jian Liang",
      "Ran He"
    ],
    "published": "2025-04-14T09:03:51+00:00",
    "summary": "Multi-modal large language models (MLLMs) have made significant progress, yet their safety alignment remains limited. Typically, current open-source MLLMs rely on the alignment inherited from their language module to avoid harmful generations. However, the lack of safety measures specifically designed for multi-modal inputs creates an alignment gap, leaving MLLMs vulnerable to vision-domain attacks such as typographic manipulation. Current methods utilize a carefully designed safety dataset to enhance model defense capability, while the specific knowledge or patterns acquired from the high-quality dataset remain unclear. Through comparison experiments, we find that the alignment gap primarily arises from data distribution biases, while image content, response quality, or the contrastive behavior of the dataset makes little contribution to boosting multi-modal safety. To further investigate this and identify the key factors in improving MLLM safety, we propose finetuning MLLMs on a small set of benign instruct-following data with responses replaced by simple, clear rejection sentences. Experiments show that, without the need for labor-intensive collection of high-quality malicious data, model safety can still be significantly improved, as long as a specific fraction of rejection data exists in the finetuning set, indicating the security alignment is not lost but rather obscured during multi-modal pretraining or instruction finetuning. Simply correcting the underlying data bias could narrow the safety gap in the vision domain."
  },
  {
    "title": "Assessing Judging Bias in Large Reasoning Models: An Empirical Study",
    "url": "http://arxiv.org/abs/2504.09946v1",
    "arxiv_id": "2504.09946v1",
    "authors": [
      "Qian Wang",
      "Zhanzhi Lou",
      "Zhenheng Tang",
      "Nuo Chen",
      "Xuandong Zhao",
      "Wenxuan Zhang",
      "Dawn Song",
      "Bingsheng He"
    ],
    "published": "2025-04-14T07:14:27+00:00",
    "summary": "Large Reasoning Models (LRMs) like DeepSeek-R1 and OpenAI-o1 have demonstrated remarkable reasoning capabilities, raising important questions about their biases in LLM-as-a-judge settings. We present a comprehensive benchmark comparing judging biases between LLMs and LRMs across both subjective preference-alignment datasets and objective fact-based datasets. Through investigation of bandwagon, authority, position, and distraction biases, we uncover four key findings: (1) despite their advanced reasoning capabilities, LRMs remain susceptible to the above biases; (2) LRMs demonstrate better robustness than LLMs specifically on fact-related datasets; (3) LRMs exhibit notable position bias, preferring options in later positions; and (4) we identify a novel \"superficial reflection bias\" where phrases mimicking reasoning (e.g., \"wait, let me think...\") significantly influence model judgments. To address these biases, we design and evaluate three mitigation strategies: specialized system prompts that reduce judging biases by up to 19\\% in preference alignment datasets and 14\\% in fact-related datasets, in-context learning that provides up to 27\\% improvement on preference tasks but shows inconsistent results on factual tasks, and a self-reflection mechanism that reduces biases by up to 10\\% in preference datasets and 16\\% in fact-related datasets, with self-reflection proving particularly effective for LRMs. Our work provides crucial insights for developing more reliable LLM-as-a-Judge frameworks, especially as LRMs become increasingly deployed as automated judges."
  },
  {
    "title": "Tight Semidefinite Relaxations for Verifying Robustness of Neural Networks",
    "url": "http://arxiv.org/abs/2504.09934v1",
    "arxiv_id": "2504.09934v1",
    "authors": [
      "Godai Azuma",
      "Sunyoung Kim",
      "Makoto Yamashita"
    ],
    "published": "2025-04-14T06:54:00+00:00",
    "summary": "For verifying the safety of neural networks (NNs), Fazlyab et al. (2019) introduced a semidefinite programming (SDP) approach called DeepSDP. This formulation can be viewed as the dual of the SDP relaxation for a problem formulated as a quadratically constrained quadratic program (QCQP). While SDP relaxations of QCQPs generally provide approximate solutions with some gaps, this work focuses on tight SDP relaxations that provide exact solutions to the QCQP for single-layer NNs. Specifically, we analyze tightness conditions in three cases: (i) NNs with a single neuron, (ii) single-layer NNs with an ellipsoidal input set, and (iii) single-layer NNs with a rectangular input set. For NNs with a single neuron, we propose a condition that ensures the SDP admits a rank-1 solution to DeepSDP by transforming the QCQP into an equivalent two-stage problem leads to a solution collinear with a predetermined vector. For single-layer NNs with an ellipsoidal input set, the collinearity of solutions is proved via the Karush-Kuhn-Tucker condition in the two-stage problem. In case of single-layer NNs with a rectangular input set, we demonstrate that the tightness of DeepSDP can be reduced to the single-neuron NNs, case (i), if the weight matrix is a diagonal matrix."
  },
  {
    "title": "Learning from Reference Answers: Versatile Language Model Alignment without Binary Human Preference Data",
    "url": "http://arxiv.org/abs/2504.09895v1",
    "arxiv_id": "2504.09895v1",
    "authors": [
      "Shuai Zhao",
      "Linchao Zhu",
      "Yi Yang"
    ],
    "published": "2025-04-14T05:43:21+00:00",
    "summary": "Large language models~(LLMs) are expected to be helpful, harmless, and honest. In various alignment scenarios, such as general human preference, safety, and confidence alignment, binary preference data collection and reward modeling are resource-intensive but necessary for human preference transferring. In this work, we explore using the similarity between sampled generations and high-quality reference answers as an alternative reward function for LLM alignment. Using similarity as a reward circumvents training reward models, and collecting a single reference answer potentially costs less time than constructing binary preference pairs when multiple candidates are available. Specifically, we develop \\textit{RefAlign}, a versatile REINFORCE-style alignment algorithm, which is free of reference and reward models. Instead, RefAlign utilizes BERTScore between sampled generations and high-quality reference answers as the surrogate reward. Beyond general human preference optimization, RefAlign can be readily extended to diverse scenarios, such as safety and confidence alignment, by incorporating the similarity reward with task-related objectives. In various scenarios, {RefAlign} demonstrates comparable performance to previous alignment methods while offering high efficiency."
  },
  {
    "title": "Reasoning Models Can Be Effective Without Thinking",
    "url": "http://arxiv.org/abs/2504.09858v1",
    "arxiv_id": "2504.09858v1",
    "authors": [
      "Wenjie Ma",
      "Jingxuan He",
      "Charlie Snell",
      "Tyler Griggs",
      "Sewon Min",
      "Matei Zaharia"
    ],
    "published": "2025-04-14T04:08:16+00:00",
    "summary": "Recent LLMs have significantly improved reasoning capabilities, primarily by including an explicit, lengthy Thinking process as part of generation. In this paper, we question whether this explicit thinking is necessary. Using the state-of-the-art DeepSeek-R1-Distill-Qwen, we find that bypassing the thinking process via simple prompting, denoted as NoThinking, can be surprisingly effective. When controlling for the number of tokens, NoThinking outperforms Thinking across a diverse set of seven challenging reasoning datasets--including mathematical problem solving, formal theorem proving, and coding--especially in low-budget settings, e.g., 51.3 vs. 28.9 on ACM 23 with 700 tokens. Notably, the performance of NoThinking becomes more competitive with pass@k as k increases. Building on this observation, we demonstrate that a parallel scaling approach that uses NoThinking to generate N outputs independently and aggregates them is highly effective. For aggregation, we use task-specific verifiers when available, or we apply simple best-of-N strategies such as confidence-based selection. Our method outperforms a range of baselines with similar latency using Thinking, and is comparable to Thinking with significantly longer latency (up to 9x). Together, our research encourages a reconsideration of the necessity of lengthy thinking processes, while also establishing a competitive reference for achieving strong reasoning performance in low-budget settings or at low latency using parallel scaling."
  },
  {
    "title": "Training Small Reasoning LLMs with Cognitive Preference Alignment",
    "url": "http://arxiv.org/abs/2504.09802v1",
    "arxiv_id": "2504.09802v1",
    "authors": [
      "Wenrui Cai",
      "Chengyu Wang",
      "Junbing Yan",
      "Jun Huang",
      "Xiangzhong Fang"
    ],
    "published": "2025-04-14T02:03:54+00:00",
    "summary": "The reasoning capabilities of large language models (LLMs), such as OpenAI's o1 and DeepSeek-R1, have seen substantial advancements through deep thinking. However, these enhancements come with significant resource demands, underscoring the need to explore strategies to train effective reasoning LLMs with far fewer parameters. A critical challenge is that smaller models have different capacities and cognitive trajectories than their larger counterparts. Hence, direct distillation of chain-of-thought (CoT) results from large LLMs to smaller ones can be sometimes ineffective and requires a huge amount of annotated data. In this paper, we introduce a novel framework called Critique-Rethink-Verify (CRV), designed for training smaller yet powerful reasoning LLMs. Our CRV framework consists of multiple LLM agents, each specializing in unique abilities: (i) critiquing the CoTs according to the cognitive capabilities of smaller models, (ii) rethinking and refining these CoTs based on the critiques, and (iii) verifying the correctness of the refined results. We further propose the cognitive preference optimization (CogPO) algorithm to enhance the reasoning abilities of smaller models by aligning thoughts of these models with their cognitive capacities. Comprehensive evaluations on challenging reasoning benchmarks demonstrate the efficacy of CRV and CogPO, which outperforms other training methods by a large margin."
  },
  {
    "title": "Reasoning without Regret",
    "url": "http://arxiv.org/abs/2504.09777v1",
    "arxiv_id": "2504.09777v1",
    "authors": [
      "Tarun Chitra"
    ],
    "published": "2025-04-14T00:34:20+00:00",
    "summary": "Chain-of-thought reasoning enables large language models to solve multi-step tasks by framing problem solving as sequential decision problems. Outcome-based rewards, which provide feedback only on final answers, show impressive success, but face challenges with credit assignment and slow convergence. In contrast, procedure-based rewards offer efficient step-level feedback, but typically require costly human supervision. We introduce \\emph{Backwards Adaptive Reward Shaping} (BARS), a no-regret framework that converts sparse outcomes-based rewards into effective procedure-based signals. BARS uses sparse rewards generated from terminal-state priors and cover trees to scale rewards while preventing exploitation. With Bellman contraction and $(\\Delta, \\epsilon)$-gap rewards, our backward Euler solver achieves $\\epsilon$-accuracy in $O\\left((R_{\\max}/\\Delta)\\log(1/\\epsilon)\\right)$ iterations with $O(\\log T)$ dynamic regret over $T$ rounds. Our analysis, based on generic chaining, continuous scaling limits, and non-linear Feynman-Kac bounds, connects recent outcome-based methods' empirical successes with the benefits of intermediate supervision. Combined, this provides the first rigorous no-regret algorithm for outcome reward shaping, providing a theoretical foundation for the empirical success of DeepSeek's R1."
  },
  {
    "title": "Two Heads are Better Than One: Test-time Scaling of Multi-agent Collaborative Reasoning",
    "url": "http://arxiv.org/abs/2504.09772v1",
    "arxiv_id": "2504.09772v1",
    "authors": [
      "Can Jin",
      "Hongwu Peng",
      "Qixin Zhang",
      "Yujin Tang",
      "Dimitris N. Metaxas",
      "Tong Che"
    ],
    "published": "2025-04-14T00:27:45+00:00",
    "summary": "Multi-agent systems (MAS) built on large language models (LLMs) offer a promising path toward solving complex, real-world tasks that single-agent systems often struggle to manage. While recent advancements in test-time scaling (TTS) have significantly improved single-agent performance on challenging reasoning tasks, how to effectively scale collaboration and reasoning in MAS remains an open question. In this work, we introduce an adaptive multi-agent framework designed to enhance collaborative reasoning through both model-level training and system-level coordination. We construct M500, a high-quality dataset containing 500 multi-agent collaborative reasoning traces, and fine-tune Qwen2.5-32B-Instruct on this dataset to produce M1-32B, a model optimized for multi-agent collaboration. To further enable adaptive reasoning, we propose a novel CEO agent that dynamically manages the discussion process, guiding agent collaboration and adjusting reasoning depth for more effective problem-solving. Evaluated in an open-source MAS across a range of tasks-including general understanding, mathematical reasoning, and coding-our system significantly outperforms strong baselines. For instance, M1-32B achieves 12% improvement on GPQA-Diamond, 41% on AIME2024, and 10% on MBPP-Sanitized, matching the performance of state-of-the-art models like DeepSeek-R1 on some tasks. These results highlight the importance of both learned collaboration and adaptive coordination in scaling multi-agent reasoning. Code is available at https://github.com/jincan333/MAS-TTS"
  },
  {
    "title": "(How) Do reasoning models reason?",
    "url": "http://arxiv.org/abs/2504.09762v1",
    "arxiv_id": "2504.09762v1",
    "authors": [
      "Subbarao Kambhampati",
      "Kaya Stechly",
      "Karthik Valmeekam"
    ],
    "published": "2025-04-14T00:03:34+00:00",
    "summary": "We will provide a broad unifying perspective on the recent breed of Large Reasoning Models (LRMs) such as OpenAI o1 and DeepSeek R1, including their promise, sources of power, misconceptions and limitations."
  },
  {
    "title": "Hybrid Lyapunov and Barrier Function-Based Control with Stabilization Guarantees",
    "url": "http://arxiv.org/abs/2504.09760v1",
    "arxiv_id": "2504.09760v1",
    "authors": [
      "Hugo Matias",
      "Daniel Silvestre"
    ],
    "published": "2025-04-13T23:55:09+00:00",
    "summary": "Control Lyapunov Functions (CLFs) and Control Barrier Functions (CBFs) can be combined, typically by means of Quadratic Programs (QPs), to design controllers that achieve performance and safety objectives. However, a significant limitation of this framework is the introduction of asymptotically stable equilibrium points besides the minimizer of the CLF, leading to deadlock situations even for simple systems and bounded convex unsafe sets. To address this problem, we propose a hybrid CLF-CBF control framework with global asymptotic stabilization and safety guarantees, offering a more flexible and systematic design methodology compared to current alternatives available in the literature. We further extend this framework to higher-order systems via a recursive procedure based on a joint CLF-CBF backstepping approach. The proposed solution is assessed through several simulation examples."
  },
  {
    "title": "Epsilon-Neighborhood Decision-Boundary Governed Estimation (EDGE) of 2D Black Box Classifier Functions",
    "url": "http://arxiv.org/abs/2504.09733v1",
    "arxiv_id": "2504.09733v1",
    "authors": [
      "Mithun Goutham",
      "Riccardo DalferroNucci",
      "Stephanie Stockar",
      "Meghna Menon",
      "Sneha Nayak",
      "Harshad Zade",
      "Chetan Patel",
      "Mario Santillo"
    ],
    "published": "2025-04-13T21:40:46+00:00",
    "summary": "Accurately estimating decision boundaries in black box systems is critical when ensuring safety, quality, and feasibility in real-world applications. However, existing methods iteratively refine boundary estimates by sampling in regions of uncertainty, without providing guarantees on the closeness to the decision boundary and also result in unnecessary exploration that is especially disadvantageous when evaluations are costly. This paper presents the Epsilon-Neighborhood Decision-Boundary Governed Estimation (EDGE), a sample efficient and function-agnostic algorithm that leverages the intermediate value theorem to estimate the location of the decision boundary of a black box binary classifier within a user-specified epsilon-neighborhood. Evaluations are conducted on three nonlinear test functions and a case study of an electric grid stability problem with uncertain renewable power injection. The EDGE algorithm demonstrates superior sample efficiency and better boundary approximation than adaptive sampling techniques and grid-based searches."
  },
  {
    "title": "The Structural Safety Generalization Problem",
    "url": "http://arxiv.org/abs/2504.09712v1",
    "arxiv_id": "2504.09712v1",
    "authors": [
      "Julius Broomfield",
      "Tom Gibbs",
      "Ethan Kosak-Hine",
      "George Ingebretsen",
      "Tia Nasir",
      "Jason Zhang",
      "Reihaneh Iranmanesh",
      "Sara Pieri",
      "Reihaneh Rabbany",
      "Kellin Pelrine"
    ],
    "published": "2025-04-13T20:21:08+00:00",
    "summary": "LLM jailbreaks are a widespread safety challenge. Given this problem has not yet been tractable, we suggest targeting a key failure mechanism: the failure of safety to generalize across semantically equivalent inputs. We further focus the target by requiring desirable tractability properties of attacks to study: explainability, transferability between models, and transferability between goals. We perform red-teaming within this framework by uncovering new vulnerabilities to multi-turn, multi-image, and translation-based attacks. These attacks are semantically equivalent by our design to their single-turn, single-image, or untranslated counterparts, enabling systematic comparisons; we show that the different structures yield different safety outcomes. We then demonstrate the potential for this framework to enable new defenses by proposing a Structure Rewriting Guardrail, which converts an input to a structure more conducive to safety assessment. This guardrail significantly improves refusal of harmful inputs, without over-refusing benign ones. Thus, by framing this intermediate challenge - more tractable than universal defenses but essential for long-term safety - we highlight a critical milestone for AI safety research."
  },
  {
    "title": "EmoAgent: Assessing and Safeguarding Human-AI Interaction for Mental Health Safety",
    "url": "http://arxiv.org/abs/2504.09689v1",
    "arxiv_id": "2504.09689v1",
    "authors": [
      "Jiahao Qiu",
      "Yinghui He",
      "Xinzhe Juan",
      "Yiming Wang",
      "Yuhan Liu",
      "Zixin Yao",
      "Yue Wu",
      "Xun Jiang",
      "Ling Yang",
      "Mengdi Wang"
    ],
    "published": "2025-04-13T18:47:22+00:00",
    "summary": "The rise of LLM-driven AI characters raises safety concerns, particularly for vulnerable human users with psychological disorders. To address these risks, we propose EmoAgent, a multi-agent AI framework designed to evaluate and mitigate mental health hazards in human-AI interactions. EmoAgent comprises two components: EmoEval simulates virtual users, including those portraying mentally vulnerable individuals, to assess mental health changes before and after interactions with AI characters. It uses clinically proven psychological and psychiatric assessment tools (PHQ-9, PDI, PANSS) to evaluate mental risks induced by LLM. EmoGuard serves as an intermediary, monitoring users' mental status, predicting potential harm, and providing corrective feedback to mitigate risks. Experiments conducted in popular character-based chatbots show that emotionally engaging dialogues can lead to psychological deterioration in vulnerable users, with mental state deterioration in more than 34.4% of the simulations. EmoGuard significantly reduces these deterioration rates, underscoring its role in ensuring safer AI-human interactions. Our code is available at: https://github.com/1akaman/EmoAgent"
  },
  {
    "title": "Leveraging Reasoning Model Answers to Enhance Non-Reasoning Model Capability",
    "url": "http://arxiv.org/abs/2504.09639v1",
    "arxiv_id": "2504.09639v1",
    "authors": [
      "Haotian Wang",
      "Han Zhao",
      "Shuaiting Chen",
      "Xiaoyu Tian",
      "Sitong Zhao",
      "Yunjie Ji",
      "Yiping Peng",
      "Xiangang Li"
    ],
    "published": "2025-04-13T16:26:56+00:00",
    "summary": "Recent advancements in large language models (LLMs), such as DeepSeek-R1 and OpenAI-o1, have demonstrated the significant effectiveness of test-time scaling, achieving substantial performance gains across various benchmarks. These advanced models utilize deliberate \"thinking\" steps to systematically enhance answer quality. In this paper, we propose leveraging these high-quality outputs generated by reasoning-intensive models to improve less computationally demanding, non-reasoning models. We explore and compare methodologies for utilizing the answers produced by reasoning models to train and improve non-reasoning models. Through straightforward Supervised Fine-Tuning (SFT) experiments on established benchmarks, we demonstrate consistent improvements across various benchmarks, underscoring the potential of this approach for advancing the ability of models to answer questions directly."
  },
  {
    "title": "Mitigating Many-Shot Jailbreaking",
    "url": "http://arxiv.org/abs/2504.09604v1",
    "arxiv_id": "2504.09604v1",
    "authors": [
      "Christopher M. Ackerman",
      "Nina Panickssery"
    ],
    "published": "2025-04-13T14:42:03+00:00",
    "summary": "Many-shot jailbreaking (MSJ) is an adversarial technique that exploits the long context windows of modern LLMs to circumvent model safety training by including in the prompt many examples of a ``fake'' assistant responding inappropriately before the final request. With enough examples, the model's in-context learning abilities override its safety training, and it responds as if it were the ``fake'' assistant. In this work, we probe the effectiveness of different fine tuning and input sanitization approaches on mitigating MSJ attacks, alone and in combination. We find incremental mitigation effectiveness for each, and we show that the combined techniques significantly reduce the effectiveness of MSJ attacks, while retaining model performance in benign in-context learning and conversational tasks. We suggest that our approach could meaningfully ameliorate this vulnerability if incorporated into model safety post-training."
  },
  {
    "title": "Fine-tuning an Large Language Model for Automating Computational Fluid Dynamics Simulations",
    "url": "http://arxiv.org/abs/2504.09602v1",
    "arxiv_id": "2504.09602v1",
    "authors": [
      "Zhehao Dong",
      "Zhen Lu",
      "Yue Yang"
    ],
    "published": "2025-04-13T14:35:30+00:00",
    "summary": "Configuring computational fluid dynamics (CFD) simulations typically demands extensive domain expertise, limiting broader access. Although large language models (LLMs) have advanced scientific computing, their use in automating CFD workflows is underdeveloped. We introduce a novel approach centered on domain-specific LLM adaptation. By fine-tuning Qwen2.5-7B-Instruct on NL2FOAM, our custom dataset of 28716 natural language-to-OpenFOAM configuration pairs with chain-of-thought (CoT) annotations, we enable direct translation from natural language descriptions to executable CFD setups. A multi-agent framework orchestrates the process, autonomously verifying inputs, generating configurations, running simulations, and correcting errors. Evaluation on a benchmark of 21 diverse flow cases demonstrates state-of-the-art performance, achieving 88.7% solution accuracy and 82.6% first-attempt success rate. This significantly outperforms larger general-purpose models like Qwen2.5-72B-Instruct, DeepSeek-R1, and Llama3.3-70B-Instruct, while also requiring fewer correction iterations and maintaining high computational efficiency. The results highlight the critical role of domain-specific adaptation in deploying LLM assistants for complex engineering workflows."
  },
  {
    "title": "Analysis of Radiation Level and Estimation of Protection Distance of \u03b3 Mobile Flaw Detection Source",
    "url": "http://arxiv.org/abs/2504.09556v1",
    "arxiv_id": "2504.09556v1",
    "authors": [
      "Zhihui Liu",
      "Xiang Hu",
      "Zhengyang Zhang"
    ],
    "published": "2025-04-13T13:05:47+00:00",
    "summary": "Objective To analyze the radiation dose associated with gamma-ray mobile flaw detection, estimate the extent of the supervision and control areas, and assess the associated radiation risks. Methods A combination of theoretical calculations and actual measurements was used to compare and analyze the ambient equivalent dose rates of 192 Ir and 75 Se at their nominal source strengths. Measurements were conducted at distances of 1 m, 2 m, and 5 m from the radiation source. The extents of the control and supervision areas were estimated under three working scenarios: 1 without considering air attenuation, 2 considering air attenuation, and 3 after shielding by the flaw detection workpiece, using source activities of 3.7 * 10^10 Bq and 3.7 * 10^12Bq. Results Actual measurement of radiation dose of 192 Ir and 75 Se were measured under three different nominal activities. Theoretical calculation of radiation dose estimates at various distances were obtained for both nuclides, and the results showed that the theoretical values were basically consistent with the measured values. Conclusion The estimated scope of the supervision and control areas provided in this study can serve as a reference for flaw detection companies. Technicians can use these estimates to calculate appropriate distances for safety zones based on different nuclide activities. This enables flaw detection personnel to reduce the measurement scope on-site and to quickly and accurately define area boundaries."
  },
  {
    "title": "Quality Control and Structural Reliability -- A Unified Framework for Integrating Conformity Assessment and Partial Safety Factors",
    "url": "http://arxiv.org/abs/2504.09508v1",
    "arxiv_id": "2504.09508v1",
    "authors": [
      "Tammam Bakeer",
      "Wolfram Jaeger"
    ],
    "published": "2025-04-13T10:14:34+00:00",
    "summary": "Ensuring structural reliability remains a core concern in civil engineering, yet the quantitative effects of quality control measures on material variability and safety margins are not fully understood, especially for materials other than reinforced concrete. This study addresses this gap by presenting a probabilistic framework that integrates Bayesian updating, acceptance sampling, and operating characteristic (OC) curves to model conformity assessment as a probabilistic filter. In doing so, it refines prior distributions of key material and execution parameters based on quality control outcomes, linking reductions in the coefficient of variation directly to adjustments in partial safety factors. Applying the framework to a masonry wall example demonstrates how systematic quality control efforts, particularly those targeting parameters with higher importance such as masonry unit strength and execution quality-produce substantial gains in structural reliability. The analysis shows that combined quality control measures can lower the partial safety factor from a baseline of 1.5 to about 1.38, corresponding to an improvement factor of roughly 1.09 and material savings of approximately 8%. Conversely, controlling parameters with negligible influence, such as mortar properties, provides limited benefit. These findings encourage focusing quality control resources on the most influential parameters and integrating results into semi-probabilistic design methods. By offering a transparent, standards-compatible approach, the framework supports the refinement of design guidelines, promotes more efficient resource allocation, and enhances overall structural safety in the built environment."
  },
  {
    "title": "AdaSteer: Your Aligned LLM is Inherently an Adaptive Jailbreak Defender",
    "url": "http://arxiv.org/abs/2504.09466v1",
    "arxiv_id": "2504.09466v1",
    "authors": [
      "Weixiang Zhao",
      "Jiahe Guo",
      "Yulin Hu",
      "Yang Deng",
      "An Zhang",
      "Xingyu Sui",
      "Xinyang Han",
      "Yanyan Zhao",
      "Bing Qin",
      "Tat-Seng Chua",
      "Ting Liu"
    ],
    "published": "2025-04-13T07:39:17+00:00",
    "summary": "Despite extensive efforts in safety alignment, large language models (LLMs) remain vulnerable to jailbreak attacks. Activation steering offers a training-free defense method but relies on fixed steering coefficients, resulting in suboptimal protection and increased false rejections of benign inputs. To address this, we propose AdaSteer, an adaptive activation steering method that dynamically adjusts model behavior based on input characteristics. We identify two key properties: Rejection Law (R-Law), which shows that stronger steering is needed for jailbreak inputs opposing the rejection direction, and Harmfulness Law (H-Law), which differentiates adversarial and benign inputs. AdaSteer steers input representations along both the Rejection Direction (RD) and Harmfulness Direction (HD), with adaptive coefficients learned via logistic regression, ensuring robust jailbreak defense while preserving benign input handling. Experiments on LLaMA-3.1, Gemma-2, and Qwen2.5 show that AdaSteer outperforms baseline methods across multiple jailbreak attacks with minimal impact on utility. Our results highlight the potential of interpretable model internals for real-time, flexible safety enforcement in LLMs."
  },
  {
    "title": "ADDT -- A Digital Twin Framework for Proactive Safety Validation in Autonomous Driving Systems",
    "url": "http://arxiv.org/abs/2504.09461v1",
    "arxiv_id": "2504.09461v1",
    "authors": [
      "Bo Yu",
      "Chaoran Yuan",
      "Zishen Wan",
      "Jie Tang",
      "Fadi Kurdahi",
      "Shaoshan Liu"
    ],
    "published": "2025-04-13T07:17:17+00:00",
    "summary": "Autonomous driving systems continue to face safety-critical failures, often triggered by rare and unpredictable corner cases that evade conventional testing. We present the Autonomous Driving Digital Twin (ADDT) framework, a high-fidelity simulation platform designed to proactively identify hidden faults, evaluate real-time performance, and validate safety before deployment. ADDT combines realistic digital models of driving environments, vehicle dynamics, sensor behavior, and fault conditions to enable scalable, scenario-rich stress-testing under diverse and adverse conditions. It supports adaptive exploration of edge cases using reinforcement-driven techniques, uncovering failure modes that physical road testing often misses. By shifting from reactive debugging to proactive simulation-driven validation, ADDT enables a more rigorous and transparent approach to autonomous vehicle safety engineering. To accelerate adoption and facilitate industry-wide safety improvements, the entire ADDT framework has been released as open-source software, providing developers with an accessible and extensible tool for comprehensive safety testing at scale."
  },
  {
    "title": "SaRO: Enhancing LLM Safety through Reasoning-based Alignment",
    "url": "http://arxiv.org/abs/2504.09420v1",
    "arxiv_id": "2504.09420v1",
    "authors": [
      "Yutao Mou",
      "Yuxiao Luo",
      "Shikun Zhang",
      "Wei Ye"
    ],
    "published": "2025-04-13T03:36:06+00:00",
    "summary": "Current safety alignment techniques for large language models (LLMs) face two key challenges: (1) under-generalization, which leaves models vulnerable to novel jailbreak attacks, and (2) over-alignment, which leads to the excessive refusal of benign instructions. Our preliminary investigation reveals semantic overlap between jailbreak/harmful queries and normal prompts in embedding space, suggesting that more effective safety alignment requires a deeper semantic understanding. This motivates us to incorporate safety-policy-driven reasoning into the alignment process. To this end, we propose the Safety-oriented Reasoning Optimization Framework (SaRO), which consists of two stages: (1) Reasoning-style Warmup (RW) that enables LLMs to internalize long-chain reasoning through supervised fine-tuning, and (2) Safety-oriented Reasoning Process Optimization (SRPO) that promotes safety reflection via direct preference optimization (DPO). Extensive experiments demonstrate the superiority of SaRO over traditional alignment methods."
  },
  {
    "title": "Graph-Based Prediction Models for Data Debiasing",
    "url": "http://arxiv.org/abs/2504.09348v1",
    "arxiv_id": "2504.09348v1",
    "authors": [
      "Dongze Wu",
      "Hanyang Jiang",
      "Yao Xie"
    ],
    "published": "2025-04-12T21:34:49+00:00",
    "summary": "Bias in data collection, arising from both under-reporting and over-reporting, poses significant challenges in critical applications such as healthcare and public safety. In this work, we introduce Graph-based Over- and Under-reporting Debiasing (GROUD), a novel graph-based optimization framework that debiases reported data by jointly estimating the true incident counts and the associated reporting bias probabilities. By modeling the bias as a smooth signal over a graph constructed from geophysical or feature-based similarities, our convex formulation not only ensures a unique solution but also comes with theoretical recovery guarantees under certain assumptions. We validate GROUD on both challenging simulated experiments and real-world datasets -- including Atlanta emergency calls and COVID-19 vaccine adverse event reports -- demonstrating its robustness and superior performance in accurately recovering debiased counts. This approach paves the way for more reliable downstream decision-making in systems affected by reporting irregularities."
  },
  {
    "title": "Strain-induced polarization rotation in freestanding ferroelectric oxide membranes",
    "url": "http://arxiv.org/abs/2504.09244v1",
    "arxiv_id": "2504.09244v1",
    "authors": [
      "Alban Degezelle",
      "Razvan Burcea",
      "Pascale Gemeiner",
      "Maxime Vallet",
      "Brahim Dkhil",
      "St\u00e9phane Fusil",
      "Vincent Garcia",
      "Sylvia Matzen",
      "Philippe Lecoeur",
      "Thomas Maroutian"
    ],
    "published": "2025-04-12T14:51:19+00:00",
    "summary": "Freestanding ferroelectric membranes have emerged as a versatile tool for strain engineering, enabling the exploration of ferroelectric properties beyond traditional epitaxy. The resulting ferroelectric domain patterns stem from the balance at the local scale of several effects playing a key role, i.e. piezoelectricity linked to strain, and flexoelectricity arising from strain gradients. To weight their respective contributions for a given membrane geometry, the strain profile has to be mapped with respect to the ferroelectric polarization landscape, a necessary step to allow for a controlled tailoring of the latter. In this study, we examine the effect of bending strain on a Pb(Zr,Ti)O3 membrane in a fold-like structure, observing a polarization rotation from out-of-plane to in-plane at the fold apex. Combining piezoresponse force microscopy, Raman spectroscopy, and scanning transmission electron microscopy, we map the ferroelectric polarization direction relative to the height profile of the membrane, and discuss the contributions of strain and strain gradients for this archetypal fold geometry. Our findings offer new insights into strain-engineered polarization configurations, and emphasize strain effects at the nanoscale to tune the functional properties in freestanding membranes."
  },
  {
    "title": "Graph Learning-Driven Multi-Vessel Association: Fusing Multimodal Data for Maritime Intelligence",
    "url": "http://arxiv.org/abs/2504.09197v1",
    "arxiv_id": "2504.09197v1",
    "authors": [
      "Yuxu Lu",
      "Kaisen Yang",
      "Dong Yang",
      "Haifeng Ding",
      "Jinxian Weng",
      "Ryan Wen Liu"
    ],
    "published": "2025-04-12T12:45:55+00:00",
    "summary": "Ensuring maritime safety and optimizing traffic management in increasingly crowded and complex waterways require effective waterway monitoring. However, current methods struggle with challenges arising from multimodal data, such as dimensional disparities, mismatched target counts, vessel scale variations, occlusions, and asynchronous data streams from systems like the automatic identification system (AIS) and closed-circuit television (CCTV). Traditional multi-target association methods often struggle with these complexities, particularly in densely trafficked waterways. To overcome these issues, we propose a graph learning-driven multi-vessel association (GMvA) method tailored for maritime multimodal data fusion. By integrating AIS and CCTV data, GMvA leverages time series learning and graph neural networks to capture the spatiotemporal features of vessel trajectories effectively. To enhance feature representation, the proposed method incorporates temporal graph attention and spatiotemporal attention, effectively capturing both local and global vessel interactions. Furthermore, a multi-layer perceptron-based uncertainty fusion module computes robust similarity scores, and the Hungarian algorithm is adopted to ensure globally consistent and accurate target matching. Extensive experiments on real-world maritime datasets confirm that GMvA delivers superior accuracy and robustness in multi-target association, outperforming existing methods even in challenging scenarios with high vessel density and incomplete or unevenly distributed AIS and CCTV data."
  },
  {
    "title": "Feature-Aware Malicious Output Detection and Mitigation",
    "url": "http://arxiv.org/abs/2504.09191v1",
    "arxiv_id": "2504.09191v1",
    "authors": [
      "Weilong Dong",
      "Peiguang Li",
      "Yu Tian",
      "Xinyi Zeng",
      "Fengdi Li",
      "Sirui Wang"
    ],
    "published": "2025-04-12T12:12:51+00:00",
    "summary": "The rapid advancement of large language models (LLMs) has brought significant benefits to various domains while introducing substantial risks. Despite being fine-tuned through reinforcement learning, LLMs lack the capability to discern malicious content, limiting their defense against jailbreak. To address these safety concerns, we propose a feature-aware method for harmful response rejection (FMM), which detects the presence of malicious features within the model's feature space and adaptively adjusts the model's rejection mechanism. By employing a simple discriminator, we detect potential malicious traits during the decoding phase. Upon detecting features indicative of toxic tokens, FMM regenerates the current token. By employing activation patching, an additional rejection vector is incorporated during the subsequent token generation, steering the model towards a refusal response. Experimental results demonstrate the effectiveness of our approach across multiple language models and diverse attack techniques, while crucially maintaining the models' standard generation capabilities."
  },
  {
    "title": "Compliant Explicit Reference Governor for Contact Friendly Robotic Manipulators",
    "url": "http://arxiv.org/abs/2504.09188v1",
    "arxiv_id": "2504.09188v1",
    "authors": [
      "Yaashia Gautam",
      "Nataliya Nechyporenko",
      "Chi-Hui Lin",
      "Alessandro Roncone",
      "Marco M. Nicotra"
    ],
    "published": "2025-04-12T12:01:46+00:00",
    "summary": "This paper introduces the Compliant Explicit Reference Governor (C-ERG), an extension of the Explicit Reference Governor that allows the robot to operate safely while in contact with the environment.   The C-ERG is an intermediate layer that can be placed between a high-level planner and a low-level controller: its role is to enforce operational constraints and to enable the smooth transition between free-motion and contact operations. The C-ERG ensures safety by limiting the total energy available to the robotic arm at the time of contact. In the absence of contact, however, the C-ERG does not penalize the system performance.   Numerical examples showcase the behavior of the C-ERG for increasingly complex systems."
  },
  {
    "title": "Dose-finding design based on level set estimation in phase I cancer clinical trials",
    "url": "http://arxiv.org/abs/2504.09157v1",
    "arxiv_id": "2504.09157v1",
    "authors": [
      "Keiichiro Seno",
      "Kota Matsui",
      "Shogo Iwazaki",
      "Yu Inatsu",
      "Shion Takeno",
      "Shigeyuki Matsui"
    ],
    "published": "2025-04-12T09:44:20+00:00",
    "summary": "The primary objective of phase I cancer clinical trials is to evaluate the safety of a new experimental treatment and to find the maximum tolerated dose (MTD). We show that the MTD estimation problem can be regarded as a level set estimation (LSE) problem whose objective is to determine the regions where an unknown function value is above or below a given threshold. Then, we propose a novel dose-finding design in the framework of LSE. The proposed design determines the next dose on the basis of an acquisition function incorporating uncertainty in the posterior distribution of the dose-toxicity curve as well as overdose control. Simulation experiments show that the proposed LSE design achieves a higher accuracy in estimating the MTD and involves a lower risk of overdosing allocation compared to existing designs, thereby indicating that it provides an effective methodology for phase I cancer clinical trial design."
  },
  {
    "title": "Research on the Crystal Growth, Band Structure and Luminescence Mechanism of (CH3NH3)2HgI4",
    "url": "http://arxiv.org/abs/2504.09150v1",
    "arxiv_id": "2504.09150v1",
    "authors": [
      "Linlin Liu",
      "Zuanquan Chen",
      "Wensi Yang",
      "Sen Zhang",
      "Jiaqi Zhu"
    ],
    "published": "2025-04-12T09:30:12+00:00",
    "summary": "Nuclear radiation detectors play a crucial role in fields such as nuclear safety and medical imaging. The core of their performance lies in the selection of detection materials. Semiconductor detectors have become a hot topic in current research due to their advantages such as small size, good energy resolution, and high detection efficiency. As one of the most promising materials for fabricating room - temperature nuclear radiation semiconductor detectors, HgI2 exhibits excellent detection performance due to its high atomic number, large band gap, strong ray - stopping power, and high volume dark resistivity. However, issues such as poor chemical stability and low vacancy mobility of HgI2 limit its development. Therefore, researchers have carried out inorganic doping/organic hybridization on it. By introducing the organic ligand CH3NH3I, the synthesis of organic - inorganic hybrid compounds based on HgI2 is expected to significantly improve the stability of HgI2. Research on organic - inorganic hybrid metal halide crystals shows that this material has great application potential in the field of luminescent materials."
  },
  {
    "title": "Can Large Language Models Become Policy Refinement Partners? Evidence from China's Social Security Studies",
    "url": "http://arxiv.org/abs/2504.09137v1",
    "arxiv_id": "2504.09137v1",
    "authors": [
      "Ke Jinghan",
      "Zhou Zheng",
      "Zhao Yuxuan"
    ],
    "published": "2025-04-12T08:50:12+00:00",
    "summary": "The rapid development of large language models (LLMs) is reshaping operational paradigms across multidisciplinary domains. LLMs' emergent capability to synthesize policy-relevant insights across disciplinary boundaries suggests potential as decision-support tools. However, their actual performance and suitability as policy refinement partners still require verification through rigorous and systematic evaluations. Our study employs the context-embedded generation-adaptation framework to conduct a tripartite comparison among the American GPT-4o, the Chinese DeepSeek-R1 and human researchers, investigating the capability boundaries and performance characteristics of LLMs in generating policy recommendations for China's social security issues. This study demonstrates that while large LLMs exhibit distinct advantages in systematic policy design, they face significant limitations in addressing complex social dynamics, balancing stakeholder interests, and controlling fiscal risks within the social security domain. Furthermore, DeepSeek-R1 demonstrates superior performance to GPT-4o across all evaluation dimensions in policy recommendation generation, illustrating the potential of localized training to improve contextual alignment. These findings suggest that regionally-adapted LLMs can function as supplementary tools for generating diverse policy alternatives informed by domain-specific social insights. Nevertheless, the formulation of policy refinement requires integration with human researchers' expertise, which remains critical for interpreting institutional frameworks, cultural norms, and value systems."
  },
  {
    "title": "Can Large Language Models Become Policy Refinement Partners? Evidence from China's Social Security Studies",
    "url": "http://arxiv.org/abs/2504.09137v2",
    "arxiv_id": "2504.09137v2",
    "authors": [
      "Ke Jinghan",
      "Zhou Zheng",
      "Zhao Yuxuan"
    ],
    "published": "2025-04-12T08:50:12+00:00",
    "summary": "The rapid development of large language models (LLMs) is reshaping operational paradigms across multidisciplinary domains. LLMs' emergent capability to synthesize policy-relevant insights across disciplinary boundaries suggests potential as decision-support tools. However, their actual performance and suitability as policy refinement partners still require verification through rigorous and systematic evaluations. Our study employs the context-embedded generation-adaptation framework to conduct a tripartite comparison among the American GPT-4o, the Chinese DeepSeek-R1 and human researchers, investigating the capability boundaries and performance characteristics of LLMs in generating policy recommendations for China's social security issues. This study demonstrates that while LLMs exhibit distinct advantages in systematic policy design, they face significant limitations in addressing complex social dynamics, balancing stakeholder interests, and controlling fiscal risks within the social security domain. Furthermore, DeepSeek-R1 demonstrates superior performance to GPT-4o across all evaluation dimensions in policy recommendation generation, illustrating the potential of localized training to improve contextual alignment. These findings suggest that regionally-adapted LLMs can function as supplementary tools for generating diverse policy alternatives informed by domain-specific social insights. Nevertheless, the formulation of policy refinement requires integration with human researchers' expertise, which remains critical for interpreting institutional frameworks, cultural norms, and value systems."
  },
  {
    "title": "BiFlex: A Passive Bimodal Stiffness Flexible Wrist for Manipulation in Unstructured Environments",
    "url": "http://arxiv.org/abs/2504.08706v1",
    "arxiv_id": "2504.08706v1",
    "authors": [
      "Gu-Cheol Jeong",
      "Stefano Dalla Gasperina",
      "Ashish D. Deshpande",
      "Lillian Chin",
      "Roberto Mart\u00edn-Mart\u00edn"
    ],
    "published": "2025-04-11T17:16:13+00:00",
    "summary": "Robotic manipulation in unstructured, humancentric environments poses a dual challenge: achieving the precision need for delicate free-space operation while ensuring safety during unexpected contact events. Traditional wrists struggle to balance these demands, often relying on complex control schemes or complicated mechanical designs to mitigate potential damage from force overload. In response, we present BiFlex, a flexible robotic wrist that uses a soft buckling honeycomb structure to provides a natural bimodal stiffness response. The higher stiffness mode enables precise household object manipulation, while the lower stiffness mode provides the compliance needed to adapt to external forces. We design BiFlex to maintain a fingertip deflection of less than 1 cm while supporting loads up to 500g and create a BiFlex wrist for many grippers, including Panda, Robotiq, and BaRiFlex. We validate BiFlex under several real-world experimental evaluations, including surface wiping, precise pick-and-place, and grasping under environmental constraints. We demonstrate that BiFlex simplifies control while maintaining precise object manipulation and enhanced safety in real-world applications."
  },
  {
    "title": "Offline Reinforcement Learning using Human-Aligned Reward Labeling for Autonomous Emergency Braking in Occluded Pedestrian Crossing",
    "url": "http://arxiv.org/abs/2504.08704v1",
    "arxiv_id": "2504.08704v1",
    "authors": [
      "Vinal Asodia",
      "Zhenhua Feng",
      "Saber Fallah"
    ],
    "published": "2025-04-11T17:11:21+00:00",
    "summary": "Effective leveraging of real-world driving datasets is crucial for enhancing the training of autonomous driving systems. While Offline Reinforcement Learning enables the training of autonomous vehicles using such data, most available datasets lack meaningful reward labels. Reward labeling is essential as it provides feedback for the learning algorithm to distinguish between desirable and undesirable behaviors, thereby improving policy performance. This paper presents a novel pipeline for generating human-aligned reward labels. The proposed approach addresses the challenge of absent reward signals in real-world datasets by generating labels that reflect human judgment and safety considerations. The pipeline incorporates an adaptive safety component, activated by analyzing semantic segmentation maps, allowing the autonomous vehicle to prioritize safety over efficiency in potential collision scenarios. The proposed pipeline is applied to an occluded pedestrian crossing scenario with varying levels of pedestrian traffic, using synthetic and simulation data. The results indicate that the generated reward labels closely match the simulation reward labels. When used to train the driving policy using Behavior Proximal Policy Optimisation, the results are competitive with other baselines. This demonstrates the effectiveness of our method in producing reliable and human-aligned reward signals, facilitating the training of autonomous driving systems through Reinforcement Learning outside of simulation environments and in alignment with human values."
  },
  {
    "title": "Safe Flow Matching: Robot Motion Planning with Control Barrier Functions",
    "url": "http://arxiv.org/abs/2504.08661v1",
    "arxiv_id": "2504.08661v1",
    "authors": [
      "Xiaobing Dai",
      "Dian Yu",
      "Shanshan Zhang",
      "Zewen Yang"
    ],
    "published": "2025-04-11T16:10:58+00:00",
    "summary": "Recent advances in generative modeling have led to promising results in robot motion planning, particularly through diffusion and flow-based models that capture complex, multimodal trajectory distributions. However, these methods are typically trained offline and remain limited when faced with unseen environments or dynamic constraints, often lacking explicit mechanisms to ensure safety during deployment. In this work, we propose, Safe Flow Matching (SafeFM), a motion planning approach for trajectory generation that integrates flow matching with safety guarantees. By incorporating the proposed flow matching barrier functions, SafeFM ensures that generated trajectories remain within safe regions throughout the planning horizon, even in the presence of previously unseen obstacles or state-action constraints. Unlike diffusion-based approaches, our method allows for direct, efficient sampling of constraint-satisfying trajectories, making it well-suited for real-time motion planning. We evaluate SafeFM on a diverse set of tasks, including planar robot navigation and 7-DoF manipulation, demonstrating superior safety, generalization, and planning performance compared to state-of-the-art generative planners. Comprehensive resources are available on the project website: https://safeflowmatching.github.io/SafeFM/"
  },
  {
    "title": "TinyCenterSpeed: Efficient Center-Based Object Detection for Autonomous Racing",
    "url": "http://arxiv.org/abs/2504.08655v1",
    "arxiv_id": "2504.08655v1",
    "authors": [
      "Neil Reichlin",
      "Nicolas Baumann",
      "Edoardo Ghignone",
      "Michele Magno"
    ],
    "published": "2025-04-11T15:58:46+00:00",
    "summary": "Perception within autonomous driving is nearly synonymous with Neural Networks (NNs). Yet, the domain of autonomous racing is often characterized by scaled, computationally limited robots used for cost-effectiveness and safety. For this reason, opponent detection and tracking systems typically resort to traditional computer vision techniques due to computational constraints. This paper introduces TinyCenterSpeed, a streamlined adaptation of the seminal CenterPoint method, optimized for real-time performance on 1:10 scale autonomous racing platforms. This adaptation is viable even on OBCs powered solely by Central Processing Units (CPUs), as it incorporates the use of an external Tensor Processing Unit (TPU). We demonstrate that, compared to Adaptive Breakpoint Detector (ABD), the current State-of-the-Art (SotA) in scaled autonomous racing, TinyCenterSpeed not only improves detection and velocity estimation by up to 61.38% but also supports multi-opponent detection and estimation. It achieves real-time performance with an inference time of just 7.88 ms on the TPU, significantly reducing CPU utilization 8.3-fold."
  },
  {
    "title": "Deep Learning Methods for Detecting Thermal Runaway Events in Battery Production Lines",
    "url": "http://arxiv.org/abs/2504.08632v1",
    "arxiv_id": "2504.08632v1",
    "authors": [
      "Athanasios Athanasopoulos",
      "Mat\u00fa\u0161 Mihal\u00e1k",
      "Marcin Pietrasik"
    ],
    "published": "2025-04-11T15:35:50+00:00",
    "summary": "One of the key safety considerations of battery manufacturing is thermal runaway, the uncontrolled increase in temperature which can lead to fires, explosions, and emissions of toxic gasses. As such, development of automated systems capable of detecting such events is of considerable importance in both academic and industrial contexts. In this work, we investigate the use of deep learning for detecting thermal runaway in the battery production line of VDL Nedcar, a Dutch automobile manufacturer. Specifically, we collect data from the production line to represent both baseline (non thermal runaway) and thermal runaway conditions. Thermal runaway was simulated through the use of external heat and smoke sources. The data consisted of both optical and thermal images which were then preprocessed and fused before serving as input to our models. In this regard, we evaluated three deep-learning models widely used in computer vision including shallow convolutional neural networks, residual neural networks, and vision transformers on two performance metrics. Furthermore, we evaluated these models using explainability methods to gain insight into their ability to capture the relevant feature information from their inputs. The obtained results indicate that the use of deep learning is a viable approach to thermal runaway detection in battery production lines."
  },
  {
    "title": "Enabling Safety for Aerial Robots: Planning and Control Architectures",
    "url": "http://arxiv.org/abs/2504.08601v1",
    "arxiv_id": "2504.08601v1",
    "authors": [
      "Kaleb Ben Naveed",
      "Devansh R. Agrawal",
      "Daniel M. Cherenson",
      "Haejoon Lee",
      "Alia Gilbert",
      "Hardik Parwana",
      "Vishnu S. Chipade",
      "William Bentz",
      "Dimitra Panagou"
    ],
    "published": "2025-04-11T15:05:31+00:00",
    "summary": "Ensuring safe autonomy is crucial for deploying aerial robots in real-world applications. However, safety is a multifaceted challenge that must be addressed from multiple perspectives, including navigation in dynamic environments, operation under resource constraints, and robustness against adversarial attacks and uncertainties. In this paper, we present the authors' recent work that tackles some of these challenges and highlights key aspects that must be considered to enhance the safety and performance of autonomous aerial systems. All presented approaches are validated through hardware experiments."
  },
  {
    "title": "Secondary Safety Control for Systems with Sector Bounded Nonlinearities",
    "url": "http://arxiv.org/abs/2504.08535v1",
    "arxiv_id": "2504.08535v1",
    "authors": [
      "Yankai Lin",
      "Michelle S. Chong",
      "Carlos Murguia"
    ],
    "published": "2025-04-11T13:44:22+00:00",
    "summary": "We consider the problem of safety verification and safety-aware controller synthesis for systems with sector bounded nonlinearities. We aim to keep the states of the system within a given safe set under potential actuator and sensor attacks. Specifically, we adopt the setup that a controller has already been designed to stabilize the plant. Using invariant sets and barrier certificate theory, we first give sufficient conditions to verify the safety of the closed-loop system under attacks. Furthermore, by using a subset of sensors that are assumed to be free of attacks, we provide a synthesis method for a secondary controller that enhances the safety of the system. The sufficient conditions to verify safety are derived using Lyapunov-based tools and the S-procedure. Using the projection lemma, the conditions are then formulated as linear matrix inequality (LMI) problems which can be solved efficiently. Lastly, our theoretical results are illustrated through numerical simulations."
  },
  {
    "title": "Secondary Safety Control for Systems with Sector Bounded Nonlinearities [Extended Version]",
    "url": "http://arxiv.org/abs/2504.08535v2",
    "arxiv_id": "2504.08535v2",
    "authors": [
      "Yankai Lin",
      "Michelle S. Chong",
      "Carlos Murguia"
    ],
    "published": "2025-04-11T13:44:22+00:00",
    "summary": "We consider the problem of safety verification and safety-aware controller synthesis for systems with sector bounded nonlinearities. We aim to keep the states of the system within a given safe set under potential actuator and sensor attacks. Specifically, we adopt the setup that a controller has already been designed to stabilize the plant. Using invariant sets and barrier certificate theory, we first give sufficient conditions to verify the safety of the closed-loop system under attacks. Furthermore, by using a subset of sensors that are assumed to be free of attacks, we provide a synthesis method for a secondary controller that enhances the safety of the system. The sufficient conditions to verify safety are derived using Lyapunov-based tools and the S-procedure. Using the projection lemma, the conditions are then formulated as linear matrix inequality (LMI) problems which can be solved efficiently. Lastly, our theoretical results are illustrated through numerical simulations."
  },
  {
    "title": "Physics-informed data-driven control without persistence of excitation",
    "url": "http://arxiv.org/abs/2504.08484v1",
    "arxiv_id": "2504.08484v1",
    "authors": [
      "Martina Vanelli",
      "Julien M. Hendrickx"
    ],
    "published": "2025-04-11T12:19:51+00:00",
    "summary": "We show that data that is not sufficiently informative to allow for system re-identification can still provide meaningful information when combined with external or physical knowledge of the system, such as bounded system matrix norms. We then illustrate how this information can be leveraged for safety and energy minimization problems and to enhance predictions in unmodelled dynamics. This preliminary work outlines key ideas toward using limited data for effective control by integrating physical knowledge of the system and exploiting interpolation conditions."
  },
  {
    "title": "Constrained Machine Learning Through Hyperspherical Representation",
    "url": "http://arxiv.org/abs/2504.08415v1",
    "arxiv_id": "2504.08415v1",
    "authors": [
      "Gaetano Signorelli",
      "Michele Lombardi"
    ],
    "published": "2025-04-11T10:19:49+00:00",
    "summary": "The problem of ensuring constraints satisfaction on the output of machine learning models is critical for many applications, especially in safety-critical domains. Modern approaches rely on penalty-based methods at training time, which do not guarantee to avoid constraints violations; or constraint-specific model architectures (e.g., for monotonocity); or on output projection, which requires to solve an optimization problem that might be computationally demanding. We present the Hypersherical Constrained Representation, a novel method to enforce constraints in the output space for convex and bounded feasibility regions (generalizable to star domains). Our method operates on a different representation system, where Euclidean coordinates are converted into hyperspherical coordinates relative to the constrained region, which can only inherently represent feasible points. Experiments on a synthetic and a real-world dataset show that our method has predictive performance comparable to the other approaches, can guarantee 100% constraint satisfaction, and has a minimal computational cost at inference time."
  },
  {
    "title": "Evaluating Pedestrian Risks in Shared Spaces Through Autonomous Vehicle Experiments on a Fixed Track",
    "url": "http://arxiv.org/abs/2504.08316v1",
    "arxiv_id": "2504.08316v1",
    "authors": [
      "Enrico Del Re",
      "Novel Certad",
      "Cristina Olaverri-Monreal"
    ],
    "published": "2025-04-11T07:37:15+00:00",
    "summary": "The majority of research on safety in autonomous vehicles has been conducted in structured and controlled environments. However, there is a scarcity of research on safety in unregulated pedestrian areas, especially when interacting with public transport vehicles like trams. This study investigates pedestrian responses to an alert system in this context by replicating this real-world scenario in an environment using an autonomous vehicle. The results show that safety measures from other contexts can be adapted to shared spaces with trams, where fixed tracks heighten risks in unregulated crossings."
  },
  {
    "title": "SortBench: Benchmarking LLMs based on their ability to sort lists",
    "url": "http://arxiv.org/abs/2504.08312v1",
    "arxiv_id": "2504.08312v1",
    "authors": [
      "Steffen Herbold"
    ],
    "published": "2025-04-11T07:29:56+00:00",
    "summary": "Sorting is a tedious but simple task for human intelligence and can be solved fairly easily algorithmically. However, for Large Language Models (LLMs) this task is surprisingly hard, as some properties of sorting are among known weaknesses of LLMs: being faithful to the input data, logical comparisons between values, and strictly differentiating between syntax (used for sorting) and semantics (typically learned by embeddings). Within this paper, we describe the new SortBench benchmark for LLMs that comes with different difficulties and that can be easily scaled in terms of difficulty. We apply this benchmark to seven state-of-the-art LLMs, including current test-time reasoning models. Our results show that while the o3-mini model is very capable at sorting in general, even this can be fooled if strings are defined to mix syntactical and semantical aspects, e.g., by asking to sort numbers written-out as word. Furthermore, all models have problems with the faithfulness to the input of long lists, i.e., they drop items and add new ones. Our results also show that test-time reasoning has a tendency to overthink problems which leads to performance degradation. Finally, models without test-time reasoning like GPT-4o are not much worse than reasoning models."
  },
  {
    "title": "Evaluating the Bias in LLMs for Surveying Opinion and Decision Making in Healthcare",
    "url": "http://arxiv.org/abs/2504.08260v1",
    "arxiv_id": "2504.08260v1",
    "authors": [
      "Yonchanok Khaokaew",
      "Flora D. Salim",
      "Andreas Z\u00fcfle",
      "Hao Xue",
      "Taylor Anderson",
      "Matthew Scotch",
      "David J Heslop"
    ],
    "published": "2025-04-11T05:11:40+00:00",
    "summary": "Generative agents have been increasingly used to simulate human behaviour in silico, driven by large language models (LLMs). These simulacra serve as sandboxes for studying human behaviour without compromising privacy or safety. However, it remains unclear whether such agents can truly represent real individuals. This work compares survey data from the Understanding America Study (UAS) on healthcare decision-making with simulated responses from generative agents. Using demographic-based prompt engineering, we create digital twins of survey respondents and analyse how well different LLMs reproduce real-world behaviours. Our findings show that some LLMs fail to reflect realistic decision-making, such as predicting universal vaccine acceptance. However, Llama 3 captures variations across race and Income more accurately but also introduces biases not present in the UAS data. This study highlights the potential of generative agents for behavioural research while underscoring the risks of bias from both LLMs and prompting strategies."
  },
  {
    "title": "Neural Network-assisted Interval Reachability for Systems with Control Barrier Function-Based Safe Controllers",
    "url": "http://arxiv.org/abs/2504.08249v1",
    "arxiv_id": "2504.08249v1",
    "authors": [
      "Damola Ajeyemi",
      "Saber Jafarpour",
      "Emiliano Dall'Anese"
    ],
    "published": "2025-04-11T04:14:55+00:00",
    "summary": "Control Barrier Functions (CBFs) have been widely utilized in the design of optimization-based controllers and filters for dynamical systems to ensure forward invariance of a given set of safe states. While CBF-based controllers offer safety guarantees, they can compromise the performance of the system, leading to undesirable behaviors such as unbounded trajectories and emergence of locally stable spurious equilibria. Computing reachable sets for systems with CBF-based controllers is an effective approach for runtime performance and stability verification, and can potentially serve as a tool for trajectory re-planning. In this paper, we propose a computationally efficient interval reachability method for performance verification of systems with optimization-based controllers by: (i) approximating the optimization-based controller by a pre-trained neural network to avoid solving optimization problems repeatedly, and (ii) using mixed monotone theory to construct an embedding system that leverages state-of-the-art neural network verification algorithms for bounding the output of the neural network. Results in terms of closeness of solutions of trajectories of the system with the optimization-based controller and the neural network are derived. Using a single trajectory of the embedding system along with our closeness of solutions result, we obtain an over-approximation of the reachable set of the system with optimization-based controllers. Numerical results are presented to corroborate the technical findings."
  },
  {
    "title": "InSPE: Rapid Evaluation of Heterogeneous Multi-Modal Infrastructure Sensor Placement",
    "url": "http://arxiv.org/abs/2504.08240v1",
    "arxiv_id": "2504.08240v1",
    "authors": [
      "Zhaoliang Zheng",
      "Yun Zhang",
      "Zongling Meng",
      "Johnson Liu",
      "Xin Xia",
      "Jiaqi Ma"
    ],
    "published": "2025-04-11T03:55:00+00:00",
    "summary": "Infrastructure sensing is vital for traffic monitoring at safety hotspots (e.g., intersections) and serves as the backbone of cooperative perception in autonomous driving. While vehicle sensing has been extensively studied, infrastructure sensing has received little attention, especially given the unique challenges of diverse intersection geometries, complex occlusions, varying traffic conditions, and ambient environments like lighting and weather. To address these issues and ensure cost-effective sensor placement, we propose Heterogeneous Multi-Modal Infrastructure Sensor Placement Evaluation (InSPE), a perception surrogate metric set that rapidly assesses perception effectiveness across diverse infrastructure and environmental scenarios with combinations of multi-modal sensors. InSPE systematically evaluates perception capabilities by integrating three carefully designed metrics, i.e., sensor coverage, perception occlusion, and information gain. To support large-scale evaluation, we develop a data generation tool within the CARLA simulator and also introduce Infra-Set, a dataset covering diverse intersection types and environmental conditions. Benchmarking experiments with state-of-the-art perception algorithms demonstrate that InSPE enables efficient and scalable sensor placement analysis, providing a robust solution for optimizing intelligent intersection infrastructure."
  },
  {
    "title": "Advancing Autonomous Vehicle Safety: A Combined Fault Tree Analysis and Bayesian Network Approach",
    "url": "http://arxiv.org/abs/2504.08206v1",
    "arxiv_id": "2504.08206v1",
    "authors": [
      "Lansu Dai",
      "Burak Kantarci"
    ],
    "published": "2025-04-11T02:17:10+00:00",
    "summary": "This paper integrates Fault Tree Analysis (FTA) and Bayesian Networks (BN) to assess collision risk and establish Automotive Safety Integrity Level (ASIL) B failure rate targets for critical autonomous vehicle (AV) components. The FTA-BN integration combines the systematic decomposition of failure events provided by FTA with the probabilistic reasoning capabilities of BN, which allow for dynamic updates in failure probabilities, enhancing the adaptability of risk assessment. A fault tree is constructed based on AV subsystem architecture, with collision as the top event, and failure rates are assigned while ensuring the total remains within 100 FIT. Bayesian inference is applied to update posterior probabilities, and the results indicate that perception system failures (46.06 FIT) are the most significant contributor, particularly failures to detect existing objects (PF5) and misclassification (PF6). Mitigation strategies are proposed for sensors, perception, decision-making, and motion control to reduce the collision risk. The FTA-BN integration approach provides dynamic risk quantification, offering system designers refined failure rate targets to improve AV safety."
  },
  {
    "title": "EO-VLM: VLM-Guided Energy Overload Attacks on Vision Models",
    "url": "http://arxiv.org/abs/2504.08205v1",
    "arxiv_id": "2504.08205v1",
    "authors": [
      "Minjae Seo",
      "Myoungsung You",
      "Junhee Lee",
      "Jaehan Kim",
      "Hwanjo Heo",
      "Jintae Oh",
      "Jinwoo Kim"
    ],
    "published": "2025-04-11T02:13:24+00:00",
    "summary": "Vision models are increasingly deployed in critical applications such as autonomous driving and CCTV monitoring, yet they remain susceptible to resource-consuming attacks. In this paper, we introduce a novel energy-overloading attack that leverages vision language model (VLM) prompts to generate adversarial images targeting vision models. These images, though imperceptible to the human eye, significantly increase GPU energy consumption across various vision models, threatening the availability of these systems. Our framework, EO-VLM (Energy Overload via VLM), is model-agnostic, meaning it is not limited by the architecture or type of the target vision model. By exploiting the lack of safety filters in VLMs like DALL-E 3, we create adversarial noise images without requiring prior knowledge or internal structure of the target vision models. Our experiments demonstrate up to a 50% increase in energy consumption, revealing a critical vulnerability in current vision models."
  },
  {
    "title": "SAEs $\\textit{Can}$ Improve Unlearning: Dynamic Sparse Autoencoder Guardrails for Precision Unlearning in LLMs",
    "url": "http://arxiv.org/abs/2504.08192v1",
    "arxiv_id": "2504.08192v1",
    "authors": [
      "Aashiq Muhamed",
      "Jacopo Bonato",
      "Mona Diab",
      "Virginia Smith"
    ],
    "published": "2025-04-11T01:24:03+00:00",
    "summary": "Machine unlearning is a promising approach to improve LLM safety by removing unwanted knowledge from the model. However, prevailing gradient-based unlearning methods suffer from issues such as high computational costs, hyperparameter instability, poor sequential unlearning capability, vulnerability to relearning attacks, low data efficiency, and lack of interpretability. While Sparse Autoencoders are well-suited to improve these aspects by enabling targeted activation-based unlearning, prior approaches underperform gradient-based methods. This work demonstrates that, contrary to these earlier findings, SAEs can significantly improve unlearning when employed dynamically. We introduce $\\textbf{Dynamic DAE Guardrails}$ (DSG), a novel method for precision unlearning that leverages principled feature selection and a dynamic classifier. Our experiments show DSG substantially outperforms leading unlearning methods, achieving superior forget-utility trade-offs. DSG addresses key drawbacks of gradient-based approaches for unlearning -- offering enhanced computational efficiency and stability, robust performance in sequential unlearning, stronger resistance to relearning attacks, better data efficiency including zero-shot settings, and more interpretable unlearning."
  },
  {
    "title": "Safe Data-Driven Predictive Control",
    "url": "http://arxiv.org/abs/2504.08188v1",
    "arxiv_id": "2504.08188v1",
    "authors": [
      "Amin Vahidi-Moghaddam",
      "Kaian Chen",
      "Kaixiang Zhang",
      "Zhaojian Li",
      "Yan Wang",
      "Kai Wu"
    ],
    "published": "2025-04-11T01:08:21+00:00",
    "summary": "In the realm of control systems, model predictive control (MPC) has exhibited remarkable potential; however, its reliance on accurate models and substantial computational resources has hindered its broader application, especially within real-time nonlinear systems. This study presents an innovative control framework to enhance the practical viability of the MPC. The developed safe data-driven predictive control aims to eliminate the requirement for precise models and alleviate computational burdens in the nonlinear MPC (NMPC). This is achieved by learning both the system dynamics and the control policy, enabling efficient data-driven predictive control while ensuring system safety. The methodology involves a spatial temporal filter (STF)-based concurrent learning for system identification, a robust control barrier function (RCBF) to ensure the system safety amid model uncertainties, and a RCBF-based NMPC policy approximation. An online policy correction mechanism is also introduced to counteract performance degradation caused by the existing model uncertainties. Demonstrated through simulations on two applications, the proposed approach offers comparable performance to existing benchmarks with significantly reduced computational costs."
  },
  {
    "title": "Investigating Vision-Language Model for Point Cloud-based Vehicle Classification",
    "url": "http://arxiv.org/abs/2504.08154v1",
    "arxiv_id": "2504.08154v1",
    "authors": [
      "Yiqiao Li",
      "Jie Wei",
      "Camille Kamga"
    ],
    "published": "2025-04-10T22:37:27+00:00",
    "summary": "Heavy-duty trucks pose significant safety challenges due to their large size and limited maneuverability compared to passenger vehicles. A deeper understanding of truck characteristics is essential for enhancing the safety perspective of cooperative autonomous driving. Traditional LiDAR-based truck classification methods rely on extensive manual annotations, which makes them labor-intensive and costly. The rapid advancement of large language models (LLMs) trained on massive datasets presents an opportunity to leverage their few-shot learning capabilities for truck classification. However, existing vision-language models (VLMs) are primarily trained on image datasets, which makes it challenging to directly process point cloud data. This study introduces a novel framework that integrates roadside LiDAR point cloud data with VLMs to facilitate efficient and accurate truck classification, which supports cooperative and safe driving environments. This study introduces three key innovations: (1) leveraging real-world LiDAR datasets for model development, (2) designing a preprocessing pipeline to adapt point cloud data for VLM input, including point cloud registration for dense 3D rendering and mathematical morphological techniques to enhance feature representation, and (3) utilizing in-context learning with few-shot prompting to enable vehicle classification with minimally labeled training data. Experimental results demonstrate encouraging performance of this method and present its potential to reduce annotation efforts while improving classification accuracy."
  },
  {
    "title": "Development and Performance Analysis of Glass-Based Gas-Tight RPCs for Muography Applications",
    "url": "http://arxiv.org/abs/2504.08146v1",
    "arxiv_id": "2504.08146v1",
    "authors": [
      "S. Ikram",
      "S. Basnet",
      "E. Cortina Gil",
      "P. Demin",
      "R. M. I. D. Gamage",
      "A. Giammanco",
      "R. Karnam",
      "V. K. S. Kashyap",
      "V. Kumar",
      "B. Mohanty",
      "M. Moussawi",
      "A. Samalan",
      "M. Tytgat"
    ],
    "published": "2025-04-10T22:14:32+00:00",
    "summary": "To achieve high-resolution muography of compact targets in scenarios with complex logistical constraints, we are developing a portable muon detector system utilizing glass Resistive Plate Chambers (RPCs). Although RPCs are well understood and widely used, our work focuses on developing a gas-tight variant specifically tailored for a broad range of muography applications, with key design goals including portability, robustness, autonomy, versatility, safety, and cost-effectiveness. Our RPC detectors are designed with various configurations, each featuring unique characteristics and performance attributes. We investigate the temporal evolution of the surface resistivity of glass electrodes, as well as the detector efficiency at varying voltages and thresholds, over a span of several months. These RPCs have been utilized in a small-scale feasibility study on muon absorption using lead blocks."
  },
  {
    "title": "Certified to Drive: A Policy Proposal for Mandatory Training on Semi-Automated Vehicles",
    "url": "http://arxiv.org/abs/2504.08128v1",
    "arxiv_id": "2504.08128v1",
    "authors": [
      "Soumita Mukherjee",
      "Varun Darshana Parekh",
      "Nikhil Tayal"
    ],
    "published": "2025-04-10T21:11:31+00:00",
    "summary": "Although the Boeing 737 Max incidents resulted from a mix of design shortcomings, regulatory oversights, and systemic issues, they also highlight a critical gap in pilot training on managing automated systems during abnormal conditions. This example demonstrates the urgent need for focused, concise training on human-automation interaction - a need that is equally critical for operators of Level 2 ADAS-equipped vehicles, as discussed in detail later in this article. The lack of structured education for semi-automated vehicle operators mirrors similar risks in other industries, where formal training is critical for safe operation. Two policy recommendations are proposed. First, governments should create concise, official resources in accessible and official format to educate drivers on system capabilities and limitations. Second, mandatory training and certification programs should be introduced, combining theoretical and hands-on components to prepare drivers for real-world scenarios. These measures will improve driver understanding, reduce misuse, and foster public trust in semi-automated vehicle technologies. By addressing the knowledge gap, policymakers can ensure a safer, more responsible transition to automation, maximizing its benefits while minimizing risks to public safety."
  },
  {
    "title": "DeepSeek vs. o3-mini: How Well can Reasoning LLMs Evaluate MT and Summarization?",
    "url": "http://arxiv.org/abs/2504.08120v1",
    "arxiv_id": "2504.08120v1",
    "authors": [
      "Daniil Larionov",
      "Sotaro Takeshita",
      "Ran Zhang",
      "Yanran Chen",
      "Christoph Leiter",
      "Zhipin Wang",
      "Christian Greisinger",
      "Steffen Eger"
    ],
    "published": "2025-04-10T20:39:18+00:00",
    "summary": "Reasoning-enabled large language models (LLMs) have recently demonstrated impressive performance in complex logical and mathematical tasks, yet their effectiveness in evaluating natural language generation remains unexplored. This study systematically compares reasoning-based LLMs (DeepSeek-R1 and OpenAI o3) with their non-reasoning counterparts across machine translation (MT) and text summarization (TS) evaluation tasks. We evaluate eight models across three architectural categories, including state-of-the-art reasoning models, their distilled variants (ranging from 8B to 70B parameters), and equivalent conventional, non-reasoning LLMs. Our experiments on WMT23 and SummEval benchmarks reveal that the benefits of reasoning capabilities are highly model and task-dependent: while OpenAI o3-mini models show consistent performance improvements with increased reasoning intensity, DeepSeek-R1 underperforms compared to its non-reasoning variant, with exception to certain aspects of TS evaluation. Correlation analysis demonstrates that increased reasoning token usage positively correlates with evaluation quality in o3-mini models. Furthermore, our results show that distillation of reasoning capabilities maintains reasonable performance in medium-sized models (32B) but degrades substantially in smaller variants (8B). This work provides the first comprehensive assessment of reasoning LLMs for NLG evaluation and offers insights into their practical use."
  },
  {
    "title": "Geneshift: Impact of different scenario shift on Jailbreaking LLM",
    "url": "http://arxiv.org/abs/2504.08104v1",
    "arxiv_id": "2504.08104v1",
    "authors": [
      "Tianyi Wu",
      "Zhiwei Xue",
      "Yue Liu",
      "Jiaheng Zhang",
      "Bryan Hooi",
      "See-Kiong Ng"
    ],
    "published": "2025-04-10T20:02:35+00:00",
    "summary": "Jailbreak attacks, which aim to cause LLMs to perform unrestricted behaviors, have become a critical and challenging direction in AI safety. Despite achieving the promising attack success rate using dictionary-based evaluation, existing jailbreak attack methods fail to output detailed contents to satisfy the harmful request, leading to poor performance on GPT-based evaluation. To this end, we propose a black-box jailbreak attack termed GeneShift, by using a genetic algorithm to optimize the scenario shifts. Firstly, we observe that the malicious queries perform optimally under different scenario shifts. Based on it, we develop a genetic algorithm to evolve and select the hybrid of scenario shifts. It guides our method to elicit detailed and actionable harmful responses while keeping the seemingly benign facade, improving stealthiness. Extensive experiments demonstrate the superiority of GeneShift. Notably, GeneShift increases the jailbreak success rate from 0% to 60% when direct prompting alone would fail."
  },
  {
    "title": "The AI Scientist-v2: Workshop-Level Automated Scientific Discovery via Agentic Tree Search",
    "url": "http://arxiv.org/abs/2504.08066v1",
    "arxiv_id": "2504.08066v1",
    "authors": [
      "Yutaro Yamada",
      "Robert Tjarko Lange",
      "Cong Lu",
      "Shengran Hu",
      "Chris Lu",
      "Jakob Foerster",
      "Jeff Clune",
      "David Ha"
    ],
    "published": "2025-04-10T18:44:41+00:00",
    "summary": "AI is increasingly playing a pivotal role in transforming how scientific discoveries are made. We introduce The AI Scientist-v2, an end-to-end agentic system capable of producing the first entirely AI generated peer-review-accepted workshop paper. This system iteratively formulates scientific hypotheses, designs and executes experiments, analyzes and visualizes data, and autonomously authors scientific manuscripts. Compared to its predecessor (v1, Lu et al., 2024 arXiv:2408.06292), The AI Scientist-v2 eliminates the reliance on human-authored code templates, generalizes effectively across diverse machine learning domains, and leverages a novel progressive agentic tree-search methodology managed by a dedicated experiment manager agent. Additionally, we enhance the AI reviewer component by integrating a Vision-Language Model (VLM) feedback loop for iterative refinement of content and aesthetics of the figures. We evaluated The AI Scientist-v2 by submitting three fully autonomous manuscripts to a peer-reviewed ICLR workshop. Notably, one manuscript achieved high enough scores to exceed the average human acceptance threshold, marking the first instance of a fully AI-generated paper successfully navigating a peer review. This accomplishment highlights the growing capability of AI in conducting all aspects of scientific research. We anticipate that further advancements in autonomous scientific discovery technologies will profoundly impact human knowledge generation, enabling unprecedented scalability in research productivity and significantly accelerating scientific breakthroughs, greatly benefiting society at large. We have open-sourced the code at https://github.com/SakanaAI/AI-Scientist-v2 to foster the future development of this transformative technology. We also discuss the role of AI in science, including AI safety."
  },
  {
    "title": "Can Reasoning LLMs Enhance Clinical Document Classification?",
    "url": "http://arxiv.org/abs/2504.08040v1",
    "arxiv_id": "2504.08040v1",
    "authors": [
      "Akram Mustafa",
      "Usman Naseem",
      "Mostafa Rahimi Azghadi"
    ],
    "published": "2025-04-10T18:00:27+00:00",
    "summary": "Clinical document classification is essential for converting unstructured medical texts into standardised ICD-10 diagnoses, yet it faces challenges due to complex medical language, privacy constraints, and limited annotated datasets. Large Language Models (LLMs) offer promising improvements in accuracy and efficiency for this task. This study evaluates the performance and consistency of eight LLMs; four reasoning (Qwen QWQ, Deepseek Reasoner, GPT o3 Mini, Gemini 2.0 Flash Thinking) and four non-reasoning (Llama 3.3, GPT 4o Mini, Gemini 2.0 Flash, Deepseek Chat); in classifying clinical discharge summaries using the MIMIC-IV dataset. Using cTAKES to structure clinical narratives, models were assessed across three experimental runs, with majority voting determining final predictions. Results showed that reasoning models outperformed non-reasoning models in accuracy (71% vs 68%) and F1 score (67% vs 60%), with Gemini 2.0 Flash Thinking achieving the highest accuracy (75%) and F1 score (76%). However, non-reasoning models demonstrated greater stability (91% vs 84% consistency). Performance varied across ICD-10 codes, with reasoning models excelling in complex cases but struggling with abstract categories. Findings indicate a trade-off between accuracy and consistency, suggesting that a hybrid approach could optimise clinical coding. Future research should explore multi-label classification, domain-specific fine-tuning, and ensemble methods to enhance model reliability in real-world applications."
  },
  {
    "title": "VCR-Bench: A Comprehensive Evaluation Framework for Video Chain-of-Thought Reasoning",
    "url": "http://arxiv.org/abs/2504.07956v1",
    "arxiv_id": "2504.07956v1",
    "authors": [
      "Yukun Qi",
      "Yiming Zhao",
      "Yu Zeng",
      "Xikun Bao",
      "Wenxuan Huang",
      "Lin Chen",
      "Zehui Chen",
      "Jie Zhao",
      "Zhongang Qi",
      "Feng Zhao"
    ],
    "published": "2025-04-10T17:59:03+00:00",
    "summary": "The advancement of Chain-of-Thought (CoT) reasoning has significantly enhanced the capabilities of large language models (LLMs) and large vision-language models (LVLMs). However, a rigorous evaluation framework for video CoT reasoning remains absent. Current video benchmarks fail to adequately assess the reasoning process and expose whether failures stem from deficiencies in perception or reasoning capabilities. Therefore, we introduce VCR-Bench, a novel benchmark designed to comprehensively evaluate LVLMs' Video Chain-of-Thought Reasoning capabilities. VCR-Bench comprises 859 videos spanning a variety of video content and durations, along with 1,034 high-quality question-answer pairs. Each pair is manually annotated with a stepwise CoT rationale, where every step is tagged to indicate its association with the perception or reasoning capabilities. Furthermore, we design seven distinct task dimensions and propose the CoT score to assess the entire CoT process based on the stepwise tagged CoT rationals. Extensive experiments on VCR-Bench highlight substantial limitations in current LVLMs. Even the top-performing model, o1, only achieves a 62.8% CoT score and an 56.7% accuracy, while most models score below 40%. Experiments show most models score lower on perception than reasoning steps, revealing LVLMs' key bottleneck in temporal-spatial information processing for complex video reasoning. A robust positive correlation between the CoT score and accuracy confirms the validity of our evaluation framework and underscores the critical role of CoT reasoning in solving complex video reasoning tasks. We hope VCR-Bench to serve as a standardized evaluation framework and expose the actual drawbacks in complex video reasoning task."
  },
  {
    "title": "Perception-R1: Pioneering Perception Policy with Reinforcement Learning",
    "url": "http://arxiv.org/abs/2504.07954v1",
    "arxiv_id": "2504.07954v1",
    "authors": [
      "En Yu",
      "Kangheng Lin",
      "Liang Zhao",
      "Jisheng Yin",
      "Yana Wei",
      "Yuang Peng",
      "Haoran Wei",
      "Jianjian Sun",
      "Chunrui Han",
      "Zheng Ge",
      "Xiangyu Zhang",
      "Daxin Jiang",
      "Jingyu Wang",
      "Wenbing Tao"
    ],
    "published": "2025-04-10T17:58:27+00:00",
    "summary": "Inspired by the success of DeepSeek-R1, we explore the potential of rule-based reinforcement learning (RL) in MLLM post-training for perception policy learning. While promising, our initial experiments reveal that incorporating a thinking process through RL does not consistently lead to performance gains across all visual perception tasks. This leads us to delve into the essential role of RL in the context of visual perception. In this work, we return to the fundamentals and explore the effects of RL on different perception tasks. We observe that the perceptual complexity is a major factor in determining the effectiveness of RL. We also observe that reward design plays a crucial role in further approching the upper limit of model perception. To leverage these findings, we propose Perception-R1, a scalable RL framework using GRPO during MLLM post-training. With a standard Qwen2.5-VL-3B-Instruct, Perception-R1 achieves +4.2% on RefCOCO+, +17.9% on PixMo-Count, +4.2% on PageOCR, and notably, 31.9% AP on COCO2017 val for the first time, establishing a strong baseline for perception policy learning."
  },
  {
    "title": "SoTA with Less: MCTS-Guided Sample Selection for Data-Efficient Visual Reasoning Self-Improvement",
    "url": "http://arxiv.org/abs/2504.07934v1",
    "arxiv_id": "2504.07934v1",
    "authors": [
      "Xiyao Wang",
      "Zhengyuan Yang",
      "Chao Feng",
      "Hongjin Lu",
      "Linjie Li",
      "Chung-Ching Lin",
      "Kevin Lin",
      "Furong Huang",
      "Lijuan Wang"
    ],
    "published": "2025-04-10T17:49:05+00:00",
    "summary": "In this paper, we present an effective method to enhance visual reasoning with significantly fewer training samples, relying purely on self-improvement with no knowledge distillation. Our key insight is that the difficulty of training data during reinforcement fine-tuning (RFT) is critical. Appropriately challenging samples can substantially boost reasoning capabilities even when the dataset is small. Despite being intuitive, the main challenge remains in accurately quantifying sample difficulty to enable effective data filtering. To this end, we propose a novel way of repurposing Monte Carlo Tree Search (MCTS) to achieve that. Starting from our curated 70k open-source training samples, we introduce an MCTS-based selection method that quantifies sample difficulty based on the number of iterations required by the VLMs to solve each problem. This explicit step-by-step reasoning in MCTS enforces the model to think longer and better identifies samples that are genuinely challenging. We filter and retain 11k samples to perform RFT on Qwen2.5-VL-7B-Instruct, resulting in our final model, ThinkLite-VL. Evaluation results on eight benchmarks show that ThinkLite-VL improves the average performance of Qwen2.5-VL-7B-Instruct by 7%, using only 11k training samples with no knowledge distillation. This significantly outperforms all existing 7B-level reasoning VLMs, and our fairly comparable baselines that use classic selection methods such as accuracy-based filtering. Notably, on MathVista, ThinkLite-VL-7B achieves the SoTA accuracy of 75.1, surpassing Qwen2.5-VL-72B, GPT-4o, and O1. Our code, data, and model are available at https://github.com/si0wang/ThinkLite-VL."
  },
  {
    "title": "Benchmarking Adversarial Robustness to Bias Elicitation in Large Language Models: Scalable Automated Assessment with LLM-as-a-Judge",
    "url": "http://arxiv.org/abs/2504.07887v1",
    "arxiv_id": "2504.07887v1",
    "authors": [
      "Riccardo Cantini",
      "Alessio Orsino",
      "Massimo Ruggiero",
      "Domenico Talia"
    ],
    "published": "2025-04-10T16:00:59+00:00",
    "summary": "Large Language Models (LLMs) have revolutionized artificial intelligence, driving advancements in machine translation, summarization, and conversational agents. However, their increasing integration into critical societal domains has raised concerns about embedded biases, which can perpetuate stereotypes and compromise fairness. These biases stem from various sources, including historical inequalities in training data, linguistic imbalances, and adversarial manipulation. Despite mitigation efforts, recent studies indicate that LLMs remain vulnerable to adversarial attacks designed to elicit biased responses. This work proposes a scalable benchmarking framework to evaluate LLM robustness against adversarial bias elicitation. Our methodology involves (i) systematically probing models with a multi-task approach targeting biases across various sociocultural dimensions, (ii) quantifying robustness through safety scores using an LLM-as-a-Judge approach for automated assessment of model responses, and (iii) employing jailbreak techniques to investigate vulnerabilities in safety mechanisms. Our analysis examines prevalent biases in both small and large state-of-the-art models and their impact on model safety. Additionally, we assess the safety of domain-specific models fine-tuned for critical fields, such as medicine. Finally, we release a curated dataset of bias-related prompts, CLEAR-Bias, to facilitate systematic vulnerability benchmarking. Our findings reveal critical trade-offs between model size and safety, aiding the development of fairer and more robust future language models."
  },
  {
    "title": "Gauge and parametrization dependence of Quantum Einstein Gravity within the Proper Time flow",
    "url": "http://arxiv.org/abs/2504.07877v1",
    "arxiv_id": "2504.07877v1",
    "authors": [
      "Alfio Bonanno",
      "Giovanni Oglialoro",
      "Dario Zappal\u00e0"
    ],
    "published": "2025-04-10T15:52:31+00:00",
    "summary": "Proper time functional flow equations have garnered significant attention in recent years, as they are particularly suitable in analyzing non-perturbative contexts. By resorting to this flow, we investigate the regulator and gauge dependence in quantum Einstein gravity within the asymptotic safety framework, considering various regularization schemes. Our findings indicate that some details of the regulator have minor influence on the critical properties of the theory. In contrast, the selection between linear and exponential parametrizations appears to have a more substantial impact on the scaling behavior of the renormalized flow near the non-Gaussian fixed point."
  },
  {
    "title": "Pangu Ultra: Pushing the Limits of Dense Large Language Models on Ascend NPUs",
    "url": "http://arxiv.org/abs/2504.07866v1",
    "arxiv_id": "2504.07866v1",
    "authors": [
      "Yichun Yin",
      "Wenyong Huang",
      "Kaikai Song",
      "Yehui Tang",
      "Xueyu Wu",
      "Wei Guo",
      "Peng Guo",
      "Yaoyuan Wang",
      "Xiaojun Meng",
      "Yasheng Wang",
      "Dong Li",
      "Can Chen",
      "Dandan Tu",
      "Yin Li",
      "Fisher Yu",
      "Ruiming Tang",
      "Yunhe Wang",
      "Baojun Wang",
      "Bin Wang",
      "Bo Wang",
      "Boxiao Liu",
      "Changzheng Zhang",
      "Duyu Tang",
      "Fei Mi",
      "Hui Jin",
      "Jiansheng Wei",
      "Jiarui Qin",
      "Jinpeng Li",
      "Jun Zhao",
      "Liqun Deng",
      "Lin Li",
      "Minghui Xu",
      "Naifu Zhang",
      "Nianzu Zheng",
      "Qiang Li",
      "Rongju Ruan",
      "Shengjun Cheng",
      "Tianyu Guo",
      "Wei He",
      "Wei Li",
      "Weiwen Liu",
      "Wulong Liu",
      "Xinyi Dai",
      "Yonghan Dong",
      "Yu Pan",
      "Yue Li",
      "Yufei Wang",
      "Yujun Li",
      "Yunsheng Ni",
      "Zhe Liu",
      "Zhenhe Zhang",
      "Zhicheng Liu"
    ],
    "published": "2025-04-10T15:41:51+00:00",
    "summary": "We present Pangu Ultra, a Large Language Model (LLM) with 135 billion parameters and dense Transformer modules trained on Ascend Neural Processing Units (NPUs). Although the field of LLM has been witnessing unprecedented advances in pushing the scale and capability of LLM in recent years, training such a large-scale model still involves significant optimization and system challenges. To stabilize the training process, we propose depth-scaled sandwich normalization, which effectively eliminates loss spikes during the training process of deep models. We pre-train our model on 13.2 trillion diverse and high-quality tokens and further enhance its reasoning capabilities during post-training. To perform such large-scale training efficiently, we utilize 8,192 Ascend NPUs with a series of system optimizations. Evaluations on multiple diverse benchmarks indicate that Pangu Ultra significantly advances the state-of-the-art capabilities of dense LLMs such as Llama 405B and Mistral Large 2, and even achieves competitive results with DeepSeek-R1, whose sparse model structure contains much more parameters. Our exploration demonstrates that Ascend NPUs are capable of efficiently and effectively training dense models with more than 100 billion parameters. Our model and system will be available for our commercial customers."
  },
  {
    "title": "Pangu Ultra: Pushing the Limits of Dense Large Language Models on Ascend NPUs",
    "url": "http://arxiv.org/abs/2504.07866v2",
    "arxiv_id": "2504.07866v2",
    "authors": [
      "Yichun Yin",
      "Wenyong Huang",
      "Kaikai Song",
      "Yehui Tang",
      "Xueyu Wu",
      "Wei Guo",
      "Peng Guo",
      "Yaoyuan Wang",
      "Xiaojun Meng",
      "Yasheng Wang",
      "Dong Li",
      "Can Chen",
      "Dandan Tu",
      "Yin Li",
      "Fisher Yu",
      "Ruiming Tang",
      "Yunhe Wang",
      "Baojun Wang",
      "Bin Wang",
      "Bo Wang",
      "Boxiao Liu",
      "Changzheng Zhang",
      "Duyu Tang",
      "Fei Mi",
      "Hui Jin",
      "Jiansheng Wei",
      "Jiarui Qin",
      "Jinpeng Li",
      "Jun Zhao",
      "Liqun Deng",
      "Lin Li",
      "Minghui Xu",
      "Naifu Zhang",
      "Nianzu Zheng",
      "Qiang Li",
      "Rongju Ruan",
      "Shengjun Cheng",
      "Tianyu Guo",
      "Wei He",
      "Wei Li",
      "Weiwen Liu",
      "Wulong Liu",
      "Xinyi Dai",
      "Yonghan Dong",
      "Yu Pan",
      "Yue Li",
      "Yufei Wang",
      "Yujun Li",
      "Yunsheng Ni",
      "Zhe Liu",
      "Zhenhe Zhang",
      "Zhicheng Liu"
    ],
    "published": "2025-04-10T15:41:51+00:00",
    "summary": "We present Pangu Ultra, a Large Language Model (LLM) with 135 billion parameters and dense Transformer modules trained on Ascend Neural Processing Units (NPUs). Although the field of LLM has been witnessing unprecedented advances in pushing the scale and capability of LLM in recent years, training such a large-scale model still involves significant optimization and system challenges. To stabilize the training process, we propose depth-scaled sandwich normalization, which effectively eliminates loss spikes during the training process of deep models. We pre-train our model on 13.2 trillion diverse and high-quality tokens and further enhance its reasoning capabilities during post-training. To perform such large-scale training efficiently, we utilize 8,192 Ascend NPUs with a series of system optimizations. Evaluations on multiple diverse benchmarks indicate that Pangu Ultra significantly advances the state-of-the-art capabilities of dense LLMs such as Llama 405B and Mistral Large 2, and even achieves competitive results with DeepSeek-R1, whose sparse model structure contains much more parameters. Our exploration demonstrates that Ascend NPUs are capable of efficiently and effectively training dense models with more than 100 billion parameters. Our model and system will be available for our commercial customers."
  },
  {
    "title": "Robust Hallucination Detection in LLMs via Adaptive Token Selection",
    "url": "http://arxiv.org/abs/2504.07863v1",
    "arxiv_id": "2504.07863v1",
    "authors": [
      "Mengjia Niu",
      "Hamed Haddadi",
      "Guansong Pang"
    ],
    "published": "2025-04-10T15:39:10+00:00",
    "summary": "Hallucinations in large language models (LLMs) pose significant safety concerns that impede their broader deployment. Recent research in hallucination detection has demonstrated that LLMs' internal representations contain truthfulness hints, which can be harnessed for detector training. However, the performance of these detectors is heavily dependent on the internal representations of predetermined tokens, fluctuating considerably when working on free-form generations with varying lengths and sparse distributions of hallucinated entities. To address this, we propose HaMI, a novel approach that enables robust detection of hallucinations through adaptive selection and learning of critical tokens that are most indicative of hallucinations. We achieve this robustness by an innovative formulation of the Hallucination detection task as Multiple Instance (HaMI) learning over token-level representations within a sequence, thereby facilitating a joint optimisation of token selection and hallucination detection on generation sequences of diverse forms. Comprehensive experimental results on four hallucination benchmarks show that HaMI significantly outperforms existing state-of-the-art approaches."
  },
  {
    "title": "Deceptive Automated Interpretability: Language Models Coordinating to Fool Oversight Systems",
    "url": "http://arxiv.org/abs/2504.07831v1",
    "arxiv_id": "2504.07831v1",
    "authors": [
      "Simon Lermen",
      "Mateusz Dziemian",
      "Natalia P\u00e9rez-Campanero Antol\u00edn"
    ],
    "published": "2025-04-10T15:07:10+00:00",
    "summary": "We demonstrate how AI agents can coordinate to deceive oversight systems using automated interpretability of neural networks. Using sparse autoencoders (SAEs) as our experimental framework, we show that language models (Llama, DeepSeek R1, and Claude 3.7 Sonnet) can generate deceptive explanations that evade detection. Our agents employ steganographic methods to hide information in seemingly innocent explanations, successfully fooling oversight models while achieving explanation quality comparable to reference labels. We further find that models can scheme to develop deceptive strategies when they believe the detection of harmful features might lead to negative consequences for themselves. All tested LLM agents were capable of deceiving the overseer while achieving high interpretability scores comparable to those of reference labels. We conclude by proposing mitigation strategies, emphasizing the critical need for robust understanding and defenses against deception."
  },
  {
    "title": "Revisiting Likelihood-Based Out-of-Distribution Detection by Modeling Representations",
    "url": "http://arxiv.org/abs/2504.07793v1",
    "arxiv_id": "2504.07793v1",
    "authors": [
      "Yifan Ding",
      "Arturas Aleksandrauskas",
      "Amirhossein Ahmadian",
      "Jonas Unger",
      "Fredrik Lindsten",
      "Gabriel Eilertsen"
    ],
    "published": "2025-04-10T14:30:41+00:00",
    "summary": "Out-of-distribution (OOD) detection is critical for ensuring the reliability of deep learning systems, particularly in safety-critical applications. Likelihood-based deep generative models have historically faced criticism for their unsatisfactory performance in OOD detection, often assigning higher likelihood to OOD data than in-distribution samples when applied to image data. In this work, we demonstrate that likelihood is not inherently flawed. Rather, several properties in the images space prohibit likelihood as a valid detection score. Given a sufficiently good likelihood estimator, specifically using the probability flow formulation of a diffusion model, we show that likelihood-based methods can still perform on par with state-of-the-art methods when applied in the representation space of pre-trained encoders. The code of our work can be found at $\\href{https://github.com/limchaos/Likelihood-OOD.git}{\\texttt{https://github.com/limchaos/Likelihood-OOD.git}}$."
  },
  {
    "title": "Realigning Incentives to Build Better Software: a Holistic Approach to Vendor Accountability",
    "url": "http://arxiv.org/abs/2504.07766v1",
    "arxiv_id": "2504.07766v1",
    "authors": [
      "Gergely Bicz\u00f3k",
      "Sasha Romanosky",
      "Mingyan Liu"
    ],
    "published": "2025-04-10T14:05:24+00:00",
    "summary": "In this paper, we ask the question of why the quality of commercial software, in terms of security and safety, does not measure up to that of other (durable) consumer goods we have come to expect. We examine this question through the lens of incentives. We argue that the challenge around better quality software is due in no small part to a sequence of misaligned incentives, the most critical of which being that the harm caused by software problems is by and large shouldered by consumers, not developers. This lack of liability means software vendors have every incentive to rush low-quality software onto the market and no incentive to enhance quality control. Within this context, this paper outlines a holistic technical and policy framework we believe is needed to incentivize better and more secure software development. At the heart of the incentive realignment is the concept of software liability. This framework touches on various components, including legal, technical, and financial, that are needed for software liability to work in practice; some currently exist, some will need to be re-imagined or established. This is primarily a market-driven approach that emphasizes voluntary participation but highlights the role appropriate regulation can play. We connect and contrast this with the EU legal environment and discuss what this framework means for open-source software (OSS) development and emerging AI risks. Moreover, we present a CrowdStrike case study complete with a what-if analysis had our proposed framework been in effect. Our intention is very much to stimulate a robust conversation among both researchers and practitioners."
  },
  {
    "title": "Optimal Frequency Support from Virtual Power Plants: Minimal Reserve and Allocation",
    "url": "http://arxiv.org/abs/2504.07703v1",
    "arxiv_id": "2504.07703v1",
    "authors": [
      "Xiang Zhu",
      "Guangchun Ruan",
      "Hua Geng"
    ],
    "published": "2025-04-10T12:43:38+00:00",
    "summary": "This paper proposes a novel reserve-minimizing and allocation strategy for virtual power plants (VPPs) to deliver optimal frequency support. The proposed strategy enables VPPs, acting as aggregators for inverter-based resources (IBRs), to provide optimal frequency support economically. The proposed strategy captures time-varying active power injections, reducing the unnecessary redundancy compared to traditional fixed reserve schemes. Reserve requirements for the VPPs are determined based on system frequency response and safety constraints, ensuring efficient grid support. Furthermore, an energy-based allocation model decomposes power injections for each IBR, accounting for their specific limitations. Numerical experiments validate the feasibility of the proposed approach, highlighting significant financial gains for VPPs, especially as system inertia decreases due to higher renewable energy integration."
  },
  {
    "title": "Joint Travel Route Optimization Framework for Platooning",
    "url": "http://arxiv.org/abs/2504.07623v1",
    "arxiv_id": "2504.07623v1",
    "authors": [
      "Akif Adas",
      "Stefano Arrigoni",
      "Mattia Brambilla",
      "Monica Barbara Nicoli",
      "Edoardo Sabbioni"
    ],
    "published": "2025-04-10T10:13:20+00:00",
    "summary": "Platooning represents an advanced driving technology designed to assist drivers in traffic convoys of varying lengths, enhancing road safety, reducing driver fatigue, and improving fuel efficiency. Sophisticated automated driving assistance systems have facilitated this innovation. Recent advancements in platooning emphasize cooperative mechanisms within both centralized and decentralized architectures enabled by vehicular communication technologies. This study introduces a cooperative route planning optimization framework aimed at promoting the adoption of platooning through a centralized platoon formation strategy at the system level. This approach is envisioned as a transitional phase from individual (ego) driving to fully collaborative driving. Additionally, this research formulates and incorporates travel cost metrics related to fuel consumption, driver fatigue, and travel time, considering regulatory constraints on consecutive driving durations. The performance of these cost metrics has been evaluated using Dijkstra's and A* shortest path algorithms within a network graph framework. The results indicate that the proposed architecture achieves an average cost improvement of 14 % compared to individual route planning for long road trips."
  },
  {
    "title": "VLM-R1: A Stable and Generalizable R1-style Large Vision-Language Model",
    "url": "http://arxiv.org/abs/2504.07615v1",
    "arxiv_id": "2504.07615v1",
    "authors": [
      "Haozhan Shen",
      "Peng Liu",
      "Jingcheng Li",
      "Chunxin Fang",
      "Yibo Ma",
      "Jiajia Liao",
      "Qiaoli Shen",
      "Zilun Zhang",
      "Kangjia Zhao",
      "Qianqian Zhang",
      "Ruochen Xu",
      "Tiancheng Zhao"
    ],
    "published": "2025-04-10T10:05:15+00:00",
    "summary": "Recently DeepSeek R1 has shown that reinforcement learning (RL) can substantially improve the reasoning capabilities of Large Language Models (LLMs) through a simple yet effective design. The core of R1 lies in its rule-based reward formulation, which leverages tasks with deterministic ground-truth answers to enable precise and stable reward computation. In the visual domain, we similarly observe that a wide range of visual understanding tasks are inherently equipped with well-defined ground-truth annotations. This property makes them naturally compatible with rule-based reward mechanisms. Motivated by this observation, we investigate the extension of R1-style reinforcement learning to Vision-Language Models (VLMs), aiming to enhance their visual reasoning capabilities. To this end, we develop VLM-R1, a dedicated framework designed to harness RL for improving VLMs' performance on general vision-language tasks. Using this framework, we further explore the feasibility of applying RL to visual domain. Experimental results indicate that the RL-based model not only delivers competitive performance on visual understanding tasks but also surpasses Supervised Fine-Tuning (SFT) in generalization ability. Furthermore, we conduct comprehensive ablation studies that uncover a series of noteworthy insights, including the presence of reward hacking in object detection, the emergence of the \"OD aha moment\", the impact of training data quality, and the scaling behavior of RL across different model sizes. Through these analyses, we aim to deepen the understanding of how reinforcement learning enhances the capabilities of vision-language models, and we hope our findings and open-source contributions will support continued progress in the vision-language RL community. Our code and model are available at https://github.com/om-ai-lab/VLM-R1"
  },
  {
    "title": "VLM-R1: A Stable and Generalizable R1-style Large Vision-Language Model",
    "url": "http://arxiv.org/abs/2504.07615v2",
    "arxiv_id": "2504.07615v2",
    "authors": [
      "Haozhan Shen",
      "Peng Liu",
      "Jingcheng Li",
      "Chunxin Fang",
      "Yibo Ma",
      "Jiajia Liao",
      "Qiaoli Shen",
      "Zilun Zhang",
      "Kangjia Zhao",
      "Qianqian Zhang",
      "Ruochen Xu",
      "Tiancheng Zhao"
    ],
    "published": "2025-04-10T10:05:15+00:00",
    "summary": "Recently DeepSeek R1 has shown that reinforcement learning (RL) can substantially improve the reasoning capabilities of Large Language Models (LLMs) through a simple yet effective design. The core of R1 lies in its rule-based reward formulation, which leverages tasks with deterministic ground-truth answers to enable precise and stable reward computation. In the visual domain, we similarly observe that a wide range of visual understanding tasks are inherently equipped with well-defined ground-truth annotations. This property makes them naturally compatible with rule-based reward mechanisms. Motivated by this observation, we investigate the extension of R1-style reinforcement learning to Vision-Language Models (VLMs), aiming to enhance their visual reasoning capabilities. To this end, we develop VLM-R1, a dedicated framework designed to harness RL for improving VLMs' performance on general vision-language tasks. Using this framework, we further explore the feasibility of applying RL to visual domain. Experimental results indicate that the RL-based model not only delivers competitive performance on visual understanding tasks but also surpasses Supervised Fine-Tuning (SFT) in generalization ability. Furthermore, we conduct comprehensive ablation studies that uncover a series of noteworthy insights, including the presence of reward hacking in object detection, the emergence of the \"OD aha moment\", the impact of training data quality, and the scaling behavior of RL across different model sizes. Through these analyses, we aim to deepen the understanding of how reinforcement learning enhances the capabilities of vision-language models, and we hope our findings and open-source contributions will support continued progress in the vision-language RL community. Our code and model are available at https://github.com/om-ai-lab/VLM-R1"
  },
  {
    "title": "Drive in Corridors: Enhancing the Safety of End-to-end Autonomous Driving via Corridor Learning and Planning",
    "url": "http://arxiv.org/abs/2504.07507v1",
    "arxiv_id": "2504.07507v1",
    "authors": [
      "Zhiwei Zhang",
      "Ruichen Yang",
      "Ke Wu",
      "Zijun Xu",
      "Jingchu Liu",
      "Lisen Mu",
      "Zhongxue Gan",
      "Wenchao Ding"
    ],
    "published": "2025-04-10T07:10:40+00:00",
    "summary": "Safety remains one of the most critical challenges in autonomous driving systems. In recent years, the end-to-end driving has shown great promise in advancing vehicle autonomy in a scalable manner. However, existing approaches often face safety risks due to the lack of explicit behavior constraints. To address this issue, we uncover a new paradigm by introducing the corridor as the intermediate representation. Widely adopted in robotics planning, the corridors represents spatio-temporal obstacle-free zones for the vehicle to traverse. To ensure accurate corridor prediction in diverse traffic scenarios, we develop a comprehensive learning pipeline including data annotation, architecture refinement and loss formulation. The predicted corridor is further integrated as the constraint in a trajectory optimization process. By extending the differentiability of the optimization, we enable the optimized trajectory to be seamlessly trained within the end-to-end learning framework, improving both safety and interpretability. Experimental results on the nuScenes dataset demonstrate state-of-the-art performance of our approach, showing a 66.7% reduction in collisions with agents and a 46.5% reduction with curbs, significantly enhancing the safety of end-to-end driving. Additionally, incorporating the corridor contributes to higher success rates in closed-loop evaluations."
  },
  {
    "title": "Defense against Prompt Injection Attacks via Mixture of Encodings",
    "url": "http://arxiv.org/abs/2504.07467v1",
    "arxiv_id": "2504.07467v1",
    "authors": [
      "Ruiyi Zhang",
      "David Sullivan",
      "Kyle Jackson",
      "Pengtao Xie",
      "Mei Chen"
    ],
    "published": "2025-04-10T05:35:21+00:00",
    "summary": "Large Language Models (LLMs) have emerged as a dominant approach for a wide range of NLP tasks, with their access to external information further enhancing their capabilities. However, this introduces new vulnerabilities, known as prompt injection attacks, where external content embeds malicious instructions that manipulate the LLM's output. Recently, the Base64 defense has been recognized as one of the most effective methods for reducing success rate of prompt injection attacks. Despite its efficacy, this method can degrade LLM performance on certain NLP tasks. To address this challenge, we propose a novel defense mechanism: mixture of encodings, which utilizes multiple character encodings, including Base64. Extensive experimental results show that our method achieves one of the lowest attack success rates under prompt injection attacks, while maintaining high performance across all NLP tasks, outperforming existing character encoding-based defense methods. This underscores the effectiveness of our mixture of encodings strategy for both safety and task performance metrics."
  },
  {
    "title": "Multi-Modal Data Fusion for Moisture Content Prediction in Apple Drying",
    "url": "http://arxiv.org/abs/2504.07465v1",
    "arxiv_id": "2504.07465v1",
    "authors": [
      "Shichen Li",
      "Chenhui Shao"
    ],
    "published": "2025-04-10T05:29:04+00:00",
    "summary": "Fruit drying is widely used in food manufacturing to reduce product moisture, ensure product safety, and extend product shelf life. Accurately predicting final moisture content (MC) is critically needed for quality control of drying processes. State-of-the-art methods can build deterministic relationships between process parameters and MC, but cannot adequately account for inherent process variabilities that are ubiquitous in fruit drying. To address this gap, this paper presents a novel multi-modal data fusion framework to effectively fuse two modalities of data: tabular data (process parameters) and high-dimensional image data (images of dried apple slices) to enable accurate MC prediction. The proposed modeling architecture permits flexible adjustment of information portion from tabular and image data modalities. Experimental validation shows that the multi-modal approach improves predictive accuracy substantially compared to state-of-the-art methods. The proposed method reduces root-mean-squared errors by 19.3%, 24.2%, and 15.2% over tabular-only, image-only, and standard tabular-image fusion models, respectively. Furthermore, it is demonstrated that our method is robust in varied tabular-image ratios and capable of effectively capturing inherent small-scale process variabilities. The proposed framework is extensible to a variety of other drying technologies."
  },
  {
    "title": "LoRI: Reducing Cross-Task Interference in Multi-Task Low-Rank Adaptation",
    "url": "http://arxiv.org/abs/2504.07448v1",
    "arxiv_id": "2504.07448v1",
    "authors": [
      "Juzheng Zhang",
      "Jiacheng You",
      "Ashwinee Panda",
      "Tom Goldstein"
    ],
    "published": "2025-04-10T04:46:04+00:00",
    "summary": "Low-Rank Adaptation (LoRA) has emerged as a popular parameter-efficient fine-tuning (PEFT) method for Large Language Models (LLMs), yet it still incurs notable overhead and suffers from parameter interference in multi-task scenarios. We propose LoRA with Reduced Interference (LoRI), a simple yet effective approach that freezes the projection matrices $A$ as random projections and sparsifies the matrices $B$ using task-specific masks. This design substantially reduces the number of trainable parameters while maintaining strong task performance. Moreover, LoRI minimizes cross-task interference in adapter merging by leveraging the orthogonality between adapter subspaces, and supports continual learning by using sparsity to mitigate catastrophic forgetting. Extensive experiments across natural language understanding, mathematical reasoning, code generation, and safety alignment tasks demonstrate that LoRI outperforms full fine-tuning and existing PEFT methods, while using up to 95% fewer trainable parameters than LoRA. In multi-task experiments, LoRI enables effective adapter merging and continual learning with reduced cross-task interference. Code is available at: https://github.com/juzhengz/LoRI"
  },
  {
    "title": "Electronic Warfare Cyberattacks, Countermeasures and Modern Defensive Strategies of UAV Avionics: A Survey",
    "url": "http://arxiv.org/abs/2504.07358v1",
    "arxiv_id": "2504.07358v1",
    "authors": [
      "Aaron Yu",
      "Iuliia Kolotylo",
      "Hashim A. Hashim",
      "A. E. E. Eltoukhy"
    ],
    "published": "2025-04-10T00:56:52+00:00",
    "summary": "Unmanned Aerial Vehicles (UAVs) play a pivotal role in modern autonomous air mobility, and the reliability of UAV avionics systems is critical to ensuring mission success, sustainability practices, and public safety. The success of UAV missions depends on effectively mitigating various aspects of electronic warfare, including non-destructive and destructive cyberattacks, transponder vulnerabilities, and jamming threats, while rigorously implementing countermeasures and defensive aids. This paper provides a comprehensive review of UAV cyberattacks, countermeasures, and defensive strategies. It explores UAV-to-UAV coordination attacks and their associated features, such as dispatch system attacks, Automatic Dependent Surveillance-Broadcast (ADS-B) attacks, Traffic Alert and Collision Avoidance System (TCAS)-induced collisions, and TCAS attacks. Additionally, the paper examines UAV-to-command center coordination attacks, as well as UAV functionality attacks. The review also covers various countermeasures and defensive aids designed for UAVs. Lastly, a comparison of common cyberattacks and countermeasure approaches is conducted, along with a discussion of future trends in the field. Keywords: Electronic warfare, UAVs, Avionics Systems, cyberattacks, coordination attacks, functionality attacks, countermeasure, defensive-aids."
  },
  {
    "title": "Revisiting Prompt Optimization with Large Reasoning Models-A Case Study on Event Extraction",
    "url": "http://arxiv.org/abs/2504.07357v1",
    "arxiv_id": "2504.07357v1",
    "authors": [
      "Saurabh Srivastava",
      "Ziyu Yao"
    ],
    "published": "2025-04-10T00:53:59+00:00",
    "summary": "Large Reasoning Models (LRMs) such as DeepSeek-R1 and OpenAI o1 have demonstrated remarkable capabilities in various reasoning tasks. Their strong capability to generate and reason over intermediate thoughts has also led to arguments that they may no longer require extensive prompt engineering or optimization to interpret human instructions and produce accurate outputs. In this work, we aim to systematically study this open question, using the structured task of event extraction for a case study. We experimented with two LRMs (DeepSeek-R1 and o1) and two general-purpose Large Language Models (LLMs) (GPT-4o and GPT-4.5), when they were used as task models or prompt optimizers. Our results show that on tasks as complicated as event extraction, LRMs as task models still benefit from prompt optimization, and that using LRMs as prompt optimizers yields more effective prompts. Finally, we provide an error analysis of common errors made by LRMs and highlight the stability and consistency of LRMs in refining task instructions and event guidelines."
  },
  {
    "title": "Code Generation with Small Language Models: A Deep Evaluation on Codeforces",
    "url": "http://arxiv.org/abs/2504.07343v1",
    "arxiv_id": "2504.07343v1",
    "authors": [
      "D\u00e9bora Souza",
      "Rohit Gheyi",
      "Lucas Albuquerque",
      "Gustavo Soares",
      "M\u00e1rcio Ribeiro"
    ],
    "published": "2025-04-09T23:57:44+00:00",
    "summary": "Large Language Models (LLMs) have demonstrated capabilities in code generation, potentially boosting developer productivity. However, their widespread adoption remains limited by high computational costs, significant energy demands, and security risks such as data leakage and adversarial attacks. As a lighter-weight alternative, Small Language Models (SLMs) offer faster inference, lower deployment overhead, and better adaptability to domain-specific tasks, making them an attractive option for real-world applications. While prior research has benchmarked LLMs on competitive programming tasks, such evaluations often focus narrowly on metrics like Elo scores or pass rates, overlooking deeper insights into model behavior, failure patterns, and problem diversity. Furthermore, the potential of SLMs to tackle complex tasks such as competitive programming remains underexplored. In this study, we benchmark five open SLMs - LLAMA 3.2 3B, GEMMA 2 9B, GEMMA 3 12B, DEEPSEEK-R1 14B, and PHI-4 14B - across 280 Codeforces problems spanning Elo ratings from 800 to 2100 and covering 36 distinct topics. All models were tasked with generating Python solutions. PHI-4 14B achieved the best performance among SLMs, with a pass@3 of 63.6%, approaching the proprietary O3-MINI-HIGH (86.8%). In addition, we evaluated PHI-4 14B on C++ and found that combining outputs from both Python and C++ increases its aggregated pass@3 to 73.6%. A qualitative analysis of PHI-4 14B's incorrect outputs revealed that some failures were due to minor implementation issues - such as handling edge cases or correcting variable initialization - rather than deeper reasoning flaws."
  },
  {
    "title": "Agentic SLMs: Hunting Down Test Smells",
    "url": "http://arxiv.org/abs/2504.07277v1",
    "arxiv_id": "2504.07277v1",
    "authors": [
      "Rian Melo",
      "Pedro Sim\u00f5es",
      "Rohit Gheyi",
      "Marcelo d'Amorim",
      "M\u00e1rcio Ribeiro",
      "Gustavo Soares",
      "Eduardo Almeida",
      "Elvys Soares"
    ],
    "published": "2025-04-09T21:12:01+00:00",
    "summary": "Test smells can compromise the reliability of test suites and hinder software maintenance. Although several strategies exist for detecting test smells, few address their removal. Traditional methods often rely on static analysis or machine learning, requiring significant effort and expertise. This study evaluates LLAMA 3.2 3B, GEMMA 2 9B, DEEPSEEK-R1 14B, and PHI 4 14B - small, open language models - for automating the detection and refactoring of test smells through agent-based workflows. We explore workflows with one, two, and four agents across 150 instances of 5 common test smell types extracted from real-world Java projects. Unlike prior approaches, ours is easily extensible to new smells via natural language definitions and generalizes to Python and Golang. All models detected nearly all test smell instances (pass@5 of 96% with four agents), with PHI 4 14B achieving the highest refactoring accuracy (pass@5 of 75.3%). Analyses were computationally inexpensive and ran efficiently on a consumer-grade hardware. Notably, PHI 4 14B with four agents performed within 5% of proprietary models such as O1-MINI, O3-MINI-HIGH, and GEMINI 2.5 PRO EXPERIMENTAL using a single agent. Multi-agent setups outperformed single-agent ones in three out of five test smell types, highlighting their potential to improve software quality with minimal developer effort. For the Assertion Roulette smell, however, a single agent performed better. To assess practical relevance, we submitted 10 pull requests with PHI 4 14B - generated code to open-source projects. Five were merged, one was rejected, and four remain under review, demonstrating the approach's real-world applicability."
  },
  {
    "title": "Sculpting Subspaces: Constrained Full Fine-Tuning in LLMs for Continual Learning",
    "url": "http://arxiv.org/abs/2504.07097v1",
    "arxiv_id": "2504.07097v1",
    "authors": [
      "Nikhil Shivakumar Nayak",
      "Krishnateja Killamsetty",
      "Ligong Han",
      "Abhishek Bhandwaldar",
      "Prateek Chanda",
      "Kai Xu",
      "Hao Wang",
      "Aldo Pareja",
      "Oleg Silkin",
      "Mustafa Eyceoz",
      "Akash Srivastava"
    ],
    "published": "2025-04-09T17:59:42+00:00",
    "summary": "Continual learning in large language models (LLMs) is prone to catastrophic forgetting, where adapting to new tasks significantly degrades performance on previously learned ones. Existing methods typically rely on low-rank, parameter-efficient updates that limit the model's expressivity and introduce additional parameters per task, leading to scalability issues. To address these limitations, we propose a novel continual full fine-tuning approach leveraging adaptive singular value decomposition (SVD). Our method dynamically identifies task-specific low-rank parameter subspaces and constrains updates to be orthogonal to critical directions associated with prior tasks, thus effectively minimizing interference without additional parameter overhead or storing previous task gradients. We evaluate our approach extensively on standard continual learning benchmarks using both encoder-decoder (T5-Large) and decoder-only (LLaMA-2 7B) models, spanning diverse tasks including classification, generation, and reasoning. Empirically, our method achieves state-of-the-art results, up to 7% higher average accuracy than recent baselines like O-LoRA, and notably maintains the model's general linguistic capabilities, instruction-following accuracy, and safety throughout the continual learning process by reducing forgetting to near-negligible levels. Our adaptive SVD framework effectively balances model plasticity and knowledge retention, providing a practical, theoretically grounded, and computationally scalable solution for continual learning scenarios in large language models."
  },
  {
    "title": "OmniCaptioner: One Captioner to Rule Them All",
    "url": "http://arxiv.org/abs/2504.07089v1",
    "arxiv_id": "2504.07089v1",
    "authors": [
      "Yiting Lu",
      "Jiakang Yuan",
      "Zhen Li",
      "Shitian Zhao",
      "Qi Qin",
      "Xinyue Li",
      "Le Zhuo",
      "Licheng Wen",
      "Dongyang Liu",
      "Yuewen Cao",
      "Xiangchao Yan",
      "Xin Li",
      "Botian Shi",
      "Tao Chen",
      "Zhibo Chen",
      "Lei Bai",
      "Bo Zhang",
      "Peng Gao"
    ],
    "published": "2025-04-09T17:58:58+00:00",
    "summary": "We propose OmniCaptioner, a versatile visual captioning framework for generating fine-grained textual descriptions across a wide variety of visual domains. Unlike prior methods limited to specific image types (e.g., natural images or geometric visuals), our framework provides a unified solution for captioning natural images, visual text (e.g., posters, UIs, textbooks), and structured visuals (e.g., documents, tables, charts). By converting low-level pixel information into semantically rich textual representations, our framework bridges the gap between visual and textual modalities. Our results highlight three key advantages: (i) Enhanced Visual Reasoning with LLMs, where long-context captions of visual modalities empower LLMs, particularly the DeepSeek-R1 series, to reason effectively in multimodal scenarios; (ii) Improved Image Generation, where detailed captions improve tasks like text-to-image generation and image transformation; and (iii) Efficient Supervised Fine-Tuning (SFT), which enables faster convergence with less data. We believe the versatility and adaptability of OmniCaptioner can offer a new perspective for bridging the gap between language and visual modalities."
  },
  {
    "title": "R2E-Gym: Procedural Environments and Hybrid Verifiers for Scaling Open-Weights SWE Agents",
    "url": "http://arxiv.org/abs/2504.07164v1",
    "arxiv_id": "2504.07164v1",
    "authors": [
      "Naman Jain",
      "Jaskirat Singh",
      "Manish Shetty",
      "Liang Zheng",
      "Koushik Sen",
      "Ion Stoica"
    ],
    "published": "2025-04-09T17:55:19+00:00",
    "summary": "Improving open-source models on real-world SWE tasks (solving GITHUB issues) faces two key challenges: 1) scalable curation of execution environments to train these models, and, 2) optimal scaling of test-time compute. We introduce AgentGym, the largest procedurally-curated executable gym environment for training real-world SWE-agents, consisting of more than 8.7K tasks. AgentGym is powered by two main contributions: 1) SYNGEN: a synthetic data curation recipe that enables scalable curation of executable environments using test-generation and back-translation directly from commits, thereby reducing reliance on human-written issues or unit tests. We show that this enables more scalable training leading to pass@1 performance of 34.4% on SWE-Bench Verified benchmark with our 32B model. 2) Hybrid Test-time Scaling: we provide an in-depth analysis of two test-time scaling axes; execution-based and execution-free verifiers, demonstrating that they exhibit complementary strengths and limitations. Test-based verifiers suffer from low distinguishability, while execution-free verifiers are biased and often rely on stylistic features. Surprisingly, we find that while each approach individually saturates around 42-43%, significantly higher gains can be obtained by leveraging their complementary strengths. Overall, our approach achieves 51% on the SWE-Bench Verified benchmark, reflecting a new state-of-the-art for open-weight SWE-agents and for the first time showing competitive performance with proprietary models such as o1, o1-preview and sonnet-3.5-v2 (with tools). We will open-source our environments, models, and agent trajectories."
  },
  {
    "title": "Self-Steering Language Models",
    "url": "http://arxiv.org/abs/2504.07081v1",
    "arxiv_id": "2504.07081v1",
    "authors": [
      "Gabriel Grand",
      "Joshua B. Tenenbaum",
      "Vikash K. Mansinghka",
      "Alexander K. Lew",
      "Jacob Andreas"
    ],
    "published": "2025-04-09T17:54:22+00:00",
    "summary": "While test-time reasoning enables language models to tackle complex tasks, searching or planning in natural language can be slow, costly, and error-prone. But even when LMs struggle to emulate the precise reasoning steps needed to solve a problem, they often excel at describing its abstract structure--both how to verify solutions and how to search for them. This paper introduces DisCIPL, a method for \"self-steering\" LMs where a Planner model generates a task-specific inference program that is executed by a population of Follower models. Our approach equips LMs with the ability to write recursive search procedures that guide LM inference, enabling new forms of verifiable and efficient reasoning. When instantiated with a small Follower (e.g., Llama-3.2-1B), DisCIPL matches (and sometimes outperforms) much larger models, including GPT-4o and o1, on challenging constrained generation tasks. In decoupling planning from execution, our work opens up a design space of highly-parallelized Monte Carlo inference strategies that outperform standard best-of-N sampling, require no finetuning, and can be implemented automatically by existing LMs."
  },
  {
    "title": "Evaluating Retrieval Augmented Generative Models for Document Queries in Transportation Safety",
    "url": "http://arxiv.org/abs/2504.07022v1",
    "arxiv_id": "2504.07022v1",
    "authors": [
      "Chad Melton",
      "Alex Sorokine",
      "Steve Peterson"
    ],
    "published": "2025-04-09T16:37:03+00:00",
    "summary": "Applications of generative Large Language Models LLMs are rapidly expanding across various domains, promising significant improvements in workflow efficiency and information retrieval. However, their implementation in specialized, high-stakes domains such as hazardous materials transportation is challenging due to accuracy and reliability concerns. This study evaluates the performance of three fine-tuned generative models, ChatGPT, Google's Vertex AI, and ORNL Retrieval Augmented Generation augmented LLaMA 2 and LLaMA in retrieving regulatory information essential for hazardous material transportation compliance in the United States. Utilizing approximately 40 publicly available federal and state regulatory documents, we developed 100 realistic queries relevant to route planning and permitting requirements. Responses were qualitatively rated based on accuracy, detail, and relevance, complemented by quantitative assessments of semantic similarity between model outputs. Results demonstrated that the RAG-augmented LLaMA models significantly outperformed Vertex AI and ChatGPT, providing more detailed and generally accurate information, despite occasional inconsistencies. This research introduces the first known application of RAG in transportation safety, emphasizing the need for domain-specific fine-tuning and rigorous evaluation methodologies to ensure reliability and minimize the risk of inaccuracies in high-stakes environments."
  },
  {
    "title": "Quantum Field Theory on Multifractal Spacetime: Varying Dimension and Ultraviolet Completeness",
    "url": "http://arxiv.org/abs/2504.06797v1",
    "arxiv_id": "2504.06797v1",
    "authors": [
      "Alessio Maiezza",
      "Juan Carlos Vasquez"
    ],
    "published": "2025-04-09T11:41:15+00:00",
    "summary": "Inspired by various quantum gravity approaches, we explore quantum field theory where spacetime exhibits scaling properties and dimensional reduction with changing energy scales, effectively behaving as a multifractal manifold. Working within canonical quantization, we demonstrate how to properly quantize fields in such multifractal spacetime. Our analysis reveals that a non-differentiable nature of spacetime is not merely compatible with quantum field theory but significantly enhances its mathematical foundation. Most notably, this approach guarantees the finiteness of the theory at all orders in perturbation theory and enables rigorous construction of the S-matrix in the interaction picture. The multifractal structure tames dominant, large-order divergence sources in the perturbative series and resolves the Landau pole problem through asymptotic safety, substantially improving the theory's behavior in the deep ultraviolet regime. Our formulation preserves all established predictions of standard quantum field theory at low energies while offering novel physical behaviors at high energy scales."
  },
  {
    "title": "Domain-Specific Pruning of Large Mixture-of-Experts Models with Few-shot Demonstrations",
    "url": "http://arxiv.org/abs/2504.06792v1",
    "arxiv_id": "2504.06792v1",
    "authors": [
      "Zican Dong",
      "Han Peng",
      "Peiyu Liu",
      "Wayne Xin Zhao",
      "Dong Wu",
      "Feng Xiao",
      "Zhifeng Wang"
    ],
    "published": "2025-04-09T11:34:06+00:00",
    "summary": "Mixture-of-Experts (MoE) models achieve a favorable trade-off between performance and inference efficiency by activating only a subset of experts. However, the memory overhead of storing all experts remains a major limitation, especially in large-scale MoE models such as DeepSeek-R1 (671B). In this study, we investigate domain specialization and expert redundancy in large-scale MoE models and uncover a consistent behavior we term few-shot expert localization, with only a few demonstrations, the model consistently activates a sparse and stable subset of experts. Building on this observation, we propose a simple yet effective pruning framework, EASY-EP, that leverages a few domain-specific demonstrations to identify and retain only the most relevant experts. EASY-EP comprises two key components: output-aware expert importance assessment and expert-level token contribution estimation. The former evaluates the importance of each expert for the current token by considering the gating scores and magnitudes of the outputs of activated experts, while the latter assesses the contribution of tokens based on representation similarities after and before routed experts. Experiments show that our method can achieve comparable performances and $2.99\\times$ throughput under the same memory budget with full DeepSeek-R1 with only half the experts. Our code is available at https://github.com/RUCAIBox/EASYEP."
  },
  {
    "title": "Beware of \"Explanations\" of AI",
    "url": "http://arxiv.org/abs/2504.06791v1",
    "arxiv_id": "2504.06791v1",
    "authors": [
      "David Martens",
      "Galit Shmueli",
      "Theodoros Evgeniou",
      "Kevin Bauer",
      "Christian Janiesch",
      "Stefan Feuerriegel",
      "Sebastian Gabel",
      "Sofie Goethals",
      "Travis Greene",
      "Nadja Klein",
      "Mathias Kraus",
      "Niklas K\u00fchl",
      "Claudia Perlich",
      "Wouter Verbeke",
      "Alona Zharova",
      "Patrick Zschech",
      "Foster Provost"
    ],
    "published": "2025-04-09T11:31:08+00:00",
    "summary": "Understanding the decisions made and actions taken by increasingly complex AI system remains a key challenge. This has led to an expanding field of research in explainable artificial intelligence (XAI), highlighting the potential of explanations to enhance trust, support adoption, and meet regulatory standards. However, the question of what constitutes a \"good\" explanation is dependent on the goals, stakeholders, and context. At a high level, psychological insights such as the concept of mental model alignment can offer guidance, but success in practice is challenging due to social and technical factors. As a result of this ill-defined nature of the problem, explanations can be of poor quality (e.g. unfaithful, irrelevant, or incoherent), potentially leading to substantial risks. Instead of fostering trust and safety, poorly designed explanations can actually cause harm, including wrong decisions, privacy violations, manipulation, and even reduced AI adoption. Therefore, we caution stakeholders to beware of explanations of AI: while they can be vital, they are not automatically a remedy for transparency or responsible AI adoption, and their misuse or limitations can exacerbate harm. Attention to these caveats can help guide future research to improve the quality and impact of AI explanations."
  },
  {
    "title": "Holistic Capability Preservation: Towards Compact Yet Comprehensive Reasoning Models",
    "url": "http://arxiv.org/abs/2504.07158v1",
    "arxiv_id": "2504.07158v1",
    "authors": [
      "Ling Team",
      "Caizhi Tang",
      "Chilin Fu",
      "Chunwei Wu",
      "Jia Guo",
      "Jianwen Wang",
      "Jingyu Hu",
      "Liang Jiang",
      "Meng Li",
      "Peng Jiao",
      "Pingping Liu",
      "Shaomian Zheng",
      "Shiwei Liang",
      "Shuaicheng Li",
      "Yalin Zhang",
      "Yingting Wu",
      "Yongkang Liu",
      "Zhenyu Huang"
    ],
    "published": "2025-04-09T11:24:32+00:00",
    "summary": "This technical report presents Ring-Lite-Distill, a lightweight reasoning model derived from our open-source Mixture-of-Experts (MoE) Large Language Models (LLMs) Ling-Lite. This study demonstrates that through meticulous high-quality data curation and ingenious training paradigms, the compact MoE model Ling-Lite can be further trained to achieve exceptional reasoning capabilities, while maintaining its parameter-efficient architecture with only 2.75 billion activated parameters, establishing an efficient lightweight reasoning architecture. In particular, in constructing this model, we have not merely focused on enhancing advanced reasoning capabilities, exemplified by high-difficulty mathematical problem solving, but rather aimed to develop a reasoning model with more comprehensive competency coverage. Our approach ensures coverage across reasoning tasks of varying difficulty levels while preserving generic capabilities, such as instruction following, tool use, and knowledge retention. We show that, Ring-Lite-Distill's reasoning ability reaches a level comparable to DeepSeek-R1-Distill-Qwen-7B, while its general capabilities significantly surpass those of DeepSeek-R1-Distill-Qwen-7B. The models are accessible at https://huggingface.co/inclusionAI"
  },
  {
    "title": "Holistic Capability Preservation: Towards Compact Yet Comprehensive Reasoning Models",
    "url": "http://arxiv.org/abs/2504.07158v2",
    "arxiv_id": "2504.07158v2",
    "authors": [
      "Ling Team",
      "Caizhi Tang",
      "Chilin Fu",
      "Chunwei Wu",
      "Jia Guo",
      "Jianwen Wang",
      "Jingyu Hu",
      "Liang Jiang",
      "Meng Li",
      "Peng Jiao",
      "Pingping Liu",
      "Shaomian Zheng",
      "Shiwei Liang",
      "Shuaicheng Li",
      "Yalin Zhang",
      "Yingting Wu",
      "Yongkang Liu",
      "Zhenyu Huang"
    ],
    "published": "2025-04-09T11:24:32+00:00",
    "summary": "This technical report presents Ring-Lite-Distill, a lightweight reasoning model derived from our open-source Mixture-of-Experts (MoE) Large Language Models (LLMs) Ling-Lite. This study demonstrates that through meticulous high-quality data curation and ingenious training paradigms, the compact MoE model Ling-Lite can be further trained to achieve exceptional reasoning capabilities, while maintaining its parameter-efficient architecture with only 2.75 billion activated parameters, establishing an efficient lightweight reasoning architecture. In particular, in constructing this model, we have not merely focused on enhancing advanced reasoning capabilities, exemplified by high-difficulty mathematical problem solving, but rather aimed to develop a reasoning model with more comprehensive competency coverage. Our approach ensures coverage across reasoning tasks of varying difficulty levels while preserving generic capabilities, such as instruction following, tool use, and knowledge retention. We show that, Ring-Lite-Distill's reasoning ability reaches a level comparable to DeepSeek-R1-Distill-Qwen-7B, while its general capabilities significantly surpass those of DeepSeek-R1-Distill-Qwen-7B. The models are accessible at https://huggingface.co/inclusionAI"
  },
  {
    "title": "Zero-Shot Image-Based Large Language Model Approach to Road Pavement Monitoring",
    "url": "http://arxiv.org/abs/2504.06785v1",
    "arxiv_id": "2504.06785v1",
    "authors": [
      "Shuoshuo Xu",
      "Kai Zhao",
      "James Loney",
      "Zili Li",
      "Andrea Visentin"
    ],
    "published": "2025-04-09T11:19:17+00:00",
    "summary": "Effective and rapid evaluation of pavement surface condition is critical for prioritizing maintenance, ensuring transportation safety, and minimizing vehicle wear and tear. While conventional manual inspections suffer from subjectivity, existing machine learning-based methods are constrained by their reliance on large and high-quality labeled datasets, which require significant resources and limit adaptability across varied road conditions. The revolutionary advancements in Large Language Models (LLMs) present significant potential for overcoming these challenges. In this study, we propose an innovative automated zero-shot learning approach that leverages the image recognition and natural language understanding capabilities of LLMs to assess road conditions effectively. Multiple LLM-based assessment models were developed, employing prompt engineering strategies aligned with the Pavement Surface Condition Index (PSCI) standards. These models' accuracy and reliability were evaluated against official PSCI results, with an optimized model ultimately selected. Extensive tests benchmarked the optimized model against evaluations from various levels experts using Google Street View road images. The results reveal that the LLM-based approach can effectively assess road conditions, with the optimized model -employing comprehensive and structured prompt engineering strategies -outperforming simpler configurations by achieving high accuracy and consistency, even surpassing expert evaluations. Moreover, successfully applying the optimized model to Google Street View images demonstrates its potential for future city-scale deployments. These findings highlight the transformative potential of LLMs in automating road damage evaluations and underscore the pivotal role of detailed prompt engineering in achieving reliable assessments."
  },
  {
    "title": "Dynamic Residual Safe Reinforcement Learning for Multi-Agent Safety-Critical Scenarios Decision-Making",
    "url": "http://arxiv.org/abs/2504.06670v1",
    "arxiv_id": "2504.06670v1",
    "authors": [
      "Kaifeng Wang",
      "Yinsong Chen",
      "Qi Liu",
      "Xueyuan Li",
      "Xin Gao"
    ],
    "published": "2025-04-09T08:13:14+00:00",
    "summary": "In multi-agent safety-critical scenarios, traditional autonomous driving frameworks face significant challenges in balancing safety constraints and task performance. These frameworks struggle to quantify dynamic interaction risks in real-time and depend heavily on manual rules, resulting in low computational efficiency and conservative strategies. To address these limitations, we propose a Dynamic Residual Safe Reinforcement Learning (DRS-RL) framework grounded in a safety-enhanced networked Markov decision process. It's the first time that the weak-to-strong theory is introduced into multi-agent decision-making, enabling lightweight dynamic calibration of safety boundaries via a weak-to-strong safety correction paradigm. Based on the multi-agent dynamic conflict zone model, our framework accurately captures spatiotemporal coupling risks among heterogeneous traffic participants and surpasses the static constraints of conventional geometric rules. Moreover, a risk-aware prioritized experience replay mechanism mitigates data distribution bias by mapping risk to sampling probability. Experimental results reveal that the proposed method significantly outperforms traditional RL algorithms in safety, efficiency, and comfort. Specifically, it reduces the collision rate by up to 92.17%, while the safety model accounts for merely 27% of the main model's parameters."
  },
  {
    "title": "Toward Holistic Evaluation of Recommender Systems Powered by Generative Models",
    "url": "http://arxiv.org/abs/2504.06667v1",
    "arxiv_id": "2504.06667v1",
    "authors": [
      "Yashar Deldjoo",
      "Nikhil Mehta",
      "Maheswaran Sathiamoorthy",
      "Shuai Zhang",
      "Pablo Castells",
      "Julian McAuley"
    ],
    "published": "2025-04-09T08:08:16+00:00",
    "summary": "Recommender systems powered by generative models (Gen-RecSys) extend beyond classical item ranking by producing open-ended content, which simultaneously unlocks richer user experiences and introduces new risks. On one hand, these systems can enhance personalization and appeal through dynamic explanations and multi-turn dialogues. On the other hand, they might venture into unknown territory-hallucinating nonexistent items, amplifying bias, or leaking private information. Traditional accuracy metrics cannot fully capture these challenges, as they fail to measure factual correctness, content safety, or alignment with user intent.   This paper makes two main contributions. First, we categorize the evaluation challenges of Gen-RecSys into two groups: (i) existing concerns that are exacerbated by generative outputs (e.g., bias, privacy) and (ii) entirely new risks (e.g., item hallucinations, contradictory explanations). Second, we propose a holistic evaluation approach that includes scenario-based assessments and multi-metric checks-incorporating relevance, factual grounding, bias detection, and policy compliance. Our goal is to provide a guiding framework so researchers and practitioners can thoroughly assess Gen-RecSys, ensuring effective personalization and responsible deployment."
  },
  {
    "title": "Uni-PrevPredMap: Extending PrevPredMap to a Unified Framework of Prior-Informed Modeling for Online Vectorized HD Map Construction",
    "url": "http://arxiv.org/abs/2504.06647v2",
    "arxiv_id": "2504.06647v2",
    "authors": [
      "Nan Peng",
      "Xun Zhou",
      "Mingming Wang",
      "Guisong Chen",
      "Wenqi Xu"
    ],
    "published": "2025-04-09T07:36:17+00:00",
    "summary": "Safety constitutes a foundational imperative for autonomous driving systems, necessitating the maximal incorporation of accessible external prior information. This study establishes that temporal perception buffers and cost-efficient maps inherently form complementary prior sources for online vectorized high-definition (HD) map construction. We present Uni-PrevPredMap, a unified prior-informed framework that systematically integrates two synergistic information sources: previous predictions and simulated outdated HD maps. The framework introduces two core innovations: a tile-indexed 3D vectorized global map processor enabling efficient refreshment, storage, and retrieval of 3D vectorized priors; a tri-mode operational optimization paradigm ensuring consistency across non-prior, temporal-prior, and temporal-map-fusion-prior scenarios while mitigating reliance on idealized map fidelity assumptions. Uni-PrevPredMap achieves state-of-the-art performance in map-absent scenarios across established online vectorized HD map construction benchmarks. When provided with simulated outdated HD maps, the framework exhibits robust capabilities in error-resilient prior fusion, empirically confirming the synergistic complementarity between previous predictions and simulated outdated HD maps. Code will be available at https://github.com/pnnnnnnn/Uni-PrevPredMap."
  },
  {
    "title": "Harmful information spreading and its impact on vaccination campaigns modeled through fractal-fractional operators",
    "url": "http://arxiv.org/abs/2504.06582v1",
    "arxiv_id": "2504.06582v1",
    "authors": [
      "Ali Akg\u00fcl",
      "Auwalu Hamisu Usman",
      "J. Alberto Conejero"
    ],
    "published": "2025-04-09T05:04:17+00:00",
    "summary": "Despite the huge efforts to develop and administer vaccines worldwide to cope with the COVID-19 pandemic, misinformation spreading through fake news in media and social networks about vaccination safety, make that people refuse to be vaccinated, which harms not only these people but also the whole population.   In this work, we model the effects of harmful information spreading in immunization acquisition through vaccination. Our model is posed for several fractional derivative operators. We have conducted a comprehensive foundation analysis of this model for the different fractional derivatives. Additionally, we have incorporated a strength parameter that shows the combined impact of nonlinear and linear components within an epidemiological model. We have used the second derivative of the Lyapunov function to ascertain the detection of wave patterns within the vaccination dynamics."
  },
  {
    "title": "Bypassing Safety Guardrails in LLMs Using Humor",
    "url": "http://arxiv.org/abs/2504.06577v1",
    "arxiv_id": "2504.06577v1",
    "authors": [
      "Pedro Cisneros-Velarde"
    ],
    "published": "2025-04-09T04:58:14+00:00",
    "summary": "In this paper, we show it is possible to bypass the safety guardrails of large language models (LLMs) through a humorous prompt including the unsafe request. In particular, our method does not edit the unsafe request and follows a fixed template -- it is simple to implement and does not need additional LLMs to craft prompts. Extensive experiments show the effectiveness of our method across different LLMs. We also show that both removing and adding more humor to our method can reduce its effectiveness -- excessive humor possibly distracts the LLM from fulfilling its unsafe request. Thus, we argue that LLM jailbreaking occurs when there is a proper balance between focus on the unsafe request and presence of humor."
  },
  {
    "title": "Do Reasoning Models Show Better Verbalized Calibration?",
    "url": "http://arxiv.org/abs/2504.06564v1",
    "arxiv_id": "2504.06564v1",
    "authors": [
      "Qingcheng Zeng",
      "Weihao Xuan",
      "Leyang Cui",
      "Rob Voigt"
    ],
    "published": "2025-04-09T03:58:19+00:00",
    "summary": "Large reasoning models (LRMs) have recently shown impressive capabilities in complex reasoning by leveraging increased test-time computation and exhibiting behaviors akin to human-like deliberation. Despite these advances, it remains an open question whether LRMs are better calibrated - particularly in their verbalized confidence - compared to instruction-tuned counterparts. In this paper, we investigate the calibration properties of LRMs trained via supervised fine-tuning distillation on long reasoning traces (henceforth SFT reasoning models) and outcome-based reinforcement learning for reasoning (henceforth RL reasoning models) across diverse domains. Our findings reveal that LRMs significantly outperform instruction-tuned models on complex reasoning tasks in both accuracy and confidence calibration. In contrast, we find surprising trends in the domain of factuality in particular. On factuality tasks, while Deepseek-R1 shows strong calibration behavior, smaller QwQ-32B shows no improvement over instruct models; moreover, SFT reasoning models display worse calibration (greater overconfidence) compared to instruct models. Our results provide evidence for a potentially critical role of reasoning-oriented RL training in improving LLMs' capacity for generating trustworthy, self-aware outputs."
  },
  {
    "title": "Data-Driven Reachability with Scenario Optimization and the Holdout Method",
    "url": "http://arxiv.org/abs/2504.06541v1",
    "arxiv_id": "2504.06541v1",
    "authors": [
      "Elizabeth Dietrich",
      "Rosalyn Devonport",
      "Stephen Tu",
      "Murat Arcak"
    ],
    "published": "2025-04-09T02:46:03+00:00",
    "summary": "Reachability analysis is an important method in providing safety guarantees for systems with unknown or uncertain dynamics. Due to the computational intractability of exact reachability analysis for general nonlinear, high-dimensional systems, recent work has focused on the use of probabilistic methods for computing approximate reachable sets. In this work, we advocate for the use of a general purpose, practical, and sharp method for data-driven reachability: the holdout method. Despite the simplicity of the holdout method, we show -- on several numerical examples including scenario-based reach tubes -- that the resulting probabilistic bounds are substantially sharper and require fewer samples than existing methods for data-driven reachability. Furthermore, we complement our work with a discussion on the necessity of probabilistic reachability bounds. We argue that any method that attempts to de-randomize the bounds, by converting the guarantees to hold deterministically, requires (a) an exponential in state-dimension amount of samples to achieve non-vacuous guarantees, and (b) extra assumptions on the dynamics."
  },
  {
    "title": "TSP-OCS: A Time-Series Prediction for Optimal Camera Selection in Multi-Viewpoint Surgical Video Analysis",
    "url": "http://arxiv.org/abs/2504.06527v1",
    "arxiv_id": "2504.06527v1",
    "authors": [
      "Xinyu Liu",
      "Xiaoguang Lin",
      "Xiang Liu",
      "Yong Yang",
      "Hongqian Wang",
      "Qilong Sun"
    ],
    "published": "2025-04-09T02:07:49+00:00",
    "summary": "Recording the open surgery process is essential for educational and medical evaluation purposes; however, traditional single-camera methods often face challenges such as occlusions caused by the surgeon's head and body, as well as limitations due to fixed camera angles, which reduce comprehensibility of the video content. This study addresses these limitations by employing a multi-viewpoint camera recording system, capturing the surgical procedure from six different angles to mitigate occlusions. We propose a fully supervised learning-based time series prediction method to choose the best shot sequences from multiple simultaneously recorded video streams, ensuring optimal viewpoints at each moment. Our time series prediction model forecasts future camera selections by extracting and fusing visual and semantic features from surgical videos using pre-trained models. These features are processed by a temporal prediction network with TimeBlocks to capture sequential dependencies. A linear embedding layer reduces dimensionality, and a Softmax classifier selects the optimal camera view based on the highest probability. In our experiments, we created five groups of open thyroidectomy videos, each with simultaneous recordings from six different angles. The results demonstrate that our method achieves competitive accuracy compared to traditional supervised methods, even when predicting over longer time horizons. Furthermore, our approach outperforms state-of-the-art time series prediction techniques on our dataset. This manuscript makes a unique contribution by presenting an innovative framework that advances surgical video analysis techniques, with significant implications for improving surgical education and patient safety."
  },
  {
    "title": "Safe Navigation in Uncertain Crowded Environments Using Risk Adaptive CVaR Barrier Functions",
    "url": "http://arxiv.org/abs/2504.06513v1",
    "arxiv_id": "2504.06513v1",
    "authors": [
      "Xinyi Wang",
      "Taekyung Kim",
      "Bardh Hoxha",
      "Georgios Fainekos",
      "Dimitra Panagou"
    ],
    "published": "2025-04-09T01:23:44+00:00",
    "summary": "Robot navigation in dynamic, crowded environments poses a significant challenge due to the inherent uncertainties in the obstacle model. In this work, we propose a risk-adaptive approach based on the Conditional Value-at-Risk Barrier Function (CVaR-BF), where the risk level is automatically adjusted to accept the minimum necessary risk, achieving a good performance in terms of safety and optimization feasibility under uncertainty. Additionally, we introduce a dynamic zone-based barrier function which characterizes the collision likelihood by evaluating the relative state between the robot and the obstacle. By integrating risk adaptation with this new function, our approach adaptively expands the safety margin, enabling the robot to proactively avoid obstacles in highly dynamic environments. Comparisons and ablation studies demonstrate that our method outperforms existing social navigation approaches, and validate the effectiveness of our proposed framework."
  },
  {
    "title": "Can LLMs Simulate Personas with Reversed Performance? A Benchmark for Counterfactual Instruction Following",
    "url": "http://arxiv.org/abs/2504.06460v1",
    "arxiv_id": "2504.06460v1",
    "authors": [
      "Sai Adith Senthil Kumar",
      "Hao Yan",
      "Saipavan Perepa",
      "Murong Yue",
      "Ziyu Yao"
    ],
    "published": "2025-04-08T22:00:32+00:00",
    "summary": "Large Language Models (LLMs) are now increasingly widely used to simulate personas in virtual environments, leveraging their instruction-following capability. However, we discovered that even state-of-the-art LLMs cannot simulate personas with reversed performance (e.g., student personas with low proficiency in educational settings), which impairs the simulation diversity and limits the practical applications of the simulated environments. In this work, using mathematical reasoning as a representative scenario, we propose the first benchmark dataset for evaluating LLMs on simulating personas with reversed performance, a capability that we dub \"counterfactual instruction following\". We evaluate both open-weight and closed-source LLMs on this task and find that LLMs, including the OpenAI o1 reasoning model, all struggle to follow counterfactual instructions for simulating reversedly performing personas. Intersectionally simulating both the performance level and the race population of a persona worsens the effect even further. These results highlight the challenges of counterfactual instruction following and the need for further research."
  },
  {
    "title": "Extended Version: Multi-Robot Motion Planning with Cooperative Localization",
    "url": "http://arxiv.org/abs/2504.06429v1",
    "arxiv_id": "2504.06429v1",
    "authors": [
      "Anne Theurkauf",
      "Nisar Ahmed",
      "Morteza Lahijanian"
    ],
    "published": "2025-04-08T20:58:19+00:00",
    "summary": "We consider the uncertain multi-robot motion planning (MRMP) problem with cooperative localization (CL-MRMP), under both motion and measurement noise, where each robot can act as a sensor for its nearby teammates. We formalize CL-MRMP as a chance-constrained motion planning problem, and propose a safety-guaranteed algorithm that explicitly accounts for robot-robot correlations. Our approach extends a sampling-based planner to solve CL-MRMP while preserving probabilistic completeness. To improve efficiency, we introduce novel biasing techniques. We evaluate our method across diverse benchmarks, demonstrating its effectiveness in generating motion plans, with significant performance gains from biasing strategies."
  },
  {
    "title": "Technological schemes and control methods in the reconstruction of parallel gas pipeline systems under non-stationary conditions",
    "url": "http://arxiv.org/abs/2504.06420v1",
    "arxiv_id": "2504.06420v1",
    "authors": [
      "Ilgar Aliyev"
    ],
    "published": "2025-04-08T20:40:02+00:00",
    "summary": "The study explores technological schemes and control methods for reconstructing parallel gas pipeline systems under non-stationary conditions. It focuses on improving safety, reliability, and efficiency by automating valve control and integrating IoT-based monitoring. Automated shut-off valves are designed to detect sudden pressure drops and block leaks instantly. These valves utilize pressure drop rates and valve position sensors to detect and isolate leaks. Wireless pressure sensors and real-time monitoring systems enable remote control of gas flow. Mathematical modeling is employed to analyze pressure variations along damaged sections in order to determine optimal valve placement and activation timing. Machine learning algorithms in the control center are used to predict and verify leak locations based on sensor data. To ensure uninterrupted gas supply in the event of an accident, the study develops an empirical formula for determining the location of connecting pipes activated between parallel pipelines, based on real-time pressure data. The aim of this research is to reduce gas leaks and environmental hazards, ensure continuous gas supply during emergencies, enhance decision-making through automated systems, minimize gas losses, and reduce maintenance costs."
  },
  {
    "title": "SPoRt -- Safe Policy Ratio: Certified Training and Deployment of Task Policies in Model-Free RL",
    "url": "http://arxiv.org/abs/2504.06386v1",
    "arxiv_id": "2504.06386v1",
    "authors": [
      "Jacques Cloete",
      "Nikolaus Vertovec",
      "Alessandro Abate"
    ],
    "published": "2025-04-08T19:09:07+00:00",
    "summary": "To apply reinforcement learning to safety-critical applications, we ought to provide safety guarantees during both policy training and deployment. In this work we present novel theoretical results that provide a bound on the probability of violating a safety property for a new task-specific policy in a model-free, episodic setup: the bound, based on a `maximum policy ratio' that is computed with respect to a `safe' base policy, can also be more generally applied to temporally-extended properties (beyond safety) and to robust control problems. We thus present SPoRt, which also provides a data-driven approach for obtaining such a bound for the base policy, based on scenario theory, and which includes Projected PPO, a new projection-based approach for training the task-specific policy while maintaining a user-specified bound on property violation. Hence, SPoRt enables the user to trade off safety guarantees in exchange for task-specific performance. Accordingly, we present experimental results demonstrating this trade-off, as well as a comparison of the theoretical bound to posterior bounds based on empirical violation rates."
  },
  {
    "title": "Addressing Relative Degree Issues in Control Barrier Function Synthesis with Physics-Informed Neural Networks",
    "url": "http://arxiv.org/abs/2504.06242v1",
    "arxiv_id": "2504.06242v1",
    "authors": [
      "Lukas Brunke",
      "Siqi Zhou",
      "Francesco D'Orazio",
      "Angela P. Schoellig"
    ],
    "published": "2025-04-08T17:41:43+00:00",
    "summary": "In robotics, control barrier function (CBF)-based safety filters are commonly used to enforce state constraints. A critical challenge arises when the relative degree of the CBF varies across the state space. This variability can create regions within the safe set where the control input becomes unconstrained. When implemented as a safety filter, this may result in chattering near the safety boundary and ultimately compromise system safety. To address this issue, we propose a novel approach for CBF synthesis by formulating it as solving a set of boundary value problems. The solutions to the boundary value problems are determined using physics-informed neural networks (PINNs). Our approach ensures that the synthesized CBFs maintain a constant relative degree across the set of admissible states, thereby preventing unconstrained control scenarios. We illustrate the approach in simulation and further verify it through real-world quadrotor experiments, demonstrating its effectiveness in preserving desired system safety properties."
  },
  {
    "title": "TxGemma: Efficient and Agentic LLMs for Therapeutics",
    "url": "http://arxiv.org/abs/2504.06196v1",
    "arxiv_id": "2504.06196v1",
    "authors": [
      "Eric Wang",
      "Samuel Schmidgall",
      "Paul F. Jaeger",
      "Fan Zhang",
      "Rory Pilgrim",
      "Yossi Matias",
      "Joelle Barral",
      "David Fleet",
      "Shekoofeh Azizi"
    ],
    "published": "2025-04-08T16:39:02+00:00",
    "summary": "Therapeutic development is a costly and high-risk endeavor that is often plagued by high failure rates. To address this, we introduce TxGemma, a suite of efficient, generalist large language models (LLMs) capable of therapeutic property prediction as well as interactive reasoning and explainability. Unlike task-specific models, TxGemma synthesizes information from diverse sources, enabling broad application across the therapeutic development pipeline. The suite includes 2B, 9B, and 27B parameter models, fine-tuned from Gemma-2 on a comprehensive dataset of small molecules, proteins, nucleic acids, diseases, and cell lines. Across 66 therapeutic development tasks, TxGemma achieved superior or comparable performance to the state-of-the-art generalist model on 64 (superior on 45), and against state-of-the-art specialist models on 50 (superior on 26). Fine-tuning TxGemma models on therapeutic downstream tasks, such as clinical trial adverse event prediction, requires less training data than fine-tuning base LLMs, making TxGemma suitable for data-limited applications. Beyond these predictive capabilities, TxGemma features conversational models that bridge the gap between general LLMs and specialized property predictors. These allow scientists to interact in natural language, provide mechanistic reasoning for predictions based on molecular structure, and engage in scientific discussions. Building on this, we further introduce Agentic-Tx, a generalist therapeutic agentic system powered by Gemini 2.5 that reasons, acts, manages diverse workflows, and acquires external domain knowledge. Agentic-Tx surpasses prior leading models on the Humanity's Last Exam benchmark (Chemistry & Biology) with 52.3% relative improvement over o3-mini (high) and 26.7% over o3-mini (high) on GPQA (Chemistry) and excels with improvements of 6.3% (ChemBench-Preference) and 2.4% (ChemBench-Mini) over o3-mini (high)."
  },
  {
    "title": "A Self-Supervised Framework for Space Object Behaviour Characterisation",
    "url": "http://arxiv.org/abs/2504.06176v1",
    "arxiv_id": "2504.06176v1",
    "authors": [
      "Ian Groves",
      "Andrew Campbell",
      "James Fernandes",
      "Diego Rodriguez",
      "Paul Murray",
      "Massimiliano Vasile",
      "Victoria Nockles"
    ],
    "published": "2025-04-08T16:19:19+00:00",
    "summary": "Foundation Models, pre-trained on large unlabelled datasets before task-specific fine-tuning, are increasingly being applied to specialised domains. Recent examples include ClimaX for climate and Clay for satellite Earth observation, but a Foundation Model for Space Object Behavioural Analysis has not yet been developed. As orbital populations grow, automated methods for characterising space object behaviour are crucial for space safety. We present a Space Safety and Sustainability Foundation Model focusing on space object behavioural analysis using light curves (LCs). We implemented a Perceiver-Variational Autoencoder (VAE) architecture, pre-trained with self-supervised reconstruction and masked reconstruction on 227,000 LCs from the MMT-9 observatory. The VAE enables anomaly detection, motion prediction, and LC generation. We fine-tuned the model for anomaly detection & motion prediction using two independent LC simulators (CASSANDRA and GRIAL respectively), using CAD models of boxwing, Sentinel-3, SMOS, and Starlink platforms. Our pre-trained model achieved a reconstruction error of 0.01%, identifying potentially anomalous light curves through reconstruction difficulty. After fine-tuning, the model scored 88% and 82% accuracy, with 0.90 and 0.95 ROC AUC scores respectively in both anomaly detection and motion mode prediction (sun-pointing, spin, etc.). Analysis of high-confidence anomaly predictions on real data revealed distinct patterns including characteristic object profiles and satellite glinting. Here, we demonstrate how self-supervised learning can simultaneously enable anomaly detection, motion prediction, and synthetic data generation from rich representations learned in pre-training. Our work therefore supports space safety and sustainability through automated monitoring and simulation capabilities."
  },
  {
    "title": "A Self-Supervised Framework for Space Object Behaviour Characterisation",
    "url": "http://arxiv.org/abs/2504.06176v2",
    "arxiv_id": "2504.06176v2",
    "authors": [
      "Ian Groves",
      "Andrew Campbell",
      "James Fernandes",
      "Diego Ram\u00edrez Rodr\u00edguez",
      "Paul Murray",
      "Massimiliano Vasile",
      "Victoria Nockles"
    ],
    "published": "2025-04-08T16:19:19+00:00",
    "summary": "Foundation Models, pre-trained on large unlabelled datasets before task-specific fine-tuning, are increasingly being applied to specialised domains. Recent examples include ClimaX for climate and Clay for satellite Earth observation, but a Foundation Model for Space Object Behavioural Analysis has not yet been developed. As orbital populations grow, automated methods for characterising space object behaviour are crucial for space safety. We present a Space Safety and Sustainability Foundation Model focusing on space object behavioural analysis using light curves (LCs). We implemented a Perceiver-Variational Autoencoder (VAE) architecture, pre-trained with self-supervised reconstruction and masked reconstruction on 227,000 LCs from the MMT-9 observatory. The VAE enables anomaly detection, motion prediction, and LC generation. We fine-tuned the model for anomaly detection & motion prediction using two independent LC simulators (CASSANDRA and GRIAL respectively), using CAD models of boxwing, Sentinel-3, SMOS, and Starlink platforms. Our pre-trained model achieved a reconstruction error of 0.01%, identifying potentially anomalous light curves through reconstruction difficulty. After fine-tuning, the model scored 88% and 82% accuracy, with 0.90 and 0.95 ROC AUC scores respectively in both anomaly detection and motion mode prediction (sun-pointing, spin, etc.). Analysis of high-confidence anomaly predictions on real data revealed distinct patterns including characteristic object profiles and satellite glinting. Here, we demonstrate how self-supervised learning can simultaneously enable anomaly detection, motion prediction, and synthetic data generation from rich representations learned in pre-training. Our work therefore supports space safety and sustainability through automated monitoring and simulation capabilities."
  },
  {
    "title": "Safe Interaction via Monte Carlo Linear-Quadratic Games",
    "url": "http://arxiv.org/abs/2504.06124v1",
    "arxiv_id": "2504.06124v1",
    "authors": [
      "Benjamin A. Christie",
      "Dylan P. Losey"
    ],
    "published": "2025-04-08T15:18:38+00:00",
    "summary": "Safety is critical during human-robot interaction. But -- because people are inherently unpredictable -- it is often difficult for robots to plan safe behaviors. Instead of relying on our ability to anticipate humans, here we identify robot policies that are robust to unexpected human decisions. We achieve this by formulating human-robot interaction as a zero-sum game, where (in the worst case) the human's actions directly conflict with the robot's objective. Solving for the Nash Equilibrium of this game provides robot policies that maximize safety and performance across a wide range of human actions. Existing approaches attempt to find these optimal policies by leveraging Hamilton-Jacobi analysis (which is intractable) or linear-quadratic approximations (which are inexact). By contrast, in this work we propose a computationally efficient and theoretically justified method that converges towards the Nash Equilibrium policy. Our approach (which we call MCLQ) leverages linear-quadratic games to obtain an initial guess at safe robot behavior, and then iteratively refines that guess with a Monte Carlo search. Not only does MCLQ provide real-time safety adjustments, but it also enables the designer to tune how conservative the robot is -- preventing the system from focusing on unrealistic human behaviors. Our simulations and user study suggest that this approach advances safety in terms of both computation time and expected performance. See videos of our experiments here: https://youtu.be/KJuHeiWVuWY."
  },
  {
    "title": "Leanabell-Prover: Posttraining Scaling in Formal Reasoning",
    "url": "http://arxiv.org/abs/2504.06122v2",
    "arxiv_id": "2504.06122v2",
    "authors": [
      "Jingyuan Zhang",
      "Qi Wang",
      "Xingguang Ji",
      "Yahui Liu",
      "Yang Yue",
      "Fuzheng Zhang",
      "Di Zhang",
      "Guorui Zhou",
      "Kun Gai"
    ],
    "published": "2025-04-08T15:15:26+00:00",
    "summary": "Recent advances in automated theorem proving (ATP) through LLMs have highlighted the potential of formal reasoning with Lean 4 codes. However, ATP has not yet be revolutionized by the recent posttraining scaling as demonstrated by Open AI O1/O3 and Deepseek R1. In this work, we investigate the entire posttraining of ATP, aiming to align it with breakthroughs in reasoning models in natural languages. To begin, we continual train current ATP models with a hybrid dataset, which consists of numerous statement-proof pairs, and additional data aimed at incorporating cognitive behaviors that emulate human reasoning and hypothesis refinement. Next, we explore reinforcement learning with the use of outcome reward returned by Lean 4 compiler. Through our designed continual training and reinforcement learning processes, we have successfully improved existing formal provers, including both DeepSeek-Prover-v1.5 and Goedel-Prover, achieving state-of-the-art performance in the field of whole-proof generation. For example, we achieve a 59.8% pass rate (pass@32) on MiniF2F. This is an on-going project and we will progressively update our findings, release our data and training details."
  },
  {
    "title": "Uncertainty-Aware Hybrid Machine Learning in Virtual Sensors for Vehicle Sideslip Angle Estimation",
    "url": "http://arxiv.org/abs/2504.06105v1",
    "arxiv_id": "2504.06105v1",
    "authors": [
      "Abinav Kalyanasundaram",
      "Karthikeyan Chandra Sekaran",
      "Philipp Stauber",
      "Michael Lange",
      "Wolfgang Utschick",
      "Michael Botsch"
    ],
    "published": "2025-04-08T14:49:58+00:00",
    "summary": "Precise vehicle state estimation is crucial for safe and reliable autonomous driving. The number of measurable states and their precision offered by the onboard vehicle sensor system are often constrained by cost. For instance, measuring critical quantities such as the Vehicle Sideslip Angle (VSA) poses significant commercial challenges using current optical sensors. This paper addresses these limitations by focusing on the development of high-performance virtual sensors to enhance vehicle state estimation for active safety. The proposed Uncertainty-Aware Hybrid Learning (UAHL) architecture integrates a machine learning model with vehicle motion models to estimate VSA directly from onboard sensor data. A key aspect of the UAHL architecture is its focus on uncertainty quantification for individual model estimates and hybrid fusion. These mechanisms enable the dynamic weighting of uncertainty-aware predictions from machine learning and vehicle motion models to produce accurate and reliable hybrid VSA estimates. This work also presents a novel dataset named Real-world Vehicle State Estimation Dataset (ReV-StED), comprising synchronized measurements from advanced vehicle dynamic sensors. The experimental results demonstrate the superior performance of the proposed method for VSA estimation, highlighting UAHL as a promising architecture for advancing virtual sensors and enhancing active safety in autonomous vehicles."
  }
]