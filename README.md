# Awesome Large Reasoning Model (LRM) Safety ğŸ”¥

[![Awesome](https://awesome.re/badge.svg)](https://awesome.re)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
![Auto Update](https://github.com/wonderNefelibata/Awesome-LRM-Safety/actions/workflows/arxiv-update.yml/badge.svg)

A curated list of **security and safety research** for Large Reasoning Models (LRMs) like DeepSeek-R1, OpenAI o1, and other cutting-edge models. Focused on identifying risks, mitigation strategies, and ethical implications.

---

## ğŸ“œ Table of Contents
- [Awesome Large Reasoning Model (LRM) Safety ğŸ”¥](#awesome-large-reasoning-model-lrm-safety-)
  - [ğŸ“œ Table of Contents](#-table-of-contents)
  - [ğŸš€ Motivation](#-motivation)
  - [ğŸ“° Latest arXiv Papers (Auto-Updated)](#-latest-arxiv-papers-auto-updated)
  - [ğŸ”‘ Key Safety Domains(coming soon)](#-key-safety-domainscoming-soon)
  - [ğŸ“š Research Papers(coming soon)](#-research-paperscoming-soon)
    - [Foundational Works](#foundational-works)
    - [Attack Vectors](#attack-vectors)
    - [Defense Mechanisms](#defense-mechanisms)
  - [ğŸ› ï¸ Projects \& Tools(coming soon)](#ï¸-projects--toolscoming-soon)
    - [Model-Specific Resources](#model-specific-resources)
    - [General Tools(coming soon)](#general-toolscoming-soon)
  - [ğŸ¤ Contributing](#-contributing)
  - [ğŸ“„ License](#-license)
  - [â“ FAQ](#-faq)

---

## ğŸš€ Motivation
Large Reasoning Models (LRMs) are revolutionizing AI capabilities in complex decision-making scenarios. However, their deployment raises critical safety concerns:
- **Adversarial attacks** on reasoning pipelines
- **Privacy leakage** through multi-step reasoning
- **Ethical risks** in high-stakes domains (e.g., healthcare, finance)
- **Systemic failures** in autonomous systems
This repository aims to catalog research addressing these challenges and promote safer LRM development.

---

## ğŸ“° Latest arXiv Papers (Auto-Updated)
<!-- LATEST_PAPERS_START --><!-- LATEST_PAPERS_END -->
<!-- HISTORICAL_PAPERS_START --><!-- HISTORICAL_PAPERS_END -->
---

## ğŸ”‘ Key Safety Domains(coming soon)
| Category               | Key Challenges                          | Related Topics                          |
|------------------------|-----------------------------------------|------------------------------------------|
| **Adversarial Robustness** | Prompt injection, Reasoning path poisoning | Red teaming, Formal verification        |
| **Privacy Preservation**  | Intermediate step memorization, Data leakage | Differential privacy, Federated learning|
| **Ethical Alignment**     | Value locking, Contextual moral reasoning | Constitutional AI, Value learning       |
| **System Safety**         | Cascading failures, Reward hacking       | Safe interruptibility, System monitoring|
| **Regulatory Compliance** | Audit trails, Explainability requirements | Model cards, Governance frameworks      |

---

## ğŸ“š Research Papers(coming soon)
### Foundational Works
- [2023] [Towards Safer Large Reasoning Models: A Survey of Risks in Multistep Reasoning Systems](https://arxiv.org/abs/example )  
  *Comprehensive taxonomy of LRM safety risks*

### Attack Vectors
- [2024] [Hidden Triggers in Reasoning Chains: New Attack Surfaces for LRMs](https://arxiv.org/abs/example )  
  *Demonstrates adversarial manipulation of reasoning steps*

### Defense Mechanisms
- [2024] [Reasoning with Guardrails: Constrained Decoding for LRM Safety](https://arxiv.org/abs/example )  
  *Novel approach to step-wise constraint enforcement*

*(Add your collected papers here with proper categorization)*

---

## ğŸ› ï¸ Projects & Tools(coming soon)
### Model-Specific Resources
- **DeepSeek-R1 Safety Kit**  
  Official safety evaluation toolkit for DeepSeek-R1 reasoning modules

- **OpenAI o1 Red Teaming Framework**  
  Adversarial testing framework for multi-turn reasoning tasks

### General Tools(coming soon)
- [ReasonGuard](https://github.com/example/reasonguard )  
  Real-time monitoring for reasoning chain anomalies

- [Ethos](https://github.com/example/ethos )  
  Ethical alignment evaluation suite for LRMs

---

## ğŸ¤ Contributing
We welcome contributions! Please:
1. Fork the repository
2. Add resources via pull request
3. Ensure entries follow the format:
   ```markdown
   - [Year] [Paper Title](URL)  
     *Brief description (5-15 words)*
   ```
4. Maintain topical categorization

See [CONTRIBUTING.md](CONTRIBUTING.md) for detailed guidelines.

---

## ğŸ“„ License
This project is licensed under the MIT License - see [LICENSE](LICENSE) for details.

---

## â“ FAQ
**Q: How do I stay updated?**  
A: Watch this repo and check the "Recent Updates" section (coming soon).

**Q: Can I suggest non-academic resources?**  
A: Yes! Industry reports and blog posts are welcome if they provide novel insights.

**Q: How are entries verified?**  
A: All submissions undergo community review for relevance and quality.

---

> *"With great reasoning power comes great responsibility."* - Adapted from [AI Ethics Manifesto]
