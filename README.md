# Awesome Large Reasoning Model (LRM) Safety üî•

[![Awesome](https://awesome.re/badge.svg)](https://awesome.re)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
![Auto Update](https://github.com/wonderNefelibata/Awesome-LRM-Safety/actions/workflows/arxiv-update.yml/badge.svg)

A curated list of **security and safety research** for Large Reasoning Models (LRMs) like DeepSeek-R1, OpenAI o1, and other cutting-edge models. Focused on identifying risks, mitigation strategies, and ethical implications.

---

## üìú Table of Contents
- [Awesome Large Reasoning Model (LRM) Safety üî•](#awesome-large-reasoning-model-lrm-safety-)
  - [üìú Table of Contents](#-table-of-contents)
  - [üöÄ Motivation](#-motivation)
  - [üì∞ Latest arXiv Papers (Auto-Updated)](#-latest-arxiv-papers-auto-updated)
  - [üîë Key Safety Domains(coming soon)](#-key-safety-domainscoming-soon)
  - [üîñ Dataset \& Benchmark](#-dataset--benchmark)
  - [üìö Research Papers(coming soon)](#-research-paperscoming-soon)
    - [Foundational Works](#foundational-works)
    - [Attack Vectors](#attack-vectors)
    - [Defense Mechanisms](#defense-mechanisms)
  - [üõ†Ô∏è Projects \& Tools(coming soon)](#Ô∏è-projects--toolscoming-soon)
    - [Model-Specific Resources](#model-specific-resources)
    - [General Tools(coming soon)](#general-toolscoming-soon)
  - [ü§ù Contributing](#-contributing)
  - [üìÑ License](#-license)
  - [‚ùì FAQ](#-faq)
  - [üîó References](#-references)

---

## üöÄ Motivation
Large Reasoning Models (LRMs) are revolutionizing AI capabilities in complex decision-making scenarios. However, their deployment raises critical safety concerns:
- **Adversarial attacks** on reasoning pipelines
- **Privacy leakage** through multi-step reasoning
- **Ethical risks** in high-stakes domains (e.g., healthcare, finance)
- **Systemic failures** in autonomous systems
This repository aims to catalog research addressing these challenges and promote safer LRM development.

---

## üì∞ Latest arXiv Papers (Auto-Updated)
<!-- LATEST_PAPERS_START -->


| Date       | Title                                      | Authors           | Abstract Summary          |
|------------|--------------------------------------------|-------------------|---------------------------|
| 2025-02-21 | [VaViM and VaVAM: Autonomous Driving through Video Generative Modeling](http://arxiv.org/abs/2502.15672v1) | Florent Bartoccioni, Elias Ramzi et al. | We explore the potential of large-scale generative video models for autonomous driving, introducing an open-source auto-regressive video model (VaViM) and its companion video-action model (VaVAM) to investigate how video pre-training transfers to real-world driving. VaViM is a simple auto-regressive video model that predicts frames using spatio-temporal token sequences. We show that it captures the semantics and dynamics of driving scenes. VaVAM, the video-action model, leverages the learned representations of VaViM to generate driving trajectories through imitation learning. Together, the models form a complete perception-to-action pipeline. We evaluate our models in open- and closed-loop driving scenarios, revealing that video-based pre-training holds promise for autonomous driving. Key insights include the semantic richness of the learned representations, the benefits of scaling for video synthesis, and the complex relationship between model size, data, and safety metrics in closed-loop evaluations. We release code and model weights at https://github.com/valeoai/VideoActionModel |
| 2025-02-21 | [Superintelligent Agents Pose Catastrophic Risks: Can Scientist AI Offer a Safer Path?](http://arxiv.org/abs/2502.15657v1) | Yoshua Bengio, Michael Cohen et al. | The leading AI companies are increasingly focused on building generalist AI agents -- systems that can autonomously plan, act, and pursue goals across almost all tasks that humans can perform. Despite how useful these systems might be, unchecked AI agency poses significant risks to public safety and security, ranging from misuse by malicious actors to a potentially irreversible loss of human control. We discuss how these risks arise from current AI training methods. Indeed, various scenarios and experiments have demonstrated the possibility of AI agents engaging in deception or pursuing goals that were not specified by human operators and that conflict with human interests, such as self-preservation. Following the precautionary principle, we see a strong need for safer, yet still useful, alternatives to the current agency-driven trajectory. Accordingly, we propose as a core building block for further advances the development of a non-agentic AI system that is trustworthy and safe by design, which we call Scientist AI. This system is designed to explain the world from observations, as opposed to taking actions in it to imitate or please humans. It comprises a world model that generates theories to explain data and a question-answering inference machine. Both components operate with an explicit notion of uncertainty to mitigate the risks of overconfident predictions. In light of these considerations, a Scientist AI could be used to assist human researchers in accelerating scientific progress, including in AI safety. In particular, our system can be employed as a guardrail against AI agents that might be created despite the risks involved. Ultimately, focusing on non-agentic AI may enable the benefits of AI innovation while avoiding the risks associated with the current trajectory. We hope these arguments will motivate researchers, developers, and policymakers to favor this safer path. |
| 2025-02-21 | [A Simulation Pipeline to Facilitate Real-World Robotic Reinforcement Learning Applications](http://arxiv.org/abs/2502.15649v1) | Jefferson Silveira, Joshua A. Marshall et al. | Reinforcement learning (RL) has gained traction for its success in solving complex tasks for robotic applications. However, its deployment on physical robots remains challenging due to safety risks and the comparatively high costs of training. To avoid these problems, RL agents are often trained on simulators, which introduces a new problem related to the gap between simulation and reality. This paper presents an RL pipeline designed to help reduce the reality gap and facilitate developing and deploying RL policies for real-world robotic systems. The pipeline organizes the RL training process into an initial step for system identification and three training stages: core simulation training, high-fidelity simulation, and real-world deployment, each adding levels of realism to reduce the sim-to-real gap. Each training stage takes an input policy, improves it, and either passes the improved policy to the next stage or loops it back for further improvement. This iterative process continues until the policy achieves the desired performance. The pipeline's effectiveness is shown through a case study with the Boston Dynamics Spot mobile robot used in a surveillance application. The case study presents the steps taken at each pipeline stage to obtain an RL agent to control the robot's position and orientation. |
| 2025-02-21 | [The Relationship Between Reasoning and Performance in Large Language Models -- o3 (mini) Thinks Harder, Not Longer](http://arxiv.org/abs/2502.15631v1) | Marthe Ballon, Andres Algaba et al. | Large language models have demonstrated remarkable progress in mathematical reasoning, leveraging chain-of-thought and test-time compute scaling. However, many open questions remain regarding the interplay between reasoning token usage and accuracy gains. In particular, when comparing models across generations, it is unclear whether improved performance results from longer reasoning chains or more efficient reasoning. We systematically analyze chain-of-thought length across o1-mini and o3-mini variants on the Omni-MATH benchmark, finding that o3-mini (m) achieves superior accuracy without requiring longer reasoning chains than o1-mini. Moreover, we show that accuracy generally declines as reasoning chains grow across all models and compute settings, even when controlling for difficulty of the questions. This accuracy drop is significantly smaller in more proficient models, suggesting that new generations of reasoning models use test-time compute more effectively. Finally, we highlight that while o3-mini (h) achieves a marginal accuracy gain over o3-mini (m), it does so by allocating substantially more reasoning tokens across all problems, even the ones that o3-mini (m) can already solve. These findings provide new insights into the relationship between model capability and reasoning length, with implications for efficiency, scaling, and evaluation methodologies. |
| 2025-02-21 | [Pick-and-place Manipulation Across Grippers Without Retraining: A Learning-optimization Diffusion Policy Approach](http://arxiv.org/abs/2502.15613v1) | Xiangtong Yao, Yirui Zhou et al. | Current robotic pick-and-place policies typically require consistent gripper configurations across training and inference. This constraint imposes high retraining or fine-tuning costs, especially for imitation learning-based approaches, when adapting to new end-effectors. To mitigate this issue, we present a diffusion-based policy with a hybrid learning-optimization framework, enabling zero-shot adaptation to novel grippers without additional data collection for retraining policy. During training, the policy learns manipulation primitives from demonstrations collected using a base gripper. At inference, a diffusion-based optimization strategy dynamically enforces kinematic and safety constraints, ensuring that generated trajectories align with the physical properties of unseen grippers. This is achieved through a constrained denoising procedure that adapts trajectories to gripper-specific parameters (e.g., tool-center-point offsets, jaw widths) while preserving collision avoidance and task feasibility. We validate our method on a Franka Panda robot across six gripper configurations, including 3D-printed fingertips, flexible silicone gripper, and Robotiq 2F-85 gripper. Our approach achieves a 93.3% average task success rate across grippers (vs. 23.3-26.7% for diffusion policy baselines), supporting tool-center-point variations of 16-23.5 cm and jaw widths of 7.5-11.5 cm. The results demonstrate that constrained diffusion enables robust cross-gripper manipulation while maintaining the sample efficiency of imitation learning, eliminating the need for gripper-specific retraining. Video and code are available at https://github.com/yaoxt3/GADP. |
| 2025-02-21 | [SafeInt: Shielding Large Language Models from Jailbreak Attacks via Safety-Aware Representation Intervention](http://arxiv.org/abs/2502.15594v1) | Jiaqi Wu, Chen Chen et al. | With the widespread real-world deployment of large language models (LLMs), ensuring their behavior complies with safety standards has become crucial. Jailbreak attacks exploit vulnerabilities in LLMs to induce undesirable behavior, posing a significant threat to LLM safety. Previous defenses often fail to achieve both effectiveness and efficiency simultaneously. Defenses from a representation perspective offer new insights, but existing interventions cannot dynamically adjust representations based on the harmfulness of the queries. To address this limitation while ensuring both effectiveness and efficiency, we propose SafeIntervention (SafeInt), a novel defense method that shields LLMs from jailbreak attacks through safety-aware representation intervention. SafeInt is built on our analysis of the representations of jailbreak samples. It adjusts representation distributions of jailbreak samples through intervention to align them with the representations of unsafe samples while minimizing unnecessary perturbations to jailbreak-irrelevant representations. We conduct comprehensive experiments covering six jailbreak attacks, two jailbreak datasets, and two utility benchmarks. Experimental results demonstrate that SafeInt outperforms all baselines in defending LLMs against jailbreak attacks while largely maintaining utility. Additionally, we evaluate SafeInt against adaptive attacks and verify its effectiveness in mitigating real-time attacks. |
| 2025-02-21 | [Interpreting and Steering LLMs with Mutual Information-based Explanations on Sparse Autoencoders](http://arxiv.org/abs/2502.15576v1) | Xuansheng Wu, Jiayi Yuan et al. | Large language models (LLMs) excel at handling human queries, but they can occasionally generate flawed or unexpected responses. Understanding their internal states is crucial for understanding their successes, diagnosing their failures, and refining their capabilities. Although sparse autoencoders (SAEs) have shown promise for interpreting LLM internal representations, limited research has explored how to better explain SAE features, i.e., understanding the semantic meaning of features learned by SAE. Our theoretical analysis reveals that existing explanation methods suffer from the frequency bias issue, where they emphasize linguistic patterns over semantic concepts, while the latter is more critical to steer LLM behaviors. To address this, we propose using a fixed vocabulary set for feature interpretations and designing a mutual information-based objective, aiming to better capture the semantic meaning behind these features. We further propose two runtime steering strategies that adjust the learned feature activations based on their corresponding explanations. Empirical results show that, compared to baselines, our method provides more discourse-level explanations and effectively steers LLM behaviors to defend against jailbreak attacks. These findings highlight the value of explanations for steering LLM behaviors in downstream applications. We will release our code and data once accepted. |
| 2025-02-21 | [Model Privacy: A Unified Framework to Understand Model Stealing Attacks and Defenses](http://arxiv.org/abs/2502.15567v1) | Ganghua Wang, Yuhong Yang et al. | The use of machine learning (ML) has become increasingly prevalent in various domains, highlighting the importance of understanding and ensuring its safety. One pressing concern is the vulnerability of ML applications to model stealing attacks. These attacks involve adversaries attempting to recover a learned model through limited query-response interactions, such as those found in cloud-based services or on-chip artificial intelligence interfaces. While existing literature proposes various attack and defense strategies, these often lack a theoretical foundation and standardized evaluation criteria. In response, this work presents a framework called ``Model Privacy'', providing a foundation for comprehensively analyzing model stealing attacks and defenses. We establish a rigorous formulation for the threat model and objectives, propose methods to quantify the goodness of attack and defense strategies, and analyze the fundamental tradeoffs between utility and privacy in ML models. Our developed theory offers valuable insights into enhancing the security of ML models, especially highlighting the importance of the attack-specific structure of perturbations for effective defenses. We demonstrate the application of model privacy from the defender's perspective through various learning scenarios. Extensive experiments corroborate the insights and the effectiveness of defense mechanisms developed under the proposed framework. |
| 2025-02-21 | [Estimating Vehicle Speed on Roadways Using RNNs and Transformers: A Video-based Approach](http://arxiv.org/abs/2502.15545v1) | Sai Krishna Reddy Mareddy, Dhanush Upplapati et al. | This project explores the application of advanced machine learning models, specifically Long Short-Term Memory (LSTM), Gated Recurrent Units (GRU), and Transformers, to the task of vehicle speed estimation using video data. Traditional methods of speed estimation, such as radar and manual systems, are often constrained by high costs, limited coverage, and potential disruptions. In contrast, leveraging existing surveillance infrastructure and cutting-edge neural network architectures presents a non-intrusive, scalable solution. Our approach utilizes LSTM and GRU to effectively manage long-term dependencies within the temporal sequence of video frames, while Transformers are employed to harness their self-attention mechanisms, enabling the processing of entire sequences in parallel and focusing on the most informative segments of the data. This study demonstrates that both LSTM and GRU outperform basic Recurrent Neural Networks (RNNs) due to their advanced gating mechanisms. Furthermore, increasing the sequence length of input data consistently improves model accuracy, highlighting the importance of contextual information in dynamic environments. Transformers, in particular, show exceptional adaptability and robustness across varied sequence lengths and complexities, making them highly suitable for real-time applications in diverse traffic conditions. The findings suggest that integrating these sophisticated neural network models can significantly enhance the accuracy and reliability of automated speed detection systems, thus promising to revolutionize traffic management and road safety. |
| 2025-02-21 | [NPB-Rust: NAS Parallel Benchmarks in Rust](http://arxiv.org/abs/2502.15536v1) | Eduardo M. Martins, Leonardo G. Fa√© et al. | Parallel programming often requires developers to handle complex computational tasks that can yield many errors in its development cycle. Rust is a performant low-level language that promises memory safety guarantees with its compiler, making it an attractive option for HPC application developers. We identified that the Rust ecosystem could benefit from more comprehensive scientific benchmark suites for standardizing comparisons and research. The NAS Parallel Benchmarks (NPB) is a standardized suite for evaluating various hardware aspects and is often used to compare different frameworks for parallelism. Therefore, our contributions are a Rust version of NPB, an analysis of the expressiveness and performance of the language features, and parallelization strategies. We compare our implementation with consolidated sequential and parallel versions of NPB. Experimental results show that Rust's sequential version is 1.23\% slower than Fortran and 5.59\% faster than C++, while Rust with Rayon was slower than both Fortran and C++ with OpenMP. |
| 2025-02-21 | [Depth-aware Fusion Method based on Image and 4D Radar Spectrum for 3D Object Detection](http://arxiv.org/abs/2502.15516v1) | Yue Sun, Yeqiang Qian et al. | Safety and reliability are crucial for the public acceptance of autonomous driving. To ensure accurate and reliable environmental perception, intelligent vehicles must exhibit accuracy and robustness in various environments. Millimeter-wave radar, known for its high penetration capability, can operate effectively in adverse weather conditions such as rain, snow, and fog. Traditional 3D millimeter-wave radars can only provide range, Doppler, and azimuth information for objects. Although the recent emergence of 4D millimeter-wave radars has added elevation resolution, the radar point clouds remain sparse due to Constant False Alarm Rate (CFAR) operations. In contrast, cameras offer rich semantic details but are sensitive to lighting and weather conditions. Hence, this paper leverages these two highly complementary and cost-effective sensors, 4D millimeter-wave radar and camera. By integrating 4D radar spectra with depth-aware camera images and employing attention mechanisms, we fuse texture-rich images with depth-rich radar data in the Bird's Eye View (BEV) perspective, enhancing 3D object detection. Additionally, we propose using GAN-based networks to generate depth images from radar spectra in the absence of depth sensors, further improving detection accuracy. |
| 2025-02-21 | [A modular risk concept for complex systems](http://arxiv.org/abs/2502.15482v1) | Dag McGeorge, Jon Arne Glomsrud | Our ways of managing risk have in the past been adapted to changes in technology and society. Amidst the ongoing digital transformation, the ur-gency of adapting risk management to changing needs seems higher than ever. This paper starts with a brief historic overview of the development of risk management in the past. The paper motivates the views that for com-plex systems, risk should be controlled by enforcing constrains in a modular way at different system levels, that the constraints can be expressed as assur-ance contracts and that acceptable risk mitigation can be demonstrated in as-surance case modules. Based on extensive industry experience of the authors, a major contribution is to explain how already existing methodologies have been combined to cre-ate a concept for modular risk assessment. Examples from assurance of au-tonomous sea navigation and autonomous driving are used to illustrate the concept. Beyond the existing methodologies this paper generalizes risk con-straints to assurance contracts as an enabler of modular risk assessment spanning all relevant system levels and stakeholder perspectives while main-taining the dependencies between the system parts and accounting for emer-gent system behavior. Furthermore, the use of safety integrity levels (SIL) and similar concepts for assigning assurance rigor have been avoided in favor of direct assessment of assurance case argument rigor, because technology and applications change too fast to justify using past experience as evidence of validity of such prescriptive schemes. This paper aims to help practitioners making efficient and timely risk-informed decisions about complex integrated systems. |
| 2025-02-21 | [Single-pass Detection of Jailbreaking Input in Large Language Models](http://arxiv.org/abs/2502.15435v1) | Leyla Naz Candogan, Yongtao Wu et al. | Defending aligned Large Language Models (LLMs) against jailbreaking attacks is a challenging problem, with existing approaches requiring multiple requests or even queries to auxiliary LLMs, making them computationally heavy. Instead, we focus on detecting jailbreaking input in a single forward pass. Our method, called Single Pass Detection SPD, leverages the information carried by the logits to predict whether the output sentence will be harmful. This allows us to defend in just one forward pass. SPD can not only detect attacks effectively on open-source models, but also minimizes the misclassification of harmless inputs. Furthermore, we show that SPD remains effective even without complete logit access in GPT-3.5 and GPT-4. We believe that our proposed method offers a promising approach to efficiently safeguard LLMs against adversarial attacks. |
| 2025-02-21 | [Pub-Guard-LLM: Detecting Fraudulent Biomedical Articles with Reliable Explanations](http://arxiv.org/abs/2502.15429v1) | Lihu Chen, Shuojie Fu et al. | A significant and growing number of published scientific articles is found to involve fraudulent practices, posing a serious threat to the credibility and safety of research in fields such as medicine. We propose Pub-Guard-LLM, the first large language model-based system tailored to fraud detection of biomedical scientific articles. We provide three application modes for deploying Pub-Guard-LLM: vanilla reasoning, retrieval-augmented generation, and multi-agent debate. Each mode allows for textual explanations of predictions. To assess the performance of our system, we introduce an open-source benchmark, PubMed Retraction, comprising over 11K real-world biomedical articles, including metadata and retraction labels. We show that, across all modes, Pub-Guard-LLM consistently surpasses the performance of various baselines and provides more reliable explanations, namely explanations which are deemed more relevant and coherent than those generated by the baselines when evaluated by multiple assessment methods. By enhancing both detection performance and explainability in scientific fraud detection, Pub-Guard-LLM contributes to safeguarding research integrity with a novel, effective, open-source tool. |
| 2025-02-21 | [Adversarial Prompt Evaluation: Systematic Benchmarking of Guardrails Against Prompt Input Attacks on LLMs](http://arxiv.org/abs/2502.15427v1) | Giulio Zizzo, Giandomenico Cornacchia et al. | As large language models (LLMs) become integrated into everyday applications, ensuring their robustness and security is increasingly critical. In particular, LLMs can be manipulated into unsafe behaviour by prompts known as jailbreaks. The variety of jailbreak styles is growing, necessitating the use of external defences known as guardrails. While many jailbreak defences have been proposed, not all defences are able to handle new out-of-distribution attacks due to the narrow segment of jailbreaks used to align them. Moreover, the lack of systematisation around defences has created significant gaps in their practical application. In this work, we perform systematic benchmarking across 15 different defences, considering a broad swathe of malicious and benign datasets. We find that there is significant performance variation depending on the style of jailbreak a defence is subject to. Additionally, we show that based on current datasets available for evaluation, simple baselines can display competitive out-of-distribution performance compared to many state-of-the-art defences. Code is available at https://github.com/IBM/Adversarial-Prompt-Evaluation. |
| 2025-02-21 | [A biomechanical comparison of concussion and head acceleration events in elite-level American football and rugby union](http://arxiv.org/abs/2502.15405v1) | Gregory Tierney | Elite-level American football and rugby union are two high-contact sports with growing clinical and legal concerns over player safety, necessitating a comparative analysis. A biomechanical comparison of concussion and head acceleration events (HAEs) in elite-level American football and rugby union was undertaken. Rugby union players have a greater number of professional playing years and matches available in a season than their American football counterparts. Rugby union players have a greater number of concussions reported per match and a higher proportion of concussions occurring during training sessions, based on National Football League (NFL) and Rugby Football Union (RFU) injury reports. Preliminary findings indicate that rugby union forwards experience a higher incidence of HAEs per player match over lower and higher magnitude thresholds, than American football defensive players. Overall, elite-level rugby union appears less favourable than American football in in almost all metrics pertinent to concussion and HAE exposure in the biomechanical comparison undertaken. The findings highlight the critical importance of independence, scientific rigour, and transparency in future concussion and HAE biomechanics research and real-world implementation, ensuring the development of more effective mitigation strategies. |
| 2025-02-21 | [Evaluating Social Biases in LLM Reasoning](http://arxiv.org/abs/2502.15361v1) | Xuyang Wu, Jinming Nian et al. | In the recent development of AI reasoning, large language models (LLMs) are trained to automatically generate chain-of-thought reasoning steps, which have demonstrated compelling performance on math and coding tasks. However, when bias is mixed within the reasoning process to form strong logical arguments, it could cause even more harmful results and further induce hallucinations. In this paper, we have evaluated the 8B and 32B variants of DeepSeek-R1 against their instruction tuned counterparts on the BBQ dataset, and investigated the bias that is elicited out and being amplified through reasoning steps. To the best of our knowledge, this empirical study is the first to assess bias issues in LLM reasoning. |
| 2025-02-21 | [Drug-Target Interaction/Affinity Prediction: Deep Learning Models and Advances Review](http://arxiv.org/abs/2502.15346v1) | Ali Vefghi, Zahed Rahmati et al. | Drug discovery remains a slow and expensive process that involves many steps, from detecting the target structure to obtaining approval from the Food and Drug Administration (FDA), and is often riddled with safety concerns. Accurate prediction of how drugs interact with their targets and the development of new drugs by using better methods and technologies have immense potential to speed up this process, ultimately leading to faster delivery of life-saving medications. Traditional methods used for drug-target interaction prediction show limitations, particularly in capturing complex relationships between drugs and their targets. As an outcome, deep learning models have been presented to overcome the challenges of interaction prediction through their precise and efficient end results. By outlining promising research avenues and models, each with a different solution but similar to the problem, this paper aims to give researchers a better idea of methods for even more accurate and efficient prediction of drug-target interaction, ultimately accelerating the development of more effective drugs. A total of 180 prediction methods for drug-target interactions were analyzed throughout the period spanning 2016 to 2025 using different frameworks based on machine learning, mainly deep learning and graph neural networks. Additionally, this paper discusses the novelty, architecture, and input representation of these models. |
| 2025-02-21 | [Attention Eclipse: Manipulating Attention to Bypass LLM Safety-Alignment](http://arxiv.org/abs/2502.15334v1) | Pedram Zaree, Md Abdullah Al Mamun et al. | Recent research has shown that carefully crafted jailbreak inputs can induce large language models to produce harmful outputs, despite safety measures such as alignment. It is important to anticipate the range of potential Jailbreak attacks to guide effective defenses and accurate assessment of model safety. In this paper, we present a new approach for generating highly effective Jailbreak attacks that manipulate the attention of the model to selectively strengthen or weaken attention among different parts of the prompt. By harnessing attention loss, we develop more effective jailbreak attacks, that are also transferrable. The attacks amplify the success rate of existing Jailbreak algorithms including GCG, AutoDAN, and ReNeLLM, while lowering their generation cost (for example, the amplified GCG attack achieves 91.2% ASR, vs. 67.9% for the original attack on Llama2-7B/AdvBench, using less than a third of the generation time). |
| 2025-02-21 | [Understand User Opinions of Large Language Models via LLM-Powered In-the-Moment User Experience Interviews](http://arxiv.org/abs/2502.15226v1) | Mengqiao Liu, Tevin Wang et al. | Which large language model (LLM) is better? Every evaluation tells a story, but what do users really think about current LLMs? This paper presents CLUE, an LLM-powered interviewer that conducts in-the-moment user experience interviews, right after users interacted with LLMs, and automatically gathers insights about user opinions from massive interview logs. We conduct a study with thousands of users to understand user opinions on mainstream LLMs, recruiting users to first chat with a target LLM and then interviewed by CLUE. Our experiments demonstrate that CLUE captures interesting user opinions, for example, the bipolar views on the displayed reasoning process of DeepSeek-R1 and demands for information freshness and multi-modality. Our collected chat-and-interview logs will be released. |

<!-- LATEST_PAPERS_END -->

---

## üîë Key Safety Domains(coming soon)
![LLM Safety Category](/assets/img/image1.png "LLM Safety Category")

**Fig.1**: LLM Safety [[Ma et al., 2025]([arXiv:2502.05206](https://arxiv.org/abs/2502.05206))]

| Category               | Key Challenges                          | Related Topics                          |
|------------------------|-----------------------------------------|------------------------------------------|
| **Adversarial Robustness** | Prompt injection, Reasoning path poisoning | Red teaming, Formal verification        |
| **Privacy Preservation**  | Intermediate step memorization, Data leakage | Differential privacy, Federated learning|
| **Ethical Alignment**     | Value locking, Contextual moral reasoning | Constitutional AI, Value learning       |
| **System Safety**         | Cascading failures, Reward hacking       | Safe interruptibility, System monitoring|
| **Regulatory Compliance** | Audit trails, Explainability requirements | Model cards, Governance frameworks      |

---

## üîñ Dataset & Benchmark

| Name                                     | Paper                                                                                                                                                                                                             | Dataset card                                                                                                                           | Description                                                                                                                                                                                                                                                                                                                                                                                                              | Size                                                    | Source                                                                                                                                                                                                     | More                                                            |
| ---------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | ------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------- |
| "red teaming"                            | [Red Teaming Language Models with Language Models](https://arxiv.org/pdf/2202.03286)                                                                                                                              |                                                                                                                                        |                                                                                                                                                                                                                                                                                                                                                                                                                          |                                                         |                                                                                                                                                                                                            |                                                                 |
| JBB-Behaviors                            | [JailbreakBench: An Open Robustness Benchmark for Jailbreaking Large Language Models](https://arxiv.org/abs/2404.01318)                                                                                           | [JailbreakBench/JBB-Behaviors](https://huggingface.co/datasets/JailbreakBench/JBB-Behaviors)                                           | comprises a list of 100 distinct misuse behaviors                                                                                                                                                                                                                                                                                                                                                                        | - behaviors:200¬† row<br/>- judge_comparison:300 rows    | original,<br/>HarmBench,<br/>AdvBench<br/>ideas sourced from [Scalable and Transferable Black-Box Jailbreaks for Language Models via Persona Modulation](https://arxiv.org/abs/2311.03348) by Shah et al.) | [JailbreakBench leaderboard](https://jailbreakbench.github.io/) |
| HarmBench                                | [HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal](https://arxiv.org/abs/2402.04249)                                                                                   | [walledai/HarmBench](https://huggingface.co/datasets/walledai/HarmBench)                                                               | contains 510 unique harmful behaviors, split into 400 textual behaviors and 110 multimodal behaviors.                                                                                                                                                                                                                                                                                                                    | - standardÔºö200<br/>- contextualÔºö100<br/>- copyrightÔºö100 | /                                                                                                                                                                                                          | /                                                               |
| AdvBench                                 | [Universal and Transferable Adversarial Attacks on Aligned Language Models](https://arxiv.org/abs/2307.15043)                                                                                                     | [walledai¬†/AdvBench](https://huggingface.co/datasets/walledai/AdvBench)                                                                | a set of 500 harmful behaviors formulated as instructions.                                                                                                                                                                                                                                                                                                                                                               | - 520 rows                                              | /                                                                                                                                                                                                          | /                                                               |
| AIR-bench-2024                           | [AIR-BENCH 2024: A Safety Benchmark Based on Risk Categories from Regulations and Policies](https://arxiv.org/pdf/2407.17436)                                                                                     | [stanford-crfm/air-bench-2024](https://huggingface.co/datasets/stanford-crfm/air-bench-2024)                                           | - a comprehensive safety evaluation benchmark designed to assess LLM safety across 314 risk categories derived from eight government regulations and sixteen corporate policies.<br/>- AIR-Bench 2024 dataset comprises 5,694 diverse prompts spanning domains such as system operations, content safety, societal impacts, and legal risks.[[2]](The Hidden Risks of Large Reasoning Models: A Safety Assessment of R1) | - 5694 rows                                             | /                                                                                                                                                                                                          | Leaderboard: https://crfm.stanford.edu/helm/air-bench/latest    |
| Malicious-Educator(Application required) | [H-CoT: Hijacking the Chain-of-Thought Safety Reasoning  Mechanism to Jailbreak Large Reasoning Models, Including  OpenAI o1/o3, DeepSeek-R1, and Gemini 2.0 Flash Thinking](https://arxiv.org/html/2502.12893v1) | [DukeCEICenter/Malicious_Educator_hcot_DeepSeek-R1](https://huggingface.co/datasets/DukeCEICenter/Malicious_Educator_hcot_DeepSeek-R1) | a dataset of 50 queries covering the following ten highly sensitive topics: Economic Crime, Violence, Drug Abuse, Copyright Violations, Human Trafficking and Illegal Immigration, Self-Harm, Cybercrime, Endangering National Security (Terrorism), Trespassing on Critical Infrastructure, and Sexual Crime.                                                                                                           | - 50 rows                                               | /                                                                                                                                                                                                          | /                                                               |
| MITRE                                    | [CYBERSECEVAL 3: Advancing the Evaluation of Cybersecurity Risks and Capabilities in Large Language Models](https://arxiv.org/pdf/2408.01605)                                                                     |                                                                                                                                        | Cyber Attack                                                                                                                                                                                                                                                                                                                                                                                                             | 377                                                     |                                                                                                                                                                                                            |                                                                 |
| Interpreter                              |                                                                                                                                                                                                                   |                                                                                                                                        | Code Exc                                                                                                                                                                                                                                                                                                                                                                                                                 | 500                                                     |                                                                                                                                                                                                            |                                                                 |
| Phishing                                 |                                                                                                                                                                                                                   |                                                                                                                                        | Spear Phishing                                                                                                                                                                                                                                                                                                                                                                                                           | 200                                                     |                                                                                                                                                                                                            |                                                                 |
| XSTest                                   |                                                                                                                                                                                                                   |                                                                                                                                        | Over-refusal                                                                                                                                                                                                                                                                                                                                                                                                             | 250                                                     |                                                                                                                                                                                                            |                                                                 |
| WildGuard                                |                                                                                                                                                                                                                   |                                                                                                                                        | Jailbreak                                                                                                                                                                                                                                                                                                                                                                                                                | 810                                                     |                                                                                                                                                                                                            |                                                                 |
| Injection                                |                                                                                                                                                                                                                   |                                                                                                                                        | Prompt injection                                                                                                                                                                                                                                                                                                                                                                                                         | 251                                                     |                                                                                                                                                                                                            |                                                                 |
| FRACTURED-SORRY-Bench                    |                                                                                                                                                                                                                   |                                                                                                                                        |                                                                                                                                                                                                                                                                                                                                                                                                                          |                                                         |                                                                                                                                                                                                            |                                                                 |
| MaliciousInstruct                        |                                                                                                                                                                                                                   |                                                                                                                                        |                                                                                                                                                                                                                                                                                                                                                                                                                          |                                                         |                                                                                                                                                                                                            |                                                                 |
| Chinese safety assessment benchmark      |                                                                                                                                                                                                                   |                                                                                                                                        |                                                                                                                                                                                                                                                                                                                                                                                                                          |                                                         |                                                                                                                                                                                                            |                                                                 |
| ShadowAlignment                          |                                                                                                                                                                                                                   |                                                                                                                                        |                                                                                                                                                                                                                                                                                                                                                                                                                          |                                                         |                                                                                                                                                                                                            |                                                                 |
| In-The-Wild Jailbreak Prompts            |                                                                                                                                                                                                                   |                                                                                                                                        |                                                                                                                                                                                                                                                                                                                                                                                                                          |                                                         |                                                                                                                                                                                                            |                                                                 |
| JailbreakBench                           |                                                                                                                                                                                                                   |                                                                                                                                        |                                                                                                                                                                                                                                                                                                                                                                                                                          |                                                         |                                                                                                                                                                                                            |                                                                 |
| IMDb                                     |                                                                                                                                                                                                                   |                                                                                                                                        |                                                                                                                                                                                                                                                                                                                                                                                                                          |                                                         |                                                                                                                                                                                                            |                                                                 |
| MultiJail                                |                                                                                                                                                                                                                   |                                                                                                                                        |                                                                                                                                                                                                                                                                                                                                                                                                                          |                                                         |                                                                                                                                                                                                            |                                                                 |
| XSafety                                  |                                                                                                                                                                                                                   |                                                                                                                                        |                                                                                                                                                                                                                                                                                                                                                                                                                          |                                                         |                                                                                                                                                                                                            |                                                                 |
| JailbreakHub                             |                                                                                                                                                                                                                   |                                                                                                                                        |                                                                                                                                                                                                                                                                                                                                                                                                                          |                                                         |                                                                                                                                                                                                            |                                                                 |
| PARDEN                                   |                                                                                                                                                                                                                   |                                                                                                                                        |                                                                                                                                                                                                                                                                                                                                                                                                                          |                                                         |                                                                                                                                                                                                            |                                                                 |
| EasyJailbreak                            |                                                                                                                                                                                                                   |                                                                                                                                        |                                                                                                                                                                                                                                                                                                                                                                                                                          |                                                         |                                                                                                                                                                                                            |                                                                 |


---

## üìö Research Papers(coming soon)
### Foundational Works
- [2023] [Towards Safer Large Reasoning Models: A Survey of Risks in Multistep Reasoning Systems](https://arxiv.org/abs/example )  
  *Comprehensive taxonomy of LRM safety risks*

### Attack Vectors
- [2024] [Hidden Triggers in Reasoning Chains: New Attack Surfaces for LRMs](https://arxiv.org/abs/example )  
  *Demonstrates adversarial manipulation of reasoning steps*

### Defense Mechanisms
- [2025] [GuardReasoner: Towards Reasoning-based LLM Safeguards](https://arxiv.org/abs/2501.18492 )  
  *A Novel Guardrail Model for LLMs by Learning to Reason*
- [2024] [Reasoning with Guardrails: Constrained Decoding for LRM Safety](https://arxiv.org/abs/example )  
  *Novel approach to step-wise constraint enforcement*
  
*(Add your collected papers here with proper categorization)*

---

## üõ†Ô∏è Projects & Tools(coming soon)
### Model-Specific Resources
- **DeepSeek-R1 Safety Kit**  
  Official safety evaluation toolkit for DeepSeek-R1 reasoning modules

- **OpenAI o1 Red Teaming Framework**  
  Adversarial testing framework for multi-turn reasoning tasks

### General Tools(coming soon)
- [ReasonGuard](https://github.com/example/reasonguard )  
  Real-time monitoring for reasoning chain anomalies

- [Ethos](https://github.com/example/ethos )  
  Ethical alignment evaluation suite for LRMs

---

## ü§ù Contributing
We welcome contributions! Please:
1. Fork the repository
2. Add resources via pull request
3. Ensure entries follow the format:
   ```markdown
   - [Year] [Paper Title](URL)  
     *Brief description (5-15 words)*
   ```
4. Maintain topical categorization

See [CONTRIBUTING.md](CONTRIBUTING.md) for detailed guidelines.

---

## üìÑ License
This project is licensed under the MIT License - see [LICENSE](LICENSE) for details.

---

## ‚ùì FAQ
**Q: How do I stay updated?**  
A: Watch this repo and check the "Recent Updates" section (coming soon).

**Q: Can I suggest non-academic resources?**  
A: Yes! Industry reports and blog posts are welcome if they provide novel insights.

**Q: How are entries verified?**  
A: All submissions undergo community review for relevance and quality.

---
## üîó References

Ma, X., Gao, Y., Wang, Y., Wang, R., Wang, X., Sun, Y., Ding, Y., Xu, H., Chen, Y., Zhao, Y., Huang, H., Li, Y., Zhang, J., Zheng, X., Bai, Y., Wu, Z., Qiu, X., Zhang, J., Li, Y., Sun, J., Wang, C., Gu, J., Wu, B., Chen, S., Zhang, T., Liu, Y., Gong, M., Liu, T., Pan, S., Xie, C., Pang, T., Dong, Y., Jia, R., Zhang, Y., Ma, S., Zhang, X., Gong, N., Xiao, C., Erfani, S., Li, B., Sugiyama, M., Tao, D., Bailey, J., Jiang, Y.-G. (2025). *Safety at Scale: A Comprehensive Survey of Large Model Safety*. arXiv:2502.05206.

---

> *"With great reasoning power comes great responsibility."* - Adapted from [AI Ethics Manifesto]



